transformer,Scaled Dot-Product Attention Invalid Configuration Error on Large batch size," Scaled DotProduct Attention Invalid Configuration Error on Large batch size  Summary The `torch.nn.functional.scaled_dot_product_attention` (sdpa) function is not working as expected when the batch size is large. It causes a `RuntimeError: CUDA error: invalid configuration argument`. This problem affects also the `torch.nn.TransformerEncoderLayer` as it relies on the `scaled_dot_product_attention` function.  Reproducing the error The following scripts are run with:  However, the same results can be obtained without setting `CUDA_LAUNCH_BLOCKING=1`.  SDPA Example This code will raise the error:    TransformerEncoderLayer Example This code will raise the error:    Cause of The Error I searched a little bit for the cause of the error. Here is what I found: The error is caused in file `pytorch/aten/src/ATen/native/transformers/cuda/attention.cu`. In particular the `launchKernel` lambda inside `_efficient_attention_forward` is responsible for the error. To my understanding this is the code snippet that triggers the invalid configuration:  It appears that, with a large batch size this kernel is run with a number of blocks that exceeds the maximum number of kernel blocks, `65,535`. This triggers the invalid configuration error. As a matter of fact, running the snippet  will not raise the error. Doing the same with a batch size of `65536` will raise the error. Note that also the number of heads contributes to the number of cuda blocks. So also an increased number of heads will trigger the error.   SDPA Workaround The error can be avoided by simply removing the number of heads dimension in the input tensor. For example,  Note that, the pytorch d",2024-12-06T10:54:13Z,,open,0,0,https://github.com/pytorch/pytorch/issues/142228
yi,implement LazyInductorBenchmarker,  CC(implement LazyInductorBenchmarker)  CC(implement GroupedInductorBenchmarker)  CC(implement InductorBenchmarker)  CC(refactor benchmarking to use dynamo_timed) ,2024-12-06T01:23:34Z,module: rocm module: inductor ciflow/inductor,open,0,1,https://github.com/pytorch/pytorch/issues/142201," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
yi,implement LazyInductorBenchmarker, * (to be filled) ,2024-12-06T01:22:16Z,module: rocm module: inductor ciflow/inductor,open,0,1,https://github.com/pytorch/pytorch/issues/142196," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
rag,Fix reductions for NJTs with ragged_idx != 1,"  CC(Support narrow() on batch dim for NJT)  CC(Unbacked SymInt fixes for subclasses + datadependent slice() bounds)  CC(Fix reductions for NJTs with ragged_idx != 1) **Background:** conversion from outer dim > inner dim makes the (previously valid) assumption that the ragged dim is immediately next to the batch dim. This is no longer the case after CC(Allow any single nonbatch dim to be ragged for NJT). This PR: * Updates the outer dim > inner dim conversion logic to match the actual ragged_idx. Since ragged_idx tells us where the packed ragged / batch dim is, both ragged and batch outer dims should map to this inner dim. The conversion logic must now take in `ragged_idx` to make this possible, so the PR updates all callsites to pass this. * Fixes outputs across keepdim settings when reducing over ragged / batch dims.",2024-12-05T21:41:07Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/142173, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,torch.export.export fails to export a model with dynamic shapes for a custom type, üêõ Describe the bug torch.export.export fails to keep a dynamic dimension. The script is using a custom class. Is it a bug or did I forget something when I registed the custom class?   Versions ,2024-12-05T19:27:45Z,,open,0,0,https://github.com/pytorch/pytorch/issues/142161
yi,avoid re-applying views for every output when there is no pending mutations. ,"  CC(avoid reapplying views for every output when there is no pending mutations. ) In  words See  CC(auto-grad graph replicate split_with_sizes(lengths) X times where X = len(lengths) effecting compile time )  It's due to this code:  When functionalized the traced program, we sync all graph outputs in case they have any pending mutations (code: https://github.com/pytorch/pytorch/blob/main/torch/_functorch/_aot_autograd/functional_utils.pyL77) That sync logic effectively says: if there is a pending mutation on one of the aliases, we need to regenerate the current (alias) tensor by replaying all of the views from the updated base tensor (code: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/FunctionalTensorWrapper.cppL363) In the case where one of those views is a split_with_sizes call, we will just... go ahead and run the view. We will do this for every graph output (which in this case, is all N views, each of which results in a split_with_size call. In principle this is causing us to generate a graph with some unnecessary ops, that we could rely on the compiler to optimize. Although here, it sounds like all of these extra nodes are making a pretty big impact in the E2E compile time. For this particular case, there is probably something simpler that we can do: there really shouldn't be any pending mutations on any of the graph outputs at the time that we return them. This is because when we first generated swaps in the example above, the mutation was already applied. There are not pending mutations at the time that we generate that view. We actually had an old optimization that did it (originally for XLA) I have a PR that adds ",2024-12-05T06:45:55Z,topic: not user facing keep-going,open,0,4,https://github.com/pytorch/pytorch/issues/142118,"draft, so don't review yet?","> draft, so don't review yet?  I made it draft because I have to fix all the failures. they are still coming ",XLA change once it lands the error here should be go https://github.com/pytorch/xla/pull/8458,I think there is 2 tests that need deeper look and might be legit issues   
yi,`linux.aws.a100` instances randomly dying for a few jobs," üêõ Describe the bug A few jobs that uses the `linux.aws.a100` label are sometimes disconnecting midjob, (example). We don't have a clear reasoning yet for the reasonings.  Versions master /pytorchdevinfra",2024-12-04T19:44:48Z,module: ci triaged,open,0,2,https://github.com/pytorch/pytorch/issues/142069,"I identified for the job that failed, the runner got evicted by SPA when it should not to. And the SPA failed.  Seems that there is a bug on SPA runs.","No, wrong, the spa was correct, it correctly waited instances to terminate. My suspicion is on memory now."
transformer,[export] NotImplementedError: No registered serialization name for <class 'diffusers.models.modeling_outputs.Transformer2DModelOutput'>, üêõ Describe the bug Flux model has been compiled using TorchTensorRT and I'm trying to export it using `torch.export.export` and see the following error   Here's the full script to reproduce the error   cc:    Versions torch.version '2.6.0.dev20241202+cu124' trt.version '10.6.0.post1' ,2024-12-04T01:54:18Z,oncall: pt2 oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/142025,"We need to go here and update their code to also include the serialized type name, similar to what I did here",Thanks  that got me forward and now it results in the  CC([export] Saving models over 4GiB requires pickle protocol 4 or higher ). Any suggestions on this ? 
transformer,[export] Saving models over 4GiB requires pickle protocol 4 or higher ," üêõ Describe the bug For large models compiled with TorchTensorRT, when we try to save them we get an error as follows   Two models have this issue flux schnell and laion2b_s34b_b82k_augreg_soup  from open_clip Is there a way to pass the pickle protocol to torch.export.save similar to torch.save ?  cc:   Here's a script to reproduce the flux error    Versions  torch.__version__ '2.6.0.dev20241202+cu124'  trt.__version__ '10.6.0.post1' ",2024-12-03T22:08:24Z,oncall: pt2 oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/142004, Any updates on this issue ?, See https://github.com/pytorch/pytorch/pull/142253
transformer,Forward from pytorch/executorch: export error with Macbook pro M4 ," üêõ Describe the bug Original Issue: https://github.com/pytorch/executorch/issues/7127  when I export .pte from Llama3.2 1B, it is enterrupt. the error is blow: python m examples.models.llama.export_llama checkpoint ""/Users/qitmac001443/.llama/checkpoints/Llama3.18B/consolidated.00.pth"" params ""/Users/qitmac001443/.llama/checkpoints/Llama3.18B/params.json"" kv use_sdpa_with_kv_cache X d bf16 metadata '{""get_bos_id"":128000, ""get_eos_ids"":[128009, 128001]}' output_name=""llama3_1.pte"" INFO:root:Applying quantizers: [] INFO:root:Loading model with checkpoint=/Users/qitmac001443/.llama/checkpoints/Llama3.18B/consolidated.00.pth, params=/Users/qitmac001443/.llama/checkpoints/Llama3.18B/params.json, use_kv_cache=True, weight_type=WeightType.LLAMA INFO:root:model.to torch.bfloat16 INFO:root:Loading custom ops library: /Users/qitmac001443/Desktop/workspace/executorch/.venv/lib/python3.12/sitepackages/executorch/extension/llm/custom_ops/libcustom_ops_aot_lib.dylib INFO:root:Model after source transforms: Transformer( (tok_embeddings): Embedding(128256, 4096) (layers): ModuleList( (031): 32 x TransformerBlock( (attention): Attention( (wq): Linear(in_features=4096, out_features=4096, bias=False) (wk): Linear(in_features=4096, out_features=1024, bias=False) (wv): Linear(in_features=4096, out_features=1024, bias=False) (wo): Linear(in_features=4096, out_features=4096, bias=False) (kv_cache): KVCache() (SDPA): SDPA( (kv_cache): KVCache() ) (apply_rotary_emb): RotaryEmbedding() ) (feed_forward): FeedForward( (w1): Linear(in_features=4096, out_features=14336, bias=False) (w2): Linear(in_features=14336, out_features=4096, bias=False) (w3): Linear(in_feature",2024-12-02T18:30:55Z,oncall: export,open,0,4,https://github.com/pytorch/pytorch/issues/141893,Forwarded here as Pytorch compiler asked me to do this if we got user export issues in executorch repo. ,Feels like it belongs in ET,Could they try using nonstrict?,"And providing the TORCH_LOGS=""+export"" with the original code? "
yi,Jobs failing on Calculate Docker image step when modifying Docker files," üêõ Describe the bug I had to revert the https://github.com/pytorch/pytorch/pull/140417 last week since multiple jobs started to fail on Calculate Docker image step. We created test PR: https://github.com/pytorch/pytorch/pull/141779 Similar failures can be seen on test PR here: https://github.com/pytorch/pytorch/actions/runs/12076210926/job/33677243151 PR: https://github.com/pytorch/pytorch/pull/141779 This looks like Infra issue, need to look into  Versions 2.6.0 nightly /pytorchdevinfra",2024-12-02T17:31:00Z,high priority triage review module: ci triaged module: infra unstable,open,0,38,https://github.com/pytorch/pytorch/issues/141885,"This is a pretty standard flake behavior we are observing in CI, isn't it?","I had a very similar issue while building for sm89, where runner just died on me with no good explanation, see:   https://github.com/pytorch/pytorch/actions/runs/12039373543/job/33576305166   https://github.com/pytorch/pytorch/actions/runs/12039373543/job/33618633219","From your example https://github.com/pytorch/pytorch/pull/141779, all the docker build jobs (running on 12xlarge) were fine while build jobs failed (running on 2xlarge).  That GHA is trying to rebuild the docker image if it couldn't find one instead of waiting for docker build jobs.  It's a simple setup, but it has some drawbacks: * Building docker images on 2xlarge could fail.  Maybe we could support retry here after the docker build jobs finish successfully.  Once the images are there, subsequent retry won't need to rebuild it again. * Rate limit to docker.io where multiple build jobs are trying to build similar images",More strange runnerdiedduringthebuild failures:  https://github.com/pytorch/pytorch/actions/runs/12061405532/job/33633401182,Could this be a OoM error?,"I see 2x.large is probably not enough, Maybe we should migrate these jobs to 4x.large ?","> From your example CC(Test docker build), all the docker build jobs (running on 12xlarge) were fine while build jobs failed (running on 2xlarge). That GHA is trying to rebuild the docker image if it couldn't find one instead of waiting for docker build jobs. It's a simple setup, but it has some drawbacks: >  > * Building docker images on 2xlarge could fail.  Maybe we could support retry here after the docker build jobs finish successfully.  Once the images are there, subsequent retry won't need to rebuild it again. > * Rate limit to docker.io where multiple build jobs are trying to build similar images I remember seeing metrics related to CPU / Memory in CloudWatch. We could confirm by checking the timestamp of the logs and cross referencing it with the metrics data for that time to see if it ran out of resources. I'll poke around and see if I can confirm that today.",Looks like Memory was fine. You can see the most it used is about 8GB. Apologies for not including the legend but the Yaxis here is 16 GB ram.  CPU however looks like it got overloaded and then crashed. Yaxis here is % of CPU.  Orange bar is CPU. So I think these jobs need to run on a beefier system. Maybe a 4xlarge. In terms of memory though 16 GB seems to be more than enough so I'd be cautious about staying on the c5 series since it will also increase our RAM. I wonder if there's an instance type that provides 1:1 CPU/Memory,"It is strange, given cpu overuse should not make the job cancel, lack of memory might...","> It is strange, given cpu overuse should not make the job cancel, lack of memory might... It would if CPU was so overloaded that the GitHub Runner process couldn't schedule any CPU time to report back to the GitHub API and / or ALI that its still present. I suspect the GitHub API and / or ALI will then think that the system has been lost and schedule it for termination.","no, for sure it was memory, I can confirm with 100% confidence, at least for i0dab1612d50ef633d !Screenshot 20241203 at 15 44 51",I was checking `i0fed297c90dd524a8` regarding this build  https://github.com/pytorch/pytorch/actions/runs/12076210926/job/33677241725?pr=141779 Maybe there's multiple issues here. Edit: I just checked all the failed jobs in that workflow and all of them experienced a spike in CPU before terminating.,"I see the same picture for `i0fed297c90dd524a8`, it used almost all available memory, until it crashed and then freed some memory.  My suspicion are 100% on memory for this one as well...  !Screenshot 20241203 at 15 51 05","anyway, I guess we all agree that we probably want to migrate those jobs to `linux.4xlarge`","> anyway, I guess we all agree that we probably want to migrate those jobs to `linux.4xlarge` Agree. I think there's 2 similar but different issues reported in this issue. 1.  's workflow https://github.com/pytorch/pytorch/actions/runs/12039373543 seems to be OoM related 2.  's workflow https://github.com/pytorch/pytorch/actions/runs/12076210926 which seems to be CPU related 's workflow https://github.com/pytorch/pytorch/actions/runs/12061405532 I'm not sure about. At least 1 job seems possibly to be CPU related however other jobs in that workflow that failed seem to be fine on both CPU and Memory.",can you share the data you are looking at? for  workflow i checked `i0526d072430a911a3` and this again is 100% OoM problem. !Screenshot 20241203 at 16 10 22 Up to this point I could not find one that wasn't over memory pressure.,"For example looking at `i0fed297c90dd524a8`. CPU maxed out while RAM seems fine to me doesn't appear to go above 8 GB used.  Green bar is total available memory and orange bar is used memory, orange bar never reaches the green bar. While the blue bar represents % of CPU used and it maxes out just before instance is terminated. Unless I'm misreading the metrics here?",Ok Thank you for investigation  and  will amend the PR https://github.com/pytorch/pytorch/pull/140417 with bigger linux.4xlarge instances for the failed jobs," please do so for all jobs that use that script, in doubt, I believe there is minimal harm on using linux.4xlarge for most/all as the $ difference is not that significant"," I see, just look at free memory, not used memory, and don't use averages, use max/min :)  There are some quirks on how cloudwatch capture this metric.","so, if you look for free memory, and instead of the 'average' for the 5 minutes, you use minimum, you'll be able to see that was memory as well for i0fed297c90dd524a8: !Screenshot 20241203 at 17 00 30","> so, if you look for free memory, and instead of the 'average' for the 5 minutes, you use minimum, you'll be able to see that was memory as well for i0fed297c90dd524a8: Ah okay, thanks for the tip! I see it now.","I think a better solution in the long run is to somehow tell the build job to wait in this case until the docker build job finishes.  At the moment, there is no connection between these twos, so both of them are trying to build the same Docker image.  While the build job uses `linux.2xlarge` by default as it's all it needs to build PyTorch with sccache, the docker build job uses a bigger `linux.12xlarge` runner.  So, even when we up the default build runner to be `4xlarge`, it would still be different than the one used by Docker build. * We probably cannot use `linux.12xlarge` for all build jobs because of the cost.  As it's also a waste most of the time because a `12xlarge` is only needed when rebuilding Docker images. * Having too many build jobs trying to rebuild the Docker images directly contribute to the rate limit issue to docker.io (As they all try to access the site at roughly the same time)",> * Having too many build jobs trying to rebuild the Docker images directly contribute to the rate limit issue to docker.io (As they all try to access the site at roughly the same time) What if we built a local mirror of Docker Hub inside of the AWS environment. This would reduce the need to reach out to the internet for the same images over and over. We should be able to setup a pull through cache following this: https://docs.docker.com/dockerhub/mirror/,"Yeah, having a cache would help.  I remember   and  mentioned that as a potential solution a while ago","I did set this up for arc, maybe something we can elaborate as a project.",Another idea that might not require us to setup any additional infrastructure. Apparently AWS has a mirror of Docker Hub. https://gallery.ecr.aws I'll do some research today and see if I can figure out how to use it.,"Hi  not sure mirror will help here, most of these images are actually in aws ecr","> Hi  not sure mirror will help here, most of these images are actually in aws ecr I think we fetch base images from hub.docker.com and then create new images that are pushed to ECR right? So the situation with the Docker Hub API limit is regarding the base images.","   you are correct. But I believe we should start and track a discussion for it in another issue, to not lose track nor diverge the main subject on this one :) "
transformer,Can not convert Llama-3.2-11B-Vision-Instruct by torch.jit.script / torch.jit.trace," üêõ Describe the bug I am trying to convert the `*.pth` file to `*.pt` file for using libtorch to load the model in C++. But both `torch.jit.script` or `torch.jit.trace` will emit a not support type error. I encountered this error, but I'm not sure if I misused the function, if it's an unimplemented feature, or if it's an upstream issue. Thank you for your guidance.    Versions  ",2024-12-02T08:35:22Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/141855
yi,Fix performance-unnecessary-copy-initialization,Fixes ISSUE_NUMBER ,2024-11-29T03:03:21Z,oncall: jit module: cpu open source better-engineering Merged NNC ciflow/trunk release notes: jit,closed,0,8,https://github.com/pytorch/pytorch/issues/141792, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""Pending job""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,running my facebook/bart-base for summarization task :  MPS does not support cumsum op with int64 input," üêõ Describe the bug ''' Summarization implementation ''' from domain.interfaces import SummarizationServiceInterface from transformers import AutoTokenizer, AutoModelForSeq2SeqLM import torch class SummarizationService(SummarizationServiceInterface):     """"""Summarizes text using a pretrained model.""""""     def __init__(self, model_name: str = ""facebook/bartbase"", device: str = ""mps""):         """"""         Initializes the summarization service.         Args:             model_name (str): Name of the pretrained model to load.             device (str): Device to run the model on ('cpu' or 'mps').         """"""         self.device = torch.device(device)         self.tokenizer = AutoTokenizer.from_pretrained(model_name)         self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)     def summarize(self, texts: list[str]) > str:         """"""         Summarizes a list of texts.         Args:             texts (list[str]): A list of text to summarize.         Returns:             str: The summarized text.         """"""         combined_text = "" "".join(texts)         inputs = self.tokenizer(             combined_text, max_length=1024, return_tensors=""pt"", truncation=True         ).to(self.device)         summary_ids = self.model.generate(             inputs[""input_ids""], max_length=150, min_length=40, num_beams=1, early_stopping=True         )         return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)         The error message is:         Summarizing Cluster 231 with 153 reviews... /Users/..../Code/clustersummreviews/venv/lib/python3.9/sitepackages/transformers/generation/configuration_utils.py:638: UserWarn",2024-11-29T01:13:41Z,triaged module: mps,open,0,1,https://github.com/pytorch/pytorch/issues/141786,"This is likely not a problem with PyTorch itself, as downstream is trying to use `cumsum` with `torch.int64`, which is not supported with the MPS backend. If the error originates in hugging face code I can contrib a fix in their code base. Can you please provide a minimal reproduction example that can be run topdown to get the same error that you got?  label ""module: mps"""
llm,torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(set) __contains__, üêõ Describe the bug   Error logs   Versions  ,2024-11-28T10:05:44Z,triaged oncall: pt2 module: dynamo,open,0,2,https://github.com/pytorch/pytorch/issues/141762, Could you please provide some suggestions on this issue/PR. Thanks.,cc:   
transformer,Lint: switch oncall owner for test_transformers,  CC(Lint: switch oncall owner for test_transformers),2024-11-27T21:09:28Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/141722," merge f ""Lint is green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,compiling attention layer with dynamic shapes yields nans,"Repro below minified from a torchtune model, still investigating:  ",2024-11-27T19:04:13Z,high priority triaged oncall: pt2 module: dynamic shapes,closed,0,5,https://github.com/pytorch/pytorch/issues/141710,Much smaller repro: ,"https://github.com/pytorch/pytorch/pull/141725 is a correctness fix, although it requires guarding on the exact value of the float. I can work on avoiding the recompilations, although it seems to me like there are two options: (1) make `scale` a `c10::Scalar`. This will be BCbreaking for the AOTInductor shim, and might make eager mode SDPA a bit slower (I'm not sure exactly how expensive it is to box and unbox a Scalar into a float in all of our eager kernels?) (2) make scale a `SymFloat`: this will require teaching the codegen + python arg parser to handle SymFloat  do we have any prior art around SymFloat that cause us to prefer (1) or (2)?","Even though that PR fixes the NaNs, I still get silently wrong results between compile and eager... still looking","With https://github.com/pytorch/pytorch/pull/141728, I now get the same results on the smaller SDPA snippet repro. I still don't get completely equivalent results between eager and compile on the E2E repro in the original post on this issue, although they are very close. I think the small differences are coming from us compiling rms_norm  filed an issue for it here:  CC(inductor and eager gives slightly different results for rms_norm, with `emulate_precision_casts=True`)","One clarification is that this is only a problem if you are running code where the innermost query dim is dynamic (`query.size(1)`), which from talking to  sounds less common"
transformer,xpu: running Huggingface hiera with initializer_range=1e-10 renders Nan output,"With: * PyTorch: https://github.com/pytorch/pytorch/commit/f2d388eddd25b5ab5b909a8b4cbf20ff34c6f3f2 * https://github.com/intel/torchxpuops/commit/15f6d654685a98b6bbf6b76ee094258b16dd21ca * https://github.com/huggingface/transformers/commit/1f6b423f0ce0c6e7afd300b59e5fd1a816e4896c * https://github.com/huggingface/accelerate/commit/e11d3ceff3a49378796cdff5b466586d877d5c60 Running Huggingface `hiera` model with nondefault `initializer_range=1e10` (default is `0.02` and it works) on Pytorch XPU backend (I used Intel PVC platform) fails  output tensor contains `Nan` values. This behavior was first found running the following Huggingface Transformer tests for `hiera` model:  Instead of running Huggingface Transformers, it's possible to reproduce the issue on the following script which is my attempt to extract essential behavior from HF test:  Issue appears when `initializer_range=1e10`. That's what HF Transformers are doing in the tests (see tests/test_modeling_common.pyL149). Output on XPU will be:  And running with default `initializer_range=0,02` on XPU you will get reasonable results (named HF hiera tests will pass with this value, but other tests form hiera will start to fail):  **Note that CUDA does handle `initializer_range=1e10` in a different way** and does not return `Nan` tensors. However, what CUDA returns is suspiciously ""good"":  And for complete picture on CUDA, that's what it returns on `initializer_range=0,02`:  CC:        ",2024-11-27T01:27:25Z,triaged module: xpu,open,0,8,https://github.com/pytorch/pytorch/issues/141642,"My guesses for the issue: 1. XPU backend calculates somewhere with the lower precision than CUDA 1. CUDA has different handling for corner cases when calculations step into precision limits Considering that CUDA did return too good results for `initializer_range=1e10`, I think that 2nd variant might be the case.",deng is looking into this issue. ,Registering a hook to dump outputs from each layer:  First time `inf/nan` appear on one of the `LayerNorm` layers. After that they waterfall in calculations in the following layers. Below output is output of layers from the very first till the `LayerNorm` where `inf/nan` appear: ,"Ok, so this seems to be an issue with XPU implementation of `LayerNorm`. Slightly modifying the hook to dump layer outputs, it's easy to dump an input of the first `LayerNorm` which has nan/inf values. I did that on my side and uploaded captured input here: https://github.com/dvrogozh/pytorch/blob/for_141642/PT_141642.pt. After that using the following script you can reproduce the issue:  Output will be:  And modifying `device` in the script to be `cpu`, you will get:  So, outputs for LayerNorm operation are different between cpu and xpu with xpu having nan/inf in some values (not in all values).","> My guesses for the issue: >  > 1. XPU backend calculates somewhere with the lower precision than CUDA > 2. CUDA has different handling for corner cases when calculations step into precision limits >  > Considering that CUDA did return too good results for `initializer_range=1e10`, I think that 2nd variant might be the case. As for 1: Current implementation for Layernorm in XPU is on the top of the onepass algorithm instead of Welford algorithm, which may lead some precision drop.","Sync with  , we will try to implement the Welford algorithm and then check if it can help. Besides, For  's guess 2, we will check the implementation for precision handling in this case.  ","Thank you, deng, !  Do we know which algorithm is implemented for CPU and CUDA? Welford? Can we just convert CUDA kernel to SYCL kernel to get the implementation?","> Thank you, deng, ! >  > Do we know which algorithm is implemented for CPU and CUDA? Welford? >  > Can we just convert CUDA kernel to SYCL kernel to get the implementation? Welford is the common solution and our changes are currently in progress"
transformer,xpu: implement aten::_thnn_fused_lstm_cell for XPU backend,"The following aten operator is not currently implemented for XPU backend and does not allow fallback with `PYTORCH_ENABLE_XPU_FALLBACK=1`:  [ ] `aten::_thnn_fused_lstm_cell`, https://github.com/intel/torchxpuops/pull/926 With: * https://github.com/huggingface/transformers/commit/bdb29ff9f3b8030772bd4be037d061f253c0e928 * https://github.com/huggingface/accelerate/commit/e11d3ceff3a49378796cdff5b466586d877d5c60 * https://github.com/pytorch/pytorch/commit/f2d388eddd25b5ab5b909a8b4cbf20ff34c6f3f2 Can be reproduced running Huggingface Transformers `encodec` model tests:  Unfortunately that's one of those aten operators for which running with `PYTORCH_ENABLE_XPU_FALLBACK=1` does not help. If ran with it getting this error:  CC:      ",2024-11-26T02:25:49Z,triaged module: xpu,open,0,1,https://github.com/pytorch/pytorch/issues/141539,Related PR: https://github.com/intel/torchxpuops/pull/926
rag,Adjust output NJT ragged_idx for reductions and select(),"  CC(NJT: Return correct number of outputs for chunk() on the batch dim)  CC(Adjust output NJT ragged_idx for reductions and select())  CC(NJT unsqueeze() fixes)  CC(Initial NJT testing over dim type / views)  CC(Forward / backward NJT support for several activation functions)  CC(Fix where() for NJT) This fixes some bugs when performing reductions / select() on dims before the ragged dim. In this case, the output NJT has a smaller number of dims, and its ragged_idx should reflect that correctly.",2024-11-25T19:52:46Z,Merged ciflow/trunk topic: bug fixes release notes: nested tensor,closed,0,3,https://github.com/pytorch/pytorch/issues/141506, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"This PR ( CC(Adjust output NJT ragged_idx for reductions and select())) was merged in 43121b6f0d83eb03d5344724bf0f7f80bf61d771 but it is still open, likely due to a Github bug, so mergebot is closing it manually.  If you think this is a mistake, please feel free to reopen and contact Dev Infra."
yi,FlexAttention with compiled block mask is slow when varying sequence lengths," üêõ Describe the bug I understand from  CC(Flexattention: creating mask[1,1,96000,96000] causes OOM error) that it's necessary to compile `create_block_mask` to avoid materialising the full NxM mask. However when doing this with a changing sequence length, recompilation is happening at every step which is costly. Are there any plans to avoid this without adding padding? In particular I'm using sliding window attention, so I think the block mask could be independent of the sequence length. I can try using the new `create_nested_block_mask` interface added in https://github.com/pytorch/pytorch/pull/136792, but it's not clear to me that this would support changing total sequence lengths batch to batch.    Versions `torch==2.5.1`",2024-11-25T12:55:52Z,triaged module: nestedtensor oncall: pt2 module: higher order operators module: pt2-dispatcher module: flex attention,closed,0,5,https://github.com/pytorch/pytorch/issues/141486,Do you have some repro code? For instance if you compile the `create_block_mask_func` with default settings and not `dynamic=False` I imagine that automatic dynamic dims should kick in here and you shouldnt need to reocmpile,"Ok I think I was recalling `create_block_mask` at each step, which is probably not correct. But fixing that I am still having issues with the backward pass (forward looks ok now):  Running with `CUDA_LAUNCH_BLOCKING=1` gives "," running your repro gives me:  > Ok I think I was recalling create_block_mask at each step, which is probably not correct. This is generally correct to do if your max sequence length changes across iterations, especially if it grows, which it looks like it does from the above error. To avoid recompiles, dynamic shapes needs to kick in as  mentioned. You could try:  and create the block mask per iteration:  Running this with `TORCH_LOGS=""recompiles""` shows me no recompiles of `create_block_mask()` and a single recompile of `flex_attention()`.  This recompile comes from the block_mask readjustment logic here: https://github.com/pytorch/pytorch/blob/000d4e9d43276964f3ae4e2ffabc1dbb8829e517/torch/nn/attention/flex_attention.pyL1243L1247  /  it seems unfortunate that this logic would cause a recompile, as my understanding is it's an optimization to avoid the need for subsequent `create_block_mask()` calls if new inputs have smaller max sequence length than that of the first run. That said, once there's a compiled graph with a guard for entering this logic and a compiled graph with a guard for not entering it, there shouldn't be a further need to recompile.","I have an issue for the above, we are going to change the API since this is not a zero cost abstraction.  I also slightly modified the repro:  If you compile w/ dynamic shapes you hit 1 recompile for going above the int32 max elements ","Thanks  , I fixed the issue using your code and switching to the latest nightly. Performance is not far from FA2 which is great to see. I ran into nans when using float16 which did not occur with FA2, but using bfloat16 seems to be running ok."
yi,DOC: Correct torch.trapezoid docstring,"This is super duper minor, but I believe this corrects a typo in the documentation of `torch.trapezoid`. The documentation says the input is a 1dimensional tensor $y_0, \dots, y_n$, but it uses summations going from 1 to n1. Since it's summing over terms $y_i  y_{i1}$, stopping at n1 excludes the last partition $y_n  y_{n1}$, which doesn't match the implementation... ",2024-11-25T00:18:12Z,open source Merged ciflow/trunk release notes: python_frontend,closed,0,2,https://github.com/pytorch/pytorch/issues/141459, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
dspy,remove allow-untyped-defs from _inductor/bounds.py,  CC(remove allowuntypeddefs from _inductor/inductor_prims.py)  CC(remove allowuntypeddefs from _inductor/bounds.py) ,2024-11-23T23:07:00Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/141440, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,"Make computeStorageNbytes a bit more lazy, don't directly guard size oblivious","  CC(Make computeStorageNbytes a bit more lazy, don't directly guard size oblivious) Signedoffby: Edward Z. Yang ",2024-11-23T04:42:12Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/141418," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Meh, probably not going to do this."
llm,Add vLLM to torchbench,"We should add at least one vllm model to torchbench, to make sure torch.compile support doesn't regress. This might be a nonstandard ""model"" though, because vllm controls the compilation process. ",2024-11-22T21:49:10Z,triage review oncall: pt2 vllm-compile,open,0,4,https://github.com/pytorch/pytorch/issues/141386," For running vllm on torchbench (the pytorch compiler benchmark suite), what configs would you be interested in seeing? We can do the llama38b, but I don't know what the right call to invoke it is","I can set it up, but I don't know how / where to add it.","> I can set it up, but I don't know how / where to add it. We have llama and llama_v2_7b in torchbench.  https://github.com/pytorch/benchmark/tree/main/torchbenchmark/models/llama https://github.com/pytorch/benchmark/tree/main/torchbenchmark/models/llama_v2_7b_16h Can you check how we add these two models? I can help to add them if you need.  How to add models to torchbench"," thanks for the information, very helpful."
transformer,[export] A node has no users in the exported program," üêõ Describe the bug The following example produces a node with no user.  The graph contains the following line:  Version of 11/17 works, version of 11/21 fails.  Versions  ",2024-11-22T19:57:00Z,oncall: pt2 oncall: export,closed,0,6,https://github.com/pytorch/pytorch/issues/141373,"Removing the assert, the code runs for me with no errors. I had to change the call line ",But it does seem like `copy_` was no longer used.,"I took some time to investigate more, the line at stake is this one https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.pyL978. !image When I compare the two graphs, I see a dead end for this specific line. ","Here is another example not using transformers. The exported program returns ``clone`` when it should return ``copy_``. However, its execution returns the correct result but it is unexpected. ",The graph produces by 11/17 version is using slice_scatter. ,I'll close this issue. It seems the default behaviour for setitem is not to decompose it into slice_scatter anymore.
chat,Pytorch notifications," üöÄ The feature, motivation and pitch **Note: this feature is under review, this is a first rough proposal, looking for user feedback right now whether this is needed or not.** Developers within the PyTorch ecosystem have quite some signal to deal with:  Github notifications for cc'ed issues  failing CI tests on HUD / monitoring for successful tests to merge  PR reverts  Infra failures (infra teams)  oncall health for some repos (TTR for issues/reviews), regression for i.e. daily benchmarks This leads to delays in merging, late response times, frustration with the build and test system. Additionally, some PRs fall off peoples review list, and developers end up using Dev Infra Office Hours to request reviews. Some people rely on email notifications and custom mail rules to triage their signals, some people use the Github notifications, others rely on manual pings in slack/other chat tools from colleagues.  The proposal here is to look into a general purpose notification system that users can subscribe to, that can ping either oncalls for certain metrics (TTR too long, regression detected, infra down), or individuals (your PR is ready to merge, you have failing tests to look into).  Alternatives  we could stick to Github notitfications, ccbot, probot) that subscribes people to labels  We could split this up in multiple use cases, and make a 'stale PR' dashboard per repo, develop an infrafailure solution, and write a simple  bot for benchmark regressions  Additional context right now we're looking to interview developers on how they deal with this workflow, how they experience developing PyTorch and how they deal with the above mentioned cha",2024-11-21T22:38:20Z,feature triaged module: devx,open,2,0,https://github.com/pytorch/pytorch/issues/141299
rag,MPS operator coverage tracking issue (2.6+ version)," üêõ Describe the bug  This issue is to have a centralized place to list and track work on adding support to new ops for the MPS backend. **PyTorch MPS Ops Project** : Project to track all the ops for MPS backend. There are a very large number of operators in pytorch and so they are not all yet implemented. We will be prioritizing adding new operators based on user feedback. If possible, please also provide link to the network or usecase where this op is getting used. As Ops are requested we will add "" *To Triage*"" pool. If we have 3+ requests for an operation and given its complexity/need the operation will be moved ""*To be implemented*"" pool. If you want to work on adding support for such op, feel free to comment below to get assigned one. Please avoid pickup up an op that is already being worked on tracked in ""*In progress*"" pool.  Link to the wiki for details on how to add these ops and example PRs. **MPS operators coverage matrix**  The matrix covers most of the supported operators but is not exhaustive. **Please look at the `In vx.x.x` column, if the box is green, it means that the op implementation is included in the latest release; on the other hand, if the box is yellow, it means the op implementation is in the nightly and has not yet included in the latest release.** Before you comment below, please take a look at this matrix to make sure the operator you're requesting has not been implemented in nightly.  More details can be found on the readme. This is a spiritual successor of  CC(General MPS op coverage tracking issue), but hopefully it will not receive requests for the OPs that has been added already since 1.13 but until 2.6 ",2024-11-21T21:15:46Z,feature triaged tracker module: mps,open,0,0,https://github.com/pytorch/pytorch/issues/141287
finetuning,Enhanced Feedback for `load_state_dict` with `strict=False`," üöÄ The feature, motivation and pitch When developers use `load_state_dict` with `strict=False`, they often face significant challenges, especially around debugging weight loading issues. The current behavior does not provide sufficient visibility into what weights were successfully loaded and which keys were unexpected or missing. This creates confusion, as many users unknowingly end up with mismatched or unloaded weights, leading to silent failures and hours of debugging. This is especially useful for people like me who load partial weights, especially for finetuning tasks and domain adaptation. I end up writing my own script to do this all the time and I believe other researchers like me will find it very useful.  Why This is Critical 1. Lack of Feedback: 	 Developers using `strict=False` are left in the dark about what was actually loaded. 	 There‚Äôs no direct mechanism to verify mismatches or missing weights without additional manual inspection. 2. Silent Errors: 	 If the weights are not loaded as intended, there‚Äôs no clear indication. Many users proceed without realizing their model isn‚Äôt correctly initialized. 3. Common Pitfall: 	 This is a recurring issue for many developers. A seemingly harmless choice to use `strict=False` often results in broken workflows, especially for those less familiar with PyTorch‚Äôs internals.  Alternatives Introduce a argument to log detailed information when `load_state_dict` is called with `strict=False`. This feedback can include: Input  Output  1. Verbosity: Provide a verbosity flag (e.g., verbose=True) to enable or disable this detailed output, ensuring backward compatibility for users who prefer sil",2024-11-21T18:07:55Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/141256,"This seems to be the intent of `strict=False`, where users explicitly say that they do not expect to to strictly enforce that the keys in state_dict match the keys returned by this module‚Äôs state_dict() Further, the default is `strict=True`, so if you set `strict=False` you are explicitly opting out of the above I'm also curious to understand how the suggestion differs from `missing_keys` and `unexpected_keys` returned by `load_state_dict`, is the suggestion to additionally return `loaded_keys`?"
transformer,Updating from torch 2.0 to 2.1/2.2/2.3/2.4/2.5 results in gradient explosion with SDPA and bf16 ," üêõ Describe the bug We are training a huggingface transformer using pytorch native SDPA and the huggingface Trainer. We recently upgraded from torch 2.0, and found that **this change alone** caused our training runs to diverge.  We cannot upload any images and unfortunately are unable to share code since it is internal/proprietary. The training loss is almost identical across torch versions for over half the run and then they diverge. The diverges look similar to this issue  CC(CUDNN sdp attention causes loss explosion), however grad_norm can sometimes get up to values near 100. Here are some comparisons before and after different changes (all with grad clipping and max grad norm of 1.0): Torch 2.0 + bf16 + SDPA + lr=5e5 + gradient accum = 2> clean loss curve :white_check_mark: Torch **2.1/2.2/2.3/2.4/2.5** + bf16 + SDPA + lr=5e5 +  gradient accum = 2 > exploding gradients :x: Torch 2.4 + bf16 + SDPA + lr=5e5 + **gradient accum = 1** >  exploding gradients :x: Torch 2.4 +  **fp16** + SDPA + lr=5e5 + gradient accum = 2 > clean loss curve :white_check_mark: Torch 2.5 +  fp16 + SDPA + **lr=1e4** + gradient accum = 2 >exploding gradients :x: Torch 2.4 + bf16 + No SPDA (eager) + lr=5e5 + gradient accum = 2 > clean loss curve :white_check_mark:  We also tried what this issue) reported as a solution (`torch.backends.cuda.enable_cudnn_sdp(False)`) but this also resulted in exploding gradients. We are unsure whether this is an issue with SDPA after 2.0 or just finicky pretraining. Is there anything that changed post torch 2.0 that could be related to this issue? Seems odd that upgrading torch alone would cause this shift in behavior. Any help wou",2024-11-21T14:59:59Z,triaged module: sdpa,open,2,0,https://github.com/pytorch/pytorch/issues/141241
transformer,`CrossEntropyLoss` much slower with class probabilities vs. class indices target," üêõ Describe the bug Hi, I noticed this when training ViTS/16 on the ImageNet1k dataset, with and without MixUp. In short: torchvision's `v2.MixUp`, just like other common implementations, converts the class indices to onehot class probabilities before linearly interpolate them, so the same call to `CrossEntropyLoss()` goes to different kernels with vs. without MixUp. In a less optimized setting (specific commit), I found that with vs. without MixUp makes the difference between **0.198s vs. 0.120s** per step (excluding data loading) with batch size 1024 on a 8x A100SXM440GB Lambda instance. In further iterations, I included `CrossEntropyLoss` in the model so they compile together. I then tested a workaround in which the custom MixUp returns both targets and the weight scalar lam(bda):  and the model calculates the cross entropy loss by using the class indices twice:  See https://github.com/EIFY/mupvit/compare/normalprefetch...lossoptim. I found that this is faster (**0.184s** per step) than just calling `CrossEntropyLoss()` with class probabilities even with `torch.compile(...,mode=""maxautotunenocudagraphs"")` (**0.191s** per step), excluding data loading. Is this expected? Intuitively I can see that the sparsity helps forward & backward pass in loss calculation, but backward pass beyond that should be the same and it's hard to understand the observed second/step difference with a 12layer transformer. To replicate the timing result, you can run the code with fake data:  with either the normalprefetch or the lossoptim branch.  Versions The output below shows PyTorch version: 2.3.1 and torchvision==0.18.1, but I am actually running  I don't ",2024-11-20T22:32:58Z,module: nn module: loss triaged,open,1,0,https://github.com/pytorch/pytorch/issues/141177
gpt,[inductor] invalidate pointwise dep cache for LOAF,  CC([inductor] invalidate pointwise dep cache for LOAF) Fixes  CC(inductor LOAF throws an error on toy GPT model)  ,2024-11-20T20:21:37Z,topic: not user facing module: inductor ciflow/inductor,open,0,0,https://github.com/pytorch/pytorch/issues/141160
yi,[user empathy day] AOTDispatcher fails when calling copy_(int), üêõ Describe the bug  fails with:   Versions main ,2024-11-20T19:14:00Z,triaged oncall: pt2 module: pt2-dispatcher empathy-day,open,0,1,https://github.com/pytorch/pytorch/issues/141149,"Looks like `aten.copy_()` supports scalar overloads, but `aten.copy` does not: "
gpt,inductor LOAF throws an error on toy GPT model," üêõ Describe the bug When I run the repro script from https://github.com/pytorch/ao/issues/1297 with `TORCHINDUCTOR_LOOP_ORDERING_AFTER_FUSION=1`, I see an error.  Here is a simplified repro: Code: https://gist.github.com/vkuzo/7dcf4f3e9f25055c9cad23ca87b74e7a Command to reproduce: `TORCHINDUCTOR_LOOP_ORDERING_AFTER_FUSION=1 python ~/local/tmp/20241120_loaf_bug.py` Full stack trace of error: https://gist.github.com/vkuzo/2766bbd5386a8a00d1b43be171a6d29d Relevant part of stack trace:  Using `TORCHINDUCTOR_LOOP_ORDERING_AFTER_FUSION=0`, the code above runs without errors.  Versions https://gist.github.com/vkuzo/d737f40af85d6b426d209f89692885c2 ",2024-11-20T16:42:58Z,triaged oncall: pt2 module: inductor,open,0,1,https://github.com/pytorch/pytorch/issues/141134," ,  ,  "
llama,[Dynamo] Dynamo is not aware of the function users override," üêõ Describe the bug Hi, we are trying to enabling Llama in Intel HPU, but found graph break in `torch.cuda.current_device()` . We have a package GPU migration tool, it replaces `torch.cuda.current_device()` to `torch.hpu.current_device()`. But we found dynamo is not aware of the function override, and still consider `torch.cuda.current_device()`, eventually leading to graph break or program failed. Can you give us some suggestions or have any plan to fix it?  Error logs Here is a simple case   Our GPU migration tool implement, override cuda current_device():  The error gets from dynamo:   Versions  ",2024-11-20T12:07:57Z,triaged oncall: pt2 module: dynamo,open,0,1,https://github.com/pytorch/pytorch/issues/141125,"  Hi, any ideas for this issue? Will the following check help? `torch.cuda.current_device.__module__.startswith(""torch.cuda"")`"
yi,Memory explodes when applying Linear to reshaped nested tensor, üêõ Describe the bug Running the following script should reproduce the problem. Applying a simple Linear layer to a nested tensor results in cached memory explosion if the nested tensor is not manually flattened.   Versions Nightly pytorch from pip: torch                     2.6.0.dev20241028+cu121          pypi_0    pypi torchaudio                2.5.0.dev20241028+cu121          pypi_0    pypi torchvision               0.20.0.dev20241028+cu121          pypi_0    pypi ,2024-11-20T06:29:31Z,high priority triage review triaged module: nestedtensor,closed,1,1,https://github.com/pytorch/pytorch/issues/141112," Thanks for the report! I was able to track this down to NJT's `linear_backward()` computation. The problem is related to inefficient grad computation and it surfaces for inputs of higher dims, such as those you're passing in after unflattening the final dim into two. Opened a PR with a fix as well."
transformer,"Cannot view a tensor with shape torch.Size([2, 1, 8, 32]) and strides (32, 512, 64, 1) as a tensor with shape (2, 256)! for transfomer MHA with permute, view"," üêõ Describe the bug When exporting my transformer model with torch.onnx.export(... dynamo=True, ...), any sequence length >= 2 gives the error. Sequence length 1 works but does not support dynamic shapes in inference. When dynamic_shapes is off, I get the above error, and when dynamic_shapes is set and seq_len >=2, I get an error about guard lengths similar to CC(torch.export.export() fails with dynamic shapes when more than one shape is dynamic). I tried latest nightly (9 PM PST on 11/19) but didn't work. Also noticed this issue is similar to CC([export] Cannot view a tensor with shape torch.Size([1, 512, 32, 128]) and strides (2097152, 128, 65536, 1) as a tensor with shape (1, 512, 4096)) and CC([export] `run_decomposition` fails for permute>view sequence). Thanks in advance for the help! onnx_export_20241119_210853216442_conversion.md **Repro**   Versions Collecting environment information... PyTorch version: 2.6.0.dev20241112+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 24.04.1 LTS (x86_64) GCC version: (Ubuntu 13.2.023ubuntu4) 13.2.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.39 Python version: 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.39 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU Nvidia driver version: 560.94 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available:",2024-11-20T05:16:40Z,needs reproduction module: onnx oncall: pt2 oncall: export,open,1,2,https://github.com/pytorch/pytorch/issues/141107, I think you may use this as a repro,
transformer,graph break when training LoRA," üêõ Describe the bug In torchtune, we offer LoRA training, which consists of freezing transformer layers and adding just a few matrices that are trainable. In this setting,  the input to the first transformer doesnt require gradients. However, the intput to the second transformer does. You can see the loop here. We compile each transformer block independently, to avoid long compilation times. Because of that, i get a graph break:   Is there a mark_dynamic equivalent for required_grad? tlparse: https://interncacheall.fbcdn.net/manifold/tlparse_reports/tree/logs/.tmptwNd6e/index.html To reproduce:    Error logs _No response_  Versions Tried it with both: 2.5.1 and 2.6.0.dev20241119+cu124 ",2024-11-19T16:34:06Z,oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/141039,"Oh  the recompile is because: (1) you are compiling two individual layers (2) on the first layer, your input `x` does not require_grad, and so we don't need to compute gradients for it (3) on the second layer, the input to that layer is an activation, which we **do** need to compute gradients for. We need to recompile for the first layer vs the second, because we need to do less compute in the backward of the first layer. This should cause at most one recompile though  is that a problem for you? Altenratively, you could try compiling both layers together into a single graph to avoid the recompiles","not really a problem, just figuring out ways to improve it. If there isnt an easy flag, then thats ok. Thank you for taking a look!"
transformer,xpu: implement aten::_linalg_eigvals for XPU backend (affecting HF Transformers v4.46.0 and later),"Recent changes in Huggingface Transformers (https://github.com/huggingface/transformers/commit/cdee5285cade176631f4f2ed3193a0ff57132d8b and https://github.com/huggingface/transformers/commit/4a3f1a686fcda27efc19b8b3a87b62a338c2ad86) introduced usage of `torch.linalg.eigvals()` which is not implemented for XPU backend (missing `aten::_linalg_eigvals`) and it's not registered for automated CPU fallback as well. To use this operator with XPU you need to use `PYTORCH_ENABLE_XPU_FALLBACK=1`. Transformers version v4.46.0 and later are affected. Can this operator, please, be added?  [ ] `aten::_linalg_eigvals` With: * https://github.com/huggingface/transformers/commit/e80a65ba4fbbf085fda2cf0fdb0e8d48785979c8 * https://github.com/huggingface/accelerate/commit/8ade23cc6aec7c3bd3d80fef6378cafaade75bbe * https://github.com/pytorch/pytorch/commit/e429a3b72e787ddcc26ee2ba177643c9177bab24 Can be reproduced running Huggingface Transformers tests for a number of models (such as albert, mpt, gemma2). Here is example for one of the model:  CC:      ",2024-11-18T18:50:50Z,triaged enhancement module: xpu,open,1,2,https://github.com/pytorch/pytorch/issues/140965,"Regarding the missed operation, we will support it on top of mkl library. Currently, the timeline is supposed to be PT 2.8 or PT 2.7.", : can we add `aten::_linalg_eigvals` for automated CPU fallback while we are waiting for the actual implementation of `aten::_linalg_eigvals` for XPU in PT2.7/2.8?
transformer,[export][quant] AttributeError: 'FunctionalTensor' object has no attribute '_quantized_linear_op'," üêõ Describe the bug I'm trying to export a model to use w8/a8 quantisation. In my example, I'm using BERT.  However running `torch.export` fails with `AttributeError: 'FunctionalTensor' object has no attribute '_quantized_linear_op'` A traceback is here: Traceback      My code is here:   Versions  ",2024-11-18T12:11:49Z,oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/140943
transformer,TypeError: Type parameter +RV without a default follows type parameter with a default in _inductor/utils.py," üêõ Describe the bug env:  python 3.9,  torch.__version__='2.5.1'  also observed in 2.4.1 when importing transformers and torch I get error triggered by typing problem in this line:  `class CachedMethod(Protocol, Generic[P, RV]):` https://github.com/pytorch/pytorch/blob/99014a297c179862af38ee86bac2051434d3db41/torch/_inductor/utils.pyL459C1L459C46 `TypeError: Type parameter +RV without a default follows type parameter with a default` suggegsted solution:  replace  `class CachedMethod(Protocol, Generic[P, RV]):` with `class CachedMethod(Protocol, Generic[RV, P]):` in https://github.com/pytorch/pytorch/blob/99014a297c179862af38ee86bac2051434d3db41/torch/_inductor/utils.pyL459C1L459C46 possibly related to https://github.com/pytorch/pytorch/pull/127685  ERROR STACK BELOW:   Versions python 3.9 torch.__version__='2.5.1' ",2024-11-17T21:23:06Z,module: typing triaged actionable oncall: pt2,open,3,1,https://github.com/pytorch/pytorch/issues/140914,"I also met this problem with PyThon 3.12 and PyTorch 2.5.1, and it led to a fatal error in my project. Hope it can be resolved soon ü§©."
yi,flex_attention + `export_for_inference` ` NYI: querying is_contiguous inside of vmap for memory_format other than torch.contiguous_format`, üêõ Describe the bug    /   Versions nightly,2024-11-15T21:16:05Z,triaged oncall: pt2 module: higher order operators module: pt2-dispatcher dynamo-functorch module: flex attention,open,0,8,https://github.com/pytorch/pytorch/issues/140849,/ ,Is it this repro enough?,Gently ping.. is this reproduction enough? It is still reproducible for me on today nightly.,seems to be a different error on main ,Yes It seems the repro error diverged with the current nightly (I've not done a bisect of the network) But in the full model I still have ,  ` Is this new reproduction ok?,Do you need any other debug info?,"oh, I was just checking that the repro works. Flex attention people need to look.  "
yi,auto-grad graph replicate split_with_sizes(lengths) X times where X = len(lengths) effecting compile time ,"autograd graph replicate split_with_sizes(lengths) X times where X = len(lengths) effecting compile time. I think this repitions might be related to the functionalization view replay? unsure yet. We spend 50% of in split_with_sizes . strobelight profile: https://fburl.com/scuba/pyperf_experimental/on_demand/pz6u9po2 for when we have lengths size =1000 we have this replicated 1000 times. I counted how many times we call split_with_sizes when input is 8 and its like 32 times.  that means for input 1000 its maybe like 32/8 *1000 times = 16K times? This is noted while debugging :  CC(`maybe_mark_dynamic` causes max recursion error when used with compile during tensordict consolidation) ttile, this pre  Dynamo graph:  auto_grad graph:  where exactly in the code that happen ? in this function:   in /home/lsakka/pytorch/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py  ",2024-11-15T18:17:47Z,triaged module: functionalization oncall: pt2 module: pt2-dispatcher,open,0,5,https://github.com/pytorch/pytorch/issues/140835,"It's due to this code:  When functionalized the traced program, we sync all graph outputs in case they have any pending mutations (code: https://github.com/pytorch/pytorch/blob/main/torch/_functorch/_aot_autograd/functional_utils.pyL77) That sync logic effectively says: if there is a pending mutation on one of the aliases, we need to regenerate the current (alias) tensor by replaying all of the views from the updated base tensor (code: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/FunctionalTensorWrapper.cppL363) In the case where one of those views is a `split_with_sizes` call, we will just... go ahead and run the view. We will do this for every graph output (which in this case, is all N views, each of which results in a `split_with_size` call. In principle this is causing us to generate a graph with some unnecessary ops, that we could rely on the compiler to optimize. Although here, it sounds like all of these extra nodes are making a pretty big impact in the E2E compile time. For this particular case, there is probably something simpler that we can do: there really **shouldn't** be any pending mutations on any of the graph outputs at the time that we return them. This is because when we first generated `swaps` in the example above, the mutation was already applied. There are not pending mutations at the time that we generate that view.","We actually had an old optimization that did it (originally for XLA) I have a PR that adds it back here https://github.com/pytorch/pytorch/pull/140882, and with that PR, I only see a single `split_with_sizes` op in the graph That PR causes a bunch of failures in `test/test_functionalization.py` that need looking into though"," Thanks !!! seems like most of the failures are just due to different graph textual representations, hopefully nothing more than updating text",I will patch the change to eval performance.,I verified that the optimization does remove the exponential growth of compile time
transformer,xpu: CTCLossLogAlphaKernelFunctor hangs on HF hubert test_retain_grad_hidden_states_attentions,With: * (pytorch) https://github.com/pytorch/pytorch/commit/33191bb664fef338d94586a05bf1f14d17a00340 * https://github.com/huggingface/transformers/commit/a32c4e6c7b7fd79eb0c94c6c6ce9446de0877ded * https://github.com/huggingface/accelerate/commit/c0552c9012a9bae7f125e1df89cf9ee0b0d250fd Huggingface Transformers `test_retain_grad_hidden_states_attentions` test for the `hubert` model hangs. Can be reproduced with the below command line. Using https://github.com/intel/ptigpu/tree/master/tools/unitrace tool with combination of NEO driver flags we can see that `CTCLossLogAlphaKernelFunctor` kernel hangs:  Root cause is likely similar to root cause of CC(xpu: huggingface levit test_retain_grad_hidden_states_attentions test hangs on exit on PVC): `CTCLossLogAlphaKernelFunctor` kernel has early returns while barrier is used in the kernel. There are 2 return calls: * https://github.com/intel/torchxpuops/blob/b75da0cba01fadafca9df95b1df88f0ed24ed312/src/ATen/native/xpu/sycl/LossCTCKernels.cppL50 * https://github.com/intel/torchxpuops/blob/b75da0cba01fadafca9df95b1df88f0ed24ed312/src/ATen/native/xpu/sycl/LossCTCKernels.cppL58 And here is a barrier: * https://github.com/intel/torchxpuops/blob/b75da0cba01fadafca9df95b1df88f0ed24ed312/src/ATen/native/xpu/sycl/LossCTCKernels.cppL114 CC:       ,2024-11-15T03:29:03Z,triaged module: xpu,closed,0,3,https://github.com/pytorch/pytorch/issues/140781,We will update the torchxpuops to fix it., Shall we close this issue? As the relevant PR has been merged.,>  Shall we close this issue? As the relevant PR has been merged. Yes. This one can be closed since the change got propagated to PyTorch: * https://github.com/intel/torchxpuops/commit/f9c7682530f4259339b6f3cd4b24fa63518cfeb8  a fix in torchxpuops * https://github.com/pytorch/pytorch/commit/81ab2cc757bab1770433c97a713fd07b59a1d80f  update of torchxpuops commit pin in pytorch This fix trends to be available in PT 2.6.
transformer,`torch._transformer_encoder_layer_forward` usage of SDPA attention instead of native_MHA," üöÄ The feature, motivation and pitch Discussion about attention usage of: https://github.com/pytorch/pytorch/blob/02d0c43c3243f967856798fa8622a8727f9d36a3/aten/src/ATen/native/transformers/transformer.cppL109C1L122C19 It is not clear to me what kind of attention version is dispatched with _transformer_encoder_layer_forward (transformer encoder layer)  it seems that `at::_native_multi_head_attention` is used, which could potentially open up a optimization opportunity in case this not the SDPA backend. I could not find the code for `at::_native_multi_head_attention` in the Github repo. Potentially, triton_multi_head_attention dispatches SDPA or multi_head_attention_cuda, which make use of a fused attention: https://github.com/pytorch/pytorch/blob/217d328764be9a8a4c5f846f762e83100a595905/aten/src/ATen/native/transformers/attention.cppL1008  Alternatives _No response_  Additional context _No response_ ",2024-11-15T00:12:22Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/140768,we dispatch to sdpa when it is using the fused version: https://github.com/pytorch/pytorch/blob/bf78a0fa968103cff9e2652316071d6cdb0e6296/aten/src/ATen/native/transformers/cuda/attention.cuL572
gemma,[ROCm] remove size restrictions in gemm_and_bias,This aligns hipblaslt behavior with CUDA_VERSION >= 12010. ,2024-11-14T17:18:00Z,module: rocm open source Merged topic: not user facing rocm rocm priority ciflow/rocm,closed,0,3,https://github.com/pytorch/pytorch/issues/140724, please review.  This PR removes any large gemm shape restrictions for gemm_and_bias on ROCm.," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
mixtral,Extend gpt-fast LLM dashboard to support torchao autoquant,"  CC(Extend gptfast LLM dashboard to support torchao autoquant) Summary: We want to test autoquant on relevant LLM models right now only llama2 and mixtral, but want to extend to more models like https://github.com/vllmproject/vllm/tree/main/vllm/model_executor/models Test Plan:  https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fpytorch in pytorch/benchmarks/gpt_fast  output:  gpt_fast_benchmark.csv:  Reviewers: Subscribers: Tasks: Tags:",2024-11-13T23:05:03Z,Merged ciflow/trunk release notes: releng ciflow/inductor ciflow/inductor-micro-benchmark ciflow/inductor-micro-benchmark-cpu-x86,closed,0,12,https://github.com/pytorch/pytorch/issues/140627,It looks there there are still some failures on both A100 and x86 run: * https://github.com/pytorch/pytorch/actions/runs/11960091588/job/33347870789 * https://github.com/pytorch/pytorch/actions/runs/11960123335/job/33346846517,"not sure about the first issue, but second one could be resolved with an update of torchao, just updated","I'm also not sure what happens yet, building ao on CUDA fails, maybe we can try to reproduce this on a regular GPU CUDA runner instead of A100","The errors you're seeing are because you're building AO for very old SMs where things like bfloat16 are not supported You can pass in optional flags to nvcc and be explicity in passing newer `gencode=arch=compute_80,code=sm_80""` so that you never build custom extensions on older hardware https://github.com/pytorch/ao/blob/main/setup.pyL82 To test locally you can probably repro most of these breakages if you try building AO from source on google colab using the default T4 machines that they provide","It could be something wrong with the build script, as I see sm80 is set here https://github.com/pytorch/pytorch/blob/main/.github/workflows/inductormicrobenchmark.ymlL50","I found the issue with building ao. The problem is that it's done during testing phase in `_linux_test.yml` workflow and it doesn't set `TORCH_CUDA_ARCH_LIST` at all. The default to the value set in the Docker image https://github.com/pytorch/pytorch/blob/main/.ci/docker/ubuntucuda/DockerfileL139 is used, which fails the build.  The failure can be reproduced locally on devgpu by running `TORCH_CUDA_ARCH_LIST=Maxwell pip install ""git+https://github.com/pytorch/ao.git""` The fix is to set TORCH_CUDA_ARCH_LIST in the test script, and a similar fix has been done elsewhere, i.e. https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/test.shL1462.  So, the same needs to be done here.","waiting for  CC(UNSTABLE inductor / cuda12.4-py3.10-gcc9-sm86 / test (inductor_timm)) to be fixed, or we could ignore that and land", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / build Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 2 checks: inductor / cuda12.4py3.10gcc9sm86 / test (inductor_timm, 1, 2, linux.g5.4xlarge.nvidia.gpu, unstable), trunk / winvs2019cpupy3 / build Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[AOTI] AOT Compile NaViT - AttributeError: 'int' object has no attribute 'node'," üêõ Describe the bug NaViT (Native Resolution Vision Transformer) is a variant of the ViT. It can be draftexported but failed when running `aoti_compile_and_package` To repro:  1. Install vitpytorch  2. Run the following:  Draft mode export works, but  AOTI failed with the following error   Versions main ",2024-11-13T22:49:27Z,high priority triage review oncall: pt2 module: dynamic shapes oncall: export module: aotinductor empathy-day,closed,0,6,https://github.com/pytorch/pytorch/issues/140625,cc:  ,This is similar to this internal issue https://fb.workplace.com/groups/1075192433118967/posts/1545164526121753 But it looks like this is an easier repro.,"There's a bug that ought to be fixed but I think the problem can be worked around by editing navit code, specifically:  The shapes should not get cast into a tensor, this is totally unnecessary.",Minimized repro: ,"I think this is some sort of constant propagation edge case, and different from the internal example",Yup  this makes the test pass
transformer,Non-actionable error message when output type is not pytree-able for aoti artifact,"From user empathy day 111324  üêõ Describe the bug Script to generate aoti artifact:  Script to run the artifact and repro error:  error: https://gist.github.com/mlazos/b459671d7a481631cc87351f7ac72366 In this case the solution is to import transformers, but also perhaps sharing the registration API for users would be useful here as well if they return their own data structures.  Versions e754611d190b323e53c5d17db0dc39a96687513c ",2024-11-13T21:36:54Z,oncall: pt2 oncall: export,open,0,1,https://github.com/pytorch/pytorch/issues/140615,",  Looks like we just need to improve the error message to point to an API where we register custom datatype. "
transformer,Bad error message for malformed args during export,"Found during user empathy day 111324  üêõ Describe the bug The below repro yields a cryptic dynamo error: `torch._dynamo.exc.Unsupported: Observed exception`  see full error: https://gist.github.com/mlazos/697b326977dad55c5298d74e8d8df103 This was tricky as I had to guess what the exception is. It looks like we just need to add more information to `ExceptionVariable` to render properly and give more information to the user about which exception was thrown. I was able to guess my way out of this, but our users would definitely be confused by this.   Versions e754611d190b323e53c5d17db0dc39a96687513c ",2024-11-13T20:53:03Z,oncall: pt2 module: dynamo oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/140607
transformer,[export][non-strict] Passing in kwargs torch.export fails at non_strict_utils.fakify()," üêõ Describe the bug  Repro   Error   Notes  If using strict=True, then it's fine.  If using strict=False and args instead of kwargs, then it's fine.  If using strict=False and use kwargs, then it fails.  Versions n/a ",2024-11-13T19:44:03Z,module: bootcamp triaged oncall: pt2 oncall: export empathy-day,open,0,2,https://github.com/pytorch/pytorch/issues/140596,Error suggests that you need to register a pytree function for this type. The error message could be improved tho. ,"what is weird is that `transformers.image_processing_base.BatchFeature` had the same functionality as a dictionary, but it didn't subclass `Dict`. So, I'm guessing if they had subclassed `Dict`, then the pytree function for `Dict` would be fine."
transformer,[Export] AssertionError in replace_autocast_with_hop_pass,Repro: //  See https://huggingface.co/nvidia/NVEmbedv1 for installation instructions //  Basically `pip install transformers` and make sure you have huggingface access  Error:  ,2024-11-13T19:12:04Z,oncall: pt2 oncall: export empathy-day,closed,0,4,https://github.com/pytorch/pytorch/issues/140589," , assigning to you since you are the author of replace_autocast_with_hop_pass",">  , assigning to you since you are the author of replace_autocast_with_hop_pass I'll take a look, thanks for reporting!","The problem is that the current pass doesn't work well with nested autocast. I'll push a fix soon. Update: I fixed the bug with nested autocast, but there's another bug with mixing set_autograd and autocast, working on a fix.","I fixed the HOP pass error in https://github.com/pytorch/pytorch/pull/141169 and https://github.com/pytorch/pytorch/pull/141065, but now I got a new error:  Here: https://github.com/pytorch/pytorch/blob/9bc9d4cdb4355a385a7d7959f07d04d1648d6904/torch/export/_trace.pyL374  This seems like a different bug now, and unrelated to the HOP pass now.  Do you know what's wrong here? "
llm,[ONNX] Improve the conversion of `from dynamic axes to shapes`,"Features: (1) Add support for tree structure. (2) Add user warning before axes to shapes conversion (3) Add suggestion of providing `dynamic_shapes` when conversion fails Notes: (1) `input_names` is crucial to the conversion, as we don't know the ONNX graph inputs. (2) min and max are set as default, so LLM has higher chance to fail if users use `dynamic_axes` in terms of the min/max constraints dependency between `attention_mask` and `sequence_length`, etc. (Found in llama3.21B_Instruct)",2024-11-13T01:25:56Z,triaged open source Merged ciflow/trunk release notes: onnx topic: improvements,closed,0,6,https://github.com/pytorch/pytorch/issues/140488,"I have tested this PR on Olive with llama, and it works:  However, I would say (1) re.sub on torch.export.Dim naming and (2) max=99999 on constraints is kind of over customized. Alternative way is that we don't do these two for users, and ask them to provide the correct information (we would have to change code in Olive side.). But still, max and min are hard to be decided in automation I think.","For test, I suggest reproducing the huggingface model signature and make sure that works."," I added some extra tests for pytree, and there are existing small model tests already in test_api.py (they were added when conversion was introduced.). I find it hard to include hf models in the test of `_from_dynamic_axes_to_dynamic_shapes`, because the function basically asks everything: model, args, kwargs, input_names, etc. Do you have a better way to test this? Otherwise, we might need to build up llm testing...",We don't need to include the hf models. (And we should not.) I would just replicate its forward signature. The test model doesn't need to do anything except for having the same forward signature as hf models., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,RuntimeError in `_group_tensors_by_device_and_dtype` (`torch/optim/optimizer.py`) when using `torchrun` on N>1 GPUs. ," üêõ Describe the bug I'm trying to finetune a model using HuggingFace's `Trainer` class on multiple GPUs using `torchrun`. If I run the following code with a single process, or without torchrun, it works fine. However, if I increase `nproc_per_node`, I get `RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding`.  This error has already been discussed here, or more recently in issue CC(Tensors of the same index must be on the same device and the same dtype except step tensors that can be CPU and float32 notwithstanding.). However, in this case, the error seems to stem from somewhere else, as the `state_steps` parameter already uses floats (which seems to be the resolution of the original issue). I reproduced the error in a standalone file with a dummy dataset, provided below. I am not sure if this is a HuggingFace or a PyTorch issue. I'm leaning on the PyTorch side since it only happens when using `torchrun` with two or more GPUs.  Output Error   Original Code   Command Command that triggers the error (considering the previous code is in a file called `bug.py`)    Debugging steps  * Running this with a single proc works as expected (the training takes place) * I was able to reproduce the same error on two servers * Using the default alpaca training code leads to the same error, both with Llama and OPT models.  * This error is triggered during the optimization step. Specifically, it is triggered by this snippet of code in `torch/optim/adamw.py:480`:  * Using `adamw_fused` triggers the same error. * I printed the datatype and device of each e",2024-11-13T00:26:30Z,oncall: distributed module: optimizer triaged module: fsdp,closed,0,7,https://github.com/pytorch/pytorch/issues/140471,"Thank you for the detailed investigation. You are correct that it's the bf16 fp32 gap between param and grad at that point, so it's weird the grads became bf16 then. ?","It happens to me as well, only get this error for FSDP training, not the DDP training, revert to transformers==4.45.0 resolve the issue, can train with FSDP ","If reverting the transformers can solve the issue, can this be an integration bug in the transformers library?",I met this problem too with fsdp. And I double check different versions. The problem was introduced in transformers==4.46.2,"I can confirm this error does not occur with transformers==4.45.0. This seems to indicate this is a huggingface issue as  suggested. I opened an issue on their GitHub. I don't know if there still is something to fix on the PyTorch side, so feel free to close this issue, thanks for your help! ","I encountered the same issue with transformers version 4.46.2, while 4.45 seems to work fine.","Closing for now, but feel free to reopen if it is indeed a PyTorch issue"
transformer,Auto SAC - Automated SAC (Selective Activation Checkpointing) Policy Construction and Wrapping ,"Endtoend workflow for automating Selective Activation Checkpointing to enable model training under memory constraints. Under the hood, this PR leverages several tools from `torch.distributed._tools`, namely `RuntimeEstimator`, `MemTracker`, `SACEstimator`, and `ILP for AutoSAC`. It first extracts the memory, runtime and activation checkpointing tradeoff statistics. Using these it solves an ILP (Integer Linear Program) to determine the modules to AC and the memory budget for the selected modules. For each of the selected modules, it obtains a checkpointing policy (optimal or greedy) that determines whether to save or recompute the operators. Finally, the modules are wrapped using the policies. Further, this PR adds the following: 1. Adds a Greedy Policy for determining whether to save or recompute an op using the ratio of (memory/runtime) as the greedy metric. 2. Adds support for estimating runtime by specifying a GPU type in `RuntimeEstimator` and `SACEstimator`. 3. Adds optimizer state memory, mandatorily saved memory, and upper bound for discarded memory in the SAC ILP. For FSDP composability, it estimates FSDP prefetch buffer, shared param, grad, and optimizer state memory. 4. Corrects the objective function of the SAC ILP to be bounded by total forward pass runtime.  CC(Memory Tracker for tracking Module wise memory)   CC(Runtime Estimator for estimating GPU compute time)   CC(SAC Estimator (Selective Activation Checkpointing) for estimating memory and recomputation time tradeoffs.)   CC(ILP for Auto SAC (Selective Activation Checkpointing))  Endtoend example with FSDP2:  Output:  cc:      ",2024-11-12T15:46:05Z,oncall: distributed triaged open source topic: not user facing,open,2,0,https://github.com/pytorch/pytorch/issues/140410
llm,[dynamo] Improve graph break message on user defined call_method,  CC([for ci][dont merge] Debugging a segfault)  CC([dynamo] Improve graph break message on user defined call_method)  CC([dynamo][userdefined] Walk __mro__ to get the member descriptor source) ,2024-11-12T00:00:07Z,ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/140333,Nah. Need more work. Closing for now.
transformer,ILP for auto FSDP wrapping,"  CC(ILP for auto FSDP wrapping) This PR presents a mixed integer linear programming (MILP) formulation that can be utilized to determine, under a memory budget, which modules to wrap as FSDP units. Similar to the auto SAC MILP introduced in https://github.com/pytorch/pytorch/pull/137908, the MILP uses information collected from MemTracker, Runtime Estimator, and SAC Estimator, introduced in these PRs: * https://github.com/pytorch/pytorch/pull/124688 * https://github.com/pytorch/pytorch/pull/134243 * https://github.com/pytorch/pytorch/pull/135208 Endtoend example and its sample output:   ",2024-11-11T19:04:07Z,oncall: distributed Merged Reverted ciflow/trunk topic: not user facing ci-no-td,open,0,6,https://github.com/pytorch/pytorch/issues/140298,  merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""to allow CI pass for PRhttps://github.com/pytorch/pytorch/pull/140410"""," revert m ""for other PR"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
agent,[ROCm][TunableOp] TunableOp on some operations on tensor views results in memory access fault," üêõ Describe the bug I'm seeing a memory access fault with TunableOp enabled when tensor views are used. I managed to squeeze the reproducer into the following:  When I run this (as `pytorch_bug.py`) with the following command (hipblaslt does not support my GPU, so it needs to be disabled):  I get output resembling the following:  I bisected and managed to narrow the starting point of this behavior down to commit e68ee2c ( CC(  TunableOp hotfix)). I also tested main (63715f6) and the memory access fault was still present. Without TunableOp the code runs fine. Also, if I directly create a tensor whose shape is identical to the view produced by `split()`, TunableOp works fine.  Versions  ",2024-11-11T17:15:26Z,module: rocm triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/140278,Thank you for doing the leg work to isolate this issue. I was able to reproduce the issue on my MI250. We will need to dig into this further.  I was able to avoid the memory fault by setting `PYTORCH_TUNABLEOP_ROTATING_BUFFER_SIZE=0`. Can you please confirm that this avoids the issue on your local machine?,"Setting that environment variable does indeed avoid the issue on my machine, too. Great find!",Here is an even simpler reproducer: ,"That code does not really work, though, as the dimensions are not compatible for matrix multiplication. Should there be a transpose or similar somewhere?","You are correct, I messed up. Still trying to reduce this to a simpler test case.","Not sure if this is helpful, but I will post it anyway: "
transformer,DDP disables torch.compile dynamic shape behaviour," üêõ Describe the bug When running a torch.compiled model with DDP with inputs that has growing sequence length, the recompilations happens every time the input shape changes. After 8 recompilations, the cache size limit is reached. Very similar to this). version 2.5.1+cu124  Notes :  works without DDP  `torch.compile` with `dynamic=None` or `dynamic=True` have the same behavior  if the model has only one layer, it works  if it's the batch size that's growing, it works **Minimal code to reproduce :**  Run with `torchrun standalone nproc_per_node=1 repl_ddp.py` You will see that the model gets recompiled each step (because the seq len changes each step), and after 8 compilations cache limit is hit. (here, the ""sequence length"" is just a second batch dimension. In my setup where this first occurred (transformer) it is really a sequence length)  Error logs Here are the relevant excerpts of the output when ran with `TORCH_LOGS=""+dynamic""`. **Without DDP (works, compiles only two times):** this get displayed during the 2nd compilation (len was 16, is now 18): `[0/1] create_symbol s0 = 18 for L['x'].size()[1] [2, int_oo]` [...] `[0/1] eval 12288*s0  [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/symbolic_shapes.py:5106] [0/1]     logits = model(inputs) [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/symbolic_shapes.py:5106] [0/1]   File ""/usr/local/lib/python3.11/distpackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/symbolic_shapes.py:5106] [0/1]     return self._call_impl(*args, **kwargs) [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/",2024-11-10T16:36:19Z,oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes pt2d-triage-nov2024,closed,0,4,https://github.com/pytorch/pytorch/issues/140229, isn't this the stride specialization thing in DDP?,looking,"Fix here: https://github.com/pytorch/pytorch/pull/140751, more details in the description",Thank you!
yi,"`Torch.distributed.tensor.parallel.Style.Rowwiseparallel` when send tensor, the use of the underlying operator is `aten.to.dtype_layout`, lack of distributed strategy. But after adding a policy, `AsyncCollectiveTensor` no placements error "," üêõ Describe the bug   first error:   The bandaid modification error 01  alteration `torch.distributed.tensor.ops._pointwise_ops` ÔºåÊ∑ªÂä†`dtype_layout`ÁöÑÁ≠ñÁï•   error   The bandaid modification error 02  alteration process ` asynccollectivetensor` type: ` Torch.distributed.tensor.parallel.Style.Rowwiseparallel: 297`   but always: error   Versions cuda11.8 python == 3.10.14,  torch==2.5.1+cu118 ",2024-11-10T16:12:31Z,oncall: distributed,open,0,0,https://github.com/pytorch/pytorch/issues/140227
yi,"Fix RMSNorm Notation: Parentheses, Indices, Comma","Fixes CC(Doc issue: Fix Inconsistent Notation for RMSNorm)  * fixed mathematical notation for RMSNorm:   * changed RMS function from brackets `[x]` to parenthesis `(x)` for consistency and align with mathematical notation standards for functions   * added indices (e.g. `y_i`)  for elementwise operations for the correctness in the context of tensor operations   * added comma `,` before $$\text{where}$$ !grafik ",2024-11-09T23:23:59Z,module: nn triaged open source Merged ciflow/trunk release notes: nn topic: docs topic: not user facing,closed,1,10,https://github.com/pytorch/pytorch/issues/140215," label ""topic: not user facing"""," label ""topic: docs"""," label ""module: nn""", Do you know why the Auto Request Review CI job was canceled?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Auto Request Review / Auto Request Review Details for Dev Infra team Raised by workflow job ", Do you know why the Auto Request Review is blocking the merge?, merge i, Merge started Your change will be merged while ignoring the following 1 checks: Auto Request Review / Auto Request Review Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Doc issue: Fix Inconsistent Notation for RMSNorm," üìö The doc issue Followup of CC([Doc issue] RMSNorm formula)  This issue report proposes a change to standardize the notation for the RMS function in the PyTorch documentation and formula expressions. The current documentation inconsistently uses both `\mathrm{RMS}[x]` and `\mathrm{RMS}(x)`, which can lead to confusion. Additionally, the formula should explicitly indicate indices for clarity.  **Why This Fix is Needed** 1. **Consistency in Notation**:     The current documentation uses both `\mathrm{RMS}[x]` and `\mathrm{RMS}(x)` interchangeably. To maintain consistency and align with standard mathematical notation, it is preferable to use `\mathrm{RMS}(x)`, which is more widely recognized in mathematical contexts as the correct functional form. 2. **Clarity with Indices**:     The formula for $$y$$ lacks explicit indexing, which can be confusing when dealing with elementwise operations on tensors or vectors. Adding indices (e.g., $$y_i$$, $$x_i$$, and $$\gamma_i$$) clarifies that the operation is applied elementwise, improving readability and correctness in the context of tensor operations. 3. **Alignment with Mathematical Standards**:     In mathematical conventions, functions are typically written using parentheses (e.g., $$f(x)$$), not brackets. Using `\mathrm{RMS}(x)` ensures that PyTorch's documentation adheres to these standards, making it easier for users to follow and reducing potential misunderstandings.  Suggest a potential alternative/fix  **Proposed Changes**  Change all instances of `\mathrm{RMS}[x]` to `\mathrm{RMS}(x)` in the formula.  Update the formula to include indices for elementwise operations as follows: !grafik Th",2024-11-08T18:50:26Z,module: docs module: nn triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/140165,"This sounds reasonable, please do not hesitate to propose the PR to address it"
yi,Doc issue: Fix Inconsistent Notation for RMSNorm,"Followup of CC([Doc issue] RMSNorm formula)  This pull request proposes a change to standardize the notation for the RMS function in the PyTorch documentation and formula expressions. The current documentation inconsistently uses both `\mathrm{RMS}[x]` and `\mathrm{RMS}(x)`, which can lead to confusion. Additionally, the formula should explicitly indicate indices for clarity.  **Why This Fix is Needed** 1. **Consistency in Notation**:     The current documentation uses both `\mathrm{RMS}[x]` and `\mathrm{RMS}(x)` interchangeably. To maintain consistency and align with standard mathematical notation, it is preferable to use `\mathrm{RMS}(x)`, which is more widely recognized in mathematical contexts as the correct functional form. 2. **Clarity with Indices**:     The formula for $$y$$ lacks explicit indexing, which can be confusing when dealing with elementwise operations on tensors or vectors. Adding indices (e.g., $$y_i$$, $$x_i$$, and $$\gamma_i$$) clarifies that the operation is applied elementwise, improving readability and correctness in the context of tensor operations. 3. **Alignment with Mathematical Standards**:     In mathematical conventions, functions are typically written using parentheses (e.g., $$f(x)$$), not brackets. Using `\mathrm{RMS}(x)` ensures that PyTorch's documentation adheres to these standards, making it easier for users to follow and reducing potential misunderstandings.  **Proposed Changes**  Change all instances of `\mathrm{RMS}[x]` to `\mathrm{RMS}(x)` in the formula.  Update the formula to include indices for elementwise operations as follows: !grafik This will ensure that both the notation and indexing are",2024-11-08T18:39:48Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/140163,":white_check_mark:login: dkleine / (7039efe1d1639bc6dedbf11f7586ac42bb681b27):white_check_mark:login: dkleine / (7039efe1d1639bc6dedbf11f7586ac42bb681b27, e13b7f0e65ac6345f492370ea4191db8683cb64e)The committers listed above are authorized under a signed CLA."," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
rag,Doc: Rewrite the storage.rst file to emphasize untyped storages,  CC(Doc: Rewrite the storage.rst file to emphasize untyped storages)  ,2024-11-08T16:56:02Z,better-engineering Merged ciflow/trunk release notes: memory format topic: docs,closed,0,4,https://github.com/pytorch/pytorch/issues/140145,"> You've addressed most of my comments, approving on good faith! Thanks I think I addressed them all","https://docspreview.pytorch.org/pytorch/pytorch/140145/storage.html looks good, merging!", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[onnx] [njt] [feature request] Export NJT-enabled SDPA / MHA ops to ORT's PackingMode Attention," üöÄ The feature, motivation and pitch I found that some support for NJTenabled SDPA / MHA exists in onnxruntime: https://github.com/microsoft/onnxruntime/issues/22764 as ""PackedAttention"" https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/convert_to_packing_mode.pyL317 NJTenabled SDPA also used to exist in FasterTransformer, known as ""Effective Transformer kernels"": https://github.com/NVIDIA/FasterTransformer/blob/main/docs/bert_guide.mdstandardbertandeffectivefastertransformer I wonder if in the long term it would be good to have some example of exporting NJT ops to ORT and mapping NJTenabled SDPA to this PackingMode directly at export time.  Alternatives _No response_  Additional context _No response_  ",2024-11-08T10:18:00Z,module: onnx triaged,open,0,1,https://github.com/pytorch/pytorch/issues/140130,"If onnx export in PyTorch supports setting up some transforms or plugins, maybe this proposal can be realized in form of example for this functionality : how to set up onnx export to make use of PackingAttention plugin in ORT"
transformer,DTensor support for fused qkv matmul," üöÄ The feature, motivation and pitch For transformer architecture (for example https://github.com/pytorchlabs/gptfast/blob/main/model.pyL195L211) it tends to be most performant to merge the qkv matrices together. If you try to shard this concatenated tensor then the subsequent SDPA op won't be shared correctly since you need each column of q sharded with the corresponding columns of  k and v [q1,k1,v1,...], but by default the sharding will be [q1, q2, q3...] When not using DTensor this is relatively easy to get to work: https://github.com/pytorchlabs/gptfast/blob/main/tp.pyL73 but for DTensor the way to enable this is really unclear. is there a way to handle this type of operation with DTensor parallelization or should we just stick to normal tensor parallel support and figure out how to get it to work with our APIs? This is currently blocking tensor parallel support in torchAO so i wanted to centralize discussion to a single location.  Alternatives don't use DTensor for tensor parallel  Additional context _No response_ ",2024-11-08T00:12:56Z,oncall: distributed module: dtensor,open,0,1,https://github.com/pytorch/pytorch/issues/140069,Depends on DTensor Strided Sharding:   CC([RFC] PyTorch DistributedTensor)issuecomment2081838245  CC([WIP][RFC][DTensor] DTensor Strided Sharding: A More Flexible Way To Shard Tensors)
transformer,`torch.export.export` infers dynamic_shape as constant," üêõ Describe the bug Install most recent transformers and pytorch.   Repro    Error  Collecting environment information... PyTorch version: 2.6.0.dev20241106+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.0121genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 550.120 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.1.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        39 bits physical, 48 bits virtual Byte Order:                           Little Endian CPU(s):                               4 Online CPU(s) list:                  03 Vendor ID:                            GenuineIntel Model name:                           Intel(R) Core(TM) i57640X CPU @ 4.00GHz CPU family:                           6 Model:                                158 Thread(s) per core:                   1 Core(s) per socket:                   4 Socket(s):                            1 Stepping:                             9 CPU max MHz:                          4200.0000 CPU min MHz:                          800.0000 BogoMIPS:",2024-11-07T13:38:15Z,oncall: pt2 module: dynamic shapes oncall: export,open,0,6,https://github.com/pytorch/pytorch/issues/140011, do you have time to have a look?," Could you try rerunning export with `TORCH_LOGS=""+dynamic""`? That should log while running, and somewhere in the log there's likely a guard that looks like `Eq(s*, 6)` that's responsible for this specializing to a constant.  Then you can rerun additionally with `TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s*, 6)""`, and that should produce a stack trace that likely points to where in model code the specialization happens. If you could post that that would be very helpful.",">  Could you try rerunning export with `TORCH_LOGS=""+dynamic""`? That should log while running, and somewhere in the log there's likely a guard that looks like `Eq(s*, 6)` that's responsible for this specializing to a constant. >  > Then you can rerun additionally with `TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s*, 6)""`, and that should produce a stack trace that likely points to where in model code the specialization happens. If you could post that that would be very helpful. Sure!  Here are the stack traces:  export TORCH_LOGS=""+dynamic""  Loading checkpoint shards: 100% 2/2 [00:01), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:00.782000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:00.787000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s1 = VR[2, 128] (update) I1118 13:45:00.787000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s1 = 6 for L['attention_mask'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s1"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:00.791000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s2 = VR[2, 128] (update) I1118 13:45:00.791000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s2 = 6 for L['position_ids'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s2"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:00.827000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:00.829000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:00.834000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 1) == False [statically known] V1118 13:45:00.839000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s0, 1) == False [statically known] V1118 13:45:00.842000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:00.844000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s1, 1) == True [statically known] V1118 13:45:00.874000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s0 = VR[6, 6] (update) I1118 13:45:00.875000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5793] [0/0] set_replacement s0 = 6 (range_refined_to_singleton) VR[6, 6] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] runtime_assert Eq(s0, 6) [guard added] causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(1, 1)   transformers/models/llama/modeling_llama.py:1091 in _prepare_4d_causal_attention_mask_with_cache_position (_refs/__init__.py:425 in _broadcast_shapes) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] User Stack (most recent call last): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   (snipped, see stack below for prefix) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/transformers/models/llama/modeling_llama.py"", line 915, in forward I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     causal_mask = self._update_causal_mask( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/transformers/models/llama/modeling_llama.py"", line 1024, in _update_causal_mask I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/transformers/models/llama/modeling_llama.py"", line 1091, in _prepare_4d_causal_attention_mask_with_cache_position I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(1, 1) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] Stack (most recent call last): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/aot_llama_test.py"", line 96, in  I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     compile_path = aot_compile('llama3.pt2', model, **inputs1) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/aot_llama_test.py"", line 58, in aot_compile I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     exported_program = torch.export.export( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/__init__.py"", line 368, in export I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _export( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1004, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ep = fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/exported_program.py"", line 122, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1957, in _export I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     export_artifact = export_func(   type: ignore[operator] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1251, in _strict_export I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _strict_export_lower_to_aten_ir( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1279, in _strict_export_lower_to_aten_ir I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level = _export_to_torch_ir( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level, _ = torch._dynamo.export( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     result_traced = opt_f(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._torchdynamo_orig_callable( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     guarded_code = compile_inner(code, one_graph, hooks, transform) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile_inner(code, one_graph, hooks, transform) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return function(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 743, in _compile_inner I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     out_code = transform_code_object(code, transform) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1348, in transform_code_object I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     transformations(instructions, code_options) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 233, in _fn I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 662, in transform I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2914, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     super().run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1738, in CALL_FUNCTION I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, {}) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 410, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1828, in CALL_FUNCTION_KW I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 294, in impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn_var.call_function(self, self.popn(nargs), {})) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 998, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return handler(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 974, in _handle_insert_op_in_graph I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy(tx, proxy) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2100, in wrap_fx_proxy I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2166, in wrap_fx_proxy_cls I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _wrap_fx_proxy( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2262, in _wrap_fx_proxy I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2247, in get_fake_value I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ret_val = wrap_fake_exception( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 1795, in wrap_fake_exception I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2248, in  I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     lambda: run_node(tx.output, node, args, kwargs, nnmodule) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2362, in run_node I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return node.target(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/utils/_stats.py"", line 21, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1271, in __torch_dispatch__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.dispatch(func, types, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1813, in dispatch I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._cached_dispatch_impl(func, types, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1381, in _cached_dispatch_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     output = self._dispatch_impl(func, types, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 2297, in _dispatch_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     r = func(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_ops.py"", line 723, in __call__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._op(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_meta_registrations.py"", line 3568, in meta_binop_inplace I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     check_inplace_broadcast(self.shape, other.shape) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_meta_registrations.py"", line 90, in check_inplace_broadcast I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     broadcasted_shape = tuple(_broadcast_shapes(self_shape, *args_shape)) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_refs/__init__.py"", line 425, in _broadcast_shapes I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     torch._check( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1615, in _check I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     _check_with(RuntimeError, cond, message) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1578, in _check_with I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     if expect_true(cond): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 1302, in expect_true I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return a.node.expect_true( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/sym_node.py"", line 515, in expect_true I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.shape_env.defer_runtime_assert( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/recording.py"", line 263, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return retlog(fn(*args, **kwargs)) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6453, in defer_runtime_assert I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self._log_guard(""runtime_assert"", orig_expr, forcing_spec=False) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6097, in _log_guard I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.log.info( V1118 13:45:00.886000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 9223372036854775807) == False [statically known] V1118 13:45:00.890000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s1 > 1 == True [statically known] V1118 13:45:00.955000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 1) == False [statically known] V1118 13:45:00.958000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 9223372036854775807) == False [statically known] V1118 13:45:00.961000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s2 What is a compiler? A compiler is a program that translates source code written Traceback (most recent call last):   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir     gm_torch_level, _ = torch._dynamo.export(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1584, in inner     raise constraint_violation_error   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner     result_traced = opt_f(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn     return fn(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__     return self._torchdynamo_orig_callable(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__     return _compile(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile     guarded_code = compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner     return _compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function     return function(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 840, in _compile_inner     check_fn = CheckFunctionManager(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 2183, in __init__     guard.create(builder)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_guards.py"", line 298, in create     return self.create_fn(builder, self)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 1766, in SHAPE_ENV     code_parts, verbose_code_parts = output_graph.shape_env.produce_guards_verbose(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 5072, in produce_guards_verbose     raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=""+dynamic"".    Not all values of seq_len = L['input_ids'].size()[1] in the specified range seq_len "," export TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s1, 6) ``` Loading checkpoint shards: 100% 2/2 [00:01), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:23.237000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:23.242000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s1 = VR[2, 128] (update) I1118 13:45:23.242000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s1 = 6 for L['attention_mask'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s1"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:23.246000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s2 = VR[2, 128] (update) I1118 13:45:23.246000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s2 = 6 for L['position_ids'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s2"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:23.282000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:23.283000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:23.288000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 1) == False [statically known] V1118 13:45:23.293000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s0, 1) == False [statically known] V1118 13:45:23.296000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:23.299000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s1, 1) == True [statically known] V1118 13:45:23.328000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s0 = VR[6, 6] (update) I1118 13:45:23.328000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5793] [0/0] set_replacement s0 = 6 (range_refined_to_singleton) VR[6, 6] I1118 13:45:23.329000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] runtime_assert Eq(s0, 6) [guard added] causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(1, 1)   transformers/models/llama/modeling_llama.py:1091 in _prepare_4d_causal_attention_mask_with_cache_position (_refs/__init__.py:425 in _broadcast_shapes), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s0, 6)"" V1118 13:45:23.336000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 9223372036854775807) == False [statically known] V1118 13:45:23.341000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s1 > 1 == True [statically known] V1118 13:45:23.406000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 1) == False [statically known] V1118 13:45:23.408000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 9223372036854775807) == False [statically known] V1118 13:45:23.412000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s2  I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     compile_path = aot_compile('llama3.pt2', model, **inputs1) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/aot_llama_test.py"", line 58, in aot_compile I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     exported_program = torch.export.export( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/__init__.py"", line 368, in export I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _export( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1004, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ep = fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/exported_program.py"", line 122, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1957, in _export I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     export_artifact = export_func(   type: ignore[operator] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1251, in _strict_export I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _strict_export_lower_to_aten_ir( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1279, in _strict_export_lower_to_aten_ir I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level = _export_to_torch_ir( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level, _ = torch._dynamo.export( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     result_traced = opt_f(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._torchdynamo_orig_callable( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     guarded_code = compile_inner(code, one_graph, hooks, transform) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile_inner(code, one_graph, hooks, transform) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return function(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 743, in _compile_inner I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     out_code = transform_code_object(code, transform) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1348, in transform_code_object I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     transformations(instructions, code_options) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 233, in _fn I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 662, in transform I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2914, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     super().run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1828, in CALL_FUNCTION_KW I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/nn_module.py"", line 442, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1816, in CALL_FUNCTION_EX I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, argsvars.items, kwargsvars) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 410, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1816, in CALL_FUNCTION_EX I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, argsvars.items, kwargsvars) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/nn_module.py"", line 442, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1816, in CALL_FUNCTION_EX I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, argsvars.items, kwargsvars) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 410, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1738, in CALL_FUNCTION I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, {}) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 294, in impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn_var.call_function(self, self.popn(nargs), {})) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 998, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return handler(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 974, in _handle_insert_op_in_graph I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy(tx, proxy) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2100, in wrap_fx_proxy I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2166, in wrap_fx_proxy_cls I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _wrap_fx_proxy( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2262, in _wrap_fx_proxy I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2247, in get_fake_value I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ret_val = wrap_fake_exception( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 1795, in wrap_fake_exception I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2248, in  I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     lambda: run_node(tx.output, node, args, kwargs, nnmodule) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2362, in run_node I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return node.target(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/utils/_stats.py"", line 21, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1271, in __torch_dispatch__ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.dispatch(func, types, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1813, in dispatch I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._cached_dispatch_impl(func, types, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1381, in _cached_dispatch_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     output = self._dispatch_impl(func, types, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 2183, in _dispatch_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs)) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_impls.py"", line 879, in fast_binary_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     final_shape = infer_size(final_shape, shape) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_impls.py"", line 833, in infer_size I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     torch._check( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1615, in _check I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     _check_with(RuntimeError, cond, message) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1578, in _check_with I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     if expect_true(cond): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 1302, in expect_true I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return a.node.expect_true( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/sym_node.py"", line 515, in expect_true I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.shape_env.defer_runtime_assert( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/recording.py"", line 263, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return retlog(fn(*args, **kwargs)) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6453, in defer_runtime_assert I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self._log_guard(""runtime_assert"", orig_expr, forcing_spec=False) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6097, in _log_guard I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.log.info( V1118 13:45:23.546000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s1 = VR[6, 6] (update) I1118 13:45:23.547000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5793] [0/0] set_replacement s1 = 6 (range_refined_to_singleton) VR[6, 6] I1118 13:45:23.547000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] eval Eq(s1, 6) [guard added] causal_mask = causal_mask[:, :, :, : key_states.shape[2]]   transformers/models/llama/modeling_llama.py:589 in forward (_dynamo/utils.py:2362 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s1, 6)"" V1118 13:45:23.548000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6217] [0/0] eval 6 [trivial] I1118 13:45:26.643000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4429] [0/0] produce_guards V1118 13:45:26.643000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].size()[0] 1 None V1118 13:45:26.643000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].size()[1] 6 StrictMinMaxConstraint(warn_only=False, vr=VR[1, 128]) V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].stride()[0] 6 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].stride()[1] 1 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].storage_offset() 0 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].size()[0] 1 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].size()[1] 6 StrictMinMaxConstraint(warn_only=False, vr=VR[1, 128]) V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].stride()[0] 6 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].stride()[1] 1 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].storage_offset() 0 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].size()[0] 1 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].size()[1] 6 StrictMinMaxConstraint(warn_only=False, vr=VR[1, 128]) V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].stride()[0] 6 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].stride()[1] 1 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].storage_offset() 0 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['cache_position'].size()[0] 6 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['cache_position'].stride()[0] 1 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['cache_position'].storage_offset() 0 None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] Error while creating guard: E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] Name: '' E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Source: shape_env E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Create Function: SHAPE_ENV E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Guard Types: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Code List: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Object Weakref: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Guarded Class Weakref: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] Traceback (most recent call last): E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_guards.py"", line 298, in create E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     return self.create_fn(builder, self) E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 1766, in SHAPE_ENV E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     code_parts, verbose_code_parts = output_graph.shape_env.produce_guards_verbose( E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 5072, in produce_guards_verbose E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     raise ConstraintViolationError( E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=""+dynamic"". E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]    Not all values of seq_len = L['input_ids'].size()[1] in the specified range seq_len What is a compiler?? A compiler is a program that translates source code Traceback (most recent call last):   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir     gm_torch_level, _ = torch._dynamo.export(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1584, in inner     raise constraint_violation_error   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner     result_traced = opt_f(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn     return fn(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__     return self._torchdynamo_orig_callable(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__     return _compile(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile     guarded_code = compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner     return _compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function     return function(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 840, in _compile_inner     check_fn = CheckFunctionManager(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 2183, in __init__     guard.create(builder)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_guards.py"", line 298, in create     return self.create_fn(builder, self)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 1766, in SHAPE_ENV     code_parts, verbose_code_parts = output_graph.shape_env.produce_guards_verbose(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 5072, in produce_guards_verbose     raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=""+dynamic"".    Not all values of seq_len = L['input_ids'].size()[1] in the specified range seq_len ","Let's debug the `Eq(s0, 6)`, since that seems upstream of and might be causing the other 2 specializations. The user stack trace seems helpful:  The work now is to look into this call, and possibly trace back, to follow shape propagation rules on how these tensors are constructed, and how they're related to `s0`. For example, during fake tensor propagation, which is where we handle symbolic shapes, if cache_position has shape `s0` (i.e. is derived from `input_ids.size(1)` somehow), and if `target_length` is just a plain int (6), then we'll reason from this elementwise operation that `s0 == 6` and specialize. The solution is to make sure all the computation leading up to here is such that both `target_length` and the size of `cache_position` use `s0` (`input_ids.size(1)`), inplace of the static value 6, so this call doesn't trigger specialization. Would you have some idea how this might be happening?","I realized that my intention was not to define any custom inputs for the model, so after simplifying inputs like this:  and dynamic shape definitions to take into account only inputs and attention masks:  the constraint error on sequence length changes to   . Then the suggested fix and the issue looks very similar to this one  CC([export] Llama3 export with dynamic shapes fails with constraint violations) . Following the hints from that issue and adding `with torch.nn.attention.sdpa_kernel([SDPBackend.MATH]):` as a context manager solves the constraint error and model gets exported and compiled. "
llm,Support AC with graph break," üêõ Describe the bug As mentioned in this blog, HigherOrderOperator does not support graph break inside the input/output function, because Dynamo cannot determine whether the operations inside the function are ""safe"". However, in most cases, graph break within the function is safe and acceptable. Is there a way to make Dynamo support these cases without falling back to eager mode? For example, this scenario frequently occurs in LLM models:  In LLM models, flash attention is often used along with activation checkpointing, which can cause the entire attention or decoder layer execution to fall back to eager mode. Is there a way to make Dynamo support graph breaking within HigherOrderOperator, such as providing some APIs to mark these functions as safe? I know that we can use PyTorch's SDPA (Scaled DotProduct Attention) op as a replacement, but our users typically hardcode the use of flash attention in their code. I hope there is a way to achieve this without requiring significant changes to the user's code.  Error logs _No response_  Minified repro _No response_  Versions PyTorch version: 2.5.0a0+git24dee99 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 11.4.02ubuntu1~20.04) 11.4.0 Clang version: Could not collect CMake version: version 3.28.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] (64bit runtime) Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] torch==2.5.0a0+git417a076 [pip3] torchvision==0.18.0 ",2024-11-07T09:12:48Z,triaged enhancement oncall: pt2 module: dynamo module: higher order operators module: pt2-dispatcher activation-checkpointing,open,0,2,https://github.com/pytorch/pytorch/issues/139989,"Changing the title because it sounds like you only care about graph breaks inside activation checkpointing. We had somewhat of a plan to do this (), I think someone from our side just needs to do this?","Thanks  . I found we can wrap `attn_forward` with `torch.compile` to allow `attn_forward` to be compiled as an independent subgraph and enable graph breaking. However, this method has a limitation: some AC without graph break will also break the whole graph."
llama,Install torchtune and ao when testing ExecuTorch llama3,"A followup on https://github.com/pytorch/pytorch/pull/139700, it's good that ET already pins ao, which makes this step much easier on PT CI.",2024-11-07T00:27:29Z,release notes: releng ciflow/inductor test-config/executorch no-runner-experiments,closed,0,5,https://github.com/pytorch/pytorch/issues/139947,"Something is still not right, this setup works locally for me which makes sense, but it fails on CI, which is weird.  I need to dig a bit deeper.","I just find out that the test passes on nightly build PyTorch but failed on the wheel built from source.  Nothing obvious stands out yet, but let me compare these wheel to see what is the difference","> Are torchtune and torchao are mandatory dependencies of ExecuTorch? If they are not, and just needed for some additional testing, we should restrict the test running in PT to set of subtests They are not atm, but ExecuTorch team is working on adding them as mandatory dependency https://fb.workplace.com/groups/222849770514616/posts/534780649321525/?comment_id=534820722650851&reply_comment_id=535663095899947, so this change anticipates that.  I have another PR on ExecuTorch to skip the failed tests when ao is not installed. Nevertheless, the tricky issue that I'm still trying to debug with this one is that the test fails only with the CPU wheel built from source but it works with nightly wheel.  Other components including ao and executorch are installed in the same way, so it's puzzling me on what causes the difference","We also don't need to explicitly pin ao on PyTorch.  Instead, we just need to call `https://github.com/pytorch/executorch/blob/main/.ci/scripts/setuplinux.sh` to setup ExecuTorch, then run the test","I found the root cause for the failures in https://github.com/pytorch/executorch/pull/6744 and should have pay more attention to the warning in the build log about `_GLIBCXX_USE_CXX11_ABI`.  So, I will close this as the main fix is on ET.  Do you know why PyTorch nightly binaries unset `_GLIBCXX_USE_CXX11_ABI` while PyTorch CI sets it.  This difference is the reason why the test couldn't find these quantized symbol. The missing torchao and torchtune was a red herring."
transformer,Inductor vs. Liger Performance Track," üêõ Describe the bug Recently, we did some benchmarking on custom operators in liger kernels compared with inductor compiled kernels. Inductor is worse on some  cases. Here is the operator and config list we need to improve.   List  Format For each operator, the data format in the following task list is:  [ ] **Operator Name**, (20th percentile speedup, 50th percentile (median), 80th percentile)  Speedup Calculation The speedup numbers are computed as follows: $$ \text{inductor\\_vs\\_liger} = \frac{\text{speedup\\_inductor}}{\text{speedup\\_liger}} = \frac{\frac{\text{latency\\_eager}}{\text{latency\\_inductor}}}{\frac{\text{latency\\_eager}}{\text{latency\\_liger}}} = \frac{\text{latency\\_liger}}{\text{latency\\_inductor}} $$ Since each operator has multiple inputs, there are multiple speedup numbers. We use the 20th, 50th, and 80th percentiles to better represent the results. If the number is >1, it means inductor's results are faster. The GPU peak memory usage is determined using the same process. We need to improve inductor's performance on cases that less than 1.  For FP32 forward_backward latency:  [ ] kl_div, (0.65, 0.65, 1.28)  [ ] rms_norm, (0.77, 1.09, 1.29)   [ ] CC(rope perf improvement)  [ ] geglu, (0.98, 1.00, 1.00)  [ ] embedding  For FP32 peak gpu memory usage:  [ ] cross_entropy, (0.75, 0.75, 0.75)  [ ] fused_linear_cross_entropy, (0.47, 0.58, 0.75)  [ ] fused_linear_jsd, (0.71, 0.72, 0.81)  [ ] geglu, (0.89, 0.90, 0.94)  [ ] kl_div, (0.86, 0.86, 1.00)  [ ] rope, (0.54, 0.56, 0.75)  [ ] swiglu, (0.90, 0.91, 0.92) For FP32 forward only latency:  [ ] embedding, (0.40, 0.47, 0.50)  CC(Embedding forward performance analysis",2024-11-06T18:37:02Z,triaged oncall: pt2 module: inductor,open,0,0,https://github.com/pytorch/pytorch/issues/139908
agent,[ROCm] [Flex attention] Memory access fault on nested_tensor UT," üêõ Describe the bug https://github.com/pytorch/pytorch/pull/136792 accidentally disabled flex attention UTs on ROCm. We are reenabling testing with https://github.com/pytorch/pytorch/pull/139632 but there is a memory access fault in a unit test.  We will skip the unit test to start running ROCm flex attention UTs again, but opening this issue for tracking. cc:    https://hud.pytorch.org/pr/pytorch/pytorch/139632 CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)76539  Will be reassessed with 3.2 triton  CC([v2.6] Inductor issue tracker for Triton release/3.2)  Versions CI ",2024-11-05T13:38:53Z,module: rocm triaged oncall: pt2 module: higher order operators module: pt2-dispatcher module: flex attention,open,0,1,https://github.com/pytorch/pytorch/issues/139754,Not release/2.6 release/3.2 blocking but would be nice to get to the bottom of.
llama,Unexpected type cast from DTensor to Tensor when applying SequenceParallel on RMSNorm.," üêõ Describe the bug I am using Sequence parallel for my RMS implementation in LLaMA torchtune. During the forward pass, layer 0 propagates successfully, but layer 1 encountered a weird bug where if I print out the type and shape and placements of output and self.scale, it is displaying the desired configs ([b, s, d] tensor and a [d] tensor, both of type DTensor, one sharded along the sequence dimension (Shard(1)), and one is replicated). But during the multiplication, the stack traces suggests that the output tensor was casted to Tensor somewhere during the execution. My reference RMSNorm implementation is here:  and it prints out the following output: > x_norm local shape is torch.Size([2, 140, 4096]), placement is (Shard(dim=1),), type is  > scale locals shape is torch.Size([4096]), placement is (Replicate(),), type is , scale dimension is 4096 However the stack following stack traces (the call to _try_replicate_spec_for_scalar_tensor) suggest that a DTensor became Tensor somewhere in the middle since this function is only invoked if it passes a type test (isinstance(arg, torch.Tensor)). I printed out the arg and it says it is a tensor of shape [2, 140, 4096], suggesting that the output DTensor was casted to a Tensor somewhere in the middle.   Versions Collecting environment information... PyTorch version: 2.5.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.5 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.30.5 Libc version: glibc2.35 Python version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] ",2024-11-05T00:37:01Z,oncall: distributed,open,0,2,https://github.com/pytorch/pytorch/issues/139681,"Hey , can you try the implicit_replication context manager to see if it resolves your issue? ,  for potential better solutions",It seems you enabled `torch.compile`. Can you disable compile to isolate the problem?
yi,Remove shebang line from easy_install generated python scripts on Windows only (#108602),"Fixes   CC(torchrun fails to run on Windows 11) On windows only, for install step: remove shebang line from python scripts generated by `easy_install`. ",2024-11-04T11:08:34Z,module: windows triaged open source topic: bug fixes topic: not user facing,open,1,2,https://github.com/pytorch/pytorch/issues/139618," label ""module: windows"""," label ""topic: bug fixes"""
,[RFC]: Adding Triton Backend for Aten operators," [RFC] Aten Operators in Triton for Multibackend support  Abstract This RFC discusses 1. the benefits and challenges of developing dispatch functions for Aten operators in Triton. 2. a practice in adding Triton backend functions to Aten operators.  Motivation Pytorch now has 2600+ entries in native_functions.yaml (as of this RFC), which grows about 600 compared to what was report at this post. The large number of operators poses a challenge for GPU vendors or other hardware manufacturers who want to add a new backend for pytorch. An opbyop implementation is needed for a comprehensive operator support. There are some efforts on decomposing pytorch operators into a smaller set of operators or lowering pytorch Aten IR into other IRs with a smaller set of operator like FuncTorch, PrimTorch and torch inductor. However these methods either work in a traceandcompile manner, or are too slow to run in eager mode. They sit above the pytorch dispatcher and are very different from the eager execution mode. We propose an alternative solution to the problem by writing operators in a language with multibackend support. In short, it is a Write once, compile anywhere method. Kernels are written once and compiled into different binaries with different compiler backends. This solution is orthogonal to the solutions listed above. Instead of reducing the number of operators by decomposing operatos to a small set, it tries to offload multibackend support to the compiler. Since it integrates into pytorch in Aten disptacher, it works with eager execution seamlessly. With higher abstraction level (tileoriented, CTA level programming) and decent performance, Trit",2024-11-04T03:34:13Z,triaged oncall: pt2 module: inductor,open,2,6,https://github.com/pytorch/pytorch/issues/139602,"   , this is the handwritten Triton implementation for Aten ops discussion we had with FlagGems at PTC.", ,We should discuss this in composability meeting / inductor meeting,"Thanks for taking the time to write this detailed RFC.  Using triton as a replacement to get high quality kernels is definitely something we've been wondering about and this is great to see concrete examples on this. I'm not sure what is your exact target with this RFC but my read seems to be:  This is still a bit early to consider a global move towards this approach within PyTorch. We need to find reliable answers for most of the problems your mention above. One extra challenge that you don't mention is support for older GPU (that triton doesn't support) and ensuring BC during the transition for ops with loosely defined numerics  For similar reasons ( the BC) even for new ops in core, it is hard to use triton as of now (even though it is technically feasible).  There is quite a bit of very useful scaffolding that still needs building (for overhead reduction, autotuning sharing, etc). I think these can be explored out of tree and generally used by many projects that use triton kernels. I would be curious if you think there are a) specific missing pieces in core that you need or b) specific features missing that we need to build with the community? > To work with torch.compile, we need to find a way to make sure that overriding the dispatch does not break the dispatch function for FakeTensor. We currently find that Aten operators with AutogradCUDA implementation added/overridden fail to work correctly with graph tracing in dynamo. The reason for this is that the expected behavior here is that the AutogradCUDA key contains only the autograd handling and the CUDA key contains the kernel. Your code works in eager because we don't rely on these two being properly separated. But torch.compile does! To fix your issue, I would say to either: use the custom_op/triton_kernel implementation from torch.library that handles that for you. Or implement what is done there: put the autograd.Function + a redispatch in the AutogradCUDA key and put the triton kernel in the CUDA key. A good way to double check that you did it right is to use `torch.inference_mode()` which completely disables autograd (by disabling the Autograd* keys) and thus should only hit the kernel. In you case, it's going to fail in weird behavior because the AutogradCUDA is skipped but there is nothing ""behind"" it to call.","Thanks for you reply. I agree that there are much to do to create an operator library with Triton language and also make a pytorch backend with it. We are going to develop it out of tree and try to solve the problems involved. As for the issue with AutogradCUDA (or other backendspecific autograd key), it is good to know that compile needs autograd and backendselect to be properly separated. We will try other ways to do this, for example, registering kernel for some backward operators, like `tanh_backward`.","You can look at https://github.com/pytorch/pytorch/blob/main/torch/_library/autograd.pyL23 which is the internal tooling being used for torch.libary.custom_op to implement this ""properly""."
yi,add opaque unary sin and cos to SYMPY_INTERP,  CC(Always unspecialize float in OSS)  CC(unspecialize float for all torch.compile callers)  CC(only unspecialize float during aot_eager)  CC(only unspecialize float during eager)  CC(Fix another item memo loss location + bool specialization bug)  CC(add special case for __round__ constant variables)  CC(Specialize symfloats that flow through is_integer)  CC(don't run z3 analysis on backed symfloat nodes)  CC(Trigger symfloat specialization in argument binding code)  CC(support symfloats in translation validation)  CC(add opaque unary sin and cos to SYMPY_INTERP) Fixes `PYTORCH_TEST_WITH_DYNAMO=1 python test/test_nn.py TestNNDeviceTypeCPU.test_affine_3d_rotateRandom_cpu` when specialize_float = False ,2024-11-03T03:36:44Z,Merged ciflow/trunk release notes: fx topic: not user facing fx ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/139569, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: inductor / linuxjammycpupy3.9gcc11inductor / test (dynamic_cpu_inductor_torchbench, 1, 2, linux.12xlarge) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[SymmetricMemory] support specifying group_name at rendezvous time,"  CC([SymmetricMemory] introduce userfacing APIs empty() and rendezvous())  CC([SymmetricMemory] support specifying group_name at rendezvous time) Before this PR, users need to call `empty_strided_p2p()` with a `group_name`:  Users can now omit `group_name` at allocation time and specify it later at rendezvous time:  Rationales for this change:  This allows the same allocation to establish symmetric memory under different groups  Specifying `group_name` at rendezvous time instead of allocation time is a more natural UX ",2024-11-01T22:53:03Z,oncall: distributed Merged ciflow/trunk release notes: distributed (c10d) topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/139529, merge,"> I am a tad lost with the indirections between (Cuda)SymmetricMemory, (Cuda)SymmetricMemoryAllocator, AllocationRef and Block. I assume some of it is due to the need to be somewhat deviceagnostic in some parts of the code? Yes. `SymmetricMemory` is the deviceagnostic, userfacing interface. `SymmetricMemoryAllocator` is the interface for devicespecific backends. `AllocationRef` and `Block` are implementation details of `CUDASymmetricMemory{Allocator}`. `AllocationRef` owns the lifetime of a (`vaddr`, `cumem_handle`) pair. It unmaps the vaddr and releases the cumem_handle upon destruction (`cumem_handle` could be an allocation or an imported handle. It is refcounted under the hood). `Block` is the metadata for an allocation (mainly for rendezvous). I'll add some proper documentation to these.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[inductor] sympy.Integer([01]) -> sympy.S.(Zero|One),  CC([inductor] Refactor reduction type choices into V.choices)  CC([inductor] sympy.Integer([01]) > sympy.S.(Zero|One))  CC([inductor] Simplify remove_kernel_local_buffers)  CC([inductor] Move remove_kernel_local_buffers to Kernel)  CC([inductor] Remove Node.last_usage mutation)  CC([inductor] Remove SIMDKernel.last_usage) ,2024-11-01T22:07:39Z,module: cpu Merged Reverted ciflow/trunk topic: not user facing module: inductor ciflow/inductor ci-no-td,closed,0,15,https://github.com/pytorch/pytorch/issues/139523, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , your PR has been successfully reverted.,"I reverted the stack at https://github.com/pytorch/pytorch/pull/139364, and somehow the bot reverted this one too.  Let me know if it's correct", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3clang12executorch / test (executorch, 1, 1, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 17f5fa8c4fe19fd100371ceb449d9534a2f0f070` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x afe45e3ed0bdc27e56d1ed7fe51b06a65a822354` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,libtorch. Executing at::sum after multiplying the slices yields different results," üêõ Describe the bug   Versions Collecting environment information... PyTorch version: 1.8.0+cpu Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 ‰ºÅ‰∏öÁâà (10.0.19044 64 ‰Ωç) GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19041SP0 Is CUDA available: False CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Nvidia driver version: 546.65 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Name: Intel(R) Core(TM) i77700K CPU @ 4.20GHz Manufacturer: GenuineIntel Family: 198 Architecture: 9 ProcessorType: 3 DeviceID: CPU0 CurrentClockSpeed: 4201 MaxClockSpeed: 4201 L2CacheSize: 1024 L2CacheSpeed: None Revision: None Versions of relevant libraries: [pip3] numpy==1.24.2 [pip3] pytorch2caffe==0.1.0 [pip3] torch==1.8.0 [conda] Could not collect",2024-11-01T03:07:05Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/139459,"this is expected, see https://pytorch.org/docs/stable/notes/numerical_accuracy.html"
rag,Expose Storage _use_count API in Python,"Would be nice to replace the torch._C._storage_Use_Count call in https://github.com/pytorch/torchtune/pull/1936, at least without needing to know about _cdata in OSS code. Initially keeping it private as Tensor._use_count is also private. In favor over https://github.com/pytorch/pytorch/pull/139109 in solving the same problem, as exposing an existing API is better than adding a new one (and this enables a more robust fix)   CC(Expose Storage _use_count API in Python)",2024-10-31T20:36:10Z,Merged Reverted ciflow/trunk release notes: python_frontend ci-no-td,open,0,14,https://github.com/pytorch/pytorch/issues/139426, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.12clang10 / test (default, 1, 5, linux.4xlarge)  pull / linuxfocalpy3.12clang10 / test (dynamo_wrapped, 1, 3, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / build Details for Dev Infra team Raised by workflow job "," merge f ""broken trunk and mac build timeout don‚Äôt look like my fault"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m 'Sorry for reverting your change, but it is failing some inductor job in trunk' c nosignal test_torch.py::TestTorch::test_storage__use_count_APIs_align GH job link HUD commit link", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
transformer,`torch.compile` error on `scaled_dot_product_attention` in `transformers.LlamaForCausalLM` when providing `attention_mask`: `RuntimeError: (*bias): last dimension must be contiguous`," üêõ Describe the bug Related: *  CC(SDPA + torch.compile: (*bias): last dimension must be contiguous) *  CC(""RuntimeError: (*bias): last dimension must be contiguous"" with F.scaled_dot_product_attention + torch.compile)  cc:   Versions PyTorch version: 2.6.0.dev20241030+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 OS: Ubuntu 22.04.5 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.0124genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.4.131 GPU 0: NVIDIA H100 80GB HBM3 Nvidia driver version: 550.90.12 Versions of relevant libraries: [pip3] pytorchtriton==3.1.0+cf34004b8a [pip3] torch==2.6.0.dev20241030+cu124 [conda] pytorchtriton            3.1.0+cf34004b8a          pypi_0    pypi [conda] torch                     2.6.0.dev20241030+cu124          pypi_0    pypi ",2024-10-31T19:50:47Z,triaged oncall: pt2 module: dynamo,open,0,10,https://github.com/pytorch/pytorch/issues/139424, I can't repro the failure. Would you mind doing the ablation described here to help me understand which layer is contributing to the failure: https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0heading=h.f8o4pwe4uv1h,">  I can't repro the failure. Would you mind doing the ablation described here to help me understand which layer is contributing to the failure: https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0heading=h.f8o4pwe4uv1h It fails with the same error with `backend` set to `""eager""`, `""aot_eager""`, or `""aot_eager_decomp_partition""`. The deepest I can go into the model to still achieve this error is the `LlamaModel` of `LlamaForCausalLM`. Running with `TORCHDYNAMO_REPRO_AFTER=aot`, I got this script that also repros (for me):  The tail of the error message I see: ",Can you repro with the minifier script or still not?, can you check if https://github.com/pytorch/pytorch/pull/139787 fixes it for you ?,"No, it didn't seem to fix it. Also, I don't know if I posted the right repro script previously... Does this reproduce for you? ",Your original repro works for me on my pr.,ü§î  Are you building torch from source when you are testing your branch? I'm trying to consider what the differences between our environments might be... I've simply been modifying the nightly wheel installed in my sitepackages with your change in `torch/_inductor/lowering.py` and I am still seeing the error persist on my original repro.,"Can repro now that I am on `transformers==4.46.2`.  , we have this argument which is being expanded inside inductor.   We are viewing a tensor of size `[2, 1, 8192, 8]` as a tensor of size `[2, 32, 8192, 8192]`.  Can we loosen the checks inside `scaled_dot_product_attention` to allow stride 0 ? this will still be 16byte aligned. The alternative is we have to materialize this much larger tensor in memory, which will be slow.", https://github.com/pytorch/pytorch/pull/139752 this should do it right?,"That just allows size 1 but it doesnt allow stride 0. The input to the sdpa op will be size `[2, 32, 8192, 8192]`"
transformer,[export] Failure to export Mixtral-8x7B-Instruct-v0.1," üêõ Describe the bug I'm experiencing an issue exporting the `Mixtral8x7B` model. I originally discussed the problem in CC(Failure to export Mixtral8x7BInstructv0.1 as onnx), but this is more about `torch.export`.  My ultimate goal is to go to StableHLO. It raises the error `error torch._dynamo.exc.Unsupported: hasattr ConstDictVariable to.` I've seen a similar issue here)), is this something that we'd need to add a custom handler for? Script to reproduce problem     I also tested with `torch.onnx.export(..., dynamo=True, report=True)` on the latest torchnightly (`2.6.0.dev20241030+cpu`), I get the following: (Markdown error report in this gist).   You both have commits to `torch.export`, any recommendations?  Versions  ",2024-10-31T15:19:22Z,oncall: pt2 export-triaged oncall: export,open,0,4,https://github.com/pytorch/pytorch/issues/139401,"I'd highly encourage you to try exporting with `strict=False`, which avoids Dynamo coverage issues such as this.","Thanks!  For LLama2 exports I've been using:  as recommended as a workaround hereissuecomment2161601418). However, trying this with Mixtral gives:  This also happens when adding `strict=False` to the normal `torch.export.export()`.",You can use the public API with `strict=False`. Could you report if Mixtral goes through with that?,"> You can use the public API with `strict=False`. Could you report if Mixtral goes through with that? Yes, I got the same behaviour with the public `torch.export.export()` API."
transformer,[MPS] Torch 2.5.x and Nightlies are using  50% more memory and are 60% slower than 2.4.1 run Stable Diffusion," üêõ Describe the bug I've seen a lot of people mentioning in various forums but not seen an issue raised here so here we go. I'm reporting this as an MPS issue as I've not been able to test on NVIDIA or AMD etc. Running various Stable Diffusion and new transformer DiT models using 2.5.x and nightly releases of PyTorch shows a very significant downgrade in performance compared to running them in in the same environment using pytorch 2.4.1. For example a standard SDXL run via Diffusers using 2.4.1, the python binary reports using 9.5Gb and runs at 5.7 seconds per iteration, under 2.5.1 or nightly it reports 14.9 GB and runs at 8.5 s/i SD3.5 goes from running without setting  PYTORCH_MPS_HIGH_WATERMARK_RATIO  on a 24Gb M3 to failing if its not set to 0.0 and using 37Gb compared to 27.7GB when they first start iterating (2.5.1 takes ages to start iterating they'll use more when it kicks in properly). Here is the diffusers SDXL script I used as that'll be the smallest and fastest model. Its a little messy as was a test script of something else  but I wanted to give you the exact same script, so you'll have to forgive the unused imports and variables :)  environment id a straight forward diffusers venv  torch version was switch around br doing an uninstall and reinstall  or   Versions  python lib/python3.11/sitepackages/torch/utils/collect_env.py  Collecting environment information... PyTorch version: 2.5.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 15.0.1 (arm64) GCC version: Could not collect Clang version: 16.0.0 (clang1600.0.26.4) CMake version: version 3.29.5 Libc version: N/A Python ve",2024-10-31T12:25:28Z,high priority module: memory usage triaged module: mps,closed,0,10,https://github.com/pytorch/pytorch/issues/139389, do you mind posting references to some of the forum threads here?,"Struggling to find them again, crappy discord,  actually I keep finding people mentioning 2.4.0 period nightlies compared to 2.3.1 but try the example  it's hard to miss the slowdown and extra memory usage.","I've modified script a bit  It reports that on my M2 Pro 2.4.1 finishes generation in 50 sec using 14Gb of memory, while 2.5.1 needs 64 sec and 16Gb RAM, so 30% slower and 15% more RAM, which is pretty bad",,The PR that caused at least of of the regressions Is https://github.com/pytorch/pytorch/pull/131863,Let's keep it open for a bit until nightly is available and someone can confirm that this is working for them.  can you build PyTorch from source using https://github.com/pytorch/pytorch/commit/68ef445c330d465d3d20f253583c2370c5df0139 or prefer to wait until nightly build is available?,"As I'm off to bed in a few minutes it'll probably have to wait until the nightlies are built, what time is that normally ?", I just ran with ToT and the memory utilization + runtime is back in line with 2.4.1,"Just tried a nightly.  speed is back, if fact its improved,  memory practically there ~ 3.4% .","Cool thank you all for validating, closing now ,but need to figure out how we can monitor for something like that...."
yi,inspect.Signature with functools.partial partially applying tensors doesn't work, üêõ Describe the bug  cc:     Versions N/A ,2024-10-31T05:32:13Z,high priority triaged oncall: pt2 module: dynamo,open,0,1,https://github.com/pytorch/pytorch/issues/139374,I think I'm also hitting this issue.
transformer,Plan to support ‚Äúdiscrete‚Äù dynamic dimension on torch.export," üöÄ The feature, motivation and pitch Is there any plan for the `torch.export` team to support the setbased dynamism in addition to rangebased dynamism? Below is a small repro code for discrete `Dim` failure mode. The `torch._check` is supposed to work within discrete sets, but it fails.   Alternatives _No response_  Additional context The conventional use of the dynamic dimension on `torch.export` is to use the continuum of the variables like `Dim(min=3, max=12)`, which is suited for accepting dynamic batch size for example. Our team is trying to support some level of ‚Äúdiscrete‚Äù dynamism on the neural network, for providing ‚Äúimage‚Äù mode and ‚Äúvideo‚Äù mode for the vision transformer. To show some idea behind this, here is a pseudocode for doing the operation. The specifics are not same in detail.  The `torch.export` immediately fails, because the second `lambda` function inside `torch.cond` is not compatible with the length other than 12. Since there is no way to enforce the model to accept the ‚Äúset‚Äù based constraints other than the continuum of the shapes, our team has no way other than exporting two almostidentical models with sharing large sets of common transformer weights. ",2024-10-30T18:44:31Z,oncall: pt2 module: dynamic shapes export-triaged oncall: export,open,1,7,https://github.com/pytorch/pytorch/issues/139307,I also wonder if it's possible to parse native Python `assert` statements (e.g. supporting constraints on shape elements and enforcing eq/lt relations between shapes of different tensors) and transform them into torch.compile constraints?   CC(Record shaping assertions and use them for tracing / scripting optimization and codegen),Error message: ,"If the graphs end up differing between the discrete values there's nothing you can do, you have to torch.cond your way to victory or export multiple graphs.","> If the graphs end up differing between the discrete values there's nothing you can do, you have to torch.cond your way to victory or export multiple graphs. Understood. Currently the dynamic dimension is in the continuum of the shapes, so it is really challenging to capture this branch in a single graph. For example one of the shapes can be 2 or 4 and not in between, there is no way to enforce this as a dynamic dimension and the graph capturing will not be run as intended. Is there any plan to support this discrete dynamic dim in the future? Or is it not possible or appropriate thing to support because of the inherent design of dynamic graph capturing?",The problem is specifically around capturing and preserving branching behavior in the set of computations you do at runtime. This is only supported through torch.cond. There may be some auxiliary problems where you are unable to refine shape symbols temporarily inside cond but those we should fix.," Following this thread, can you add a bit more color to your response? In this modified toy example above,   the assert `torch._check(x.shape[0] in {2, 3})` forces `torch.export` to infer `x` has a constant shape of 3. That seems odd, especially given that shape guards can check for x in a range (e.g. `torch._check(2 <= x.shape[0] <= 3)` is completely fine). In principle, couldn't there be a `torch.export.Dim` object that took `values={...}` as a param and applied a disjunctive shape guard, e.g.  which would then pass ","We technically support disjunction but it is not exposed in a user facing way because it's.... not very useful ü§£ . We don't currently do any reasoning based off of the information, so it would basically be equivalent to not having the check at all. I suppose that in principle discrete values could be used to statically reason about unbacked SymInts, as you could enumerate all disjunct values to see if a given expression is statically known in all situations... but that's pretty obscure. When you write `x.shape[0] in {2, 3}`, there is an additional problem that we must recognize that this is something that we can generate a disjunction on, as opposed to guarding. Without Dynamo, this would be impossible to do as the only thing the reasoning system would see is the equality test on 2 and then the test on 3 (or, I suppose in this case, you would try to hash the SymInt, which doesn't work.) With Dynamo, in principle, we could write a special case for test of membership in a set which is known to be constant...... I guess I would accept a patch that does this, but it seems pretty niche."
rag,`torch.package` warning -- `TypedStorage` is deprecated," üêõ Describe the bug With torch  `2.5.1+cu124`, just using `package_importer` and `package_exporter` throws the warning.  and   Versions ",2024-10-30T17:03:55Z,oncall: package/deploy,open,0,0,https://github.com/pytorch/pytorch/issues/139297
llm,Calculate speed," üêõ Describe the bug !image I test the speed when calculating the matrix multiplication or dot multiplication, it is so strange and counterintuitive, why the tensor is larger and the speed is faster? I am working on a pruning method on LLM and could make it a 0.3 sparsity LLM, means the parameters will be reduced to 0.7. And when test the forward it is slower. Why could it happen?  Error logs No  Minified repro _No response_  Versions 2.0.1+cu118  ",2024-10-30T13:29:13Z,oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/139282, there's not much to go on here with the title or the summary. Can you add detail on the actual problem / request?,"Since there are no actionable details, I'm going to close this out. Please reopen if you can provide the context."
transformer,torch.compile'ing individual linears for torchtitan debug model + FSDP2 leads to errors," üêõ Describe the bug When I change torchtitan's torch.compile logic to compile individual linear layers instead of transformer blocks, I see an error:   There is a full repro here: https://github.com/pytorch/torchtitan/pull/661  you can check out that PR against torchtitan and run the test plan on a machine with at least 2 GPUs to see the error. Note that this seems similar to  CC(inductor error with PT Lightning + FSDP + torchao.float8 + torch.compile), but this repro is on FSDP2.  Versions Pytorch version 2.6.0.dev20241023+cu121 ",2024-10-29T21:19:49Z,oncall: distributed triaged module: fsdp oncall: pt2 module: dynamo module: guards pt2d-triage-nov2024,open,0,4,https://github.com/pytorch/pytorch/issues/139222," ,  ","hmm... this does seem eerily similar to https://github.com/pytorch/pytorch/pull/138819, I'll try to confirm if that fixes this for FSDP2 as well later this week","Confirmed locally that this is also fixed by https://github.com/pytorch/pytorch/pull/138819. I still need to look more into the CI failures on that PR, with some help from Animesh. With that change, the repro runs E2E for me. I do see many recompiles though, which are probably worth looking into"," these are the recompiles I get with the above patch:  And this sort of makes sense: we have 3 linear layers, where each layer has a parameter with a difference shapes. compile treats parameter shapes as static by default, so we recompile for each distinct layer's weight shapes. You could try the suggestion in the error message (dont force compile to treat parameter shapes as static  although specializing on param shape might give better matmul perf)"
finetuning,"RuntimeError: Storage size calculation overflowed with sizes=[1, 4605674770382112385, 4128, 128]"," üêõ Describe the bug Hi,  I am finetuning a large multimodal model (basedon InternVL28B, with some additional modules) on 8 * A100 GPUs, using DDP + MP (NPROC=2).  When I execute `self.accelerator.pad_across_processes(logits, dim=1, pad_index=100)` in `sitepackages/transformers/trainer.py`, the error occured: !image The info of the input argument `logits` is below:  We can see that there is no unusual thing about the `logits`.  Furthermore, I print out the info of the variables: `new_size`, `max_size`, and `sizes` in the method `_pad_across_processes`. Hope they can help.  !image Could you please tell me how to solve this error? Thanks a lot!  Versions ",2024-10-29T09:57:54Z,triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/139181,"Hi, do you have a minimal reproducer for this? Perhaps this might be a more appropriate issue for the transformers library.","Sorry, I just saw there was another issue which is similar to this (but different), so I put this issue to your pytorch repo üòÇ. Indeed, this issue should be given to transformers lib. "
llm,SubgraphMatcher may fail to match when the matching pattern having call_module IR," üêõ Describe the bug Hey guys, when i use SubgraphMatcher to match a pattern having call_module IR. For example, when i claim a model like this :  when i using the pattern and replacement like this, it may fail to match:  My torch version is '2.3.1+cu121'. And after reading the source code of SubgraphMatcher, I find that it will only match the pattern having call_module IR when the source graph and pattern call the module with the same name(e.g. the torch.nn.Linear in the TestModel and PatternClass shall named as the same name mlp). I think it is a bug since we should match the pattern depends on the module class instead of the name of the module  Versions PyTorch version: 2.3.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.11.027genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.2.152 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A6000 Nvidia driver version: 550.78 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          96 Online CPU(s) list:             095 Thread(s) pe",2024-10-29T06:49:51Z,triaged module: fx,open,0,0,https://github.com/pytorch/pytorch/issues/139163
llm,[PyTorch] Migrate bf16 gemv fast path kernel from intrinsics to vec::Vectorized,"  CC(Hook up bf16_gemv_trans to x86 bf16 GEMM)  CC(Build bf16 gemv fast path & entry points for nonARM architectures too)  CC([PyTorch] Migrate bf16 gemv fast path kernel from intrinsics to vec::Vectorized)  CC(Move bf16_gemv_trans to ReducedPrecisionFloatGemvFastPathKernel)  CC(Add Vectorized specialization for ARM)  CC(Extract value_typegeneric NEON Vectorized functions to CRTP base class) Very similar to CC([PyTorch] Migrate fp16 gemv fast path kernel from intrinsics to vec::Vectorized), but for bf16. (This is building toward enabling this fast path on nonARM architectures, and in particular on x86 for machines without AVX512BF16). Testing: checked for regression with llm_experiments' benchmarks/benchmark_torch_mm.py llm on M1 Mac and it appeared to be neutral. Supported this assessment by inspecting assembly for the bf16 dot kernel (`objdump d noleadingaddr noshowrawinsn build/caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/cpu/ReducedPrecisionFloatGemvFastPathKernel.cpp.DEFAULT.cpp.o | c++filt` from pytorch root directory after `python setup.py develop`); observed minor instruction scheduling changes but nothing more. Differential Revision: D65120325 ",2024-10-29T05:41:06Z,module: cpu fb-exported topic: not user facing ciflow/mps ciflow/linux-aarch64,closed,0,8,https://github.com/pytorch/pytorch/issues/139159,This pull request was **exported** from Phabricator. Differential Revision: D65120325,This pull request was **exported** from Phabricator. Differential Revision: D65120325,This pull request was **exported** from Phabricator. Differential Revision: D65120325,This pull request was **exported** from Phabricator. Differential Revision: D65120325,This pull request was **exported** from Phabricator. Differential Revision: D65120325,This pull request was **exported** from Phabricator. Differential Revision: D65120325,This pull request was **exported** from Phabricator. Differential Revision: D65120325,folding this one into https://github.com/pytorch/pytorch/pull/139081 because I am needing increasingly large parts of it there anyway
agent,[ROCm] PyTorch TunableOps results in Memory Access Fault ," üêõ Describe the bug We are seeing a Memory Access Fault by GPU Node error when running the below script `run.py` with PyTorch TunableOps enabled. All the testing was done on systems with 8xMI300X GPUs with ROCm 6.2.1 and the latest compatible GPU firmware. Running the script without TunableOps enabled works. Error message:  The specific GPU node number changes, and this was reproduced on multiple systems. Steps to reproduce:  `run.py`:  `requirements.txt`:   Versions pytorchtritonrocm==3.0.0 torch==2.4.1+rocm6.1 torchaudio==2.4.1+rocm6.1 torchvision==0.19.1+rocm6.1 ",2024-10-28T22:39:05Z,module: crash module: rocm triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/139116,"We also saw this error on the following PyTorch versions:  We also tried the following rocm/pytorch container: `rocm/pytorch:rocm6.2.1_ubuntu22.04_py3.10_pytorch_release_2.3.0` which had `torch==2.3.0a0+git1b935e2`. In the rocm/pytorch container, we were able to run `run.py` with TunableOps enabled and without a Memory Access Fault.",tensorwave https://github.com/pytorch/pytorch/pull/138791 This PR is supposed to fix TuanbleOp mem issue. It has not landed :),We're finding that the size calculations for the buffer rotation feature of TunableOp are incorrect.  PR CC([ROCm] set hipblas workspace) addresses an OOM issue.  We need a hotfix PR for this new issue.," confirm the OOM issue solved. but this one seems a different one, PYTORCH_TUNABLEOP_ENABLED=1 PYTORCH_TUNABLEOP_VERBOSE=3 PYTORCH_TUNABLEOP_MAX_TUNING_ITERATIONS=5 python3 run.py GemmTunableOp_BFloat16_NN(nn_16_7200000_1) > Gemm_Rocblas_684,0.134859 missing params_signature, returning null ResultEntry for GemmTunableOp_BFloat16_NN,nn_16_1_7200000 finding fastest for GemmTunableOp_BFloat16_NN(nn_16_1_7200000) out of 2801 candidates Rotating buffer 32 MiB. Needed Size: 219 MiB. Needed number of param copies: 1 GPU core dump failed Memory access fault by GPU node2 (Agent handle: 0x88b6bc0) on address 0x7f4f81801000. Reason: Unknown. Aborted (core dumped)","tensorwave  for this run.py, the matrix size is large, 16_7200000_1, it requires workspace 219MB, and the default HIPBLASLT_WORKSPACE_SIZE is 32MB (32*1024). a workaround is to increase workspace (kb, 256*1024 = 262144Kb = 256MB), ""**HIPBLASLT_WORKSPACE_SIZE=262144**"" PYTORCH_TUNABLEOP_ENABLED=1 PYTORCH_TUNABLEOP_VERBOSE=3 HIPBLASLT_WORKSPACE_SIZE=262144 python3 run.py   should we increase default hipblaslt workspace size or add some explanations in README, https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/cuda/tunable I think this may be a logic error: if all algos require space greater than 32MB, then no valid algo to tune. so either the code increase workspace, or choose the default algo.", indeed this is a different issue. CC(TunableOp use dense size calculations as minimum sizes) will fix it.  Tested locally.  Waiting for CI and upstream reviewer.
llm,Move bf16_gemv_trans to ReducedPrecisionFloatGemvFastPathKernel,"  CC(Hook up bf16_gemv_trans to x86 bf16 GEMM)  CC(Build bf16 gemv fast path & entry points for nonARM architectures too)  CC(Move bf16_gemv_trans to ReducedPrecisionFloatGemvFastPathKernel)  CC(Unbreak vec128_half_neon comparison without FP16 hardware support)  CC(Add Vectorized specialization for ARM)  CC(Extract value_typegeneric NEON Vectorized functions to CRTP base class) Following the previous move of fp16_gemv_trans. Testing: Checked for performance regression with llm_benchmarks' `python benchmarks/benchmark_torch_mm.py llm`, didn't find one Differential Revision: D64930872 ",2024-10-28T17:20:45Z,module: cpu fb-exported Merged ciflow/trunk topic: not user facing ciflow/mps ciflow/linux-aarch64,closed,0,22,https://github.com/pytorch/pytorch/issues/139081,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,"need to reorder this after https://github.com/pytorch/pytorch/pull/139090 and pull the fmadd(Vectorized, Vectorized, Vectorized) from https://github.com/pytorch/pytorch/pull/139159 into this rev so that the vectorized tail loop will use SVE if appropriate rather than trying to mix Vectorized with NEON intrinsics.",This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,This pull request was **exported** from Phabricator. Differential Revision: D64930872,"some tests still in progress, but none of them are mps/aarch64 so I'm optimistic and rerequesting review."
llm,"[dynamo] ""skip_guard_eval_unsafe"" API for power users","  CC([dynamo] ""skip_guard_eval_unsafe"" API for power users)  CC([dynamo][eval_frame] Set the callback to None earlier for guard eval)  Motivation We have spent quite some time this year on improving guard performance and soundness. Nevertheless, guards STILL take time. We have seen multiple requests/evidences from power users where they want to have almost 0% guard overhead. First, we saw this in vLLM where even 1% overhead is bad. And recently we saw this in hqq (low precision LLM generation)   CC(torch 2.5 slower than 2.4.1 ? ). To put some numbers for perspective, low precision LLM inference reaches around 250 tokens/second, i.e, each token takes a mere 4 milliseconds. If guard overhead is even 200 us, its still 5% overhead in total. Here, users ask  ""we can guarantee that there will no more recompilations in the steady state, give us the lowest guard overhead""  Design A musthave consideration is to support fast inference where the model has recompiled, i.e., has multiple cache entries for a code object (could be because of dynamism, or just tensor dtype change in the case of hqq). So, we still have to run the guards to figure out which compiled graph to run. What we need is the ""minimal set of differentiating guards""  i.e. minimals set of guards that we can run to choose the compiled graph. Note that this works ONLY with the assumption that users really guarantee no more recompilation scenarios (no more mutations, no more dynamism after the model has been warmed up). It is possible that if user violates this assumption, and it is not covered by the diff guard set, we will choose a wrong compiled graph to run. When we designed C++ gua",2024-10-28T05:08:49Z,ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/139038,"I'm positive on this. We should log entrance to this stance to tlparse so we can easily tell if someone is in this mode for debugging purposes. I think hard error on miss is a must. I might suggest a softer version of this that still does dynamic shapes guards (or in general, guards that are ""likely"" to catch correctness problems). I'm trying to remember who was most against the sticky cache concept. Maybe  but he's not available right now. I would show it to that person to get the stiffest feedback.","> I might suggest a softer version of this that still does dynamic shapes guards (or in general, guards that are ""likely"" to catch correctness problems). >  Sounds good, this is quite easy to do today. Dynamic shape guards are sitting on the RootGuardManager, so we can always run the RootGuardManager in the ""diff guard set"".",This is interesting and I find some similarities to the `torch.export` Dims. So I think that a common overview between this two concepts it could help to expose it to the users also if we are talking about two different use case. Can an explicit interface like `torch.export` Dims be more clear to the user for a warmup analysis? I think we could adopt something similar to `torch.export` logic and API when we want to skip guards but currently I found two main issue with that API?   Not able to predeclare all the discrete/sparse dims I expect in my program  CC([torch.export] Dim discrete/finite set)   Counterintuitive interplay between ranges and guards  CC([torch.export] Detect internal constrains), `export` is little bit out of scope here. `torch.export` is AOT while `torch.compile` is JIT. This completely changes how you represent and run guards.,"> export is little bit out of scope here. torch.export is AOT while torch.compile is JIT.  I know this > This completely changes how you represent and run guards. Are we sure about this also for the specific `skip_guard_eval` cases in this thread? > Note that this works ONLY with the assumption that users really guarantee no more recompilation scenarios (no more mutations, no more dynamism after the model has been warmed up). I know this is not exactly like the AOT case but in this specific case I see many similarities.  E.g. we can still have cases where I need to JIT optimize the code (e.g. running on different HW instead the same hardware as with `export`) and in the warmup phase I can really expose all the possible inputs I expect to give to the program.  Also talking about this warmup phase we can pass these inputs as real ""live"" inputs or we can always pass eventually a discrete set of a continuous range of dims right?  As mentioned in the previous comment currently this is not supported if we consider to have an API similar to `export` Dims. I don't know if this is still so offtopic in this specific AOT case after this little expanded rationale/use cases. I am not a lot deep inside these internals but I still see a logical connection from the public API surface and by the end user model develop/deployment angle.","> Note that this works ONLY with the assumption that users really guarantee no more recompilation scenarios (no more mutations, no more dynamism after the model has been warmed up). P.s. There are also cases where this could be guaranteed, with the owner responsibility, by what the program were already historically collected in the remote cache and all its historically collected dynamic inputs.","just find this and chime in here. vLLM finally goes to the solution of custom dispatcher, we have some variables not seen by `torch.compile`, and we dispatch to compiled cache entries based on those variables. see https://github.com/vllmproject/vllm/blob/main/vllm/compilation/wrapper.py and https://github.com/vllmproject/vllm/blob/91c9ebbb1bfc39e98aa2bd444b9569e5f2f92c9e/vllm/worker/tpu_model_runner.pyL680L696  not sure if this helps.", This is ready for real first round of review.,"Moving to https://github.com/pytorch/pytorch/pull/140251 The main reason for closing this is implementation. In the other stack, I am introducing the facility to clone the guard manager which makes it more readable and performant."
transformer,"When I load the model data of torch_model.bin, it will go directly to GPU loading, and cannot load the whole process on CPU"," üêõ Describe the bug When I load the model data of torch_model.bin, it will go directly to GPU loading, and cannot load the whole process on CPU. Below is my code  source fileÔºötorch/_utils.py:_cuda:113   Versions Collecting environment information... PyTorch version: 2.3.0 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.0119genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 4090 GPU 1: NVIDIA GeForce RTX 4090 GPU 2: NVIDIA GeForce RTX 4090 GPU 3: NVIDIA GeForce RTX 4090 Nvidia driver version: 550.107.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        43 bits physical, 48 bits virtual Byte Order:                           Little Endian CP",2024-10-26T05:24:06Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/138977,"Use `map_location=""cpu""`?",Please ask the question on how to use PyTorch on https://discuss.pytorch.org and use issues to report unexpected behaviour/feature requests. `map_location` option is indeed a solution here
rag,[dynamo][guards] Log average time of constructed guard_manager,  CC([dynamo][guards] Skip guards on empty nn module hooks)  CC([dynamo][guards] Skip no tensor aliasing guards on parameters)  CC([dynamo][guards] Log average time of constructed guard_manager)  CC([dynamo][refactor][configcleanp] Use guard_manager consistently instead of check_fn)  CC([dynamo][configcleanup] Remove enable_cpp_guard_manager=False codepath) ,2024-10-25T21:37:47Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/138941," merge f ""stuck jobs"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,"[ROCm] ""No available kernel"" when running EFFICIENT_ATTENTION sdpa",Hit this error when running https://huggingface.co/genmo/mochi1preview. Repro script as followed.   Environment:  ,2024-10-24T23:42:49Z,module: rocm triaged,open,0,4,https://github.com/pytorch/pytorch/issues/138864, ,"  This is expected, ME on ROCM only support head dimension <= 256. The example needs head dimension == 512.",Hi  will there be plan to support this in ROCm?,"> Hi  will there be plan to support this in ROCm? There is no solid plan for arbitrary head dimension support right now, since it requires major rework of the kernel. "
yi,Fixing issue in move pass for copying Parameter,Summary: Fixing bug for Parameter copy during move pass of exported graph. Test Plan: UT runs on APS models. Differential Revision: D64876951,2024-10-24T21:50:00Z,fb-exported Merged ciflow/trunk release notes: export,closed,0,6,https://github.com/pytorch/pytorch/issues/138855,This pull request was **exported** from Phabricator. Differential Revision: D64876951, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""macos runner issues  pushing this one through""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Move inner loop of _create_symbolic_sizes_strides_storage_offset into its own method,Making the next PR easier to review:  move the inner loop of  _create_symbolic_sizes_strides_storage_offset() into a separate function  fix lintrunner lints   CC(Simplify _compute_symbolic_stride())  CC(Move inner loop of _create_symbolic_sizes_strides_storage_offset into its own method) ,2024-10-24T19:10:14Z,release notes: fx fx ciflow/inductor,open,0,0,https://github.com/pytorch/pytorch/issues/138843
transformer,[export] `run_decompositions` fails with pytree error on `Llama-3.2-vision`,The attached code fails with the following error when `run_decompositions` is run. **It is a regression from torch 2.5.** This affects the ONNX exporter.  Error   Code   Env   ,2024-10-24T18:41:19Z,triaged oncall: pt2 oncall: export,closed,0,9,https://github.com/pytorch/pytorch/issues/138839,  bump on this, ,"> bump on this  not sure I see what the ask is here. I've passed it to the export team, they are looking into it. If you think this is a high priority issue, please add a label and it'll be discussed during the next triage review ","> > bump on this >  >  not sure I see what the ask is here. I've passed it to the export team, they are looking into it. If you think this is a high priority issue, please add a label and it'll be discussed during the next triage review Thanks, just looking for an initial response as it's been around for some time now. Hopefully it's resolved in a week or two as it's blocking high pri model export. Would this time frame require a high pri tag?",I will take a look today. ,"Could you patch this before export:  The issue is not run_decompositions, it is more like export failed to recognize the user subclass dict input as a proper dict and it accidentally worked. I think we need better detection mechanism in export to error out early on. ",Thank you!," If you got unblocked, can i close this issue?","Yes, thanks!"
llama,[inductor][cpu]llama amp single thread inductor_max_autotune performance regression in 2024-10-16 nightly release, üêõ Describe the bug amp static shape default wrapper               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       llama       single       1       3.992154       0.013228828       0.052811518615512004       81.807902       1       6.126736       0.008996031       0.055116306984816       85.39452       0.65       1.04       0.68       1.04          amp dynamic shape default wrapper               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       llama       single       1.0       4.138349       0.013100655       0.054215082518595       80.950906       1       5.796402       0.008985967000000001       0.052086277090734004       84.002409       0.71       0.96       0.69       1.04          the bad commit 4e8997744cee18c0c5d8112951d2f5a93175e63e ``` /workspace/pytorch bash inductor_single_run.sh single inference performance torchbench llama amp first static default 0 inductor_max_autotune Testing with inductor_max_autotune. singlethread tes,2024-10-24T15:30:17Z,triaged oncall: pt2 oncall: cpu inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/138823, will help to double confirm the guilty commit.,"The suspected commit turns on `coordinate_descent_tuning` by default when compile mode is `maxautotune`. In this model, the last linear with shape: `[1, 512]` and `[512, 32000]` has been decomposed into `mul + sum` as in https://github.com/pytorch/pytorch/blob/e6e140c3d731a15fbdb1a8df93a288915c4c0e97/torch/_inductor/decomposition.pyL289L293 which cause 2 issues here: 1.  Autocast will not cast act and wgt into BF16 for `mul` when turns on AMP. 2. `mul + sum` pattern can't be fused and hit the onednn which will be codegen by cpp backend with low performance. A quick solution might be disable this decomposition on CPU for now,  ","Here is the graph after decomposition   Autocast will insert node to cast act/wgt to bf16  Since the decomposition of `mm` has been registered as `pw_cast_for_opmath`, it will cast inputs to fp32 and cast output to bf16. Here is the graph after constant folding and before we do the weight prepack  There is no node to cast act to bf16 and `_frozen_param133` has been folded back to fp32.","> A quick solution might be disable this decomposition on CPU for now, .",https://github.com/pytorch/pytorch/pull/139537 landed to fix this issue  please help to verify.,The issue is fixed
transformer,"There is no header file ""torch/torch.h""file int the include folder.I doubt if that file was removed in the latest version.","**_I download the  release binary files from the https://pytorch.org/  and ,my version is CUDA 12.4._** However,after I unziped the release,I discover that when I use cmake to contain the Libtorch to my build system,I connot find the key header file ""Torch/torch.h"" in the include folder. You can see it below.I install both Debug and Release version for that,so ...I doubt if the header was removed from libtorch,and I cannot query any information about it... Can anyone help me slove this question,thanks a lot...  ",2024-10-24T15:25:49Z,needs reproduction module: cpp triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/138822,"And there is a few files  exists in the torch folder such as ""torch/script.h""...What can I say...","Not sure which binaries you are trying to test against (and what this folder graph is about), but `torch.h` is present in 2.5.0 binaries. Taking windows cpu one: ","> Not sure which binaries you are trying to test against (and what this folder graph is about), but `torch.h` is present in 2.5.0 binaries. Taking windows cpu one: >  thank you a lot ÔºåI would try this package later.","Closing, but please do not hesitate a new issue if you run into a new problem with PyTorch"
chat,[Pytorch][ATEN] Enable FP8 NCCL in Pytorch ATEN,Summary: Enable FP8 NCCL in Pytorch ATEN to unblock FP8 collective communication such as FP8 alltoall Test Plan: CI & D64374424 Differential Revision: D64866426 ,2024-10-24T00:15:25Z,oncall: distributed fb-exported Merged ciflow/trunk topic: not user facing,closed,0,13,https://github.com/pytorch/pytorch/issues/138776,This pull request was **exported** from Phabricator. Differential Revision: D64866426,This pull request was **exported** from Phabricator. Differential Revision: D64866426,"> Does this need a test? Yeah, added new tests to cover changes"," label ""topic: not user facing""",This pull request was **exported** from Phabricator. Differential Revision: D64866426,This pull request was **exported** from Phabricator. Differential Revision: D64866426,This pull request was **exported** from Phabricator. Differential Revision: D64866426,This pull request was **exported** from Phabricator. Differential Revision: D64866426,This pull request was **exported** from Phabricator. Differential Revision: D64866426,This pull request was **exported** from Phabricator. Differential Revision: D64866426,This pull request was **exported** from Phabricator. Differential Revision: D64866426," merge f ""Unrelated Failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llama,[AOTI] add C shim for _weight_int8pack_mm,  CC([AOTI] Use `len(serialized_weights)` when calculating `consts_size`)  CC([AOTI] add C shim for QConvPointWise)  CC([AOTI] fix pointer_to_list)  CC([AOTI] add C shim for _weight_int8pack_mm) Fixes the error of running WOQINT8 LLaMA:   ,2024-10-23T08:43:42Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/138691, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: inductor / cuda12.1py3.10gcc9sm86 / test (inductor_cpp_wrapper, 1, 1, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,AOT eager accuracy regression in Segformer in 2.5.0 release," üêõ Describe the bug I noticed a qualitative regression in the inference quality when compiling a segformerb0 model from the `transformers` library. This regression was introduced in the 2.5.0 release  previously, the model output was qualitatively identical. The code below segments an example image. In **eager mode**, the segmentation looks like this: !eager_2 4 1 Previously, in 2.4.1, the AOTinductor output was identical: !aot_2 4 1 Now, in 2.5.0, in the AOTinductor output the predicted masks corresponding to sidewalk, car, tree, and lamppost have subtle artifacts: !aot_2 5 0 The code I used to create the visualization is below. Please let me know if a more minimal or quantitative example is required. Is this a genuine regression, or just a change in the default autotune config? Is there a way I can configure AOT Inductor to produce identical results to eager mode? Thank you!   Versions For 2.5.0 model:  For 2.4.1 model:  ",2024-10-22T22:32:43Z,high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,open,0,9,https://github.com/pytorch/pytorch/issues/138652,"A small repro will always be helpful. The first thing I will check is if torch.compile causes a similar regression. That will help us to triage if this is a torch.compile (Inductor) issue, or a AOT Inductor specific issue.","Oh, good idea. Here's a more minimal and quantitative example. Using this example, I think that the issue is in Inductor.  **Output for 2.4.1:**  **Output for 2.5.0**  To me, this suggests that: 1. For both versions of Pytorch, 2.4.1 and 2.5.0, the absolute and relative error was similar for `torch.compile` and `torch._export.aot_compile`. 2. There is a much larger error between eager mode and the Inductor product in 2.5.0 than in 2.4.1. In case it helps to reproduce it, I've been running these examples in Docker with a workflow similar to: ","I think this issue actually has to do with `nn.functional.interpolate`, which is used in the Decode Head of the segformer implementation in `transformers` here. I manually minified this bug a bit and determined that setting `align_corners=True` in this function in `transformers` reduces the error by several orders of magnitude. Maybe there has been some recent change to the CUDA implementation of `interpolate` causing a regression in this usage? My intuition says that the decode head should be using `align_corners=True` in the first place, so maybe it has to do with that.","After a bit more debugging, I have unfortunately been unable to reproduce this bug in a minified example. However, I'm fairly confident that it involves an interaction of `interpolate` with `align_corners=False` and strided memory. That is because the `SegformerDecodeHead::forward()` function does some ambitious `flatten` and `transpose` operations inside of a helper class, and I discovered that adding a call to `contiguous` after one of them fixes the bug. Here's my hack that I have found fixes the issue on my machine:  I was unable to reproduce this bug in a reduced file with only `torch` dependencies, but from my experience with strided memory operations in Inductor I really suspect there's an issue in Pytorch code here. Do you have any advice  for how I could narrow down the search for a minimal, reproducible example?", ,"This fails with me on aot_eager  note, not aot_eager with decomp partition.    gives "," does this fail with both aot_eager, and aot_eager_decomp_partition, or just one of them?",It fails with both for me. Slightly smaller repro: ,"  sorry, yea, meant to say, ""it fails on aot_eager, even without decompositions"""
transformer,[ONNX] Export Phi3.5 onnx graph multiple slice nodes missing starts or ends attribute," üêõ Describe the bug  In the exported graph, multiple slides nodes are missing starts and/or ends attributes !image !image Because starts and ends attributes are required. When running inference session  , I get below error onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Nonzero status code returned while running Slice node. Name:'/Slice' Status Message: slice.cc:195 FillVectorsFromInput Starts must be a 1D array  Versions PyTorch version: 2.4.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.24.0 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.164.11.cm2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.6.68 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 550.54.15 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_engines_precompiled.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_engines_runtime_compiled.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_graph.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_heuristic.so.9.4.0 /usr",2024-10-22T20:35:47Z,module: onnx triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/138637,"It is working for me with ``torch.onnx.export(model, inputs, ""repro.onnx"", dynamo=True)`` (nightly build). Which version of transformers are you using?","I used stable version pytorch and without dynamo=True. Let me try that For transformers, I used transformers             4.45.2",It looks like they are just dynamic from the outputs of other nodes?,Yes it's dynamic based on shape of a tensor.,Sounds good. The issue is resolved I guess?
transformer,ILP for auto FSDP wrapping,"  CC(ILP for auto FSDP wrapping) This PR presents a mixed integer linear programming (MILP) formulation that can be utilized to determine, under a memory budget, which modules to wrap as FSDP units. Similar to the auto SAC MILP introduced in https://github.com/pytorch/pytorch/pull/137908, the MILP uses information collected from MemTracker, Runtime Estimator, and SAC Estimator, introduced in these PRs: * https://github.com/pytorch/pytorch/pull/124688 * https://github.com/pytorch/pytorch/pull/134243 * https://github.com/pytorch/pytorch/pull/135208 Endtoend example and its sample output:   ",2024-10-22T20:31:46Z,oncall: distributed Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,1,13,https://github.com/pytorch/pytorch/issues/138635,thanks for moving fast on this, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: linuxbinarylibtorchprecxx11 / libtorchcpusharedwithdepsprecxx11build / build, linuxbinarylibtorchcxx11abi / libtorchcpusharedwithdepscxx11abibuild / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / workflowchecks / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""revert"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, Reverting PR 138635 failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch revert noedit e92208c919cf1a491f1f0ccdf4b14139308f68f1` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job ,"Clicked on the ""squash and merge"" button :( Reopened PR here  https://github.com/pytorch/pytorch/pull/140298"
yi,Add deprecated warning for lazyInitXXX API,  CC(Make Context to be Deviceagnostic Step by Step (4/N))  CC(Make Context to be Deviceagnostic Step by Step (3/N))  CC(Make Context to be Deviceagnostic Step by Step (2/N))  CC(Add deprecated warning for lazyInitXXX API) Detailed Descriptions: Involved APIs are as followed:  ``lazyInitCUDA``  ``lazyInitHIP``  ``lazyInitXPU``  ``lazyInitMTIA``  ``lazyInitPrivateUse1``,2024-10-18T11:11:50Z,open source Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/138323, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,`(*bias): last dimension must be contiguous` when running compiled SDPA on length 1 tensors," üêõ Describe the bug When using `torch.compile` on `torch.nn.functional.scaled_dot_product_attention` with length 1, a RuntimeError occurs during the backward pass:   The code is heavily modified from `BertSDPASelfAttention`, available from the transformers library: https://github.com/huggingface/transformers/blob/b54109c7466f6e680156fbd30fa929e2e222d730/src/transformers/models/bert/modeling_bert.pyL357L455 I suspect that the combination of `view`, `permute`, and `attention_mask` is to blame here. Modifying any one of these eliminates the bug. Tested on CUDA 12.4 w/ PyTorch `2.4.0`, `2.4.1`, `2.5.0`, and nightly `2.6.0.dev20241017+cu124`.  Versions PyTorch version: 2.6.0.dev20241017+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.5 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU Nvidia driver version: 565.90 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                  ",2024-10-18T07:40:36Z,triaged oncall: pt2 module: inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/138317,Error only happens with inductor backend.,"Hi guys, I could repro this on my local, is anyone work on this issue now, if there is not, I would like to take a look, thanks!",Nobody as far as I'm aware of  feel free to work on it and submit a PR!,"I tried to investigate on this issue, IIUC there is some unexpected behavior from `inductor/triton`. From the log I saw the `bias` tensor for the attention backward calculation looks like:  Its shape and strides is `size=[8, 1, 1, 1], stride=[8, 8, 8, 1]`, and then the compiler tried to reinterpret it as `size=[8, 8, 1, 1], stride=[8, 0, 0, 0])`. In the `attention_backward` method, it checks for the `""last stride""` for the `bias` tensor is `1`, which means the tensor is continuous. I though in such a case, if the `size` of last dim is `1`, we didn't have to check for the stride, so I drafted a PR to do it. I'm not clear about why `inductor/triton` would like to reinterpret it as `size=[8, 8, 1, 1], stride=[8, 0, 0, 0])`, and I didn't know is it an expected behavior, if this behavior didn't meet our expectation, I thought we could try to fix this issue from this side too, I'm happy to take a further study in these components. What do you think of this method? If there is anything I did wrong, please feel free to correct me, thank you!",cc'ing inductor people  ,". i am also going to make similar pr for inductor.  If you look at the generated output code we are doing a view on an input, which causes us to change stride. ","Thank you for the quick reply! Your PR fixed the issue well! And It is also a good entry point for me to study `inductor`. IIUC, there are still some redundant check in the kernel, I leaved a comment in my PR about it, how did you think of them, shall we remove them? PTAL, thanks!"
mistral,[ROCm] FlexAttention Sliding Window Attention Numeric Error," üêõ Describe the bug Hi AMD Team, I am currently trying to improve Mistral performance on MI300X by attempting to use FlexAttention Sliding Attention but unfortunately it is running into numeric errors. I have Verified that on H100 that this passes the correctness check. I have tested on both bfloat16 & float16 run into these numeric errors on MI300X but not on H100. cc:   H100 Correctness Passing & Showing Massive 35.61x Speedup  !image  MI300X 8.61x Speedup (But not passing correctness) !image  MI300X Error Msg   Reprod (Slightly Minified from https://github.com/pytorchlabs/attentiongym/blob/main/examples/benchmark.py)   Versions  Pytorch Labs Flex Attention Attention Gym Installation   Version  ",2024-10-18T02:45:06Z,module: rocm triaged oncall: pt2 module: higher order operators module: pt2-dispatcher module: flex attention,open,0,9,https://github.com/pytorch/pytorch/issues/138300,"This numeric error also affects ""document"" mask in the example as it is basically similar to SWA with max seq=32k and randomly generated window. https://github.com/pytorchlabs/attentiongym/blob/f7c93ded4abf9fd8d7dc9d8bcbf57e420b891e2d/examples/benchmark.pyL218 ",Hi  the issue has been assigned to developer. Thank you Oren.,Anticipating that this will require a triton update to resolve.  Will track flexattention support as part of  CC([v2.6] Inductor issue tracker for Triton release/3.2),"This is still reproducible on the beta triton 3.2 testing, taking a look to see where the issue lies. Whether on triton kernel or the sdpa call. Curiously with S<=1024 correctness is fine ","I had to tune down some parameters for sidebyside comparison at the minute as I'm comparing with V100 instead of H100 for the time being   I compared MI300 vs V100 for the 3 output tensors (atol 1e1 rtol 1e2): causal_fa2_out.pth:  Tensors match within tolerance. Closeness: 100.00% of elements are close within tolerance. flex_out.pth:   Tensors do not match within tolerance. Closeness: 90.01% of elements are close within tolerance.  Max Difference: 2.562500, Mean Difference: 0.049805 sdpa_mask_out.pth:   Tensors do not match within tolerance. Closeness: 51.56% of elements are close within tolerance.  Max Difference: nan, Mean Difference: nan Looks like the main accuracy issue here is coming from the nonecausal sdpa call (cc: ):  When I have H100 access I will rerun with original reported configuration","hi  , Thanks for confirming that these numeric errors are reproducible on your end too","Compared original configuration outputs H100 vs MI300, flex attention and fa2 are within 90% of same result as H100 but sdpa_mask_out results are off   Moreover there is some NaN issue in sdpa_mask_out  This reproducer seems to be catching some edge case for us, the flex and causal fa2 results seem okay but the causal sdpa path is behaving poorly.",Hi! Thanks for taking the time to reprod this on your end too. this is quite interesting. the atol is quite high on mi300x. ,"The issue with NaNs is related to boolean attention mask, 0.8b AOTriton version will resolve this when merged upstream. Aiming for 2.6 release."
transformer,Torch 2.5.0 vs 2.4.1: torch nested in BetterTransformer fastpath implementation, üêõ Describe the bug Thanks for the new torch release!  I am using torch nested in the bettertransfromer implementation. https://pytorch.org/blog/abettertransformerforfasttransformerencoderinference/ Here is the exact code where torch compile breaks: https://github.com/huggingface/optimum/blob/1e5014e70f17e0437c4b0a7f4e65e170688d8ab0/optimum/bettertransformer/models/encoder_models.pyL203 and https://github.com/huggingface/optimum/blob/1e5014e70f17e0437c4b0a7f4e65e170688d8ab0/optimum/bettertransformer/models/encoder_models.pyL146   Versions Torch=2.5.0 Torch=2.4.1 ,2024-10-17T22:23:40Z,needs reproduction triaged module: nestedtensor oncall: pt2,open,0,10,https://github.com/pytorch/pytorch/issues/138274,Maybe related:   CC(torch.compile does not support strided NestedTensor),Do you have a repro?,The reproduction (i pushed two docker images for you)  docker on infinity torch2.4 image with all deps:  torch2.5. poetry lock with just torch 2.4.1 to 2.5.0 bumped. all other dependencies are frozen.   Minimal repro Interactive ssh into container:  ,"   Reproduction above, can you update the triage status?"," I'm not the maintainer, so can't update the status...",Can you check if you still repro on nightly PyTorch? And can the repro only run one setting combination of bettertransform/compile? ,"It's still is the same error on the `torch2.6.0.dev20241112+cu121`. bettertransform + compile: breaks with error above Only bettertransform (torch.sdpa, mask, no nested): works Only compile: works No bettertransform, no compile: works","Hi , sorry for the delay on this. BetterTransformer currently uses an older version of nested tensor (AKA NST or ""nested strided tensor"") which is not compatible with torch.compile. This is in contrast with the newer NJT AKA ""nested jagged tensor"" that is supported within torch.compile. I suggest disabling the BetterTransformer fast path when utilizing torch.compile for now. This flag can be used to do so: `torch.backends.mha.set_fastpath_enabled(False)`. We could swap out use of NST for NJT within BetterTransformer when compiling; is this something that would be useful to you?"," btw is the difference between NST and NJT layouts explained in the docs anywhere? in https://pytorch.org/docs/stable/nested.html i found no detailed explanation besides notes that two layouts are supported it also suggests that `torch.layout` instance is expected, but the link https://pytorch.org/docs/stable/tensor_attributes.htmltorch.layout does not mention jagged",NJT is the normal layout you would expect. NST is a weird one where we maintain explicit size/stride metadata for each inner tensor (lol). NST predates NJT though
rag,bizzarre OOM: LLM model uses 2x memory during training AFTER load_state_dict() then otherwise (pytorch 2.2)," üêõ Describe the bug Hi, this seems to be related to CC(Memory surges when loading models), but not quite. Here are the symptoms I am finding. I am able to train a shrunk down LLM llama3.2 model perfectly fine on my data. Each forward pass for each layer of the LLM sucks up about 1gb of gpu memory. And this usage pattern is steady: See the (http://pytorch.org/memory_viz) snapshot below: !trainmemusebeforeresume So each of the 20 ""peaks"" in the above picture is for each iteration (20, before validation and save) Each of the roughly 1gb thick horizontal lines are for each layer of the LLM. There are six layers. The memory usage is roughly 1gb per layer, with the final layer (probably due to the last feedforward layer in llama before logits) sucking up 2gb. Before load_state_dict for the model and the optimizer (AdamW), the biggest chunk allocated on the GPU is 2gb for each iteration of the training loop. Now, here is the memory usage pattern AFTER resume (i.e. restart script and resume(), which calls torch.load_state_dict) !trainmemuseafterresume AFTER the load_state_dict for resume, it fails on the very first forward pass. You can see each of the layers sucking up 2x the  memory. It then fails after trying to allocate ~4gb on the GPU, then barfing. Here is the screencap of the tail end of the training run before I hit CtrlC:  Here is the relevant code for resume:  The `optimizer_to()` routine is stolen from CC(Memory surges when loading models)  Can a kind soul help me diagnose WHY the forward pass on the training suddenly starts to suck up 2x the GPU memory  as compared to BEFORE the resume?? Thank you  Versions ``` (rag) dell01e:x86_64Li",2024-10-17T20:49:21Z,triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/138253,"If I understand your problem correctly, the OOM happens once you've already loaded the model and are running the steps, NOT during the load, yes? The issue you linked is about the loading process, which has its fixes. Here, it looks like your activations that are getting saved are bigger now, assuming that I'm understanding correctly that each of the peaks in the first memory snapshot matches a fwdbwd pass. You may find more responders for this question on our community forum: https://discuss.pytorch.org/. I do have a potentially really dumb q: do you make sure that your model is the same dtype? e.g., the dtype didn't go from bf16 to fp32 after loading the state dict?",Doh    You are 100% right. I neglected `model.to(model.args.dtype)` Sorry for the noise.,"Still though, the fact that state_dict auto converted to float, either during save or load (the original model was all  bfloat16) is surprising... huh.", is the casting the expected behavior for load_state_dict?," thank you so very much for responding, btw! Kudos to you!  AFAIK, the docs on torch.save/torch.load don't really discuss any type conversion e.g. (https://pytorch.org/docs/2.3/generated/torch.save.htmltorchsave) If it is a feature, it's well disguised several layers below the public api. :/",`torch.save` should not be doing type conversion If you're doing `m.load_state_dict(state_dict)` I expect each `sd['param']` to be converted to the `m.param`'s dtype due to the inplace copy in the vanilla `load_state_dict`. Is that what's happening? https://github.com/pytorch/pytorch/blob/2e48788a359163db715abbac92e2d849b1b3247f/torch/nn/modules/module.pyL2441
rag,Update lint failure msg to encourage lintrunner -a locally,"This is only a minor patch that I hope will change how I talk to contributors when lint fails, so that I can tell them to read the logs about lintrunner. There have been too many times when I have had to click the ""approve all workflows"" just for lint to fail again cuz the developer is manually applying every fix and using CI to test. I understand there are times when lintrunner doesn't work, but I'd like most contributors to at least give it a swirl once to start.   CC(Update lint failure msg to encourage lintrunner a locally)",2024-10-17T17:27:39Z,Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/138232,Maybe edit the PR title though to look better in the commit history., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64mps / test (mps, 1, 1, macosm113) Details for Dev Infra team Raised by workflow job "," merge f ""lint passes and this code is very minor and only touches lintrunner.sh AND the current failure is from a mac runner losing connection"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llama,[inductor][cpu]float32 dynamic shape cpp wrapper performance regression in 2024-10-14 nightly release, üêõ Describe the bug float32 dynamic shape cpp wrapper               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       fastNLP_Bert       multiple       1       1.218653       0.04585516       0.055881528299479996       33.555859       1       1.570334       0.034004676000000005       0.053398698881784       54.845855       0.78       0.96       0.74       1.63                 torchbench       functorch_maml_omniglot       multiple       1       1.432401       0.000538722       0.0007716659315220001       8.513217       1       1.587037       0.000468542       0.0007435934900540001       14.643397       0.9       0.96       0.87       1.72                 torchbench       hf_Albert       multiple       1       1.283144       0.044370564       0.056933822973216       17.722765       1       1.629522       0.035830205       0.058386107312009994       24.0779       0.79       1.03       0.81       1.36                 torchbench       hf_Bert       multiple       1       1.246668       0.050736508       0.063251580955344       20.51889       1       1.484188       0.04130538       0.061304949331440005       25.851807       0.84       0.97       0.81       1.26                 torchbench       hf_Bert_large       multiple       1       1.284968       0.132621571       0.17041447484472802      ,2024-10-17T16:16:34Z,oncall: pt2 oncall: cpu inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/138227,w please help to take a look for the default turning on of `TORCHINDUCTOR_ABI_COMPATIBLE`.,I checked `levit_128` and the regression can be fixed by https://github.com/pytorch/pytorch/pull/137880 (landed).  could you help confirm if the performance of all the other models are also back in our latest weekly test?,"Yes, according to the latest weekly test, the issue is fixed."
rag,Dynamo x autograd.Function: leverage compiled autograd on graph break in backwards,"There are four quadrants (see table below); we're talking about quadrant III. When we are torch.compiling an autograd.Function, we trace the forward and backward into subgraphs. If the forward can be traced into a subgraph, but the backward cannot, we graph break on the forward. With compiled autograd in the loop, we don't actually need to graph break on the forward: we can put a black box into the backward graph and rely on compiled autograd to read it, like how we handle backward hooks today.  ¬†  IV) graph break in forward ",2024-10-17T14:27:44Z,triaged oncall: pt2 module: compiled autograd dynamo-autograd-function,open,0,0,https://github.com/pytorch/pytorch/issues/138215
llm,Compile error with -WERROR on clang-12, üêõ Describe the bug EDIT(albanD): removed llm example showing the final result works and reworded a little bit. When trying to compile the lattest torch version on my system with amd cpu. the compilation failed. In order for it to compile i had to modify a file which i specified below. The file i had to modify to get it to compile was . /home/myles/pytorch/test/cpp/api/CMakeLists.txt at line 59 replaced that text below.   Error logs   Minified repro _No response_  Versions  ,2024-10-17T10:48:06Z,triaged actionable,open,0,6,https://github.com/pytorch/pytorch/issues/138197,"Hi, could you reformat this issue a little bit so it is clear what the issue and desired solution would be? It is currently hard to parse."," Hello i reformatted the issue for you, and tried to clearly explain what the problem was and how i had to resolve it.",What is the meaning of  > assistant Otherwise it feels like a build failure with `clang12` if one attempts to compile with WERROR, yes there is a null pointer issue with the dataloader.cpp. in order to get to compile you have to modify the make file to ignore it.so I thought this would be annoying so letting you know where the error originated from,Perhaps it's time to update minimal clang version to clang14 or something...,Actionable to ensure we specify the minimum supported version of clang somewhere.
transformer,Torch Dynamo support for Flux T5 model," üêõ Describe the bug I'm using the script below to export the Flux T5 model to ONNX using torch.onnx.dynamo_export(). However, I run into an error due to missing support for `fused_layer_norm_cuda.PyCapsule.rms_forward_affine`. The script below can be used to reproduce the issue:  The error is pasted below:   Versions transformers               4.42.2 diffusers                  0.31.0.dev0 torch                      2.5.0a0+b465a5843b.nv24.9 (Nvidia NGC 24.09 PyTorch container)",2024-10-17T10:42:38Z,module: onnx triaged,open,0,2,https://github.com/pytorch/pytorch/issues/138196,"Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated report if there is an error. Thanks!","I was able to successfully export your model with ``torch.onnx.export(model, (dict(input_ids=inputs),), dynamo=True)``. You should use the nightly build."
transformer,Torch Dynamo support for Flux Transformer model," üêõ Describe the bug I'm using the script below to export the Flux Transformer model to ONNX using torch.onnx.dynamo_export(). However, I run into a TypeError relating to an attribute type.  The script below can be used to reproduce the issue:  The error is pasted below:   Versions transformers              4.42.2 torch                     2.6.0.dev20241016+cu124 diffusers                 0.31.0.dev0",2024-10-17T10:23:08Z,module: onnx triaged,open,0,14,https://github.com/pytorch/pytorch/issues/138195,"Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated report if there is an error. Thanks!", Is fake mode exposed through this integrated API? I don't see a flag for it. If not what's the recommended way to use it now?,It should. You may refer to https://pytorch.org/docs/main/onnx_dynamo.htmltorch.onnx.enable_fake_mode,"Thanks for the pointer  . As per v2.4 docs, ""A ONNXFakeContext object that must be passed to dynamo_export() through the ExportOptions.fake_context argument."" This statement is missing in v2.5 docs. I'm guessing this means that using the context manager with `torch.onnx.export` is sufficient and the ONNXFakeContext need not be passed as an argument during export. Hope my understanding is accurate",That's right!,Sorry there may be a change to this. FakeMode is by default enabled for tensor propergation. Could you explain what your main goal is when using fake mode?,The main goal is to reduce VRAM usage during ONNX export. How can I enable fake mode for this objective?,"Does the VRAM usage happen during model initialization, or model export? We are going to create a tutorial on how to reduce memory usage when the model is being initialized so that the weights are not loaded; the export process itself is already running on fake tensors and requires minimal memory space.","The Flux Transformer is 23GB but the peak usage during the export step is 46GB. I haven't captured the usage for model load and model export separately yet.  > the export process itself is already running on fake tensors and requires minimal memory space. Does this apply to the torchscriptbased exporter by default, too, or is there an arg to enable this for TSbased export? Looking forward to the docs for optimizing memory usage during model load as well. Please feel free to share snippets here for quicker turnaround ","Only the torch.onnx.export(, dynamo=True) option will have an optimized memory usage for now. There is no plan to enable it for the torch script based export (the models are traced differently).",Sounds good. Please share the usage for loading models without the weights even if the tutorial is not ready yet. We can try it and provide feedback,"> Sounds good. Please share the usage for loading models without the weights even if the tutorial is not ready yet. We can try it and provide feedback Here is a simply example, with latest torchnightly version, to help calling the latest dynamo enabled exporter to export your model. This latest exporter will leverage FakeTensorMode internally to reduce the memory usage.  After execute above code, you will get a file like ""dynamo_exporter_xxxx.pickle"" which will save the cuda memory uage during the export process. Launch this page and drag and drop this .pickle file on this web page, you will see the details of memory usage.","Thanks for the example z . Just to clarify, the model is still initialized with random weights. However, the weights aren't loaded from disk.  Is this what you meant in  CC(Torch Dynamo support for Flux Transformer model)issuecomment2427244659  ?  We still will have the weights in memory, but the randomly initialized ones?","This may not impact my original use case of Flux transformer as `low_cpu_mem_usage` is True by default during `model = FluxTransformer2DModel.from_pretrained(model_dir, subfolder=""transformer"", torch_dtype=torch.float16).to(device)` . This only loads the pretrained weights and does not initialize the weights"
yi,"pytorchÂØºÂá∫onnxÊñá‰ª∂Êó∂Êä•Èîô  torch.onnx.export(model,         (dummy_input, img_metas),         onnx_file,         export_params=True,         opset_version=12,  do_constant_folding=False,  input_names=['input'],     output_names=['output'],     dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # Âä®ÊÄÅËΩ¥)"," üêõ Describe the bug Traceback (most recent call last):   File ""/opt/RD003/smartvision_service/smartvision/tools/model_converters/mmdet2onnx.py"", line 69, in      mmdet2onnx('/opt/models/porn/models/detector/organ', '/opt/models/onnx')   File ""/opt/RD003/smartvision_service/smartvision/tools/model_converters/mmdet2onnx.py"", line 62, in mmdet2onnx     dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}   Âä®ÊÄÅËΩ¥   File ""/root/miniconda3/envs/porn_terror/lib/python3.7/sitepackages/torch/onnx/utils.py"", line 519, in export     export_modules_as_functions=export_modules_as_functions,   File ""/root/miniconda3/envs/porn_terror/lib/python3.7/sitepackages/torch/onnx/utils.py"", line 1539, in _export     dynamic_axes=dynamic_axes,   File ""/root/miniconda3/envs/porn_terror/lib/python3.7/sitepackages/torch/onnx/utils.py"", line 1123, in _model_to_graph     module=module,   File ""/root/miniconda3/envs/porn_terror/lib/python3.7/sitepackages/torch/onnx/utils.py"", line 582, in _optimize_graph     _C._jit_pass_lower_all_tuples(graph) RuntimeError: outerNode>outputs().size() == node>inputs().size() INTERNAL ASSERT FAILED at ""../torch/csrc/jit/passes/dead_code_elimination.cpp"":140, please report a bug to PyTorch.  Versions Collecting environment information... PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.17 Python version: 3.7.16 (default, Jan 17 2023, 22:20:44)  [GCC 11.2.0] (64bit runtime) Python platform: L",2024-10-17T02:02:13Z,module: onnx triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/138148,It is difficult to tell without knowing the model. Can you try with ``dynamo=True``?,"Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated report if there is an error. Thanks!"
transformer,DISABLED test_transformer_backend_inductor_fullgraph_True (__main__.TestFullyShardCompile),This test was disabled because it is failing in CI machine upgrade (recent examples). ,2024-10-17T01:25:37Z,oncall: distributed skipped,closed,0,2,https://github.com/pytorch/pytorch/issues/138147, seems need to guard this on the availability of bf16?,Closing because I am doing wholeclass skip in test file. See  CC(DISABLED __main__.TestFullyShardCompile)issuecomment2418441754
transformer,Error while exporting wav2vec2 speech emotion recognition, üêõ Describe the bug Code to reproduce the problem  Error Logs   Versions  ,2024-10-16T21:26:44Z,oncall: pt2 oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/138120,This is resolved by adding  `hidden_states = hidden_states.clone()` in  https://github.com/huggingface/transformers/blob/main/src/transformers/models/wav2vec2/modeling_wav2vec2.pyL1112,"Hi, this is likely caused by the following: currently in export, we do not allow mutation to the output of `aten.to` and dropout (or any of its alias). This is because the aliasing behavior of them depends on runtime (for example for aten.to, it's sometimes a copy, sometimes an alias).  You fix of adding a clone is the right thing to do now. We are working on relaxing this restriction for aten.to, and I can let you know after we landed the fix to relax this restriction."
transformer,Error while exporting BLIP, üêõ Describe the bug Steps: 1) `pip install timm==0.4.12 fairscale transformers` 2) `git clone https://github.com/salesforce/BLIP.git` Code for reproducing the problem  Error   Versions  ,2024-10-16T20:41:07Z,oncall: pt2 oncall: export,open,0,9,https://github.com/pytorch/pytorch/issues/138111,This is similar to  CC([export] Cannot mutate tensors with frozen storage) This is resolved by adding `text.input_ids = text.input_ids.clone()` in https://github.com/salesforce/BLIP/blob/main/models/blip.pyL111,"Hi , I seem to run into a number of similar problems that you have during your investigation, including this issue and  CC(Error while exporting depth estimation model `intel-isl/MiDaS`). We both seem to be trying to run nonstrict export on a large set of models. Is this part of an effort to make nonstrict torch.export more stable? Discover broken edgecases? Or is there another motivation? I'm trying to gauge how much effort my team should dedicate to patching over these kinds of issues on our side. I'm also wondering if there is some public issue that is tracking the status of these explorations and the maturity of torch.export.",Hi  Would love to understand what your team is working on and see how we can collaborate. Will reach out to you,"Sounds good, we would love to collaborate!","Hi, this is likely caused by the following: currently in export, we do not allow mutation to the output of `aten.to` and dropout (or any of its alias). This is because the aliasing behavior of them depends on runtime (for example for aten.to, it's sometimes a copy, sometimes an alias).  Your fix of adding a clone is the right thing to do now. We are working on relaxing this restriction for aten.to, and I can let you know after we landed the fix to relax this restriction."," Hi, any progress on this matter? It is preventing us from exporting many xnnpack quantized models :sweat_drops:  This PR is the one with more details about the problem:  CC(Cannot export a quantized ResNet-18 model)","> RuntimeError: cannot mutate tensors with frozen storage `While executing %add_ : [num_users=1] = call_functiontarget=torch.ops.aten.add_.Tensor, kwargs = {})` I have same message with an export from pytorch `ao`","  We're still working on it, but it might take some time before the constraint is relaxed. For now, you probably can bypass the error by adding a clone somewhere. This is similar to this issueissuecomment2417997285).", Sometimes it is hard to find the offending tensor that needs to be cloned. Can you take a look at https://github.com/pytorch/ao/issues/1381issuecomment2521475118 ?
rag,[BE]: Improve typing storage,Improves typing slightly in torch.Storage ,2024-10-16T16:26:01Z,module: typing open source better-engineering topic: not user facing suppress-bc-linter,open,0,0,https://github.com/pytorch/pytorch/issues/138084
chat,[Pytorch][ATEN] Enable FP8 concatenate,"Summary: Float8 is becoming and increasingly popular datatype now that it is well supported on GPUs. This  diff enables FP8 to work with `torch.cat`. This is pretty straight forward since memory operations dont vary based on the input dtype, but can be quite helpful for FP8 based models. Test Plan:  Differential Revision: D64443965 ",2024-10-16T01:57:12Z,module: cpu fb-exported Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/138046,This pull request was **exported** from Phabricator. Differential Revision: D64443965,This pull request was **exported** from Phabricator. Differential Revision: D64443965,"After these changes, you won't need the new macros, either.",This pull request was **exported** from Phabricator. Differential Revision: D64443965, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,Added zb1p and zb-v schedules,"  CC(Added zb1p and zbv schedules) Adds the ZB1P schedule in https://arxiv.org/pdf/2401.10241. The ZB2P schedule might not be zero bubble when pp_group_size > 4. Proof: !image Since ZB2P generates longer schedules for some cases, and we might need a collective for fault tolerance all reduce at the end of every iteration for llama 4, so holding off to implement a more fancier ZBV schedule for now unless it would be useful. Rebased/squashed from original PRs https://github.com/pytorch/pytorch/pull/130210, https://github.com/pytorch/pytorch/pull/132623 ",2024-10-15T22:58:34Z,oncall: distributed,open,0,1,https://github.com/pytorch/pytorch/issues/138034," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
llama,RMSNorm with bias kwarg option," üöÄ The feature, motivation and pitch Currently, the RMSNorm module supports the `elementwise_affine` kwarg but not the `bias` kwarg. Would it be easy to implement a mode where `elementwise_affine` is set True, but `bias` can be set False, like LayerNorm? This sort of configuration is fairly prevalent and is used by many modern LLM models including Llama(s), and Mamba(s).   Alternatives _No response_  Additional context _No response_ ",2024-10-15T22:14:08Z,module: nn triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/138025,"I actually realized just now that the current behavior of nn.RMSNorm is to not apply a bias, which actually fits the common usage. This is probably good to close, unless there is a popular usage where bias=True for RMSNorm (which I'm currently not aware)"
rag,Fix .to(cpu) for Storage,  CC(Fix .to(cpu) for Storage),2024-10-15T19:55:40Z,Merged ciflow/trunk release notes: python_frontend topic: bug fixes suppress-bc-linter,closed,0,7,https://github.com/pytorch/pytorch/issues/138011, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: linuxbinarylibtorchprecxx11 / libtorchcpusharedwithdepsprecxx11build / build Details for Dev Infra team Raised by workflow job ", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/mikaylagawarecki/273/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/138011`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,Enable the Group GEMM fusion through cpp template,"  CC([Inductor][CPP] Enable horizontal Transverse)  CC([Inductor][CPP] Enable Linear Silu Mul fusion when concat linear enabled)  CC(Enable the Group GEMM fusion through cpp template)  CC([Inductor][CPP] Extract common functions to be reused in other CPP Template) **Summary** In this PR, we introduce a new CPP template: `Group GEMM Template`. In the group GEMM:  All instances share the same dimensions (`m`, `n`, `k`) and the same leading dimensions (`lda`, `ldb`, `ldc`) for their `A`, `B`, and `C` matrices.  All instances have distinct or shared activations, have distinct weight, have unique bias or no bias, have distinct epilogues.  Current implementation assumes the outputs of all instances are reduced using pointwise epilogues. This behavior may be extended in the future if needed. With this template, we enable the `LinearSiluLinearMul` fusion, a common pattern in the MLP modules of LLMs. The fusion pattern is as follows:  **Test Plan**  ",2024-10-15T09:31:39Z,open source ciflow/trunk topic: not user facing module: inductor ciflow/inductor,open,0,1,https://github.com/pytorch/pytorch/issues/137975,"Hi  w, as discussed offline, we can abstract the implementation into Group GEMM Template to enable broader usage. Please kindly take another look at it."
yi,Bazel builds intermittently fail while trying to download mkl from anaconda %), üêõ Describe the bug See this log for example:  Which comes from those lines https://github.com/pytorch/pytorch/blob/aef3591998f4e46f7ade3914e6ad758619954672/WORKSPACEL183L191  Versions CI /pytorchdevinfra,2024-10-14T23:30:12Z,module: ci triaged,open,0,1,https://github.com/pytorch/pytorch/issues/137941,"I guess for the purposes of CI, I should mirror those to one of CI test buckets and patch WORKSPACE..."
transformer,ILP for Auto SAC (Selective Activation Checkpointing),"  CC(ILP for Auto SAC (Selective Activation Checkpointing)) This PR presents a mixed integer linear programming (MILP) formulation that can be utilized to determine, under a memory budget, which modules to apply activation checkpointing (AC) and the amount of activation memory that should be discarded for each module. The MILP uses information collected from MemTracker, Runtime Estimator, and SAC Estimator, introduced in these PRs: * https://github.com/pytorch/pytorch/pull/124688 * https://github.com/pytorch/pytorch/pull/134243 * https://github.com/pytorch/pytorch/pull/135208 Endtoend example and its sample output:   ",2024-10-14T17:03:54Z,oncall: distributed Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,3,8,https://github.com/pytorch/pytorch/issues/137908, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.12clang10 / test (default, 4, 4, linux.4xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalrocm6.2py3.10 / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm, hipBLASLt causes vLLM to crash only on -tp 8 with on an MI300 node (gfx942)," üêõ Describe the bug I am running with a very recent nightly of PyTorch installed as this is require by a source build of the latest vLLM:  I am running this on a new 8 x MI300X node (Ubuntu 22.04.5 LTS w/ ROCm 6.2.2) in a clean python3.11 mamba env. When running this (after quite a bit of debugging) I've found a very curious error. When I run with `tp 1`, `tp 2`, or `tp 4`, vLLM runs without errors, however when I run with `tp 8` then I will get 14 of the threads giving this error and killing vLLM:  The `TensileLibrary_lazy_gfx942.dat` file exists! (and I also have this problem is I symlink the system `/opt/rocm6.2.2/lib/hipblaslt`) which makes me think this must be some sort of filereading race condition that affects 8, but not 4 or less threads? My current workaround is to use `TORCH_BLAS_PREFER_HIPBLASLT=0` when running with `tp 8` and interestingly, at least for tp 14, it appears that this runs faster than using hipBLASLt for vLLM! If this is more of a vLLM specific issue I can file a bug with their repo as well, but this seemed maybe more of a how PyTorch is loading hipBLASLt thing?  Versions ",2024-10-10T14:31:37Z,triaged rocm,closed,0,6,https://github.com/pytorch/pytorch/issues/137695," Hi, we have managed to identify the issue. We have resolved this by increasing the open file handler limit. `ulimit n 131072`. Try listing out the handler config. using `ulimit`. "," thanks for following up! Confirmed working w/ the change BTW, I think you had a typo. Should be `ulimt n 131072`  `m` sets the ""max memory size"" not file handles. BTW, just for those interested, with 0.6.4.dev8+ge9d517f2:  so about +3% (mostly from output token speed) w/ hipblaslt in this case. I'll leave this issue open so whomever is triaging can do the appropriate thing? The easiest things seems to be to just add info in the docs. But I guess you can add a check for file handles somewhere:  I think to actually adjust the file handle limit you'd need root access..."," Interesting, cz in my case, increaseing the open file helps. `ulimit n` so I think increasing both (`n` and `m`) is a better bet. We have document all the steps to setup of vLLM on AMD environment here https://embeddedllm.com/blog/howtobuildvllmonmi300xfromsource . It includes installing CK Flash Attention which in many cases is better than the triton implementation of flash attention as triton implementation flash attention prefers input size to be of power of two.","Just in case there is confusion, the open file limit is what worked for me as well. In your original comment you point this out: ``` > max memory size             (kbytes, m) unlimited > open files                          (n) 1024       We have resolved this by increasing the open file handler limit. `ulimit m 131072`. However, you can see the `m` flag sets the ""max memory size""  `n` is the flag to set ""open files"" is all I'm saying (typo). I am currently working on the getting upstream FA to work in a separate issue: https://github.com/DaoAILab/flashattention/issues/1269issuecomment2407843170 I just got back to my desk and will be doing some followup but even the recommended Docker did not work. Moving to doing everything in ipynb for more legible output/easier replicability but I'll give your just published docs a look as well soon and let you know how it turns out. If I run into issues, I'll file a new one in the vLLM repo for that.",Thank you. I understood your sentence now. I did make a typo. As you have pointed out it should be `ulimit n` instead of `ulimit m`. Thank you so much. üòÑ ,"thank you  for the workaround, and I am closing this issue now."
llama,[benchmark] mimic new copy-out kernels with out_views,"  CC([benchmark] mimic new copyout kernels with out_views)  CC([FSDP2] Added `shard_placement_fn` arg)  CC([FSDP2] Fixed incorrect tensor meta after `.to(dtype)`) cmd: `pytest s test/distributed/_composable/fsdp/test_fully_shard_training.py k test_train_parity_shard_placement_fn_shard_largest_dim` test on llama: `CONFIG_FILE=""./train_configs/llama3_8b.toml"" ./run_llama_train.sh experimental.fsdp_sharding_on_largest_dim` for dim1, use out_views trick to avoid extra gpu copies, at the cost of cpu overhead with many torch.narrow gpu copy time are indeed similar * 1st picture is out_views trick * 2nd picture is dim0 sharding   cpu cost is crazy (6144 `aten.narrow`) that the workload becomes cpu bound. looks like we have to rely on data pointers manipulation at cpp layer   ",2024-10-10T05:50:59Z,oncall: distributed release notes: distributed (fsdp2),open,0,1,https://github.com/pytorch/pytorch/issues/137683,confirmed the gpu copy time. next step is to implement a new kernel. converting this PR to draft
transformer,Fixed issue with nn.Transformer().generate_square_subsequent_mask(),Fixed issue where nn.Transformer().generate_square_subsequent_mask() doesn't respect set_default_device() and set_default_dtype(). Fixes CC(`nn.Transformer().generate_square_subsequent_mask()` doesn't care `set_default_device()` and `set_default_dtype()`),2024-10-09T22:00:00Z,open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/137654," label ""topic: not user facing""",Apologies if I have done anything wrong in this pull request.  I am new to open source and am happy to make changes as needed :), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Fixes issue where nn.Transformer().generate_square_subsequent_mask() doesn't respect set_default_device() and set_default_dtype(),Fixes CC(`nn.Transformer().generate_square_subsequent_mask()` doesn't care `set_default_device()` and `set_default_dtype()`),2024-10-09T20:50:53Z,open source topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/137640,:white_check_mark:login: FinlaySanders / (b37d1bcd92fabb72b8c34c92a7f41926a46d2c17)The committers listed above are authorized under a signed CLA.," label ""topic: not user facing"""
yi,make it clearer (in docs) one can double decorate with torch.library.impl_* APIs,"Fixes CC(make it clearer (in docs) one can double decorate with torch.library.impl_* APIs). Fix originally attempt by  with PR: https://github.com/pytorch/pytorch/pull/121469. PR was almost ready to merge, but then went stale (over 6 months old). This PR implements original fix with refactoring for clarity.  CC: ",2024-10-09T17:21:12Z,triaged open source ciflow/trunk topic: not user facing,open,1,6,https://github.com/pytorch/pytorch/issues/137608," label ""topic: not user facing""",    review request. Thanks!! , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3clang12executorch / test (executorch, 1, 1, linux.2xlarge)  pull / linuxfocalcuda11.8py3.10gcc9 / test (distributed, 3, 3, linux.g4dn.12xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," I see that some of the tests that ran have failed, after looking at the results I am unable to determine the reason for failure, but the failures do not look related to the change. Any suggestions how to resolve this?"
yi,make it clearer (in docs) one can double decorate with torch.library.impl_* APIs,Fixes CC(make it clearer (in docs) one can double decorate with torch.library.impl_* APIs). Adds second example for impl function to demonstrate double decorate. ,2024-10-09T15:39:17Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/137594,The committers listed above are authorized under a signed CLA.:white_check_mark: login: jacksontsang578 / name: Jackson Tsang  (8c2e06978d78ee1a203c6c35dc61527495844669)," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
yi,Expose underlying cudastream_t for torch.cuda.Stream," üöÄ The feature, motivation and pitch I want to add PyTorch support to GStreamer project to support video analytics. GStreamer is written in C and Rust with Python bindings. The only way to pass a  Cuda stream between GStreamer Python elements is to serialize the underlying handle to integer in one element and then recreate the stream in the next element. If multiple elements can share the same cuda stream, we can get a big performance boost and increase PyTorch adoption in GStreamer community. Being able to pass the underlying stream handle into PyTorch, and also to extract the underlying stream handle from `torch.cuda.Stream` would improve the interop capabilities when integrating with C/C++/Rust.  I am interested in creating the patch if it would be something acceptable to upstream. Thanks!!  Alternatives I haven't found any other way of doing this.  Additional context _No response_ ",2024-10-09T11:55:02Z,module: docs module: cuda triaged enhancement actionable module: python frontend,open,0,13,https://github.com/pytorch/pytorch/issues/137581,"I thought that's already the case (see torch._C._cuda_getCurrentRawStream) https://github.com/pytorch/pytorch/blob/de4c2a3b4e89d96334dc678d1c3f2ae51a6630a0/torch/csrc/cuda/Module.cppL208L218 But if you need something else, please do not hesitate to propose a PR and short documentation that outlines expected lifetime of the stream if it is returned via such API (i.e. should it be referncecounted somehow?)"," thank you for quick reply! This is great news! So this would be used as `torch.current_stream_raw(device)`   ?  I also have a use case where another element, just using Cuda and not Pytorch, is using a cuda stream and wants to share this with the Python elements. Is it possible to pass the raw stream **into** PyTorch and have it create a stream object for use by the model ?",We already expose that:  Be careful to convert that int into a proper int64 type on your way back into c++.,Excellent! How about the other way  passing an int stream handle back into PyTorch ?,I believe the constructor has a `stream_id` argument where you can pass that value in. Haven't double checked though," thank you  yes there is a `stream_id` arg but according to docs this is an index into a stream pool, not a low level handle to the stream.",We have a rather fancy way to handle these ids as they can contain a couple things. But an int of a valid pointer to a stream_t is a valid id: https://github.com/pytorch/pytorch/blob/c73d2634b9164d4c7085cd2c420d963347efca74/c10/cuda/CUDAStream.cppL69L96,"Fantastic! I will play around with setting/retrieving stream from `cudaStream_t` pointer and come back if I have any questions, otherwise I will close this. Thanks again for your help.","btw I feel like this is not well documented. If these end up working well for you, it would be great if you could drop a small PR that extends the Stream or CudaStream class with details on these ids and how to successfully use them to get cudaStream_t to other applications.","absolutely, I will","  I've done some experimenting with the code.  The code below works for device `cuda:0`  When I print out the `stream_id` after creating the stream, it is equal to 3. And if `stream_id` arg is set to anything other than 0, for example 37, I get the following error:  I think a few examples for native or external streams would be helpful, also a definition of external and native streams.",I finally got this working with a little guess work  But it looks like it still uses the default stream  I don't synchronize explicitly but the model still executes.,"Hi   , any ideas on why default stream is used in this case ?"
rag,[SymmetricMemory] set the storage_offset of tensors returned by get_buffer() to 0,"  CC([SymmetricMemory] set the storage_offset of tensors returned by get_buffer() to 0)  CC([SymmetricMemory] fix a bug where numel calculation overflows when the tensor size is large) It seems that there's a bug in `TensorMaker`  it would treat `storage_offset` as bytes when calculating the storage size, but as numel when setting the tensor `storage_offset`. This seems to be causing tensors returned by get_buffer() with non0 offset to report wrong storage size. Will look into the `TensorMaker` issue further. But for `get_buffer()`, it seems more natural to just incorporate the offset into the data pointer. ",2024-10-09T07:36:11Z,oncall: distributed Merged ciflow/trunk release notes: distributed (c10d),closed,0,3,https://github.com/pytorch/pytorch/issues/137569, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"This PR ( CC([SymmetricMemory] set the storage_offset of tensors returned by get_buffer() to 0)) was merged in ea83c78174e0feb7b9c8b9dfe3dcd4586830f8fa but it is still open, likely due to a Github bug, so mergebot is closing it manually.  If you think this is a mistake, please feel free to reopen and contact Dev Infra."
llama,export_for_training regression on Llama3_2_vision text decoder," üêõ Describe the bug When exporting TorchTune's Llama3_2_vision text decoder, I get this error:   However in the state_dict, I see this: `layers.3.sa_norm.scale`. This stems from switching from `capture_pre_autograd_graph` to `export_for_training` in this commit. Repro: export commands in this pr.  Versions PyTorch version: 2.6.0.dev20241007+cpu Is debug build: False          CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: CentOS Stream 9 (x86_64)      GCC version: (GCC) 11.5.0 20240719 (Red Hat 11.5.02) Clang version: Could not collect CMake version: version 3.30.2 Libc version: glibc2.34      Python version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64bit runtime) Python platform: Linux6.4.30_fbk12_hardened_2624_g7d95a0297d81x86_64withglibc2.34 Is CUDA available: False         CUDA runtime version: Could not collect  CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA PG509210 Nvidia driver version: 550.90.07           cuDNN version: Could not collect HIP runtime version: N/A                                                                                                                 MIOpen runtime version: N/A                                                                                                              Is XNNPACK available: True                                                                                                               CPU:                                                                                                                                     Architecture:                       x86_64                                       ",2024-10-08T23:12:10Z,oncall: pt2 export-triaged oncall: export,closed,0,6,https://github.com/pytorch/pytorch/issues/137540,Debugged a bit with  but we could not figure out,"we've found that the root cause is Llama3_2_vision has a state_dict_hook, which cause the state_dict to be different from the module hierarchy. working on a fix.","We have a potential solution, which modifies the verifier to match against the state_dict without the hook, so you won‚Äôt see the error. One caveat is then for any model with state_dict hook (like this one), you won‚Äôt be able to  interchange between export_program.state_dict() & mod.state_dict().   export_program.state_dict() still works with it self, but you can‚Äôt load a model‚Äôs state_dict to exported_program, or vice versa. ","May present some issues with FL, if we want to load the FL server's state_dict into the client's exported state_dict. But we could also work around this by reexporting. This workstream has not yet been solidified yet to my knowledge. ?","> for any model with state_dict hook (like this one), you won‚Äôt be able to interchange between export_program.state_dict() & mod.state_dict().  update:  We got a PR here:  https://github.com/pytorch/pytorch/pull/137609 One can still do weight swapping, but the state_dict to be swapped needs to be obtained under `torch._export.utils._disabled_load_state_dict_hooks(M)` context manager when calling `state_dict()`  if the model has state_dict_hooks.",Thanks for the fix + workaround üôèüèª 
rag,Support multiple ragged dims for NJT,"Currently, NJT has a hard restriction that only a single dim can be ragged. It's useful to generalize this, allowing any nonbatch dim to be ragged, for certain use cases: * Images of ragged width / height (e.g. as in SAM) * The math fallback of SDPA computes QK^T, resulting in an intermediate with two ragged dims Following from CC(Allow any single nonbatch dimension to be ragged for NJT), this issue proposes that we relax this restriction and allow for multiple ragged nonbatch dims per NJT, with each defined as ragged with respect to the batch dim. For example: an NJT of shape `(B, J1*, J2*, D)` with ragged `J1*` and `J2*` has both `J1*` and `J2*` ragged wrt the batch dim, which can be described by two `offsets` tensors of shape `B + 1`. This allows for use of preexisting kernels written to handle data in this format (e.g. the jagged  padded dense conversion kernels from fbgemm). There are some open things to resolve: * Should we allow mixed `offsets` / `lengths` metadata? e.g. `offsets` for one ragged dim and `lengths` for another * Should we introduce new layouts for such NJTs? e.g. `torch.jagged2d`, `torch.jagged3d`, etc. ",2024-10-08T19:27:44Z,triaged module: nestedtensor,open,1,0,https://github.com/pytorch/pytorch/issues/137514
yi,Avoid generating guards for  conjunction expression involving dynamic shapes if the whole expression yields false regardless of guards satisfaction..,"if we have something like: if(x.size() ==b.size() and false):     do ... and X.size() is a SymInt, then we do add guards even though we do not take the branch.  TORCH_LOGS=""recompiles, guards""  python example.py   How did I encounter this? I was doing something in functionalization that is similar to the above and it was coausing recompilations (guards not added due to user code), instead of False I had other conditions that do not have symbols and turn out false. for now I reordered the check but it would be nice to have better solutions so other folks dont hit the issue.   ",2024-10-08T19:23:35Z,triaged oncall: pt2 module: dynamic shapes,closed,0,3,https://github.com/pytorch/pytorch/issues/137513,"This is a bit difficult. Let me modify your sample slightly:  Now, I have a situation where the value of the size in question doesn't matter for whether or not we take the branch, but it DOES affect the short circuiting behavior of the ""and"", and short circuiting has an observable effect. It is hard for me to imagine an algorithm that can drop the guard in your example, but NOT drop the guard in this case.","seems like at least for the auto_functionalize use case that I have I can use this or something similar not exactly this.   while this does not exactly do what I have above in the example, it is sufficient for the use case here , statically_known should be good  https://github.com/pytorch/pytorch/blob/main/torch/_higher_order_ops/auto_functionalize.pyL93","yes, if we're talking internal tests in functorch tracing you have more latitude. My statement here is purely about user facing python code"
rag,Allow any single non-batch dimension to be ragged for NJT,"Currently, NJT is restricted to representing shapes of the form `(B, J*, D_0, ..., D_N)` where only `J*` is allowed to be ragged. For this shape, it is always the case that `J*` is ragged wrt the batch dim `B`, and there is no ambiguity about this. On the other hand, consider the shape `(B, D, J*, E_0, ..., E_N)`. Is `J*` ragged wrt `B` only or both `B` and `D`, for a different ragged value per `(B, D)`? It is ambiguous from this information alone. As an exception to the restriction above, note that NJT today provides barebones `transpose()` support on the ragged dimension to fit with SDPA's API, where inputs are commonly of shape `(B, num_heads, seq*, embedding_dim)` and `seq*` is ragged. For this case, we know `seq*` should be ragged with respect to `B`. So this establishes a precedent for the above example where `J*` is ragged wrt `B` only. Q: Is it possible to expand the supported set of the shapes for the jagged dimension in an incremental way? i.e. 1. Allow for a single ragged dim to be wrt `B` only and appear anywhere in the shape, continuing the precedent set by our minimal `transpose()` support on the ragged dim 2. Later on, allow for the single ragged dim to be wrt all previous dims I think this sort of incremental support is possible, with the ambiguity being resolved by considering the *cardinality* of the metadata associated with the nested int. For example: * `(B, D, j0)` with `j0` wrt `B` only: associated offsets are of shape B + 1, associated lengths are of shape B * `(B, D, j0)` with `j0` wrt `B` and `D`: associated offsets are of shape B\*D + 1, associated lengths are of shape B\*D That is, we can maintain some metadata",2024-10-08T19:15:42Z,triaged module: nestedtensor,closed,0,0,https://github.com/pytorch/pytorch/issues/137512
chat,Faster Faster BatchSampler,"Builds upon CC(faster batch sampler). Benchmarking code is the same as in CC(Faster BatchSampler). AMD Ryzen Threadripper PRO 3995WX:  When `drop_last == True`, it uses `zip` to speed things up. When `drop_last == False`, it uses `itertools` to speed things up. `itertools` was the fastest way I could find that deals with the last batch if it is smaller than `batch_size`. I have a pure python method too, but it is slower when `batch_size` is 4 or 8, so I have committed the `itertools` version for now.  Happy to chat further about this change :) I understand you may not want to introduce the `itertools` package into sampler.py.",2024-10-07T10:08:11Z,triaged open source Merged ciflow/trunk release notes: dataloader,closed,1,11,https://github.com/pytorch/pytorch/issues/137423,"Thanks for the PR, super curious how the results stand for even smaller batch sizes. (We probably should have a uniform script to benchmark and check correctness, I can maybe look into that!).","> Thanks for the PR, super curious how the results stand for even smaller batch sizes. >  > (We probably should have a uniform script to benchmark and check correctness, I can maybe look into that!). I have some (quite janky) batch verification code I used with the aforementioned benchmarking code. I've put it in a gist here. It's janky because to test the `drop_last == False` functionality for the itertools change, you need to manually set `DATA_SIZE` to be smaller than `VERIFICATION_BATCHES * BATCH_SIZE` . I didn't have much time to spend on it sorry üòÖ There's an example output as a comment to the gist :) ",Fixed for lintrunner requirements.  New benchmarks: AMD Ryzen Threadripper PRO 3995WX:  Apple M1 Max:  Now it's slower for `batch_size == 4` Silly linting :'(,> Fixed for lintrunner requirements. >  > New benchmarks: AMD Ryzen Threadripper PRO 3995WX: >  >  >  > Apple M1 Max: >  >  >  > Now it's slower for `batch_size == 4` Silly linting :'( I mean you can ignore the linting with special comments if it affects speed...,I buy the itertools islice is faster for sure. It's a shame we can't use itertools.batched both because of our minimum Python version and because it returns tuple instead of lists lol.,Minus those nits: it looks good to me.,"Annoyingly, for the `itertools` part, if you don't convert to a list before yielding, it's seemingly really slow. The zip part is slower now, I just need a bit of time to fix that then I think I'm good.",Ok. I think I've tidied it up now.  Here's the benchmarks. Seems to pass the lintrunner fine :)  AMD Ryzen Threadripper PRO 3995WX:  Apple M1 Max: , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,Hmmm. A bunch of the trunk tests failed after merging (same with some other PRs unrelated to this one). I think it's to do with changes in this PR: CC(Make Context to be Deviceagnostic Step by Step (1/N)) ?   
llama,cuDNN dBias error starting on 10/2 nightly," üêõ Describe the bug We are seeing the following error during backward on torchtune's Llama 3.2 Vision models starting with the 10/2 nightly:  A minimal repro is given by   This script passes on 10/1 nightlies and fails on 10/2 nightlies. Looks like CC([BE][Ez]: Update cudnn_frontend submodule to v1.7.0) updated the cuDNN frontend to a version that added this check (ref). It was pointed out by  that the dimension `s_q` and `s_kv` is referring to is the sequence length dimension. In general I'm not sure why we would constrain that to be a multiple of 64.  Versions Collecting environment information... PyTorch version: 2.6.0.dev20241002+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: CentOS Stream 9 (x86_64) GCC version: (GCC) 11.5.0 20240719 (Red Hat 11.5.02) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.34 Python version: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.12.00_fbk7_zion_6511_gd766966f605ax86_64withglibc2.34 Is CUDA available: True CUDA runtime version: 12.0.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA PG509210 GPU 1: NVIDIA PG509210 GPU 2: NVIDIA PG509210 GPU 3: NVIDIA PG509210 GPU 4: NVIDIA PG509210 GPU 5: NVIDIA PG509210 GPU 6: NVIDIA PG509210 GPU 7: NVIDIA PG509210 Nvidia driver version: 525.105.17 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bit",2024-10-04T16:46:19Z,high priority module: cudnn triaged module: regression module: sdpa,open,2,10,https://github.com/pytorch/pytorch/issues/137347,This could also be related to some better error checking (or flawed error checking) on CUDNN_frontend and we may need to file a ticket upstream there.  Any idea what needs to change with cudnn_frontend?,I'll open a PR to bypass this in cuDNN. The check itself might still be valid as it looks like this code snippet isn't actually checking for the correctness of dBias.,Opened CC([cuDNN][SDPA] Fix cudnn frontend dBias check) as a workaround after confirming with cuDNN that the check is incorrect for our usecase. A `cudnn_frontend` release coming in the near future should contain the proper fix on their end.,"Is this change part of 2.5? If some, must it be cherrypicked? Perhaps it's better to fall back to older codepath if dBias is passed.", it should not be part of 2.5.0 as IIUC the 1.7.0 upgrade was not part of the branch cut, Can you check if the problem happening with release/2.5 branch?,Just tried on latest 2.5 candidate and the original error is not being raised,>  Can you check if the problem happening with release/2.5 branch? I also checked this last Friday and the `dBias` error was not present,"Thanks, removing the milestone then.",Next steps: 1. No action needed for 2.5 release on this issue 2.  Revert https://github.com/pytorch/pytorch/pull/136920 3. Jump 1.7.0 and go straight more recent version when released
yi,`torch.equal` yields `False` if tensor contains `NaN` values.," üêõ Describe the bug According to the documentation > True if two tensors have the same size and elements, False otherwise. From this description, it would be expected that `torch.equal(x, x)` **always** returns `True`, however:   Versions  Collecting environment information... PyTorch version: 2.4.1+cu121 Is debug build:  CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.5 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.30.4 Libc version: glibc2.35 Python version: 3.12.4 (main, Jun 17 2024, 10:48:36) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.8.045genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.6.77 GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 560.35.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.7 HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypy==1.11.2 [pip3] mypyextensions==1.0.0 [pip3] numpy==2.1.1 [pip3] torch==2.4.1 [pip3] torchinfo==1.8.0 [conda] Could not collect  ",2024-10-04T09:23:12Z,module: docs triaged actionable,closed,0,3,https://github.com/pytorch/pytorch/issues/137337,"There are multiple options: 1. Just update the documentation to reflect current runtime behavior 2. Change the runtime behavior to compare `NaN`s as equal. 3. Add an optional `equal_nan` flag, as in `torch.allclose`, `torch.testing.assert_close` or `numpy.array_equal`.",This is expected and matches the standard python behaviour expectations for `nan` ,"This is an expected behavior, as NaN are not equal to each other... But our docs could indeed say that..."
transformer,RuntimeError:  Expression of type - cannot be used in a type expression: __torch__.transformers_modules.code-5p-110m-embedding.modeling_codet5p_embedding.___torch_mangle_1368.CodeT5pEmbeddingModel ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE," üêõ Describe the bug I use the model: https://huggingface.co/Salesforce/codet5p110membedding and with `torch.jit.load`, it makes error:  the error message is:   Versions Collecting environment information... PyTorch version: 2.1.2 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 3.28.4 Libc version: glibc2.17 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.10.112005.ali5000.al8.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA L20 Nvidia driver version: 535.161.08 cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.9.0 /usr/lib64/libcudnn_adv_infer.so.8.9.0 /usr/lib64/libcudnn_adv_train.so.8.9.0 /usr/lib64/libcudnn_cnn_infer.so.8.9.0 /usr/lib64/libcudnn_cnn_train.so.8.9.0 /usr/lib64/libcudnn_ops_infer.so.8.9.0 /usr/lib64/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] torch==2.1.2 [pip3] torchdct==0.1.6 [pip3] torchaudio==2.1.2 [pip3] torchvision==0.16.2 [pip3] triton==2.0.0 [conda] blas                      1.0                         mkl   [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] libjpegturbo             2.0.0                h9bf148f_0    pytorch [conda] mkl                       2023.1.0         h213fc3f_46344   [c",2024-10-03T04:58:33Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/137252
gpt,[PT2][Inductor][Reliablity] Fix PT2 latency regression on nanogpt,Differential Revision: D63806598 ,2024-10-03T02:04:45Z,fb-exported Stale module: inductor ciflow/inductor release notes: inductor,open,0,4,https://github.com/pytorch/pytorch/issues/137248,This pull request was **exported** from Phabricator. Differential Revision: D63806598,This pull request was **exported** from Phabricator. Differential Revision: D63806598,This pull request was **exported** from Phabricator. Differential Revision: D63806598,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
finetuning,[DTensor] Register replication strategy for a few upsampling interpolate ops,  CC([DTensor] Register replication strategy for a few upsampling interpolate ops) To unblock Llama 3.2 vision's use case to resize positional embeddings for finetuning. Context in workplace post. ,2024-10-02T18:42:50Z,oncall: distributed Merged ciflow/trunk topic: not user facing module: dtensor,closed,0,5,https://github.com/pytorch/pytorch/issues/137201,> Replicate strategy should be fine for now. Ye. I look at the ops and it doesn't seem we can improve sharding strategy for these ones so it would always require redistributing to replicate. ,How would the replicated DTensor get converted back to sharded in the state dict load flow?,"> How would the replicated DTensor get converted back to sharded in the state dict load flow? Ah. Thanks for raising it. We won't be able to define the layout for the output, since it is determined by the next op.  In order to shard it back, users would have to do a . This way there is no communication compared with using `distribute_tensor()`. ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Setting `nn.TransformerEncoder()` and `nn.TransformerDecoder()` to `nn.Transformer()` doesn't get any errors with `nn.Transformer()`'s arguments having wrong type values, üêõ Describe the bug Setting nn.TransformerEncoder() and nn.TransformerDecoder() to nn.Transformer() doesn't get any errors with `nn.Transformer()`'s arguments having wrong type values as shown below:   Versions  ,2024-10-02T17:28:55Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/137192
transformer,`nn.Transformer().generate_square_subsequent_mask()` doesn't care `set_default_device()` and `set_default_dtype()`," üêõ Describe the bug nn.Transformer().generate_square_subsequent_mask() doesn't care set_default_device() and set_default_dtype(), getting `cpu` and `float32` instead of `cuda` and `float64` as shown below. *I know I can set `device` and `dtype` manually to `nn.Transformer().generate_square_subsequent_mask()`:  In addition, torch.tensor() cares `set_default_device()` and `set_default_dtype()` as shown below:   Versions  ",2024-10-02T15:58:24Z,high priority triage review good first issue module: correctness (silent),closed,0,4,https://github.com/pytorch/pytorch/issues/137186,Marking hipri tentatively for silent incorrectness,Shouldn't be too hard to fix probably,I'd love to contribute.  Will have a look :),Have found the problem.  Will submit a pull request shortly.
gpt,[CI] Remove nanogpt from perf smoke test,  CC([AOTI] Turn on the ABIcompatible mode as default)  CC([AOTI] Add standalone version of TORCH_CHECK)  CC([AOTI] Add C shim for MKLDNN _linear_pointwise)  CC([CI] Remove nanogpt from perf smoke test) Summary: nanogpt's performance is not stable. Remove it from the perf smoke test. We may want to use another test instead.,2024-10-02T14:21:28Z,Merged topic: not user facing ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/137176," merge f ""inductor_torchbench_smoketest_perf has passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","was it unstable, or was there just a regression ? https://github.com/pytorch/pytorch/pull/137248","> was it unstable, or was there just a regression ? CC([PT2][Inductor][Reliablity] Fix PT2 latency regression on nanogpt) It was unstable, but only seems like a recent issue, !Screenshot 20241003 at 9 37 30‚ÄØPM"
gpt,[CI] Remove nanogpt from perf smoke test, * (to be filled) Summary: nanogpt's performance is not stable. Remove it from the perf smoke test. We may want to use another test instead.,2024-10-02T14:20:48Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/137175," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
transformer,`batch_first` argument of `nn.Transformer()` should be `True` by default because it's not convenient to get a warning with the default settings," üêõ Describe the bug nn.Transformer() with the default settings gets the warning as shown below because `batch_first` argument is `False` by default:  > UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)   warnings.warn(f""enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}"") So `nn.Transformer()` with `batch_first=True` doesn't get the warning as shown below:  My suggestion is `batch_first` should be `True` by default because it's not convenient to get the warning with the default settings.  Versions  ",2024-10-02T11:30:17Z,module: nn triaged,open,2,1,https://github.com/pytorch/pytorch/issues/137173,"thank you by this !!! batch_first=True is necessary to avoid the warning, God bless you"
yi,[numpy2.0 compat] Fix test_parse_numpy_int_overflow for NumPy 2.0,NumPy now throws an OverflowError when trying to create np.uint64(1),2024-10-01T21:36:14Z,open source Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/137135,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: jakeharmon8  (bc4a8f9bf43e3bc0b25aa9e7b17b9b06eba60117, db63884e9c0be6d06835d1c87634ac15c2ad7487)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: linuxbinarylibtorchprecxx11 / libtorchcpusharedwithdepsprecxx11test / test Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Allow any single non-batch dim to be ragged for NJT,"  CC(Propagate NJT lengths through op calls)  CC(NJT OpInfo tests v2)  CC(Allow any single nonbatch dim to be ragged for NJT) Fixes CC(Allow any single nonbatch dimension to be ragged for NJT) Relaxes the restriction that the ragged dim is immediately next to the batch dim e.g. `(B, *, D_0, ..., D_N)`. This allows for constructing NJTs of shape e.g. `(B, D, j0)` directly. It's possible before this PR to get an NJT of e.g. shape `(B, D, j0)` by constructing an NJT of shape `(B, j0, D)` and transposing it. This PR allows a user to go straight there without the transpose. The standard `torch.nested.nested_tensor(list)` constructor has been updated to support this. At the very least, this is useful for testing on transposed NJTs. I'm willing to make this functionality private if needed.",2024-10-01T19:26:42Z,Merged ciflow/trunk topic: improvements release notes: nested tensor,closed,0,2,https://github.com/pytorch/pytorch/issues/137125, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
mistral,KeyError: 'CUDA_VISIBLE_DEVICES'," üêõ Describe the bug Dear Community, I am trying to fine one of Mistral AI model using the following code: https://github.com/mistralai/mistralfinetune. It fails when running (University GPU cluster):  With the following error:  Content of the /example/7B.yaml file:  You will find the complete description of the error here (open thread on `mistralfinetune` repo): https://github.com/mistralai/mistralfinetune/issues/98issuecomment2381426638. Since I am fairly new to PyTorch, I may have misunderstood what the error actually means, hence do not hesitate to let me know if I should adjust the thread title. Best wishes, C.  Versions  ",2024-10-01T11:46:41Z,oncall: distributed,closed,0,2,https://github.com/pytorch/pytorch/issues/137082,"This does not sounds like a PyTorch issue to me, as error is raised from by Mistral codebase. One suggestion would be to define CUDA_VISIBLE_DEVICES, but  `torchrun` should do it for you, if there are any GPUs available on the system, but they are not, according to the collect_env info you've posted. Are you try to run the training on CPUonly system? If so, perhaps Mistral needs to tune their codebase a bit to accommodate such usecase. Considering above, I would advice you to start a chat thread on https://discuss.pytorch.org to ask generic question about using PyTorch, but please do not hesitate to create a new issue if you'll manage to narrow problem down a bit more.","> This does not sounds like a PyTorch issue to me, as error is raised from by Mistral codebase. One suggestion would be to define CUDA_VISIBLE_DEVICES, but `torchrun` should do it for you, if there are any GPUs available on the system, but they are not, according to the collect_env info you've posted. Are you try to run the training on CPUonly system? If so, perhaps Mistral needs to tune their codebase a bit to accommodate such usecase. >  > Considering above, I would advice you to start a chat thread on https://discuss.pytorch.org to ask generic question about using PyTorch, but please do not hesitate to create a new issue if you'll manage to narrow problem down a bit more. Dear , My bad as I ran the `collect_env.py` script on the master node of our GPU cluster (which has no GPUs connected to it). Running it on the appropriate node, I get:  I will try again with Mistral developpers but they are actually assuming that the error is coming from PyTorch and not from their codebase... Thanks for support, best, C."
transformer,Can not translate Llama model to MLIR, üêõ Describe the bug I use the following script to translate Llama27bhf to MLIR. It failed in the translation to torchscript.  backtrace   Versions  ,2024-09-29T10:19:54Z,oncall: pt2 export-triaged oncall: export,open,0,6,https://github.com/pytorch/pytorch/issues/136948,"According to the backtrace, it seems the bug happended when translating python code to torchscript."," I haven't reproed yet, but are you sure the model runs with sample inputs `torch.randn(1, 8)` in eager mode? The error sounds like the sample input being a float tensor is crashing the embedding op, which should take in token ids, which are integer/long tensors?", Sorry for the late reply. I had a vacation last week. comes from the following python code.  I dump the shape of the inputs and hard code it when export Llama model.,  I change the input tensor type to torch.long. But it still failed.  Here is the backtrace. ,I can reproduce with the following code. ,
rag,` RuntimeError: cannot mutate tensors with frozen storage` when attempting to export with `nn.ReLU(True)` in one model but not another?," üêõ Describe the bug I have a runnable Colab here testing conversions from PyTorch to Mediapipe (see my comments here, optionally).  When creating an exported program for a DCGAN generator using `nn.ReLU(True)`, everything seems to work fine, but when I do the same for a Pix2Pix UNet generator, the program export fails (and reverting to a plain `nn.ReLU()` allows me to export). This feels like something else is going on here, and maybe the export shouldn't succeed in the first place with the DCGAN? (Both of those conversions to tflite fail, and that might be a reason why?) Here's the full stack:  Any insight would be greatly appreciated!  Versions Collecting environment information... PyTorch version: 2.4.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.30.3 Libc version: glibc2.35 Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.85+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudn",2024-09-27T10:31:11Z,oncall: pt2 export-triaged oncall: export,closed,0,2,https://github.com/pytorch/pytorch/issues/136846,"Hi,  As the error message suggests (""cannot mutate tensors with frozen storage""), there's an error because the inplace relu is trying to mutate a tensor storage that's frozen.  In the DCGAN's case, it's ok because the tensor storage is not frozen, so you can mutate it. In Pix2Pix case, it's not ok because the tensor storage is frozen, so you cannot mutate it. The reason it's frozen is because the tensor is a result of dropout. In export, we don't allow any mutation on result of dropout as of now.  FYI, we freeze it here: https://github.com/pytorch/pytorch/blob/1f3a79379012b408e0375e81fe9205dcba5e34ba/torch/_subclasses/functional_tensor.pyL552.   ","Thanks  for this! I assumed the first two steps of your reasoning, but this I didn't know (and I'm not sure how easy it would have been for me to discover it): > The reason it's frozen is because the tensor is a result of dropout. In export, we don't allow any mutation on result of dropout as of now. >  > FYI, we freeze it here: >  > https://github.com/pytorch/pytorch/blob/1f3a79379012b408e0375e81fe9205dcba5e34ba/torch/_subclasses/functional_tensor.pyL552 Very useful! I still hope it will be possible to get these models converted to tflite/mediapie, or at least understand why the DCGAN and the pix2pix fail to pass the test, but this bit at least is clear!"
yi,Add back DistributedDataParallel types that were lost when pyi was removed,"When the stub file `nn/parallel/distributed.pyi` was removed ( CC(Delete stub file to enable mypy check (4649))), some types that existed are no longer available. This pull request adds them back. Just for reference, these types are used in pytorchlightning's LightningCLI. Command line interfaces are created automatically, and having type hints make them nicer. ",2024-09-27T04:10:09Z,oncall: distributed triaged open source Merged Reverted ciflow/trunk topic: not user facing ci-no-td,open,0,33,https://github.com/pytorch/pytorch/issues/136835," label ""topic: not user facing""","lintrunnernoclang gave mypy errors, so I pushed a new commit to fix them.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / Test `run_test.py` is usable without boto3/rockset Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,I noticed that tests on mac arm64 were failing because of the reducer type added. Made the type hint explicitly a string to avoid these failures., merge, rebase , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `typesDistributedDataParallel` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout typesDistributedDataParallel && git pull rebase`)", merge,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revet m ""this pr is causing typecheck errors internally"" c ""ghfirst""  can you look into fixing the type check errors so we can merge this"," revert m ""this pr is causing typecheck errors internally"" c ghfirst Stack Trace for your reference: Incompatible parameter type [6]: In call `nn.parallel.distributed.DistributedDataParallel.__init__`, for argument `device_ids`, expected `Optional[Sequence[TypeUnion[int, device]]]` but got `Optional[List[Optional[device]]]`.", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,"> Stack Trace for your reference: Incompatible parameter type [6]: In call `nn.parallel.distributed.DistributedDataParallel.__init__`, for argument `device_ids`, expected `Optional[Sequence[TypeUnion[int, device]]]` but got `Optional[List[Optional[device]]]`. I have changed the type for `device_ids` from `Sequence` to `List` just based on the error message. Though, would be nice to know how to reproduce that error locally. `lintrunner` was not giving such an error before, so I don't really know if a similar thing would happen again. ,  any advice?","> I have changed the type for `device_ids` from `Sequence` to `List` just based on the error message. Though, would be nice to know how to reproduce that error locally. `lintrunner` was not giving such an error before, so I don't really know if a similar thing would happen again. ,  any advice?  maybe you can let me know where it failed and how to reproduce? Anyway, I changed the code and you could look again.",Thanks for the change. Let's just try merge again., rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `typesDistributedDataParallel` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout typesDistributedDataParallel && git pull rebase`)", merge, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,"In the forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False) method of MultiheadAttention, there is an issue with the key_padding_mask parameter.||"," üêõ Describe the bug DESCRIPTION: When using the key_padding_mask parameter: (1) If using [[1., 0.]] that conforms to the description in the API is normal, [1., 0.] will be added to the corresponding key. (2) However, if [0., 1.] is used, an error will occur. [0., 1.] is not simply added to the key, and even the result returns the value of attn_output_weights, which does not add up to 1, seriously deviating from the description in the API. In the API, it is stated that if key_padding_mask is a floatingpoint number, it will be added to the corresponding key. Obviously, there is a bug in (2). api link is https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.htmltorch.nn.MultiheadAttention.merge_masks CODE:  OUTPUT:   Versions Collecting environment information... PyTorch version: 2.3.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Home China GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22631SP0 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1650 Ti Nvidia driver version: 556.12 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2496 DeviceID=CPU0 Family=205 L2CacheSize=1024 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2496 Name=Int",2024-09-27T01:34:26Z,,closed,1,3,https://github.com/pytorch/pytorch/issues/136816,this  is really a bug,helloÔºü,All of these are returning an attnetion weight that sums to 1 
llm,Test LLM,Fixes ISSUE_NUMBER,2024-09-26T21:43:02Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/136796
chat,PyTorch_ROCm xformer unsupported input for memory_efficient_attention_forward for gfx1032," üêõ Describe the bug Debian 13 python 3.10.12 venv ROCm 6.1 / HIP Using PyTorch2.4.1_rocm & compile xformers from source When using txt2img and having xformers utilized in the script the running of the script afer several seconds ends with the following  ERROR: No operator found for `memory_efficient_attention_forward` with inputs:      query       : shape=(1, 2, 1, 40) (torch.float32)      key         : shape=(1, 2, 1, 40) (torch.float32)      value       : shape=(1, 2, 1, 40) (torch.float32)      attn_bias   :       p           : 0.0 `ckF` is not supported because:     dtype=torch.float32 (supported: {torch.bfloat16, torch.float16})     If I disable the xformers entry in the script the image building works until it stops with OOM.     Have queried GPT Chat for corrections and still ends up at above error.     GPT Chat gave all kinds of rewrites to spcify float16 again and again,,,yet always ends up with the above error.      Question:     Will PyTorch/xformers_rocm try and use any avaliable Agent via HSA unless specified someplace in the script.     Am wondering if xformers is using the CPU somehow rather than the actual GPU,, hence everything shows 'float32'     Am very green at this..  Flame suit on!      rocminfo:     ROCk module is loaded =====================     HSA System Attributes     =====================     Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE                               System Endianness:       LITTLE                              Mwaitx:                  DISABLED DMAbuf Suppor",2024-09-26T14:37:09Z,needs reproduction module: rocm triaged,open,0,4,https://github.com/pytorch/pytorch/issues/136758, Can you upload the script so we can try a reproduction?,"amd  Thank You, Attaching the processing.py script. This is from an fairly new App called wunjo AI Worth noting all modules of wunjo Ai work,,,other than the 'generation' tab is were this is failing, The other modules calls xformers also,,,but the generation module is only module of all in the app that fails on rocm.  So it appears is either PyTorch or xformers is not interacting correctly Also worth mentioning,,this is the original script. Chat GPT gave many different iterations to try but always ended up at the error i posted in regards to float32 Just trying to give a little background.. processing.py.zip", Thanks for providing this script. I managed to get all the dependencies installed on an MI200 with upstream PyTorch and ROCm 6.2 I am was not able to reproduce the issue. I suspect the issue is specific to our consumer grade GPU cards.  Can you try using one of the supported data types in your script instead of torch.float32?,It seems your issue might be better answered in https://github.com/ROCm/xformers github. Can you post the issue there?
gpt,change GPT2ForSequenceClassification inference accuracy tolerance,"Fixes  CC([inductor][cpu]GPT2ForSequenceClassification AMP static/dynamic shape default/cpp wrapper single thread accuracy crash). https://github.com/pytorch/pytorch/pull/121866 makes GPT2ForSequenceClassification hit the SDPA pattern 18 and then encounter the accuracy issue. The issue only happens with BF16 inference single thread. This PR tends to increase the model tolerance from 4e3 to 5e3 and make the check pass. Note that the issue is due to some small implementation diff. For example, the sdpa math backend scales q, k before matmul for stability; the flash attention backend has more diffs as a new algorithm. ",2024-09-26T11:17:32Z,triaged open source Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,1,6,https://github.com/pytorch/pytorch/issues/136749,"Same issue with pattern 19. > Note that the issue is due to some small implementation diff. Hi , do you already know the rootcause (and is it something that can't be fixed asap)? Thanks!","> Same issue with pattern 19. >  > > Note that the issue is due to some small implementation diff. >  > Hi , do you already know the rootcause (and is it something that can't be fixed asap)? Thanks! Hi , I found other sdpa pattern UTs, e.g. pattern 1, also had accuracy issues with BF16. As the implementations are not exactly the same, it is reasonable to relax the tolerance bounds in this case. Note that the SDPA math backend also meets the accuracy issues with the pattern match.","Hi  , could you help review the PR? Thanks!","Hi  , could you help review the PR? Thanks!", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Bias gradient calculation for NJT linear backward,  CC(Bias gradient calculation for NJT linear backward) Previously NYI   needs it for Transformers. Fixes CC([NJT] Gradients for bias do not get populated for nn.Linear),2024-09-25T18:49:54Z,Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/136660, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/jbschlosser/179/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/136660`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Fix 136201-Compile with USE_CPP_CODE_COVERAGE=ON throw erros: use lld‚Ä¶,"I add linker options: "" fuseld=lld"" to force clang using lld in the step of linking when USE_CPP_CODE_COVERAGE=ON with clang.  Now I can compile it normally and use `tools/code_coverage/oss_coverage.py`   Fixes CC(Compile with USE_CPP_CODE_COVERAGE=ON throw erros)",2024-09-25T13:50:07Z,triaged open source Stale topic: not user facing,open,0,2,https://github.com/pytorch/pytorch/issues/136632,:white_check_mark:login: xuesu / (1988fdca9c9127ae25287039466e04c8ab0d0793)The committers listed above are authorized under a signed CLA.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,[Doc issue] RMSNorm formula," üìö The doc issue **Description:** There appears to be a discrepancy in the RMSNorm formula as documented on the PyTorch docs website compared to the original formulation presented in the paper ""Root Mean Square Layer Normalization"" (arXiv:1910.07467). **Details:** The formula for RMSNorm in the PyTorch documentation is currently represented as: $$ y = \frac{x}{\sqrt{\text{RMS}[x] + \epsilon}} * \gamma $$ However, according to the original paper, the correct formula should be: $$ y_i = \frac{x_i}{\text{RMS}(x)} \gamma_i, \quad \text{where} \quad \text{RMS}(x) = \sqrt{\epsilon + \frac{1}{n} \sum_{i=1}^{n} x_i^2} $$ **Issue:**  The PyTorch documentation formula includes an additional square root operation around RMS[x] and epsilon, which is not present in the original paper's formula.  This discrepancy could lead to confusion or incorrect implementation by users relying on the PyTorch documentation.  Suggest a potential alternative/fix **Suggested Correction:** Please update the PyTorch documentation to reflect the correct formula as per the original paper. The corrected formula should be: $$ y = \frac{x}{\sqrt{\text{MS}[x] + \epsilon}} * \gamma $$ where MS is the Mean Square. Implementing this correction will enhance the accuracy and reliability of the PyTorch documentation. By aligning with the original paper's formula, users will have a clearer understanding of how RMSNorm is intended to function, thereby reducing potential implementation errors. This update will also help maintain consistency across different resources and ensure that users can confidently rely on the documentation for their projects.  ",2024-09-25T02:18:58Z,module: docs module: nn triaged,closed,1,0,https://github.com/pytorch/pytorch/issues/136597
transformer,torch.export support for the latest transformers `DynamicCache` as input,Hugging Face `transformers` is moving to use the `DynamicCache` class as part of the model inputs for the kv cache values. Currently `torch.export` will complain that it is not a tensor. So all models that take `DynamicCache` as input will not be exportable. This affects torch.onnx and other exporters dependent on torch.export alike.   Will raise an error  ,2024-09-24T23:28:48Z,oncall: pt2 export-triaged oncall: export,open,0,1,https://github.com/pytorch/pytorch/issues/136582," The same issue is addressed in transformers repo already. See https://github.com/huggingface/transformers/pull/32830, where we create an integration point to ""Export to ExecuTorch"". Basically, it's hiding the cache object from the graph I/O to make it exportable. For ExecuTorch, we only support StaticCache atm as there is no way to resize the size when putting the exported artifacts ondevice. However, I believe it can work with DynamicCache as well if you have a usecase where you do want to export with DynamicCache"
llm,torch._dynamo.exc.Unsupported: ObservedKeyError exception running Gguf llama model in vLLM, üêõ Describe the bug Running the `TheBloke/TinyLlama1.1BChatv1.0GGUF` model with gguf quantization leads to a `ObservedKeyError` in dynamo. Reproduction steps: Save to `bug.py`   I'm using the 2.5 nightly version of pytorch and the 0.6.1.post2 (b05f5c923) version of vLLM. Note: this was masked by a different issue in pytorch 2.4 Partial Stacktrace:   Versions  ,2024-09-24T02:25:50Z,triaged oncall: pt2 module: dynamo vllm-compile,open,0,0,https://github.com/pytorch/pytorch/issues/136502
llama,torch._dynamo.exc.Unsupported: 'immutable_list' object does not support mutation when running MiniCPM-Llama model in vLLM, üêõ Describe the bug Running one of the minicpmllama model tests results in a dynamo error on the builtin `iadd` function.  I'm using the 2.5 nightly version of pytorch and the 0.6.1.post2 (b05f5c923) version of vLLM. Stacktrace:   Versions  ,2024-09-24T02:15:15Z,triaged oncall: pt2 module: dynamo vllm-compile,open,0,0,https://github.com/pytorch/pytorch/issues/136499
llm,torch._dynamo.exc.Unsupported: Unexpected type in sourceless builder torch.Tensor when running Mamba models in vLLM," üêõ Describe the bug The Mamba models in vLLM contain a user defined class `MambaCacheParams` which dynamo seems to be unable to process.  The error can be reproduced running the following steps:  I'm using the 2.5 nightly version of pytorch and the 0.6.1.post2 (b05f5c923) version of vLLM. Note: this also fails with pytorch 2.4 due to `itertools.zip_longest` being unsupported. Stacktrace:   Versions Collecting environment information... PyTorch version: 2.5.0+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.30.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.5.82 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 555.42.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 57 bits virtual Byte Order:                         Little Endian CPU(s):                             128 Online CPU(s) list:                0127 Vendor ID",2024-09-24T02:07:23Z,triaged oncall: pt2 module: dynamo vllm-compile,open,0,2,https://github.com/pytorch/pytorch/issues/136497,"Hey , did you manage to work around this?","> Hey , did you manage to work around this? It looks like someone refactored some of the mamba code recently and this test is no longer failing.  I'm not sure what fixed it though, so I'm guessing the underlying problem is still there."
llm,dataclasses.replace not supported by dynamo," üêõ Describe the bug The `dataclasses.replace` function appears to be unimplemented in dynamo.   In this particular case it is used from the xformers (0.0.27.post2) package in `xformers/ops/fmha/cutlass.py:259`.  I'm using the pytorch 2.5 nightly (2.4 fails in a different spot). It can be reproduced with the following steps: 1. Save to `bug.py`  2. Download and build VLLM version/hash: 0.6.1.post2, `b05f5c923` 3. Run the following command.  Stacktrace:   Versions  ",2024-09-23T22:39:48Z,good first issue triaged oncall: pt2 module: dynamo vllm-compile,open,0,0,https://github.com/pytorch/pytorch/issues/136481
llm,[ROCm] install_miopen.sh exit for ROCm >= 6.3,Follow up to CC([ROCm][CI] upgrade CI to ROCm 6.2). ,2024-09-23T15:24:40Z,module: rocm open source Merged topic: not user facing rocm priority ciflow/rocm,closed,1,5,https://github.com/pytorch/pytorch/issues/136436, Should/would this not be part of the set of changes for ROCm6.3 upgrade in the future? Trying to understand the motivation for making this ROCm6.3specific change now.,">  Should/would this not be part of the set of changes for ROCm6.3 upgrade in the future? Trying to understand the motivation for making this ROCm6.3specific change now. amd ,  Internal dev builds of ROCm 6.3 are breaking on upstream main branch. So we need to handle this case.",> lgtm. What is the ETA for Rocm 6.3 CD builds ? November.," merge f ""this change can only affect ROCm CI images and all ROCm CI is passing"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Make it possible to run pr_time_benchmarks without explicitly specifying PYTHONPATH, üêõ Describe the bug It's annoying to have to always pass PYTHONPATH. If we put the benchmark harness somewhere in torch/_testing this would no longer be necessary. But maybe that would make backtesting more difficult?  Versions main ,2024-09-23T14:11:01Z,triaged module: testing oncall: pt2,open,0,1,https://github.com/pytorch/pytorch/issues/136430, 
yi,Undefined reference to `std::ios_base_library_init()'," üêõ Describe the bug I have compiled LibTorch 2.4.1 from source with MPI and Rocm support enabled. whenever I compile and link and executable to it, the linking step stops with the error  This happens for instance with all binaries, e.g., `parallel_info.cc`  Versions Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: SUSE Linux Enterprise Server 15 SP5 (x86_64) GCC version: (GCC) 12.2.0 20220819 (HPE) Clang version: Could not collect CMake version: version 3.29.6 Libc version: glibc2.31 Python version: 3.10.10 (main, Aug  4 2023, 20:19:42) [GCC 9.3.0 20200312 (Cray Inc.)] (64bit runtime) Python platform: Linux5.14.21150500.55.49_13.0.56cray_shasta_cx86_64withglibc2.31 Is CUDA available: N/A CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             128 Online CPU(s) list:                0127 Vendor ID:                          AuthenticAMD Model name:                         AMD EPYC 7A53 64Core Processor CPU family:                         25 Model:                              48 Thread(s) per core:                 2 Core(s) per socket:                 64 Socket(s):                    ",2024-09-20T06:37:59Z,module: binaries module: rocm triaged,open,0,0,https://github.com/pytorch/pytorch/issues/136351
transformer,FSDP2 error loading `PreTrainedModel`'s `torch.Tensor` state_dict into `DTensor`," üêõ Describe the bug I'm trying to follow the instructions to efficiently load Hugging Face models from `torchtitan`'s docs for FSDP1 > FSDP2: MetaDevice Initialization. When I get to the last step of actually loading the model's weights, I get an error about trying to copy `torch.Tensor` into the FSDP2 `DTensor`s.   Coming from: https://github.com/pytorch/pytorch/blob/172ecf78b75f55948f9102c85e72314d1f8f1a22/torch/distributed/tensor/_dispatch.pyL458L473 `accelerate.load_checkpoint_and_dispatch` is just calling `load_state_dict` under the hood. How are users expected to initialize from pretrained weights in this scenario?     Versions Collecting environment information... PyTorch version: 2.4.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.0119genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 535.183.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(",2024-09-19T21:48:49Z,triaged module: fsdp,open,0,5,https://github.com/pytorch/pytorch/issues/136336,"I will be out today, but I will try to look at this beginning of next week.","For terminology, FSDP considers two kinds of state dicts:  Full state dict: values are unsharded `torch.Tensor`s  Sharded state dict: values are sharded `DTensor`s (sharded on dim0) FSDP1 allows saving and loading these two kinds of state dicts configured via `StateDictConfig`. FSDP2's parameter representation directly matches the sharded state dict.  Calling `model.state_dict()` on an FSDP2sharded model returns a sharded state dict without any computation or communication.  Conversely, FSDP2 only supports loading a sharded state dict. All this means is that the conversion between full and sharded state dicts should happen outside of FSDP2. One way to do this conversion is to load the full state dict on rank 0 and iteratively broadcast and shard it on all ranks; this is to avoid loading the same full state dict in CPU RAM multiple times per host. Here is an example of how to do this: https://github.com/pytorch/pytorch/blob/3bc073d7280d36eec2af3b1969c165b336a8ecae/test/distributed/_composable/fsdp/test_fully_shard_init.pyL701 You may customize this logic further to support specific constraints. For example, torchtune has this logic (code) with some special support for `NF4Tensor`s.",It makes sense to use the sharded meta tensors of the model to inform how to distribute a loaded nonsharded state dict. But how should I distribute loaded nonsharded optimizer state tensors? The optimizer state starts out empty so I don't have any sharded meta tensors to use to inform how to shard.,"For optimizer state dict, if you want to roll your own solution, then you could base it off the model's sharding. Otherwise, I think distributed state dict provides utilities for this. cc:   for example API usage","  For both model state and optimizer state (optimizer state initialization), you can use distributed state dict. `get_optimizer_state_dict` would initialize the states for you. `set_optimizer_state_dict` would load the optim state dict into the optimizer.  Here is a test case(https://github.com/pytorch/pytorch/blob/main/test/distributed/_composable/test_composability/test_2d_composability.pyL410) for loading a full state dict into a 2D parallelized (FSDP2+TP) model. Usage should be identical.  For more details, you could refer to the doc here: https://pytorch.org/docs/stable/distributed.checkpoint.htmltorch.distributed.checkpoint.state_dict.get_state_dict"
agent,"On AMD GPUs (ROCm 5.7-6.2), cannot call `torch.unique()` on a floating-point tensor"," üêõ Describe the bug When `gt_features[""asym_id""]` (which represents a `torch.float32` tensor of shape [B, N] which at some indices may contain `float(""inf"")`) is used in the function call  the following memory aperture violation error is raised and crashes my training script without reporting any meaningful error logs to debug:  I've fixed this issue by instead casting `gt_features[""asym_id""]` as a `torch.int` tensor instead and replacing `inf` with a large integer value (i.e., `int(1e6)`). However, on NVIDIA cards such as an A100, I did not encounter any such memory exceptions. In fact, my training script (i.e., the same script) runs just fine on NVIDIA GPUs. I've included this patch in another GitHub repository as a proof of concept that this (casting) solution is a temporary workaround.  Versions Collecting environment information... PyTorch version: 2.3.0a0+git96dd291 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 6.2.41133dd7f95766 OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvmproject roc6.2.0 24292 26466ce804ac523b398608f17388eb6d605a3f09) CMake version: version 3.26.4 Libc version: glibc2.35 Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.14.21150500.55.49_13.0.57cray_shasta_cx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Instinct MI250X (gfx90a:sramecc+:xnack) Nvidia driver version: Could not collect cuDNN version: Could not collect H",2024-09-19T17:16:44Z,module: rocm triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/136323, Please assign this one to me.,"Hi, . Have you discovered anything more about what might be causing this issue?","Hi  , I haven't had time to dig into this one. But it is in my queue."," Can you test this to see this is a problem? I used this and it works fine:  Also, if this does not reproduce the problem, please create a reproducer as simple this one, so that we can reproduce it.", Please let us know if you are able to test with the above reproducer. The current plan is to close this issue in a week due to inactivity.
agent,Fault tolerance with k8s," üöÄ The feature, motivation and pitch I am working on training fault tolerance.  We want to restart only one training node when there is a hardware failure, but the existing design does not allow the agent exit and will restart the training process. I hope that it can be realized that the agent can exit when an error occurs, and other nonfaulty training node agents can survive.  Alternatives 1. Write the number of restarts to etcd/tcp_store 2. Add configuration parameters. If a training process exits with error, the agent exits  Additional context _No response_ ",2024-09-19T09:44:05Z,oncall: distributed needs design,open,2,2,https://github.com/pytorch/pytorch/issues/136312,"> the existing design does not allow the agent exit and will restart the training process Yeah, the current implementation is a bit harsh, in that the watchdog will kill the process when detecting failure or timeout.  As a first step, we can relax this and leave the process alive, which would allow the user to take control.  Second step, we imagine that the user might want to kill the dead communicator  a NCCL kernel may be hanging, etc. For this, we are experimenting with a new API, see https://github.com/pytorch/pytorch/pull/132291 Then enable calling `init_process_group` again."," The second step seems to be a final solution with some architectural changes. We are currently trying to make some modifications to complete the first step, which allows the agent to exit and let the external system decide whether to continue training or not, so there is some flexibility to do things externally. Discuss it if necessary"
llm,Torch installation fails while installing VLLM," üêõ Describe the bug While installing TGI from source, VLLM needs to be installed. When the CMake command is run, Torch needs to be imported. Torch version 2.3 is needed. While importing, this error is coming: CMAKE_CUDA_ARCHITECTURES must be nonempty if set. It's coming at line 67 of CMakeLists.txt which has this code: `find_package(Torch REQUIRED) ` Cuda 12.6 is installed on my machine.  Versions 2.3 ",2024-09-19T09:37:17Z,module: build,closed,0,2,https://github.com/pytorch/pytorch/issues/136311,This is the guide being followed to build TGI from source. https://github.com/huggingface/textgenerationinference/?tab=readmeovfilelocalinstall The CMakeLIsts file is here: https://github.com/vllmproject/vllm/blob/main/CMakeLists.txtL70,This doesn't sound like a bug in PyTorch to me. VLLM and TGI are not projects maintained by PyTorch. We use github issues only for bug reports or feature requests for PyTorch. You can ask in the PyTorch forums (https://discuss.pytorch.org) or in the relevant projects
yi,"[CompiledAutograd] ""compiled_args nyi"" in CppFunctionTensorPreHook"," üêõ Describe the bug Hi, I met this error when testing compiled autograd:  Part of the using codes are:  How can I locate the hook that will use CppFunctionTensorPreHook? Thanks!  Error logs _No response_  Minified repro _No response_  Versions 2.4 ",2024-09-19T09:25:23Z,triaged oncall: pt2 module: compiled autograd,open,0,4,https://github.com/pytorch/pytorch/issues/136309,"Thanks for reporting the issue, compiled autograd doesn't yet support for C++ hooks, it is a feature we're looking at for v2.6+. It looks like you're using the python frontend, are you using a library that registers C++ tensor pre hooks?"," Thanks! Yes, I'm using Python frontend for a training workload. Could you please give me some hints on what APIs will register C++ tensor pre hooks? I'll check which lib uses this and see whether I can work around it.","Maybe from some `at::Tensor::register_hook`: https://pytorch.org/cppdocs/api/classat_1_1_tensor.html_CPPv4I0ENK2at6Tensor13register_hookE18hook_return_void_tI1TERR1T If your libraries are still compatible and you're able to build from source, you could try to figure out what is calling this: https://github.com/pytorch/pytorch/blob/02871461f7b378f6dc3bb89cae8335e87907e142/torch/csrc/autograd/function.hL497",I encountered the same issue. In my case issue is caused by hooks registered in https://github.com/pytorch/pytorch/blob/main/torch/csrc/autograd/autograd_not_implemented_fallback.cppL178
llm,torch inductor fails on vllm model that uses fp8 data types," üêõ Describe the bug The following script results in an error when run with vllm 0.6.1post2 and PyTorch 2.4. The model is using fp8 types.  error message:   Versions Collecting environment information... PyTorch version: 2.4.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.29.5 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.5.82 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 555.42.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 57 bits virtual Byte Order:                         Little Endian CPU(s):                             128 Online CPU(s) list:                0127 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Xeon(R) Platinum 8462Y+ CPU family:                         6 Model:                              143 Thread(s) per core:                 2 Core(s) per ",2024-09-18T21:49:31Z,triaged oncall: pt2 module: inductor vllm-compile,closed,0,5,https://github.com/pytorch/pytorch/issues/136294,'m not able to add the vllmcompile label myself.," Does this also repro for you in eager mode? I cannot repro locally, but I also see: `WARNING 0921 16:34:25 utils.py:727] Your GPU does not have native support for FP8 computation but FP8 quantization is being used...`. ",">  Does this also repro for you in eager mode? I cannot repro locally, but I also see: `WARNING 0921 16:34:25 utils.py:727] Your GPU does not have native support for FP8 computation but FP8 quantization is being used...`.  If you mean `backend=""eager""`, then yes this model works with that backend.  This only happens with the inductor is enabled.",PR out fixing this here https://github.com/pytorch/pytorch/pull/128780,Should be fixed with https://github.com/pytorch/pytorch/pull/137341  please reopen if still fails on master.
llm,[VLLM/Inductor] Inductor reported to uses more memory and less efficient that ,"you can take a look at the log when you run with inductor:  GPU blocks: 790 when I turn on inductor, the blocks become smaller, which means inductor takes more memory and we have less memory for the kv cache. See comment in https://github.com/vllmproject/vllm/pull/8384 ",2024-09-18T15:22:40Z,triaged oncall: pt2 module: inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/136269,This is resolved with the investigation results in   CC(Inductor max used memory perform worse with as_strided vs split_with_sizes + subscripts) and Functionalization V2.
rag,PT2 should leverage partial reductions to speed up larger reductions," üöÄ The feature, motivation and pitch In float8 recipes, we need to ""scale"" our tensors, which consists in computing the absmax along certain dimensions. One given tensor is usually scaled multiple times to be used in different operators (e.g., the forward vs the backward matmuls). If we scale in the exact same way in all cases, PT2 is already able to eliminate the common subexpressions and avoid redundant calculations. However, sometimes the reductions are not identical, but they are still related (e.g., one is a superset/subset of the other). It should still be possible to leverage the result of one such reduction to speedup the other, however PT2 is currently unable to do so, and thus generates suboptimal code. Concretely, this is a common scenario:   Alternatives This logic could be handled by the user code in ""eager"" mode, but this can add a lot of complexity (especially when this crosses fwd/bwd boundaries), and it feels like the kind of things that compilers are very good at optimizing automatically.    Additional context  This snippet:  gives this output code:  where both kernels take the original tensor as input, thus needing to read it in full, which is the bottleneck for such memorybandwidthbound kernels. However, PT2 should be able to detect that the first reduction is a partial/intermediate step of the second reduction, and leverage that. In a nutshell, it should rewrite the above snippet as follows:  which indeed produces more efficient output code:  where the output of the first kernel is given as input to the second kernel.  ",2024-09-18T14:31:02Z,triaged oncall: pt2 module: inductor,open,2,5,https://github.com/pytorch/pytorch/issues/136267,"CC sq, I think the fix for  CC([Inductor] Fusion of Tiled Point-Wise and Reduction Operators) might lend itself towards fixing this issue as well.","Yes, in  CC([Inductor] Fusion of Tiled Point-Wise and Reduction Operators), we try to solve the issue by deferring splitting reductions until the scheduler, and look at fusion opportunities to decide the split number. At this point, Inductor should be able to make the decision to fuse rowwise_scales (and rowwise casting also?) and tensorwise_scales into one kernel.","While waiting for a proper solution to this issue (which, I believe, ought to happen in Inductor), I needed a workaround in order to make the `LW_AXISWISE_WITH_GW_HP` recipe for float8 work in torchao. Thus I implemented a custom FX pass, which I registered on the `torch._inductor.config.joint_custom_post_pass` hook, which manually looks for the relevant pattern (`abs` followed by several disjoint `max`es or `amax`es) and replaces it with a chain of ops. This gave us a ~56% endtoend speedup in our Llamalike training runs. For reference, this is the code of that FX pass: ",", this is pass on operating on the joint graph. does it affect the saved activations in your use case ? if so, we'd still need to do it on the joint graph as an fx pass, even if we upstream it to core.",here is a way to reproduce the recipe which needs this in torchao: 
transformer,Flex Attention Extremely Slow," üêõ Describe the bug I try to use flex attention in huggingface transformer, only to find it very slow. Compared to the sdpa implementation, flex attention is about 45 times slower, but it does save the CUDA memory. Tested on RTX3090, A6000 and A100. Here is the example code: https://gist.github.com/whyinShanghaitech/8b8205f98568c6741a2e38dfcdb9d362 I have no idea what is happening. Is this normal? Can anyone reproduce this? Or this problem is related to huggingface transformers?  Versions  ",2024-09-18T11:51:57Z,oncall: pt2 module: higher order operators module: pt2-dispatcher module: flex attention,closed,0,2,https://github.com/pytorch/pytorch/issues/136261,"It looks like you are using flex_attention in eager mode. Please use compilation mode instead. i.e., `flex_attention = torch.compile(flex_attention)` ","> It looks like you are using flex_attention in eager mode. Please use compilation mode instead. i.e., `flex_attention = torch.compile(flex_attention)` Thank you so much! This exactly solves the problem."
transformer,Performance regression in torch.compile," üêõ Describe the bug Hi fangintel  The compiled model's generation latency is slower than the eager mode. Regression happens from torch 0806  torch 0807. transformers version: 4.44.2 Script:   Versions Collecting environment information...                                                                                                                                                                                 PyTorch version: 2.5.0.dev20240806+cpu                                                                                                                                                                                Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Rocky Linux release 8.10 (Green Obsidian) (x86_64) GCC version: (condaforge gcc 12.3.013) 12.3.0 Clang version: Could not collect CMake version: version 3.27.9 Libc version: glibc2.28 Python version: 3.10.14  (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64bit runtime) Python platform: Linux4.18.0553.5.1.el8_10.x86_64x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              224 Online CPU(s) list: 0223 Thread(s) per core:  2 Core(s) per socket:  56 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU family:          6 Model:               143 Model name:          In",2024-09-18T06:01:53Z,oncall: pt2 oncall: cpu inductor,open,0,16,https://github.com/pytorch/pytorch/issues/136254,"Suspected guilty commit is https://github.com/pytorch/pytorch/commit/de00c7958301ce81b9716bdef5731ed40d4d14ca. In this model, it will cause recompilations.  Could you please help check this ?", https://github.com/pytorch/pytorch/pull/136516 might fix this. I will work on it tomorrow., I tried https://github.com/pytorch/pytorch/pull/136516. The recompilation is fixed.,Hi . It seems https://github.com/pytorch/pytorch/pull/136516 can not fix this issue when it is merged into main branch. Applying your PR on https://github.com/pytorch/pytorch/commit/de00c7958301ce81b9716bdef5731ed40d4d14ca can solve recompilation., I can take a look. But can you elaborate? You mention that applying it on the above commit solves recompile. But that commit is already on main.," Ah, were you suggesting that you want to be at 2.5 release branch and manually apply the changes? In that we might need both commits. Let me check if I can backport these fixes."," Just to be sure I am writing down my analysis 1) The base commit (or the commit de00c79) that you mentioned. We don't need that in 2.5 release candidate. That PR was reverted. And we relanded that in https://github.com/pytorch/pytorch/pull/134136. This PR/commit is already present in the 2.5 release branch !image 2) Now, I am sending a backport for https://github.com/pytorch/pytorch/pull/136516 in the release branch here  https://github.com/pytorch/pytorch/pull/137025. If that is merged, then you won't see extra recompiles with 2.5 release branch.", Thanks for your fixing. Sorry for the late reply. I'm on vacation. There may be commits on the main branch that caused the regression before your fix https://github.com/pytorch/pytorch/pull/136516. We will verify it next.,"Hi  , please reopen this issue as it's not been resolved in the main branch. Thanks.","Can you share TORCH_LOGS=guards,recompiles for the  fast and slow case? "," It seems that even with the fix https://github.com/pytorch/pytorch/pull/136516, the commit 042b733ddd7d7eda5aa01e8f919b2847a7fb6692  still cause the recompilation.  "," can you share TORCH_LOGS=guards,recompiles for the old and new case. You can use backend=eager to speed it up.", We collected the logs and sent them to you via email.,"Hi  . Do you have any updates? BTW, `backend=eager` no helps.","I wonder if this is related, I am seeing a massive performance falloff (90% slowdown) in sunoai/bark model: https://github.com/sunoai/bark/issues/609", Is there any update on this issue ? Thanks
transformer,[BE] Make `NestedTensorTransformerFunctions.cu` compilable without warnings,Before the change compilation produced following warnings:  after it compiled without a warning Fixes ISSUE_NUMBER,2024-09-17T20:34:28Z,Merged release notes: cuda topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/136222," merge f ""builds + lint are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Compile with USE_CPP_CODE_COVERAGE=ON throw erros," üêõ Describe the bug when I use ` USE_CPP_CODE_COVERAGE=ON BUILD_TEST=ON CMAKE_BUILD_TYPE=Debug  CMAKE_RULE_MESSAGES=ON CC=clang CXX=clang++ python setup.py develop` to compile pytorch, I couldn't success and I have those error printed:  I am using the llvm tag 17.0.6. I have rebuilt all related libraries multiple times and I can successfully build pytorch without `USE_CPP_CODE_COVERAGE=ON`  Versions  ",2024-09-17T15:37:13Z,module: build triaged actionable,open,0,2,https://github.com/pytorch/pytorch/issues/136201,"I don't think we are actively test CPP_CODE_COVERAGE option. If you don't have a PR to fix the linking issue, please do not hesitate submitting a fix",Thx! Lemme try it!
yi,Add back optim type hints that were lost when *.pyi files were removed,"When stub files (`*.pyi`) were removed from `optim` ( CC(add typing in torch.optim.lr_scheduler), CC([optim] Merge the pyi files into py files of optimizer)), some types that existed are no longer available. This pull request adds them back. Just for reference, these types are used in `pytorchlightning`'s `LightningCLI`. Command line interfaces are created automatically, and having type hints make them nicer.",2024-09-17T04:53:42Z,open source Merged ciflow/trunk release notes: optim,closed,0,4,https://github.com/pytorch/pytorch/issues/136185, merge, Merge failed **Reason**: Not merging any PRs at the moment because there is a merge blocking https://github.com/pytorch/pytorch/labels/ci:%20sev issue open at:   CC(linux-aarch64 CI tests are being timed out resulting in test failures) Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,"[inductor] gradient numeric discrepency when fusing Transfromer (Embedding, LayerNorm, Linear)"," üêõ Describe the bug In Transformer, we have 3 basic modules (Embedding, LayerNorm, Linear) (see `EmbNormLinear` below). Graident numerics are different betwen `torch.compile(backend='inductor')` and eager The numerics are important because I want to compare float8 numerics with high precision numerics. If high precision numerics itself is not accurate, it would be hard to attribute numeric loss repro: `TORCH_LOGS=""output_code"" CUBLAS_WORKSPACE_CONFIG=:4096:8 TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 python test_emb_numerics.py` * bisecting on PT2 stack, backend='aot_eager' have exact numerics. Just backend='inductor' yield different numerics * bisecting on code, removing 1 of the modules will yield exact numerics * bisecting on forward/backward, forward has exact numerics, just backward is different. There is no optimizer.step    Versions `python c ""import torch;print(torch.__version__)""`: 2.5.0a0+git94d2471 ",2024-09-15T23:25:14Z,oncall: pt2 module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/136121, do you think this deserves to be hipri (and discussed at triage review)?,torch.compile will not produce bitwise equal numerics. when switched to torch.testing.assert_allclose the repro passes. The diverence here is very small: `AssertionError: i=0 774.6301879882812 vs 774.6292114257812`
rag,[BE][Ez]: Add missing cpu float16 coverage for avg_pool_3d,Fixes a edge case where avg pool 3d didn't support all half dtypes on CPU. Adds the missing kernel to give near parity between GPU and CPU. ,2024-09-15T17:35:11Z,module: cpu open source better-engineering Stale release notes: python_frontend,open,0,2,https://github.com/pytorch/pytorch/issues/136116,You should also check if the numerics make sense. It may be necessary to do intermediate compute in higher precision to avoid catastrophic loss in precision,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,"Setting wrong type values to `kernel_size`, `stride`, `padding` and `dilation` argument of `nn.MaxPool1d()` gets the wrong error messages saying only `tuple of ints`"," üêõ Describe the bug The doc of `nn.MaxPool1d()` says `kernel_size`, `stride`, `padding` and `dilation` argument are `int` or `tuple` of `int`) as shown below: > Parameters >  kernel_size (Union[int, Tuple[int]]) ‚Äì The size of the sliding window, must be > 0. >  stride (Union[int, Tuple[int]]) ‚Äì The stride of the sliding window, must be > 0. Default value is kernel_size. >  padding (Union[int, Tuple[int]]) ‚Äì Implicit negative infinity padding to be added on both sides, must be >= 0 and   dilation (Union[int, Tuple[int]]) ‚Äì The stride between elements within a sliding window, must be > 0. But setting wrong type values to them gets the wrong error messages saying only `tuple of ints` as shown below. *It also happens with nn.MaxPool2d() and nn.MaxPool3d():  > TypeError: max_pool1d(): argument 'kernel_size' (position 2) must be tuple of ints, not float > TypeError: max_pool1d(): argument 'stride' (position 3) must be tuple of ints, not float > TypeError: max_pool1d(): argument 'padding' (position 4) must be tuple of ints, not float > TypeError: max_pool1d(): argument 'dilation' (position 5) must be tuple of ints, not float So, they should be something like as shown below. *`list` of `int` also works so `list` of `int` can also be added: > TypeError: conv_transpose1d(): argument 'stride' (position 4) must be `int` or `tuple` of `int`, but found element of type float at pos 0 > TypeError: conv_transpose1d(): argument 'padding' (position 5) must be `int` or `tuple` of `int`, but found element of type float at pos 0 > TypeError: conv_transpose1d(): argument 'output_padding' (position 6) must be `int` or `tuple` of `int`, but found element of type",2024-09-14T17:33:52Z,low priority module: error checking triaged actionable module: python frontend,open,0,1,https://github.com/pytorch/pytorch/issues/136093,"Please do not hesitate to propose a PR to refine an error message, though in the grand scheme of things they seems fine to me: if arguments of a wrong type are passed it will tell you which argument was it"
rag,[BE][Ez]: Fix missing float16 coverage for adaptive_pool3d_cpu,Testing if op info coverage has issues ,2024-09-14T16:47:11Z,open source better-engineering Merged ciflow/trunk release notes: python_frontend module: inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/136091, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `skylion007/addcpufloat16adaptivemaxpool` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout skylion007/addcpufloat16adaptivemaxpool && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,"Setting wrong type values to `stride`, `padding`, `output_padding` and `dilation` argument of `nn.ConvTranspose1d()` gets the wrong error messages saying only `tuple of ints`"," üêõ Describe the bug The doc of `nn.ConvTranspose1d()` says `stride`, `padding`, `output_padding` and `dilation` argument are (`int` or `tuple`, optional) as shown below: > Parameters >  ... >  stride (int or tuple, optional) ‚Äì Stride of the convolution. Default: 1 >  padding (int or tuple, optional) ‚Äì dilation * (kernel_size  1)  padding zeropadding will be added to both sides of the input. Default: 0 >  output_padding (int or tuple, optional) ‚Äì Additional size added to one side of the output shape. Default: 0 >  ... >  dilation (int or tuple, optional) ‚Äì Spacing between kernel elements. Default: 1 But setting wrong type values to them gets the wrong error messages saying only `tuple of ints` as shown below. *It also happens with nn.ConvTranspose2d() and nn.ConvTranspose3d():  > TypeError: conv_transpose1d(): argument 'stride' (position 4) must be tuple of ints, but found element of type float at pos 0 > TypeError: conv_transpose1d(): argument 'padding' (position 5) must be tuple of ints, but found element of type float at pos 0 > TypeError: conv_transpose1d(): argument 'output_padding' (position 6) must be tuple of ints, but found element of type float at pos 0 > TypeError: conv_transpose1d(): argument 'dilation' (position 8) must be tuple of ints, but found element of type float at pos 0 So, they should be something like as shown below. *`list` of `int` also works so `list` of `int` can also be added: > TypeError: conv_transpose1d(): argument 'stride' (position 4) must be `int` or `tuple` of `int`, but found element of type float at pos 0 > TypeError: conv_transpose1d(): argument 'padding' (position 5) must be `int` or `tuple` of `int`",2024-09-14T02:57:53Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/136071
transformer,xpu: set of aten ops are missing for Huggingface Transformers,"With: * https://github.com/pytorch/pytorch/commit/cd472bb1e368a711a2bd34d5671c77dab336d312   * Plus this applied: https://github.com/pytorch/pytorch/pull/135567 * https://github.com/intel/torchxpuops/commit/c6981a238cdaf93774b1c6a3550a436f530f4736 * https://github.com/huggingface/accelerate/commit/4b4c036933f7c50fe3a7027b0380fcec53c6975e * https://github.com/huggingface/transformers/commit/d70347726577e9823e35c11883e98c5b2c520b37 I was running Huggingface transformers models tests, i.e.:  Overall: * **Few aten ops not implemented for XPU (this issue is about this)** * `PYTORCH_ENABLE_XPU_FALLBACK=1` was not needed at all (which is good) *  CC(xpu: huggingface levit test_retain_grad_hidden_states_attentions test hangs on exit on PVC)  hang happens in levit tests on Intel PVC (only in levit, other models ran fine) * https://github.com/huggingface/transformers/pull/33485  few HF tests were not enabled for HW devices (cuda/xpu wise), this PR adds support The following aten ops are not yet implemented for XPU and affect HF tests. Please, implement:  [x] `aten::_ctc_loss`, https://github.com/intel/torchxpuops/pull/925  [x] `aten::_ctc_loss_backward`, https://github.com/intel/torchxpuops/pull/925  [ ] `aten::_fft_c2c`, https://github.com/intel/torchxpuops/pull/526  [ ] `aten::_fft_c2r`  [ ] `aten::_fft_r2c`  [x] `aten::index_copy.out`, https://github.com/intel/torchxpuops/pull/793  [x] `aten::linspace.out`, https://github.com/intel/torchxpuops/pull/850  [x] `aten::take`, https://github.com/intel/torchxpuops/pull/868  [x] `aten::upsample_bicubic2d_backward.grad_input`, https://github.com/intel/torchxpuops/pull/914  [x] `aten::xlogy.OutTensor`, h",2024-09-14T00:04:50Z,triaged module: xpu,open,4,4,https://github.com/pytorch/pytorch/issues/136065,All remaining missing ones are on PT2.6 task list.," Please refresh the issue status, if any updates.","As of https://github.com/pytorch/pytorch/commit/d08dbd0436637a04098151b0e03c69dcdab3f44e, the following ops are not yet available in pytorch build:  [ ] `aten::_fft_c2c`, https://github.com/intel/torchxpuops/pull/526  [ ] `aten::_fft_c2r`  [ ] `aten::_fft_r2c` The rest of ops are implemented and available and should appear in PT 2.6 all except `aten::upsample_bicubic2d_backward.grad_input` which made it into PT 2.5.",Huggingface Transformers v4.46.0 introduced usage of `aten::_linalg_eigvals` via a call to `torch.linalg.eigvals` which is not implemented for XPU and requires `PYTORCH_ENABLE_XPU_FALLBACK=1`. I filed separate CC(xpu: implement aten::_linalg_eigvals for XPU backend (affecting HF Transformers v4.46.0 and later)) request for this operator to be added.
llama,SKIP llama for dynamic size testing,  CC(SKIP llama for dynamic size testing) Running Torchbench llama with dynamic size failed with  Skip this model for marking dynamic dim. ,2024-09-13T05:29:56Z,open source Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/135960, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
agent,[torchelastic][c10d] Fix store prefix race in rendezvous,"  CC([torchelastic][c10d] Fix store prefix race in rendezvous) 1. We want to take option 3 as discussed in  CC(TORCH_DISABLE_SHARE_RDZV_TCP_STORE=0 is not compatible with torchelastic restarts), so every time when we retry, we create a new TCPStore server first so that we don't need to append attempt count as prefix and avoid eventually TCPStore sync failure. (This is only for the TCPStore sharing enabled case)  2. We start a new server bound to an ephemeral port (i.e. 0) so it gets assigned to a free port. We then pass that downstream (trainer or c10d). By doing so, TCPStore is managed by the elastic agent rather than having a race condition on binding to a specific port in the trainer. 3. Then the port be broadcasted for dynamic_rendezvous. Only one more question, what do we do about the store created from (_create_tcp_store) torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py, are we ok with creating a duplicate TCPStore server? : D63396829",2024-09-13T05:00:56Z,oncall: distributed Merged Reverted ciflow/trunk release notes: distributed (torchelastic),closed,0,21,https://github.com/pytorch/pytorch/issues/135957,Send out the PR for RFC first and unit test is on the way.,"Looks like there are some existing test cases already, we just need to update them.",> We reuse the port from the newly created TCPStore server and first bind it to port 0 so that the system will allocate a new available one to us. I don't quite understand the point between reuse and allocate new one.," I think that comment is slightly out of date  in this PR we start a new server bound to an ephemeral port (i.e. 0) so it gets assigned to a free port. We then pass that downstream. We don't reuse the server or ports here. It's not really ""shared"" it's more that the TCPStore is managed by the elastic agent rather than having a race condition on binding to a specific port in the trainer"," yes, the port is for passing downstream to trainer or c10d. I updated the PR descriptions to make it more accurate.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m=""Diff reverted internally"" c=""ghfirst"" This Pull Request has been reverted by a revert inside Meta. To reland this change, please open another pull request, assign the same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).)", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator! Please try reimporting/rexporting the PR! Details for Dev Infra team Raised by workflow job , merge f 'landed internally'," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator! Please try reimporting/rexporting the PR! Details for Dev Infra team Raised by workflow job , merge f 'landed internally. one more try'," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator! Please try reimporting/rexporting the PR! Details for Dev Infra team Raised by workflow job ,Please note. This change was reverted internally. I suggest we close this PR and open a new one if we want to land this change,recreate a PR in https://github.com/pytorch/pytorch/pull/136768
yi,OpenReg: Fix issue when copying on the same device,Current copy gets wrong value when src and dst are both openreg. ,2024-09-13T04:47:35Z,open source Merged ciflow/trunk topic: not user facing,closed,1,5,https://github.com/pytorch/pytorch/issues/135956, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Add _addmm_activation to lower precision cast policy on AutocastCPU,"Fixes CC(autocast to float16/bfloat16 fails on transformer encoder (in `eval` mode)). Add `_addmm_activation` to lower precision cast policy on AutocastCPU. `_addmm_activation`  https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/transformer.cppL39 of `transformer_encoder_layer_forward` may throw `RuntimeError: mat1 and mat2 must have the same dtype, but got BFloat16 and Float` when autocast is enabled, as `_native_multi_head_attention` is put in lower data type cast policy https://github.com/pytorch/pytorch/pull/107674 and `_addmm_activation` may encounter mixed data types. ",2024-09-13T03:34:15Z,triaged open source module: amp (automated mixed precision) Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/135936, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[Inductor] simplify indexing_exprs in LoopBody._init_with_copy (#135574),"This PR fixes the performance regression of GroupNorm in Inductor on CPU. This PR uses `var_ranges` information to simplify `indexing_exprs` in `LoopBody._init_with_copy` to reduce occurrences of `FloorDiv` and `ModularIndexing` in the `indexing_exprs`. Pull Request resolved: https://github.com/pytorch/pytorch/pull/135574 Approved by: https://github.com/jgong5, https://github.com/lesliefangintel, https://github.com/jansel ",2024-09-13T03:31:06Z,open source ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/135935, merge,"PR targets release/2.5 rather than main, refusing merge request"
mixtral,[GPT-fast] Update compilation time target for Llama & Mixtral,  CC([GPTfast] Update compilation time target for Llama & Mixtral),2024-09-12T03:53:10Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/135817, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
finetuning,Pytorch Data Distributed Training across 8 H100 randomly gets stuck," üêõ Describe the bug Hi I have a script that runs with the DataParralell trainer on a machine with 8 H100 GPUs (aws p5 vm). When we run the script it starts to randomly get stuck forever at some iteration relatively late in the process (between 2000  4000th iteration). We start the script with the following command: accelerate launch src/model_back/healing/scripts/fine_tune_accelerate.py config_file src/model_back/healing/configs/mixtral_8x7b/config.yaml The gpus are only at 30% memory occupied and util is at 0%. The stack trace of the relevant processes looks the following:  pgrep P $(pgrep o accelerate) | xargs I {} pyspy dump pid {} Process 39: /usr/bin/python3.10 u src/model_back/healing/scripts/fine_tune_accelerate.py config_file src/model_back/healing/configs/mixtral_8x7b/config.yaml Python v3.10.12 (/usr/bin/python3.10) Thread 39 (idle): ""MainThread""     backward (torch/autograd/__init__.py:266)     backward (torch/_tensor.py:522)     backward (deepspeed/runtime/fp16/loss_scaler.py:63)     backward (deepspeed/runtime/zero/stage3.py:2213)     wrapped_fn (deepspeed/utils/nvtx.py:15)     backward (deepspeed/runtime/engine.py:1976)     wrapped_fn (deepspeed/utils/nvtx.py:15)     backward (accelerate/utils/deepspeed.py:166)     backward (accelerate/accelerator.py:2126)     training_loop (src/model_back/healing/scripts/fine_tune_accelerate.py:410)     training_function (src/model_back/healing/scripts/fine_tune_accelerate.py:540)     main (src/model_back/healing/scripts/fine_tune_accelerate.py:583)      (src/model_back/healing/scripts/fine_tune_accelerate.py:587) Thread 930 (idle): ""Thread1""     wait (threading.py:324)     wait (threading",2024-09-11T16:08:27Z,oncall: distributed,closed,0,3,https://github.com/pytorch/pytorch/issues/135703," the information you provide is not enough for further debugging, like what how do you wrap the model? What parallelisms do you use? You mentioned that you are using DataParallel, the the error message seems to fail in `all_gather_into_tensor` which is not used by DDP or DataParallel. Please identify and provide more detail if you think the issue is related to PyTorch DDP.",> deepspeed/runtime/zero/parameter_offload.py:339) this may be a deepspeed issue maybe better to close this and file an issue on https://github.com/microsoft/DeepSpeed,I'll close the issue for now. Feel free to reopen the issue if the root cause is suspected to be PyTorch distributed library related.
yi,Python's REPL trying to do invoke `.T` / `.mH` / `.mT` on 0-dim tensor when doing tab-completion for some reason (default install of Python on new Ubuntu 24.04)," üêõ Describe the bug Importing torch, defining a 0dim tensor as `t = torch.tensor(1)`, typing `t.` and pressing `Tab` key (I was looking to check if tensors natively support `.pad(...)` instance function  I hope it would get promoted at some point from `torch.nn.functional.pad` to `torch.pad`/instance function :) ) produces the following completely unexpected warning. The syntax error part comes out when I hit `Enter` trying to escape the warning. On the side note, is there any benefit for removing the warning and simply returning identity for 0dim tensors for `.mH` / `.mT`/ `.T`?   Versions Ubuntu 24.04, python 3.12.3, torch 2.4.0+cpu",2024-09-11T13:50:51Z,needs reproduction triaged release notes: python_frontend,open,0,8,https://github.com/pytorch/pytorch/issues/135693,Note that it doesn't repro on ipython !image Which autocomplete system do you use?,"you need to import rlcompleter first, I can repro","Well, I did not `import rlcompleter` manually, I guess it comes as some preimport in the Ubuntu 24.04 install of Python?",I can repro with this:  (running prints the warning),"Oh, it seems like it's because: (1) `rlcompleter` figures out what to show by `getattr`ing all of the possible attributes on `torch.Tensor` (2) `torch.Tensor.H` (and a few others) are technically derived properties, not methods. So doing `getattr(tensor, ""H"")` will actually invoke the transpose logic and return a tensor:  It looks like `rlcompleter` (which I guess ships with some installs of python?) loops over every attribute on `torch.Tensor` and tries to access each result, including any that are derived properties. So... this might make more sense to fix in that library? I'm not really sure what the right way to fix this is. But one idea: (1) tweak the attribute lookup to figure out which attributes are derived properties (2) for derived properties, just check `hasattr()` instead of `getattr` (3) only (lazily) invoke the `getattr()` if a user actually selects that option, at which point we would (correctly) emit the warnig","> which I guess ships with some installs of python? It appears that some installs might be all installs of Ubuntu 24.04 which will be quite many installs :) when people start to migrate more But yeah, it seems a bit crazy that it actually invokes the props when doing `Tab`, luckily for `.T`/`.mH`/`.H` it's just a transpose/conj bit mod, right? so in most cases it should succeed as is, right (even if rlcompleter is not fixed in Ubuntu 24.04)? My proposal would be removing the warning and letting `.T`/`.mH`/`.mT` return identity for scalar tensors. I would suggest that when it's implemented as prop, it should always succeed/do sth reasonable and not print any warnings > although not sure if these might at all be supported for various tensor subclasses or sparse tensors (a bit related old discussion:  CC([ux] Proposal to have t() === transpose(-1, -2), since batches are very frequent))  and I would propose to somehow add a test of iterating over all attributes, so that things like `t.`>`Tab` or `torch.`>`Tab` work fine on this rlcompleter/Ubuntu 24.04 python Also, `rlcompleter` appears to be a builtin module of Python: https://docs.python.org/3/library/rlcompleter.html",btw https://github.com/python/cpython/pull/27401 from rlcompleter should avoid this? So it might be you're using a very old version of rlcompleter in some cases that has this bug?,"maybe in reverse, as it actually tries to do `getattr`? And also this is open: https://github.com/python/cpython/issues/112821 > Ubuntu 24.04, python 3.12.3  I'm using whatever got installed with stock Python on new Ubuntu.  > Python 3.12.3 (main, Jul 31 2024, 17:43:48) [GCC 13.2.0] on linux This is a rather fresh version, I would say... I would suggest that PyTorch adapts in the meanwhile and removes warnings in T/mT/mH for scalar tensors and simply returns identity and also audits any other properties to not throw exceptions/produce warnings (or test that tab completion works)"
gpt,[inductor][cpu]GPTNeoForSequenceClassification AMP single/multiple thread static/dynamic shape default/cpp accuracy failure, üêõ Describe the bug GPTNeoForSequenceClassification AMP single/multiple thread static/dynamic shape default/cpp accuracy failure in 20240907 nightly release   Versions SW info               name       target_branch       target_commit       refer_branch       refer_commit                       torchbench       main       23512dbe       main       23512dbe                 torch       main       3bebc09be9845c0779f190489e8d4caa9e2653c8       main       c140fa1426603322a5a69ef91300f13489db5970                 torchvision       main       0.19.0a0+d23a6e1       main       0.19.0a0+d23a6e1                 torchtext       main       0.16.0a0+b0ebddc       main       0.16.0a0+b0ebddc                 torchaudio       main       2.5.0a0+97ed7b3       main       2.5.0a0+97ed7b3                 torchdata       main       0.7.0a0+11bb5b8       main       0.7.0a0+11bb5b8                 dynamo_benchmarks       main       nightly       main       nightly           Repro: inductor_single_run.sh bash inductor_single_run.sh multiple inference accuracy huggingface GPTNeoForSequenceClassification amp first static cpp Suspected guilty commit: https://github.com/pytorch/pytorch/commit/52c7c89ea486f8124b740ad6d2ee055812e913ab huggingfaceGPTNeoForSequenceClassificationinferenceampdynamicdefaultsingleaccuracycrash_guilty_commit.log ,2024-09-11T10:12:17Z,oncall: pt2 module: inductor oncall: cpu inductor,open,0,0,https://github.com/pytorch/pytorch/issues/135689
llm,Implementation of new state-of-the-art LLM optimizer: The AdEMAMix Optimizer,Implement issue CC(AdEMAMix: Adaptive Exponential Moving Average Mix Optimizer) Here is the link for the original paper. ,2024-09-10T19:43:51Z,module: optimizer triaged open source release notes: optim,closed,0,6,https://github.com/pytorch/pytorch/issues/135610,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: ovuruska / name: Oguz Vuruskaner  (a9ac046d21afc10ba62528351b8fc32944c1f4e2, 51587705cd15cd718777efc99259653bca2161fb, 749980f402754b3ce0a66a93c6c6667aeeb310e5)","Hi  , any updates on this?"," I'm not a maintainer, can't advise on this..."," No problem, thanks for letting me know."," I've read your code. There seems to be a lot of typos which make your implementation very different from the original paper. Please double check. For example, the calculation of beta3_t (mostly exponent) doesn't seem right. I think it should be: exponent = torch.log(torch.tensor(beta3)) * (                 (1  t_ratio) * log_ratio + t_ratio             ) Furthermore, bias_correction1 should not be applied to exp_avg2 if you want to make it aligned with the original paper, thus bias_correction1 should not be combined with lr in step_size. There could be more other issues that I haven't found.","Hi  , thanks for the feedback."
,AdEMAMix: Adaptive Exponential Moving Average Mix Optimizer," üöÄ The feature, motivation and pitch AdEMAMix combines Adam and EMA optimization techniques. It addresses slow convergence and poor generalization in large language models and noisy datasets using three beta parameters and an alpha parameter for flexible momentum and adaptive learning rates.  Alternatives We considered using existing optimizers with custom learning rate schedules and separate EMA implementations. AdEMAMix unifies adaptive learning rates and exponential moving averages in a single optimizer for better performance and easier implementation.  Additional context AdEMAMix showed promising results in large language models, computer vision tasks with noisy labels, and reinforcement learning with sparse rewards. Integration into PyTorch's optimizer suite would benefit researchers working on challenging deep learning problems.  It introduces new moment to existing AdamW optimizer. Implementation ",2024-09-10T19:43:07Z,module: optimizer triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/135609,"Hi  . IMO AdEMAMix is very interesting and I would like to try it for LLM training. To me (and I guess for many others) it is unclear what you exactly want to say by opening this GitHub issue. Are you requesting anybody to implement it? Do you suggest to contribute / implement it yourself? Perhaps you want to clarify this. Many thanks, Philip","Hi  , first of all I share same excitment with this method. I opened this issue to link implementation here P.S.: After our conversation, I also added PR link to issue description. "," Thanks for bringing this optimizer to our attention! PyTorch Core has some pretty strict rules when it comes to adding a new Optimizer or Module: https://github.com/pytorch/pytorch/wiki/DeveloperFAQihaveanewfunctionorfeatureidliketoaddtopytorchshoulditbeinpytorchcoreoralibraryliketorchvision At this time, the way to go may be to put it in a separate library and, of course, please don't hesitate to let us know if there is any low level feature that you're missing to implement this externally. Additionally, according to my naive understanding of this optimizer, it would be more generalized/scalable if the EMA portion was more easily applied to all optimizers if possible without needing to add more and more Optimizer classes to our codebase with duplication.", Appreciate the feedback. I'll consider your suggestion to reuse EMA. ü´°,I have made a different trials on AdEMAMix vs AdamW on same dataset and in same settings. Still the AdamW performed better than AdEMAMix.
transformer,LLaMA v3.1 on MPS backend breaks in BinaryOp mps::add_sub_lerp_template," üêõ Describe the bug Error: `failed assertion `[MPSNDArray initWithDevice:descriptor:isTextureBacked:] Error: total bytes of NDArray > 2**32'` Requires similar tiling approach to BinaryOp that was done for the batch matmul op here: PR CC(Enable batch matmul for result sizes > 2**32 the tensor can be split along batch axis) Reproduces with running LLaMA v3.1 on the current nightly PyTorch. Script to repro   Versions Collecting environment information... PyTorch version: 2.5.0.dev20240909 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 15.0 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: version 3.30.2 Libc version: N/A Python version: 3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Ultra Versions of relevant libraries: [pip3] numpy==2.0.1 [pip3] torch==2.5.0.dev20240909 [pip3] torchaudio==2.5.0.dev20240909 [pip3] torchvision==0.20.0.dev20240909 [conda] numpy                     2.0.1                    pypi_0    pypi [conda] torch                     2.5.0.dev20240909          pypi_0    pypi [conda] torchaudio                2.5.0.dev20240909          pypi_0    pypi [conda] torchvision               0.20.0.dev20240909          pypi_0    papi ",2024-09-10T18:25:32Z,high priority triaged module: mps,open,0,2,https://github.com/pytorch/pytorch/issues/135598, do you know how much memory one would need to reproduce it? I've tried to repro it machine with 32Gb of memory and all I got was: ,Hi  ! This was reproduced this on a 128GB machine but since its a 8B model it should run on a 64GB at least. TBH I was kinda assuming it would have worked on a 32GB machine as well but evidently that's not the case here.
transformer,Discussion about the testing methods in test_transformers.py after math backend always uses fp32," üêõ Describe the bug The current method to estimate the numerical errors in test_transformers.py is based on the difference b/w results from current precision and higher precision tensors https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL3107L3108 https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL3118L3122 https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL132L133 https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL141L145 However, with PR 128922, the math backend always casts the input tensors to fp32 from 16bit datatypes for better accuracy: https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/aten/src/ATen/native/transformers/attention.cppL787L801 Although PR 128922 fixed the tests on NV GPUs by raising the fudge factors, the assumptions of current numerical error estimation method do not hold anymore because `ref` and `_lp` tensors are fundamentally doing the same computation, and the errors between the two tensors are probably not measuring something meaningful for the purpose of estimating the SDPA's numerical errors. More concretely. I have inspected the output tensors of UT `TestSDPACudaOnlyCUDA.test_flash_attention_vs_math_ref_grads_batch_size_8_seq_len_q_2048_seq_len_k_2048_head_dim_8_is_causal_False_dropout_p_0_0_float16_scale_l1_enable_gqa_False_n_heads1_cuda_float16` The difference b/w the gold and reference is `4.76837158203125e07` for `grad_query`, and `3.0510127544403076e05`",2024-09-10T17:15:07Z,triaged module: sdpa,open,0,3,https://github.com/pytorch/pytorch/issues/135590,I wrote up my thoughts on the testing strategy here: https://github.com/pytorch/pytorch/pull/128922discussion_r1695568715 I think the goal/concencus from the sdpa_math change was  to provide a decomposed func that should more closely align to the casting behavior of the fused kernels.  I dont really understand why the AMD tests are orders of magnitude higher in terms of devotion  ,"> I think the goal/concencus from the sdpa_math change was to provide a decomposed func that should more closely align to the casting behavior of the fused kernels With new sdpa_math, the testing method should be improved as well because the only difference b/w `out` and `out_lp` is when the input tensors are casted from low precision to fp32 (note the golden is never fp64 unless the input is fp32). `out` does it in Python through `query_key_value_clones` and `out_lp` does it in sdpa_math's C++ code. Actually it's more surprising that `out` and `out_lp` are not matching precisely.","> Actually it's more surprising that out and out_lp are not matching precisely. This is the crux of it for me. Ideally every fudge factor should be 1. And potentially the fudge factor might need to be bumped up slightly due to fp operation reordering and iterative softmax, but that is not the case. I want to drill down on why that is "
yi,[Inductor] simplify indexing_exprs in LoopBody._init_with_copy,  CC([inductor] Relax the conditions for loop split)  CC([Inductor] simplify indexing_exprs in LoopBody._init_with_copy) This PR uses `var_ranges` information to simplify `indexing_exprs` in `LoopBody._init_with_copy` to to reduce occurrences of `FloorDiv` and `ModularIndexing` in the `indexing_exprs`. ,2024-09-10T08:09:12Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/135574," , could you please review this PR? Thanks!", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,xpu: runtime error in safetensors: Error while trying to find names to remove to save state dict,With: * https://github.com/pytorch/pytorch/commit/16c3b8f87cfa9cb5acee8104820baa389e7ee2bd * https://github.com/intel/torchxpuops/commit/12065904d4c3c870059d746eb0fb45a0459f1d6d * https://github.com/huggingface/accelerate/commit/4b4c036933f7c50fe3a7027b0380fcec53c6975e * https://github.com/huggingface/safetensors/commit/5db3b92c76ba293a0715b916c16b113c0b3551e9 Error running few Huggingface acceleration tests on XPU (CUDA works). One of the tests:  CC:      ,2024-09-09T23:39:30Z,triaged bug module: xpu,closed,0,3,https://github.com/pytorch/pytorch/issues/135550,"A mini reproducer:  This is an overflow issue related to `tensor.data_ptr()`, I will file a PR to fix this issue.","     The fix for this issue was unfortunately reverted in https://github.com/pytorch/pytorch/commit/538ee7bf600fcda19b7d720f6c91864c475c0673, see https://github.com/pytorch/pytorch/pull/135567issuecomment2371204797. As such this issue is not fixed and needs to be reopened. Can you, please, help to reopen it?","Hi,   It makes sense to reopen this issue. PR CC([Reland] Fix tensor.data_ptr() representation overflow) blocks the Triton CI. We are working on the new Triton commit acceptance test. Once the acceptance test passes, I will reland CC([Reland] Fix tensor.data_ptr() representation overflow) with a new Triton commit."
transformer,"BUG: `torch.cuda.is_available()` returns `False` in certain torch, CUDA and driver version"," üêõ Describe the bug Hi, I'm trying to create a Docker container with the following (**minimal reproducible**) CUDA `12.4.1` Dockerfile (host info: Driver Version: `550.107.02`     CUDA Version: `12.4`):  This just create a basic `nvidia/cuda:12.4.1cudnndevelubuntu22.04` image and install `conda` and `pip`. Then, I run the container with the following command:  Then, inside the container, I install the latest stable torch (`2.4.1`) by:  After that, I run the simplest torch cuda test by:  What I got is:  This is quite strange, since if I simlply turn to use the base image `nvidia/cuda:12.5.1cudnndevelubuntu22.04`, I can got the correct result that `torch.cuda.is_available()` returns `True`. Any advice will be sincerely appreciated, thx!  Versions  ",2024-09-09T17:18:26Z,module: binaries module: cuda triaged module: docker,open,0,5,https://github.com/pytorch/pytorch/issues/135508,"From your error message ""Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)"" it looks like there's a mismatch between the CUDA version in your container and the NVIDIA driver version on your host machine. To confirm, could you please check the NVIDIA driver version on your host by running: ",Did you run `collect_env` inside or outside of your container? It feels like container setup is somehow wrong in one of the case. Can you compile and run simple hello.cu in that container (or run `nvidiasmi` and share the output here) ,"> From your error message ""Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)"" >  > it looks like there's a mismatch between the CUDA version in your container and the NVIDIA driver version on your host machine. >  > To confirm, could you please check the NVIDIA driver version on your host by running: >  > `nvidiasmi` The result of `nvidiasmi` on the host is as follows: ","> Did you run `collect_env` inside or outside of your container? It feels like container setup is somehow wrong in one of the case. Can you compile and run simple hello.cu in that container (or run `nvidiasmi` and share the output here) >  >  The `collect_env.py` script is executed inside the container. I have compile and run the `hello.cu` and the result is as follows:  Nothing outputs. I've further inspect the error type by:  And the output is as follows:  It seems that something wrongs on the CUDA and driver version, but in my settings the container has CUDA 12.4 and the driver version on the host is 550, still strange...","Thank you guys, I have found the reason and solution: The root reason is that: > If hosts have new enough driver then there is no need to use compat lib. By removing `/usr/local/cuda/compat` inside the envvar `LD_LIBRARY_PATH`, the mismatch error won't occur. Thanks again! Ref: https://github.com/NVIDIA/nvidiadocker/issues/1256"
rag,Preserve storage offset on meta conversions.,"  CC(Preserve storage offset on meta conversions.)  CC(Set size, stride, and offset of functional tensor.)  CC(Forward more metadata functions to the inner XLA tensor.) This PR modifies the `to_meta` function present in the functionalization layer, so that it also preserves the storage offset. It does so by creating a base tensor, with enough elements for a view with the source tensor's metadata to live. Then, we follow that by calling `as_strided`, resulting in a tensor with the expected metadata. ",2024-09-09T15:48:38Z,open source Stale topic: not user facing module: functionalization,open,0,7,https://github.com/pytorch/pytorch/issues/135498,"Will add some tests. Also waiting for  to take a look at this stack of PRs. In summary, this is the state of things in this stack:  Mutating the XLA tensor metadata (sizes, strides, and storage offset) at the end of each functionalization kernel  Removed the `is_contiguous_` assertion that lived in `LTCTensorImpl::is_contiguous_custom`.      This is also necessary for fixing the PyTorch/XLA CI failure These changes mean that the XLA tensor will have metadata that reflects on what it would look like if we were running on PyTorch eager. That is, until `setup_size_properties` is called (e.g. in a `sizes_custom` call) after a generation bump. Which, in that case, the metadata will be reset to reflect the actual XLA data. While this does work, I think it's not the ideal solution. In my opinion, XLA tensors' metadata should reflect on eager execution, always. The main reason being that that would make a program written for eager work with PyTorch/XLA (e.g. even when the program contains operations such as `as_strided`, and other view operations). Also, I believe that's usually what the user expects. For more discussion on this, I have opened an issue on PyTorch/XLA.", can you please have a look at this PR?,"After offline discussion with , we decided to go ahead and merge this PR. We will have  (currently out) take a look at it afterwards.    XLA CI seems to be failing (as expected) due to the previous PR in this stack CC(Set size, stride, and offset of functional tensor.). I have a PR open https://github.com/pytorch/xla/pull/7998 for fixing that (not yet merged). What is the protocol for merging this stack of PRs?","Land xla, and then land pytorch with xla pin update, I think?  ",No concerns on the change.,"This PR is blocked by https://github.com/pytorch/xla/pull/8032. Basically, there are some CI failures related to `is_contiguous(memory_format)` calls.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,No error raised when trying in-place operation on tensor of shape[0] > 1 (single memory location)," üêõ Describe the bug The following code raises a RuntimeError, as expected (`RuntimeError: unsupported operation: some elements of the input tensor and the writtento tensor refer to a single memory location. Please clone() the tensor before performing the operation.`)  However, if `a` has shape (2, 2), the error does not appear. This causes an issue, as the result is NOT what you could expect due to the inplace operation acting on the same memory location. This is shown by the following example :   Versions PyTorch version: 2.4.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.2.01011awsx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.5.119 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 535.54.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             8 Online CPU(s) list:                07 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz CPU family:             ",2024-09-09T12:46:06Z,module: cpu module: error checking triaged module: advanced indexing module: partial aliasing module: edge cases,open,0,2,https://github.com/pytorch/pytorch/issues/135487,Marked for triage review because module: partial aliasing seems to have no observers? Not sure which label to assign.,"This may be related: we do catch trivial memory overlap errors, as catching all cases of overlap would be expensive. If we take on catching all cases of overlap, we would likely want to gate that behind a potential debugmode (, ) where perf is less an issue. "
yi,Applying torch.export on a model containing torch.fft.fft2 failed with the exception: illegal argument '-1', üêõ Describe the bug    Versions  ,2024-09-09T08:41:29Z,module: codegen oncall: pt2 oncall: export,closed,0,3,https://github.com/pytorch/pytorch/issues/135470,"This seems to only occur in predispatch export  this piece of code works:  But the root cause is in torchgen's parsing. In predispatch it goes down to `aten.fft_fft2.default` instead of `aten._fft_c2c.default`. This fails during args parsing here because to parse the arguments, it splits the signature by splitting by commas (here). But since the signature contains `dim=[2, 1]`, it incorrectly splits the `1]` out. Here's the entire stacktrace for debugging: ", here about using a real parser  do you have ideas for a more legit parser? ,"Not really in more detail than what's there. Define a grammar, write code to parse the grammer (either via a yacclike generator, or parser combinators). It's annoying because torchgen is very light dependency wise and I'd like to keep it that way but parsing usually requires a library. If grammar is valid Python can use Python's builtin parser but it is not. To fix illegal argument 1 you can just add a special case for it like our other special constants, shouldn't be hard."
yi,Why does using nn.DataParallel add a dimension after the nn.Parameter parameter? Then it results in a dimension mismatch and an error when multiplying.," üêõ Describe the bug class RFNO(nn.Module):     def __init__(self, out_channels, modes1, modes2):         super(RFNO, self).__init__()         self.out_channels = out_channels         self.modes1 = modes1         self.modes2 = modes2         self.scale = (1 / out_channels)         self.weights0 = self.scale * torch.rand(1, out_channels, 1, 1, dtype=COMPLEXTYPE)         self.weights1 = self.scale * torch.rand(1, out_channels, self.modes1, self.modes2, dtype=COMPLEXTYPE)         self.weights2 = self.scale * torch.rand(1, out_channels, self.modes1, self.modes2, dtype=COMPLEXTYPE)         self.weights0 = nn.Parameter(self.weights0)         self.weights1 = nn.Parameter(self.weights1)         self.weights2 = nn.Parameter(self.weights2)      Complex multiplication     def compl_mul2d(self, input, weights):         return torch.einsum(""bixy,ioxy>boxy"", input, weights)     def forward(self, x):         batchsize = x.shape[0]          Move weights to the same device as input `x`         weights0 = self.weights0.to(x.device)         weights1 = self.weights1.to(x.device)         weights2 = self.weights2.to(x.device)         x_ft = torch.fft.rfft2(x)         x_ft = x_ft * weights0          Multiply relevant Fourier modes         out_ft = torch.zeros(batchsize, self.out_channels, x.size(2), x.size(1)//2 + 1, dtype=COMPLEXTYPE, device=x.device)         out_ft[:, :, :self.modes1, :self.modes2] = self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], weights1)         out_ft[:, :, self.modes1:, :self.modes2] = self.compl_mul2d(x_ft[:, :, self.modes1:, :self.modes2], weights2)          Return to physical space         print(out_ft.shape)         x = torc",2024-09-09T03:55:15Z,triaged module: data parallel,open,0,1,https://github.com/pytorch/pytorch/issues/135462,"I'm running it on 4 GPUs. CUDA_VISIBLE_DEVICES=3,4,5,6 python3 /home/user/opc/lithobenchDP/lithobench/train.py"
llama,Partitioner error when functionalizing RNG state for LLAMA2 + LoRA 70b decoder layer graph," üêõ Describe the bug When doing pretraining of LLAMA2 70b model with LoRA with torch.compile in our environment, we saw an exception raised from partitioner.py while splitting the Joint graph to forward and backward graph.  Because the layer have dropout ops in the forward, and we have activation checkpointing enable on the decoder layer, the AOT needs to do functionalization of the RNG related ops. The issue happens in functionalize_rng_ops. Because the environment of running is complex and has many component, I dumped the graph modules (Joint graph and splitted forward module and backward module) right before functionalize_rng_ops. (These are not final forward module and backward module returning to the user) From these graphs, we know that the backward graph has no native_dropout_3 op which exists on forward. This breaks the current code assumption. (Maybe the graph split is right and this is the right case to consider)   Versions Collecting environment information... PyTorch version: 2.4.0a0+git95a8420 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.0112genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK avai",2024-09-09T02:02:26Z,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,open,0,1,https://github.com/pytorch/pytorch/issues/135459, 
llm,`torch._dynamo.exc.Unsupported: torch.* op returned non-Tensor bool call_method is_inference`," üêõ Describe the bug I'd like to compile a function √πsing `fullgraph=True` where the function call makes forward passes in a module which checks if we're in inference mode using either  `torch.is_inference_mode_enabled`, or `Tensor.is_inference()`, but I'm getting the above error. This is to disambiguate forward calls which are both in `torch.no_grad()`, but one forward pass should behave differently  in my case it's for using KVcacheing in a compiled generation function.  Error logs  and   Minified repro _No response_  Versions  ",2024-09-08T15:23:50Z,good first issue triaged oncall: pt2 module: dynamo,closed,0,10,https://github.com/pytorch/pytorch/issues/135439,"This is conceptually easy to fix; as long as we're guarding on the inference modeness of a tensor, it's safe to return a constant true/false here", Can I pick this up?,>  Can I pick this up?  Are you still planning on doing this work?,go for it,  Taking a look at this to unblock some work over at torchtune. I've added the following in `_dynamo/variables/tensor.py`:  Which doesn't throw any errors. Is this along the right lines? I'm curious what you meant by guarding on the inference modeness of the tensor.,"If you extract a Tensor.is_inference() from a Tensor, you need to guard on the inference modeness of the tensor. You can check if we already have a guard for this by sending a program a regular and an inference mode tensor and seeing if that triggers recompilation.","> If you extract a Tensor.is_inference() from a Tensor, you need to guard on the inference modeness of the tensor. You can check if we already have a guard for this by sending a program a regular and an inference mode tensor and seeing if that triggers recompilation. Yeah I couldn't see any recompilations when I checked tlparse. ",Which means we're not guarding on it. So that means you need to register a new guard when you access this (or change the tensor guards to include whether or not tensor is inference tensor or not),Thanks for the advice . Is this roughly the right place to register a new guard on the tensor https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/variables/builder.pyL951? EDIT: Would it be simpler for me to use `torch.is_inference_mode_enabled` by adding here? https://github.com/xinyuintel/pytorch/blob/b09ef280c20ad82889767a40a76d094328321e75/torch/_dynamo/variables/torch.pyL115. I get the following recompiles with this minimal repro:   Does this mean the guards on dispatch key set pick up the inference modedness?,"You should install a guard when you rely on some property. So the guard should be installed when is_inference is queried on the tensor.  is_inference_mode_enabled is different, and its own distinct thing from ""tensor is inference""."
transformer,AssertionError: Dict types must use ConstDictVariable," üêõ Describe the bug I am trying to execute following code                  However, I am getting following error.   Error logs   Minified repro _No response_  Versions  ",2024-09-06T06:52:09Z,high priority triage review oncall: pt2 module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/135329,"triage review to discuss if this should be hipri, given it is a crash on a transformers model","This is because of  is empty dict by default, and it hits a Dynamo bug when tracing the . I just submitted CC([Dynamo] Fix Huggingface PretrainedConfig get non const attr) to fix it. BTW, I recommend to just wrap  with . The model constructors and print calls should be outside of compile region."
llama,Explore VLLM trace when indcutor is used for meta-llama/Meta-Llama-3-8B ,"The trace is collected by modifying the profiling iteration in the vllm benchmark and generating   p.export_chrome_trace(""trace.json"") at file: vllm/benchmarks/benchmark_latency.py , func:     def run_to_completion(profile_dir: Optional[str] = None):  to access the trace checkout https://github.com/pytorch/pytorch/pull/135284 what i see by looking at the trace is  torch compiled region :    followed by bunch of those   zooming in  ",2024-09-06T00:21:43Z,triaged oncall: pt2 module: inductor vllm-compile,closed,0,1,https://github.com/pytorch/pytorch/issues/135288,I added the pot grad graph here for ref also  it can be observed here: https://github.com/pytorch/pytorch/pull/135284/filesdiff45ff5981c7bfd6a47e1228a533c5a0909502af6f2acdbf786962bd17c33e666c in file graph.txt
llm,compiled trace for vllm inductor,  CC(compiled trace for vllm inductor)  CC(Track base of FunctionalTensor in inference mode. ),2024-09-06T00:01:26Z,Stale,closed,0,1,https://github.com/pytorch/pytorch/issues/135284,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,SAC Estimator (Selective Activation Checkpointing) for estimating memory and recomputation time trade-offs.,"This PR adds a Selective Activation Checkpointing (SAC) Estimator, built on top of the `Runtime Estimator`, for estimating memory and recomputation time tradeoffs. It provides a `TorchDispatchMode` based context manager that estimates the memory and runtime tradeoffs of functions or `torch.nn.Modules` for SAC, using the `Runtime Estimator` CC(Runtime Estimator for estimating GPU compute time)  under the hood to support two estimation modes: 'operatorlevelbenchmark' and 'operatorlevelcostmodel' (roofline model). The SAC Estimator provides detailed statistics and metadata information for operators of each module, including greedy order for selecting operators to be recomputed/checkpointed and permodule tradeoff graphs. This estimator is designed to be used under FakeTensorMode and currently supports estimation of compute time and memory usage."" It's inspired from: XFormers SAC by   Endtoend example:    Example AC Stats for one of the transformer layers: !Screenshot 20241011 at 10 09 13‚ÄØPM Example AC Tradeoff for one of the transformer layers: !Screenshot 20241011 at 10 09 58‚ÄØPM Example AC TradeOff graph one of the transformer layers: !Transformer layers 3 cc:    ",2024-09-05T12:01:09Z,oncall: distributed open source Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,1,5,https://github.com/pytorch/pytorch/issues/135208, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalcuda12.4py3.10gcc9sm86 / test (default, 1, 5, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ","  merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,enable lazy_init for hpu,enables lazy_init for hpu device,2024-09-05T09:40:42Z,open source Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/135203,The committers listed above are authorized under a signed CLA.:white_check_mark: login: RafLit / name: RafLit  (1ab2de9536b1deb223cddb69cf82a01a1f6f7c95)," label ""topic: not user facing""",, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 4, 5, linux.g5.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 4, 5, linux.g5.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Undocumented fast pass behavior in nn.TransformerEncoderLayer causes failures in test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA)," üêõ Describe the bug This issue is related to  CC(DISABLED test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA)) Unit test test_transformerencoderlayer_cuda_float32 (in `test_nn.py`) performs the following tests: https://github.com/pytorch/pytorch/blob/a8611da86f42a442c3ab891a038af55440ccd8d0/test/test_nn.pyL12426L12450 This test expects all NaN output tensor if the `TransformerEncoderLayer` not using fast path. However, the determining factor of all NaN output is if the `SDPBackend.FLASH_ATTENTION` or `SDPBackend.EFFICIENT_ATTENTION` is used by the underlying SDPA operator within TransformerEncoderLayer However, the nonfast path of `TransformerEncoderLayer` does not explicitly disables the FA/MEFF attention: https://github.com/pytorch/pytorch/blob/a8611da86f42a442c3ab891a038af55440ccd8d0/torch/nn/modules/transformer.pyL896L927 Therefore, the actual backend selection logic depends on three parts of the whole codebase 1. The fast path/nonfast path logic in `TransformerEncoderLayer` 2. bool can_use_flash_attention(sdp_params const& params, bool debug) 3. bool can_use_mem_efficient_attention(sdp_params const& params, bool debug) 2 and 3 are undocumented, and FA may be used even if `TransformerEncoderLayer` should use nonfast path according to its document.  Suggested solution When calling `TransformerEncoderLayer._sa_block`, explicitly disables FA/MEFF backend by enclosing `x = self.self_attn` with `with sdpa_kernel([SDPBackend.MATH]):`  Versions  ",2024-09-04T21:32:27Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/135150,The Transformerencoderlayer takes 1 of two paths: 1.) Path that leads to SDPA 2.) FastPath: https://github.com/pytorch/pytorch/blob/96880148204fffd2f309b075672e8dfacbc21c90/aten/src/ATen/native/native_functions.yamlL14721 If it goes to 1.) there shouldnt be NaNs if it goes to 2 there might be NaNs depending on if it calls into SDPA or not I updated the efficient attention kernel on cuda to provide the updated masked out row semantic but did not make the same change to the rocm kernel. I think a better fix would be to update the ROCM kernel. Example of the reasoning can be found in the readme here: https://github.com/pytorch/pytorch/pull/133882
rag,[export] Expand coverage to more copied sym ops for unflattener.,Test Plan: buck2 test 'fbcode//mode/opt' fbcode//torchrec/ir/tests:test_serializer  rundisabled  Differential Revision: D62190172,2024-09-04T16:53:13Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/135119,This pull request was **exported** from Phabricator. Differential Revision: D62190172,This pull request was **exported** from Phabricator. Differential Revision: D62190172,I think eventually this should go away when we protect submodules from CSE.,This pull request was **exported** from Phabricator. Differential Revision: D62190172,This pull request was **exported** from Phabricator. Differential Revision: D62190172,This pull request was **exported** from Phabricator. Differential Revision: D62190172," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag, [torch/multiprocessing] Use multiprocessing.reduction.register ForkingPickler.register to register custom tensor and storage reductions,"Right now `multiprocessing.reduction.register()` is simply an alias to `multiprocessing.reduction.ForkingPickler.register()`  https://github.com/python/cpython/blame/main/Lib/multiprocessing/reduction.pyL56, but the toplevel `register()` function exposes less of the internal details of `multiprocessing.reduction` module.",2024-09-03T16:42:50Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/135030, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: linuxbinarylibtorchcxx11abi / libtorchcpusharedwithdepscxx11abitest / test Details for Dev Infra team Raised by workflow job ", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC( [torch/multiprocessing] Use multiprocessing.reduction.register ForkingPickler.register to register custom tensor and storage reductions), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,"[inductor][cpu] AlbertForMaskedLM, DebertaV2ForMaskedLM and timm_vision_transformer_large AMP AOT inductor crash issue"," üêõ Describe the bug AMP static shape default wrapper               suite       name       thread       accuracy       perf       reason(reference only)                       huggingface       AlbertForMaskedLM       multiple       X       X       AlbertForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 huggingface       DebertaV2ForMaskedLM       multiple       pass_due_to_skip       X       DebertaV2ForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 torchbench       timm_vision_transformer_large       multiple       pass_due_to_skip       X       timm_vision_transformer_large, Error: weights_offset must be aligned to 16K boundary                 huggingface       AlbertForMaskedLM       single       X       X       AlbertForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 huggingface       DebertaV2ForMaskedLM       single       pass_due_to_skip       X       DebertaV2ForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 torchbench       timm_vision_transformer_large       single       pass_due_to_skip       X       timm_vision_transformer_large, Error: weights_offset must be aligned to 16K boundary           Versions SW info               name       target_branch       target_commit       refer_branch       refer_commit                       torchbench       main       23512dbe       main       23512dbe                 torch       main       f9f85bfc0b5b63274fa3fdd22afb0a456abf53f4       main       920ebccca2644881ece4f9e07b4a4b4787b8f2b1                 torchvision       main       0.19.0a0+d23a6e1       main    ",2024-09-03T16:14:47Z,oncall: pt2 oncall: cpu inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/135027," when you create those issue, do you mind adding respective oncall labels?", ,">  when you create those issue, do you mind adding respective oncall labels? Hi , could you please help to add the permission to ? He don't have the permission now. Thanks.",verified the fix in https://github.com/pytorch/pytorch/pull/135205
transformer,"Error ""attn_bias is not correctly aligned"" When Offloading torch.nn.functional.scaled_dot_product_attention Using torch.autograd.graph.save_on_cpu()"," üêõ Describe the bug I'm encountering an issue while training a Llama2 model when I use torch.autograd.graph.save_on_cpu() to offload activation tensors to CPU memory. The problem arises specifically when I attempt to offload the attention layer. The implementation of self_attn is based on the LlamaSdpaAttention class from the transformers library. I have pinpointed the issue to the torch.nn.functional.scaled_dot_product_attention operator. If I offload this operation as follows:  I get the following error:  However, if I offload other parts of the LlamaSdpaAttention class without offloading this specific operation, the issue does not occur. What could be causing this misalignment error when offloading the attention layer, specifically with torch.nn.functional.scaled_dot_product_attention, and how can I resolve it?  Versions Collecting environment information... PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.30.1 Libc version: glibc2.31 Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0187genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 535.183.0",2024-09-03T07:32:42Z,triaged module: sdpa,open,0,0,https://github.com/pytorch/pytorch/issues/134995
rag,DISABLED test_serialization_array_with_storage (__main__.TestCuda),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_serialization_array_with_storage` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_cuda.py` ",2024-09-03T06:43:30Z,module: cuda triaged module: flaky-tests skipped,open,0,0,https://github.com/pytorch/pytorch/issues/134991
rag,DISABLED test_serialization_array_with_storage (__main__.TestCuda),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_serialization_array_with_storage` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_cuda_expandable_segments.py` ",2024-09-03T06:43:21Z,module: cuda triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/134990
transformer,"Bert model training is not running on cpu (X86/ARM) with torch.compile() mode, generating value error."," üêõ Describe the bug I was trying to run Bert model training on ICELAKE CPU with torch.compile mode then it is giving a value error, but when i am running it with eager mode then it is running fine without any error. this is the error which i am getting when running in compile mode. !image this is script which i am running. !image the similar behavior is observed in Graviton machine also.   Versions using torch==2.4, the requirement file which i am using. requirement.txt Code: ` import os, time from transformers import AutoTokenizer from transformers import DataCollatorWithPadding import evaluate import numpy as np import torch from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer from torch.profiler import profile, record_function, ProfilerActivity from datasets import load_dataset imdb = load_dataset(""imdb"",split=['train[:5]','test[:5]']) tokenizer = AutoTokenizer.from_pretrained(""bertbaseuncased"") def preprocess_function(examples):     return tokenizer(examples[""text""], truncation=True,padding=True) tokenized_imdb_tr = imdb[0].map(preprocess_function, batched=True) tokenized_imdb_ts = imdb[1].map(preprocess_function, batched=True) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) accuracy = evaluate.load(""accuracy"") def compute_metrics(eval_pred):     predictions, labels = eval_pred     predictions = np.argmax(predictions, axis=1)     return accuracy.compute(predictions=predictions, references=labels) id2label = {0: ""NEGATIVE"", 1: ""POSITIVE""} label2id = {""NEGATIVE"": 0, ""POSITIVE"": 1} model = AutoModelForSequenceClassification.from_pretrained(     ""bertbaseuncased"", num_labels=2, id2label=id2",2024-09-02T10:30:41Z,triaged oncall: pt2 module: dynamo,open,0,5,https://github.com/pytorch/pytorch/issues/134950,here's the formatted repro script for anyone who wants to copypaste. I can repro it locally: ,"This also repros with `compile(..., backend=""eager"")` for me, which looks like a dynamo bug",(triage review to assess priority  it's a hard crash not a graph break),"This is happening because of unfortunate interaction of this line  https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.pyL818 `torch.compile` wrapper on the nn.Module changes the codepath for the above line. To solve this issue, you can use inplace nn.Module compile wrapper, something like  This fails further in the tool chain but resolves atleast the weird error.",Removing high prio because of a workaround. devang please confirm if this helps.
llm,Enabling ATen Distribution kernels for AARCH64 using OpenRNG,"_**Enables Distribution ATen kernel for ARM CPU's using OpenRNG Library.**_ **_Enhances performance of bernoulli and exponential distribution operations for ARM CPU's._** (https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cpu/DistributionKernels.cpp) Contribution:  With this PR, we propose to add OpenRNG to the pytorch/third_party folder as a submodule and support static linkage of OpenRNG : (libopenrng.a)  USE_OPENRNG is implemented as cmake_dependent_option, which is available only on AARCH64, and by default is set to ON.  We also give user the flexibility to USE_SYSTEM_OPENRNG (similar to USE_SYSTEM_SLEEF), to dynamically link libopenrng.so to libtorch_cpu.so, if user sets the OPENRNG_ROOT_DIR environment variable before build. The enablement summary is captured below: !Screenshot 20241126 135226 This change targets performance enhancement for distribution kernels (Bernoulli and Exponential) for ARM. The primary use case targeted is DL/LLM training, where Bernoulli kernel is used in dropout implementation, to randomly drop neurons during weight updation to prevent overfitting. This change leverages OpenRNG, which uses neon backend for RNG (random number generator), and utilises same function calls as MKL VSL, which provides the observed performance boost over the native aten implementation route currently followed for ARM. **OP level performance boost:** Example: Sampled random input tensors of shapes from popular LLM models. !image **Model level performance boost:** Example: BERT training for single epoch. !Screenshot 20240903 111921 Note: GR Graviton 3 Perf Instance **Insights:** 1. For Bernoulli kernel using OpenRNG",2024-09-02T06:23:36Z,module: performance module: cpu triaged open source module: arm Stale release notes: build topic: not user facing no-stale,open,0,21,https://github.com/pytorch/pytorch/issues/134942,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: agrawalaka / name: Akash Agrawal  (80ff8568176a6d05b88e1a4983e6426b742dbbba, 250b5008513e69d3980cb485327c420b476a8233)"," label ""module: arm"""," label ""ciflow/linuxaarch64""",  Can you please review this PR whenever you get some time.,"Hello , with the current change, significant speedup is achieved even for a single core as well. As highlighted, with the change we are able to leverage neon implementation of RNG engines. The performance numbers for single core are also attached for tensor shapes from BERT model for bernoulli op, showing upto ~4x speedup for largest shapes.   With parallelization using multiple cores, we achieve further performance boost as already mentioned in the main comment. Also, for reference, a recent blog published on ARM website talks about leveraging openrng (part of ARMPL) for accelerating bernoulli operator in PyTorch:  https://community.arm.com/armcommunityblogs/b/toolssoftwareidesblog/posts/openrng2404","hardwick Hello Robert, can you please add the below CI label to this PR. **""ciflow/linuxaarch64""**",CC  . Can you please review this PR and provide your feedback whenever you get some time.,The armpl binary future releases will continue to have VSL RNGs from OpenRNG making this integration more stable. !image **Source:** https://community.arm.com/armcommunityblogs/b/toolssoftwareidesblog/posts/openrng2404,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", nostale,"  label ""module: arm""", help,"  label ""module: performance"""," label ""topic: not user facing"""," label ""release notes: build"""," label ""nostale""",**Previous Description of this PR :**   ,"The PR is currently updated to integrate OpenRNG instead of ARMPL, for performance enhancement of distribution kernels for AARCH64, which avoids having two BLAS libraries in PyTorch as suggested as feedback. The PR description is also updated to reflect the same. cc:   ","aka in addition to performance numbers, would it be possible to add some sort of benchmark about distribution properties of the proposed generators? And is there a mirror of the repository on GitHub? If not, can it be integrated into docker images, as Gitlab is often down","> And is there a mirror of the repository on GitHub? If not, can it be integrated into docker images, as Gitlab is often down Hi.  No, Arm do not have a mirror of this repository on GitHub. When you say ""Gitlab is often down"" do you mean the regular www.gitlab.com site, or the Arm specific one (gitlab.arm.com) that is run by Arm?  If it's the latter then any evidence you've seen of that being the case would be useful to gather to be able to talk to the relevant teams to get the quality of the service improved. Thanks!","Hi , it is important to point out with regards to distribution quality of RNG: 1. The issue  CC(`torch.multinomial` generates incorrect distribution) is still open. And currently buggy code exists in OSS for the MKL route. As this PR enables OpenRNG route utilising same APIs as MKL, the above integration suffers from the same quality issue until a fix is merged. 2. I applied the changes mentioned in this PR CC(Use VSL_BRNG_PHILOX4X32X10 for MKL exponential and bernoulli to avoid duplicated permutations under a low number of iterations) that fixes the issue CC(`torch.multinomial` generates incorrect distribution), and verified on both IL and graviton (with OpenRNG integration of this PR), and it seems to give 0 duplicates:   Essentially it does fix the randomness issue to a certain extent and atleast better than the current version in OSS, though not completely addressing the concern that seeding a local MT with a global MT would not be a best practice. 3. If either this fix https://github.com/pytorch/pytorch/pull/132603 or any other fix for the above issue (current discussion is in this thread: https://github.com/pytorch/pytorch/pull/132742) goes through, as OpenRNG utilises same APIs as MKL VSL, the accelerated route for ARM via OpenRNG would also be enabled via this PR https://github.com/pytorch/pytorch/pull/134942. With regards to the concern:  can refer the comment by  "
transformer,Pytorch Distributed DataParallel hanging, üêõ Describe the bug Basically I have 2 T4 GPU's and I am using kaggle when i try and load it in It shows some outputs that were put there for debugging but nothing else  Input   Output  Then it hangs and the CPU goes down (GPU doesn't change) and makes it not able to interrupt the execution It is super frustrating having 2 GPU's and only being able to use one  Versions N/A as using Kaggle ,2024-09-01T07:38:38Z,oncall: distributed triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/134920, May I ask how you are running your script? Are you using torchrun?,The output log you show seems to suggest that there is only 1 process being launched. Could that be the case and reason for the hang?,"Hey sorry, I got really busy,  I forgot about it for a while came back and eventually got it fixed, you can close this now, sorry for wasting your time On Mon, Sep 16, 2024 at 5:31‚ÄØPM Ke Wen ***@***.***> wrote: > The output log you show seems to suggest that there is only 1 process > being launched. Could that be the case and reason for the hang? > > ‚Äî > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >"
yi,Fakifying subclass tensors that don't implement have certain metadata," üêõ Describe the bug `RuntimeError: Internal error: NestedTensorImpl doesn't support sizes. Please file an issue.`  came up when I'm trying to save tensor metadata when compiling saved tensor hooks: https://github.com/pytorch/pytorch/pull/134754/filesr1737877026.  To correctly trace the unpack hook and the downstream uses of the unpacked tensor, we need the correct metadata for the returned Proxy. But not all subclasses seem to have these metadata, e.g. NestedTensor. Is there a proper FakeTensor representation for a NestedTensor? Is an alternative to move compiled autograd tracing to predispatch?    Versions main",2024-08-30T18:43:06Z,triaged module: nestedtensor tensor subclass oncall: pt2 module: compiled autograd,open,0,3,https://github.com/pytorch/pytorch/issues/134873, ,The de facto state of affairs right now is everywhere metadata is saved there are tests for nested tensor and special behavior otherwise. The long term fix is nested int.,"From the error message, it appears you're operating with a strided layout nested tensor (AKA NST). Note that only jagged layout nested tensors (NJTs) support `.shape` and torch.compile. For compiled autograd, imo it's okay to error out if an NST is encountered, since those aren't expected to work with torch.compile at all. > Is there a proper FakeTensor representation for a NestedTensor? NJT is a ""traceable wrapper subclass"" and thus its fake representation is a composition of fakeified components. There's no fake tensor repr for NST since it doesn't support torch.compile."
yi,Trying to build from source with use_flash_attention fails on windows due to fatal error C1189, üêõ Describe the bug Trying to build pytorch with Cuda from source with Flash Attention is failing due to fatal error C1198:    Versions  ,2024-08-30T12:47:41Z,module: build module: windows triaged module: sdpa,open,0,0,https://github.com/pytorch/pytorch/issues/134854
yi,[Dynamo] Support for proxying frozen dataclasses,"Fixes  CC(Dynamo treats dataclasses as UserDefinedVariable, prevents proxying into graph) Details: Previously Dynamo would treat dataclasses as UserDefinedVariables. This was nondesirable if we would like to proxy the value into the graph, which is needed for TensorSubclassMetadata. To rectify this, frozen dataclasses are now able to be proxied similarly to NamedTuples. We require the object to be frozen, because if arbitrary mutation were allowed, we would need to replay those mutations in the graph after construction of the object. For tracing construction of the variable, the generated `__init__` for the dataclass uses `object.__setattr__` because frozen dataclasses throw errors on the usual `__setattr__` invocation. With this treatment, no special handling is needed in dynamo for frozen dataclass construction.   CC([Dynamo] Support for proxying frozen dataclasses) ",2024-08-30T09:26:21Z,Merged ciflow/trunk module: dynamo ciflow/inductor release notes: dynamo,closed,1,2,https://github.com/pytorch/pytorch/issues/134846, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,Tune int8 AMX WoQ micro-kernel for CPU,This patch prevents performance regression against the default ATen implementation for LLaMA 3.1 int8 GPTQ WoQ workload. Uses AMX microkernel only if `M` >= `block_m` ,2024-08-30T05:37:44Z,open source Merged ciflow/trunk topic: performance topic: not user facing intel module: inductor ciflow/inductor,closed,0,16,https://github.com/pytorch/pytorch/issues/134832,", maybe it'd be better to disable the int8 AMX microkernel in case of a low `M`, instead of disabling it altogether. Even on Xeon Gen 5, AMX performs better if `M` >= `block_m`, but since `M` is variable, it's possible that a higher value of M may be encountered at runtime. Also this PR has a workaround for the RHEL libstdc++ segfault issue.", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `sanchitj/temporarily_disable_amx_int8_woq_microkernel` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout sanchitj/temporarily_disable_amx_int8_woq_microkernel && git pull rebase`)","Sorry, ! This branch is of PyTorch repo and not my fork, so there'd be a problem merging this one, and I created CC(Disable int8 WoQ AMX microkernel if m < block_m) instead.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 1, 5, linux.g5.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `sanchitj/temporarily_disable_amx_int8_woq_microkernel` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout sanchitj/temporarily_disable_amx_int8_woq_microkernel && git pull rebase`)", rebase b main, started a rebase job onto refs/remotes/origin/main. Check the current status here,"Successfully rebased `sanchitj/temporarily_disable_amx_int8_woq_microkernel` onto `refs/remotes/origin/main`, please pull locally before adding more changes (for example, via `git checkout sanchitj/temporarily_disable_amx_int8_woq_microkernel && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,DISABLED test_transformer_runtime (__main__.TestRuntimeEstimator),Platforms: rocm Failure introduced by https://github.com/pytorch/pytorch/pull/134243 This test was disabled because it is failing on main branch (recent examples). ,2024-08-30T04:18:10Z,module: rocm triaged skipped,open,0,3,https://github.com/pytorch/pytorch/issues/134824,"? If these are to be skipped more permanently, it would be better to migrate this to an incode skip.","Yes, we will be trying to resolve this within the next two weeks.",Cc  amd    I am disabling the accuracy check on the time estimation **in the test code** because it breaks when we upgrade CI GPUs from M60 to T4. 
yi,[Dynamo] propagate required_grad info while applying autograd function," üêõ Describe the bug Hi, we found the required_grad function won't be propagated while applying autograd function. Is it expected?  Error logs   Minified repro   Versions  ",2024-08-30T03:31:48Z,oncall: pt2 module: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/134820," Hi, may I have your inputs?",'t speculating the forward under no_grad. intel a fix is to add torch.no_grad inside the autograd.Function.forward, Thank you for the quick fix:)
llm, RuntimeError: Cannot set version_counter for inference tensor when get_data_attr is called ,"When running lama models with inductor in VLLM we get the error: Cannot set version_counter for inference tensor the part of the graph where the issue happens is the get_data_attr bellow  The python code self.weight.data,     ` ",2024-08-29T22:16:44Z,triaged oncall: pt2 module: inductor module: dynamo vllm-compile,closed,0,3,https://github.com/pytorch/pytorch/issues/134798,reverting this recent change fix the issue : https://github.com/pytorch/pytorch/pull/131403  ,Minimal repro: ,I'll put up a fix soon
transformer,hf_T5_generate torch.complie() failed with latest transformers==4.44.2,"Latest `transformers=4.44.2` packages from HuggingFace uses `torch.isin()` which prevents graph break in certain control flows. It also adds the condition branch to avoid `copy.deepcopy` which is not supported by `dynamo` See this PR: https://github.com/huggingface/transformers/pull/30788 However, when I update to `transformers==4.44.2`. I got the error when running with the following command. **Repro**  **Error log**   ",2024-08-29T20:29:26Z,,closed,0,5,https://github.com/pytorch/pytorch/issues/137133,Is this the only model having this issue?,Can you try to add a `decoder_start_token_id=None` in here? https://github.com/pytorch/benchmark/blob/e6251f1838701030872e276f94987b7a24a3d6c8/torchbenchmark/util/framework/huggingface/model_factory.pyL162,upstream issue https://github.com/huggingface/transformers/issues/30892,Opened the issue in HuggingFace transformers repo: https://github.com/huggingface/transformers/issues/33283,"This is more an issue for huggingface, as you have opened, or potentially torchbench. I'm going to close this issue feel free to open an issue in torchbench."
transformer,Fusion Benchmarking records 0.000ms," üêõ Describe the bug I was trying out the fusion benchmarking option in the inductor (`torch._inductor.config.benchmark_fusion = True`) and noticed that the triton benchmark of operators (unfused and fused) takes 0.000ms:  This seems to come from this benchmarking code:  source: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/triton.pyL3105L3121 Running the generated kernel does take > 0ms:  There are also other examples where the difference is starker (it reports 0.000ms for 0.03ms). A step through with the debugger suggests this discrepancy is due this line:  I can't really follow the motivation from the comments in the code. Can someone please clarify if this is really correct or a bug? Attached a minimal code example (compilation of HF BERT), generated kernel for the above.  Error logs **main.py**  **cretgkc37eour7kyivicvw23qmm2fa2degqjm3dggag5n3u5ysye.py**   Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.4.0+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.14 (main, Jun  3 2024, 16:55:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.5.044genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Quadro GV100 GPU 1: Quadro GV100 GPU 2: Quadro GV100 GPU 3: Quadro GV100 GPU 4: Quadro GV100 GPU 5: Quadro GV100 GPU 6: Quadro GV100 ",2024-08-29T15:21:25Z,triaged oncall: pt2 module: inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/134768, 
yi,"[c10d][MPI] Attempting to create a new group after MPI causes ""RuntimeError: Underlying Non-PrefixStore shouldn't be null""", üêõ Describe the bug    Versions main ,2024-08-28T17:54:17Z,oncall: distributed triaged module: mpi,open,0,5,https://github.com/pytorch/pytorch/issues/134700,  Huang ,Is the use case here you want to use MPI for cpu and NCCL for cuda? Does the following work as a workaround? ,"> Is the use case here you want to use MPI for cpu and NCCL for cuda? Does the following work as a workaround? >  >  This will not work in the case of hybrid network, where NCCL cannot cover all the ranks. The workaround for such network would be having MPI as a global process group over Ethernet and NCCL for subnetwork interconnected via IB: ",The error seems to occur when trying to fetch a global store to coordinate dumps. https://github.com/pytorch/pytorch/blob/951c21d6790334d57862e94a3f582ac724147a53/torch/csrc/distributed/c10d/ProcessGroupNCCL.cppL851L857 cc:  pio ,"> The error seems to occur when trying to fetch a global store to coordinate dumps. >  > https://github.com/pytorch/pytorch/blob/951c21d6790334d57862e94a3f582ac724147a53/torch/csrc/distributed/c10d/ProcessGroupNCCL.cppL851L857 >  > cc:  pio Correct, because the `underlying_store` was never set, nor it was ever created. In the PR CC([MPI] Create TCPStore for MPI PG when necessary) I am creating a new store if it is required."
transformer,DISABLED test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA),Platforms: rocm Broken by https://github.com/pytorch/pytorch/pull/133331 ,2024-08-28T16:08:18Z,module: rocm triaged skipped,open,0,5,https://github.com/pytorch/pytorch/issues/134687,Is there going to be a followup fix?,", I believe you will be filing a github issue to discuss the proposed fix?"," amd I'm not very familiar with the process, should I start an issue, or file a PR directly if I thought I found a solution?",Feel free to file an issue to discuss the fix and link to this one.,> Feel free to file an issue to discuss the fix and link to this one. Filed as  CC(Undocumented fast pass behavior in nn.TransformerEncoderLayer causes failures in test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA)) amd   
yi,[ONNX][DORT] Lazy-import `onnxruntime`,"Currently, if installed, `onnxruntime` will be imported when importing `torch._inductor` (which will be imported by some other library, e.g. transformerengine):  This issue breaks generated triton kernel because it imported torch, and unexpected runtime libraries as well. I've also added a test for this specific case under `test/onnx`, perhaps we should add more somewhere else? Related issue: https://github.com/huggingface/accelerate/pull/3056",2024-08-28T08:00:13Z,module: onnx open source Merged ciflow/trunk release notes: onnx topic: improvements,closed,0,28,https://github.com/pytorch/pytorch/issues/134662,onnxruntime shouldn‚Äôt be imported at torch import because the whole torch.onnx module is lazily imported. Did I miss anything?," importing some submodule in torch might trigger importing `torch.onnx`, e.g.:  where `from torch._inductor.utils import maybe_profile` can be in inductorgenerated triton kernels.",I see. Thanks!, merge, , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , Please run `lintrunner a` to fix lint issues. After which this can be merged. Thanks!, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 20 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.9clang10onnx / build  pull / linuxfocalpy3.12clang10 / test (default, 2, 4, linux.2xlarge)  pull / linuxfocalpy3.12clang10 / test (dynamo, 3, 3, linux.2xlarge)  pull / linuxfocalpy3.12clang10experimentalsplitbuild / test (default, 2, 3, linux.2xlarge)  pull / linuxfocalpy3.12clang10experimentalsplitbuild / test (dynamo, 3, 3, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `lazyimportonnx` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout lazyimportonnx && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.12clang10 / test (default, 2, 4, linux.2xlarge)  pull / linuxfocalpy3.9clang10onnx / test (default, 2, 2, linux.2xlarge)  pull / linuxjammypy3.9gcc11 / test (jit_legacy, 1, 1, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","unexpected imported `onnxscript` breaks some type annotation, I'm not sure about how the annotation part, trying to get more laziness to fix it.",Please do not alter the onnxscript imports. They should be fine,"> * pull / linuxfocalpy3.12clang10 / test (default, 2, 4, linux.2xlarge) > * pull / linuxfocalpy3.9clang10onnx / test (default, 2, 2, linux.2xlarge) > * pull / linuxjammypy3.9gcc11 / test (jit_legacy, 1, 1, linux.2xlarge) onnxscript somehow modified the typing extension and breaks these test. Previously, onnxscript is imported after onnxruntime, i.e. only if onnxruntime installed, and in the previous version of this PR (f797bc1), I tried to import onnxscript first, then check for onnxruntime later. So onnxscript will be imported at that point. After I just delete the onnxscript import, the tests pass.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  BC Lint  pull  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / build Details for Dev Infra team Raised by workflow job "," merge f ""macos failure unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llama,Memory leak starting with torch==2.5.0dev20240824 during training," üêõ Describe the bug Between 20240823 and 20240824 version, there is a serious memory leak. The latest nightly 20240827 still has the issue  Without compile, there is no memory leak. This is happening with torchtune, though I think I should be able to reproduce it in a small standalone script too. Will update if I can. Extra relevant info: finetune Llama3.18B with torchtune. Full BF16 training, with activation checkpointing.    Error logs OOM  Minified repro Install deps  Download Llama  Run the finetune job   Versions ",2024-08-28T02:15:07Z,high priority triage review oncall: pt2,closed,1,5,https://github.com/pytorch/pytorch/issues/134642,High priority because of the memory leak,A smaller snippet to reproduce memory leak ,If someone can run this under memory profiler that would help with triage,I ran memory profile on this snippet for the first 10 iterations  CC(Memory leak starting with torch==2.5.0dev20240824 during training)issuecomment2314063019 memory_snapshot.pickle.zip Seems like activations are not freed?  I can confirm that this only happens when activations checkpointing is enabled. So removing the following line will make memory leak go away. ,"nernst For the fullmodel compile memory leak issue, this should now be resolved by reverting https://github.com/pytorch/pytorch/pull/134272.  For TorchTune specifically, we've also switched to using perlayer compile (https://github.com/pytorch/torchtune/pull/1419), which also won't have the memory leak issue. Please feel free to reopen the issue if it still exists. Thanks!"
yi,Update `ForeachfuncInfo.sample_inputs_func` to yield scalars & scalarlists that are more friendly to test_meta,"for `test_meta.py` to see more ""PASSED"" instead of ""XFAIL"".  `pytest test_meta.py k ""_foreach_""` ran 6400 test cases and:  This PR: 4702 passed, 260 skipped, 73732 deselected, 1698 xfailed  main (92c4771853892193d73d87bd60eca4dc7efc51d8): 3906 passed, 260 skipped, 73732 deselected, 2494 xfailed",2024-08-27T05:44:55Z,triaged open source Merged ciflow/trunk release notes: foreach_frontend,closed,0,2,https://github.com/pytorch/pytorch/issues/134552, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,MiniCPM-V 2.6 transformer model returns nonsense tokens when running on MPS backend vs CPU backend.," üêõ Describe the bug When I run MiniCPMv2.6 model on my MacBook, the outputs look fine when using CPU backend, but they tend to contain nonsense English tokens or foreign language tokens when running on MPS backend. Here's the code to reproduce the issue:  Here are some example generations with MPS backend:  Here are example generations with CPU backend for the exact same input.   Versions  ",2024-08-27T00:31:53Z,triaged module: mps,open,0,0,https://github.com/pytorch/pytorch/issues/134534
rag,Prototype changes to create fake checkpoints with empty storages,  CC(Add torch.serialization.skip_data context manager)  CC(Prototype changes to create fake checkpoints with empty storages),2024-08-26T20:55:23Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/134503
llm,Too many warnging," üöÄ The feature, motivation and pitch As we know, many warning in torch will printed in every process, such as: https://github.com/pytorch/pytorch/blob/1ff226d88c70e66434ed9ce50deb5ae09e7e766c/torch/csrc/tensor/python_tensor.cppL78L81. It's OK when I use a few devices. But with the rise of LLM, we use a lot of cards for training, like tens of thousands of cards.I redirected the printed information to one file for better find the problem„ÄÇ At this time, a large number of repeated warnings filled this file, which brought a trobule to my work. So I wondered if there was a switch that would turn off the warning or print it just once throughout the training network.  Today, many models are started by torchrun or deepspeed, they will set `RANK` for every process. So printing only when the environment variable RANK is 0 maybe a simple and efficient way to do this?  Alternatives _No response_  Additional context _No response_ ",2024-08-26T11:24:21Z,oncall: distributed module: error checking triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/134458,Does something like this work? ,"You can set global logging level, see https://docs.python.org/3/library/logging.htmllogginglevels. If you use `torchrun` you can filter log on specific ranks by using `localranksfilter`, see https://github.com/pytorch/pytorch/blob/main/torch/distributed/run.pyL576","> You can set global logging level, see https://docs.python.org/3/library/logging.htmllogginglevels. >  > If you use `torchrun` you can filter log on specific ranks by using `localranksfilter`, see https://github.com/pytorch/pytorch/blob/main/torch/distributed/run.pyL576 Thanks for your advice. I test `localranksfilter` in v2.3.0, it worked."
chat,[Inductor] torchchat segmentation fault in int8 using max-autotune," üêõ Describe the bug Torchchat of int8 woq will encounter segmentation fault in https://github.com/pytorch/pytorch/commit/f951fcd1d7c4e991d1c9ef642fe7761d7104cda2 when using `maxautotune` in torch.compile. Reproducer: Basic README is in https://github.com/pytorch/torchchat?tab=readmeovfileinstallation . git clone https://github.com/pytorch/torchchat.git cd torchchat install dependencies ./install_requirements.sh  This will download CUDA version of Torch, you can wait, and finally it will install 20240814 cpu nightly Uninstall torch (default is 20240814 nightly, the above commit is not included). Install 20240815 nightly or build from source. TORCHINDUCTOR_FREEZING=1 python3 torchchat.py generate llama3.1 prompt 'Hello my name is' quantize '{""linear:int8"": {""bitwidth"": 8, ""groupsize"": 0}}' compile  numsamples 5 device cpu Need the following temp diff in PyTorch:  Need to use `maxautotune` in torchchat: https://github.com/pytorch/torchchat/blob/main/generate.pyL670   Versions",2024-08-26T07:04:57Z,oncall: cpu inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/134449,"Hi  , could you please take a look?","Sorry j, I had missed the notification. I'll fix it today. I suspect this issue is due to a change in the AMX microkernel code after my commit, which should have changed int8 WoQ case as well, but did not. Can you please also ping me on MS Teams if I miss a GitHub notification? Thanks","Hi fangintel, I tried debugging this issue on a PyTorch base commit dated Aug 23, but couldn't reproduce it, and also noticed that the ATen kernel was faster than the autotuning based AMX microkernel for all cases of decoding one token (which uses M = 1). I used 48 cores of a socket of an SPR machine. I also used `TORCH_COMPILE_DEBUG=1 TORCH_LOGS=""+inductor""` environment variables to verify compilation of AMX int8 WoQ microkernel. I used a machine with Ubuntu 22.04 that also used gcc 11.2.1. Moreover, the int8 WoQ GEMM for AMX only ensures that int8 weights are converted to BF16 before computation if activation is also BF16. It can't write beyond its temp buffer, so it can't result in a segfault, but if j indeed encountered an issue at her end with some different configuration, then my commit may have exposed some other issue with the autotuning GEMM microkernel for AMX. i.e. since _correlation doesn't necessarily imply causation_, it's possible that the use of int8 WoQ GEMM may have exposed some issue unrelated to my commit, so can you please explain why you're mooting reverting my commit, since doing so would only hide the potentially unrelated rootcause, but not fix it (and it'd continue being a problem for BF16xBF16 GEMM of the same input shapes.)? ",Also tested with the latest CPU nightly but couldn't reproduce the issue.,"fangintel , let's enable stock PyTorch CI testing for int8 AMX microkernel for Xeon gen 4 (SPR). If the UTs would pass on publicly available hardware, is there any reason to revert the PR?","The codegened C++ code works fine on a Xeon 4th gen machine running Ubuntu 22.04, but segfaults on a Xeon 5th gen machine running RHEL. If I use `alignas(128)` for the temp buffer on RHEL at line 282 of the python file, it works fine! This is probably due to a libstdc++ issue. We had to preload a custom libstdc++ to use the PyTorch nightly with RHEL, anyway.","Closing the issue after offline discussion, as it's related to a custom C++ runtime library."
rag,torch.compile does not work with test coverage tools," üêõ Describe the bug I'm using the https://github.com/nedbat/coveragepy tool to measure the test coverage. However, I find that it does not work with `torch.compile` . Simple example: Before compilation:  The test coverage is 100%:  After adding compilation:  The test coverage becomes strange:  The new generated file might be irrelevant, but how can we make it work for the `test.py` ?  Error logs _No response_  Minified repro _No response_  Versions 2.4.0 official release ",2024-08-26T06:47:37Z,triaged OSS contribution wanted oncall: pt2,closed,0,8,https://github.com/pytorch/pytorch/issues/134447,I'd probably propose instructing coveragepy to exclude these files," thanks for the response. I'm not worried about the generated file. I can easily exclude the generated file. However, I'm asking if we can do anything to fix the `test.py` . After `torch.compile` , it has one test coverage miss.","Oh if I had to guess, it's because we didn't actually execute the code, instead we executed to the compiled code, and coverage.py doesn't know how to map the generated code to the original source code. I guess a super fancy approach would be to teach Dynamo to write to coverage.py's coverage format as it Dynamo traces so that when we compile code it counts as covered. Not sure lol.","Yeah, I can confirm this is because dynamo doesn't actually run the original Python (it runs a compiled version of it).   If you just want to compute coverage, you can disable dynamo temporarily (env TORCHDYNAMO_DISABLE=1). Contributions are welcome to make torch.compile interoperate with coverage.","I admit test coverage with `torch.compile` is tricky, in the sense that dynamo does not execute the bytecode but just symbolically evaluate it. thanks for the response!","FWIW, I think it is actually feasible to interoperate with the hooks that coveragepy does, just definitely not a priority for us haha","I agree this is doable, more specifically this function: https://github.com/pytorch/pytorch/blob/ca03a14cf744337b94f5cb56e5a8f4738617c3a7/torch/_dynamo/symbolic_convert.pyL854L860 Would need some new logic that called some into a coverage API to say ""this line is covered"".  ",we need someone who understands `coverage` then :)
transformer,Make the arguments in `torch.func.functional_call` optional," üöÄ The feature, motivation and pitch > torch.func.functional_call(module, parameter_and_buffer_dicts, args, kwargs=None, *, tie_weights=True, strict=False) I do not see why `args` is a strictly required argument. In huggingface transformers, the dataloader will load the data in a dictionary, and usually we call the model forward function like this:  However, we cannot directly do this using `torch.func.functional_call`. I think it is more reasonable to make `args` an optional argument with default value `None`. If necessary, we could add a postcheck to ensure `args` and `kwargs` are not both `None`. I am not sure whether it has specific reasons to make `args` not optional, so I raise an issue instead of a PR.  Alternatives Currently, I am using the following workaround:  Use an empty tuple to serve as a placeholder of `args`.  Additional context This issue was raised when I was adapting the codes from the ACL 2024 best paper ""Why are Sensitive Functions Hard for Transformers?"" ",2024-08-25T02:25:50Z,triaged module: functorch,closed,1,1,https://github.com/pytorch/pytorch/issues/134408,"Seems reasonable, we'd accept a PR fixing this."
transformer,[torch.compile] Graphs differ between 2.4 and 2.5," üêõ Describe the bug Within TorchTensorRT, we use aot_export_joint_simple to get aten level graph for TensorRT compilation. This used to return a graph with weights registered as `get_attr` nodes in the graph in Pytorch 2.4. However, this behavior seems to be changed in 2.5. All the constants are now registered as placeholders. We observed this to potentially impact performance of models.  To reproduce:   Please find the attached gm_2.4.txt and gm_2.5.txt to notice the differences.  Is there a setting to disable this behavior ?  cc:     Error logs _No response_  Minified repro _No response_  Versions Pytorch 2.4 and 2.5.0.dev20240822+cu124 ",2024-08-23T20:50:00Z,triaged oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/134369
llm,Very large memory allocation by torch.linalg.qr," üêõ Describe the bug Working with LLMs, I got a strangely large CUDA OOM error. I was using torch.svd_lowrank, which again calls on torch._lowrank.get_approximate_basis. Below I paste the minimal viable code for reproduction, which only calls on torch.linalg.qr:  Full traceback:  1EB = 10^9 GB. That is an insane number.  When using q=36, it uses at most around 25GB on my GPU. For every larger q, the error is as above. Perhaps the number given by the traceback is wrong?  The reason for the Atensor being shaped like (size, 1) was another bug forgetting to reshape a flattened tensor, which is how I came over this. Using square tensors I do not get this error, for instance using A.shape = (2^16, 2^16) works fine.   Versions Collecting environment information... PyTorch version: 2.4.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.12.2  (main, Feb 16 2024, 20:50:58) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.15.0117genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA H100 PCIe GPU 1: NVIDIA H100 PCIe Nvidia driver version: 535.183.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        46 bits ",2024-08-23T12:48:41Z,triaged module: linear algebra,open,0,3,https://github.com/pytorch/pytorch/issues/134326,A simpler and easier way to reproduce: ,I get the feeling that this is caused by the returned value of `xgeqrf_bufferSize ` at https://github.com/pytorch/pytorch/blob/1dfb1052395d908ed6e67288c9357e16022da272/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cppL988 I think that it's returning a very large value whenever the number of elements doesn't fit in an int32_t...,"I can't check this ATM tho, but it would be a matter of 1. Printing USE_CUSOLVER_64_BIT. If it's False, then the issue is that you are trying to do 64 bit arithmetic in a 32 bit system and all goes wrong. The solution here would be to figure out how can you get a PyTorch with Cusolver with 64 bits. 2. If it uses 64 bits, print the size of `worksize_device` for the two given inputs, see if it is indeed the case that it's returning a number that's too large. For this, you'd need to compile PyTorch from source, of course."
yi,[compiled autograd] finish classifying tests,  CC([compiled autograd] fix saved tensor hook firing count)  CC(Reenable skipped compiled autograd eager tests)  CC(add compiled_autograd to programmatic set_logs API)  CC([compiled autograd] finish classifying tests)  CC([compiled autograd] match eager behavior for ctx.saved_variables)  CC([compiled autograd] match eager behavior for post acc grad hooks)  CC([compiled autograd] add config patching for certain eager tests)  CC([compiled autograd] match eager behavior for inplace detached activations) ,2024-08-22T23:06:44Z,Merged ciflow/trunk topic: not user facing module: inductor release notes: dynamo merging,closed,0,3,https://github.com/pytorch/pytorch/issues/134290, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."
rag,"Always emit end events even on failure, use thread local storage for stack","Summary: We should always emit an end event in a finally block so that if a unit test or job fails, the stack is still correct. Also, we use thread local storage for the stack, so that in multithreaded scenarios the stack will still be correctly added. Test Plan: Run benchmark and see that everything still works Run  With some extra logging to see that start events with the correct stack are emitted, and the end events are also emitted even though the test fails at runtime. Differential Revision: D61682556 ",2024-08-22T21:15:59Z,fb-exported Merged ciflow/trunk module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/134279,This pull request was **exported** from Phabricator. Differential Revision: D61682556,This pull request was **exported** from Phabricator. Differential Revision: D61682556,This pull request was **exported** from Phabricator. Differential Revision: D61682556," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Runtime Estimator for estimating GPU compute time,"This PR adds a basic Runtime Estimator for singledevice models. It estimates the GPU runtime in milliseconds using various estimation methods under the ``FakeTensorMode``. It provides a ``TorchDispatchMode`` based context manager that can estimate the eager runtime of PyTorch functions. It supports two estimation modes, benchmarking (`operatorlevelbenchmark`) and roofline cost modeling (`operatorlevelcostmodel`). For modules executed under this context manager, it agggregates the forward and backward operation runtimes and records their execution orders.      ",2024-08-22T17:00:33Z,oncall: distributed triaged open source Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,0,8,https://github.com/pytorch/pytorch/issues/134243,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: sanketpurandare  (ff46f47d012d27a199586d22d48fe0fc2b80984c, 9ce025489c5c4c0c56527106d737eb1e906201df, 5d0945123cef3675ed6727c219522734cfbe76c8, 5742c250f4b823c5c952bed6efdccd72bb145426, df96f5c0ab87582b27d6d1a493cd8e83ff947ca9, 5a6fa9a19443ece79113747154c903403d34cf70, 395d32eda34f4fbc42b4925cb13f71cb92b3241b)",the core looks good and left minor comments. look forward to unit test for santity checking. then we should be good, this PR is in draft mode now. Are you planning on changes? asking to make sure I am not blocking you ,>  this PR is in draft mode now. Are you planning on changes? asking to make sure I am not blocking you Added description and now the PR is ready for full review.,any chance to test on smaller tensors?  CI error is `distributed/_tools/test_runtime_estimator.py::TestRuntimeEstimator::test_conv_model_runtime  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.52 GiB. GPU 0 has a total capacity of 7.43 GiB of which 3.07 GiB is free`,> happy to stamp once we use less memory for conv test Fixed test params. , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,torch.utils.flop_counter.FlopCounterMode broke with torch-2.4," üêõ Describe the bug We have been successfully using `torch.utils.flop_counter.FlopCounterMode` up to torch2.4 and now it breaks and is impossible to use. It either warns: `The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch` or crashes with: `The Module hierarchy tracking is wrong. Report a bug to PyTorch` The relevant part of the trace is:  here is how we use it:  This happens with any HF transformers model I tried  Bert, Lllama, Mistral  clearly their models are perfectly fine. Rolling back to 2.3.1 restores the functionality. Questions: 1. what is the workaround to unblock us using FlopCounterMode with pt2.4+ 2. what is the longterm solution Suggestion: If I may suggest the warning/error is meaningless to the user. What does ""messed up mean""?  In particular this one:  https://github.com/pytorch/pytorch/blob/3c5b246d3c6461ef59fa38e8c4265b2c6b223412/torch/distributed/_tools/mod_tracker.pyL175C10L177C72 `""The module hierarchy tracking maybe be messed up. Please file a bug to PyTorch, if it is the case"" `how can a user tell if ""it is the case""?  Versions the problem happens on multiple setups  the only common ground is pt2.4.0 ,   ",2024-08-22T16:59:43Z,high priority triage review module: regression module: flop counter,closed,1,9,https://github.com/pytorch/pytorch/issues/134242,"Adding triage review as it feels like this belong to `oncall: profiler`, but it's not..",  can you please take a look? The module tracker warning is affected by your recent PR which is in the correct regression timeframe: https://github.com/pytorch/pytorch/commit/2e5366fbc04f819c62612e8c56fb786b43c1c67d,>   can you please take a look? The module tracker warning is affected by your recent PR which is in the correct regression timeframe: 2e5366f  https://github.com/pytorch/pytorch/blob/058302494cdf65fc59a1784fb9cea6c61e2be3f7/torch/utils/flop_counter.pyL5 Th FlopCounter does not use the `ModTracker` in `torch.distributed._tools`. It uses the one by   in `torch.utils`? https://github.com/pytorch/pytorch/blob/main/torch/utils/module_tracker.py cc:   ,"Thank you for clarifying,  that they are 2 very similar but different copies of code  I was just flagging that at the end of the OP that the error message isn't userfriendly or actionable  and that one is in torch.dist To repeat the specifics: ""The module hierarchy tracking maybe be messed up. Please file a bug to PyTorch, if it is the case"" how is the user to know ""if it is the case""? what does ""may be messed up"" mean?  so this could probably be improved as well to tell the user where the problem is or how to identify it or some such guidance.  In other words, I ended up asking for 2 related things in this OP 1. overcoming regression 2. while helping users to make sense of these cryptic warnings and errors Thank you.","Just to add a small voice of support here. We're also using the FlopCounter on nightly (2.5) and it's a great tool! But, I can confirm the same problem, concerning the warning `The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch`.  For what it's worth, the counted FLOPs seem correct, even with the warning? EDIT: Additional Info: For us, the warning (but not error) is always caused by turning on activation checkpointing (for which it would be amazing to count flops correctly)","In https://github.com/pytorch/pytorch/blob/main/torch/utils/module_tracker.py there is no warning at the moment. A similar error message is ""print""ed only when you enter the same Module multiple times. We should definitely update that message to say this. The is a (not much more helpful) error thrown when you exit the Module during the forward pass (ignored during the backward): https://github.com/pytorch/pytorch/blob/75c22dd8bf8801a6eaf9bbe30e08cf8c05ded6a1/torch/utils/module_tracker.pyL112 These messages are because tracking entry/exit of module is quite hard and brittle and we wanted to provide as much suggestion there. We can also remove the print/error if you think that they are not helpful. For this particular error, it will depend on the model and what is done. One known case (and why there is no error in backward) is during the backward when the input doesn't require gradients. I think there are issues with calling the same Module recursively and I know there are issues when reusing the same input for multiple modules (I need to prepare a PR for this but it is tricky if we want to catch Tensors inside data structures for inputs).","FWIW, I found the cause  it's gradient_checkpointing (`torch.utils.checkpoint`) that triggers it. When activated, the modules will be reentered and it should be normal/expected. Weirdly I lost the use case where this lead to the exception, at the moment I can only reproduce the warning repeated on all ranks.",", thank you for the quick fix  and adding it to 2.4.1 milestone!",Validated the fix seems to work for 2.4.1
yi,Python310 trying to run GFPGAN comes: ImportError: cannot import name 'sym_float' from 'torch' (unknown location)," üêõ Describe the bug Windows 11 Python310 trying to run GFPGAN ( python inference_gfpgan.py i inputs/whole_imgs o results v 1.3 s 2)  whatever I try it comes:  "" from torch import sym_float, sym_int, sym_max ImportError: cannot import name 'sym_float' from 'torch' (unknown location)""  Versions Any remedy?",2024-08-22T04:23:26Z,triaged,open,0,0,https://github.com/pytorch/pytorch/issues/134201
rag,storage_module && PyModule_Check(storage_module) INTERNAL ASSERT FAILED at DynamicTypes.cpp," üêõ Describe the bug If I run a PyTorch code for the first time using the `eval` function and the builtin functions are restricted, then an internal assertion failure occurs.   The problem does not occur if the same code was called once before running them in the `eval` function.  This is where the exception occurs: https://github.com/pytorch/pytorch/blob/v2.4.0/torch/csrc/DynamicTypes.cppL71 It seems that`PyImport_ImportModule(""torch.storage"")` failed because `PyImport_Import` invokes the `__import__()` function from the builtins of the current globals, and `__import__()` is not available in the current globals. This is understandable from a developer's point of view.  On the other hand, from the user's point of view, I feel that lazy loading is an implementation detail and it would be nice to behave as if everything was loaded when `import torch` is called outside of the `eval`.  Versions  ",2024-08-22T00:42:24Z,module: error checking triaged module: python frontend module: edge cases,open,0,1,https://github.com/pytorch/pytorch/issues/134183,"This is an interesting edge case, some of the PyTorch lazy initialization fails will fail when __builtins__ are not available, and this is exactly what happens..."
llama,[MPS] MPSNDArray error: product of dimension sizes > 2**32," üêõ Describe the bug This is related to  CC([MPS] MPSNDArray error: product of dimension sizes > 2**31) and  CC(`torch.multinomial` on MPS crashes with `Error: total bytes of NDArray > 2**32'`). The error is slightly different that the first (2**32 vs 2**31) and the second is specific to matmul. The machine has 64 GB of unified RAM and is not a physical limitation. I discovered this bug when attempting to perform LoRa on Llama 8B. I can replicate the error by using a similar script as shown with the first bug report.  I receive the error when running `python repro.py n_samples 17` or higher values.   Versions PyTorch version: 2.3.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.5 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: Could not collect Libc version: N/A Python version: 3.10.14 (main, Aug 19 2024, 18:36:18) [Clang 15.0.0 (clang1500.3.9.4)] (64bit runtime) Python platform: macOS14.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Ultra Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] onnx==1.16.2 [pip3] torch==2.3.1 [pip3] torchaudio==2.3.1 [pip3] torchvision==0.18.1 [conda] Could not collect ",2024-08-21T23:12:26Z,triaged module: 64-bit module: mps,open,0,7,https://github.com/pytorch/pytorch/issues/134177,Hi Roy! I think this issue got fixed with a tiling implementation for batch matmul that got just merged (https://github.com/pytorch/pytorch/pull/133430). I am able to reproduce the error with the version of torch you listed but when building locally with the above PR I'm seeing the test pass with values n_samples > 17. Would you be able to try out either locally building on top of the current main branch or wait for a day and try with the nightly torch build to verify if this also solves your issue?,Thanks   I'll wait for the nightly build to test and update you shortly.,"Note that Roy is using macOS  b i j', ones(16 * n_samples, 4096, 40, device='mps'), ones(16 * n_samples, 4096, 40, device='mps')).shape  Output:  RuntimeError: Tiling of batch matmul for larger than 2**32 entries only available from MacOS15 onwards ```","Thanks  that's a good comment here, I missed that detail. Indeed Roy there is the unfortunate aspect to the fix that the necessary API in the MPSNDArray that allows me to tile the computation is only available for MacOS15 and later.","> Thanks  that's a good comment here, I missed that detail. Indeed Roy there is the unfortunate aspect to the fix that the necessary API in the MPSNDArray that allows me to tile the computation is only available for MacOS15 and later. Oh I do see that now in the merged code. Well thanks for the update and good eye .","`macOS 15.0` has been released, and with it I get:  Roy have you had a chance to update to latest macOS yet? If so, do you still see an error?", Sorry I have not yet responded. This is for a work device and we have not yet upgraded. Should be shortly  I will respond soon.
yi,Improve error msg on _lazy_init() error,Reviewed By: hanzlfs Differential Revision: D61627609,2024-08-21T21:56:32Z,better-engineering fb-exported Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/134159,This pull request was **exported** from Phabricator. Differential Revision: D61627609,"Failing CI is failing on master, will merge", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend,"Description: 1. Quantize Linear Layer Weights to 4bits: Quantize the weights of the Linear layer to 4 bits, using symmetric quantization. Pack two 4bit weights into one uint8 container. Choose a quantization scheme (channelwise or groupwise), with the group size being a multiple of 32. 2. Prepare Quantized Weights, Scales, and Optional Bias: After quantizing, obtain the quantized_weights, scales, and groupsize. If the original Linear layer has a bias, prepare it as well. 3. Pack the Weights Efficiently: Use torch.ops.aten._dyn_quant_pack_4bit_weight to optimally pack the weights, scales, and optional bias. packed_weights = torch.ops.aten._dyn_quant_pack_4bit_weight(weight, scales_and_zeros, bias, groupsize, in_features, out_features) Input parameters should include: in_features and out_features (the same as the Linear layer‚Äôs corresponding parameters). 4. Perform Dynamic Quantized Matrix Multiplication: Use torch.ops.aten._dyn_quant_matmul_4bit to perform matrix multiplication with quantized weights. output = torch.ops.aten._dyn_quant_matmul_4bit(input, packed_weights, scales_and_zeros, bias, groupsize, in_features, out_features) Inputs required include: The input tensor, packed_weights, scales, bias, groupsize, and the in_features and out_features. Model Perf : 7B Transformer model: Prefill : 340 t/s Decode  : 40  t/s 2B Transformer model Prefill : 747 t/s Decode  : 80  t/s Tests: python test/test_linalg.py k test__dyn_quant_pack_4bit_weight Ran 1 test in 0.016s OK python test/test_linalg.py k test__dyn_quant_matmul_4bit Ran 8 tests in 0.077s OK python test/test_linalg.py k test_compile_dyn_quant_matmul_4bit Ran 8 tests in 11.454s Chang",2024-08-21T17:42:15Z,module: cpu triaged open source module: arm release notes: linalg_frontend module: dynamo ciflow/inductor ciflow/linux-aarch64,open,0,24,https://github.com/pytorch/pytorch/issues/134124,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: ng05 / name: Nikhil Gupta  (80d263f5343806d3169f5c180f23cfe975264bdf, 1c0ef38138d8621ab4a044860404fbf01c7504a6)"," Attention! native_functions.yaml was changed If you are adding a new function or defaulted argument to native_functions.yaml, you cannot use it from preexisting Python frontend code until our FC window passes (two weeks).  Split your PR into two PRs, one which adds the new C++ functionality, and one that makes use of it from Python, and land them two weeks apart.  See https://github.com/pytorch/pytorch/wiki/PyTorch'sPythonFrontendBackwardandForwardCompatibilityPolicyforwardscompatibilityfc for more info.  _Caused by:_   aten/src/ATen/native/native_functions.yaml"," label ""ciflow/linuxaarch64"" ""module:arm"""," label ""module:arm"""," label ""module: arm"""," label ""ciflow/linuxaarch64""", can you please help in merging this PR. ,">  can you please help in merging this PR. I believe prerequisite for merging is passing build and test for specific target and this PR clearly fails aarch64 build right now, see https://github.com/pytorch/pytorch/actions/runs/10616020253/job/29425421107?pr=134124  Perhaps it's just a matter or rebase, but in general, I would strongly advice against merging a relatively large change if one could not get a clear signal from the platform it targets. For comparison, here is result of the build/test for the recent trunk commit: https://github.com/pytorch/pytorch/actions/runs/10636148674/job/29487324878","Hello  , We are in the process to refactor the PR after your valuable inputs. We are planning to : 1. keep all the files , kernel interface and kernel implementation as it is in  aten/src/ATen/native/kleidiai/* 2. Plug kleidiai int4 matmul kernel with _weight_int4pack_mm_cpu() and modify  the signature of _weight_int4pack_mm_cpu() to fit our requirements 3. Plug kleidiai int4 weght pack kernel with _convert_weight_to_int4pack_cpu() and modify the signature of _convert_weight_to_int4pack_cpu() to fit our requirements 4. Register a new op in torchao for kai_pack_rhs_size() kernel and use it in torchao for quantization and packing step. The implementation will still be in pytroch but op will be registered in torchao 5. Add / Reuse Symmetric_quantization() in torchao . This will be used for functioning of  _weight_int4pack_mm_cpu() and _convert_weight_to_int4pack_cpu() ops 6. Modify/Add the existing tests for _weight_int4pack_mm_cpu() to accomodate kleidiai kernel and use quantization scheme from torchao directly Please let me know your thoughts on this and if this addresses all your concerns regarding PR.",The current CI failure are not observed on our local machine but it seems we are unable to detect sve vector lenght on the CI environment `Error in cpuinfo: prctl(PR_SVE_GET_VL) failed` to reproduce run :  `python c 'import torch; print(torch.__config__.show())' ` or `python bb  test_multiprocessing.py shardid=1 numshards=1 v vv rfEX p no:xdist usepytest x reruns=2 importslowtests importdisabledtests` We are observing these failures in other PRs (119571) as well: https://github.com/pytorch/pytorch/actions/runs/10777650193/job/29891036346?pr=119571step:20:6914 Source : https://github.com/pytorch/cpuinfo/pull/255 cc:    ,> The current CI failure are not observed on our local machine but it seems we are unable to detect sve vector lenght on the CI environment `Error in cpuinfo: prctl(PR_SVE_GET_VL) failed` to reproduce run : `python c 'import torch; print(torch.__config__.show())' ` or `python bb test_multiprocessing.py shardid=1 numshards=1 v vv rfEX p no:xdist usepytest x reruns=2 importslowtests importdisabledtests` >  > We are observing these failures in other PRs (119571) as well: https://github.com/pytorch/pytorch/actions/runs/10777650193/job/29891036346?pr=119571step:20:6914 >  > Source : pytorch/cpuinfo CC(Fix batch_first in AutogradRNN) >  > cc:   There's a bug using this new API  cpuinfo_get_max_arm_sve_length() on the HW which doesn't have SVE support. It fails on HW like Graviton2. A simple fix was required and there's already a PR in place for this. https://github.com/pytorch/cpuinfo/pull/258,"> A simple fix was required and there's already a PR in place for this. Merged, please update this PR to include new cpuinfo. Thanks","> > A simple fix was required and there's already a PR in place for this. >  > Merged, please update this PR to include new cpuinfo. Thanks I will make a separate PR https://github.com/pytorch/pytorch/pull/135857",We have added a new 4 bit matmul operator along with its reference implementation to work across all CPU architectures. The new Operator   dynamically quantizes the input tensor from fp32 to int8 and matmuls it with prepacked 4 bit groupwise weights. A group can be of a full channel as well. Operator internal details can be found here and here  : ,"Hi  , We have also provided the reference kernels here https://github.com/pytorch/pytorch/pull/134124/filesdiff3dcbe3f22985e9eb2b5ce8fbfb3c7222cef9a7ad1de4a44494995c98d4b4f77cR816 and https://github.com/pytorch/pytorch/pull/134124/filesdiff3dcbe3f22985e9eb2b5ce8fbfb3c7222cef9a7ad1de4a44494995c98d4b4f77cR975","Hi  , Thanks for the comment. We are looking into it."," I Internally rebased this PR to pytorch latest and observed big regression in the torch.compile() . I constantly see below warning when I compile the model  ``W1121 14:33:31.913000 20422 torch/_inductor/ir.py:6377] [0/0] aten._dyn_quant_matmul_4bit.default is missing a cshim implementation, using proxy executor as fallback`` Any idea what is going wrong here or if pytorch refactored something around aten ops + compile","> observed big regression in the torch.compile() What is the baseline? And did you profile where is time being spent? > ""is missing a cshim implementation, using proxy executor as fallback"" Hmm.. not super familiar with it, but per the comment at the raise site, I guess you already added this op in `torchgen/aoti/fallback_ops.py`?","> > observed big regression in the torch.compile() >  > What is the baseline? And did you profile where is time being spent? >  > > ""is missing a cshim implementation, using proxy executor as fallback"" >  > Hmm.. not super familiar with it, but per the comment at the raise site, I guess you already added this op in `torchgen/aoti/fallback_ops.py`? I profiled end to ende execution of 4 bit matmul op before and after. This data is average of multiple runs. matmul shape [M=1 , N = 4096 , K = 4096] `Compile mode` Before : 129 us  After :    232 us `Eager Mode` Before : 79 us After :    80 us I used the torch profiler to get a breakdown.  The actual 4 bit matmul operation is taking exactly the same time before and after (55 us) but the overall execution of the compiled graph is slower in compile mode in latest pytorch.  I have not added the my 4 bit matmul ops to `torchgen/aoti/fallback_ops.py` . I ran the `python torchgen/gen.py updateaoticshim` to update the list but cant see anything changing. Can you please point me to relevant PR/code to correctly enable this change?  I am unsure why this change is needed but I could see some recent development and PRs around this fallback mechanism. UPDATE: Added to fallback_ops.py and the issue seems to be fixed.", > The actual 4 bit matmul operation is taking exactly the same time before and after (55 us) but the overall execution of the compiled graph is slower in compile mode in latest pytorch. I am not sure how you ran it. Does the profile say where the time outside matmul is spent? Can we amortize any static costs those by running a bunch of times? .,  Please merge this PR. There are two unsuccessful checks but they are not related to this PR IMO.,">   Please merge this PR. There are two unsuccessful checks but they are not related to this PR IMO. 05 I believe we had this discussion a while back:   Why does is have to go into core vs AO?   PR title/description are very confusing to the users: this is indeed a new feature, so title should be something like `Add 4 bit dynamic quantization matmuls` and description should mention how to use new API and perf numbers that dynamic quantization delivers on supported platforms vs say static 4 bit quantization","> >   Please merge this PR. There are two unsuccessful checks but they are not related to this PR IMO. >  > 05 I believe we had this discussion a while back: >  > * Why does is have to go into core vs AO? > * PR title/description are very confusing to the users: this is indeed a new feature, so title should be something like `Add 4 bit dynamic quantization matmuls` and description should mention how to use new API and perf numbers that dynamic quantization delivers on supported platforms vs say static 4 bit quantization   I think we had an agreement on 1st point with meta team already?","> > * Why does is have to go into core vs AO? >   I think we had an agreement on 1st point with meta team already? Yes, when  reviewed PR early in November he was happy with new ATen operation for dynamic quantization to go to core as long as there is a reference implementation for setup without KleidiAI which was added by 05 now."
transformer,fix for fp16,"This PR is a replacement for https://github.com/pytorch/pytorch/pull/133085 for pushing a quick fix for RMSNorm. The original author is  Previous PR summary: Since FP16 has quite small dynamic range it is very easy to overflow while computing `at::pow(input, 2)` , and it happens in real world computation. I've tried to use `nn.RMSNorm` fused implementation instead of `LlamaRMSNorm` inside `transformers` implementation of Llama (`src/transformers/models/llama/modeling_llama.py`). It started to give wrong answers in Fp16 while still giving good in FP32. I figured out happens due to overflow while computing square of the input tensor. Original `LLamaRMSNorm` implementation upcasts input to fp32 to prevent this and give better numerical stability.  Proposed commit fixed the issue. FP16 in RMSNorm has to be treated in special way, to be usable in real world implementations. ",2024-08-21T13:41:15Z,oncall: distributed module: cpu triaged module: mkldnn open source Merged ciflow/trunk release notes: quantization topic: not user facing module: inductor module: dynamo,closed,0,32,https://github.com/pytorch/pytorch/issues/134106, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: trunk / macospy3arm64mps / test (mps, 1, 1, macosm113), trunk / macospy3arm64mps / test (mps, 1, 1, macosm114) Details for Dev Infra team Raised by workflow job ", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  BC Lint  pull  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , can you merge this? I dont think the failing test is coming from my PR, pinging again for quick resolution for merge. Unsure about failing tests. pretty sure they are unrelated to this PR, merge r ,"The failing tests look related to me `test_modules.py::TestModuleMPS::test_forward_nn_RMSNorm_mps_float16`, rebasing to check", started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `fixrmsnorm` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout fixrmsnorm && git pull rebase`)","This PR updates submodules third_party/cudnn_frontend, third_party/ideep If those updates are intentional, please add ""submodule"" keyword to PR title/description.", seems like 3 fails compared to 5 after the rebase," There are third_party changes (ideep and CuDNN) in this PR. Could you please clarify whether the changes are needed? If not, please sync the submodule when doing the rebase.",looks like a botched merge?,j  I have not done the rebasing. I think  has been trying something out. Can you fix  ?, can you fix the rebase?, I have fixed the blotched rebase., Perhaps you could just modify the reference function in the sample inputs function to match the semantic you implemented for fp16. I wonder why the test only fails on MPS though...,"> nit: could we use `OpMath` rather than hardcoding the `half`, `float` case here? e.g., >  > https://github.com/pytorch/pytorch/blob/bf7db4e4f9f07765e63d9ce1c98c5962bafe4608/aten/src/ATen/OpMathType.hL16  I dont understand how to use this. Can you give an example?",  I am done with the changes and all cases pass now. I am not sure how to use the OpMathType. Can one of you push changes for that?,  pinging again,guys can we get this merged?,"> > nit: could we use `OpMath` rather than hardcoding the `half`, `float` case here? e.g., > > https://github.com/pytorch/pytorch/blob/bf7db4e4f9f07765e63d9ce1c98c5962bafe4608/aten/src/ATen/OpMathType.hL16 >  >  I dont understand how to use this. Can you give an example? see e.g., https://github.com/pytorch/pytorch/blob/3daca187aa1a536e0b28449c6381dc3f7a4a3ae8/aten/src/ATen/native/cpu/ReduceUtils.hL213","  my knowledge of C++ isnt great  this doesn't compile, can you push a fix?", figured it out I think its fixed now., pinging again,failing tests seem unrelated   , any updates on this?
gpt,"[BF16]For GPT2ForSequenceClassification model on stock pytorch, Softmax cost time on pvc-1100 3x of A100", üêõ Describe the bug pvc1100 perf data                                                                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg      Self XPU    Self XPU %     XPU total  XPU time avg     of Calls                                                               aten::_softmax        13.07%       8.951ms        23.80%      16.306ms     135.885us      67.493ms       100.00%      67.493ms     562.441us           120 at::native::xpu::impl::DispatchSoftmaxForwardKernelF...         0.00%       0.000us         0.00%       0.000us       0.000us      67.493ms       100.00%      67.493ms     562.441us           120                                           aten::softmax         0.11%      74.286us        23.91%      16.380ms     136.504us       0.000us         0.00%      67.493ms     562.441us           120                                         piDeviceGetInfo         4.33%       2.968ms         4.33%       2.968ms       0.343us       0.000us         0.00%       0.000us       0.000us          8642                                 piextDeviceSelectBinary         0.90%     614.818us         0.90%     614.818us       0.320us       0.000us         0.00%       0.000us       0.000us          1920                                         piProgramRetain         0.20%     134.186us         0.20%     134.186us       0.280us       0.000us         0.00%       0.000us       0.000us           480                                          piKernelRetain         0.20%     136.735us         0.20%     136.735us       0.285us       0.000us         0.00%       0.000us       0.000us           480                   ,2024-08-21T02:16:02Z,module: performance module: xpu,closed,0,2,https://github.com/pytorch/pytorch/issues/134070," , thanks for the information. The PVC 1100 is a singletile card. But the CPU overhead, it seems like, does not make sense.",This is an xpu operator issue. And I will remove it.
transformer,DTensor sharding propagation for `scaled_dot_product_efficient_attention` should be more conservatively cached," üêõ Describe the bug The current `DTensor` sharding propagation caching policy for  `aten.scaled_dot_product_efficient_attention` (default) can result in silently incorrect gradients or trigger an IMA after cuda kernel launch (bt below) in mixed `require_grad` configurations. Testing a variety of TP `requires_grad` patterns (validating mechanistic interpretability probing use cases among others) revealed that if the first `aten.scaled_dot_product_efficient_attention` sharding propagation has `compute_log_sumexp=False` (e.g. first Transformer block attention layer has `require_grad=False`), subsequent `aten.scaled_dot_product_efficient_attention` ops may reuse the sharding propagation cache inappropriately (e.g., for any later Transformer block attention layer that may have `requires_grad=True`) resulting in the aforementioned silent gradient incorrectness or IMA. For instance, the silent correctness scenario can be observed with the test referenced below using a simple twodevice mesh and the default `n_heads=4` Transformer config. It occurs because:   The corrupted cache triggers an extra sharding operation. `scaled_dot_product_efficient_attention_strategy` sets the `logsumexp` `DTensorSpec` to `Replicate()` and shape `[bs, n_heads, 0]`, triggering an unintended redistribute and `_replicate_to_shard` before the local backward op.  Using the `num_heads_dim_sharding` strategy, the shard placement `mesh_dim` equals `sqrt(n_heads)` so the extra sharding operation yields a `local_tensor_args` tensor that will not will not raise an IMA when passed to our cuda kernel     Alternatively, if `mesh_dim sizes()[0] $13 = (const long &) : 8 (gdb) p log",2024-08-20T21:13:57Z,oncall: distributed module: dtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/134050,"cc: , l "," Thank you for helping identify the bug! We appreciate the report and the proposed solution. I agree to fix the bug we should simply adding `RuntimeSchemaInfo` for the two ops (efficient attention fwd, flash attention fwd). I'd suggest submitting a separate PR for this issue, both because it seems to be an independent problem, and because we need more time to digest the other issue. I can help review your PR. Since personally I believe this is a simple fix, it's up to you whether you'd like to merge without further testing, or modify the existing test cases/file for the two attentions in https://github.com/pytorch/pytorch/blob/main/test/distributed/_tensor/test_matrix_ops.py."
yi,`torch.compile` crash while trying to remove a node," üêõ Describe the bug I am trying to compile a model where part of the computed tensors are discarded. Eager mode runs successfully while the compiled model crashes. Removing the accuracy calculation line from the `train_step` function avoids the crash, see the workaround below:   Error logs   Minified repro Run https://github.com/dmlc/dgl/pull/7722 with after installing pytorch geometric and dgl.   Versions Collecting environment information... PyTorch version: 2.4.0a0+3bcc3cddb5.nv24.07 Is debug build: False CUDA used to build PyTorch: 12.5 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.30.0 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.0117genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.5.82 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB Nvidia driver version: 550.54.15 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.9.2.1 /usr/lib/x86_64linuxgnu/libcudnn_adv.so.9.2.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn.so.9.2.1 /usr/lib/x86_64linuxgnu/libcudnn_engines_precompiled.so.9.2.1 /usr/lib/x86_64linuxgnu/libcudnn_engines_runtime_compiled.so.9.2.1 /usr/lib/x86_64linuxgnu/libcudnn_graph.so.9.2.1 /usr/lib/x86_64linuxgnu/libcudnn_heuristic.so.9.2.1 /usr/lib/x86_64linuxgnu/libcudnn_ops.so.9.2.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPAC",2024-08-20T02:20:26Z,triaged oncall: pt2 module: inductor,open,0,1,https://github.com/pytorch/pytorch/issues/133942,I have updated the issue with the run of the minified reproducer script.
agent,Torch Distributed Elastic has no way to address non-retriable errors," üöÄ The feature, motivation and pitch Looking at https://pytorch.org/docs/stable/elastic/errors.html, I don't see any way to avoid restarts when a nonretriable user error has happened. It is often important for user errors to have varieties as well. For instance, user's training module may discover an error at runtime that is nonretriable (say a node level issue that requires an orchestration level restart). There is currently no way to tell elastic agent that the exception should not be retried.    Alternatives Wait until we run out of retries.  Additional context None ",2024-08-19T17:51:33Z,oncall: distributed triaged,open,0,5,https://github.com/pytorch/pytorch/issues/133877,Hey  pio can you help answer?,I don't believe we have any way to do this currently. 's technically feasible to do. Would require: 1. adding a special error type to indicate a nonretryable error and propagate it through the error file https://github.com/pytorch/pytorch/blob/b567ca0f515f5910535cd5b9c25130a68b1e0836/torch/distributed/elastic/multiprocessing/errors/error_handler.pyL76 2. indicate to scheduler that things need to restart. The easiest way is to fast fail and have the scheduler assume any failure is hard. If we need to do a distributed exit we'd need to close the rendezvous which may require a bit more work You could also use a dirty hack and have the worker process SIGTERM the agent and thus trigger an orchestration level restart,"We worked around this by using a modified elastic agent. However, I am a bit surprised that this is not supported by PyTorch as the concept of nonretriable errors are very fundamental.", you are more than welcomed to improve pytorch to sending out a PR to make it better.,"This requires a mini design in my opinion. If there is someone who is more familiar with the internals of PyTorch distributed, who is willing to pair with me, I don't mind putting cycles on it."
yi,"Dynamo treats dataclasses as UserDefinedVariable, prevents proxying into graph","One restriction around authoring subclasses today is that if you want to construct a subclass ingraph: (1) if you have some constant metadata on your subclass in the form of a `NamedTuple`, this is allowed. Dynamo can proxy the constructor into the graph, with a `NamedTuple` as one of the input nodes to the constructor (2) Using a `dataclass` instead of a `NamedTuple` will break. This is because dynamo ends up treating the `dataclass` as a `UserDefinedObject`, and dynamo graph breaks when we try to proxy it into the graph. This originally came from FSDP2 + NF4 repro here. I tried making a smaller repro below:  ",2024-08-19T14:27:07Z,triaged module: __torch_dispatch__ tensor subclass oncall: pt2 module: aotdispatch module: dynamo module: pt2-dispatcher dynamo-tensor-subclasses,closed,0,2,https://github.com/pytorch/pytorch/issues/133858,'re interested?," I will look into this, qq will this just be limited to data classes and named tuples? If so I think we can just handle both of these cases, I'm curious if you foresee someone using any other structures here to represent constant metadata. "
llama,Inductor-CPU int4 GPTQ WoQ GEMM AVX512 micro-kernel & template for auto-tuning," Summary AVX512 microkernel & template for int4 GPTQ WoQ GEMM added. Adapted from existing ATen kernel by also blocking on K dimension, and also using cacheblocking, as well as perthread blocking (the latter two are exclusively done by the CPP template).  Benchmarking data: Tabulated perf data that compares against the ATen kernel at https://gist.github.com/sanchitintel/cfe03a701ee595360a0dfb4866ed02bc?permalink_comment_id=5177217gistcomment5177217 For LLaMA 3.1, on Intel Xeon 4th gen, 48 physical cores of one socket (and Intel OpenMP, tcmalloc preloaded)   Future work Split Zero Point & Scale tensors AMX microkernel ",2024-08-19T08:55:22Z,module: cpu triaged open source release notes: quantization release notes: linalg_frontend intel module: inductor module: dynamo ciflow/inductor ciflow/slow,open,1,2,https://github.com/pytorch/pytorch/issues/133846,"Hi , the failures are unrelated. Can you please review the microcode template again? Thanks","Hi , before the PyTorch conference, you had said you'd rereview this PR after the PyTorch Conference. But we didn't get a chance to discuss it later  I guess AMX would help when M would be large, so I'll add an AMX microkernel in a separate PR. For GEMV, though, the current microkernel doesn't perform as well as the ATen kernel E2E, but performs better in GEMM benchmarking, due to different cache behavior. I'll modify it to perform better"
chat,pytorch/aten/src/ATen/cuda/CUDABlas.cpp:1422:28: error," üêõ Describe the bug I am trying to compile pytorch on the PSC Bridges2 system and getting the following error using make in the build directory. /jet/home/gcommuni/projects/AIProteins/pytorch/aten/src/ATen/cuda/CUDABlas.cpp:1422:28: error: 'CUBLASLT_MATMUL_DESC_A_SCALE_POINTER' was not declared in this scope; did you mean CUBLASLT_MATMUL_DESC_BIAS_POINTER'?  1422  /usr/bin/which ttyonly readalias readfunctions showtilde showdot $@ } BASH_FUNC_module%%=() {  eval $($LMOD_CMD bash ""$@"") && eval $(${LMOD_SETTARG_CMD::} s sh);  ret=$?;  /usr/share/lmod/8.2.7/libexec/log_modules ""$@"";  return $ret } BASH_FUNC_scl%%=() {  if [ ""$1"" = ""load"" o ""$1"" = ""unload"" ]; then  eval ""module $@"";  else  /usr/bin/scl ""$@"";  fi } BASH_FUNC_ml%%=() {  eval $($LMOD_DIR/ml_cmd ""$@"") } _=/usr/bin/env  Versions  python3 collect_env.py Collecting environment information... PyTorch version: 2.4.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Red Hat Enterprise Linux release 8.8 (Ootpa) (x86_64) GCC version: (GCC) 13.2.1 20240113 Clang version: 15.0.7 (Red Hat 15.0.71.module+el8.8.0+17939+b58878af) CMake version: version 3.20.2 Libc version: glibc2.28 Python version: 3.9.13 (main, Aug 25 2022, 23:26:10)  [GCC 11.2.0] (64bit runtime) Python platform: Linux4.18.0477.27.1.el8_8.x86_64x86_64withglibc2.28 Is CUDA available: True CUDA runtime version: 12.4.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla V100SXM232GB Nvidia driver version: 545.23.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_6",2024-08-18T11:46:16Z,needs reproduction module: build module: cuda triaged,open,0,1,https://github.com/pytorch/pytorch/issues/133808,"Looks like some sort of a weird setup issue: on one hand you are building for CUDA12.4, but you also have cudatoolkit11.8 installed as part of the condaforge package"
chat,"ARM64! - Problem installing torch on Notebook with Windows 11 with Snapdragon CPU (ASUS Vivobook S15) with Python 3.12.5 -> ""No matching distribution found for torch"""," üêõ Describe the bug Just not yet supported? Or different way to install? Perhaps nightly is helpful?  Just not yet supported? Or different way to install? Perhaps nightly is helpful?  Versions (wget not available by default, curl used, but does not really matter...)  ",2024-08-18T09:12:25Z,module: windows module: collect_env.py triaged module: arm,closed,0,4,https://github.com/pytorch/pytorch/issues/133804,"WMIC is meanwhile an optional feature. It was not installed by default on my system, but I added it. Then I am able to deliver also debug info queried via wmic: https://www.elevenforum.com/t/addorremovewmiccommandfeatureinwindows11.5119/ ",Indeed there are no PyTorchonWindowsARM binaries yet. ,"Hey , is it okay to close this issue for now? We are currently working on building pytorch for Windows on ARM64 and if you are interested in this topic please follow this issue instead:  CC(Readily available python wheels for windows ARM). ","Hi , many thanks for the feedback and linking to the other issue. I agree to close this entry here and continue there."
transformer,PyTorch torch.compile inductor fails to compile yolo.," üêõ Describe the bug Reproducer:  Error:  This model compiles successfully with torch<=2.3.1.  Versions Collecting environment information... PyTorch version: 2.4.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 20.0.0git (https://github.com/llvm/llvmproject.git 0795ab4eba14b7a93c52c06f328c3d4272f3c51e) CMake version: version 3.29.3 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A3000 12GB Laptop GPU Nvidia driver version: 536.45 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 46 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 24 Online CPU(s) list: 023 Vendor ID: GenuineIntel Model name: 12th Gen Intel(R) Core(TM) i712850HX CPU family: 6 Model: 151 Thread(s) per core: 2 Core(s) per socket: 12 Socket(s): 1 Stepping: 2 BogoMIPS: 4838.39 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnow",2024-08-17T10:37:36Z,triaged oncall: pt2 module: dynamic shapes module: inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/133759
llm,sympy.core.relational.StrictLessThan throws Invalid NaN comparison one VLLM model. ,I could not generate repo for this trivially  (other than the vllm unit test)  pytest s tests/models/test_models.py::test_models[96floatbigscience/bloom560m] another repo:  I noticed that the following test fails with the same error: python test/inductor/test_torchinductor.py CpuTests.test_dtype_sympy_expr_cpu  stack full:    ,2024-08-16T23:06:24Z,high priority triage review triaged oncall: pt2 module: dynamic shapes vllm-compile,closed,0,6,https://github.com/pytorch/pytorch/issues/133735,I noticed that the following test fails with the same error: python test/inductor/test_torchinductor.py CpuTests.test_dtype_sympy_expr_cpu ,This self contained repro also triggers this error https://gist.github.com/ezyang/8a137eabebb4abf174c06f951439a6f5,"This situation seems to occur when one of the arguments has an integer range, while the other has a float range, as 0 * oo results in NaN. It's not clear to me why this occurs though. First range is [0, int_oo], second range is [oo, oo] ",The problem is floordiv doesn't correctly preserve type when 0 in b: ,"It's actually very difficult to trigger this synthetically. I thought this would work:  But it does not, because before we do static evaluation we shift the value ranges so the variables in question are positive, so [0, int_oo] becomes [1, int_oo]","Oh, I have to upgrade sympy to trigger this"
llm,[torchbind x PT2] Handle vllm's ScalarType,"There's a use case for TorchBind objects that are just constant metadata. There's two options here:  we develop some mechanism for unflattening/flattening the object so that it does not appear in the schema  we add an option to mark a TorchBind class as sideeffectfree. If it is sideeffectfree, then we don't need to generate effect tokens for it.",2024-08-16T15:32:21Z,triaged module: torchbind vllm-compile,open,0,0,https://github.com/pytorch/pytorch/issues/133689
yi,Unexpected Termination in PyTorch When Applying torch.atan to Large Tensors," üêõ Describe the bug I am experiencing an issue while running two pieces of PyTorch code that seem to behave differently despite their apparent similarity. The first piece of code works as expected:  This code generates 10,000 random numbers using torch.rand(10000) and then applies the torch.atan function to each of these numbers in a loop. It runs without any issues. However, when I try to execute the following code:  The program terminates abruptly without throwing any exceptions or errors. It simply exits as if sys.exit() was called. When I try to debug this issue in a debugger, the program also exits, with the debugger reporting  before the program completely shuts down.  Versions Collecting environment information... PyTorch version: 2.4.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.5 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: Could not collect Libc version: N/A Python version: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS14.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] torch==2.4.0 [pip3] torchaudio==2.4.0 [pip3] torchvision==0.19.0",2024-08-15T19:57:58Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/133617,"The program will not have any output unless you print the output of your tensor operations:  Your second program will also run signficantly faster, because the call to `torch.rand` and `torch.atan` are vectorized. Your first program is slower because you are manually looping through a tensor of 10000 elements in python, requiring converting every element of the tensor (which lives in C++) into a python number. Feel free to post any further questions on the pytorch forums! https://discuss.pytorch.org/"
rag,Fix warning when pickle.load torch.Storage,"Fixes  CC(`pickle.loads` throws warning with `nn.Module`) Since `torch.save` does not use pickle for storages, the `torch.load` in `_load_from_bytes` should not ever be called when `torch.load`ing a checkpoint. Setting weights_only=False explicitly in `_load_from_bytes` to avoid the weights_only warning when using the pickle module   CC(Fix warning when pickle.load torch.Storage)",2024-08-15T17:59:23Z,open source release notes: nn,closed,0,0,https://github.com/pytorch/pytorch/issues/133597
rag,Add storage_utils,"  CC(Add storage_utils) Summary: Currently huggingface_hub, safetensors, accelerate all have their own implementation of `get_storage_id` and `get_storage_size`, `storage_ptr`, which makes assumption on internal implementation details of torch.Tensor, and storage, and does not work for wrapper tensor subclasses Motivated by https://github.com/huggingface/huggingface_hub/pull/2440issuecomment2284248247 maybe it makes more sense to add these as utils in pytorch so they can be maintained by us instead This PR added `get_storage_id`: returns a unique identifier for the tensor storage, for tensor subclasses, it returns a nested tuple of unique ids from underlying plain tensors `get_storage_size`: returns the size in bytes for the underlying storage, for tensor subclasses, it returns the sum of the size from all underlying plain tensors Test Plan: python test/test_utils.py TestStorageUtils Reviewers: Subscribers: Tasks: Tags:",2024-08-15T00:16:17Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/133524,"we'll support sharding properly through distributed checkpointing (https://pytorch.org/docs/stable/distributed.checkpoint.html), distributed checkpointing team is following up with huggingface on the concrete plans"
transformer,Enable DTensor sharding propagation of `native_layer_norm_backward` to more fully accommodate optional args,"Fixes CC(DTensor sharding propagation of `native_layer_norm_backward` does not fully accommodate optional args)  The issue Testing a variety of TP `requires_grad` patterns (validating maximally flexible finetuning) revealed `DTensor` sharding propagation of `aten.native_layer_norm_backward` (default) fails with an `IndexError` for certain `requires_grad` patterns (pattern 1) (e.g. `output_mask` `[True, False, False]`) and an `AssertionError` for others (pattern 2) (e.g. output mask `[False, True, *]`). Please see issue CC(DTensor sharding propagation of `native_layer_norm_backward` does not fully accommodate optional args) for a full description of the observed failure patterns along with reproduction.  Use Cases and Remediation Failure pattern 1 is potentially problematic for a variety of finetuning scenarios. Though failure pattern 2 is really an xfail right now since it's not fully supported, IMHO there are use cases (e.g. especially wrt to mechanistic interpretability research, but certain finetuning scenarios too potentially) that justify supporting this output mask (especially since supporting it is fairly straightforward I think). In this PR I propose some modest changes that:    * Address the aforementioned failure modes.   * Add a couple tests that I'm hopeful will help ensure `DTenso`r op dispatch (which is so well implemented and such a pleasure working with btw! :rocket: :tada:) accommodates a wide variety of (potentially unanticipated) `requires_grad` patterns as it evolves. To address both failure modes, I'm proposing the following changes: 1. To `torch.distributed._tensor.ops._math_ops.layer_norm_bwd_strategy`:    Refactor",2024-08-14T20:17:11Z,oncall: distributed triaged open source better-engineering Merged ciflow/trunk topic: not user facing ciflow/inductor module: dtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/133502," merge f ""lint error is not related to this PR"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,DTensor sharding propagation of `native_layer_norm_backward` does not fully accommodate optional args," üêõ Describe the bug Testing a variety of TP `requires_grad` patterns (validating maximally flexible finetuning) revealed `DTensor` sharding propagation of `aten.native_layer_norm_backward` (default) fails with the following `IndexError` (traceback below) with certain `requires_grad` patterns (e.g. `output_mask` `[True, False, False]`):  Pattern 1   Pattern 2 Additionally, the op fails (expectedly according to this comment) with the following `AssertionError` (traceback below) for the currently unsupported output mask `[False, True, *]`.  Though failure pattern 2 is not fully supported, IMHO there are usage patterns (e.g. especially wrt to mechanistic interpretability research, but certain finetuning scenarios too potentially) that justify supporting this output mask (especially since supporting it is fairly straightforward I think). In a PR I will submit shortly, I propose some modest changes that:  1. Address the aforementioned issues 2. Add a couple tests that I'm hopeful will help ensure `DTensor` op dispatch (which is so well implemented and such a pleasure working with btw! :rocket: :tada:) accommodates a wide variety of (potentially unanticipated) `requires_grad` patterns as it evolves.  To reproduce To reproduce the two failure patterns listed above as manifest with a variety of `output_mask` patterns, run the following test provided in the PR I'm submitting shortly associated with this issue. You should observe `8/20` subtests failing with the relevant subtest configurations provided:   Additionally, to reproduce a specific failure patterns with the standard `DTensor` test Transformer, one can run the following new test parameter",2024-08-14T20:06:40Z,oncall: distributed triaged module: dtensor,closed,2,5,https://github.com/pytorch/pytorch/issues/133499,"Thanks a lot for the report and the PR! We appreciate them! We happen to be planning on some changes to how layer norm does sharding propagation in DTensor. Specifically we might need to create candidate sharding options (like we do for matmul ops), instead of propagating along inputs sharding. Let us carefully study your report and PR and see if it aligns with / contributes to the new strategy we need. cc:  ","> Thanks a lot for the report and the PR! We appreciate them! We happen to be planning on some changes to how layer norm does sharding propagation in DTensor. Specifically we might need to create candidate sharding options (like we do for matmul ops), instead of propagating along inputs sharding. >  > Let us carefully study your report and PR and see if it aligns with / contributes to the new strategy we need. >  > cc:  Happy to contribute! The distributed team's code is always a pleasure to work with, shrewdly designed and adroitly implemented. üéâ üöÄ  I definitely appreciate the API is still evolving but whatever portions of the PR could be merged (even the test refactor) would be helpful while the candidate sharding option is being explored (agreed something akin to `gen_einsum_strategies` could be useful!).  I'm the primary maintainer of a downstream package (FineTuning Scheduler) and am currently adding support for the DTensor APIs and FSDP2 etc. I'm also planning to experiment with using DTensor dispatch for some mechanistic interpretability research functionality so if the team wants to solicit thoughts from the user base on anticipated usage patterns beyond the typical RFCs I'd be glad to provide any feedback that could be useful.  Anyway, if you think about it, please tag me once the candidate sharding refactor work gets going. Thanks again for your work!!","Thanks  for reporting the issue and categorizing it into 2 patterns which largely help us understand the problem. 1. for pattern 1, this is a mistake in `layer_norm_bwd` implementation. I misunderstood that when `output_mask[1] = False` and `weight` is not `None, the desired spec should still be added to the list. Your fix in PR should address this. 2. for pattern 2, your fix in PR complements the missing case support.  The PR looks good to me on these 2 parts, but my question is on the decision on filtering `None` from `args_spec` or `desired_specs` because they should contain no `None` objects.","> Thanks  for reporting the issue and categorizing it into 2 patterns which largely help us understand the problem. >  Thanks for all your great contributions, they're a pleasure working with! > 1. for pattern 1, this is a mistake in `layer_norm_bwd` implementation. I misunderstood that when `output_mask[1] = False` and `weight` is not `None, the desired spec should still be added to the list. Your fix in PR should address this. > 2. for pattern 2, your fix in PR complements the missing case support. >  > The PR looks good to me on these 2 parts, but my question is on the decision on filtering `None` from `args_spec` or `desired_specs` because they should contain no `None` objects. I just had a chance to review your PR suggestions this morning. I had more conservatively refactored `layer_norm_bwd_strategy`, always adhering to the full op schema for the target input specs (i.e. including `None` for optional args) to ensure alignment with `op_schema.args_spec` but your suggestion that it should be safe to rely on the presence of `(weight|bias)_strategy` allowed me to push a `layer_norm_bwd_strategy` refactor that obviates the need for the referenced `None` filtering and it's now been removed. Thanks again for your thoughtful collaboration! ","close this issue as https://github.com/pytorch/pytorch/pull/133502 has been merged. Again, really appreciate  for the detail issue report and quick fix!! "
rag,[Reland] Add wrappers for synchronous GPUDirect Storage APIs,Reland CC(Add wrappers for synchronous GPUDirect Storage APIs) Based in part on https://github.com/NVIDIA/apex/pull/1774 USE_CUFILE turned off by default in this version,2024-08-14T18:18:54Z,Merged ciflow/trunk,closed,0,3,https://github.com/pytorch/pytorch/issues/133489," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,[Reland] Add wrappers for synchronous GPUDirect Storage APIs,  CC([Reland] Add wrappers for synchronous GPUDirect Storage APIs),2024-08-14T17:28:06Z,ciflow/trunk,closed,0,0,https://github.com/pytorch/pytorch/issues/133482
llm,[TD] llm retrieval to not use bash -l {0},"https://github.com/pytorch/pytorch/pull/129720 swapped the action used to setup miniconda from conda incubator to the custom action we have in testinfra that comes with caching.   The original miniconda relies on bash profiles to set the environment variables needed to run conda, but the test infra version relies on the user using the env vars that are set during the step.   This PR changes the job to not use `bash l {0}` to see if not activating bash profile has an effect on the run.  Unfortunately this failure happens rarely on main so I'm not sure I will be able see if this has an effect.  On the plus side, changing this doesn't seem to have a negative effect on the job, so it should be a noop at worst.   Example failure: https://github.com/pytorch/pytorch/actions/runs/10389945235/job/28769219135",2024-08-14T16:18:59Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/133464," merge f ""lint and llm retrieval jobs passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llama,Enable batch matmul for result sizes > 2**32 the tensor can be split along batch axis,"Fixes CC(MPS backend breaking on llama 3.1 8B on Macbook M3). Addresses the issue seen when running llama v3.1 8B parameter model on MPS backend where the batch matmul output size can go over the 32bit indexing limit of MPS tensors, causing an assert. Test case to reproduce the issue with the dimensions encountered in llama v3.1 and verify this fix works around it:  Notably the current change only works as long as the individual output matrix in the bmm does not exceed the number of elements 2**32. This lets us split up the computation along the batch axis to avoid going over the limit.  Added a TORCH_CHECK to raise an error if the individual matrix dimensions are too large to handle for this op until a more general workaround tiling the matmuls is available.",2024-08-14T11:48:20Z,triaged open source Merged release notes: mps ciflow/mps,closed,0,13,https://github.com/pytorch/pytorch/issues/133430,"Need some guidance on how do we want to test this:  In order to reach the original failure a 96GB configuration is required. With a 64GB config we will hit another error when trying to allocate the initial buffers: `RuntimeError: Invalid buffer size: 47.99 GB`. This is due to the max memory fraction pytorch is allowed. In order to validate that the MPS implementation matches the CPU implementation we'll need a 128GB config. This is because the output tensors in the test case that fails are around ~50GB each leading to my 96GB config killing the process when asserting that the CPU and MPS outputs are the same. This can however be worked around by reducing the tensors a bit, batch_size = 11 is just enough to take the new code path. That seems to be able to run all the way including the result validation on a 96GB config.   Do we want guarantee our mac configurations now and going forwards are at least 96GB ones? Or should we try to work around this in some other way? The first I can think of would be to add a skip / xFail decorator if we detect that the test is running on a config with less than 96GB. I'm thinking it might have to be a skip since if the python process hogs too much memory it looks like the OS just kills it off instead of the test just failing due to some assert for not being able to allocate more memory."," I'm a bit confused about the repro case, because llama3.1 should never run in full 32bit mode, as it will be extremely slow and at least for 8B model would have zero advantage over bfloat16 variant that has a 2x smaller  memory footprint)",">  I'm a bit confused about the repro case, because llama3.1 should never run in full 32bit mode, as it will be extremely slow and at least for 8B model would have zero advantage over bfloat16 variant that has a 2x smaller memory footprint) Well yes that is a bit confusing. But it comes directly from the developer's provided repro script where their llama v3.1 pipeline gives  and leads to the op that fails being also of type f32 `aten::bmm_out_mps_impl:f32[32,20064,128]:f32[32,128,20064]`. But now that you mentioned it I tested the behavior when running with the half data types and there is clearly some other issue we need to tackle there since the MPSMatrixMultplication is doing some op underneath in those cases, leading to the same error in a slightly different context. And for bfloat16 specifically it raises an assert for unsupported data type so I'll need to consult the MPS team on workaround for that. While I wait for their feedback to see what should be done I'll add a check for the data type to raise the unsupported error for the half types for now. Since fp32 is the specific case the developer asked for this PR would still unblock them. ",So the FP16 issue happens on the pytorch side by us since we are calling `mps::copy_cast_mps` with a tensor that is too large for MPS to handle. That is something should be able to address.,Updated the PR to work also on float16 and bfloat16. With this the LLaMA3.1 works on all the supported float data types. Now I'll just need to figure a precompiler directive to hide it from preMacOS15 machines since the compiler will refuse the `MPSNDArrayDescriptor.preferPackedRows` on those machines.,"  I ended up adding the tests for fp16 and bf16 since those have a smaller memory footprint. The tests will  still take a quite a bit more time than many other unittests we have, around 2 minutes per data type on a powerful machine. This is mostly down to how long the CPU takes computing the reference value and doing the comparison. Let me know if this is a concern and if there are other ideas on what would be a better test. Other than that this PR should be ready to review.","> Other than that this PR should be ready to review.  isn't it just failed on MacOS 15, see https://github.com/pytorch/pytorch/actions/runs/10620567384/job/29441019041 ",MacOS15 now passing.," merge f ""MPS + Lint are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: PR CC(Enable batch matmul for result sizes > 2**32 the tensor can be split along batch axis) has not been reviewed yet Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ," merge f ""MPS + Lint are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Enable clang-tidy coverage on torch/*.h,Fixes ISSUE_NUMBER,2024-08-14T08:15:33Z,open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/133422, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[FSDP2] Added eager fast-path for fp32->bf16 param cast,"  CC([FSDP2] Added eager fastpath for fp32>bf16 param cast)  CC([FSDP2] Set `ctx.set_materialize_grads(False)` for postbackward) Some recommendation models have a high number of `nn.Parameter`s. This exacerbates pertensor CPU overheads in FSDP2 compared to FSDP1. This PR adds a fastpath for the common bf16/fp32 mixed precision case for the casting the parameters from fp32 to bf16 to reduce CPU overhead and possibly have more efficient copy.  Old: `for` loop + `.to(torch.bfloat16)`, incurring dispatcher overhead per parameter  New: `torch.empty` + `torch.split` + `torch._foreach_copy_`, incurring three dispatches  Example on Llama38B which does not have many `nn.Parameter`s (compared to recommendation models): (Old) on Llama38B (0.46 ms CPU overhead for allgather): !Screenshot 20240813 at 6 19 39‚ÄØPM (New) on Llama38B (0.37 ms CPU overhead for allgather): !Screenshot 20240813 at 6 20 32‚ÄØPM  Same example as above but now with float8 allgather: (Old) on Llama38B with float8 (0.996 ms CPU overhead for allgather): !Screenshot 20240815 at 11 27 46‚ÄØAM (New) on Llama38B with float8 (1.014 ms CPU overhead for allgather): !Screenshot 20240815 at 11 26 33‚ÄØAM The times are relatively comparable for float8 with the new one possibly slightly slower, but this is mainly because for Llama's transformer blocks, there are only two norm weights that need to cast to bf16. These screenshots are mainly to show that the optimization still works in the mixed case. : D61236983",2024-08-13T22:15:14Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (fsdp2),closed,0,5,https://github.com/pytorch/pytorch/issues/133369," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","cc:  depending on how many weights use fp8, the fp8 casting CPU overhead, even with the precompute, might become a problem", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[DO NOT COMMIT] trying new AMI,  CC([DO NOT COMMIT] trying new AMI) Summary: Trying new AMI to catch test failures Test Plan: Reviewers: Subscribers: Tasks: Tags:,2024-08-13T20:39:01Z,topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/133356, rebase i main
rag,Prototype changes to create fake checkpoints with empty storages,"New version of CC(Use write_record_metadata in torch.save under certain conditions), opening just to show dependencies to prevent colab OOM in https://github.com/pytorch/torchtune/pull/1315 with some changes to FakeTensor `__reduce_ex__` (or a subclass of it, TBD how we'll actually do this) such as those here, we can do things like  ",2024-08-12T22:14:34Z,Stale,closed,0,1,https://github.com/pytorch/pytorch/issues/133272,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,[export] overwrite placeholder names when deepcopying,"In jointgraph export we have a `copy.deepcopy(ep.graph_module)` call. This turns out to be an imperfect deepcopy, because deepcopy allows objects to overwrite their `__deepcopy__` methods. For fx.Graph, this ends up deferring to `Graph.create_node()`, which checks the graph namespace, and can avoiding copying the exact name in niche examples, like where the name is a Python keyword (e.g. `input` gets renamed to `input_1`). Names like `input` happen because export's placeholder naming pass overwrites what the namespace creates, based on the model's `forward()` signature. So we can either 1) avoid overwriting such cases, which requires rewriting the naming pass logic, or 2) force another overwrite after deepcopying. This goes with 2).",2024-08-12T21:32:33Z,fb-exported Merged,closed,0,5,https://github.com/pytorch/pytorch/issues/133269," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D61160119,This pull request was **exported** from Phabricator. Differential Revision: D61160119," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,"[Torch.Export] [OpenELM] Failed to Export OpenELM: ""Pop from Empty List"""," üêõ Describe the bug When I was trying to export OpenELM  I was hit by   Versions Tried both torch `2.3.0` and `2.4.0`, both fails Transformers version is `4.38.2` ",2024-08-12T18:45:29Z,export-triaged oncall: export,closed,0,3,https://github.com/pytorch/pytorch/issues/133252," Sorry we just got to this, the embedding weights seem to be not tracked properly, I'll look into this."," if you're able to use nonstrict mode export `export(strict=False)` I would recommend it while we work on this, nonstrict seems to export successfully.",Also putting up https://github.com/pytorch/pytorch/pull/134500 to fix this
llm,Tensor parallel for convolutions and groupnorm ," üöÄ The feature, motivation and pitch Recurrent LLM architectures such as Mamba (v1 and v2) and xLSTM use depthwise convolution and GroupNorm in each block. Both are currently not supported by Pytorch tensor parallel. Depthwise convolution is independent over the channels and GroupNorm corresponds to independent LayerNorm for every head within a block. Therefore, both should be easily parallelizable over the independent heads. However, the current tensor parallel API in Pytorch does not support convolutions and GroupNorm.  Is this feature currently being developed? Are there fundamental reasons in pytorch distributed that make an extension of Rowwise and Columnwise strategies to convolution and other modules difficult?  Alternatives Megatron supports the depthwise separable convolution and the groupnorm for mamba, but it is not based on the pytorch tensor parallel API.   Additional context _No response_ ",2024-08-12T11:09:19Z,triaged module: dtensor,open,2,5,https://github.com/pytorch/pytorch/issues/133221,I agree these are features TP should support. Are you interested in submitting PRs by improving style.py so we can help review; o/w we may need to wait until we have bandwidth to do it.,"l I will need to look into this in the next weeks for sure. But at the moment, I am unsure whether to use style.py or avoid it entirely and instead build on a framework such as Megatron.  Do you think Pytorch will continue with style.py or replace this by a different approach soon? I am suspicious that it might not be continued, because no other modules are mentioned in the documentation, I could find a roadmap, and torch.distributed.tensor does not receive commits recently. Furthermore, I just found https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/_tp_conv.pyL106. Where is this TP for convolutions used currently? It does not seem to be used in torch.distributed.tensor.  So I am wondering if a different API is being developed instead...","style.py is the place to go!  pytorch Tensor Parallel is only recently betareleased (https://pytorch.org/docs/stable/distributed.tensor.parallel.html). We will continue working on it. I think the reason TP is only supported on selected module types (e.g. Linear, Embedding, Norm, etc.) is because our focus has been on llamastyle model. I agree we should continue make it better, and we appreciate your feedback and potential contribution.  There is no other effort planned for TP.  DTensor (torch.distributed.tensor) is the backbone of our TP. DTensor itself is almost ready and will be betareleased shortly. We are constantly developing it.  The _tp_conv.py you linked is used here (https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/_dispatch.pyL100). It's for lowlevel DTensor treatment of the op `aten.convolution.default`, and is not directly related to the highlevel TP API.","l I am trying to TP nn.Conv1D by using distribute_module and trying to split the input tensor colwise (Shard(1) in distribute_tensor) This leads to an error because in _conv_ops.py, the convolution rules seem to be made for Conv2D and expect (N, C, W, H) as input shape. Am I correct in assuming that I need to create new rules for Conv1D? How can I link them to the conv1d function call?","  > Am I correct in assuming that I need to create new rules for Conv1D? Yes, I think so. You'll need to create a similar rule / strategy for the conv1d aten ops (forward and backward). To find what aten ops to support, the error msg should contain it, e.g. https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/_sharding_prop.pyL456 E.g. the corresponding aten op for forward Conv1D might be https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yamlL1711 Let me know if you need further help."
rag,MPS regression: RuntimeError: result.storage().use_count() == 1 INTERNAL ASSERT FAILED,I'm hitting this assert when `relu` is called: ~It looks like a regression.~ (actually not regression but new bug)  `v2.2.2` does not have this issue but `v2.3.0` onward is having it.  Repro:  Env:  _Originally posted by  in  CC(Leaky ReLU of empty tensor doesn't work on MPS)issuecomment2282656408_ ,2024-08-11T12:11:37Z,triaged module: regression module: mps,closed,0,0,https://github.com/pytorch/pytorch/issues/133182
transformer,[torch.compile] Integers stored on nn.Modules as dynamic causing errors," üêõ Describe the bug reporduce code  The above code is fine on 2.3, but it will throw an error on 2.4 and the main branch. What is error looks like:  This seems to be a problem with inductor or triton, but it is actually because *self.num_heads* is dynamic. This phenomenon occurred after this commit:  CC([torch.compile] nn.Module int/float attributes caused re-compile) The problem is that torch made all ints in nn.Module dynamic, but this is actually not necessary. I understand that there will be an automatic dynamic mechanism for this in the future, but for the moment if I can have a simple way to avoid the problems.  Versions  ",2024-08-10T13:47:51Z,high priority triaged actionable oncall: pt2 module: dynamic shapes,open,1,5,https://github.com/pytorch/pytorch/issues/133166,pytorch 2.4 release error infomation: !image,:  CC([torch.compile] nn.Module int/float attributes caused re-compile) ,"Adding insult to injury there's no way to force the integer to be static lol. Well, I guess you could do something like `tensor_num_heads = torch.empty((num_heads, 0))` and then `num_heads = tensor_num_heads.size(0)` LOL",It looks like this is causing many errors internally,"I actually circumvent this problem by using warmup. I don't specifiy dynamic=True, but use default setting. Then I warmup with inputs of different lengths. Eventually, the dimension of the sequence length become dynamic, while num_heads is still static, which serves my purpose."
yi,add option for specifying backend,  CC(Tune flex_decoding performance: block scheduling and split calculation)  CC(Add flash decoding backend)  CC(add option for specifying backend) Example usage  ,2024-08-09T16:26:56Z,Stale topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/133099," Thanks for the heads up, will do!","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Fix typo in `mypy.ini`,"A missing comma in the file list currently leads to errors when running mypy, introduced in CC(Move test_utils.py back to MYPY)  Fixes CC(Running mypy fails due to broken `mypy.ini`) ",2024-08-09T15:45:15Z,open source Merged ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/133097,:white_check_mark:login: DeinAlptraum / (7af587134dfd42353d0005d15612459e4c8a5d1d)The committers listed above are authorized under a signed CLA., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""Lint is green""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""Lint is green""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""Lint is green""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Running mypy fails due to broken `mypy.ini`," üêõ Describe the bug Running `mypy` in the repository root produces the following error:  I would expect a list of type erors  Versions Collecting environment information... PyTorch version: 2.5.0.dev20240809 Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (GCC) 14.2.1 20240805 Clang version: 18.1.8 CMake version: version 3.30.2 Libc version: glibc2.40 Python version: 3.12.4  (main, Jun 18 2024, 15:12:24) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.10.3arch12x86_64withglibc2.40 Is CUDA available: False CUDA runtime version: 12.5.82 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU Nvidia driver version: 555.58.02 cuDNN version: Probably one of the following: /usr/lib/libcudnn.so.9.2.1 /usr/lib/libcudnn_adv.so.9.2.1 /usr/lib/libcudnn_cnn.so.9.2.1 /usr/lib/libcudnn_engines_precompiled.so.9.2.1 /usr/lib/libcudnn_engines_runtime_compiled.so.9.2.1 /usr/lib/libcudnn_graph.so.9.2.1 /usr/lib/libcudnn_heuristic.so.9.2.1 /usr/lib/libcudnn_ops.so.9.2.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        39 bits physical, 48 bits virtual Byte Order:                           Little Endian CPU(s):                               8 Online CPU(s) list:                  07 Vendor ID:                            GenuineIntel Model name:                           11th Gen Intel(R) Core(TM) i51135G7 @ 2.40GHz CPU family:                           6",2024-08-09T15:44:31Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/133096
transformer,Fixing Pytorch RMS norm implementation,"Since FP16 has quite small dynamic range it is very easy to overflow while computing `at::pow(input, 2)` , and it happens in real world computation. I've tried to use `nn.RMSNorm` fused implementation instead of `LlamaRMSNorm` inside `transformers` implementation of Llama (`src/transformers/models/llama/modeling_llama.py`). It started to give wrong answers in Fp16 while still giving good in FP32. I figured out happens due to overflow while computing square of the input tensor. Original `LLamaRMSNorm` implementation upcasts input to fp32 to prevent this and give better numerical stability.  Proposed commit fixed the issue. FP16 in RMSNorm has to be treated in special way, to be usable in real world implementations.",2024-08-09T10:02:21Z,triaged open source,closed,0,7,https://github.com/pytorch/pytorch/issues/133085," :x:  login:  / name: Karol Kontny . The commit (c79577d3d55483360ac339b14f605656d3f30c42, 6458021f9eaa4e13d3bb63d08df361286c424d65) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.",> Could you sign CLA in  CC(Fixing Pytorch RMS norm implementation) (comment) please I have to process the CLA through the company legal. Unfortunately it will be probably done early next week.," would you mind if I open this PR again instead? Its blocking me as well, want to get it in as soon as possible",">  would you mind if I open this PR again instead? Its blocking me as well, want to get it in as soon as possible  Please go on, I tried to hurry them, but with no effect. It takes much more time than it should...", I have created a PR here: https://github.com/pytorch/pytorch/pull/134106, I think we can close this. this has been fixed now in https://github.com/pytorch/pytorch/pull/134106,Fixed via CC(fix for fp16) 
transformer,Optimize test_transformers.py," Reduced number of skipped test cases  Merged redundant test cases **Benchmark:**  _These are approximate numbers from running test_transformers.py on a single H100, and can change based on the device._",2024-08-08T23:31:50Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/133049, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Optimize test transformers,Fixes ISSUE_NUMBER,2024-08-08T23:20:11Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/133046
rag,inductor input storage unbacked test,  CC(inductor input storage unbacked test)  CC([inductor] tensor_is_align fallbacking False if unbacked expr not comptime evaled) ,2024-08-08T14:32:36Z,Stale release notes: fx module: inductor ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/133002,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,[FSDP2] reset FSDPParam.sharded_param in lazy_init,"  CC([FSDP2] reset FSDPParam.sharded_param in lazy_init)  CC([DDP][FSDP2] keep DTensor params for replicate(fully_shard)) motivated by FSDP2 + DoRA  CC([FSDP2] meta init + `initialize_dora_params` for DoRALinear.magnitude) after meta init, we need a userdefined function to move DoRALinear.magnitude from device=meta to device=cuda The problem is how to trigger reset_sharded_param or _apply to update FSDPParam. Otherwise lazy_init complains that DoRALinear.magnitude are still on device=meta credit to  for chasing after DDP lazy_init to unblock the PR ",2024-08-08T00:25:29Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,3,https://github.com/pytorch/pytorch/issues/132954,the CI error is relevant and I want to discuss my minimized repro with   CC([FSDP2] model.parameter() should be DTensor for replicate(fully_shard)), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[sym_shapes] Not eval sym expression for printing storage_offset,  CC([sym_shapes] Not eval sym expression for printing storage_offset) ,2024-08-07T18:48:26Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/132911, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[NestedTensor] Add support for reducing along multiple non-ragged dimensions for the mean operator in NestedTensor,"Summary: Add support for reducing across multiple nonragged dimensions using the `mean` operator in `NestedTensor`. For example, for a nested tensor of shape `(B, *, M, N)`, this diff enables reducing across dimensions `(M, N)` to an output of shape `(B, *)`. Test Plan: Verify that existing and modified unit tests pass via the following commands:   Differential Revision: D60680005",2024-08-07T18:22:54Z,triaged open source fb-exported Stale,closed,0,5,https://github.com/pytorch/pytorch/issues/132902,This pull request was **exported** from Phabricator. Differential Revision: D60680005,This pull request was **exported** from Phabricator. Differential Revision: D60680005,This pull request was **exported** from Phabricator. Differential Revision: D60680005,This pull request was **exported** from Phabricator. Differential Revision: D60680005,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Exclude test_transformers and unit tests which require recent GPU arch,This PR is to exclude test_transformers on ROCm temporarily and skip some unit tests which require recent GPU arch. ,2024-08-07T16:56:32Z,oncall: distributed triaged open source Merged topic: not user facing ciflow/periodic ciflow/inductor rocm rocm priority keep-going ciflow/rocm ciflow/inductor-rocm,closed,0,38,https://github.com/pytorch/pytorch/issues/132895, label ciflow/rocm, label keepgoing, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", label ciflow/trunk
transformer,[Memory] snapshot wrong with optimizer_state and gradient - ü§ó Transformers," üêõ Describe the bug  Training GPT2XL with ü§ó Transformers and ü§ó Accelerate  Using _record_memory_history and _dump_snapshot for profiler Training GPT2XL with ü§ó Transformers, optimizer set to adamw_torch. Since we are using adamw, the memory of optimizer_state should be 2 times of gradient if fp32training, or 6 times of gradient if fp16 or bf16training. However, from the snapshot, the optimizer_state is always the 2 times of gradient, which is obviously impossible. Reproduce  model: GPT2XL  batch size: 4  sequence length: 1024  single GPU  optimizer: adamw_torch  bf16  !Trace_gpt2xldataset_alpacagpus1bs4seq1024lr0 0004linearbf16_step_3_to_5  Versions PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04 LTS (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0155genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.2.91 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A800SXM480GB GPU 1: NVIDIA A800SXM480GB GPU 2: NVIDIA A800SXM480GB GPU 3: NVIDIA A800SXM480GB GPU 4: NVIDIA A800SXM480GB GPU 5: NVIDIA A800SXM480GB GPU 6: NVIDIA A800SXM480GB GPU 7: NVIDIA A800SXM480GB Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn.so.9.1.0 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv.so.9.1.0 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn.so.9.1",2024-08-07T10:04:12Z,module: optimizer triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/132855, (not sure what label to add for memory snapshot),"Why should optim state take up 6 times of gradient in fp16 or bf16training? If the optimizer was created with fp16/bf16 parameters and there's no additional logic to store copies of the params, then the optimizer being 2x the size of the params is expected. Also I believe this may be a better question for the huggingface transformers repo https://github.com/huggingface/transformers as it is likely setup related.","Sorry I made a mistake, the 2x size is expected. > then the optimizer being 2x the size of the params is expected. Closing this issue."
rag,[NestedTensor] Modify softmax on ragged dimension to allow for 2D nested tensors,"Summary: Modify `softmax` on the ragged dimension, where `ragged_idx == 1`, to allow for 2D nested tensors. This diff now enables a `softmax` operation on tensors of shape `(B, *)`, where `*` is the ragged dimension. Extend existing `softmax` unit tests to include 2D nested tensors using the `include_2d_tensor=True` keyword argument. Test Plan: Verify that existing and modified unit tests pass using the following commands:   Reviewed By: davidberard98 Differential Revision: D60780975",2024-08-06T21:46:55Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/132812,This pull request was **exported** from Phabricator. Differential Revision: D60780975,This pull request was **exported** from Phabricator. Differential Revision: D60780975,This pull request was **exported** from Phabricator. Differential Revision: D60780975, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,[dynamo][inline_inbuilt_nn_modules] Mark nn module tensor static for cudagraphs,"  CC([dynamo][inline_inbuilt_nn_modules] Mark nn module tensor static for cudagraphs)  CC([dynamo] Reland 132308, 132314, 132318, 132334) Fixes  CC([inline_inbuilt_nn_modules][cudagraphs] Large regression in llama models because cudagraph is not picked up) ",2024-08-06T05:52:42Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/132736, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Reverted by CC(Revert 132806, 132736, 132539, 132487)"
llama,[inline_inbuilt_nn_modules][cudagraphs] Large regression in llama models because cudagraph is not picked up,"Repro  This is the warning  If I disable the inlining flag, the speedup goes back to 8.6x. ",2024-08-05T23:46:17Z,high priority triaged module: cuda graphs oncall: pt2 module: inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/132714,Looks like it's just the caches again  they used to be fully internal to an NN module (and constant on the graph) but are now inputs that are getting mutated. Perhaps we should just mark them as buffers or as static inputs? Alternatively we could search nn modules and make all tensor attributes static for cudagraphs to do this automatically. I don't have a preference one way or the other.
transformer,Forward Hooks in Transformer Not Working When `model.eval()` and `torch.no_grad()`," üêõ Describe the bug For `TransformerEncoder` network, when I created a forward hook, it does not get called if there are `model.eval()` and with `torch.no_grad():` at the same time. Here is a code to reproduce:  It gives ‚ÄúHook for * is set‚Äù as output, but it does not print ‚ÄúHook working‚Äù, (and it does not actually call the hook). I observed that when there is only model.eval(), it works. Also, when there is only with torch.no_grad(), it also works. Somehow using both of them make it not working. And also, I only observed this with TransformerEncoder (not happens in `nn.Linear`) I first shared this in discuss.pytorch, and as a first impression ptrblck stated that "" 'faster transformer' pass is used as described here which does not seem to support forward hooks.""  Versions Collecting environment information... PyTorch version: 2.3.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.30.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.85+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib",2024-08-05T21:26:43Z,triaged module: sdpa,open,0,3,https://github.com/pytorch/pytorch/issues/132700,Could you try setting `torch.backends.mha.set_fastpath_enabled(False)`,"Thanks, it works after your suggestion. If this was an intentional design choice, I think there should be at least some note about this in the documentation because it took a lot of my time to figure out where is the problem","Hi, I was using a forward hook to extract intermediate layers, which breaks in eval mode due to this behaviour.  What is the recommended way to extract intermediate layer feature maps when fastpath is enabled?"
yi,Hanging at waitForFutureOrTimeout when actively destroying ProcessGroups," üêõ Describe the bug Hi, we want to use the existing APIs to actively destroy the ProcessGroups in the main thread without tearing down the health processes. For example, we launch the following script with `NCCL_CUMEM_ENABLE=1 torchrun nproc_per_node=4 nnodes=1 test.py` . In the script, we mimic that the rank 2 is hanging and we want to destroy the ProcessGroups at Rank0, 1, 3 so that we can do some reconfiguration later in these processes.  However, we find only Rank0,1 would successfully reach the final print statement `print(f""finish {rank}"")` but Rank 3 would would hang at `dist.destroy_process_group()` until the watchdog heartbeat monitor thread is triggered. By GDB, we find the Rank3 would hang at `waitForFutureOrTimeout`:  I find that this issue is related to the previous unfinished collective operations. When I comment out the ` torch.distributed.all_reduce(tensor0, group=pg)` after Rank 2 sleeps, then Rank 3 can successfully reach to the end as expected.  Versions The NCCL is from https://github.com/NVIDIA/nccl/tree/v2.22.31  ",2024-08-05T20:24:06Z,oncall: distributed triaged,open,0,2,https://github.com/pytorch/pytorch/issues/132696,"In this case, on rank 3 have two PGs created, so when we destroy PGs we will destroy all of them. There is no way we can know which rank is hang, so I guess even if we support it, it would be hard to configure which PG to destroy unless you know which rank is going to hang in advance?","Hi, I want to use this test case to mimic a multinode setting by viewing each rank here as a node: Suppose we find a node hangs, then for other nodes we destroy ProcessGroups and recreate new ones. "
gemma,[export][reland] Convert autocast to HOO,"Summary: Reland of D60206382. Suggested in  CC([export] Failed to trace HF Llama2 model). If there's an autocast context manager, the predispatch (strict) graph can look something like:  But the operator `torch.amp.autocast_mode._enter_autocast` is not a valid ATen op. We remove these nodes by turning autocast into a higher order operator and make a submodule for the blocks between `_enter_autocast` and `_exit_autocast`. Some potential followup improvement: 1) Merge some of the duplicated logic with `replace_set_grad_with_hop_pass.py` 2) Check the current autocast status (any enabled? dtype?) and not create a submodule if the autocast args matches current autocast status. Test Plan: CI  Verified that now we can export the llama model in  gh issue 128394 and the gemma model in  gh issue 131829 without error. Differential Revision: D60770038",2024-08-05T16:03:55Z,fb-exported Merged ciflow/trunk ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/132677,This pull request was **exported** from Phabricator. Differential Revision: D60770038," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,`torch.distributed.pipelining` hang and timeout in CPU gloo backend," üêõ Describe the bug When executing the pippy_bert.py example with cpu gloo backend:  I got the following hang and timeout error:  BTW, pippy_gpt2.py works but pippy_llama.py is also timeout.  Versions Collecting environment information... PyTorch version: 2.5.0.dev20240805+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.092genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      43 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          AuthenticAMD Model name:                         AMD Ryzen 9 3950X 16Core Processor CPU family:                         23 Model:                              113 Thread(s) per core:                 2 Core(s) per socket:                 16 Socket(s):                          1 Stepping:                           0 Frequency boost:                    enabled CPU max MHz:                        3500.00",2024-08-05T09:40:07Z,oncall: distributed triaged module: pipelining,open,0,4,https://github.com/pytorch/pytorch/issues/132644,cc: Huang  ,"Hi , we don't have testing for `pippy` (now under `torch.distributed.pipelining`) for CPU and gloo. We would accept improvements and fixes in this area. Please take a look under https://github.com/pytorch/pytorch/tree/main/torch/distributed/pipelining if you are interested in contributing.","Hi Huang, what are the e2e models you are testing after `pippy` has been merged into `torch.distributed.pipelining`? Or do you mean that `torch.distributed.pipelining` support and testing are only for GPU? ","> torch.distributed.pipelining support and testing are only for GPU Yes we are only testing for GPU. The e2e model tests have not been upstreamed from `pippy` to `torch`, we would also accept contributions there"
llm,[dynamo][vllm] Support construction of TypedDict," üêõ Describe the bug This graph breaks right now ~~~ from typing import TypedDict, Literal import torch class LlavaImagePixelInputs(TypedDict):     type: Literal[""pixel_values""]     data: torch.Tensor     """"""Shape: `(batch_size, num_channels, height, width)`"""""" def fn(x, y):     obj = LlavaImagePixelInputs(int, y)     return x * obj.y opt_fn = torch.compile(fn, backend=""eager"", fullgraph=True) opt_fn(torch.randn(4), torch.randn(4)) ~~~ Stack trace ~~~   File ""/home/anijain/local/pytorch2/torch/_dynamo/symbolic_convert.py"", line 557, in wrapper     return inner_fn(self, inst)   File ""/home/anijain/local/pytorch2/torch/_dynamo/symbolic_convert.py"", line 1564, in CALL_FUNCTION     self.call_function(fn, args, {})   File ""/home/anijain/local/pytorch2/torch/_dynamo/symbolic_convert.py"", line 804, in call_function     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype]   File ""/home/anijain/local/pytorch2/torch/_dynamo/variables/user_defined.py"", line 492, in call_function     return super().call_function(tx, args, kwargs)   File ""/home/anijain/local/pytorch2/torch/_dynamo/variables/base.py"", line 308, in call_function     unimplemented(f""call_function {self} {args} {kwargs}"")   File ""/home/anijain/local/pytorch2/torch/_dynamo/exc.py"", line 283, in unimplemented     raise Unsupported(msg, case_name=case_name) torch._dynamo.exc.Unsupported: call_function UserDefinedClassVariable() [BuiltinVariable(), LazyVariableTracker()] {} ~~~ For now, we should specialized for TypedDict to unblock vllm work. In the long term, we might need Dynamo to inline into `__new__` and make it understand metaclasses to make it work generically. This",2024-08-05T05:55:52Z,triaged oncall: pt2 module: dynamo dynamo-triage-june2024 vllm-compile,closed,0,2,https://github.com/pytorch/pytorch/issues/132629,I am not sure but I think you maybe need to use something like VariableBuilder to implement that method,"Just to add some context, the following vllm models all use the same `TypedDict` pattern:  qwen2_vl.py  paligemma.py  llava.py  qwen.py  chameleon.py  llava_onevision.py  minicpmv.py  llava_next_video.py  llava_next.py  phi3v.py  internvl.py  fuyu.py  ultravox.py  blip2.py"
finetuning,set_optimizer_state_dict does not support partial state dict, üêõ Describe the bug  The error is :  This behavior is different from `optimizer.load_state_dict`. This pattern is useful when finetuning a model with more learnable parameters  in this case some parameters have existing states to load while some don't.  Versions torch2.4 ,2024-08-05T05:36:24Z,oncall: distributed feature triaged module: distributed_checkpoint,closed,1,6,https://github.com/pytorch/pytorch/issues/132628,How do you handle the extra learnable parameters? Are you using `strict` option or are you using a separate checkpoint?  We can add `strict` option support for `set_optimizer_state_dict` but you can also use a separate checkpoint for the extra learnable parameters.,"We use the `strict` option for model state dict. Note that in this particular use case, the extra parameters do not exist before finetuning and are expected to initialize randomly, so it does not make sense to load a separate checkpoint.",Same here. Wondering if there are any updates on this? Thanks!,",  The regular Pytorch optimizer will also raise an exception if parameter group does not match: https://github.com/pytorch/pytorch/blob/main/torch/optim/optimizer.pyL880. So I'm not sure how this case has to be supported in general. If one parameter doesn't exist before finetuning, then we should use a different optimizer for it. Otherwise, the parameter group won't match.","You're right that the regular optimizer does not support this either. For this case we actually have implemented our own ""strict=False"" logic to match a partial state dict's param group against a new optimizer, but it might not make sense for pytorch to support that feature in general.","It is difficult to support this feature without the support from PyTorch optimizer. The main concern is that PyTorch optimizer doesn't support dynamically changing the parameters being optimized. In such a case, I would still argue that the best way is to have a separate optimizer or users should have a proprietary implementation to fit their own requirements.  do you have any comment about this feature?  I'll close the issue later unless there are other proposal to make this feature move forward."
transformer,autocast to float16/bfloat16 fails on transformer encoder (in `eval` mode)," üêõ Describe the bug Continuing from https://github.com/LightningAI/pytorchlightning/issues/19980 Autocasting a `TransformerEncoder` to `bfloat16` works in training time, but not in eval time:  Yields:   Versions  ",2024-08-04T13:02:16Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/132613, please take a look at this one.
llm,topk with mask - efficent," üöÄ The feature, motivation and pitch topk operation is important for many domains, but still suffer from high complexity. (espically regards in the LLM domain) Suppuse you get an input with a coresponding binary mask; where you need to find the topk only on the mask. The goal is not to get correcntess but to get efficentcy.  Where the complexity should be affacted by the mask. It seems that the current implmentation cannot take this advantage. I can be great if we can have such advaced topk operation.  Alternatives _No response_  Additional context note: I have considered to get into that issue, but it seems that there is a need to edit both cpp+cuda versions. where i did not find documntation. _No response_ ",2024-08-04T10:50:54Z,module: performance triaged module: sorting and selection module: masked operators,open,0,2,https://github.com/pytorch/pytorch/issues/132608,Have you benchmarked simply calling  Topk(tensor[mask]) ? Like do we know its actually slower? Also I don't think I follow how this helps LLMs... the only masked topk I can think of in llms is with gpt4all and there you are better of doing  Topk(tensor [:pad_size]) ,"hi nevakrien, I will be concrete with an example, it't help with approximation of the attention. It's known to be good to look on the topk entries of the attention matrix. Supoose the attention size is [B, H, L, L] where L is the number of tokens. may be huge matrix! We also know that the matrix (L,L) are causel...  Suppose you have even stronger knowlege, and you have mask with the active attentions. This mask may have different number of active values per row. The standard topk seems not to be able to exploit even the causal mask. > Have you benchmarked simply calling Topk(tensor[mask]) ? Like do we know its actually slower? >  > Also I don't think I follow how this helps LLMs... the only masked topk I can think of in llms is with gpt4all and there you are better of doing >  > Topk(tensor [:pad_size])"
rag,[ONNX] New export logic leveraging ExportedProgram and ONNX IR,  CC([ONNX] New export logic leveraging ExportedProgram and ONNX IR) 1/n PR to  Move code from torchonnx from commit https://github.com/justinchuby/torchonnx/commit/395495e566c1ee5fad438775528626cc45f88431 into torch.onnx and fixes imports.  Integrate the new export logic with the torch.onnx.export API and include basic set of tests.  Refactor the API for the change.  Improve documentation. Next PRs will be more tests and docs. Fix  CC([ONNX] Migrate logic from torch-onnx to torch.onnx),2024-08-02T17:47:29Z,module: onnx open source Merged Reverted ciflow/trunk release notes: onnx suppress-api-compatibility-check suppress-bc-linter,closed,0,22,https://github.com/pytorch/pytorch/issues/132530,, merge i, Merge started Your change will be merged while ignoring the following 1 checks: Lint / prsanitychecks Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge i, Merge started Your change will be merged while ignoring the following 1 checks: Lint / prsanitychecks Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge i," Merge started Your change will be merged while ignoring the following 3 checks: pull / linuxdocs / builddocspythonfalse, pull / linuxjammypy3.8gcc11 / test (docs_test, 1, 1, amz2023.linux.2xlarge), Lint / prsanitychecks Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / build Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 4 checks: pull / linuxdocs / builddocspythonfalse, pull / linuxjammypy3.8gcc11 / test (docs_test, 1, 1, amz2023.linux.2xlarge), Lint / prsanitychecks, trunk / macospy3arm64 / build Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c nosignal m ""Sorry but it seems like Dr. CI incorrectly flagged the pull / linuxdocs / builddocspythonfalse77682) failure as being flaky. The job started failing consistently on CI once your PR was merged. GH job link HUD commit link""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.," Thanks, the docs error seems to be  Is this the error you see? Do you have an idea on what may have caused it? This change doesn't seem to add any new docs or introduce changes that will trigger this",I think I know what is going on. I am going to rebase after CC([ONNX] Remove logging apis from public) , rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to   Raised by https://github.com/pytorch/pytorch/actions/runs/10479306647," merge f ""Rocm tests unrelated."""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,return_and_correct_aliasing: skip dispatcher when swapping storage,"`return_and_correct_aliasing` is used by FunctionalTensor today to ensure that when we call view/inplace ops, the input and output `FunctionalTensors` share the same storage. This was previously done with a dispatcher call to `aten.set_`. In this PR I swap it out with a util that just manually does the storage swap. Benefits: (1) we know this is safe in the specific way it is used by FunctionalTensor: avoiding the extra assertions in `aten.set_` is necessary to avoid some unbacked symint errors (2) this should improve compile times a bit   CC([test] dynamo: dont hold onto real tensors in the __dict__ of any graph inputs)  CC(return_and_correct_aliasing: skip dispatcher when swapping storage)  CC(fsdp.set_: convey to functionalization that it mutates storage)  CC(move torch._functionalize APIs to pybind. add one for marking storage mutations)  CC(make functorch CSE respect mutations as barriers (like fsdp.set_))",2024-08-02T16:16:24Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/132524, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Change owners of test/test_transformers.py to module: multi-headed-attention,So flaky tests get tagged with `module: multiheadedattention` instead of `module: nn`   CC(Change owners of test/test_transformers.py to module: multiheadedattention),2024-08-02T15:51:23Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/132519, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[Testing only] See what fails when we no longer use numpy to serialize tensors without storage,  CC([Testing only] See what fails when we no longer use numpy to serialize tensors without storage),2024-08-01T20:04:14Z,Stale,closed,0,1,https://github.com/pytorch/pytorch/issues/132438,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,tensor.to(device) not copying data correctly between two GPUs," üêõ Describe the bug It seems that tensor.to(device) isn't copying the data correctly when it is moved from one GPU to another GPU. Copying to/fro a GPU and a CPU seems to work as intended. Minimal example   Machine specs  AWS g6.48xlarge dedicated host with 8 L4 GPUs on it. Pytorch version  2.4.0  Versions PyTorch version: 2.4.0 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 24.04 LTS (x86_64) GCC version: (Ubuntu 13.2.023ubuntu4) 13.2.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.39 Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.8.01012awsx86_64withglibc2.39 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA L4 GPU 1: NVIDIA L4 GPU 2: NVIDIA L4 GPU 3: NVIDIA L4 GPU 4: NVIDIA L4 GPU 5: NVIDIA L4 GPU 6: NVIDIA L4 GPU 7: NVIDIA L4 Nvidia driver version: 555.42.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        48 bits physical, 48 bits virtual Byte Order:                           Little Endian CPU(s):                               192 Online CPU(s) list:                  0191 Vendor ID:                            AuthenticAMD Model name:                           AMD EPYC 7R13 Processor CPU family:                           25 Model:                                1 Thread(s) per core:              ",2024-08-01T10:53:17Z,module: cuda triaged module: correctness (silent) dependency issue,open,0,6,https://github.com/pytorch/pytorch/issues/132397,High pri to get a repro (which seems to be deterministic) Abovementioned code works for me on `p4dn.24xlarge` ,UPDATE: I could also reproduce the same bug on a completely different dedicated host of the same type (AWS g6.48xlarge with 8x L4s).,Could you check if disabling IOMMU fixes the issue as described here? It would also be great if you could run a pure p2p test (without using PyTorch) to check if this is a system issue and unrelated to PyTorch. ,"I ran the test at https://github.com/NVIDIA/cudasamples/blob/master/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu Seemingly, all devices can access each other, but the latency seems to increase massively (and bandwidth decreases massively) when P2P is enabled (?!?!?!) Here are the logs of the test   p2p_test.log.txt I hope I am not misinterpreting the results. I suspect this is a lowerlevel system issue?",Downgrading high prio as it's likely an issue with CUDA/the hardware setup,"Indeed, the issue went away when I switched from Ubuntu to Amazon Linux, which indicates that it might be an OS/CUDA interface issue. EDIT: Do let me know if I should close this issue."
transformer,dynamic tensor shape of vmap dimention," üêõ Describe the bug I've have a model that internally consists of multiple data streams, each of which having their own unique parameters. To optimize for performance I had stacked different parameters for all the streams together and issued a single call for linear operations with help of `torch.vmap`. This approach worked fine back in torch 2.3, but ever since upgrading to torch2.4 the compile operation fails if I mark the vmap dimension as dynamic. I've stripped all the details from the code down to the smallest repro script:  It seems the constraints created because of vmap are enforcing constant guards rather than dynamic/relative ones. In my searches I've seen that similar issues have previously been reported but nothing was explicitly addressing vmap transforms. I additionally noted that in 3.3 this was issue was bypassed since the graph broke automatically when hitting function transformers, but the default behavior has changed in 3.4. I have captured the following logs: Hopefully it helps:   Versions Collecting environment information... PyTorch version: 2.4.0+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 GPU",2024-08-01T05:32:55Z,actionable module: functorch module: dynamic shapes export-triaged oncall: export,open,0,10,https://github.com/pytorch/pytorch/issues/132381,"Thanks for the detailed report. In the programming model for export, shapes of any state in a module cannot be dynamic. Here `self.weight_streams` is specialized to the value `2` provided as `n_streams` during module init. Then in the forward, `self.weight_streams` is interacting with `stream_idx`. I'm pretty sure this is causing the dynamic dim to be specialized to `2`, which is always an export error. The suggested fix given as part of the error message asks to make this dim be `2` instead of dynamic.","I have tried this with any dimension for the `self.weight_streams`. It doesn't cause any issues.  For example if we create the `StreamedLinear(20, 3, 4, False)` but with the same `stream_idx = [0,1]` the code still fails with specialized value set to `2`. Any operation (such as addition) doesn't cause issues either.  From my understanding, the constraints emitted when processing `vmap` are too restrictive. In essence `vmap` always emits static shape contraints regardless of whether the other parameter is dynamic or not.  Effectively instead of emitting `X[0] = S1 & Y[0]= S1` the `X[0] = 2 & Y[0] = 2` constraints are being emitted.","I also have to mention that similar to export, compiling this module also fails to properly make it dynamic and results in static shapes. which in turn leads to recompilation and finally omitting the module from compilation.","OK, looks like this deserves a deeper look. I'll report back what I find."," you're right, here are the relevant logs for the vmap specialization using `TORCH_LOGS=""export"" TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s0, 2)"" ...` ",  do you know if we can do better here? Looks like `_vmap_increment_nesting` is specializing on its input `batch_size` (which here is `s0`).,This code in particular is causing the specialization: https://github.com/pytorch/pytorch/blob/1962f9475fbc289eee35c1fc24bf71e43f5dbba8/torch/_dynamo/variables/torch.pyL285L289,We should be able to thread the batch size through as dynamic,"I have to mention, just a naive approach of removing guard and returning `x.sym_num` in case of `SymNodeVariable` doesn't cause any additional bugs (to the best of my knowledge) but still will down the line result in specialization. It seems vmap will at some point call `__index__` for the batch dimension when calling into c++ function `_maybe_remove_batch_dim` which will result in guarding and specialization again. ",In triage review this was acknowledged as something we could fix but it remains unassigned. Feel free to send a PR.
transformer,[inline_inbuilt_nn_modules] [BUG] C++ error in `torch.compile` when `dynamic=True`," üêõ Describe the bug This commit f44446e851 breaks the usage of dynamic=True, it can be produced by the following script:   Error is too long, here is just a few part of it: !image  Versions Env:  ",2024-08-01T03:19:44Z,high priority needs reproduction oncall: pt2 oncall: cpu inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/132373,Thanks for opening the issue. I will take a look tomorrow and keep it high prio.,"feng Is it possible for you to check it this stack works for you? https://github.com/pytorch/pytorch/pull/132334 I can try more but on my machine, it does not fail. But it take a very long time to finish. UPDATE  I tried to repro the error on `main` branch, but the error does not repro. Can you share your transformers version? Otherwise, please try the above pull request to see it that resolves the problem.","> feng Is it possible for you to check it this stack works for you? CC([dynamo] Track params/buffers and mark them as static) >  > I can try more but on my machine, it does not fail. But it take a very long time to finish. >  > UPDATE  I tried to repro the error on `main` branch, but the error does not repro. Can you share your transformers version? Otherwise, please try the above pull request to see it that resolves the problem. Hi  . The error could be reproduced with the parameter `export TORCHINDUCTOR_FREEZING=1`.","This also looks like a normal C++ codegen problem that should be fixed (when generating initializer lists, we need to ensure we cast every inner element to a consistent type, because you can't mix SymInt and integer literals)  ","feng Even with the flag, I can't repro. So, please provide some more information around conda/pip packages. Removing my assignment for now. ","It has been fixed in the latest main branch, thanks! "
rag,move torch._functionalize APIs to pybind. add one for marking storage mutations,  CC([test] dynamo: dont hold onto real tensors in the __dict__ of any graph inputs)  CC(return_and_correct_aliasing: skip dispatcher when swapping storage)  CC(fsdp.set_: convey to functionalization that it mutates storage)  CC(move torch._functionalize APIs to pybind. add one for marking storage mutations)  CC(make functorch CSE respect mutations as barriers (like fsdp.set_)),2024-07-31T22:39:25Z,Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/132337
rag,fsdp.set_: convey to functionalization that it mutates storage,Fixes  CC(torch.ops.fsdp.set_  on input doesn't actually modify the input (under torch.compile))   CC([test] dynamo: dont hold onto real tensors in the __dict__ of any graph inputs)  CC(return_and_correct_aliasing: skip dispatcher when swapping storage)  CC(fsdp.set_: convey to functionalization that it mutates storage)  CC(move torch._functionalize APIs to pybind. add one for marking storage mutations)  CC(make functorch CSE respect mutations as barriers (like fsdp.set_)) ,2024-07-31T20:17:34Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) module: dynamo,closed,0,6,https://github.com/pytorch/pytorch/issues/132322,going to try to refactor all our `torch._functionalize*` utils to pybind in a prior PR to reduce boilerplate, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,`torch.nn.transformer.forward` returns incorrect value inside `torch.no_grad()` blocks., üêõ Describe the bug `torch.nn.transformer` returns incorrect value inside `torch.no_grad()` blocks. A minimal example is available. You could also find the same code in Colab.   Versions  ,2024-07-30T12:31:15Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/132136, shouldn't this indeed be part of `oncall: transformer` as it uses `torch.nn.transformer`?
transformer,UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. ," üêõ Describe the bug I get this error:  after all my print statements and training loop properly run. I am on MacOS Ventura 3.4 and have installed the latest version of Pytorch 2.4.4 /opt/anaconda3/envs/andrej_gpt2/lib/python3.10/sitepackages/torch/_tensor_str.py:138: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:335.)   nonzero_finite_vals = torch.masked_select( tensor(0.0029, device='mps:0', grad_fn=) Following along Andrej Kaparthy's tutorial:   Versions Collecting environment information... PyTorch version: 2.4.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.14  (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ] (64bit runtime) Python platform: macOS13.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Versions of relevant libraries: [pip3] numpy==2.0.1 [pip3] torch==2.4.0 [pip3] torchaudio==2.4.0 [pip3] torchvision==0.19.0 [conda] numpy                     2.0.1           py310h52bbd9b_0    condaforge [conda] torch                     2.4.0                    pypi_0    pypi [conda] torchaudio   ",2024-07-30T02:13:36Z,triaged module: mps,closed,0,1,https://github.com/pytorch/pytorch/issues/132110,The natively supported macOS version for nonzero op was lifted to 14.0 due to the previous implementation being flawed. I proposed a pr to correct the error message.
yi,Fix pyi annotation for `ProcessGroupGloo.Options` ,This PR fixes the pyi annotation for `ProcessGroupGloo.Options` based on the definition in the `torch/csrc/distributed/c10d/init.cpp` file.  Fixes CC(Fix the rest of pyi annotations in torch/_C/_distributed_c10d.pyi) ,2024-07-29T19:38:00Z,open source Merged ciflow/trunk release notes: distributed (c10d),closed,0,6,https://github.com/pytorch/pytorch/issues/132080,":white_check_mark:login: ishon19 / (e6b70d9dcb762e7d55da6b8e5854921a10f73c04):white_check_mark:login: ishon19 / (e6b70d9dcb762e7d55da6b8e5854921a10f73c04, 8df2f86a1f6550f6e88b1be9bf103af80de4bc23)The committers listed above are authorized under a signed CLA.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalcuda12.4py3.10gcc9experimentalsplitbuildtest / test (default, 5, 5, amz2023.linux.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: trunk / linuxfocalcuda12.4py3.10gcc9experimentalsplitbuildtest / test (default, 5, 5, amz2023.linux.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Fix pyi annotation for `ProcessGroupGloo.Options` ,This PR fixes the pyi annotation for `ProcessGroupGloo.Options` based on the definition in the `torch/csrc/distributed/c10d/init.cpp` file.  Fixes CC(Fix the rest of pyi annotations in torch/_C/_distributed_c10d.pyi) ,2024-07-29T19:38:00Z,open source Merged ciflow/trunk release notes: distributed (c10d),closed,0,6,https://github.com/pytorch/pytorch/issues/132080,":white_check_mark:login: ishon19 / (e6b70d9dcb762e7d55da6b8e5854921a10f73c04):white_check_mark:login: ishon19 / (e6b70d9dcb762e7d55da6b8e5854921a10f73c04, 8df2f86a1f6550f6e88b1be9bf103af80de4bc23)The committers listed above are authorized under a signed CLA.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalcuda12.4py3.10gcc9experimentalsplitbuildtest / test (default, 5, 5, amz2023.linux.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: trunk / linuxfocalcuda12.4py3.10gcc9experimentalsplitbuildtest / test (default, 5, 5, amz2023.linux.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Fix the rest of pyi annotations in torch/_C/_distributed_c10d.pyi," üêõ Describe the bug Example fix PR for NCCL: https://github.com/pytorch/pytorch/pull/130957 Update the annotations by going to torch/csrc/distributed/c10d/init.cpp finding the binding for the class in question and making sure they match. If the property is read write, there is no need to do separate  annotations, just represent it as a plain field. At minimum ProcessGroupGloo.Options needs to be updated, but there may be others. I am happy to review PRs that fix this, tag me on them.  Versions main ",2024-07-29T15:25:42Z,oncall: distributed good first issue module: typing,closed,0,1,https://github.com/pytorch/pytorch/issues/132054,"Hey , thanks and linked the PR above to fix the issue, please let me know if you've got any feedback! :)"
gpt,Compilation of gpt-fast fails with `torch._dynamo.exc.Unsupported: UNPACK_SEQUENCE MapVariable()`," üêõ Describe the bug After another update, compilation of `gptfast` failed for me with   while compiling following code snippet:   Versions trunk  ",2024-07-29T14:35:42Z,high priority triaged module: regression oncall: pt2 module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/132044,"Let me know if someone else is working on it, otherwise I'd like to try to fix it tomorrow",Somewhat related PR: https://github.com/pytorch/pytorch/pull/131961,patching with https://github.com/pytorch/pytorch/pull/131961 doesn't fix this issue,This should fix it https://github.com/pytorch/pytorch/pull/132069,> This should fix it CC([dynamo] make more unpack_var_sequence calls forced) Confirmed. 
rag,torch.QUInt4x2Storage: RuntimeError cuda_dispatch_ptr INTERNAL ASSERT FAILED on torch 2.5.0.dev20240708+cu121," üêõ Describe the bug I met an error `RuntimeError cuda_dispatch_ptr INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/DispatchStub.cpp"":137, please report a bug to PyTorch. DispatchStub: missing CUDA kernel` when running the following code. Code:  Log:   Versions  ",2024-07-29T10:29:29Z,oncall: quantization,open,0,0,https://github.com/pytorch/pytorch/issues/132029
transformer,All-reduce followed by division does not correctly divide data," üêõ Describe the bug When sharing weights between two layer of a model (such as tying input and output embeddings in a transformer model), an allreduce sum followed by averaging by world_size doesn't average the weights. Dividing the .data works. For example, you can run the following script:   Versions PyTorch version: 2.1.1 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0144genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA H800 GPU 1: NVIDIA H800 GPU 2: NVIDIA H800 GPU 3: NVIDIA H800 GPU 4: NVIDIA H800 GPU 5: NVIDIA H800 GPU 6: NVIDIA H800 GPU 7: NVIDIA H800 Nvidia driver version: 550.54.15 cuDNN version: Probably one of the following: /lib/x86_64linuxgnu/libcudnn.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_",2024-07-29T10:15:32Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/132027,"`param = param / world_size` creates a new tensor object for the lefthandside `param` compared to the righthand side `param`, but this new lefthandside `param` is never registered back to the module. In other words, you are doing something like `new_param = param / world_size` but then throwing away the result of `new_param`.",Oh can't believe I missed this. Closing now
,improve mkldnn_linear_pointwise_binary performance for contiguous tensor with non default contiguous strides,Fixes  CC([INDUCTOR] Performance regression when batch size = 1 in text generation) ,2024-07-29T07:31:30Z,module: cpu open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/132019, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Layer normalization on Nested Tensor ragged dimension fails when `lengths is not None`, üêõ Describe the bug Performing layer normalization along the ragged dimension on a nested tensor with holes (`lengths is not None`) is currently incorrect but does not throw an error:   Versions fbcode H100 7/26/24 ,2024-07-26T23:40:47Z,triaged module: nestedtensor,open,1,0,https://github.com/pytorch/pytorch/issues/131967
transformer,[FSDP2] root moduel parameters stays unsharded after forward before backward," üìö The doc issue this question comes up from torchtune. after forward before backward, root module parameters are unsharded. want to confirm if this is by design. I know it's performant to keep root unsharded because they will be used in backward immeidately. but want to confirm TLDR repo  full repro   Suggest a potential alternative/fix _No response_",2024-07-26T23:18:39Z,triaged release notes: distributed (fsdp2),closed,0,3,https://github.com/pytorch/pytorch/issues/131965,This is by design.,"If people find this kind of implicit behavior confusing, we could put the burden on the user to pass `reshard_after_forward=False` to the root module. I would be okay with that. For now, the user can call `root_module.reshard()` if they want to shard the root module after forward.","> If people find this kind of implicit behavior confusing, we could put the burden on the user to pass `reshard_after_forward=False` to the root module. I would be okay with that. >  > For now, the user can call `root_module.reshard()` if they want to shard the root module after forward. thanks. confirmed with torchtune that they only ask this for understanding purpose"
transformer,Optimize test transformers," Reduced number of skipped test cases  Merged redundant test cases **Benchmark:**  _These are approximate numbers from running test_transformers.py on a single H100, and can change based on the device._ ",2024-07-26T18:00:02Z,oncall: distributed topic: not user facing module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/131919, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/131919/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/10308563271
mistral,"Add `padding_side` to `pad_sequence` with `""left""` and `""right""` options (`""right""` as default)","Fixes CC([feature request] Adding Pre and Post padding functionalities to pad_sequence function) Reattempt of CC(add pad_from_front param to pad_sequence). Thank you so much to  for your excellent work! As I was trying to create a more efficient LLM data collator, I realized that `pad_sequence` only supports right padding, even though left padding is a very common format for LLMs, like Llama and Mistral. The proposed alternative implementation was to use multiple flips, which tends to be 1.5x2x slower. Instead we can add a `padding_side` parameter as there is for for Hugging Face tokenizers, which requires only a very small change in the C++ code. Here are the benchmarks of the new implementation! `float32`: !eaaa95ef938445d2be566898bc1d3514 `bool`: !892f32da8d9a492b950718d3f0a41e8e Code:  cc:     ",2024-07-26T10:02:10Z,triage review module: nn module: rnn triaged open source Merged ciflow/trunk release notes: cpp,closed,1,7,https://github.com/pytorch/pytorch/issues/131884,"So it would seem that `test_jit` isn't happy about `typing.Literal`, something that was added to Python in 3.8 almost 5 years ago. While I could forego using `Literal`, it does well to document the acceptable values and is not without precedence in `torch` itself. Given JIT's unmaintained status, I wonder what alternatives there might be to removing the `Literal` annotation? What the plan would be to unblock ourselves from being limited by JIT's frozen set of supported annotations?  ",I'm happy to take the algorithm change but some bikeshedding on the API may be necessary. The easiest way to bypass the bikeshed is to appeal to some preexisting API that we can copy,> The easiest way to bypass the bikeshed is to appeal to some preexisting API that we can copy I linked the Hugging Face tokenizer API that I based this name off of in the description. Is something like that what you had in mind?,"We discussed this at nn triage and were generally ok with the naming of the API being consistent with that of the Huggingface Tokenizer api, but leaving triage review in case there were other thoughts",Triage review looked over the API and we have decided to accept it as is. So we just need a correctness review on this PR., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,MPS backend breaking on llama 3.1 8B on Macbook M3," üêõ Describe the bug python code being run under the hood:  I am getting the following error:  This is a device issue, the same code runs fine on the cpu. I am installing torch via `poetry add 'transformers[torch]'` Related issues I've found on this topic (without a clear solution):  CC(MPS convolution crashing on .expand kernel (non-contiguous))  CC(MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 20480 bytes ')  Versions Collecting environment information... PyTorch version: 2.4.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.12.4 (main, Jul 13 2024, 01:18:59) [Clang 15.0.0 (clang1500.1.0.2.5)] (64bit runtime) Python platform: macOS14.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Max Versions of relevant libraries: [pip3] mypy==1.11.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==2.0.1 [pip3] torch==2.4.0 [conda] Could not collect ",2024-07-26T05:17:10Z,high priority triaged module: mps,closed,1,9,https://github.com/pytorch/pytorch/issues/131865,"Hi ! Thanks for reporting this! The `2**32` byte limitation is currently a hard set limitation on MPSNDArray side and currently we should try to circumvent this on the PyTorch backend side by checking if the computation truly needs to happen with a  > `2**32` byte array and if so, tile it into smaller subarrays. For that to happen I would need a code snippet I am able to run as is to reproduce the error and follow the stack the which op is causing this.  would you be able to extend the code snippet a little bit so that it can be run by anyone to reproduce the error you are seeing? Currently it's missing some parts.","> Hi ! Thanks for reporting this! The `2**32` byte limitation is currently a hard set limitation on MPSNDArray side and currently we should try to circumvent this on the PyTorch backend side by checking if the computation truly needs to happen with a > `2**32` byte array and if so, tile it into smaller subarrays. For that to happen I would need a code snippet I am able to run as is to reproduce the error and follow the stack the which op is causing this. Hmm, I thought this is no longer the case in Ventura, i.e. I can allocate 4+Gb array and we even test it in CI, see https://github.com/pytorch/pytorch/blob/9440a4824dee84ddbdd56437e4f022bbbe0eaa6f/test/test_mps.pyL7153L7160"," , this is not to do with 4GB allocation issue, but rather operation on Large tensors using MPS kernels which require indexing beyond 4GB boundaries in the kernel.",Yeah sorry my bad on not being explicit in the initial message. Indeed it is the feature of MPS kernel indexing taking happening in 32bits. Once I get a permission to look at the licensed model I'll check if we could tile the problematic computation into pieces that are not going over the indexing limit.,"> Hi ! Thanks for reporting this! The `2**32` byte limitation is currently a hard set limitation on MPSNDArray side and currently we should try to circumvent this on the PyTorch backend side by checking if the computation truly needs to happen with a > `2**32` byte array and if so, tile it into smaller subarrays. For that to happen I would need a code snippet I am able to run as is to reproduce the error and follow the stack the which op is causing this. >  >  would you be able to extend the code snippet a little bit so that it can be run by anyone to reproduce the error you are seeing? Currently it's missing some parts.  the prompt contains PII, so I'd rather not share, but I can say that this prompt that caused the issue was about 95k characters long. I did not get the same error testing with a prompt around 100 characters. I put together the following script that reproduces the error on my machine with no external dependencies outside of transformers and torch: ", Totally understandable. Thanks for the stand alone repro script that should be all we need to nail this down. However I still need to wait for people higher up to sign agreements before I can load the LLaMA v3.1 8B weights. But once they do I'll get to fixing this.,Ok I have the model to look at. From the trace I can see that the offending operation is batch matmul and here's a minimal repro case for the issue using the shapes encountered in the script posted by :  I'll start looking into what can be done about the op.,Ok I can confirm that I'm able to run the model without issues after adding a workaround that breaks the batch matmul above into smaller batches that are able to run within the MPS boundaries. I'll need to do some more cleanup to make it slightly more optimal but I aim to get a PR up for it tomorrow.,"  I started PR CC(Enable batch matmul for result sizes > 2**32 the tensor can be split along batch axis) that should solve the issue you are seeing by splitting up the problematic batch matmul to a more manageable size. With this change I am able to run the code snippet you provided to reproduce the error and get a reasonable looking printout of  python def find_needle(haystack):     """"""     Find the index of the needle in the haystack.     Args:     haystack (str): The string to search in.     Returns:     int: The index of the needle if found, 1 otherwise.     """"""     try:          Use the index() method to find the index of the needle         index = haystack.index('needle')         return index     except ValueError:          If the needle is not found, return 1         return 1 haystack = """""" hay hay hay hay hay hay hay hay hay hay hay hay hay hay hay hay hay hay test_index: 42 ``` Feel free to try out the PR if you are building pytorch locally, and let us know if solves the issue. Or you can wait for us to get the change reviewed and merged and try out the nightly release once it has landed. I'll update here once the change lands and is available in a nightly wheel."
gemma,[export] Node.meta _enter_autocast is missing val field,"When exporting the gemma model with huggingface `transformers`,   fails with   Environment  ",2024-07-25T22:50:34Z,export-triaged oncall: export,closed,0,9,https://github.com/pytorch/pytorch/issues/131829,Seems autocast related. Assign to . ,  Thanks! I think this should be resolved after we land CC([export] Convert autocast to HOO). ," we have landed a fix in https://github.com/pytorch/pytorch/pull/132677, please let us know if you still encounter problems with this. Thanks!","Thanks, I will verify.","I'm still seeing this error, although not with transformers. I'm using torch 2.4.1","> I'm still seeing this error, although not with transformers. I'm using torch 2.4.1 can you post your error message and a repro? The fix should be included in 2.4.1. edit: The fix is NOT included in 2.4.1. ",Sure. It involves the lucidrains repo `rotaryembeddingtorch`. The forward pass of RotarEmbedding is decorated with (enabled=False)  Gives the error: ,"I don't think the fix is in 2.4.1 (the feature was too big to cherry pick into the build), but you could try the nightlies?",It works on nightlies. Thanks
transformer,xpu: add check for supported devices to xpu initialization and torch.xpu.is_available(),"As of now, https://github.com/pytorch/pytorch/commit/bf6aae1468659d611959d94c314639e402edc3fe, XPU backend supports limited set of GPU devices, PVC, Meteor Lake. However, XPU backend does not check for supported device types and backend will be reported to be supported on a lot more Intel GPU devices, for example, on Alder Lake. However, trying to run backend on such devices will actually fail. On Alder Lake I get:  Can XPU backend, please, start to check whether it can actually run on the underlying Intel GPU, fail to initialize and report `False` in `torch.xpu.is_available()` if it can't? That's important because sometimes there are device agnostic use cases. For example  consider below Huggingface example which specifies just `device_map=auto`. User's intent might be to run on CPU or on CUDA, but it might get dispatched to unsupported Intel GPU and fail.  CC:      ",2024-07-25T19:33:25Z,triaged module: intel module: xpu,open,0,9,https://github.com/pytorch/pytorch/issues/131799,"Yes, we mentioned it in our discussion. But I might think there was no such error log or not informative log on Alder lake. The error log makes sense to me.  Return false from calling `torch.xpu.is_available` is, 1. Not reasonable, if there are iGPU (unsupported) and dGPU (E.g. Arc, supported) installed on the system. 2. Not aligned with semantics of other PyTorch backends. Return true, only if CUDA source is compiled and device count doesn't equal to 0. Like,  Options in my mind, 1. There is not that big difference between bypassing Alder lake but going into CPU/CUDA and raising runtime error without the gating, when people tend to use Alder lake capability only. So we should enhance the error log with explicit platform name or adding other informative log. 2. Find a new API, like `get_device_capability`, `get_device_properties`, not sure if it works. And add it HF framework for checking  workability of the current platform. I might think CUDA has the same issue. As for NV, they just consider dGPU only. Archs of iGPU and dGPU may have big difference before for us, requiring different implementation in driver and compiler. The panic of compatibility is much slighter than us?","Having the case with iGPU and dGPU to handle is a valid point. I don't see contradictions here though. Can logic be modified as follows? 1. `device_count()` should count only supported devices. If there is iGPU + dGPU and both are supported it should return =2, if iGPU is not supported it should return =1 (counting only dGPU). And it will return =0 if there are no supported devices. 2. `is_available()` don't need modifications. It will return true or false depending on the cases described above 3. `xpu:0` will mean first supported intel device instead of first found (and potentially unsupported) device  this also don't need any modifications I think. I just note the behavior. Will this work?","LGTM. The feature should be about XPU device enumeration in XPU device pool. XPU device pool should only include devices supported by current driver and compiler. So you mention me there seems showing up a trick or a contradiction in SYCL runtime/driver. We query a GPU device from SYCL runtime, which should mean driver commits its workability, but operations on the device fails due to driver runtime error. Two options, 1. Walk around in PyTorch to maintain a list saying what platforms are supported. But I think it is not compatible or hard to maintain since user cases are various. 2. Require driver to return a valid GPU device basing on current driver version and hardware platform.   Any thoughts?","From my perspective, this doc clarifies the hardware prerequisites. And the user can get the device name via `torch.xpu.get_device_name` or `torch.xpu.get_device_properties`.  It seems enough to let the users judge if their GPU is supported.","> From my perspective, this doc clarifies the hardware prerequisites. And the user can get the device name via torch.xpu.get_device_name or torch.xpu.get_device_properties. It seems enough to let the users judge if their GPU is supported No, it's not enough. This will lead to ~~disaster~~ bad user experience. Software must report what it supports and nothing else. And it must actually work on what it supports. The doc you refer to further refers to intel side documentation which clearly states that limited set of Intel GPUs is supported depending on PyTorch release. For 2.4 that's PVC, for 2.5 that's PVC, ATSM, DG2, MTL. Do you see AlderLake in this list? no. Why it's reported by torch.xpu then? is it after all supported or not? This is not right to offload lists of supported/not supported hardware to end user applications   such lists will need to be for each application, will run out of sync pretty soon, etc., etc. And I believe that's definitely a concern for any packaging of pytorch with XPU backend: you get a package which you must be extremely careful with and install only on appropriate environment and if you will install it on a wrong environment you risk to break something. For example, consider what user should install in a system with iGPU (ADL) +dGPU (DG2)? what if user wants to run both CPU and XPU workloads? what if user use oneAPI without pytorch with ADL, but wants pytorch with DG2? We have multiple variants of usage here, I highlighted few, but there are much more combinations. With such variety of choices it will be a nightmare for the user to install correct versions of pytorch/oneapi if these versions attempts to automatically recognize GPUs they are not supposed to work on.","> So you mention me there seems showing up a trick or a contradiction in SYCL runtime/driver.  XPU backend in pytorch declared support (via doc) for the subset of GPUs (PVC, ATSM, DG2, MTL) supported by SYCL/runtime driver which supports much more. According to at least the following docs they support TGL+: * https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapidpcppsystemrequirements.html * https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapibasetoolkitsystemrequirements.html  Setup doc provides repackaging of oneAPI components. How these packages were built? and how they differ from original oneAPI packages available via different setup instruction? Were supported platforms fused to the actual list exposed for pytorch (PVC, ATSM, DG2, MTL)? I believe there was no fuse and that's just repackaging putting packages in another location. As a result we actually get oneAPI capable for all oneAPI platforms (TGL+) which reports exactly that  we support TGL. I agree with your outline for 2 options above.. Some additional thoughts on top of them: 1. (list of supported devices in PyTorch) Realistically, I think that's the only viable option in the short/middle term (on the span of ~1 year). I would add such a list on pytorch side + added environment variable to bypass the list (might be useful to try out new platforms). 2. Here Intel needs to decide which GPUs it supports for pytorch and how this maps to oneAPI platforms. On our technical level the thing we can consider is this. Can we query for any sycl device properties which will tell us whether XPU backend will be able to run on such device? I afraid this might be nontrivial thing to do. Basically someone will need to debug platforms which fail like ADL and extract corresponding property characteristic.  Problem is that most likely there is no such characteristic and it actually should work on TGL and ADL, but there are bugs preventing this. And this might be not rational thing to invest in fixing such bugs. If this will be the case, then 1st will be the only viable option  restrict reported list of supported GPUs somewhere. With all above, I want to add that as of now ball to restrict reported list of supported platforms is on torchxpuops side. That's because you actually already have such list in a form of dpcpp command line flags. Here: https://github.com/intel/torchxpuops/blob/bbd200abe2d3a3e290c45e04127723f65293ed7e/cmake/BuildFlags.cmakeL87 You can just make this list available to you at runtime and check available devices against it.","> https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapidpcppsystemrequirements.html > https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapibasetoolkitsystemrequirements.html Actually, https://dgpudocs.intel.com/devices/hardwaretable.html, UMD (LTS or Rolling) and KMD (OS kernel + kernel drivers) decide which hardware works on current SW and HW platform. In the moment, it is hard to say oneAPI package can provide correct behavior or correct claim independently. In my experience, these things are decoupled, 1. Be able to compile. oneAPI (SYCL compiler/runtime compiler) commits a platform only if which is listed in compilation targets list. But probably, our IGC (included in UMD) doesn't work. 2. Be able to compute. oneAPI (SYCL runtime) returns a valid device only it is able to be probed in driver. But probably, the current kernel driver (KMD) doesn't work (https://dgpudocs.intel.com/devices/hardwaretable.html) when some computations occur. So I prefer oneAPI claims a max support list. Users have to check the matrix in driver Docs and find what they could actually get. I think it is confused to Intel GPU users. The best practice in my mind is, 1. Re Docs, we should not have a separate support list both in oneAPI and drivers, when we have a page to couple them together. 2. Re runtime, I m not sure of if there is a bug on ADL or the ADL SKU is not supported on your kernel driver. SYCL runtime/driver should always return a valid device only if it is workable (be able to compile and compute) in computation subsystem (SYCL/L0/KMD), not it is able to be probed only. > With all above, I want to add that as of now ball to restrict reported list of supported platforms is on torchxpuops side. That's because you actually already have such list in a form of dpcpp command line flags. Here: https://github.com/intel/torchxpuops/blob/bbd200abe2d3a3e290c45e04127723f65293ed7e/cmake/BuildFlags.cmakeL87 You can just make this list available to you at runtime and check available devices against it. It's a list of AOT build, not the support list. Devices not on the list still work with JIT build only if both oneAPI and driver support them. AOT build list aligns CUDA implementation in PyTorch supports the most recent and popular platforms. They are performance consideration. The white list we are talking about is functionality."," : please, decide which GPUs are going to be supported with XPU backend. Once decided, you can treat this issue either as the issue to fix the reporting list (if you will chose to restrict supported platforms to something, as of now DG2+) or as an issue to fix bugs to make full range of reported platforms work (as of now ADL fails).",WIP in https://github.com/intel/torchxpuops/issues/664
transformer,[INDUCTOR] Performance regression when batch size = 1 in text generation," üêõ Describe the bug The performance is much worse when batch size = 1 compared to batch size > 1 Profile when batch size = 1 (500ms) !image Profile when batch size = 4 (60ms) !image Command:     Versions Collecting environment information...                                                                                                                                                                              PyTorch version: 2.5.0.dev20240722+cpu                                                                                                                                                                             Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Rocky Linux release 8.10 (Green Obsidian) (x86_64) GCC version: (GCC) 12.2.0 Clang version: Could not collect CMake version: version 3.27.9 Libc version: glibc2.28 Python version: 3.8.19  (default, Mar 20 2024, 12:47:35)  [GCC 12.3.0] (64bit runtime) Python platform: Linux4.18.0553.5.1.el8_10.x86_64x86_64withglibc2.10 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              224 Online CPU(s) list: 0223 Thread(s) per core:  2 Core(s) per socket:  56 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU family:          6 Model:               143 Model name:          Intel(R) Xeon(R) Plati",2024-07-25T02:39:59Z,oncall: cpu inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/131734,fangintel ,"sync with feng, this is a regression issue which only for BS = 1. He will help to narrow down the nightly version which causes this regression.",The regression happens from `2.5.0.dev20240613+cpu` to `2.5.0.dev20240614+cpu`,"The rootcause is onednn has hit the ref path, for input of size [1, 1, 4096] and view it to [1, 4096], we get the stride of [0, 1]. In this case, oneDNN uses ref implementation."
llama,[tracker] Open issue with inline_inbuilt_nn_modules," üêõ Describe the bug We are planning to turn on the inline_inbuilt_nn_modules flag to True by default. There are a few hiccups here and there, but we will turn on the flag and fix these issues later. This is the issue to track.  ",2024-07-24T20:11:37Z,triaged oncall: pt2 module: dynamo,open,1,2,https://github.com/pytorch/pytorch/issues/131696,https://github.com/pytorch/pytorch/commit/f44446e851 causes a cuda memory leak on the torchbench sam_fast model: https://github.com/pytorch/benchmark/issues/2399 is this expected?,"> f44446e851 causes a cuda memory leak on the torchbench sam_fast model: pytorch/benchmark CC(Allow kwargonly inputs to DataParallel) is this expected? This commit f44446e851 also breaks the usage of `dynamic=True`, it can be produced by the following script:  Tracked in  CC([inline_inbuilt_nn_modules] [BUG] C++ error in `torch.compile` when `dynamic=True`)."
transformer,Projeto liliti stk 3.6.9 intelig√™ncia artificial multimidal fase 5 , **1. Simula√ß√£o de Viagem no Tempo (Virtual)** **Objetivo:** Simular cen√°rios futuros baseados em dados hist√≥ricos.   **2. Interface de Realidade Aumentada e Virtual** **Objetivo:** Implementar uma interface b√°sica para RA/RV.   **3. Computa√ß√£o Qu√¢ntica** **Objetivo:** Implementar uma simula√ß√£o b√°sica para algoritmos qu√¢nticos.   **4. Reconhecimento Facial e An√°lise Emocional** **Objetivo:** Detectar e analisar express√µes faciais.   **5. Assistentes Pessoais Hologr√°ficos** **Objetivo:** Implementar um assistente hologr√°fico simples.   **6. Algoritmos de Previs√£o Temporal** **Objetivo:** Implementar algoritmos para previs√µes temporais.   **7. Gera√ß√£o de Conte√∫do Criativo com IA** **Objetivo:** Gerar conte√∫do criativo com IA.   **8. Interfaces de C√©rebroComputador** **Objetivo:** Implementar uma interface b√°sica c√©rebrocomputador.   **9. Simula√ß√£o de Realidades Paralelas** **Objetivo:** Criar e explorar realidades alternativas. ,2024-07-24T19:55:42Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/131692,"Closing, as unclear what this has to do with PyTorch, if you have a more concrete example please do not hesitate to open a new issue"
yi,TCP socket keeps trying IPv6 address and doesn't try IPv4," üêõ Describe the bug We see the following consistent log message which eventually ends with an error:  The intention of this code seems to be to try IPv4 if IPv6 fails: https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/socket.cppL785, but IPv4 is never tried and an exception is thrown.  Versions PyTorch 2.3 ",2024-07-24T19:54:26Z,oncall: distributed triaged,open,1,8,https://github.com/pytorch/pytorch/issues/131691, please note that link you've posted is unaccessible,"I think the code being referred to is here: https://github.com/pytorch/pytorch/blob/6e640a0acfbf9073aae1734db2fceb94c3d8f28d/torch/csrc/distributed/c10d/socket.cppL510L526 But yeah it does look like it errors without getting to the second `tryConnect` for IPv4. I'm not sure why, it seems like a bug. One alternative is instead of throwing the error we just exit after a certain amount of retries, but we would need to figure out how this would align with the timeout value.  ",>  please note that link you've posted is unaccessible Fixed the link :),Taking a look, can you try running with:  To get the full logs? I suspect we're throwing an error somewhere and thus the retry isn't working due to the exception," I don't have a reproducible example for this, pulled logs above from a job. I believe the exception you are referring to is this which I've mentioned above: `torch.distributed.DistNetworkError: The client socket has timed out after 1800s while trying to connect to (host, port).` My understanding is that we get stuck in this loop despite hitting errors: https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/socket.cppL839 (since retry=True). Then eventually we throw the timeout error here: https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/socket.cppL878", I recall this happening before on poorly configured dual stack machines  do you know the configuration of the machine? It has an ipv4 address but it's not in DNS/not routable and only the ipv6 address works?,"> It has an ipv4 address but it's not in DNS/not routable and only the ipv6 address works? Its actually the opposite, where only ipv4 works and ipv6 kept failing and we never tried ipv4."
llm,Add Sleef Implementation for maximum Kernel for ARM,The NEON Vectorized implementation does not use SLEEF functions for maximum Implementation. So updated maximum function with sleef calls for better performance on graviton3.It showed good performance improvement in LLM models. The results are taken in graviton3 machine as follows:  This maximum kernel is used in softmax. The performance timing of softmax with default and sleef change is as below:(graviton3 machine)   ,2024-07-24T08:33:42Z,module: cpu triaged open source Merged ciflow/trunk topic: performance ciflow/linux-aarch64,closed,1,14,https://github.com/pytorch/pytorch/issues/131642,  Can you please initiate the CI pipelines for this PR. This change is related to ARM CPU's. Please also add the **ciflow/linuxaarch64** label to this PR.     Can you please review this PR.,"Hi  ,  the performance improvement looks good!  what tests did you run to make sure there are no precision issues with this implementation?","> Hi  , the performance improvement looks good! what tests did you run to make sure there are no precision issues with this implementation? Hi , Thank you. For accuracy check, I have taken the output of softmax layer with and without change as numpy array and checked using np.allclose() (checked for 4 to 5 different shapes)","  label ""module: cpu"""," Merge failed **Reason**: Approvers from one of the following sets are needed:  CPU ATen backend (mingfeima, XiaobingSuper, jgong5, vfdev5, lesliefangintel)  CPU inductor (lesliefangintel, jgong5, EikanWang)  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10, ...)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet, ...) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","Hi  , the CI failures that occurred 2 days ago seem legit. `ciflow/trunk` runs more CI jobs than the default case, so it looks like it exposed some new failures.","> Hi  , the CI failures that occurred 2 days ago seem legit. `ciflow/trunk` runs more CI jobs than the default case, so it looks like it exposed some new failures.  ,Thank you. I will check and update .", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,">  label ""topic: not user facing""  why did you use this label? Not user facing should be reserved for PR that do not have any user impact, while this one improves perf, so it should be ""topic: performance"" or something","> >  label ""topic: not user facing"" >  >  why did you use this label? Not user facing should be reserved for PR that do not have any user impact, while this one improves perf, so it should be ""topic: performance"" or something I see, Thanks for letting me know. I will take care of it from future PR's. Right now, sleef isn't enabled in the default CPU builds on ARM, so I kind of put the ""not user facing"" label thinking it wouldn't impact the default pypi aarch64 binaries performance. Will add appropriate labels for future PR's whenever its perf related."
transformer,[dtensor][debug] adding new noise level which allows users to only print operations with dtensors,"  CC([dtensor][debug] adding new noise level which allows users to only print operations with dtensors) **Summary** I have added a new noise level between the existing levels of 1 and 2, such that the noise level controls are now:           0. prints modulelevel collective counts           1. prints dTensor operations not included in trivial operations (new noise level)           2. prints operations not included in trivial operations           3. prints all operations This gives the user more flexibility in controlling what information they want to use. The noise levels are used both for creating the console/file log and the json dump. In the example file, I have changed the module_tracing examples to noise level 0 and have changed my transformer examples to show off the new noise level.  **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_json_dump 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing 3. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_module_tracing 4. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_module_tracing ",2024-07-24T00:13:42Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/131592, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""CI timed out"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[easy][inline-inbuilt-nn-modules] Update test,  CC([inlininginbuiltnnmodules] Change cpu inductor threshold for lennard_jones)  CC([easy][inlineinbuiltnnmodules] Update test)  CC([dynamo][exception] Remove older specialization for StopIteration)  CC([dynamo] Rename TENSOR_ALIASING to OBJECT_ALIASING. Permit OBJECT_ALIASING for dict guards)  CC([dynamo] Delete wrong assertion in bind_args)  CC([dynamo] Support set on KeysView)  CC([dynamo] Support __contains__ on  __dict__ on UserDefinedClassVariable)  CC([dynamo] Support dict conversion of objects derived from MutableMapping)  CC([dynamo] Support if callable on list) ,2024-07-23T23:51:32Z,Merged ciflow/trunk topic: not user facing module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/131563," merge f ""stuck ci"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[DONOTREVIEW] Modifying existing APIs to verify componentized utils work as expected,  CC([DONOTREVIEW] Modifying existing APIs to verify componentized utils work as expected)  CC([DONOTREVIEW]distribute_tensor_takes_global_padding)  CC([DONOTREVIEW] Compute global padding)  CC([DTensor][utils] Create utils for get padded and unpadded tensor)  CC([DTensor][utils] Add util to compute_padded_and_unpadded_local_shape) This is a prototype testing out the componentized utils for static padding. We will not land this PR. ,2024-07-23T21:30:29Z,oncall: distributed Stale ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/131528,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,[NestedTensor] Add support for transposed NestedTensors where ragged_idx > 1 for the layer_norm operator,"  CC([NestedTensor] Add support for transposed NestedTensors where ragged_idx > 1 for the layer_norm operator)  CC([NestedTensor] Integrate the layer normalization operator along the jagged dimension into NestedTensor)  CC([NestedTensor] Integrate the softmax operator along the jagged dimension into NestedTensor)  CC([NestedTensor] Add support for transposed NestedTensors where ragged_idx > 1 for sum and mean operators) Add support for transposed, noncontiguous `NestedTensor`s, where `ragged_idx > 1`, for the `layer_norm` aten operator. This diff enables reducing along the jagged dimension for noncontiguous `NestedTensor`s, transposed between nonbatch dimensions as well as between a ragged and a nonbatch dimension. For example, users can now reduce a `NestedTensor` of shape `(B, M, *, N)` along `*` or `(B, N, M, *)` along `*`. Write unit tests to verify the accuracy of the transposed ragged reduction implementation for `torch.nn.functional.layer_norm`. Differential Revision: D60015387",2024-07-23T20:46:05Z,open source fb-exported Stale,closed,0,11,https://github.com/pytorch/pytorch/issues/131520,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,This pull request was **exported** from Phabricator. Differential Revision: D60015387,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,[NestedTensor] Add support for transposed NestedTensors where ragged_idx > 1 for sum and mean operators,"  CC([NestedTensor] Add support for transposed NestedTensors where ragged_idx > 1 for the layer_norm operator)  CC([NestedTensor] Integrate the layer normalization operator along the jagged dimension into NestedTensor)  CC([NestedTensor] Integrate the softmax operator along the jagged dimension into NestedTensor)  CC([NestedTensor] Add support for transposed NestedTensors where ragged_idx > 1 for sum and mean operators) Add support for transposed, noncontiguous `NestedTensor`s, where `ragged_idx > 1`, for the aten operators `sum` and `mean`. This diff enables reducing along the jagged dimension for noncontiguous `NestedTensor`s, transposed between nonbatch dimensions as well as between a ragged and a nonbatch dimension. For example, users can now reduce a `NestedTensor` of shape `(B, M, *, N)` along `*` or `(B, N, M, *)` along `*`. Parametrize existing unit tests and add new unit tests verifying the accuracy of implementations on `NestedTensor`s that transpose between 2 nonbatch dimensions as well as between a ragged and a nonbatch dimension. Differential Revision: D59847927",2024-07-23T20:45:56Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/131517,This pull request was **exported** from Phabricator. Differential Revision: D59847927,This pull request was **exported** from Phabricator. Differential Revision: D59847927," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D59847927," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D59847927," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Fix fake tensor SymInt caching when there's a SymInt storage_offset,Test Plan: Internal unit tests failed before and succeeded after. Differential Revision: D60131273 ,2024-07-23T19:04:23Z,fb-exported Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/131500,The committers listed above are authorized under a signed CLA.:white_check_mark: login: aorenste / name: Aaron Orenstein  (baa7095ba24d17acc5eec8ea50ac852d20fc5648),This pull request was **exported** from Phabricator. Differential Revision: D60131273,This pull request was **exported** from Phabricator. Differential Revision: D60131273,This pull request was **exported** from Phabricator. Differential Revision: D60131273,This pull request was **exported** from Phabricator. Differential Revision: D60131273, merge (Initiating merge automatically since Phabricator Diff has merged)," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," merge f ""merged internally"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,[do not merge]test CI test coverage,test CI tests coverage make some changes to the test to see if they run in the CI tests   CC([do not merge]test CI test coverage)  CC([3/3] 3D Composability  move tp dp tests)  CC([2/3] 3D Composability  move pp tests)  CC([1/3] 3D Composability  move fsdp tests) ,2024-07-22T17:02:27Z,oncall: distributed Stale topic: not user facing ciflow/periodic ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/131343,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Applying the same function to tensors with equal values yields different results. ," üêõ Describe the bug I am encountering a situation where running exactly the same function on two sets of tensors with corresponding equal values yields different results (i.e., (t11, t12, t13) = (t21, t22, t23); the two triplets of tensors are different tensor objects, but they have the same corresponding values). Any idea what might be going on? The differences are on the order of 1E7, but if the arguments are exactly equal, it's odd that the result isn't exactly equal.    Versions Collecting environment information... PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.11.6  (main, Oct  3 2023, 10:40:35) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.4.0189genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.147.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.0 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU",2024-07-22T15:25:58Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/131330,Hey! Small numerical differences are expected due to floating point numbers (1e7 is exactly in that range of floating point precision error). See https://pytorch.org/docs/stable/notes/numerical_accuracy.html for more details. Also see https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.htmltorch.use_deterministic_algorithms for forcing deterministic algorithms to be used.
llama,[inductor pattern][cpu] add a new int8 woq mm pattern,"Add int8 woq mm pattern for llama, which successfully hits all the 675 woq linears.  Implementation Differences with previous patterns:  scale is fp32 instead of bf16.  no reshape for x and output.  Performance Performance data on llama, with 1 numa node, freezing mode:  Before pattern match:  Summary:  inferencelatency: 4.810 sec. firsttokenlatency: 0.280 sec. resttokenlatency: 0.146 sec. P90resttokenlatency: 0.148 sec.  After pattern match:  Summary:  inferencelatency: 2.537 sec. firsttokenlatency: 0.964 sec. resttokenlatency: 0.051 sec. P90resttokenlatency: 0.052 sec. ",2024-07-22T07:27:10Z,triaged open source Stale topic: not user facing module: inductor ciflow/inductor,closed,2,10,https://github.com/pytorch/pytorch/issues/131310,Why the first token latency even worse after this PR?,Summary 1. The fp32related discussions above are expected to be solved by https://github.com/pytorch/ao/pull/534.2.  2. The regression of first token needs a further analysis.,"Hi , can you please add info about the precise benchmark you used, perhaps from your bash history? I used the following command, but the benchmark goes on for a long time, and only posts E2E latency. I'm guessing computing pertoken latency requires modifying the transformers library. Please confirm, thanks!  I'm also seeing this warning. Please advise if it's normal. Thanks! ","> Hi , can you please add info about the precise benchmark you used, perhaps from your bash history? I used the following command, but the benchmark goes on for a long time, and only posts E2E latency. I'm guessing computing pertoken latency requires modifying the transformers library. Please confirm, thanks! >  >  >  > I'm also seeing this warning. Please advise if it's normal. Thanks! >  >  This is the script to run on one node: `numactl C 56111 m 1 python ../../../../../../models/language_modeling/pytorch/llama/inference/cpu/run_llm.py benchmark numwarmup 1 numiter¬†2 tokenlatency dtype 'bf16' m 'metallama/Llama27bhf' maxnewtokens 32 inputtokens 32 batchsize 1¬†weightonlyquant torchao weightdtype INT8`. Can't remember about the warning.","After offline discussion with , it turns out that the runtime error I'm encountering at my end (`torch._dynamo.exc.InternalTorchDynamoError: 'PlainAQTLayout' object has no attribute 'layout_type') is similar to https://github.com/pytorch/ao/pull/534issuecomment2246726387. , is it possible for you to rebase your local PyTorch repo & verify whether or not you're also encountering the same problem? Thanks","**Update for first token regression:** First token has a regression because `aten::_weight_int8pack_mm` is slower than the decomposed one, given certain input shapes.    First token For the first token, the input shapes are x: [128, 4096], w: [4096, 4096], scale: [4096]. **Profiling** With woq pattern: !image Without woq pattern: !image  Next token For the next token, the input shapes are x: [4, 4096], w: [4096, 4096], scale: [4096]. **Profiling** With woq pattern: !image Without woq pattern: !image  Reproduce You could reproduce the result by running the UT added in this PR."," Autotuning with WoQ GEMM would not run into this issue with the first token because for the first token, the WoQ AMX microkernel based implementation would be 1.5x faster than its ATen kernel counterpart (benchmarked with 32 physical cores of a Xeon 4th gen machine with Intel OpenMP & libtcmlloc preloaded). However, it'd be 4x slower for the second token (x: [4, 4096], w: [4096, 4096], scale: [4096]).  :( ","While autotuning would be 1.5x faster for the first token, it too would've resulted in a regression for the first token. `x: [128, 4096], w: [4096, 4096], scale: [4096]` seems to do better if weights are dequantized upfront. However, the overall performance order (higher is better) is: Autotuning int8 WoQ GEMM (another PR that needs this PR for LLaMA2) > ATen int8 WoQ GEMM kernel (this PR) > Dequantizing weights upfront","Potential fix  since Xeon CPUs are used on machines with a large amount of RAM, we can _optionally_ cache both quantized & dequantized weights for large values of `M` (based on some heuristic), and lower the corresponding FX pattern to a custom function (like the way `quantized_decomposed` custom functions have been defined) that accepts both quantized & dequantized weights. Then that custom function should be lowered to a templatebased autotuning implementation of WoQ GEMM. In such a case, the autotuning GEMM template should allow a fallback that could be used for large values of `M`, and use the cached dequantized weights if a large value of `M` would be encountered at runtime.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[torch.compile][HF] torch.compile issue tracker for torch.compile on forward method of Meta Llama model ," üêõ Describe the bug Script ~~~ from transformers import AutoTokenizer, AutoModelForCausalLM import torch import os os.environ[""TOKENIZERS_PARALLELISM""] = ""false""   To prevent long warnings :) torch._dynamo.config.inline_inbuilt_nn_modules = True tokenizer = AutoTokenizer.from_pretrained(""metallama/MetaLlama38B"") model = AutoModelForCausalLM.from_pretrained(""metallama/MetaLlama38B"", device_map=""auto"", torch_dtype=torch.float16) model.generation_config.cache_implementation = ""static"" model.forward = torch.compile(model.forward, mode=""reduceoverhead"")  backend=""eager"") input_text = ""The theory of special relativity states "" input_ids = tokenizer(input_text, return_tensors=""pt"").to(model.device) outputs = model.generate(**input_ids) print(tokenizer.batch_decode(outputs, skip_special_tokens=True)) ~~~ There are 2 issues I found * Recompilation because of stride change  We can probably build on top of https://github.com/pytorch/pytorch/pull/130232 and update `mark_dynamic` to work with strides as well. I think HF team is ok with the source code change to avoid this single recompilation as well. ~~~ DEBUG:torch._dynamo.guards.__recompiles:Recompiling function forward in /home/anijain/local/transformers/src/transformers/models/llama/modeling_llama.py:1021     triggered by the following guard failure(s):      0/0: tensor 'L['input_ids']' stride mismatch at index 0. expected 9, actual 1 ~~~ * Cudgraph skipping  ~~~ skipping cudagraphs due to mutated inputs (64 instances). Found from :    File ""/home/anijain/local/transformers/src/transformers/models/llama/modeling_llama.py"", line 1069, in forward     outputs = self.model(   File ""/home/anijain/loc",2024-07-19T23:28:50Z,triaged oncall: pt2 module: dynamo,open,0,3,https://github.com/pytorch/pytorch/issues/131265, (dynamic strides)   (cudagraph)  for static cache related issues,"Hmm, we handle two cases today  nn module buffers and parameters. These should be marked as static if https://github.com/pytorch/pytorch/pull/130391 is included in your runs (I assume it is because it relanded after a revert ~5 days ago) It's possible that these cache tensors are being handled in a nonstandard way that was papered over by the noninlining implementation. I can take a look Monday  I should be able to move the static marking from your PR into dynamo","Took a look, it appears that that the cache here is actually an input that is getting mutated: https://github.com/huggingface/transformers/blob/96a074fa7e2c04b904f72d9e827398d4c5f90f25/src/transformers/models/llama/modeling_llama.pyL582 We can mark it as a static input to resolve this, or no longer make it an input. , I think in the noninlining impl, this just wasn't an input (if it was instantiated as an attribute of an NN module higher in the hierarchy)"
transformer,Masked Attention has no effect in ``TransformerEncoderLayer``," üêõ Describe the bug  Problem description The forward method of `TransformerEncoderLayer` provides an argument to pass in a mask to zero specific attention weights. However, the latter has no effect. Here is a minimal script to reproduce. Notice that  the code doesn‚Äôt raise an error, though I set on purpose the mask to a wrong shape (it must be `(len_seq, len_seq)`)  changing the values of the mask (also its shape) has no effect on the output   Preliminary debugging The `TransformerEncoderLayer` uses `MultiheadAttention` and always calls its forward with ``need_weights=False`` https://github.com/pytorch/pytorch/blob/4aef5a1134b2735b43d41eaeb328103deb97e43c/torch/nn/modules/transformer.pyL918L926 Hence, because of the this `if` statement,  https://github.com/pytorch/pytorch/blob/4aef5a1134b2735b43d41eaeb328103deb97e43c/torch/nn/functional.pyL6021L6025 the attention mask is always set to `None`  Comments I would be happy to open a PR to fix the issue. Also pinging   Versions  ",2024-07-19T21:32:42Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/131254
rag,Clean up coverage_ignore_functions," üìö The doc issue As we were triaging for the docathon, we discovered that many of the functions listed in coverage_ignore_functions were already documented. In addition, we documented quite a few of them during the docathon.  Suggest a potential alternative/fix  We need to clean up this list and add the missing autofunction, autoclass directives as needed to the corresponding .rst files.  /pytorchdevinfra",2024-07-19T17:53:08Z,module: docs module: ci triaged,open,0,0,https://github.com/pytorch/pytorch/issues/131213
transformer,Missing float8 storage," üöÄ The feature, motivation and pitch I'm trying to use float8 to try mistral nemo which was trained in a quantization aware way, to do inference in FP8. Ref:  https://mistral.ai/news/mistralnemo/  https://huggingface.co/mistralai/MistralNemoBase2407transformers  Alternatives _No response_  Additional context Environment:  macOS 14.5  Python 3.12.4  pytorch 2.3.1  transformers at 43ffb785c0b2f24948d2011883d40dccb609d341 Repo:  Failure:  When I try with float8_e5m2, the exception is Float8_e5m2Storage ",2024-07-19T15:40:09Z,triaged module: float8,open,0,8,https://github.com/pytorch/pytorch/issues/131196,`torch.set_default_dtype` seems to be broken with Float8  I think the error is here  https://github.com/pytorch/pytorch/blob/c64ad2403c0954a0c4a36c720f0c87b3d284d0c1/torch/csrc/tensor/python_tensor.cppL217L236 where it should be using `UntypedStorage` perhaps?,"  I think issue is that its mentioned that environment is a macOS environment, but I think macOS systems dont support float8 natively, could this be the problem?",any update on this guys?,"I had the same problem on ubuntu 22, the graphics card is 2080ti","I believe only Nvidia H100 and the like support fp8 right now, correct?",same issue on torch 2.4 on 4090,"Float8 support in PyTorch so far has focused on target hardware which supports the float8 gemm, such as NVIDIA machines with CUDA capability 8.9 and above.  We would welcome community contributions to make float8 work as a storage type on Mac OS!","same issue here, Ubuntu 22, 4090 GPUs, latest verison of most libraries"
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_5_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_5_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T12:50:44Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131179
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_2_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_2_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T06:49:01Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131146
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 18 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T03:43:43Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131123
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T03:43:35Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131120
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T01:06:15Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131107
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 4 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T01:04:21Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131095
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_0_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_0_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T01:04:09Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131086
llava,xeon.run_cpu will not use numactl when whithin a docker container run with --cpuset-cpus non-0 numa node,"If we are using docker container which created by following command line, it will only use numa node 1.   In this case, numactl is available for `numactl C 40 m 1 hostname`. But `run_cpu.py` will use taskset to replace numactl. Therefore, performance might not be stable, taskset do not support memory binding. https://github.com/pytorch/pytorch/blob/433ef4e4443af4d1921ccd6cd49fb2253481b58c/torch/backends/xeon/run_cpu.pyL307L322  ",2024-07-18T03:17:54Z,module: performance module: cpu triaged,open,0,1,https://github.com/pytorch/pytorch/issues/131011,  
llama,[Distributed][PP export] update tracing to handle autocast inclusion,Fixes  CC([export] Failed to trace HF Llama2 model) This updates PP export tracing to use no_grad() context along with avoid predispatch.   This enables tracing for HF llama models that currently fail due to not handling the use of autocast in the Rope embeddings. ,2024-07-17T23:06:08Z,oncall: distributed Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/130998,How do we verify that this will fix the issue?,> How do we verify that this will fix the issue? I've already verified it fixes the original issue in terms of it now exports a graph with autocast vs failed to export previously.  I made this PR to get feedback on the fix and also to see how it fares on overall CI as a next step. , help, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Runtime error when running huggingface pretrained model with torch-xla," üêõ Describe the bug Encountered `RuntimeError: isDifferentiableType(variable.scalar_type())` during the forward pass of the training script of a hugging face wav2vec2 conformer model. This error happens with a CPU device.  Full trace of the error is    Reproduction example Please find the reproducing script in the following section. This script is modified from https://github.com/huggingface/transformers/blob/main/examples/pytorch/speechpretraining/run_wav2vec2_pretraining_no_trainer.py, to include only the forward part of the training loop.  To run the script, invoke with the following command in a venv satisfying the requirement:  Or similarly    Versions The result include:  Additional packages include:   ",2024-07-17T21:04:12Z,triaged module: xla,open,0,0,https://github.com/pytorch/pytorch/issues/130985
yi,Fix pyi annotation for ProcessGroupNCCL.Options,"  CC(Fix pyi annotation for ProcessGroupNCCL.Options) Probably all the other options need updating too, but this is the one I needed.  The accurate annotation was determined by reading torch/csrc/distributed/c10d/init.cpp Signedoffby: Edward Z. Yang ",2024-07-17T17:03:30Z,Merged ciflow/trunk release notes: distributed (c10d) topic: docs,closed,0,10,https://github.com/pytorch/pytorch/issues/130957, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Check mergeability of ghstack PR / ghstackmergeabilitycheck Details for Dev Infra team Raised by workflow job ","Yeah, We need to make this pyi more accurate and updatetodate..","When I rebased, it looked like someone  had modified these annotations as part of adding the split_group API. However, I double checked the annotations and I like mine better (as they are more accurate, and they don't use the  decorator unnecessarily).", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,[Reland][FSDP2] Allowed `List[nn.Module]` as arg,"  CC([Reland][FSDP2] Allowed `List[nn.Module]` as arg)  CC([Reland][PTD] Relaxed `contract` to allow `Sequence[nn.Module]` (127773)) This PR allows `fully_shard`'s first argument to be `List[nn.Module]` instead of strictly `nn.Module`. This allows more flexible grouping of modules/parameters for communication, which can lead to memory savings and/or more efficient communication. **Approach** At a high level, we can think of a model as a tree of modules. Previously, we could only select specific module nodes in this tree as representing one FSDP parameter group. With this PR, we can select a group of module nodes, effectively becoming a single super node. To implement the runtime schedule, we define new forward hooks that run based on the following semantics:  If a module is the first to run the prehook, actually run the given prehook. Otherwise, the prehook is noop.  If a module is the last to run the posthook, actually run the given posthook. Otherwise, the posthook is a noop.  First and last are determined by scoreboarding against a set of the modules.  This set must get cleared at the end of backward in the case that >=1 module in the list is never used, in which case we still want the forward hooks to run in the next forward after this backward. Beyond these new forward hooks, everything else is some simple generalization from `Module` to `List[Module]` or `Tuple[Module, ...]`. **Examples** This PR enables wrapping Llama models more efficiently by grouping the final norm and output linear together: https://github.com/pytorch/torchtitan/pull/382. If at least one of the modules in the list does not run forward before backward, then the",2024-07-17T15:51:07Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,2,https://github.com/pytorch/pytorch/issues/130949, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[Reland][PT-D] Relaxed `contract` to allow `Sequence[nn.Module]` (#127773),"  CC([Reland][FSDP2] Allowed `List[nn.Module]` as arg)  CC([Reland][PTD] Relaxed `contract` to allow `Sequence[nn.Module]` (127773)) This PR relaxes `` to allow the 1st argument to be `Sequence[nn.Module]` instead of strictly `nn.Module`. This is required for the next PR, which allows `fully_shard` to take in `List[nn.Module]`.  **Changes for reland:**  The previous PR assumed that any `func` decorated with `` would return the same input `module` as output (which is true for PTD composable APIs).  However, TorchRec `shard` returns a different module as output (though that module _does_ satisfy the `` FQN check).  This PR removes the assumption and instead only enforces the FQN check following the input module order. In other words, if calling `func([x1, ..., xN])` for `N` modules `x1, ..., xN` that returns `[y1, ..., yM]` for `M` modules, we require that `N = M` and that FQNs are preserved coordinatewise: `xi` and `yi` have same FQNs for all `i = 1, ..., N`. : D59863438",2024-07-17T15:35:30Z,oncall: distributed Merged ciflow/trunk release notes: distributed (composable),closed,0,4,https://github.com/pytorch/pytorch/issues/130947," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",I verified that both previously failing internal tests are now passing. Refer to the comments in D59863438. cc:  , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,BertForSequenceClassification.from_pretrained broken when using FSDP," üêõ Describe the bug The following code successfully loads the model checkpoint if ran using `python foo.py` or `accelerate launch foo.py` but not with FSDP enabled `accelerate launch use_fsdp foo.py`.  This seems like a bug where we say in `PreTrainedModel.from_pretrained` that `pretrained_model_name_or_path` can be None ""if you are both providing the configuration and state dictionary"", which I do here. But then if `is_fsdp_enabled()` is True, we set `low_cpu_mem_usage = True` and thus in turn `state_dict = None`, which causes the loading to fail.  Error message:   Versions 20240716 16:34:29  https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com):443... connected. HTTP request sent, awaiting response... 200 OK Length: 23357 (23K) [text/plain] Saving to: ‚Äòcollect_env.py‚Äô collect_env.py                                                                                    100%[===========================================================================================================================================================================================================================================================>]  22.81K  .KB/s    in 0.002s   20240716 16:34:29 (9.20 MB/s)  ‚Äòcollect_env.py‚Äô saved [23357/23357] Collecting environment information... PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4",2024-07-16T23:35:20Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/130875,"At the first glance, it does not look like a PyTorch issue, but rather a bug in `accelerate` framework(as backtrace suggestion) or in your code snippet. (perhaps you need to download checkpoint first and put it in a specific location?)  Have you tried asking this question on https://discuss.pytorch.org ?  Closing, but please do not hesitate to open a new issue if you can narrow it down to be a problem with PyTorch APIs",Opened an accelerate issue here
transformer,Training on M1 MBP: Placeholder storage has not been allocated on MPS device," üêõ Describe the bug Trying to do a simple training isn't working as expected with the following error:  I've been Googling around for a long time now and I I've done everything I can think of that might be my fault... I can't get the below to work on my MBP via MPS or CPU.... Reproduce Script:   Versions PyTorch version: 2.5.0.dev20240715 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.6.7 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: version 3.27.8 Libc version: N/A Python version: 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.6.7arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Max Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] onnx==1.16.1 [pip3] onnxconvertercommon==1.14.0 [pip3] onnxruntime==1.18.0 [pip3] skl2onnx==1.17.0 [pip3] torch==2.5.0.dev20240715 [pip3] torchaudio==2.4.0.dev20240715 [pip3] torchvision==0.20.0.dev20240715 [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.1.0.dev20230416          pypi_0    pypi [conda] torchaudio                2.1.0.dev20230416          pypi_0    pypi [conda] torchvision               0.16.0.dev20230416          pypi_0    pypi ",2024-07-16T00:27:58Z,needs reproduction triaged module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/130790,"Note this seems similar to CC(on MPS, torch.embedding, Linear and others raise: RuntimeError: Placeholder storage has not been allocated on MPS device!) which was closed, but it's uncertain why exactly it was closed to me?","Please note, that you somehow get both torch2.1.0 and torch2.5.0 installed in your environment, which can potentially cause conflicts","`Placeholder storage has not been allocated on MPS device` error means that one of the tensors passed to the operator has been allocated on CPU, while other is on MPS, which indicates problem in the training code rather than in PyTorch.  CC(on MPS, torch.embedding, Linear and others raise: RuntimeError: Placeholder storage has not been allocated on MPS device!) has been closed, because it was not a PyTorch error, though in 2.4 an improved message will be printed indicating which of the passed tensors has been on CPU instead of MPS device. Closing, as this does not look like a PyTorch issue, but rather a great topic for discussion on https://discuss.pytorch.org/"
yi,FileNotFoundError: Could not find module when trying to load C++ extension," üêõ Describe the bug I'm trying to JIT compile following blank C++ extension: **File test.py:**  **File csrc/cuda_impl.cpp:**  **File csrc/cuda_kernel.cu**  But I get following error:  C++ extension compilation succeeds, but Python is unable to load results my_cuda.pyd file. Previously I've used SRU custom torch layer ( https://github.com/asappresearch/sru ), and it loads fine, so the problem is somewhere on my machine, but I can't understand where. I know I have mismatched CUDA version and torchcuda version (because I had to install newer version of pytorch on a different enviroment), but it didn't cause any problem with SRU layer.  Can you help me?  Versions Collecting environment information... PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: –ú–∞–π–∫—Ä–æ—Å–æ—Ñ—Ç Windows 10 –î–æ–º–∞—à–Ω—è—è GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: True CUDA runtime version: 12.5.40 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2060 Nvidia driver version: 555.85 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2701 DeviceID=CPU0 Family=205 L2CacheSize=1024 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2701 Name=Intel(R) Core(TM) i56400 CPU @ 2.70GHz ProcessorType=3 Revision=24067 Versions of relevant libraries",2024-07-15T14:38:05Z,module: windows module: cpp-extensions,closed,0,4,https://github.com/pytorch/pytorch/issues/130737,"Can you please try a bit recent version of PyTorch? Also, not sure if `O3` is a valid option for VC++ compiler",I'd prefer to solve this problem on pytorch version I have currently.,"Alright, I've fixed the problem by adding ...\Python39\Lib\sitepackages\torch\lib to PATH. Worth noting, I've reinstalled CUDA (downgraded from 12.5 to 11.8) and Pytorch (2.0.1_cuda118) before that, but it didn't help.","It seems that actual problem lies in https://bugs.python.org/issue42114  ctypes.CDLL(path, winmode=0) loads pyd file, but without winmode it does not."
yi,Tighten torch.library.infer_schema input types,  CC(Tighten torch.library.infer_schema input types)  CC([FlopCounterMode] Fix register_flop_formula) Made the following changes:  mutates_args is now keywordonly and mandatory. This is to align with   torch.library.custom_op (which makes it mandatory because it's easy to   miss)  op_name is now keywordonly. This helps the readability of the API  updated all usages of infer_schema This change is not BCbreaking because we introduced torch.library.infer_schema a couple of days ago. Test Plan:  tests,2024-07-15T01:04:14Z,Merged Reverted ciflow/trunk release notes: composability keep-going ci-no-td,closed,0,10,https://github.com/pytorch/pytorch/issues/130705, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert c ghfirst m ""Failing internal CI""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to   Raised by https://github.com/pytorch/pytorch/actions/runs/10117435965," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[easy][inline-inbuilt-nn-module] Update test output,  CC([DONT MERGE YET][dynamo] Inline inbuilt nn modules)  CC([easy][inlineinbuiltnnmodule] Update test output)  CC([dynamo][unsoundness but very controlled] Skip guards on inbuilt nn module hooks)  CC([dynamo][cppguards] Use dict tags to skip guards on immutable dict getitems) ,2024-07-13T20:06:16Z,Merged ciflow/trunk topic: not user facing module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/130681, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,typing: storage,This isn't a full typing of the file  it just fixes some uses of unbound 'T' (if you use a TypeVar as an output it also needs to be an input).   CC(typing: convert_frame)  CC(typing: storage),2024-07-13T06:01:31Z,Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/130669
rag,Add wrappers for synchronous GPUDirect Storage APIs,Based in part on https://github.com/NVIDIA/apex/pull/1774   CC(Add wrappers for synchronous GPUDirect Storage APIs) Differential Revision: D60155434,2024-07-12T18:04:55Z,Merged Reverted ciflow/trunk topic: not user facing,closed,0,19,https://github.com/pytorch/pytorch/issues/130633, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""breaking a lot of jobs and build rules internally D60085885, possibly needs to update some bazel build?"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Build failures present on D60085885 do not exist on the imported D60155434 (also verified by running some of the builds locally on the diff and they succeeded), the service_lab signal that is failing previously succeeded so is flaky. Going to rebase and merge", merge, Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator! Please try reimporting/rexporting the PR! Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""still failing internally D60265673"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
rag,"torch.UntypedStorage.from_file(filename, shared=False, size=0) documentation error: 'nbytes' instead of 'size'"," üìö The doc issue I found a mistake in the documentation of `torch.UntypedStorage` in the static method `from_file(filename, shared=False, size=0) ‚Üí Storage`. Link If I call the function with the `size` keyword, I get: `TypeError: 'size' is an invalid keyword argument for this function` The correct keyword is `nbytes`.  Suggest a potential alternative/fix To improve the documentation, please either change the keyword to `size` in the function implementation or update the documentation to use `nbytes` instead of `size`. ",2024-07-12T16:55:14Z,module: docs triaged,open,0,0,https://github.com/pytorch/pytorch/issues/130629
transformer,PyTorch 2.4 windows performance regression compared with 0410 nightly," üêõ Describe the bug We are measuring performance on windows and observed that Windows whl performance regressed on rls/2.4 prereleased whl. In my local env, dev20240410 nightly whl was downloaded few month ago and we can see windows performance improved which might related to optimization PR by  . We are get our best to search which nightly whl caused this regression, but the oldest nightly whl is 0513 which is already regressed.  Hardware: 13th Gen Intel Core i713700H 2.4GHz OS: Windows 11 23H2 22631.3593  Versions How to reproduce: https://github.com/WeizhuoZhangintel/win_benchmarks/blob/main/torchvision_models.py     ",2024-07-12T14:01:15Z,high priority module: binaries module: windows triaged module: regression,closed,0,22,https://github.com/pytorch/pytorch/issues/130619,"Thanks for intel 's effort.  Here are two key PRs on the timeline:  1. https://github.com/pytorch/pytorch/pull/118980 merged on Mar 31. This PR enabled AVX2/AVX512 on pytorch Windows. So `Pytorch 0410` has perf improvememt to `Pytorch 0314`.  2. https://github.com/pytorch/builder/pull/1798 merged on April 26. This PR changes: a. switch mkl to static link. b. change mkl version. We can download the earliest nightly build was `0514` : https://download.pytorch.org/whl/nightly/torch/ The earlier nightly of `0514` was deleted. I plan to pull `release/2.4` code to debug https://github.com/pytorch/builder/pull/1798, Wish it has some progress.","Hi intel ,  give us the URL can download the nightly build before `0514`: https://pytorch.s3.amazonaws.com/whl/nightly/cpu/torch2.4.0.dev20240420%2Bcpucp311cp311win_amd64.whl Please do the binary search between `0410` to `0514`, and locate the issue date. Thanks.",Test with   on Pytorch 2.3.1:  on Pytorch 2.4.0:  Pytorch 2.1.0  Pytorch nightly from 0420  ,Tests:  Pytorch 2.4.0 (wheels)  Pytorch 2.4.0 (conda)  2.3.1  2.1.0  Pytorch 0420:  Pytorch 0424:  Pytorch 0428 ," could you please test the 0428, which after mkl changed?", We can fork from `release/2.4` and then: revert https://github.com/pytorch/builder/pull/1798 and https://github.com/pytorch/pytorch/pull/129493 Let's check if static link mkl cause the performance regression.,"If there are no regression between 2.3 and 2.4, we don't need to make any reverts",Rerun of 2.3.1 ,"> If there are no regression between 2.3 and 2.4, we don't need to make any reverts Hi  , This a regression since `0425` and it before the `release/2.4` branch cut. The data shows it seems not a regression to `2.3.1`. The reason is https://github.com/pytorch/pytorch/pull/118980 boost the performance and fill the `mklstatic` regression's performance gap. I think we need revert them for better performance.",I think the challenge here is that it's a bit late(and too risky) in the release cycle to do something like that.,"> I think the challenge here is that it's a bit late(and too risky) in the release cycle to do something like that. I known your concern, but the revert only impact on Windows. We can only rebuild the Windows binary.",hi  we would need to do complete rebuild of the rc. I agree with  we should target this change for 2.4.1 release,> hi  we would need to do complete rebuild of the rc. I agree with  we should target this change for 2.4.1 release release 2.4.1 is good for me.  we can revert them in the `main` branch. We need to fix and confirm them in nightly build. And then I will involve Intel MKL team look into the issue.,I suspect one can solve it by using wholelib flag when linking with the library,> I suspect one can solve it by using wholelib flag when linking with the library What flag? could you please show me the detailed information?,"Hi   Maybe we can try to PR fix build options for `main` branch, and fix `mklstatic`.  https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkllinklineadvisor.htmlgs.c3oeg2","MKL build options comparsion: Linux:  Windows:  It seems Windows MKL cmake file, miss some configuration. I will try to figure out it.","Hi  ,   I have submited a PR to fix Windows version `mklstatic` build options issue: https://github.com/pytorch/pytorch/pull/130697 . Please take a look. After this PR merged, intel we can test the new nightly build, and check if the perf regression issue would be fixed.",We verified the latest 0716 nightly whl of windows and this performance issue has been fixed.  ,"Hi  , We have verified PyTorch v2.4.1 RC build whl: https://download.pytorch.org/whl/test/cpu/torch2.4.1%2Bcpucp38cp38win_amd64.whl. And this performance issue was fixed.    ",Closing this one since fixed in Nightly and cherrypicked to RC,Confirmed final rc 2.4.1: 
llama,xpu: implement torch.xpu.mem_get_info() to support huggingface auto dispatch modes,See: * https://github.com/huggingface/transformers/issues/31922 * https://github.com/huggingface/accelerate/issues/2929 As of https://github.com/pytorch/pytorch/commit/3477ee38e4dd1429ecfd7e6f20a30cce0f4f78e7 XPU backend in pytorch is missing `torch.xpu.mem_get_info()`. This function is required to support auto dispatch modes to run large models such as LLAMA 3 on the systems with devices which don't have enough memory to fit in the model. See [1] and [2] for details. It's supported for CUDA: https://pytorch.org/docs/main/generated/torch.cuda.mem_get_info.htmltorch.cuda.mem_get_info. [1] https://huggingface.co/docs/accelerate/usage_guides/big_modeling [2] https://huggingface.co/blog/acceleratelargemodels CC:       matrix ,2024-07-12T01:26:06Z,triaged intel module: xpu,closed,0,2,https://github.com/pytorch/pytorch/issues/130599,"Any update on this? This is definitely needed for things like `accelerate` (or anything needs to calculate the VRAM usage) to work correctly (see https://github.com/intel/intelextensionforpytorch/issues/522).  I've made a PR adding the `torch.xpu.mem_get_info()` (https://github.com/intel/intelextensionforpytorch/pull/549) before, but it requires `root` user or `CAP_PERFMON` to work. I think this limitation should be relaxed first to make it happen, there seems to be no reason to protect memory info from normal users."," Thanks for your ideas. Currently, `torch.xpu.mem_get_info` depends on compiler 2025.0. We need the compiler to support this feature without being affected by sysman environment variable."
transformer,"RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'/Reshape_5' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:45 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, onnxruntime::TensorShapeVector&, bool) input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,32,512}, requested shape:{1,1,8,64}"," üêõ Describe the bug I am trying to convert a pytorch transformer model to onnx. My model architecture consists of multiple nn.modules, so I am converting each to onnx separately. I am having to use a combination of torch.onnx.export() and torch.onnx.dynamo_export(), because some module conversions do not support dynamo_export yet. I am able to convert all the modules to onnx. However, when I run an inference session through the decoder module, I get the mentioned error. For reference, here is my Decoder class   Here is my code for onnx conversion of the module   Here is my inference code in onnx   I have marked the line which throws the error above. Here is the full error   Any help in resolving this issue would be appreciated. Thanks.  Versions PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.9 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.85+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linux",2024-07-11T14:08:18Z,module: onnx triaged,open,0,1,https://github.com/pytorch/pytorch/issues/130539,"Could you try `torch.onnx.export(..., dynamo=True, report=True)` with nightly? And attach the error report please."
llm,Fail to offload FSDP model weights and optimizer states without using CPUOffload(offload_params=True)," üöÄ The feature, motivation and pitch Hi Pytorch maintainers, I am currently engaged in training multiple large language models (LLMs) sequentially on a single GPU machine, utilizing FullShardDataParallel (FSDP) for each model. A significant challenge we face is managing the storage demands for multiple LLMs, including their optimizer states, gradients, and activations. We notice that FSDP supports offloading the model parameters and optimizer states during training  `cpu_offload=CPUOffload(offload_params=True)`. However, this feature will offload the model parameters and optimizer states in CPU and perform optimizer step in CPU, which will affect the training throughput. In our scenario, the models are computed one by one so we manage to offload the other models when one LLM is performing computation on GPU. However, we fail to offload the _fsdp_wrapped_module into the CPU with the following code. (We offload the FSDP model by offloading the parameters in `named_paramteres()`). We found identical GPU memory usage before/after the offload operation, indicating no effective offloading. It appears that there might be persistent references to these parameters, causing p.data.to('cpu') to merely copy the data in CPU and preventing PyTorch's garbage collector from freeing the original GPU storage. Could you provide guidance on how to properly offload these FSDPwrapped parameters to the CPU to alleviate GPU memory constraints effectively? Any insights or updates to facilitate this process would greatly enhance our training capabilities and efficiency. Thank you for your attention to this feature/issue!  ",2024-07-11T10:34:50Z,needs reproduction oncall: distributed triaged,open,0,3,https://github.com/pytorch/pytorch/issues/130530,"Ugh, this is tricky. I think the problem might be because you are using `use_orig_params=True`. With `use_orig_params=True`, each parameter returned from `named_parameters()` is a view into the underlying `FlatParameter` (which would otherwise be returned from `named_parameters()`  for `use_orig_params=False`). This means that when you call `.to('cpu')` on each parameter, each parameter view into the `FlatParameter` is moved to CPU, but the underlying `FlatParameter` is on GPU. FSDP takes care to preserve the invariant that the parameters are always views into the `FlatParameter`, so if you break this invariant, it may be tricky. There should be some logic in the preforward to restore the invariant by copying parameters back into their `FlatParameter`. First, could you try to run with `use_orig_params=False`?","> Ugh, this is tricky. I think the problem might be because you are using `use_orig_params=True`. >  > With `use_orig_params=True`, each parameter returned from `named_parameters()` is a view into the underlying `FlatParameter` (which would otherwise be returned from `named_parameters()` for `use_orig_params=False`). This means that when you call `.to('cpu')` on each parameter, each parameter view into the `FlatParameter` is moved to CPU, but the underlying `FlatParameter` is on GPU. >  > FSDP takes care to preserve the invariant that the parameters are always views into the `FlatParameter`, so if you break this invariant, it may be tricky. There should be some logic in the preforward to restore the invariant by copying parameters back into their `FlatParameter`. >  > First, could you try to run with `use_orig_params=False`? I tested use_orig_params=False with the following setup:  Model: Llama3.18B  GPUs: 8 x A10080GB Results: Before model to CPU:  Memory allocated: 8.461610496GB  Memory reserved: 37.589352448GB After model to CPU:  Memory allocated: 8.066843136GB  Memory reserved: 13.501464576GB This method shows some effect, but PyTorch still retains some parameters. Is it possible to further hack the code to unload these remaining GPU parameters?","By the way, when you call `torch.cuda.empty_cache()`, if any underlying caching allocator segment still has some active allocation, then that entire segment cannot be freed. However, when you do more allocations later, you will still be able to use the unused parts of those segments, so it is not like the extra 5.5 GB you see as (reserved  allocated) is lost."
transformer,[dtensor][debug] adding new noise level which allows users to only print operations with dtensors,"  CC([dtensor][debug] adding new noise level which allows users to only print operations with dtensors)  CC([debug][dtensor] implemented activation checkpointing differentiation)  CC([dtensor][debug] changed which module tracker I inherited from to fix bug with activation checkpointing) **Summary** I have added a new noise level between the existing levels of 1 and 2, such that the noise level controls are now:           0. prints modulelevel collective counts           1. prints dTensor operations not included in trivial operations (new noise level)           2. prints operations not included in trivial operations           3. prints all operations This gives the user more flexibility in controlling what information they want to use. The noise levels are used both for creating the console/file log and the json dump. In the example file, I have changed the module_tracing examples to noise level 0 and have changed my transformer examples to show off the new noise level.  **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_json_dump 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing 3. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_module_tracing 4. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_module_tracing  ",2024-07-11T03:56:50Z,oncall: distributed ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/130518,PR does not adapt to inserted PRs
yi,Applying torch.cumsum does not recover the original vector after torch.diff," üêõ Describe the bug I noticed an issue while working with `torch.diff` and `torch.cumsum`. I would   After the final print statement, I get:  The difference between `tmp` and `cumsum_delta` is:  I would expect, however, to get the original `tmp` tensor.  Versions PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.14.0284.40.1.el9_2.x86_64x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             48 Online CPU(s) list:                047 Vendor ID:  ",2024-07-10T16:38:27Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/130447,Note that the differences are quite small. It is generally expected that operations do not execute with 100% precision (see docs on numerical accuracy here).  Adjusting the tolerances for `torch.allclose()`:  Closing as expected behavior.
transformer,[Feature Request] Lazy upcasting for mmap'd state dicts,"In transformers as a rule we load models always in as `float32` for stability, even if the weights are in `bfloat16`. As a result, loading `llama38B` can't be done lazily via mmap, since we have to upcast all the values in the `state_dict` immediately leading to some pretty slow timings. It'd be nice if there were an API exposed that could take a lazyloaded `state_dict` and hook into it to where once we request something from that state dict (before it's read from disk etc), we convert that parameter to `.to(float32)` automatically (or whatever precision you may want). transformers PR where this behavior would be *very* useful: https://github.com/huggingface/transformers/pull/31771discussion_r1672525371  ",2024-07-10T16:33:05Z,module: nn module: serialization triaged needs design,open,0,11,https://github.com/pytorch/pytorch/issues/130480,"Or, if this exists in PyTorch core I don't know, a hook when loading in the model weight via mmap to perform some operation on it? ",Actually thinking about this more but maybe we should discuss this on pytorch/pytorch since  and  can give a better answer than me,"This seems like a potential use case for `torch.Tensor.module_load`, which can hook into the `nn.Module.load_state_dict` call.  If the model definition contains a `__torch_function__` tensor subclass for all parameters, and the state_dict was loaded via mmap, `YourTensorSubclass.module_load` could be defined via `__torch_function__` to do the transformation from bfloat16 to float32 when each individual parameter is being loaded. Will look into this further!","That sounds perfect , looking forward to it!",My other fear with this is how easy it is to scale (e.g. can it be generic enough to apply via inheritance? and then all `transformer`'s models can make use of this functionality if configured). A toy example to see how `__torch_function__` would work in this case would be exceedingly helpful :) ,"Hey , does this snippet achieve what you are looking for? "," close but not quite! We still wind up loading the state dict in there, rather than delaying it as far as we can (aka an input goes through to the model). I'm not sure if what I want there actually *can* be possible.  As this is not really different from just going through and doing `.to(dest.dtype)` manually after loading in the state dict ourselves no? ","Oh, I see what you mean, hmmm I am slightly surprised that the decrease in first pass throughput is so small when lazy loading when the first input is passed to the model as compared to the model init time before :o Curious, for the throughput numbers for the first pass on https://github.com/huggingface/transformers/pull/31771issue2388519274 is the page cache cleared each time before mmaping the checkpoint?","*Probably* not, I'm not sure how to do that so some advice would be nice! :) However the hardware I'm working with (M.2 drives) can read up to 14.5GB/s, so it's not unreasonable to read llama8B's data so fast it seems almost instantaneous ","I'm not sure if there's a way to do it from python, but on linux I think it would be `sudo sysctl vm.drop_caches=1`"," destroyed the cache between runs.   New timings (llama38B in bfloat16 on CPU)  Model init: W/o lazy loading: 3.025 seconds W/ lazy loading: 0.319 seconds  First pass: W/o lazy loading: 2.353 tok/s (total time == 8.499s) W/ lazy loading: 2.020 tok/s (total time == 9.903s) At ~16gb for the weights, that's ~11.4GB/s (which makes sense for the speed of my m.2)  Second pass: W/o lazy loading: 2.444 tok/s (total time == 8.182s) W/ lazy loading: 2.434 tok/s (total time == 8.218s)"
yi,[Easy][Inductor] Add comment for .min_order and .max_order,  CC([Easy][Inductor] Add comment for .min_order and .max_order) ,2024-07-09T22:40:56Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/130390, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Not merging any PRs at the moment because there is a merge blocking https://github.com/pytorch/pytorch/labels/ci:%20sev issue open at:   CC(Too many GPU instances requested - any *.nvidia.gpu and ephemeral instances should experience significant queue times) Details for Dev Infra team Raised by workflow job ," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,Add support for conditional nodes in cuda graphs.,"This allows torch.cond and torch.while_loop to be captured in a single cuda graph. This is done by manually inserting conditional IF nodes and conditional WHILE nodes during stream capture. This approach is discussed here: https://developer.nvidia.com/blog/dynamiccontrolflowincudagraphswithconditionalnodes Previously, datadepenent control flow would force the usage of cuda graph trees, since datadependent control flow was done on the CPU. Now, datadependent control flow is done on the GPU. This work depends upon CUDA 12.4, since cuda graph conditional nodes were introduced in CUDA 12.4. This works only with torch.compile(..., backend=""eager"") and torch.compile(..., backend=""cudagraphs"") backends right now. Notably, there is no inductor support at this time! Conditional nodes for cuda graphs were first experimented with in https://arxiv.org/abs/2406.03791 . While this paper showed strong improvements in datadependent workloads that were very CPUoverhead bound, the next place to look for improvement is horizontal and vertical kernel fusion, which can eventually be enabled automatically once conditional nodes work with backends like inductor. This PR is the first step towards that. I also expect this work to benefit more sophisticated models like autoregressive decoding of LLMs, at least for users that are using static shape kvcaches. This is work done by  and me. We have a sophisticated example of RNNT greedy decoding (the algorithm discussing in the paper) working with this new feature here: https://github.com/tingyangk/NeMo/commit/975a80673edff84a78097bb370fe2d37316e5ad6diff2c2a72c9a5392d4a6ea5149fea3ce7900b9fd2c630e460bbab94547379553cea",2024-07-09T22:27:11Z,open source Stale ciflow/trunk ciflow/periodic module: dynamo ciflow/inductor,closed,4,10,https://github.com/pytorch/pytorch/issues/130386,The committers listed above are authorized under a signed CLA.:white_check_mark: login: galv / name: Daniel Galvez  (e0192caa903cac99fa1efd9d7559a55818f9e8b2),CC      , ,please rerequest review when ready  I know you were thinking about torch.cond," sure,  is looking into figuring out input mutation checking https://github.com/pytorch/pytorch/pull/130386discussion_r1684576985 and I am looking at adding a way to do warmup on all ""basic blocks"" of a graph with control flow, without side effects (probably via relaxed stream capture via a new TorchDispatchMode, as discussed). So don't expect a rereview until we iron those out.",", do we have any restrictions on mutation in control flow that would simplify this ? Also, , not sure what exactly target use cases are, but I will also mention that as part of inductor torch.cond we have existing facilities for warming up control flow and would not need to handle this. ","> Also, , not sure what exactly target use cases are, but I will also mention that as part of inductor torch.cond we have existing facilities for warming up control flow and would not need to handle this.  do you mind pointing me to a file I can read to learn more?","> , do we have any restrictions on mutation in control flow that would simplify this ? Also, , not sure what exactly target use cases are, but I will also mention that as part of inductor torch.cond we have existing facilities for warming up control flow and would not need to handle this. When we get to Inductor, the cond operator should have no mutations in either subgraph. Does that help? (I haven't read the rest of the PR, am catching up)","  we have code in aot_inductor to warmup triton kernels, but I realize it doesn't necessarily warm up kernels like cudnn which is needed here.  It might be simplest and easiest to iterate over both branches of the cond, instantiate tensors for each node, then run the target. ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Add Exponential Moving Average module wrapper in PyTorch," üöÄ The feature, motivation and pitch Current state of the art research frequently uses exponential moving average and approaches like it to smooth model weight updates during treaining. In pytorch, this functionality is not yet implemented as a separate module.   Alternatives Currently, there is an EMA Handler in Ignite but it doesn't fit in well with traditional training loops.  There is also an existing pip library that essentially implements most of the functionality (emapytorch). The suggestion here would be to collaborate with the author to implement that feature directly into the main Pytorch library.   Additional context _No response_ ",2024-07-09T19:27:47Z,module: nn triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/130361,Some related discussion:    CC(EMA optimizer: class-form and function-form (using new foreach_lerp) - can be used for explicit robust updates of BatchNorm stats),Might this be what you are looking for https://pytorch.org/docs/stable/optim.htmlweightaveragingswaandema
rag,add MTIA as valid device type for prof averages,Summary: Add MTIA as valid device option for getting profile averages Test Plan: Tested with autotrace on MTIA Differential Revision: D59486392,2024-07-09T16:44:28Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/130340,This pull request was **exported** from Phabricator. Differential Revision: D59486392,This pull request was **exported** from Phabricator. Differential Revision: D59486392,This pull request was **exported** from Phabricator. Differential Revision: D59486392,This pull request was **exported** from Phabricator. Differential Revision: D59486392,This pull request was **exported** from Phabricator. Differential Revision: D59486392, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[BE][Easy] update type hints for `torch/nn/parameter.pyi`,  CC(Resolve circular dependence between `torch.autograd` and `torch.nn.parameter`)  CC(Move class `torch.nn.Parameter` out of module `torch.nn`)  CC([BE][Easy] update type hints for `torch/nn/parameter.pyi`)  CC([Easy] reorder functions in `torch.nn.functional`) ,2024-07-09T12:26:50Z,oncall: distributed open source Stale release notes: distributed (fsdp) ciflow/inductor,open,0,1,https://github.com/pytorch/pytorch/issues/130326,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[SDPA] Clean up `print` in `test/test_transformers.py` ,"Left this in CC([cuDNN][SDPA] Remove `TORCH_CUDNN_SDPA_ENABLED=1`, enable cuDNN SDPA by default on H100 and 2nd on other archs >= sm80), oops...",2024-07-09T00:54:05Z,open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/130302, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Test copying Torchtitan test to Pytorch,"CLI commend: PYTHONPATH=/data/users/yifanmao/pytorch/torchtitan:$PYTHONPATH torchrun nproc_per_node=4 rdzv_backend c10d rdzv_endpoint=localhost:0 localranksfilter 0,1,2,3 role rank tee 3 test/distributed/_composable/test_composability/test_torchtitan.py job.config_file test/distributed/_composable/test_composability/train_configs/debug_model.toml job.dump_folder outputs/1d_compile model.flavor debugmodel training.compile model.norm_type=rmsnorm ",2024-07-08T23:37:00Z,oncall: distributed topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/130293
grok,[issue scrubbing] Fix imports in test_memory_planning.py to work with pytest,"  CC([issue scrubbing] Fix imports in test_memory_planning.py to work with pytest) Summary: I actually don't grok why this pattern works; I guess pytest expects a different import syntax for these relative imports?? But this pattern is used in many other tests here (notably `test_aot_inductor.py`), so it must be right ;) Test Plan: Ran both ways: * `python test/inductor/test_memory_planning.py` * `pytest test/inductor/test_memory_planning.py` ",2024-07-08T21:26:58Z,Merged ciflow/trunk topic: not user facing module: inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/130275,To fix  CC(pytest test/inductor fails),> Summary: I actually don't grok why this pattern works; I guess pytest expects a different import syntax for these relative imports?? I have had the same exact question before and I'm not sure why, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Failure to export Mixtral-8x7B-Instruct-v0.1 as onnx," üêõ Describe the bug trying to export loaded torch model as onnx: Simple python script here: `from transformers import AutoModelForCausalLM, AutoTokenizer import torch mistral_models_path = ""/home/ceti/models/textgen/Mixtral8x7BInstructv0.1"" tokenizer = AutoTokenizer.from_pretrained(mistral_models_path) model = AutoModelForCausalLM.from_pretrained(mistral_models_path, device_map=""auto"") onnx_program = torch.onnx.dynamo_export(model)` UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.   warnings.warn( Traceback (most recent call last):   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/exporter.py"", line 1428, in dynamo_export     ).export()   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/exporter.py"", line 1171, in export     graph_module = self.options.fx_tracer.generate_fx(   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/fx/dynamo_graph_extractor.py"", line 213, in generate_fx     graph_module, graph_guard = torch._dynamo.export(   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1311, in inner     result_traced = opt_f(*args, **kwargs)   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 451, in _fn     return fn(*args, **kwargs)   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/fx/dynamo_gr",2024-07-08T21:16:48Z,module: onnx triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/130274,Can you make sure `torch.export.export()` succeeds?,"I've experienced a similar issue with the `Mixtral8x7B` model. A more reproducible script is:  `torch.export.export` raises the error `torch._dynamo.exc.Unsupported: hasattr ConstDictVariable to`. I've seen a similar issue here)), is this something that we'd need to add a custom handler for?","Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated markdown report if there is an error. Thanks!","> Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated markdown report if there is an error. Thanks! Cheers!  My specific goal is to use `torch.export`, since I'm trying to use the StableHLO export system. However, with the latest torchnightly (`2.6.0.dev20241030+cpu`), I get the following: Markdown error report   PyTorch ONNX Conversion Error Report  Error message:    ",  you may want to open a new issue and tag the torch export team
transformer,[codemod] Fix deprecated dynamic exception in pytorch/FasterTransformer/FasterTransformer/tests/unittests/unittest_utils.h +5 (#129471),Summary: LLVM has detected a violation of `Wdeprecateddynamicexceptionspec`. Dynamic exceptions were removed in C++17. This diff fixes the deprecated instance(s). See Dynamic exception specification and noexcept specifier. Test Plan: Sandcastle Reviewed By: dmmfb Differential Revision: D58953076,2024-07-08T18:37:20Z,fb-exported ciflow/trunk topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/130263,This pull request was **exported** from Phabricator. Differential Revision: D58953076
rag,Fix warning when pickle.load torch.Storage,"Fixes  CC(`pickle.loads` throws warning with `nn.Module`) Since `torch.save` does not use pickle for storages, the `torch.load` in `_load_from_bytes` should not ever be called when `torch.load`ing a checkpoint. Setting weights_only=False explicitly in `_load_from_bytes` to avoid the weights_only warning when using the pickle module   CC(Fix warning when pickle.load torch.Storage)",2024-07-08T14:51:23Z,Merged ciflow/trunk release notes: python_frontend topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/130246, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"From the discussion re FutureWarnings on  CC(Internal uses of `torch.load` are missing `weights_only` and raise FutureWarning) in triage review yesterday, it was discussed that resolving those should be milestoned for 2.4.1 This PR fixed another related weights_only warning when `pickle.dump` `torch.Storage` mentioned here https://github.com/pytorch/pytorch/pull/130663issuecomment2227144736, so I am also adding this one to track that this PR should be in 2.4.1  , let me know if this makes sense", cherrypick onto release/2.4 c critical fixes  CC(`pickle.loads` throws warning with `nn.Module`), Cherry picking CC(Fix warning when pickle.load torch.Storage) The cherry pick PR is at https://github.com/pytorch/pytorch/pull/133597 and it is linked with issue  CC(`pickle.loads` throws warning with `nn.Module`). The following tracker issues are updated: *  CC([v2.4.1] Release Tracker)issuecomment2291864486 Details for Dev Infra team Raised by workflow job 
yi,Autoheuristic: add config options for specifying optimizations to collect data for and use heuristics,"Previously, it was only possible to collect data or use a heuristic regardless of where autoheuristic is used. This PR makes it possible to collect data for some optimizations while using a learned heuristic for other optimizations. ",2024-07-08T14:44:55Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/130245," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Meta InternalOnly Changes Check Details for Dev Infra team Raised by workflow job ", merge i, Merge started Your change will be merged while ignoring the following 0 checks:  Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,"RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, int, NoneType, Device, bool,"," üêõ Describe the bug I have been trying to JIT trace Whisper, using this code:  However it fails with:  I did see  CC(RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, NoneType, NoneType, Device, bool, ), but while the error message looks similar, I was not doing an ONNX export, and also provided a reproduction. I did see lots of warnings like the following, but I don't believe they are relevant to this issue.   Versions  ``` ",2024-07-08T04:32:36Z,oncall: jit,open,2,9,https://github.com/pytorch/pytorch/issues/130229,", I only noticed today that this issue had been assigned to me :(",  any chance you have looked into this?  I am kind of blocked on my project until this is fixed..  Thanks in advance.,"I faced the same issue. Tried different versions of Pytorch: 2.4.0, 2.3.1, 2.3.0, 2.2.2","I found the solution. Changing the source code helped me. For example, the original code:  I changed to: ","sitsky, sorry, I had missed the notification! I haven't had a chance to work on it due to the approaching PyTorch v2.5 code freeze (Sep 6). I'll work on it after Sep 6. Just curious, BTW  why are you not using `torch.compile` to leverage Inductor? It's supposed to support a lot more models without requiring modifications in their sourcecode. TorchScript has been deprecated in the sense that its support is still present in PyTorch, but no new features would be added (except bug fixes, as in this case).","  thanks for your reply.  The purpose of this script is to generate a torchscript model for use with DJL, as described here: https://github.com/deepjavalibrary/djl/blob/master/examples/docs/whisper_speech_text.mdtracethemodel.",So is the solution is to change rewrite `torch.full` operation ?," This error seems to be happening because https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/ir/ir.cppL1035 returned false for various `aten::full` schemas for argument 1, which is a bool. Snippet of code to reproduce:  , please advise as to whether `bool` should be considered a subtype of `Scalar` for `torch::jit`. Thanks!","> So is the solution is to change rewrite `torch.full` operation ? In general, TorchScript supports fewer OOB models than torch.compile, and may require modifications in sourcecode to be supported. If modifying the model is feasible at your end, please go ahead with a workaround."
yi,[BE][Easy] fix ruff rule unsupported-method-call-on-all (PYI056),Some static type linters do not support `list.append` and `list.extend`. This PR changes all `__all__.append()` and `__all__.extend()` to `__all__ += [...]` if the extra exports are string literals.   CC([BE][Easy] fix ruff rule unsupportedmethodcallonall (PYI056)),2024-07-06T21:00:30Z,open source better-engineering Stale ciflow/trunk release notes: fx topic: not user facing ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/130205, merge," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,[BE][Easy] fix ruff rule bad-exit-annotation (PYI036),Normalize signature of `__exit__` to:  or    CC([BE][Easy] fix ruff rule badexitannotation (PYI036)) ,2024-07-06T21:00:15Z,oncall: distributed module: cpu open source better-engineering module: amp (automated mixed precision) Stale release notes: distributed (c10d) topic: not user facing ciflow/mps module: inductor module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/130204,This signature breaks torch.jit / torch.compile,> This signature breaks torch.jit / torch.compile Reverted the signature change. Only normalize the argument names (they are positionalonly) with the Python documentation.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llm,"make torch.compile work with vLLM (facebook/opt-125m , meta-llama/Llama-2-7b-hf, meta-llama/Llama-3-8b-hf) models"," üöÄ The feature, motivation and pitch vLLM is a highthroughput and memoryefficient inference and serving engine for LLMs. We would like to use `torch.compile` to accelerate LLM inference. This issue is to track the progress of the integration. The first step is to make `torch.compile` work for a minimal example of vLLM:  To use `torch.compile`, we need to add `self.model = torch.compile(self.model)` in this line: https://github.com/vllmproject/vllm/blob/main/vllm/worker/model_runner.pyL253 . Currently, when I run it on H100 (with vllm 0.5.1), I get the following error:  Thanks to  for bringing it up.  Alternatives _No response_  Additional context _No response_ ",2024-07-05T21:01:40Z,triaged tracker oncall: pt2 vllm-compile,closed,4,22,https://github.com/pytorch/pytorch/issues/130174,"Removing high priority. This is a result of a meeting with vLLM developers.  Just an update   is working on a developerfriendly environment so that we can iterate faster. Currently, vLLM depends on many packages, which need to move in lockstep. Therefore it is imperative that we can first setup a developer env, otherwise testing fixes will be very painful. Assigning to Laith for now.",note that I am using flash_attn master,"vllm uses a vllmflashattention, a custom build in https://github.com/vllmproject/flashattention . .",I was able to repro this with main! ,most probably      key_cache = kv_cache[0]     value_cache = kv_cache[1]   Are introducing additional allocations of tensors ,why would this introduce allocations of tensors?,seems related to   CC(Inductor generates unnecessary allocation + copy operations for custom ops with mutable inputs),seems like someone else  is also  working on compiling paged attention  CC(torch.compile slows down paged flash attention),the issue narrowed down to   CC(custom ops don't reinplace when mutated arg is a view of a graph input) I was able to by pass that issue by rewriting VLLM code in the flash attention as the following:     ```         key_cache = None         value_cache = None          if kv_cache is not None:             key_cache = kv_cache[0]             value_cache = kv_cache[1]         torch._dynamo.graph_break()         ....        rest of the function   wrapping forward at https://github.com/vllmproject/vllm/blob/a5314e8698b7c0f20cf3facf921c54917c89a9ba/vllm/attention/backends/flash_attn.pyL431  does actually runs the inference fine. (it used to OOMS before)  I am getting another issue when wrapping the whole model that I am looking at now. ,"Ok the model runs properly now with torch.compile! Summary : 1. We have graph breaks on vllm_flash_attn_2_cuda.PyCapsule.fwd_kvcache and  vllm_flash_attn_2_cuda.PyCapsule.varlen_fwd. 2. I had to refactor forward in flast_attn and add a graph break after creating tensor views  to avoid  CC(custom ops don't reinplace when mutated arg is a view of a graph input). 3. applying torch.compile to self.model, can cause issues where some functions are called on self.model.X  such as (sample and compute_logits) so we need to still have access to those functions, I did simple some code refactor for that.  I will try to fork LLVM and publish changes for visibility.","Model  metallama/Llama27bhf also work with the changes above with the exact same two graph breaks  I just changed model name, so does metallama/Llama38bhf.   is there other ways to call into inference other than generate that can potentially trigger more unvisited paths ","good to know. you can try to enable lora then, see https://docs.vllm.ai/en/latest/models/lora.html for example.",performance wise i have bad news! I ran  python benchmarks/benchmark_latency.py model facebook/opt125m  tp=1 batchsize=1   with and with out torch compile and where is what i get  with out   ` with  30X slower.   . INFO 0723 22:54:53 model_runner.py:1212] Graph capturing finished in 10 secs. INFO 0723 22:56:52 model_runner.py:1212] Graph capturing finished in 54 secs.,not surprising to me. we definitely need lots of efforts here to make it work finally :),30X seems too high. Can you check if there are recompilations? We can also run profiler and compare.,"> 30X seems too high. Can you check if there are recompilations? We can also run profiler and compare. I did run it with recompile we had a lot recompilations, running it with nn modules inlining enabled reduces the recompilations and reduces the reported Graph capturing runtime  from 60s to 30s . However, there is no compilations happening during the actual profiling phase, I am collecting profiles right now to check thats the issue. ",I collected profiler  with out torch.compile  meta only link: https://fburl.com/scuba/pyperf_experimental/on_demand/lnnekk8b with torch.compile  meta only link: https://fburl.com/scuba/pyperf_experimental/on_demand/hxm6r9sm The function _rand_sample is taking 95% of the time with torch.compile. So trying to see whats going on  https://github.com/vllmproject/vllm/blob/6a1e25b1514a25d3da96d0d78c4568f6e581e242/vllm/model_executor/layers/sampler.pyL350 namely in this line  note that this is not part of the compiled code.  confirmed with cProfile ,"Has anyone met this issue:  CC(Avoid recompilation for inputs integer number), recompilation happens in each next token generation. I need to make some workaround in vLLM to avoid these recompilation: wrap `max_decode_seq_len` in `attn_metadata` from `integer` to `torch.tensor`. After that,  running with batch size of 1, I can get 2 complete graphs now: 1 for first token and 1 for next token. But the performance is much worse than eager, probably due to the same issue mentioned above as in:  CC(custom ops don't reinplace when mutated arg is a view of a graph input)","> Has anyone met this issue: CC(Avoid recompilation for inputs integer number), recompilation happens in each next token generation. I need to make some workaround in vLLM to avoid these recompilation: wrap `max_decode_seq_len` in `attn_metadata` from `integer` to `torch.tensor`. After that, running with batch size of 1, I can get 2 complete graphs now: 1 for first token and 1 for next token. >  > But the performance is much worse than eager, probably due to the same issue mentioned above as in: CC(custom ops don't reinplace when mutated arg is a view of a graph input) I have  a darft fix for  CC(custom ops don't reinplace when mutated arg is a view of a graph input) https://github.com/pytorch/pytorch/pull/133045","functionalization v2 is landed and support for inference mode is up https://github.com/pytorch/pytorch/pull/134409 https://github.com/pytorch/pytorch/pull/135141 running python benchmarks/benchmark_latency.py model facebook/opt125m tp=1 batchsize=1 There is no graphs breaks, no regresison and no OOM. ! results: No compile  opt125m  with inductor.   For Model:  metallama/MetaLlama38B eager:  inductor   ","Note if we run with enforceeager (no cuda graph model capture) then the compile stack is actually 3X faster TORCH_LOGS=""recompiles"" python benchmarks/benchmark_latency.py model facebook/opt125m tp=1 batchsize=1 enforceeager eager:  inductor:  ",Done 
yi,HF YituTechConvBert accuracy test is flaky for training under max-autotune mode, üêõ Describe the bug The pass status for YituTechConvBert is flaky for training in maxautotune mode.  I've seen this again in a recent run with maxautotune link Tried command  but can not repro locally.  Error logs _No response_  Minified repro _No response_  Versions . ,2024-07-05T20:33:09Z,triaged oncall: pt2 module: pt2 accuracy,open,0,0,https://github.com/pytorch/pytorch/issues/130172
yi,Make sympify'ing SymInt/etc produce their sympy expression,"  CC(Make sympify'ing SymInt/etc produce their sympy expression) There is one huge problem this fixes: today, sympify(symint) produces a float(!!) because Sympy attempts to see if you can coerce the symint to float in sympify and of course this works on SymInt. However, this also has another nontrivial effect: anywhere in Inductor where sympy expressions are passed around, it is also valid to pass around a SymInt now.  I'm ambivalent about this: it's currently a mistake to be passing around a SymInt when a sympy expression is expected.  But maybe this is fine? Signedoffby: Edward Z. Yang  ",2024-07-05T18:57:23Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/130166, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.10clang15asan / test (default, 3, 6, linux.4xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge i," Merge started Your change will be merged while ignoring the following 4 checks: pull / linuxfocalpy3.12clang10 / test (dynamo, 1, 3, linux.2xlarge), pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 3, linux.2xlarge), pull / linuxfocalpy3.11clang10 / test (dynamo, 1, 3, linux.2xlarge), pull / linuxfocalpy3.12clang10experimentalsplitbuild / test (dynamo, 1, 3, linux.2xlarge, unstable) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Memory leak when exporting hf model to onnx ," üêõ Describe the bug I want to export models from HuggingFace to ONNX format, but I encountered a memory leak issue with torch.onnx.export after performing multiple rounds of exports. Here is a quick code snippet to reproduce the problem  1. **When I commented out 'torch2onnx'**, I found that the peak memory usage for each iteration was around 30GB and was very stable. 2.  **When I added torch2onnx**, I discovered that the peak memory usage increased with each iteration, from 36GB to 48GB to 64GB.  Versions ",2024-07-04T12:09:02Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/130115
yi,[TEST ONLY][Yifu's PR #129980] Fix and improve raise_comms and sink_waits,  CC([Traceable FSDP2][Inductor] Create grouped nodes for FSDP2 allgather code block and reducescatter code block)  CC(fix reorder pass algo after rebase)  CC([TEST ONLY][Yifu's PR 129980] Fix and improve raise_comms and sink_waits)  CC([Traceable FSDP2][Inductor] Add GroupedSchedulerNode to contain nodes that must be scheduled together) ,2024-07-04T04:13:16Z,oncall: distributed module: inductor ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/130094
yi,[Intel GPU] Add XPU into device list of copy_impl,  CC([Intel GPU] Add XPU into device list of copy_impl)  CC([Intel GPU] xpuops codegen via backend whitelist)  CC([Intel GPU] Dispatch Stub support),2024-07-04T02:18:01Z,open source,closed,0,0,https://github.com/pytorch/pytorch/issues/130083
transformer,[TS2EP] Failing on longformer," üêõ Describe the bug I am testing the logic on huggingface longformer. This exact repro is a little complex because I have changed some source code in transformers, but the error stack may give you some ideas?   Versions main  ",2024-07-03T01:42:12Z,module: dynamic shapes oncall: export,open,0,3,https://github.com/pytorch/pytorch/issues/130008,torch._check_is_size(pad_idx) or whatever u0 is  another reason to implement your stack walker lol,Was this change needed in `torch/_refs` or in user code? Sorry I am not yet familiar with `_check_is_size`,User code. I guess it's also possible that maybe some framework code could use a check_is_size but if pad_idx is only passed into the indexing operation we can't do that because negative indices are supported there.
llm,[experiment] Use setup-python for llm retrieval,Fixes ISSUE_NUMBER,2024-07-02T23:56:39Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/129999
transformer,torch.distributed in different groups at the same time leads to `socketStartConnect: Connect to xxx.xxx.xxx.xxx<xxxx> failed : Software caused connection abort`," üêõ Describe the bug ```python import ray import torch import torch.distributed as dist from ray.air.util.torch_dist import (     TorchDistributedWorker,     init_torch_dist_process_group,     shutdown_torch_dist_process_group, ) from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy .remote(num_gpus=1) class TestWorker(TorchDistributedWorker):     def __init__(self):         super().__init__()     def run(self):         rank = torch.distributed.get_rank()         dev = f""cuda:{ray.get_gpu_ids()[0]}""         tensor = torch.tensor([rank]).to(dev)          case 1: whole group, part workers => blocking         if rank > 1:             group = dist.new_group([0, 1, 2, 3])             dist.broadcast(tensor, 2, group)          case 2: part group, all workers => success         group = dist.new_group([2, 3])         dist.broadcast(tensor, 2, group)          case 3: part group, part workers => success         if rank > 1:             group = dist.new_group([2, 3])             dist.broadcast(tensor, 2, group)          case 4: different groups, all workers => error         if rank  20240702 10:33:31,362 INFO worker.py:1749  Started a local Ray instance. > (TestWorker pid=2123894) [W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt) > (TestWorker pid=2123892) [W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt) > (TestWorker pid=2123891) [W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use",2024-07-02T02:41:34Z,oncall: distributed,closed,0,1,https://github.com/pytorch/pytorch/issues/129926,I fixed this problem by replacing `new_group` with `new_subgroups_by_enumeration`. This issue can be closed.
yi,[CI] Run `lintrunner` on generated `.pyi` stub files in CI,"See https://github.com/pytorch/pytorch/pull/129420issue2371257204, this PR is aimed at preventing potential typing errors in the future. I have fixed all lint errors on the current generated `.pyi` stub files.   CC([CI] Run `lintrunner` on generated `.pyi` stub files in CI)  CC(Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`)  CC([BE][Easy] Fix `PYI034`: nonselfreturntype in tensor method hints)  CC(Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC([torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path`) /pytorchdevinfra",2024-07-01T18:08:44Z,module: typing module: ci module: lint open source release notes: releng topic: not user facing,open,0,2,https://github.com/pytorch/pytorch/issues/129887,Ref https://github.com/pytorch/pytorch/pull/129875issuecomment2200537167 > This I am unsure about. The problem is that it is often nonobvious how to actually fix a given lint error.  How about raising warnings rather than errors for these files? See my comment at https://github.com/pytorch/pytorch/pull/129420issue2371257204. ,"Warnings is pretty pointless if you're not going to fix them, IMO"
yi,[BE][Easy] Fix `PYI034`: non-self-return-type in tensor method hints,"In `gen_pyi.py`, we add generated method hints for dunder magic methods for `class TensorBase` at: https://github.com/pytorch/pytorch/blob/8c2c3a03fb87c3568a22362d83b00d82b9fb3db2/torch/_C/__init__.pyi.inL1734 such as:  This PR changes the return type of inplace binary operations (e.g. `__iadd__`, `__imul__`, ...) from `Tensor` to `Self` to fix nonselfreturntype (PYI034).    CC([CI] Run `lintrunner` on generated `.pyi` stub files in CI)  CC(Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`)  CC([BE][Easy] Fix `PYI034`: nonselfreturntype in tensor method hints)  CC(Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC([torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path`) ",2024-07-01T18:08:35Z,module: typing open source topic: not user facing,open,0,1,https://github.com/pytorch/pytorch/issues/129886,"> To be frank, I'm quite at loss if how PR title connected to any of the changes you are proposing here Sorry, I have updated the PR comment for what this PR is doing."
yi,[BE][Easy] Fix `PYI001`: unprefixed-type-param in `torch/utils/data/datapipes`,  CC([CI] Run `lintrunner` on generated `.pyi` stub files in CI)  CC([Easy] Add whitespace after comma when rerendering tuple default value in schema)  CC(Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`)  CC([BE][Easy] Fix `PYI034`: nonselfreturntype in tensor method hints)  CC(Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC([CI][lintrunner] bump `black` version to 24.4.2)  CC([BE][Easy] Fix `PYI001`: unprefixedtypeparam in `torch/utils/data/datapipes`) ,2024-07-01T18:08:27Z,module: bc-breaking module: typing open source Merged ciflow/trunk release notes: dataloader topic: bc breaking topic: not user facing suppress-bc-linter,closed,0,7,https://github.com/pytorch/pytorch/issues/129885, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  BC Lint / bc_linter Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"This was reverted internally for breaking stateful dataloader, but that was already fixed in https://github.com/pytorch/data/pull/1282 I will look on Friday how to proceed best, I'd prefer not reverting this PR and reland it internally.  ","Internal breakages are being fixed internally. But strictly speaking removal of `T_co` is backward compatibility breaking, should be marked as such, and we need to be careful with such changes."
yi,Run `lintrunner` on generated `.pyi` stub files,  CC(Run `lintrunner` on generated `.pyi` stub files)  CC(Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`)  CC(Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC([torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path`) /pytorchdevinfra,2024-07-01T13:43:29Z,module: typing module: ci module: lint open source release notes: releng topic: not user facing suppress-bc-linter,closed,0,2,https://github.com/pytorch/pytorch/issues/129875,This I am unsure about. The problem is that it is often nonobvious how to actually fix a given lint error.,Split into 4 PR after a rebase:  CC([Easy] Add whitespace after comma when rerendering tuple default value in schema)  CC([BE][Easy] Fix `PYI001`: unprefixedtypeparam in `torch/utils/data/datapipes`)  CC([BE][Easy] Fix `PYI034`: nonselfreturntype in tensor method hints)  CC([CI] Run `lintrunner` on generated `.pyi` stub files in CI)
yi,Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`,  CC([CI] Run `lintrunner` on generated `.pyi` stub files in CI)  CC(Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`)  CC([BE][Easy] Fix `PYI034`: nonselfreturntype in tensor method hints)  CC(Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC([torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path`) ,2024-07-01T13:43:10Z,open source module: codegen Stale release notes: dataloader topic: not user facing,open,0,3,https://github.com/pytorch/pytorch/issues/129873,"Sorry, what exactly does this do?","To let you know how I plan to do reviews on these, you need to get reviews on the diffs below the stack before I will look at this","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`,  CC([CI] Run `lintrunner` on generated `.pyi` stub files in CI)  CC(Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`)  CC([BE][Easy] Fix `PYI034`: nonselfreturntype in tensor method hints)  CC(Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC([torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path`) ,2024-07-01T13:43:00Z,open source module: codegen topic: not user facing,open,0,1,https://github.com/pytorch/pytorch/issues/129872,How can we verify that the list is correct?
rag,[Traceable FSDP2] Mark inductor.resize_storage_bytes_ as mutation op,"Since `inductor.resize_storage_bytes_` is indeed a mutation op, it could be a good idea to mark it as so.   CC([Traceable FSDP2][AOTAutograd] Move .set_ and .resize_ to end of joint graph)  CC([Traceable FSDP2] Mark inductor.resize_storage_bytes_ as mutation op)",2024-07-01T04:18:00Z,ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/129855
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_0_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_0_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-01T03:41:42Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/129853
chat,[BE] Print all pip packages installed on the system after TorchChat,"To make debugging regressions like ones happened last Wed when new version of torchao was released, that resulted in TorchBench downgrading pytorch version to 2.3.1 Test plan: Look at the log output for example https://github.com/pytorch/pytorch/actions/runs/9720408234/job/26832794157?pr=129809step:20:1158 contains ",2024-06-29T01:14:46Z,Merged ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129809," merge f ""Lint and inductor tests are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
gpt,[GPT-fast] Update micro benchmark numbers as A100-50G,Fixes ISSUE_NUMBER,2024-06-28T23:36:53Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/129799, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
glm,Add XGLMForCausalLM to the flaky model list,  CC([DONT MERGE][dynamo] Turn on inlining of inbuilt nn modules)  CC(Add XGLMForCausalLM to the flaky model list)  CC(Add lcnet to the inline_inbuilt_nn_module list)  CC([dynamo][onnx] Skip some dynamic=True test with inlining in built nn modules)  CC([dynamo][compiletime] Manually trace torch.nn.Module.parameters) Not failing on devGPU. Went to CI machine ... flaky. So adding to the flaky list. ,2024-06-28T18:22:10Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/129776, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Trying windows fix,Fixes ISSUE_NUMBER,2024-06-28T00:21:49Z,ciflow/trunk release notes: cuda,closed,0,1,https://github.com/pytorch/pytorch/issues/129723, rebase
llm,Disable llm-td step,As it often fails during conda install step with `Unexpected HTTP response: 429`,2024-06-27T23:52:39Z,Merged ciflow/trunk topic: not user facing ciflow/slow,closed,0,6,https://github.com/pytorch/pytorch/issues/129722,Let's leave periodic and slow as is?,> Let's leave periodic and slow as is? Aren't broken slow job will block promotions? But I can leave it in periodic if somebody can confirm it's not blocking viable/strict,"> Aren't broken slow job will block promotions? Can it block? It doesn't run on every commit. And if we stop trunk and pull, likely 429 will disappear","> > Aren't broken slow job will block promotions? >  > Can it block? It doesn't run on every commit. And if we stop trunk and pull, likely 429 will disappear 429 is an temporary thing, it will likely disappear on its own in 30 min or so, but we should not have a job that installs conda every  single run"," merge f ""I see pull trunk and slow jobs being scheduled"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[FSDP2] Ran reduce-scatter copy-in in default stream,"  CC([FSDP2] Ran reducescatter copyin in default stream) This PR runs the reducescatter copyin in the default stream, allowing the reducescatter input (large allocation proportional to unsharded gradients) to be allocated in the default stream to avoid fragmenting that memory across stream memory pools.  In general, minimizing memory usage spikes in nondefaultstream memory pools helps because otherwise, that memory cannot be reused by the default stream outside of that spike. This reducescatter input allocation represents one such spike. The reducescatter outputs are still allocated in the separate `reduce_scatter` stream since they are small and have a nonspiky allocation/free pattern (we iteratively allocate them through backward and free them altogether after optimizer).  This PR should not have any impact on overlap (I sanity checked Llama38B traces from torchtitan; plus we have the `test_fully_shard_overlap.py` unit tests).  **Experiment** **(Before)** Llama38B, 1D FSDP, 8 H100s, bf16/fp32 mixed precision, no AC, local batch size 1:  **(After)** Llama38B, 1D FSDP, 8 H100s, bf16/fp32 mixed precision, no AC, local batch size 1:  **2.53 GiB reduction in peak reserved memory.** ",2024-06-27T23:48:55Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (fsdp2),closed,1,11,https://github.com/pytorch/pytorch/issues/129721,"inductor / rocm6.1py3.8inductor / test (inductor, 1, 1, linux.rocm.gpu.2)08009) test timed out overall (not one specific test timing out) and looks unrelated.  There seems to be some error in the autotuning (again, unrelated):  ``` 20240628T13:50:01.7312163Z inductor/test_max_autotune.py::TestTuningProcess::test_tuning_pool_crash E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120] Exception in TuningProcess 20240628T13:50:01.7314719Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120] Traceback (most recent call last): 20240628T13:50:01.7317515Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120]   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_inductor/autotune_process.py"", line 118, in process_main 20240628T13:50:01.7320134Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120]     TuningProcess.workloop(request_queue, response_queue) 20240628T13:50:01.7322952Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120]   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_inductor/autotune_process.py"", line 135, in workloop 20240628T13:50:01.7325397Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120]     response_queue.put(obj.benchmark()) 20240628T13:50:01.7327668Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120]   File ""/var/lib/jenkins/pytorch/test/inductor/test_max_autotune.py"", line 794, in benchmark 20240628T13:50:01.7330042Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120]     assert visible_devices == self.parent_visible_devices 20240628T13:50:01.7331796Z E0628 09:33:12.215221 139686107169152 torch/_inductor/autotune_process.py:120] AssertionError 20240628T13:50:01.7332961Z PASSED [8.1687s] [ 97%] 20240628T13:50:01.7334258Z inductor/test_max_autotune.py::TestTuningProcess::test_tuning_pool_multiple_devices Traceback (most recent call last): 20240628T13:50:01.7336238Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 324, in _lazy_init 20240628T13:50:01.7337411Z     queued_call() 20240628T13:50:01.7338683Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/random.py"", line 124, in cb 20240628T13:50:01.7340092Z     default_generator = torch.cuda.default_generators[i] 20240628T13:50:01.7340902Z IndexError: tuple index out of range 20240628T13:50:01.7341369Z  20240628T13:50:01.7341844Z The above exception was the direct cause of the following exception: 20240628T13:50:01.7342548Z  20240628T13:50:01.7342780Z Traceback (most recent call last): 20240628T13:50:01.7343429Z   File """", line 1, in  20240628T13:50:01.7344562Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 116, in spawn_main 20240628T13:50:01.7345725Z     exitcode = _main(fd, parent_sentinel) 20240628T13:50:01.7346816Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 125, in _main 20240628T13:50:01.7347898Z     prepare(preparation_data) 20240628T13:50:01.7348939Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 236, in prepare 20240628T13:50:01.7350313Z     _fixup_main_from_path(data['init_main_from_path']) 20240628T13:50:01.7351796Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 287, in _fixup_main_from_path 20240628T13:50:01.7353283Z     main_content = runpy.run_path(main_path, 20240628T13:50:01.7354291Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 265, in run_path 20240628T13:50:01.7355369Z     return _run_module_code(code, init_globals, run_name, 20240628T13:50:01.7356508Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 97, in _run_module_code 20240628T13:50:01.7357582Z     _run_code(code, mod_globals, init_globals, 20240628T13:50:01.7358585Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 87, in _run_code 20240628T13:50:01.7359524Z     exec(code, run_globals) 20240628T13:50:01.7360563Z   File ""/var/lib/jenkins/pytorch/test/inductor/test_max_autotune.py"", line 38, in  20240628T13:50:01.7362012Z     from torch.testing._internal.inductor_utils import HAS_CPU, HAS_CUDA 20240628T13:50:01.7363864Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/testing/_internal/inductor_utils.py"", line 37, in  20240628T13:50:01.7365359Z     HAS_CUDA = torch.cuda.is_available() and has_triton() 20240628T13:50:01.7367008Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/utils/_triton.py"", line 35, in has_triton 20240628T13:50:01.7368551Z     return is_device_compatible_with_triton() and has_triton_package() 20240628T13:50:01.7370352Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/utils/_triton.py"", line 31, in is_device_compatible_with_triton 20240628T13:50:01.7372065Z     if device_interface.is_available() and extra_check(device_interface): 20240628T13:50:01.7373755Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/utils/_triton.py"", line 21, in cuda_extra_check 20240628T13:50:01.7375256Z     return device_interface.Worker.get_device_properties().major >= 7 20240628T13:50:01.7377076Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/device_interface.py"", line 175, in get_device_properties 20240628T13:50:01.7378571Z     device = CudaInterface.Worker.current_device() 20240628T13:50:01.7380166Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/device_interface.py"", line 164, in current_device 20240628T13:50:01.7381517Z     return torch.cuda.current_device() 20240628T13:50:01.7382918Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 928, in current_device 20240628T13:50:01.7384138Z     _lazy_init() 20240628T13:50:01.7385318Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 330, in _lazy_init 20240628T13:50:01.7386580Z     raise DeferredCudaCallError(msg) from e 20240628T13:50:01.7387936Z torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: tuple index out of range 20240628T13:50:01.7389303Z  20240628T13:50:01.7389608Z CUDA call was originally invoked at: 20240628T13:50:01.7390075Z  20240628T13:50:01.7390309Z   File """", line 1, in  20240628T13:50:01.7391540Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 116, in spawn_main 20240628T13:50:01.7392904Z     exitcode = _main(fd, parent_sentinel) 20240628T13:50:01.7394055Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 125, in _main 20240628T13:50:01.7395142Z     prepare(preparation_data) 20240628T13:50:01.7396211Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 236, in prepare 20240628T13:50:01.7397484Z     _fixup_main_from_path(data['init_main_from_path']) 20240628T13:50:01.7398815Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 287, in _fixup_main_from_path 20240628T13:50:01.7400074Z     main_content = runpy.run_path(main_path, 20240628T13:50:01.7401101Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 265, in run_path 20240628T13:50:01.7402188Z     return _run_module_code(code, init_globals, run_name, 20240628T13:50:01.7403322Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 97, in _run_module_code 20240628T13:50:01.7404400Z     _run_code(code, mod_globals, init_globals, 20240628T13:50:01.7405395Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 87, in _run_code 20240628T13:50:01.7406381Z     exec(code, run_globals) 20240628T13:50:01.7407611Z   File ""/var/lib/jenkins/pytorch/test/inductor/test_max_autotune.py"", line 26, in  20240628T13:50:01.7408917Z     from torch._inductor.test_case import run_tests, TestCase 20240628T13:50:01.7409953Z   File """", line 991, in _find_and_load 20240628T13:50:01.7411099Z   File """", line 975, in _find_and_load_unlocked 20240628T13:50:01.7412224Z   File """", line 671, in _load_unlocked 20240628T13:50:01.7413333Z   File """", line 843, in exec_module 20240628T13:50:01.7414508Z   File """", line 219, in _call_with_frames_removed 20240628T13:50:01.7416178Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_inductor/test_case.py"", line 5, in  20240628T13:50:01.7417440Z     from torch._dynamo.test_case import ( 20240628T13:50:01.7418318Z   File """", line 991, in _find_and_load 20240628T13:50:01.7419427Z   File """", line 975, in _find_and_load_unlocked 20240628T13:50:01.7420536Z   File """", line 671, in _load_unlocked 20240628T13:50:01.7421600Z   File """", line 843, in exec_module 20240628T13:50:01.7422768Z   File """", line 219, in _call_with_frames_removed 20240628T13:50:01.7424410Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/test_case.py"", line 8, in  20240628T13:50:01.7426112Z     from torch.testing._internal.common_utils import (   type: ignore[attrdefined] 20240628T13:50:01.7427329Z   File """", line 991, in _find_and_load 20240628T13:50:01.7428457Z   File """", line 975, in _find_and_load_unlocked 20240628T13:50:01.7429737Z   File """", line 671, in _load_unlocked 20240628T13:50:01.7430859Z   File """", line 843, in exec_module 20240628T13:50:01.7432265Z   File """", line 219, in _call_with_frames_removed 20240628T13:50:01.7434209Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/testing/_internal/common_utils.py"", line 817, in  20240628T13:50:01.7435544Z     torch.manual_seed(SEED) 20240628T13:50:01.7436708Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_compile.py"", line 31, in inner 20240628T13:50:01.7437996Z     return disable_fn(*args, **kwargs) 20240628T13:50:01.7439482Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 601, in _fn 20240628T13:50:01.7440664Z     return fn(*args, **kwargs) 20240628T13:50:01.7441882Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/random.py"", line 46, in manual_seed 20240628T13:50:01.7443051Z     torch.cuda.manual_seed_all(seed) 20240628T13:50:01.7444442Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/random.py"", line 127, in manual_seed_all 20240628T13:50:01.7445687Z     _lazy_call(cb, seed_all=True) 20240628T13:50:01.7447149Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 248, in _lazy_call 20240628T13:50:01.7448714Z     _lazy_seed_tracker.queue_seed_all(callable, traceback.format_stack()) 20240628T13:50:01.7449425Z  20240628T13:50:01.7449647Z Traceback (most recent call last): 20240628T13:50:01.7450994Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 324, in _lazy_init 20240628T13:50:01.7452170Z     queued_call() 20240628T13:50:01.7453258Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/random.py"", line 124, in cb 20240628T13:50:01.7454506Z     default_generator = torch.cuda.default_generators[i] 20240628T13:50:01.7455314Z IndexError: tuple index out of range 20240628T13:50:01.7455776Z  20240628T13:50:01.7456236Z The above exception was the direct cause of the following exception: 20240628T13:50:01.7456948Z  20240628T13:50:01.7457166Z Traceback (most recent call last): 20240628T13:50:01.7457811Z   File """", line 1, in  20240628T13:50:01.7458940Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 116, in spawn_main 20240628T13:50:01.7460093Z     exitcode = _main(fd, parent_sentinel) 20240628T13:50:01.7461201Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 125, in _main 20240628T13:50:01.7462289Z     prepare(preparation_data) 20240628T13:50:01.7463322Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 236, in prepare 20240628T13:50:01.7464590Z     _fixup_main_from_path(data['init_main_from_path']) 20240628T13:50:01.7465912Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 287, in _fixup_main_from_path 20240628T13:50:01.7467180Z     main_content = runpy.run_path(main_path, 20240628T13:50:01.7468179Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 265, in run_path 20240628T13:50:01.7469249Z     return _run_module_code(code, init_globals, run_name, 20240628T13:50:01.7470459Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 97, in _run_module_code 20240628T13:50:01.7471528Z     _run_code(code, mod_globals, init_globals, 20240628T13:50:01.7472525Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 87, in _run_code 20240628T13:50:01.7473455Z     exec(code, run_globals) 20240628T13:50:01.7474501Z   File ""/var/lib/jenkins/pytorch/test/inductor/test_max_autotune.py"", line 38, in  20240628T13:50:01.7475884Z     from torch.testing._internal.inductor_utils import HAS_CPU, HAS_CUDA 20240628T13:50:01.7477671Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/testing/_internal/inductor_utils.py"", line 37, in  20240628T13:50:01.7479159Z     HAS_CUDA = torch.cuda.is_available() and has_triton() 20240628T13:50:01.7480643Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/utils/_triton.py"", line 35, in has_triton 20240628T13:50:01.7482070Z     return is_device_compatible_with_triton() and has_triton_package() 20240628T13:50:01.7483868Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/utils/_triton.py"", line 31, in is_device_compatible_with_triton 20240628T13:50:01.7485521Z     if device_interface.is_available() and extra_check(device_interface): 20240628T13:50:01.7492068Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/utils/_triton.py"", line 21, in cuda_extra_check 20240628T13:50:01.7493767Z     return device_interface.Worker.get_device_properties().major >= 7 20240628T13:50:01.7495585Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/device_interface.py"", line 175, in get_device_properties 20240628T13:50:01.7497096Z     device = CudaInterface.Worker.current_device() 20240628T13:50:01.7498692Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/device_interface.py"", line 164, in current_device 20240628T13:50:01.7500046Z     return torch.cuda.current_device() 20240628T13:50:01.7501454Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 928, in current_device 20240628T13:50:01.7502660Z     _lazy_init() 20240628T13:50:01.7503837Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 330, in _lazy_init 20240628T13:50:01.7505095Z     raise DeferredCudaCallError(msg) from e 20240628T13:50:01.7506476Z torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: tuple index out of range 20240628T13:50:01.7507560Z  20240628T13:50:01.7507801Z CUDA call was originally invoked at: 20240628T13:50:01.7508247Z  20240628T13:50:01.7508482Z   File """", line 1, in  20240628T13:50:01.7509771Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 116, in spawn_main 20240628T13:50:01.7510933Z     exitcode = _main(fd, parent_sentinel) 20240628T13:50:01.7512017Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 125, in _main 20240628T13:50:01.7513092Z     prepare(preparation_data) 20240628T13:50:01.7514119Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 236, in prepare 20240628T13:50:01.7515393Z     _fixup_main_from_path(data['init_main_from_path']) 20240628T13:50:01.7516830Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/multiprocessing/spawn.py"", line 287, in _fixup_main_from_path 20240628T13:50:01.7518340Z     main_content = runpy.run_path(main_path, 20240628T13:50:01.7519533Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 265, in run_path 20240628T13:50:01.7520817Z     return _run_module_code(code, init_globals, run_name, 20240628T13:50:01.7522170Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 97, in _run_module_code 20240628T13:50:01.7522948Z     _run_code(code, mod_globals, init_globals, 20240628T13:50:01.7523444Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/runpy.py"", line 87, in _run_code 20240628T13:50:01.7523885Z     exec(code, run_globals) 20240628T13:50:01.7524370Z   File ""/var/lib/jenkins/pytorch/test/inductor/test_max_autotune.py"", line 26, in  20240628T13:50:01.7524963Z     from torch._inductor.test_case import run_tests, TestCase 20240628T13:50:01.7525449Z   File """", line 991, in _find_and_load 20240628T13:50:01.7525986Z   File """", line 975, in _find_and_load_unlocked 20240628T13:50:01.7526514Z   File """", line 671, in _load_unlocked 20240628T13:50:01.7527029Z   File """", line 843, in exec_module 20240628T13:50:01.7527581Z   File """", line 219, in _call_with_frames_removed 20240628T13:50:01.7528354Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_inductor/test_case.py"", line 5, in  20240628T13:50:01.7528944Z     from torch._dynamo.test_case import ( 20240628T13:50:01.7529348Z   File """", line 991, in _find_and_load 20240628T13:50:01.7529864Z   File """", line 975, in _find_and_load_unlocked 20240628T13:50:01.7530376Z   File """", line 671, in _load_unlocked 20240628T13:50:01.7530877Z   File """", line 843, in exec_module 20240628T13:50:01.7531490Z   File """", line 219, in _call_with_frames_removed 20240628T13:50:01.7532310Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/test_case.py"", line 8, in  20240628T13:50:01.7533089Z     from torch.testing._internal.common_utils import (   type: ignore[attrdefined] 20240628T13:50:01.7533652Z   File """", line 991, in _find_and_load 20240628T13:50:01.7534181Z   File """", line 975, in _find_and_load_unlocked 20240628T13:50:01.7534698Z   File """", line 671, in _load_unlocked 20240628T13:50:01.7535207Z   File """", line 843, in exec_module 20240628T13:50:01.7535750Z   File """", line 219, in _call_with_frames_removed 20240628T13:50:01.7536574Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/testing/_internal/common_utils.py"", line 817, in  20240628T13:50:01.7537200Z     torch.manual_seed(SEED) 20240628T13:50:01.7537751Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_compile.py"", line 31, in inner 20240628T13:50:01.7538293Z     return disable_fn(*args, **kwargs) 20240628T13:50:01.7538910Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 601, in _fn 20240628T13:50:01.7539460Z     return fn(*args, **kwargs) 20240628T13:50:01.7540020Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/random.py"", line 46, in manual_seed 20240628T13:50:01.7540559Z     torch.cuda.manual_seed_all(seed) 20240628T13:50:01.7541200Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/random.py"", line 127, in manual_seed_all 20240628T13:50:01.7541772Z     _lazy_call(cb, seed_all=True) 20240628T13:50:01.7542371Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 248, in _lazy_call 20240628T13:50:01.7543047Z     _lazy_seed_tracker.queue_seed_all(callable, traceback.format_stack()) 20240628T13:50:01.7543375Z  20240628T13:50:01.7543530Z Uncaught exception in compile_worker subprocess 20240628T13:50:01.7543883Z Traceback (most recent call last): 20240628T13:50:01.7544563Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_inductor/compile_worker/__main__.py"", line 42, in main 20240628T13:50:01.7545157Z     pre_fork_setup() 20240628T13:50:01.7545780Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_inductor/async_compile.py"", line 57, in pre_fork_setup 20240628T13:50:01.7546392Z     caching_device_properties() 20240628T13:50:01.7547106Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_inductor/async_compile.py"", line 73, in caching_device_properties 20240628T13:50:01.7547799Z     device_interface.Worker.get_device_properties() 20240628T13:50:01.7548566Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/device_interface.py"", line 178, in get_device_properties 20240628T13:50:01.7549196Z     device_prop = [ 20240628T13:50:01.7549840Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_dynamo/device_interface.py"", line 179, in  20240628T13:50:01.7550448Z     torch.cuda.get_device_properties(i) 20240628T13:50:01.7551132Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 519, in get_device_properties 20240628T13:50:01.7551891Z     return _get_device_properties(device)   type: ignore[namedefined] 20240628T13:50:01.7553039Z RuntimeError: device >= 0 && device  20240628T13:50:01.7561015Z     torch.cuda.get_device_properties(i) 20240628T13:50:01.7561686Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 519, in get_device_properties 20240628T13:50:01.7562452Z     return _get_device_properties(device)   type: ignore[namedefined] 20240628T13:50:01.7563579Z RuntimeError: device >= 0 && device ", merge i," Merge started Your change will be merged while ignoring the following 20 checks: pull / beforetest / llmretrieval, inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 2, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 2, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (dynamic_cpu_inductor_torchbench, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (dynamic_cpu_inductor_torchbench, 2, 2, linux.12xlarge), inductor / rocm6.1py3.8inductor / test (inductor, 1, 1, linux.rocm.gpu.2), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (aot_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (aot_inductor_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamic_aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamic_aot_eager_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: 19 jobs have failed, first few of them are: pull / beforetest / llmretrieval, inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 2, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 2, 2, linux.12xlarge) Details for Dev Infra team Raised by workflow job ","As mentioned above, the failing inductor test looks unrelated.", merge i," Merge started Your change will be merged while ignoring the following 22 checks: pull / beforetest / llmretrieval, inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 2, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 2, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (dynamic_cpu_inductor_torchbench, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (dynamic_cpu_inductor_torchbench, 2, 2, linux.12xlarge), inductor / rocm6.1py3.8inductor / test (inductor, 1, 1, linux.rocm.gpu.2), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (aot_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.1py3.10gcc9sm86 / test (aot_inductor_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamic_aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamic_aot_eager_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), trunk / macospy3arm64mps / test (mps, 1, 1, macosm113), trunk / macospy3arm64mps / test (mps, 1, 1, macosm114) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: 21 jobs have failed, first few of them are: pull / beforetest / llmretrieval, inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench, 2, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 1, 2, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_torchbench_freezing, 2, 2, linux.12xlarge) Details for Dev Infra team Raised by workflow job ",No new failures since the 1st attempt at `merge i` Going to use `merge f` since `merge i` is not ignoring ," merge f ""unrelated failures and i not working"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,Use setup-miniconda step from test-infra for llm retrival workflow,Undo https://github.com/pytorch/pytorch/pull/129722 Use the setupminiconda step in written in testinfra to install miniconda in the llm retrieval workflow.  It comes with a cache so we don't have to worry about hitting cache limits.  The llm retrieval job was failing due to too many requests  CC(UNSTABLE trunk / before-test / llm-retrieval)issue2379260544 https://github.com/pytorch/testinfra/blob/2aba8f107aca99710cca5cd97b6c16500eb808e2/.github/actions/setupminiconda/action.ymlL1,2024-06-27T23:48:38Z,Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/129720, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64mps / test (mps, 1, 1, macosm113) Details for Dev Infra team Raised by workflow job ",> setuppython action I tried to use it in https://github.com/pytorch/pytorch/pull/129999 but ran into issues regarding not being able to find the correct version.  I think I'm missing some information about our runner types >  if one wants to use GH runners for this job I don't think this is going to happen since this job requires a gpu," merge f ""failures exist on master"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,UNSTABLE trunk / before-test / llm-retrieval,The jobs fail with Unexpected HTTP response: 429 (Too Many Requests) when downloading miniconda: https://hud.pytorch.org/pytorch/pytorch/commit/67416a2996349a2339328cac0f7e54c7d3b3c1d9 CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)21099 /pytorchdevinfra,2024-06-27T23:36:35Z,module: ci unstable,closed,0,4,https://github.com/pytorch/pytorch/issues/129718,"Hmm, wouldn't that just mark all tests unstable (as llmretrieval depends on targetdetermination and all tests depend on targetdetermination)?","> wouldn't that just mark all tests unstable I don't think so. llmretrieval is unstable already on HUD, but not everything.",Closing as job was removed by https://github.com/pytorch/pytorch/pull/129722,The llmretrieval job is being reenabled and now uses a cache in https://github.com/pytorch/pytorch/pull/129720 to prevent rate limiting in the future.
llm,UNSTABLE pull / before-test / llm-retrieval,The jobs fail with Unexpected HTTP response: 429 (Too Many Requests) when downloading miniconda: https://hud.pytorch.org/pytorch/pytorch/commit/67416a2996349a2339328cac0f7e54c7d3b3c1d9 CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)21099 /pytorchdevinfra,2024-06-27T23:35:34Z,module: ci unstable,closed,0,1,https://github.com/pytorch/pytorch/issues/129717,Closing as job was removed by https://github.com/pytorch/pytorch/pull/129722
yi,[TensorDict - compile] Dynamo: set(arbitrary_iterable_object)` does not work but `{k for k in arbitrary_iterable_object}` does," üêõ Describe the bug In TensorDict, we observe that with dynamo,  does not work but   does. https://github.com/pytorch/tensordict/blob/b5cb40aaeb824ac632c0a2f178430105513525d1/tensordict/tensorclass.pyL778 Error   Versions Collecting environment information... PyTorch version: 2.5.0.dev20240627 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.5 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: version 3.29.3 Libc version: N/A Python version: 3.11.7 (v3.11.7:fa7a6f2303, Dec 4 2023, 15:22:56) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS14.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Max Versions of relevant libraries: [pip3] mujocotorch==0.1.0 [pip3] numpy==1.26.4 [pip3] optree==0.11.0 [pip3] torch==2.5.0.dev20240627 [pip3] torchao==0.1 [pip3] torchaudio==2.4.0.dev20240627 [pip3] torchrl==0.4.0+7c0a7c8 [pip3] torchtext==0.18.0 [pip3] torchvision==0.20.0.dev20240627 [conda] Could not collect ",2024-06-27T15:25:00Z,triaged oncall: pt2 module: graph breaks,closed,0,0,https://github.com/pytorch/pytorch/issues/129664
transformer,Implementation of a numerically stable log(1 - softmax) function in PyTorch," üöÄ The feature, motivation and pitch Hello, I am working on editing knowledge in transformers which, in the case of knowledge deletion, requires the minimization of the likelihood of targeted sequences of tokens, and therefore, I need in my work, a numerically stable log(1softmax) function. Details are below. The maximization of the likelihood of correct tokens is extensively used when training LLMs, and it requires an efficient and numerically stable implementation of the log(softmax(x)) function (to compute the CrossEntropy Loss) which was available in Pytorch for as long as I can remember. On the other hand, the minimization requires the implementation of a stable log(1softmax(x)) function **which is not available at the moment**. The naive implementation of log(1softmax) using torch.log and torch.softmax is highly unstable when the softmax(x) is close to 1 for one element of its output. I used it myself and I found that my loss sometimes equaled ""inf."" which consequently, produced ""nan"" values when backpropagating. I consider the log(1softmax) to be an important function in my usecase but also more generally in classification tasks. One could assume that someone would like to maximize some classes while minimizing others in some usecase. After asking for a numerically stable log(1softmax) function in a Pytorch forum, a user named KFrank proposed an elegant solution:  I generalized it to multidimensional tensors:  Finally I tested its numerical stability ; the methodology and results are explained in the previous Pytorch forum.  Request Is it possible to add this function to the list of native functions of torch, such that it can be u",2024-06-27T14:36:42Z,module: numerical-stability triaged module: python frontend,open,3,2,https://github.com/pytorch/pytorch/issues/129657,I'm curious why we need a native implementation for this? Would it be more performant than the python implementation suggested above? Also curious if using `torch.compile()` on this python implementation works well. That would remove the need to have to write a fast kernel by hand!," maybe the question is on having a shortcut for this in the core (as maybe for `log(1sigmoid(x))`), and the premise is that these are needed fairly frequently (and tricky to come about this impl handling numerics correctly) if you judge these are fairly frequently needed, I think it's great that if torch.compile produces fast fused codegen, then more of such functions can be added in core just as:  and maybe CI tests could exist to track regressions or memory consumption of produced generated code (in the meanwhile, it's a great workaround to have it pasted in user code as is of course)"
yi,include jit/*.pyi,"Fixes CC(Improve type annotations for `jit.script`), see https://github.com/pytorch/pytorch/pull/108782issuecomment1927321532 ",2024-06-27T12:57:08Z,module: typing open source Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/129654," label ""module: typing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,`torch.compile()` failing with PyTorch 2.3 for tupled inputs," üêõ Describe the bug One of our Torch compile tests was running successfully with PyTorch 2.2 but isn't anymore with PyTorch 2.3.1. Have tried with the nightlies but no luck.  Error logs   Minified repro I am not sure if running the minifier would be sensible here. So, going to provide all the steps to reproduce: 1. `git clone https://github.com/huggingface/diffusers/` 2. `cd diffusers && pip install e .` 3. `cd benchmarks && mkdir benchmark_outputs` 4. `pip install transformers accelerate` 5. `python benchmark_ip_adapters.py run_compile`  Versions  ",2024-06-27T07:33:19Z,triaged oncall: pt2 module: dynamo,open,0,0,https://github.com/pytorch/pytorch/issues/129637
yi,Add more dtypes to __cuda_array_interface__,`__cuda_array_interface__` was missing some unsigned integer dtypes as well as BF16. numba doesn't support BF16 so I skip tests for that one.,2024-06-27T01:09:35Z,triaged open source Merged ciflow/trunk release notes: cuda,closed,0,4,https://github.com/pytorch/pytorch/issues/129621,Maybe  for numpy compat? I'm not sure who owns this code,"  > Any reason why we should add `bfloat16` in one place but not the test? The tests are using numba and numba doesn't support BF16 so we can't extend the tests for it > It seems a bit odd to me to translate bfloat16 and float16 silently. If the user wants to do that, they should explicitly perform the cast, I think. Can you expand on this? We're not translating BP16 to FP16 silently, they just happen to both occupy the same amount of bytes and both be floating point numbers so their typestr is the same",Thank you for the PR!  merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[dtensor][be] Reduced redundant LOC by creating functions to set up models used in example,"  CC([dtensor][debug] added deviceMesh for relevant operations and module parameter sharding and module fqn)  CC([dtensor][debug] Added functionality to convert log into a json file)  CC([dtensor][be] Reduced redundant LOC by creating functions to set up models used in example)  CC([dtensor][debug] Added forward and backward differentiation for module level tracing) **Summary** As the CommModeFeature example file grew, there were to many LOC that was repeated for setting up the models used. I created two functions, one to handle MLP and MLPStacked models and the other for transformer models. The output of the examples will not have changed. **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_distributed_sharding_display 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLPStacked_distributed_sharding_display 3. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_module_tracing 4. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_module_tracing 5. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_operation_tracing 6. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing ",2024-06-26T23:28:11Z,oncall: distributed better-engineering Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129613, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model,  CC([Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model)  CC([Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP)  CC([Traceable FSDP2] Add autofunctionalize support for mutable list[Tensor] (copy from Brian's PR 127347); enable E2E inductor unit test for transformer model) ,2024-06-26T23:16:59Z,oncall: distributed Stale topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/129611,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llm,torch._dynamo.exc.Unsupported: call_method NNModuleVariable() state_dict [] {}, üêõ Describe the bug I'm seeing error:` torch._dynamo.exc.Unsupported: call_method NNModuleVariable() state_dict [] {}` with following code:  Wondering if anyone knows a solution or any idea on how to rewrite the ^ part to avoid error?  Versions PyTorch version: 2.2.0 PyTorch CUDA version: 11.2 ,2024-06-26T21:29:08Z,triaged oncall: pt2 module: dynamo,closed,0,1,https://github.com/pytorch/pytorch/issues/129604,"If you just want to get the device of parameters, you can try: "
langsmith,"OSError: [WinError 126] The specified module could not be found. Error loading ""\.venv\Lib\site-packages\torch\lib\fbgemm.dll"" or one of its dependencies."," üêõ Describe the bug **I was working on fine tuning the llm, while executing the below piece of code encounter the [OSError] ** **Error **   **Stack Trace **   **Code **     Versions System Information  > OS:  Windows > OS Version:  10.0.22631 > Python Version:  3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)] > Environment: Virtual Environment (.venv)  Package Information  > langchain_core: 0.2.9 > langchain: 0.2.5 > langchain_community: 0.2.5 > langsmith: 0.1.77 > langchain_chroma: 0.1.1 > langchain_openai: 0.1.8 > langchain_text_splitters: 0.2.1",2024-06-26T13:47:31Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/129568,It looks like fbgemm is missing from your local environment. You might get better help posting on the pytorch forum  can you post there? https://discuss.pytorch.org/,libomp140.x86_64.dll‰∏ãËΩΩÂú∞ÂùÄÔºöhttps://www.dllme.com/dll/files/libomp140_x86_64/00637fe34a6043031c9ae4c6cf0a891d/download Âè™ÈúÄ‰∏ãËΩΩ[libomp140.x86_64.dll]Âπ∂Â∞ÜÂÖ∂ÊîæÂú®‚ÄúC:\Windows\System32‚Äù‰∏≠ÔºåÂ∞±ÂèØ‰ª•Ëß£ÂÜ≥ÈóÆÈ¢ò
llm,[torchbind] Support torchbind's call_method in inductor,"Example output: P1444674437 Currently the `call_torchbind` ops get directly inlined into the python code. I originally was thinking we could codegen `call_torchbind` calls to be `object.method`, but it seems like directly inlining the `call_torchbind` calls work :thonk: ",2024-06-26T00:28:52Z,Stale module: inductor module: dynamo ciflow/inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/129537,"> I'm surprised that there are this many changes for call_method. Can we make call_method into a function where the first argument is self? The calling convention has self as the first argument: `call_torchbind(self: ScriptObject, method_name: str, *method_args, **method_kwargs)`",why do we need both ScriptObject and FakeScriptObject ?,"> why do we need both ScriptObject and FakeScriptObject ? Theoretically we only need FakeScriptObject, but I'm not sure what's the coverage of converting the current usages of ScriptObject to FakeScriptObject.",Could we do that as part of this PR ?,updated!,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[CP] Make CP loss curve on par with TP,"  CC([CP][Experimental] Introduce enable_context_parallel() and context_parallel_buffers())  CC([dtensor][experimental] distribute_function)  CC([CP] Refactor the code)  CC([CP] Make CP loss curve on par with TP)  CC([CP] Fix the incorrect ring schedule in the fwd and bwd)  CC([dtensor] Add dtensor to TORCH_LOGS) Summary: This PR changes two implementations to make CP (CP8) lose curve be on par with TP (TP8). 1. Making key and value contiguous before doing ring attention. It is unclear why this is a requirement as SDPA does not have this requirement. 2. Use the out, grad_out, softmax_lse passed by autograd to do the backward. This implementation is similar to the implementation in transformer engine. The original implementation reruns the SDPA to get the output and logsumexp and uses that reculcated results to infer the corrected softmax_lse. But that implementation does not give a better accuracy or lose curve. Instead, that implementation converges slower. ",2024-06-25T21:42:51Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129515, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[Traceable FSDP2] Add auto-functionalize support for mutable list[Tensor] (copy from Brian's PR #127347); enable E2E inductor unit test for transformer model,Copy of Brian's PR: https://github.com/pytorch/pytorch/pull/127347 with additional changes to support mutable `List[Tensor]` in Inductor. Also enable E2E inductor unit test for Traceable FSDP2 + transformer model. Test commands:  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_trace_fsdp_set_`  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_simple_mlp_fullgraph_backend_aot_eager`  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_simple_mlp_fullgraph_backend_inductor`  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_transformer_fullgraph_backend_aot_eager`  `pytest rA test/dynamo/test_misc.py::MiscTests::test_auto_functionalize_tensorlist`  `pytest rA  test/inductor/test_torchinductor.py::GPUTests::test_fallback_mutable_op_list_cuda`   CC([Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model)  CC([Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP)  CC([Traceable FSDP2] Add autofunctionalize support for mutable list[Tensor] (copy from Brian's PR 127347); enable E2E inductor unit test for transformer model) ,2024-06-25T20:33:07Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing module: inductor module: dynamo ciflow/inductor keep-going,closed,0,8,https://github.com/pytorch/pytorch/issues/129502,"Ok, we found that we can't actually functionalize `fsdp.split_with_sizes_copy` without also functionalizing `fsdp.set_`, and we already know we can't functionalize the latter because we want to avoid AOTAutograd input dedup struggle. So we prefer to also defunctionalize `fsdp.split_with_sizes_copy` instead.",Closing in favor of https://github.com/pytorch/pytorch/pull/129422, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / test (default, 1, 3, macosm1stable) Details for Dev Infra team Raised by workflow job ","CI shows error in `test_ci_sanity_check_fail.py::TestCISanityCheck::test_env_vars_exist` macospy3arm64 / test (default, 1, 3, macosm1stable), which I don't believe is related to this PR. I will proceed to land this PR."," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
gpt,[GPT-benchmark] Distinguish LLM models and mirco-benchmarks,Fixes ISSUE_NUMBER,2024-06-25T20:05:55Z,Merged ciflow/trunk topic: not user facing test-config/default ciflow/inductor-micro-benchmark test-config/inductor-micro-benchmark,closed,0,2,https://github.com/pytorch/pytorch/issues/129498," merge f ""irrelevant failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[codemod] Fix deprecated dynamic exception in pytorch/FasterTransformer/FasterTransformer/tests/unittests/unittest_utils.h +5,Summary: LLVM has detected a violation of `Wdeprecateddynamicexceptionspec`. Dynamic exceptions were removed in C++17. This diff fixes the deprecated instance(s). See Dynamic exception specification and noexcept specifier. Test Plan: Sandcastle Reviewed By: dmmfb Differential Revision: D58953076,2024-06-25T13:24:28Z,fb-exported topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/129471,This pull request was **exported** from Phabricator. Differential Revision: D58953076,This pull request was **exported** from Phabricator. Differential Revision: D58953076
transformer,[PT2][fp8][FSDP2] compile the function that pre-computes fp8 amax ," üöÄ The feature, motivation and pitch share repro for  ,  on the gaps of torch.compile for FSDP2 fp8 allgather for FSDP2 fp8 allgather, it's criticial to precompute amax for `model.parameters()`  However, `compute_amaxes_compile` is slower than `compute_amaxes_eager` Sanpshot of cpu overhead  snapshot of gpu time  repro * pytorch with nested tensor subclass support, otherwies IMA during torch.compile https://github.com/pytorch/pytorch/pull/127431 * torchao: https://github.com/pytorch/ao * command `torchrun standalone nproc_per_node=2 compile_precompute.py`     Alternatives _No response_  Additional context _No response_ ",2024-06-25T07:00:52Z,oncall: distributed triaged module: fsdp oncall: pt2 pt2d-triage-nov2024,open,0,10,https://github.com/pytorch/pytorch/issues/129457,"Also, I think that _foreach_max was added in :   https://github.com/pytorch/pytorch/pull/127187"," I think we are already using `_foreach_norm(ord=math.inf)` in the eager path, which can compute both the abs and the max together. (If we used `_foreach_max`, then we still need to run abs first separately.)","Yeah, I was mainly wondering about the compile path... Currently it has a list comprehension for computing a max...","Findings so far: Eager version contains 1 nccl:all_reduce operation when it handles torch.clamp Compiled version contains a lot of nccl:all_reduce ( I guess one for every split item). They are coming from generating backward and keeping that backward inside forward. (I suspect mutations were registered.) E.g. https://gist.github.com/IvanKobzarev/245e088b449885af54eaf992050d6928 is the default graph (with training) If to use   Compiled duration is x2 faster than eager (used torch.no_grad() for compiled and eager)   , if you use it only for inference, potentially to unblock for compilation will be to add `with torch.no_grad()`: With no_grad subclasses overhead is pretty feasible on the profiling is about 300us for wrap and 300us unwrap when compute_amaxes_compiled region execution is 400us In total 1ms. Debugging it.","> Findings so far: Eager version contains 1 nccl:all_reduce operation when it handles torch.clamp >  > Compiled version contains a lot of nccl:all_reduce ( I guess one for every split item). They are coming from generating backward and keeping that backward inside forward. (I suspect mutations were registered.) is it possible for compiler to have 1 nccl:all_reduce? there can be N parameters (say N = 100), and we have to do them together for perf reasons I do not need backward so `no_grad` is feasible. just the entering no_grad and exiting no_grad might degrade the perf? ","> is it possible for compiler to have 1 nccl:all_reduce? there can be N parameters (say N = 100), and > we have to do them together for perf reasons no_grad compiled version has 1 all_reduce, the same as eager. Additional all_reduce show up only when grad_enabled(). > I do not need backward so no_grad is feasible. just the entering no_grad and exiting no_grad  > might degrade the perf? There should not be any degradation in entering no_grad.  It will instruct aot_autograd to not generate backward and any autograd tracking. So it will not present in any graph behind the dynamo graph.","Checked the unwrap/wrap logic, have not found any special. The model has 20 fp8 weights. They become the input arguments. Unwrap/wrap of each of them takes 14us. As a result we get 20 * 14us. The problem is that compiled kernel will be optimized to be smaller and smaller, but this overhead of wrap/unwrap is proportional to number of weights. ","If you need only inference, we can fuse all wrap/unwrap of weights into the graph using inductor freezing.  Results in compiled to be x6 faster than eager ","Found that specifying `__slots__` a bit helps with the cost of `flatten`, on my profile it i s `14us > 11us` per fp8 weights. For 20 weights in this example this makes 300us > 260us. https://github.com/pytorch/ao/pull/1211 ","> I do not need backward so no_grad is feasible. just the entering no_grad and exiting no_grad might degrade the perf?  another reason to use `no_grad()` (even if you are worried about a few microseconds of python overhead) is that without no_grad, compile will (pessimistically) assume that you need to train on that function, and potentially compute and saveforbackward extra activations that you don't actually need."
llama,[dynamic][inline-inbuilt-nn-modules] Extra compilation - pytest test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py::TestDynamoWithONNXRuntime::test_llama_attention_with_local_backend_0," üêõ Describe the bug **Note that this is the only failure that prevents us from turning on inlining inbuilt nn modules** (see this PR). So, we should prioritize fixing/skipping this test. **This test uses dynamic=True which is not recommended for torch.compile**  Repro instructions Get CI HF commit    Run this cmd    Full paste  https://www.internalfb.com/intern/paste/P1442682822/ !image There is a recomp happening here on seemingly very different inputs because they are using the same symbol for representing a size. I guess this is just an heuristic to reduce the number of symints if they have the same int value. Maybe the heuristic should change such that each param/buffer (which are lifted as inputs to the Dynamo graph) have their own symbols, and do not collide with input symbols. Just guessing. I can definitely skip this test because it uses `dynamic=True`, but I want us to think if the test has exposed some fundamental issue.    Error logs _No response_  Minified repro _No response_  Versions NA",2024-06-25T06:57:48Z,module: onnx triaged oncall: pt2 module: dynamic shapes,open,0,7,https://github.com/pytorch/pytorch/issues/129456,://github.com/pytorch/pytorch/pull/129804 I'm less certain about how exactly we should setup the heuristic to disable duck sizing though ü§î ," assuming duck sizing is desirable for faster compile times and better codegen, does it make sense to disable when there are few duck sizable inputs?","I don't really know what you're proposing here. Maybe you're saying something like, if there are only two sizevars that have the same hint 5, don't duck size them, but if you have a lot (over some threshold K) of sizevars that have all the same hint, duck size them? But I don't really know how I would implement this, because we have to make an alloc/no alloc decision as we encounter them. I guess we could do something similar to how we are doing compile collectives, which is run Dynamo once with static sizes to see where all the potential sizes show up, and then use this to make a determination later...","I am a bit spooked by unpredictable recompiles in this case, since if you've got a lot of variables your more likely to get ""unlucky"" and have two sizes falsely alias with same hint when it is spurious and you trigger a recompile next time. This is why I'm kind of down on duck sizing, I kind of want to just get rid of it entirely...",,"I'd be ok getting rid of it, but we should do some testing to see if that causes perf/graph break regressions.",OSS models seem fine: https://hud.pytorch.org/benchmark/compilers?dashboard=torchinductor&startTime=Mon%2C%2012%20Aug%202024%2019%3A03%3A43%20GMT&stopTime=Mon%2C%2019%20Aug%202024%2019%3A03%3A43%20GMT&granularity=hour&suite=torchbench&mode=inference&dtype=bfloat16&deviceName=cuda%20(a100)&lBranch=xmfan/disable_duck_shape&lCommit=1e05c9ecda8e8ac3ac3aded59e7f02e775d6c8dc&rBranch=main&rCommit=5ed3b70d09a4ab2a5be4becfda9dd0d3e3227c39 probably need to add a JK and communicate for internal
yi,[Intel GPU] Add XPU into device list of copy_impl," Motivation `copy_`  calls `copy_stub`  for each backend.  However, solely using `REGISTER_XPU_DISPATCH` could not goes into `copy_stub`  since there is another device list in `copy_impl` func and XPU is not in it. This PR add XPU device into the supported list of `copy_impl` so as to allow calling xpu kernel via `copy_stub`   CC([Intel GPU] Add XPU into device list of copy_impl)  CC([Intel GPU] xpuops codegen via backend whitelist)  CC([Intel GPU] Dispatch Stub support) ",2024-06-25T05:34:56Z,oncall: distributed module: cpu open source module: amp (automated mixed precision) NNC ciflow/trunk release notes: quantization topic: not user facing ciflow/mps module: inductor module: dynamo ciflow/inductor module: distributed_checkpoint ciflow/xpu release notes: xpu module: xpu,closed,0,0,https://github.com/pytorch/pytorch/issues/129452
yi,OperatorBase.py_impl should support with_keyset=True extra argument.," üöÄ The feature, motivation and pitch In export, we want to override Autograd alias keys so that we don't decompose via CIA. As a result, we would want to error when someone tries to run autograd while tracing (like running to joint graph). To do this correctly, we want to reuse the logic that we use for custom ops which relies on op.redispatch behaviour which requires us to pass in DispatchKeySet. This doesn't work today with python dispatchers today and we should implement it.   Alternatives _No response_  Additional context _No response_ ",2024-06-25T00:22:35Z,triaged oncall: pt2 oncall: export module: pt2-dispatcher,open,0,0,https://github.com/pytorch/pytorch/issues/129430
yi,Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files,"https://github.com/pytorch/pytorch/pull/129001discussion_r1645126801 is the motivation for the whole stack of PRs. In `torch/__init__.py`, `torch._C.Type` shadows `from typing import Type`, and there is no type stub for `torch._C.Type` in `torch/_C/__init__.pyi`. So we need to use `from typing import Type as _Type`. After enabling Generic TypeAlias (PEP 585) in the `.pyi` type stub files, we can use `type` instead of `typing.Type` or `from typing import Type as _Type`.    CC([CI] Run `lintrunner` on generated `.pyi` stub files in CI)  CC(Refactor `torch/utils/data/datapipes/gen_pyi.py` with `torchgen`)  CC([BE][Easy] Fix `PYI034`: nonselfreturntype in tensor method hints)  CC(Add `__all__` to `torch/nn/functional.pyi` and `torch/return_types.pyi`)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC([torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path`)   Generic TypeAlias (PEP 585): e.g. `typing.List[T] > list[T]`, `typing.Dict[KT, VT] > dict[KT, VT]`, `typing.Type[T] > type[T]`.  Union Type (PEP 604): e.g. `Union[X, Y] > X  None`. Note that in `.pyi` stub files, we do not need `from __future__ import annotations`. So this PR does not violate issue CC(Deprecate and remove usage of from __future__ import annotations in codebase):  CC(Deprecate and remove usage of from __future__ import annotations in codebase) ",2024-06-24T23:02:52Z,module: typing open source better-engineering ciflow/trunk release notes: dataloader topic: not user facing skip-pr-sanity-checks ciflow/inductor suppress-bc-linter,open,0,0,https://github.com/pytorch/pytorch/issues/129420
yi,Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in `.pyi` stub files,"  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in generated `.pyi` stub files)  CC(Use Generic TypeAlias (PEP 585) and Union Type (PEP 604) in `.pyi` stub files)  CC([BE][Easy] enable postponed annotations in `torchgen`)  CC([BE][Easy] enable postponed annotations in `tools`)   Generic TypeAlias (PEP 585): e.g. `typing.List[T] > list[T]`, `typing.Dict[KT, VT] > dict[KT, VT]`, `typing.Type[T] > type[T]`.  Union Type (PEP 604): e.g. `Union[X, Y] > X  None`. Note that in `.pyi` stub files, we do not need `from __future__ import annotations`. So this PR does not violate issue CC(Deprecate and remove usage of from __future__ import annotations in codebase):  CC(Deprecate and remove usage of from __future__ import annotations in codebase) ",2024-06-24T23:02:42Z,oncall: distributed module: typing open source better-engineering Merged Reverted ciflow/trunk release notes: distributed (c10d) topic: not user facing,closed,0,13,https://github.com/pytorch/pytorch/issues/129419, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," f ""all tests passed, stuck at unrelated ci"""," merge f ""all tests passed, stuck at unrelated ci""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","~Ugh, I fear this might break any older torch.jit code which never supported PEP585, but we will see.~ Oh nvm, it's just PYI files, that's fine.", your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: periodic / winvs2019cuda11.8py3 / test (default, 3, 4, windows.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""failed test unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,Flex Attention HOP: Add support for flex decoding, Flex Decoding tl;dr This PR adds `flex_decoding` kernel to higherorderop: `flex_attention` as the backend for multihead attention decoding.  Higherorderop `flex_attention` was introduced in (https://github.com/pytorch/pytorch/pull/121845) to accept a user defined score modification callable (`score_mod`) and through `torch.compile`to create an efficient fused flash attention kernel instatiation. The `flex_attention` kernel is efficient for long queries (>512 tokens) attention. This PR introduces `flex_decoding` kernel as an alternative backend for `flex_attention` HOP to handle LLM inference where short queries ( 16.  i.e.   Backward path for short queries (<128 token) currently does not work because the `flex_attention_backward` kernel is lacking mask support and only takes query length of a multiple of 128.   Dynamic shape and max_autotuning is currently not working  Add block sparse mask support ( CC([Inductor] FlexAttention supports block sparse mask) is a draft for flex_attention kernel)   Add explicit GQA support. ( CC(Flex_attention explicit GQA support) is a draft for GQA support on flex_attention kernel)  ,2024-06-24T21:47:39Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/129415,Works around before Triton Fix https://github.com/tritonlang/triton/pull/4061 is added to pin,Use inductor generated fused reduction kernel. Performance is similar to hand written templated kernel.  Hand written templated reduction: 1.089 > inductor generated 1.028 ,Split_kv tuning: one block per SM.   Speedups w.r.t Xformers SplitK Kernel on GQA Decoding ~1 > avg. 1.22x max 1.8x  , merge, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[no ci][FSDP2] Squash for ghimport,"  CC([no ci][FSDP2] Squash for ghimport)  CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out) ",2024-06-24T14:44:17Z,oncall: distributed release notes: distributed (fsdp2),closed,0,1,https://github.com/pytorch/pytorch/issues/129377,Still `patch does not apply` üò¢ 
finetuning,[easy][DCP] Fix test_fine_tuning.py for get/set_state_dict API changes,Update test/distributed/checkpoint/e2e/test_fine_tuning.py for https://github.com/pytorch/pytorch/pull/112203 change. ,2024-06-24T12:04:25Z,oncall: distributed triaged open source Merged ciflow/trunk topic: not user facing module: distributed_checkpoint,closed,0,2,https://github.com/pytorch/pytorch/issues/129365, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Does it still make sense to prevent saving multiple tensors that have a shared storage but different dtypes?,"Here we raise an exception if multiple tensors with a common storage have different dtypes: https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/serialization.pyL808L816 However in the related issue,  indicates that the fact that we're switching to untyped storages may resolve this issue  CC(Cannot `torch.load` tensors of different dtypes that view the same data)issuecomment848444799  Context In tensordict, we are building a method to consolidate a series of arbitrary tensors on a common device into a single storage https://github.com/pytorch/tensordict/pull/814 The storage and metadata are written in the `tensordict._consolidated` dictionary. It's easy to serialize / deserialize this but if one makes a view of the tensordict, eg  then the view lacks the `_consolidated` attribute. In that case we can't really tell pickle what to do and fall back on regular serialization methods. Saving such an object will fail because of the common storage check.  Note that we can pass that data through a `mp.Pipe`, only `torch.save` currently fails. ",2024-06-24T08:04:16Z,module: serialization triaged,open,0,7,https://github.com/pytorch/pytorch/issues/129356,"I don't particularly understand the scenario where this check is needed,  could you share more details here please","It should in principle be possible to save tensors of different dtypes that view the same data. However, in  CC(Cannot `torch.load` tensors of different dtypes that view the same data)issuecomment848435011,  suggested that we shouldn't make this change because it would not be forward compatible. I don't really understand/remember how it breaks FC though. Sure, if a user saves a set of tensors that view the same data with different dtypes, then the save file cannot be loaded by older versions of pytorch. But to give another situation that I think is analogous, if we add a new kind of `nn.Module` and a user saves an instance of it, that file cannot be loaded by older versions of pytorch either. But we wouldn't consider that FCbreaking because the new module is simply a new feature that didn't exist in older versions. I think a change only breaks serialization FC if (1) a save file generated by the new version only relies on features that the old version supports and (2) the save file cannot be loaded by the old version. I can't think of a case where both (1) and (2) would necessarily happen if we enable saving tensors that view the same data with different dtypes. If the change really would not break FC, then I'm happy to do it","We should eventually support this. The reason I told Kurt not to support it is because the ""obvious"" way to implement it isn't FC. So for FC reasons we need to do something different but ONLY when you have a mixed dtype situation.","fwiw an error if we remove this check seems to be around resizing of storages, will investigate further ","We're still investigating the error with  to see how that could be enabled. Regarding the FC compatibility issue:  was proposing the following scheme, where we   establish a window for FC to be safe  at `t=0` within that window, we upgrade `torch.load` to make it compatible with that feature  at `t=T` within that window we update `torch.save` and enable the feature to be used. That way, any PT build that is of `T` time in the past will be able to read saved tensors if these point to the same storage even when they have different dtypes. Would that be a suitable solution? What would be the appropriate window for this?    ","So, if we don't mind the format being more complicated, I'd just suggest we detect if we are the same dtype situation and do the old format, and only if we detect multidtype do we switch to the new format...","Got it! Recap:   we can patch the `load` to allow for untyped storage used with multiple tensors with different dtypes, and patch `save` subsequently. The complexity of doing so would need to be investigated as currently `save` and `load` rely on typed storages.  we could have an if/else within save and a corresponding adaptive behaviour on the `load` side to account for this use case. Long term, this behaviour could be the default for load/save (since it's more generic and doesn't rely on typed storages),  and since we'll have both options available we will be able to finely select in which case the previous or new serialization logic should be used."
finetuning,Fuyou Training Framework Integration for PyTorch," üöÄ The feature, motivation and pitch Fuyou Training Framework Integration for PyTorch Description: Integrate the Fuyou training framework into PyTorch to enable efficient finetuning of large language models (up to 175B parameters) on lowend GPUs with limited CPU memory. This framework optimizes the use of SSDCPU communication to maximize GPU utilization and support huge model finetuning on commodity hardware. Motivation and Pitch Motivation: 	1.	Accessibility: Highend GPU servers are prohibitively expensive for many academic researchers. Enabling efficient training on lowend GPUs democratizes access to cuttingedge AI research. 	2.	Efficiency: Existing solutions like ZeROInfinity are optimized for highend hardware and perform poorly on lowend systems. Fuyou addresses this gap, offering substantial improvements in GPU utilization and training efficiency. 	3.	Performance: Empirical results demonstrate Fuyou‚Äôs superior performance in finetuning large models on consumergrade GPUs, making it a valuable addition to PyTorch. Pitch: 	‚Ä¢	CostEffective: Fuyou allows researchers to finetune massive models without the need for expensive hardware, significantly reducing the cost barrier. 	‚Ä¢	High Utilization: By optimizing SSDCPUGPU communication, Fuyou ensures high GPU utilization, making the training process more efficient and faster even on lowend GPUs. 	‚Ä¢	Wider Adoption: Integrating Fuyou into PyTorch would expand its user base, particularly among researchers and developers with limited hardware resources.  Alternatives 1.	ZeROInfinity: 	‚Ä¢	Pros: Already integrated and optimized for highend GPU servers. 	‚Ä¢	Cons: Inefficient on lowend hardware, leadin",2024-06-23T12:47:31Z,feature triaged,open,0,3,https://github.com/pytorch/pytorch/issues/129330,This seems more relevant to torchtune https://github.com/pytorch/torchtune  ,"Thanks so much !  thanks for opening this issue. Have you had a chance to look at the torchtune repo? Making finetuning of LLMs on commodity hardware easy is the goal of the library, though it currently only supports 70B or smaller models.",This is a cool effort that could work well within the PyTorch ecosystem without being in core pytorch/pytorch. There may also be collaboration opportunities with torchtunewe could move this issue there if you'd like to further discuss?
transformer,[test only] TORCH_LOGS_RANKS,"  CC([test only] TORCH_LOGS_RANKS)  CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out)  CC([Traceable FSDP2] Fix support for CUDA resize_storage_bytes_)",2024-06-21T22:37:06Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/129283
transformer,fix test_fake_distributed_aot_eager,"  CC(fix test_fake_distributed_aot_eager)  CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out)  CC([Traceable FSDP2] Fix support for CUDA resize_storage_bytes_) ",2024-06-21T19:55:19Z,oncall: distributed release notes: distributed (fsdp) module: inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/129263
transformer,Release corresponding CUDA deps of `pytorch-nightly::pytorch-cuda` along with `pytorch-nightly::pytorch`," üöÄ The feature, motivation and pitch When compiling PyTorch with BuildExtension, it is common to encounter issues related to mismatched CUDA versions between different dependencies. This feature aims to address and resolve such issues efficiently. Consider the following example command to setup an environment: `conda create n dev python=3.12 pytorch torchvision torchaudio transformers scipy numpy rich ninja pytorchcuda cuda c pytorchnightly c nvidia y` In this case, the latest available version of CUDA is 12.5, and the pytorchnightly::pytorch package also aligns with this version at 12.5. However, the pytorchnightly::pytorchcuda package remains at version 12.4. This misalignment can cause compatibility issues during the build process, leading to potential failures or suboptimal performance.  Alternatives _No response_  Additional context _No response_ ",2024-06-21T11:24:53Z,module: binaries triaged enhancement needs design,open,0,4,https://github.com/pytorch/pytorch/issues/129230,"Do I understand correctly, that ask here is to build PyTorch using cuda12.5? I don't think that's the case (or the plan) right now.  If  there is an issue already that asks for more frequent CUDA updates, it should be closed as duplicate. Otherwise, let's keep this one as such issue.  IMO the goal is worthy, but in practice it's too problematic, as PyTorch might not be fully compatible with each version of CUDA released. But perhaps we can relax the restrictions, at least for the pytorchnightly channel  what do you think about it?"," , thx for your reply. I suggest to release the `nightly` version of `deps` of pytorch, so that we can perform testing withouting struggling to guess the `cuda` related environment setup. For example, if `pytorchnightly::pytorch` is built on `cuda12.5.x`, it would be best to release the `pytorchnightly::pytorchcuda` on the same `cuda12.5.x` to match with each other. Also, if u check the `pytorchnightly::pytorch` install, u will find it was already built on `cuda12.5`, see https://anaconda.org/pytorchnightly/pytorch","> For example, if pytorchnightly::pytorch is built on cuda12.5.x, it would be best to release the pytorchnightly::pytorchcuda on the same cuda12.5.x to match with each other. ius this is already the case, isn't it? I.e. `conda install c pytorchnightly c nvidia pytorch pytorchcuda=12.4` will install PyTorch built against 12.4, and if you change the version to 12.1, it will downgrade all the dependencies to 12.1, wouldn't it? > Also, if u check the pytorchnightly::pytorch install, u will find it was already built on cuda12.5, see https://anaconda.org/pytorchnightly/pytorch Where do you see 12.5 binaries? There are 11.8, 12.1 and 12.4, aren't there?","> IMO the goal is worthy, but in practice it's too problematic, as PyTorch might not be fully compatible with each version of CUDA released. But perhaps we can relax the restrictions, at least for the pytorchnightly channel PyTorch itself (i.e. the code in `upstream/main`) should always be compatible with the latest CUDA release, as we upstream needed changes and are also releasing a TOT PyTorch builds with the latest CUDA stack each month in our NGC containers.  The binary build process, however, is not trivial and is currently described here. We are working on improving this workflow as it's currently too slow for quick CUDArelated updates ( already has PRs open to unify `pytorch/builder` with `pytorch/pytorch`, which would hopefully improve the update speed). > For example, if pytorchnightly::pytorch is built on cuda12.5.x I don't see any binaries built with CUDA 12.5 and the link also does not show any. If you've found some, maybe a community member created these (the build would not be maintained by us then). > But perhaps we can relax the restrictions, at least for the pytorchnightly channel  Yes, we could try to relax some versions in the nightly binaries and might want to test these in CI in the same PR to see if any conflicts would be raised. The stable binaries might need to stick to a `MAJOR.MINOR` tag. "
transformer,model.generate(..) slow and huge GPU memory consumption," üêõ Describe the bug Hi, my setup  When calling the following code with `torch==2.3.0` or `torch==2.3.1`  it seems to load the model a second time to the GPU instead of just performing inference. Observed with `watch n 1 d nvidiasmi`. When using `torch==2.2.2` inference runs smoothly within seconds. Thank you very much and best regards  Versions ",2024-06-21T10:44:25Z,needs reproduction triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/129226,"Hey , I'm unable to run your repro code to try reproducing the problem.  If you are able to provide a runnable repro, that would help us help you  thanks!","> Hey , I'm unable to run your repro code to try reproducing the problem. >  >  >  > If you are able to provide a runnable repro, that would help us help you  thanks! Ah yes, sorry. So I was using the following model: https://huggingface.co/casperhansen/llama370binstructawq Download/clone it and adjust the path to your local model path, but I guess you know that. Or do you need it in some other form?",Just want to bring this comment to your attention which may be relevant: https://github.com/casperhansen/AutoAWQ/issues/523issuecomment2195365651,"oh, alright thank you very much   I close this issue then"
transformer,[Traceable FSDP2] Fix support for CUDA resize_storage_bytes_,"Currently if `x` is a CUDA tensor, calling `x.untyped_storage().resize_()` seems to always go into the `built without cuda` branch of `resize_storage_bytes_()` regardless of whether PyTorch is built with CUDA. I suspect this is because `inductor_ops.cpp` is only included in `libtorch_cpu.so` thus doesn't have the `USE_CUDA` information or ability to link to CUDArelated functions. This PR moves `resize_storage_bytes_()` related custom op functions out of `inductor_ops.cpp` into its standalone file `resize_storage_bytes.cpp` to be included in `libtorch_python.so` instead. This mimics the setup for `StorageMethods.cpp`. This way, `resize_storage_bytes_()` can have access to the CUDArelated functions, which passes the CUDA unit test.   CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out)  CC([Traceable FSDP2] Fix support for CUDA resize_storage_bytes_) ",2024-06-21T05:53:37Z,Merged topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129215," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,"[Traceable FSDP2][Brian's PR #128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out","This is a copy of Brian's PR https://github.com/pytorch/pytorch/pull/128754, with some changes in the test_distributed_patterns.py unit tests to more closely reflect FSDP2 patterns. Also disabled two tests `test_input_mutation_storage_resize_up_down` and `test_input_mutation_storage_resize_not_supported` in test_aotdispatch.py until we figure out the right behavior for them.   CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out) ",2024-06-21T04:25:50Z,oncall: distributed Merged release notes: distributed (fsdp) topic: not user facing module: inductor module: dynamo ciflow/inductor keep-going,closed,0,2,https://github.com/pytorch/pytorch/issues/129203," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,"Export model using onnx.dynamo_export has bug of ""torch._dynamo.exc.Unsupported: call_method TupleVariable() size [ConstantVariable(int)] {}"""," üêõ Describe the bug I am trying to use `torch.onnx.dynamo_export` to export my segmentation model to onnx.  In my model, I have a step getting the image size and resize the output to the original image size:  However, I got error complaining about unsupported operation of TupleVasriable:   I have tried to use imgs.shape as well but got the similar error. Can someone help me to export the model to onnx? Thanks!  Versions PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.29.0 Libc version: glibc2.35 Python version: 3.9.19 (main, Mar 21 2024, 17:11:28)  [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.66 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 535.161.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          GenuineIntel Model name:                         13th Gen Intel(R) Core(TM) i913900KF CPU family:                         6 Model:                              183 Thread(s) per core:         ",2024-06-21T02:51:49Z,module: onnx triaged,open,0,2,https://github.com/pytorch/pytorch/issues/129200,This is my sarif report: ,"Feel free to try torch.onnx.export(‚Ä¶, dynamo=True) with the latest torch nightly. "
transformer,[Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model,"This PR adds Traceable FSDP2 `aot_eager` backend E2E tests for simple MLP as well as transformer model.   CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out) ",2024-06-20T19:04:29Z,oncall: distributed Merged topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129157," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Regression in loading optimizer learning rate," üêõ Describe the bug Hey PyTorch team! Thanks for your hard work on the distributed checkpointing, big fan of the work! There seems to have been a regression between `2.2.2` and `2.3.0` that resulted in the optimizer learning rate being loaded incorrectly. It seems like the `lr` key in params group is not loaded from the checkpoint, resulting in the first step having the initialisation learning rate instead of the checkpointed one. This was not the case in `2.2.2` and only occurs after `2.3.0`.    debug_dcp.py    Torch 2.2.2     scratch222.txt    cont222.txt    Torch 2.3.0    scratch230.txt   cont230.txt      Versions PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 Nvidia driver version: 535.86.10 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      43 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:    ",2024-06-19T18:41:18Z,oncall: distributed triaged module: distributed_checkpoint oncall: distributed checkpointing,closed,0,5,https://github.com/pytorch/pytorch/issues/129079,Did a git bisect and found that f518cf811d8645b801de773c8d3f44fc00d9af1e is the first bad commit  ,"Narrowed down the root cause to the `tree_map_only` used by `_init_state_dict`. https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/planner_helpers.pyL297L303  Doing the `tree_map_only` outofplace and then assigning it this way only maintains references for the leaves but not the branches. Which I guess breaks some assumptions made by the reading code. Using the inplace `tree_map_only_` seems to resolve the issue, but im not sure if it breaks something else as suggested by  in the comment.   "," For now, you can use `dcp_state_dict[""optimizer""]` to ensure getting the correct loaded state_dict.  I think our test cases do not catch this use case where we access the state_dict through the original reference."," It seems like `dcp_state_dict[""optimizer""]` doesnt work either. ", I put up a fix for this. https://github.com/pytorch/pytorch/pull/129398
transformer,[inductor] don't materialize the large sparse matrix in CE bwd,"  CC([inductor] don't materialize the large sparse matrix in CE bwd) Inductor currently materialize a large sparse matrix in the backward pass for CrossEntropyLoss and load that to compute gradients of Softmax input. If we could fuse the sparse matrix computation to the consumer sides, we gonna have both perf and memory usage wins. The Fx graph snippets that construct this aforementioned sparse matrix looks like:  Leveraging the following observations:  the scatter is applied upon a all zero (or more generally a const tensor)  the index tensor for the scatter has a single element on the scatter dimension. In this case it's the label tensor allow us to lower this 'scatter_upon_const_tensor' pattern to a pointwise kernel that can be easily fused with downstream kernels:   Test result on microbenchmark For the microbenchmark added as `test_cross_entropy_loss`, we improve latency from 47.340ms to 42.768ms, memory footprint from 10.524GB to 7.227GB on A100. (on H100, we improve latency from 27.54ms to 23.51ms, memory footprint from 10.574GB to 7.354GB). The saving matches the backofenvelope calculation. We avoid storing a BF16 tensor with shape [30K, 50K] which is about 3GB in size. On A100, avoid loading and storing such a tensor can roughly save 3GB x 2 / 1.5TBGS = 4ms  Test result on llm.c We also test this on llm.c and the saving is much larger especially for memory footprint. The reason is due to autotuning that allocates extra memory for benchmarking. (Check  CC(Move Memory Allocation for Autotuning out of the critical path) and https://github.com/pytorch/pytorch/pull/129399 for more details). For llm.c PyTorch implementation on A100, w",2024-06-19T06:19:10Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/129043,What's the graph we're matching here?,> What's the graph we're matching here?  : Here is the whole backward graph https://gist.github.com/shunting314/2082ee22613848b937ef992cc8717bd7 for the microbench I run. And the matched part is: ,"Horace gave me a nice suggestion offline to pattern match only the scatter+allzero part and let inductor figure out the rest itself. It works well. This makes code much simpler and more general. And now I don't need to duplicate the patterns to make it work for llm.c (which does not have the convert element type node). For llm.c, I see larger gain then I expected. I was only expecting a saving of 6GB memory (a [30k, 50k] fp32 tensor). But the benchmark results shows the memory usage drops from 33556 MiB > 18595 MiB.  I would like to dig more on this", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[dtensor][debug] add operation tracing to comm_mode,"  CC([dtensor][be] Reduced redundant LOC by creating functions to set up models used in example)  CC([dtensor][debug] Added forward and backward differentiation for module level tracing)  CC([dtensor][debug] add operation tracing to comm_mode) **Summary** I have added an even more detailed module tracker that now includes the collective counts and operations that happen in each submodule making it easier for users to debug. The tracing now includes the operation's DTensor arguements' input shape and sharding. Like the module collective tracing, the user also has the option to log the tracing table to output.txt file. I have decided not to include the example output for transformer as it is too many lines. The expected output for the MLP_operation_tracing is shown below:   **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_operation_tracing 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing ",2024-06-19T00:21:48Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/129017,let's convert this PR to Draft since it's still WIP.,Curious of what is your decision of printing out the mesh for the DTensor., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Trying to use forward AD with _scaled_dot_product_flash_attention that does not support it because it has not been implemented yet.," üöÄ The feature, motivation and pitch As suggested in the error message, I am reporting this error so its implementation can be prioritized. Trying to use forward AD with **_scaled_dot_product_flash_attention** that does not support it because it has not been implemented yet. Trying to use forward AD with **_scaled_dot_product_flash_attention_for_cpu** that does not support it because it has not been implemented yet.  Alternatives _No response_  Additional context `torch.__version__: '2.3.1+cu121'` ",2024-06-18T16:18:26Z,module: autograd triaged actionable,open,1,1,https://github.com/pytorch/pytorch/issues/128971, 
llm,RuntimeError: NCCL error: invalid usage when deploy LLM model by vllm. (torch version: 2.3.0+cu118)," üêõ Describe the bug 1„ÄÅ torch 2.3.0+cu118 Requires: filelock, fsspec, jinja2, networkx, nvidiacublascu11, nvidiacudacupticu11, nvidiacudanvrtccu11, nvidiacudaruntimecu11, nvidiacudnncu11, nvidiacufftcu11, nvidiacurandcu11, nvidiacusolvercu11, nvidiacusparsecu11, nvidiancclcu11, nvidianvtxcu11, sympy, triton, typingextensions 2„ÄÅ the version of nccl in environment is ""nvidiancclcu11  2.20.5"" 3„ÄÅ when deploy LLM model by vllm, there have Errors  as follows: 20240618 22:03:51 grep nccl nvidiancclcu11                  2.20.5 ",2024-06-18T15:03:45Z,oncall: distributed triaged,open,0,1,https://github.com/pytorch/pytorch/issues/128963,  You could rerun the script with export NCCL_DEBUG=INFO and check the logs for NCCL errors.
yi,[MAC] Convolution with kernel size 3 yields different results depending on whether gradient is enabled or not., üêõ Describe the bug   Versions [pip3] numpy==1.26.4 [pip3] torch==2.5.0.dev20240617 [pip3] torchaudio==2.4.0.dev20240617 [pip3] torchvision==0.19.0.dev20240617 [conda] nomkl                     1.0                  h5ca1d4c_0    condaforge [conda] numpy                     1.26.4          py312h8442bc7_0    condaforge [conda] pytorch                   2.5.0.dev20240617        py3.12_0    pytorchnightly [conda] torchaudio                2.4.0.dev20240617       py312_cpu    pytorchnightly [conda] torchvision               0.19.0.dev20240617       py312_cpu    pytorchnightly ,2024-06-18T11:26:07Z,module: convolution triaged module: macos,closed,0,5,https://github.com/pytorch/pytorch/issues/128945,"Hmm, the differences I'm seeing are in range of 1e7, but it is a bit strange that they are there to begin with. I.e.  indeed produces a nonzero tensor",The range changes depending on the input data. For the data I'm working with I get 1e5 but for these random minimal examples I get 1e7 and 1e8 values.,"Hii   and  . I am also a macOS user and experienced the same things, I further experimented with and found out two things:) 1) I experiment from 1 to 5 kernel_size and found out 3 is the only problem. Code:) *** import torch as tt import torch.nn as nn  Random image image = tt.randn(1, 1, 512, 512)  Conv2d with kernel size of 1 results in different output if conv_layer has gradients enabled or not conv_layer_1 = nn.Conv2d(1, 1, 1) test_1_a = conv_layer_1(image) conv_layer_1.requires_grad_(False) test_1_b = conv_layer_1(image) print(tt.equal(test_1_a, test_1_b))   True  Conv2d with kernel size of 2 results in different output if conv_layer has gradients enabled or not conv_layer_2 = nn.Conv2d(1, 2, 2) test_2_a = conv_layer_2(image) conv_layer_2.requires_grad_(False) test_2_b = conv_layer_2(image) print(tt.equal(test_2_a, test_2_b))   True  Conv2d with kernel size of 3 results in different output if conv_layer has gradients enabled or not conv_layer_3 = nn.Conv2d(1, 3, 3) test_3_a = conv_layer_3(image) conv_layer_3.requires_grad_(False) test_3_b = conv_layer_3(image) print(tt.equal(test_3_a, test_3_b))  False. The differences are between 1e5 and 1e8  Conv2d with kernel size of 4 results in different output if conv_layer has gradients enabled or not conv_layer_4 = nn.Conv2d(1, 4, 4) test_4_a = conv_layer_4(image) conv_layer_4.requires_grad_(False) test_4_b = conv_layer_4(image) print(tt.equal(test_4_a, test_4_b))   True  Conv2d with kernel size of 5 does not show the same behavior conv_layer_5 = nn.Conv2d(1, 5, 5) test_5_a = conv_layer_5(image) conv_layer_5.requires_grad_(False) test_5_b = conv_layer_5(image) print(tt.equal(test_5_a, test_5_b))  True *** Output: *** True True False True True ***  2. Second thing I noticed is when I keep the precision to float64, then all are True. *** Code: import torch as tt import torch.nn as nn  Random image image = tt.randn(1, 1, 512, 512).type(tt.float64)  Conv2d with kernel size of 1 results in different output if conv_layer has gradients enabled or not conv_layer_1 = nn.Conv2d(1, 1, 1,dtype=tt.float64) test_1_a = conv_layer_1(image) conv_layer_1.requires_grad_(False) test_1_b = conv_layer_1(image) print(tt.equal(test_1_a, test_1_b))   True  Conv2d with kernel size of 2 results in different output if conv_layer has gradients enabled or not conv_layer_2 = nn.Conv2d(1, 2, 2,dtype=tt.float64) test_2_a = conv_layer_2(image) conv_layer_2.requires_grad_(False) test_2_b = conv_layer_2(image) print(tt.equal(test_2_a, test_2_b))   True  Conv2d with kernel size of 3 results in different output if conv_layer has gradients enabled or not conv_layer_3 = nn.Conv2d(1, 3, 3,dtype=tt.float64) test_3_a = conv_layer_3(image) conv_layer_3.requires_grad_(False) test_3_b = conv_layer_3(image) print(tt.equal(test_3_a, test_3_b))  False. The differences are between 1e5 and 1e8  Conv2d with kernel size of 4 results in different output if conv_layer has gradients enabled or not conv_layer_4 = nn.Conv2d(1, 4, 4,dtype=tt.float64) test_4_a = conv_layer_4(image) conv_layer_4.requires_grad_(False) test_4_b = conv_layer_4(image) print(tt.equal(test_4_a, test_4_b))   True  Conv2d with kernel size of 5 does not show the same behavior conv_layer_5 = nn.Conv2d(1, 3, 5,dtype=tt.float64) test_5_a = conv_layer_5(image) conv_layer_5.requires_grad_(False) test_5_b = conv_layer_5(image) print(tt.equal(test_5_a, test_5_b))  True *** Output: True True True True True ","Following discussion with the group: different kernels could be used depending on whether grad is turned on or off, so minor numerical discrepancies are too be expected. If you swap out the torch.equal calls to torch.allclose, more of these should become true.","Closing this issue as expected behavior, but feel free to call out if these discrepancies are more than what is expected from https://pytorch.org/docs/stable/notes/numerical_accuracy.htmlnumericalaccuracy"
transformer,[inductor][cpu]transformers models static/dynamic quant performance/accuracy crash in 2024-06-17 nightly release, üêõ Describe the bug   Versions SW info               SW       Branch       Target commit       Refer commit                       Pytorch       nightly       8410bf5       963d450                 Torchbench       chuanqiw/inductor_quant       ee35d764       ee35d764                 torchaudio       nightly       b829e93       1980f8a                 torchtext       nightly       b0ebddc       b0ebddc                 torchvision       nightly       d23a6e1       d23a6e1                 torchdata       nightly       11bb5b8       11bb5b8                 dynamo_benchmarks       nightly       fea73cb       fea73cb           Repro:  Suspected guilty commit: https://github.com/pytorch/pytorch/commit/2229884102ac95c9dda0aeadbded1b04295d892e textclassification+albertbasev1staticquantaccuracycrash_guilty_commit.log ,2024-06-18T06:03:53Z,oncall: pt2 module: dynamic shapes oncall: cpu inductor,open,0,8,https://github.com/pytorch/pytorch/issues/128933,"Hi , could you kindly help to take a look? Prepare the script to reproduce this issue: https://gist.github.com/lesliefangintel/696041fa7e7352ecb985b04a5e1188de and it starts to fail since https://github.com/pytorch/pytorch/commit/2229884102ac95c9dda0aeadbded1b04295d892e Here are the version of transformer I used `pip install ""git+https://github.com/huggingface/transformers""` in case needed.",vision_maskrcnn and detectron2_fcos_r_50_fpn AMP/float32 single/multiple thread static/dynamic shape default/cpp wrapper  meet `TypeError: Invalid NaN comparison` https://gist.github.com/zxd1997066/5f1fc727ced62f4ae82df88ea232f863 And they have the same guilty commit https://github.com/pytorch/pytorch/commit/2229884102ac95c9dda0aeadbded1b04295d892e bisect log: torchbenchvision_maskrcnninferencefloat32staticdefaultmultipleaccuracycrash_guilty_commit.log Repro: inductor_single_run.sh ,"Running this test with `TORCH_LOGS=""+dynamic""` We can find the guard difference before and after this commit:  Previously, we can statically known `s0 != 9223372036854775807`  However, after this commit, we have to add the guard which causes the failure. ","Further looking into the why we can't statically known `s0 != 9223372036854775807` after this commit:  Before regression    Here the upper of `vr` is `9223372036854775806` and `offset` is `1` which make `add` returning `9223372036854775805`      https://github.com/pytorch/pytorch/blob/cac6f99d41baa8418e6b31834eb7829257acb24c/torch/fx/experimental/symbolic_shapes.pyL4533     Here comparing `b.lower` `9223372036854775807` and `a.upper` `9223372036854775806`, we can known they are not equal statically .      https://github.com/pytorch/pytorch/blob/17d1723aeeadbfc6d33c02ab56c5aacb8c671876/torch/utils/_sympy/value_ranges.pyL468   After regression:    Here the upper of `vr` is `int_oo` and `offset` is `1` which make `add` returning `int_oo`        https://github.com/pytorch/pytorch/blob/cac6f99d41baa8418e6b31834eb7829257acb24c/torch/fx/experimental/symbolic_shapes.pyL4533     Here comparing `b.lower` `9223372036854775807` and `a.upper` `int_oo`, we can't known they are not equal statically.      https://github.com/pytorch/pytorch/blob/17d1723aeeadbfc6d33c02ab56c5aacb8c671876/torch/utils/_sympy/value_ranges.pyL468 ","Not sure how to make the correct fix. If `b.lower` is larger than `sys.maxsize1` and `a.upper` is `int_oo`, can we say they are not equal in `SymPyValueRangeAnalysis`? ","But that guard sounds reasonable to me, no? It's asking that `s0` should be representable in `int64`. I'm not sure how the points above are related to the failure, and it's difficult to know without more context. Looking at the error in  CC([inductor][cpu]transformers models static/dynamic quant performance/accuracy crash in 2024-06-17 nightly release)issuecomment2179004620, it might suggest that our `safe_mul` is not as safe as it should be. In particular, it might be doing something like `0 * sympy.oo` and it's returning a `NaN`. In that case, we should probably treat in that operation `0 * sympy_oo` (and same with `sympy_oo`) as 0, as this formula is equivalent to the limit `lim_{x>inf} 0 * x = 0`.  this shows a larger issue that's lurking with the inf treatment: Our bounds are inclusive... unless one of the ends is `oo`, in which case they are not...","> But that guard sounds reasonable to me, no? It's asking that s0 should be representable in int64. I'm not sure how the points above are related to the failure, and it's difficult to know without more context. Yean, any suggestions for how to further debug why the guard failed? I am just listing out the difference before and after this commit and maybe there is another potential issue which fails the guard :(  Update for why the new added guard fail   By previous debug, we will create a new guard as `Ne(s0, 9223372036854775807)`  And I see we will evaluate this again here which returns result of None. It actually hit the lru_cache, but I think it will follow same analysis in  CC([inductor][cpu]transformers models static/dynamic quant performance/accuracy crash in 2024-06-17 nightly release)issuecomment2184408260 even it didn't hit the lru_cache.    https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/fx/experimental/symbolic_shapes.pyL4107    Then we `record_constraint_violation` in `constraint_violations` here since there is a constraint with instance of `StrictMinMaxConstraint`        https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/fx/experimental/symbolic_shapes.pyL4090  When `constraint_violations` is not empty, we raise the error here https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/fx/experimental/symbolic_shapes.pyL4180","This is sort of expected, but what we probably can do is make the constraint violation error more tolerant for this case. The big question I had to answer in https://github.com/pytorch/pytorch/pull/127693/ was what I should do if there legitimately was different behavior when s0 == sys.maxsize.  Previously, I simply assumed this couldn't happen, because who makes sys.maxsize type tensors. But with int_oo modeling, ""just assuming"" it doesn't happen is not so convenient. But it's also not a big deal, you just get a guard testing that the int is not maxsize, nbd. Except for the constraint stuff. The constraint violation says ""if there is ANY guard, error out"". But we can probably make it softer, e.g., a guard that the value is not maxsize shouldn't trigger this."
gemma,[reland][ROCm] TunableOp for gemm_and_bias,"Reland of CC([ROCm] TunableOp for gemm_and_bias) but added `alpha` and `bias` initialization to `launchTunableGemmAndBias` Thus far TunableOp was implemented for gemm, bgemm, and scaled_mm. gemm_and_bias was notably missing. This PR closes that gap. ",2024-06-18T01:53:39Z,module: rocm triaged open source Merged ciflow/trunk release notes: linalg_frontend rocm rocm priority keep-going ciflow/rocm,closed,0,13,https://github.com/pytorch/pytorch/issues/128919, Can you please review and approve if this PR looks good?,Hi   appreciate your help to get the PR reviewed., I think  is OOO. Can you please approve and merge this PR?, reping,"/: I see a unit test was added for this PR: `test_addmm_relu_tunableop_rocm` in `test_linalg.py`. I have added the `keepgoing` label to this PR to ensure if runs all unit tests, since we have some unrelated failures on ROCm CI that might prevent that unit test from running in CI. Can you please post a snippet showing that the unit tests related to this PR ran successfully in the PR's CI runs?", merge i," Merge started Your change will be merged while ignoring the following 10 checks: pull / linuxfocalpy3.12clang10 / test (dynamo, 1, 3, amz2023.linux.2xlarge), pull / linuxfocalpy3.12clang10 / test (dynamo, 3, 3, amz2023.linux.2xlarge), pull / linuxfocalpy3.12clang10experimentalsplitbuild / test (dynamo, 1, 3, amz2023.linux.2xlarge), pull / linuxfocalpy3.12clang10experimentalsplitbuild / test (dynamo, 3, 3, amz2023.linux.2xlarge), pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 3, amz2023.linux.2xlarge), pull / linuxfocalpy3.8clang10 / test (dynamo, 3, 3, amz2023.linux.2xlarge), pull / linuxfocalpy3.11clang10 / test (dynamo, 1, 3, amz2023.linux.2xlarge), pull / linuxfocalpy3.11clang10 / test (dynamo, 2, 3, amz2023.linux.2xlarge), rocm / linuxfocalrocm6.1py3.8 / test (default, 4, 6, linux.rocm.gpu.2), rocm / linuxfocalrocm6.1py3.8 / test (default, 5, 6, linux.rocm.gpu.2) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / test (default, 3, 3, windows.4xlarge.nonephemeral) Details for Dev Infra team Raised by workflow job ", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / test (default, 3, 3, macosm1stable) Details for Dev Infra team Raised by workflow job "," merge f ""unrelated macos cpu job failed, all other CI is known flaky or passing"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Unable to export Phi-3-vision model to exported program," üêõ Describe the bug Repro:  Error message:   Versions Collecting environment information... PyTorch version: 2.4.0.dev20240412+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4080 Nvidia driver version: 535.161.08 cuDNN version: Probably one of the following: /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_ops_train.so.8.9.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                   ",2024-06-17T22:45:52Z,module: dynamic shapes oncall: export,open,1,7,https://github.com/pytorch/pytorch/issues/128906,"To fix `RuntimeError: shape '[1, s0]' is invalid for input of size 2`, could you try the following code? It seems the repro code uses a wrong order of inputs.  After using correct `args/kwargs`, I found another error `torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from datadependent expression u0 (unhinted: u0).  (Sizelike symbols: u0).` It comes from `if len(positions.tolist()) > 0: ...`. One fix could be specializing on one branch by updating the code. Full trace for new error: ", Thanks for your suggestion. I tried the code snippet you provided and got the exactly same error as yours. Do you have any ideas to fix it?, Do you have any updates for the bug?,"> It comes from if len(positions.tolist()) > 0: .... One fix could be specializing on one branch by updating the code. I removed `if len(positions.tolist()) > 0:` as  suggested, but unfortunately, this fix doesn't work for me. Here's the new error:  ",Using strict export will fix the problem. You can also rewrite the the `sub_img[:B_]` to use narrow instead which will avoid the specialization forced by Python slice," Thanks for the reply. I tried `strict=True` and `sub_img = sub_img.narrow(0, 0, B_)` and it works for this line, but afterwards, when hitting `sub_img.reshape(B_,H,H,C)`, got the error:  It seems there's a similar open issue here:  CC(Torch.onnx.dynamo_export stuck at reshape)issuecomment1751919505 Do you have any idea for this?",You should read https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/editheading=h.44gwi83jepaj This one looks like it would work if u1 was marked as size like. Find where you did the item() call and torch._check_is_size the result
gemma,"Back out ""[ROCm] TunableOp for gemm_and_bias (#128143)""","Summary: Original commit changeset: 35083f04fdae Original Phabricator Diff: D58501726 This PR is bringing a large numerical gap. e.g. for 256 x 4096 x 4096 GEMM, if we enable tunable op + DISABLE_ADDMM_HIP_LT=0, the results are way off. Differential Revision: D58660832 ",2024-06-17T08:31:02Z,module: rocm fb-exported Merged ciflow/trunk topic: not user facing ciflow/rocm,closed,0,12,https://github.com/pytorch/pytorch/issues/128815,This pull request was **exported** from Phabricator. Differential Revision: D58660832, merge (Initiating merge automatically since Phabricator Diff has merged)," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", any chance you can accept this backout? ,">  any chance you can accept this backout? No objections from our end, I don't believe this should affect cuBLAS/cuBLASLt",thanks! Yes this shouldn't have any impact on the cuda path (tunable op is only for rocm). , merge," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,[ROCm] Tunableop record untuned,"When enable tunableop, It is easy to have OOM since APP usually needs large video memory size, such as running a LLM for inference.  So we need a offline mode to tune the GEMMs. This PR provide an offline mode for tunableOp:  record untuned GEMMs to file.  a python API named tune_gemm_in_file is added to read the untuned file and tune the GEMMs in file ",2024-06-17T06:29:40Z,module: rocm triaged open source Merged ciflow/trunk release notes: linalg_frontend rocm rocm priority ciflow/rocm,closed,0,29,https://github.com/pytorch/pytorch/issues/128813,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: jfactory07 / name: Jin Zhou  (adb4a8b94b6b7306afb5d9364dbaef92a9f3a4c7, ae32d63a62539cde581d8d046d12900193dec0b3, 1bc4abf67aea57313dcee3e6256d9cca29b706e5, 49648b890166e3fd3c07f47dd5794d243ea259d2, 40aef0a0c1f43994604d7f0957c48db378fd564f, bbf6894e9277717c1354679c5e32a54e40338270, 8df9ff04ab8f05a1f23bbd97e5c3d6ea67329552, eb815ff9dc3b5e2c257430c0a696a214db6ada91, 02b5fc2848ff0435472dc433b633280223977f1e, 996bd88bdc1fff50686c6a94be5430277b7dc96f, f3f2045bee24b5e3b5d203d93baf377de92b0f20, 762050146e880280acba76adc93a7c17ad2cf1b9, 82b6bb6c3f2b985b617f3855226319df856dab9a, fcd18e9d5efc9daa7af71b4997e32edb7a371608, c547971206f2409cac394765f783189de458ba33, 8bcf8775941b48777242ecbfe87655e1dd535c39, f06dd089962483b1a5e5b95888e8551ad4e4bbdb, 3effc32667a61d01c3219a9b828dd8020c24fa3c, ffef56d00edc7f5d79bc51b92e472892caebd091, cbb5d3294aaada21124f33c767013a71cc6e76fd, c257d369f038a7efd11617ffd1810bed21fa26a2, 78e6d11cac3b78b376e2ca61a9dd7027b81b8c38, 5d96d63359043e4edc7a4c0dba1553b544dec8bc, 78d7301ce3da76275e4d717c87b9407a47054542, 15f493e0083a81d0a4bf0e7fc58d3389c536da12, 7951f675d056ff26161daf0e1923131ffc23ee9c, 6e6961759a377d9e7831d6ae67661886f9052716, 314a1e600c57d2d535c0e94c86471d3f28069d44, a432b81d9fac4cea89a2f7d8f450b9b5858b91fe, f4e5399d0380331d92747fb43d89faa2c09b2887)","Some nice work .   A couple of comments:  You do need to sign the LFX: EasyCLA (see the button above). You need to sign the individual contributor version, since AMD doesn't have corporate agreement.  It would be desirable to have a unit test as part of this PR since its a new feature.  Please address the failures that are reported. Thanks","> Some nice work . >  > A couple of comments: >  > * You do need to sign the LFX: EasyCLA (see the button above). You need to sign the individual contributor version, since AMD doesn't have corporate agreement. > * It would be desirable to have a unit test as part of this PR since its a new feature. > * Please address the failures that are reported. >  > Thanks I will add a unit test.","Thanks  for the most recent commits. As I was testing the PR myself,  I realized that it may not be obvious how to use offline tuning to the average user. It would be helpful to users if you supplement with some more documentation in the README.md. I would suggest a section called ""Offline Tuning"" and include:    Motivation for offline tuning. Basically it is used for workload with highmemory utilization where one might run out of memory with regular tuning.  Description of the workflow for Offline tuning. There are basically two steps: 1) Set the environment variables to collect the untuned GEMM and this will generate `tunableop_untuned?.csv` files 2) Run a Python script that reads the `tunableop_untuned?.csv` and generates the `tunableop_results?.csv`. It would be desirable if documentation covered the multiGPU case which would be of great interest.","> Thanks  for the most recent commits. >  > As I was testing the PR myself, I realized that it may not be obvious how to use offline tuning to the average user. It would be helpful to users if you supplement with some more documentation in the README.md. I would suggest a section called ""Offline Tuning"" and include: >  >     * Motivation for offline tuning. Basically it is used for workload with highmemory utilization where one might run out of memory with regular tuning. >  >     * Description of the workflow for Offline tuning. There are basically two steps: 1) Set the environment variables to collect the untuned GEMM and this will generate `tunableop_untuned?.csv` files 2) Run a Python script that reads the `tunableop_untuned?.csv` and generates the `tunableop_results?.csv`. It would be desirable if documentation covered the multiGPU case which would be of great interest.   I have added the section. For multiGPU, I think the original document doesn't have description about it, i.e. each `tunableop_results?.csv` for each GPU. The new interface (tune_gemm_in_file) I add doesn't consider multiGPU, it should always tune GEMM on GPU0  ", Thanks for your most recent set of changes. I think it is OK if we do *not* have full support for the multiGPU case in this PR. We can do a followup PR later to enable full multiGPU support. I can lead that since it is more pressing for my use case. Let us focus on getting an initial version of offline tuning support in this PR approved and merged in., Can you please take a look at the CI failures? Start with the lint failures as those will be the easiest ones to fix., Moving to Draft until CI failures are resolved,amd I think the CI is clean now., Please review ,">  Please review amd lint failures seems relevant, aren't they? (as entries to `__init__.pyi.in` are missing in the PR)","> >  Please review >  > amd lint failures seems relevant, aren't they? (as entries to `__init__.pyi.in` are missing in the PR)  The linting failure has been resolved. Could you please continue the review?", Pinging again about this PR. , rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/128813/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/10968616506,"I can confirm that the new UT ran successfully in `rocm / linuxfocalrocm6.2py3.10 / test (default, 3, 6, linux.rocm.gpu.2)` ", No CI failures now. This PR is ready to be rereviewed., A gentle reminder to rereview this PR., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / Test `run_test.py` is usable without boto3/rockset Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/128813/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/11228573236,I can confirm that the new test ran in the 5th shard: , drci," merge f ""only failure is known flaky"""
transformer,`RuntimeError: invalid dtype for bias - should match query's dtype` when using torch.compile + FSDP + hf transformer," üêõ Describe the bug I was trying to use torch.compile + FSDP + huggingface transformer. I was able to make it work on one GPU, however, on 8 A100 GPUs, I ran into the following errors. I made a reproduce repo here: https://github.com/ByronHsu/torchcompilefsdperror.   Versions  ",2024-06-16T23:12:25Z,oncall: distributed triaged module: fsdp oncall: pt2 pt2d-triage-nov2024,open,2,3,https://github.com/pytorch/pytorch/issues/128798, ,Does anyone solved it?,"I got the same error on `GH200` + `HF trainer` + `compile` when training on **FP16** or **BF16**. **Container**: `nvcr.io/nvidia/pytorch:24.08py3`  Without model compile, it is all alright."
gpt,Remove dtype from gpt-fast micro benchmark experiments model name,"Per comments on https://github.com/pytorch/testinfra/pull/5344, we already have a dtype column with the same information",2024-06-16T06:04:53Z,Merged ciflow/trunk topic: not user facing test-config/default ciflow/inductor-micro-benchmark test-config/inductor-micro-benchmark,closed,0,7,https://github.com/pytorch/pytorch/issues/128789, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , merge f 'Bypass ROCm queue',"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
gpt,[GPT-benchmark] Fix memory bandwidth for MoE,  CC([GPTbenchmark] Fix memory bandwidth for MoE)  CC([GPTbenchmark] Add metric: compilation time for GPT models),2024-06-16T01:30:43Z,Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/128783, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / test (default, 3, 3, windows.4xlarge.nonephemeral) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,[GPT-benchmark] Add metric: compilation time for GPT models,  CC([GPTbenchmark] Fix memory bandwidth for MoE)  CC([GPTbenchmark] Add metric: compilation time for GPT models),2024-06-15T05:49:16Z,Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/128768
rag,"[ROCm] TunableOp hotfix, do not use static storage for signatures",PR CC([ROCm] TunableOp for gemm_and_bias) introduced a bug when trying to optimize the overhead of TunableOp string signatures.  This PR is a hotfix that switches from using static storage to c10::call_once. ,2024-06-15T04:34:29Z,module: rocm triaged open source ciflow/rocm,closed,0,4,https://github.com/pytorch/pytorch/issues/128766,We're observing numerical issues with tunable op. Is this PR related? ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",Let's add unit test to tunable op for sure. Otherwise it's too dangerous to use in prod. ,PR https://github.com/pytorch/pytorch/pull/128143 was reverted.  Hotfix no longer needed; will fix as part of reland.
transformer,Change index_put on GPU to accept FP8 inputs,"As the title says, this PR changes the dispatcher for the CUDA index_put_ kernel to accept FP8 inputs. This is useful for Transformers models where the KV cache is FP8 and has been preallocated. ",2024-06-14T23:55:20Z,module: cpu triaged open source Merged ciflow/trunk release notes: cuda,closed,0,15,https://github.com/pytorch/pytorch/issues/128758,"Thanks, looks good after CI passes! Can we also add a test to test/quantization/core/experimental/test_float8.py?","> Thanks, looks good after CI passes! Can we also add a test to test/quantization/core/experimental/test_float8.py? actually, sorry, even better would be to add testing to `test/test_torch.py` instead of `test/quantization/core/experimental/test_float8.py`.  We should move the float8 tests in `test/quantization/core/experimental/test_float8.py` as well in a future PR.","ok, I'll add the test and check what's going on with CI tests!","added the test, but seems like CI is currently really broken...", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `index_put_fp8` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout index_put_fp8 && git pull rebase`)"," merge f ""unrelated test failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge i ""unrelated test failures""", merge i," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Fix Storage.filename to not track the filename when storage was mmap-ed with MAP_PRIVATE,  CC(Fix Storage.filename to not track the filename when storage was mmaped with MAP_PRIVATE) ,2024-06-14T18:12:54Z,Merged ciflow/trunk release notes: python_frontend topic: improvements,closed,0,8,https://github.com/pytorch/pytorch/issues/128725, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8clang10onnx / test (default, 2, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge i ""inductor huber loss error unrelated"""," merge I ""inductor huber loss error unrelated""", help, merge i," Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxfocalpy3.8clang10onnx / test (default, 2, 2, linux.2xlarge) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llama,Add RMS Norm layer," üöÄ The feature, motivation and pitch Hey team, i love building things from scratch, and as i was implementing the LLaMa paper by meta obviously using pytorch i saw that pytorch did not have a nn.rmsnorm function for RMS Normalization layer . I did end up implementing it on my own, but i still feel like RMS Normalization being used in mutliple other new language models, LLaMa being the most popler one, needs to be implemented in Pytorch and should be a part of it.   Alternatives There are many other normalization layer functions already in pytorch, but as results show, RMS Norm has been really impactful in the success of LLaMa, so i think the community might benefit with it being in pytorch  Additional context Also, ive been using pytorch for a long time and ive always build things from scratch, and it has always been on my todo list to contribute to pytorch, i feel like this might be it.",2024-06-14T16:26:57Z,triaged module: norms and normalization,open,0,2,https://github.com/pytorch/pytorch/issues/128713,"Related:   CC(upstream `apex.normalization.FusedRMSNorm`) I think, this layer was recently added in PyTorch, but not in a fused/optimized version yet"," Oh gotcha, maybe ill read through that thread and work on the optimization"
llama,Add Swiglu activation function," üöÄ The feature, motivation and pitch Hey team, i love building things from scratch, and as i was implementing the LLaMa paper by meta obviously using pytorch i saw that pytorch did not have a nn.swiglu activation function. I did end up implementing it on my own, but i still feel like swiglu being used in mutliple other new language models, LLaMa being the most popler one, needs to be implemented in Pytorch and should be a part of it.   Alternatives There are many other activation functions already in pytorch, but as results show, Swiglu has been really impactful in the success of LLaMa, so i think the community might benefit with it being in pytorch  Additional context Also, ive been using pytorch for a long time and ive always build things from scratch, and it has always been on my todo list to contribute to pytorch, i feel like this might be it. ",2024-06-14T16:24:41Z,module: nn triaged actionable,open,4,11,https://github.com/pytorch/pytorch/issues/128712,Should i start working on it?,From nn triage: feel free to send a PR that implements this,"sorry i'm quite new to contributing here, but is okay for me to pick this up? Just done setting up my environment and adding some kernel stubs."," Hey, I'm still working on this, a little busy with some work but planning to get this going in a few days, feel free to go ahead with your implementation as well, would be great to compare and find the best solution ",Sounds good. thanks for the quick response :),Any update on this?,"Been quite busy with some other stuff, but yes still working on this",Hope to see it soon!," I was just getting started on the PR for this, wanted to clear up a few things beforehand. I have the python implementation of swiglu ready, setup the class and methods for swiglu which I'm planning to add to nn/modules/activation.py and for the forward method call nn/functional.py in which I'll add the swiglu function. Also run some tests and update docs for this in my PR. Does that sound like a good start?. Also was wondering that would the final implementation of this have to be in CPP? Because as I was going through the other activations in Pytorch and I see the aten implementations for them, let me know if we'd require to setup that as well.","   Been a week, just wanted to clear this up so that I can implement and send the PR in for review ","Hi, sorry for the delayed response. At a glance, what you mentioned sounds good.  Unless you have a fused kernel that you would like to register, a Python implementation sounds like a good step"
llm,does FSDP support AMSP (a new DP shard strategy)," üöÄ The feature, motivation and pitch there's a new DP shard strategy which is more flexible and general, see more detail at https://arxiv.org/abs/2311.00257 AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training Does FSDP support similar feature? If not, any plan to support it? thanks.  Alternatives _No response_  Additional context _No response_ ",2024-06-14T15:28:20Z,oncall: distributed triaged,open,0,2,https://github.com/pytorch/pytorch/issues/128706,"I do not think FSDP supports this currently. In my high level understanding, the flexibility introduced in AMSP is mainly useful when doing microbatching / gradient accumulation?","My understanding is that the flexibility comes from the new solution that the sharding strategy for parameter, gradient and optimizer states can be different. It by nature provides many sharding strategies, including DDP, ZeRO1, ZeRO2, ZeRO3, HSDP and MiCS and many more. With a given cluster and a given model, we may find a better sharding strategy, such as the table iv in the paper, also copy below. !image another thing is that the sharding strategy is represented in two dims, one for node, the other for gpu in one node, it is more clear. It is general because all these sharding strategies can be obtained by just changing the values of the configuration. We can even loose some constrains in the paper with the key idea from the paper, if possible."
llm,Flex Decoding," Flex Decoding tl;dr This PR adds `flex_decoding` kernel to higherorderop: `flex_attention` as the backend for handling multihead attention for short queries.  Higherorderop `flex_attention` was introduced in (https://github.com/pytorch/pytorch/pull/121845) to accept a user defined score modification callable (`score_mod`) and through `torch.compile`to create an efficient fused flash attention kernel instatiation. The `flex_attention` kernel is very efficient when input query is relatively long (>64 tokens) and batch size is relatively large but struggles with short queries used during decoding (LLM inference). This PR adds `flex_decoding` kernel as an alternative backend kernel for `flex_attention` HOP to handle short query efficiently, while maintaining the same user interface API and `score_mod` flexibility.  LLM decoding iteratively attends each newly generated token ( query length = 1 ) to a long key/value context (up to 132k). `flex_attention` only parallelizes attention along query length and batch size dimension, resulting in low GPU occupancy during decoding. `flex_decoding` adds parallelization along key/value sequence length by spliting key/value to multiple CTAs to improve occupancy. The bottleneck of `flex_decoding` becomes HBM bandwidth for loading key/value cache after sufficient parallelization.   Numeric Accuracy `flex_decoding` kernel has the same numerical accuracy as xformers split_k memory efficient attention kernel. It does suffer from some accuracy loss compared to `flex_attention` eager implementation in all data types (`fp32`, `fp16`, `bf16`). This is likely due to online softmax and output reduction. Note that i",2024-06-14T02:02:42Z,module: inductor ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/128678,"  :white_check_mark: login: joydddd / name: Joy Juechu Dong  (220e455cbd031d50460f0188ba8f1b9404a89f4b, 51e487df579b245cf6e39540e7dbf9290d785fd5, 1890ef9bb19394d590ab1286cd1c8f543cbe7a1c, f157c71346e909493310fc10c2a459de31418d37, 3494f5cf4b8557f6923051f61fde3cdb938e35f2, df4aeaa6addcbf4a3aed891a4d97c5d3b863dffb, 30d80d7ce5b653ca4f957273b0500d59d7e16804, 74abaf13bcf86d5d23a1f4f2b48dd2b354747bf0, 354b5fb580d1c0153c09c975ddbcc3fce8372095, 6e7d3c36313f27758c7340db422a7b1073abc69d, 332fa3b81ff0749a1ea231138c9227cce6bf465c, a49292815f5d811798b96a06942ca5fd879f6d5b, 47d86948e9614fe30a792228dec37646be61565e, 843ddfbc6650d97b4359d4120195598535a878aa, f7bf0fd8df0489b2b0095e63c2cd3c59580f9e07, 9478a466cfb85fe95600fa81e3b7001690437474, 0b979e6e4c48dba14594dd603a09b414058cbfff, 8d08e7a12d29d5ef8670f3eee99831af8a172a14, 8301ce374aed91a33612b60f9ae03324133c5d19, 72c5d483c3b2ef3105c152b3a5609e8514c0a2a0, e82d8cc06604f94a0921c6fe0a656bb3f937d091, 1630b16500be897a9f6353c02b4dc3860e26805d, 555c41c838ec44668d484f6528b8ba4fdb8dbb66, 1f5e6e9f5e8b302d56ff5cc26d4b131b1591db60, 947a1e0237bf0d089ec6f716546139d4018d023c, 9c625fd0599dd193e5e0796e2601ddb08f8eea3a, 731e92a7babb84bc06422ebb867d2da7df5e6a44, 7a688ff620b9f7cd3a21e1fc01f8266367c00100) :x: The email address for the commit (2809fb1c2e540382c89031ef3ac12a8efbc542d6, 436cdc14d7868a09b95ae4667f5b0d1f68fff7ca) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."
llama,Use mv kernel for small M,"  CC(Use mv kernel for small M) Previously we are using: * mv kernel for M == 1 * mm kernel for 1 = 4 This PR consolidate it to only 2 kernels, use the same mv kernel for M < 12. Benchmarked on https://github.com/malfet/llm_experiments/blob/main/metalperf/int8mm.mm Mac M1 Max, input size M x 4128 x 4096 !llama cpp shader and ATen shader (2)",2024-06-13T18:20:22Z,Merged ciflow/trunk topic: not user facing release notes: mps ciflow/mps,closed,0,5,https://github.com/pytorch/pytorch/issues/128632, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,jacrev and jacfwd yield different results if one uses torch.no_grad blocks in module," üêõ Describe the bug When running a model through `torch.func.functional_call` that contains a `with torch.no_grad` block, you get different results when computing the jacobian with forward/reverse mode: ` The errors are not within numerical tolerances (i.e. 0.1 or 0.2). It does work as intended if you manually detach `y` ` gives zero error. In general `torch.Tensor.detach` has the expected behaviour (as far as I can test it) The same thing holds if you compute the jacobian wrt parameters and for vjp. `  Once again, the manual detach trick works. I would imagine this to work as documented here.  Versions `  This test is under WSL (since that is what I currently have access to), but I also tested this under baremetal linux and it has the same behaviour there. Behaviour stays the same on cuda as well. ",2024-06-13T07:59:48Z,module: docs module: autograd triaged actionable module: functorch,open,0,4,https://github.com/pytorch/pytorch/issues/128600,"I think this is expected  torch.no_grad() only disables gradient computation for reversemode AD. It does not disable gradient computation for forwardmode AD. The workaround is to use detach().  should reconsider torch.no_grad() working for forwardmode AD? Otherwise, this should just be a docs fix.","Doc fix sounds fine to me for now! It is already documented in the nograd docs that it does not apply to forward AD, but we could explicitly call out jacfwd there as well.  do you think there is anywhere else worth updating?","Thank you for your quick answer! Just as a general comment: I'm not sure that `no_grad` not working with forward mode is a good idea. The way I ran across this is because I needed to compute the jacobianvector product using jvp. For me to get that this would not work with no_grad I would need to  Notice that jvp is computed with forward ad   Notice that forward ad can behave differently than backward ad   find the docs for `no_grad` and notice that this specifically does nothing with forward mode Intuitively, I would have assumed that forwardad and backwardad is just an implementation detail, not something that inherently changes the behavior of what computing a derivative even means. This is especially a problem with jvp since this is twoimplementation details removed from the actual issue. For now I would be totally fine with making this a docs fix, but long term I would favor forward and backward semantics to be as similar as possible (i.e. forward/backward changes performance but not the meaning of the operator). Nevertheless, thank you for your quick response.","Thank you for the feedback! I would agree that from the perspective of computing the Jacobian, it can be confusing why computing using forward and backward mode produce different results. However, I would add that in general, forward mode and backward mode are different semantically and not considered implementation details i.e., they are used to produce different quantities (jvp vs vjp). Though it may seem useful to have a context manager that applies to forward and backward, changing nograd is not straightforward as it would be a bcbreaking change, and there would no longer be a easy way to detach the backward graph without also detaching the forward graph. "
chat,[Traceable FSDP2][Inductor] Add GroupedSchedulerNode to contain nodes that must be scheduled together,"As discussed with  and  in the Inductor group chat, we need the concept of `GroupedSchedulerNode` to be able to express nodes that must be scheduled together oneafteranother (i.e. no other node is allowed to fuse into them or schedule inbetween them). This is particularly important for comm reordering and finegrained control of peak memory. For Traceable FSDP2, there are two very important requirements:  At any time, there must be only one AllGather in flight. However, our existing comm reordering pass will naturally raise **all** of AllGather ops to the beginning of the graph, which will clearly blow up memory usage. Instead, we leverage GroupedScheduleNode which provides simple connection points to build the ""chaining"" on. i.e. we use it to express the schedule `(copyin + AllGather1) > (AllGather1Wait+copyout) > (copyin + AllGather2) > (AllGather2Wait+copyout) ...` by setting up fake dep between the GroupedScheduleNode, which is a very clean and easytounderstand way to express this schedule.  The ""comms"" in FSDP2 are not just comms, but a combination of compute and comm. We must prevent other nodes from being scheduled inbetween that set of nodes, otherwise we are artificially delaying the release of comm buffer memory which makes the peak memory usage quite bad. This is particularly pronounced for `AllGatherWait+copyout`. From these two requirements, we derive the behavior of `GroupedSchedulerNode`: it contains nodes that must be scheduled together oneafteranother (i.e. no other node is allowed to fuse into them or schedule inbetween them).  Q: Can we leverage `ir.Subgraph`? A: I looked into the possibility of using `ir.Subgraph` to i",2024-06-12T23:32:11Z,oncall: distributed Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor keep-going,closed,0,8,https://github.com/pytorch/pytorch/issues/128568,"> just to clarify requirement  do we want to allow fusion into or out of these nodes ? If not, we could add a subgraph, that would require fewer changes  We don't want to allow fusion into these nodes (i.e. no outside node can fuse into these nodes), but we eventually might want to do fusion within this grouped node (among some of its constituent nodes). Wonder would a subgraph allow us to move it around in the larger graph, in order to do reordering?","We'll still fuse within the subgraph. See, pr here: https://github.com/pytorch/pytorch/pull/122069/","I looked into the possibility of using `ir.Subgraph` to implement this, but realized that: 1. `ir.Subgraph` requires defining the subgraph in FX IR. 2. There is no guarantee that the Inductor IR nodes that we want to group together will all have a corresponding FX IR node, because some of those Inductor IR nodes can potentially be dynamically generated by a custom pass in the scheduler (e.g. for merging multiple allgathers into one big allgather, and later we want to group that big allgather with some other op). Dynamically generated Inductor IR node doesn't have a corresponding upstream FX IR node. 3. For the above reasons, we can't use the `ir.Subgraph`, and need to define a new (and more lightweight) concept of `GroupedSchedulerNode` to achieve the behavior we need (this PR).", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[torch.compile] Llama2 failure using dynamic shapes with Torch 2.4 nightly," üêõ Describe the bug torch.compile fails with the following error on Llama2. The error seems to occuring here : https://github.com/pytorch/TensorRT/blob/main/py/torch_tensorrt/dynamo/backend/backends.pyL90L97  Please run the following instructions to reproduce: 1) Please login via huggingfacecli (install it via pip install U ""huggingface_hub[cli]"" ) https://huggingface.co/docs/huggingface_hub/en/guides/clihuggingfaceclilogin. The user access token can be accessed in your settings. 2) Please install transformers via `pip install transformers=4.41.2`. You can install torch_tensorrt nightly via `pip install torch_tensorrt indexurl https://download.pytorch.org/whl/nightly/cu121`  3) Run the following script  The whole log can be found here  llama2_tc.log cc:   Any suggestions here ? Thanks   Error logs _No response_  Minified repro _No response_  Versions [pip3] numpy==1.26.4 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240610+cu121 [pip3] torchtensorrt==2.4.0.dev0+a8a079715 [pip3] torchvision==0.19.0.dev20240610+cu121 [pip3] triton==2.3.1 transformers==4.41.2 ",2024-06-12T20:03:40Z,triaged oncall: pt2 module: dynamic shapes,open,0,1,https://github.com/pytorch/pytorch/issues/128548,Confirmed that it fails without tensorrt. 
rag,Preserve storage size when generating functional tensor,  CC(Preserve storage size when generating functional tensor) Original PR from shazqadeer at https://github.com/pytorch/pytorch/pull/128141 Signedoffby: Edward Z. Yang  ,2024-06-12T19:54:52Z,oncall: distributed Stale ciflow/trunk ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/128546,This is good to go.  you might be the right person for the review here.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Sorry for the late review. I don't see any tests in this PR but I'm wondering if it is obviated by this change last month? https://github.com/pytorch/pytorch/pull/132524 The main annoyance/problem was that the ""storage preservation"" logic would use `aten.set_()` to slam storages from the input to the output, but this ran `aten.set_`'s shape checks that could cause extra guards. I added a helper that just does the storage slamming unsafely, which I think should prevent the need to suppress in the first place (and also probably improve compile times a bit, although I wasn't able to get great measurements)","ok, we can kibosh this, I don't even remember what we needed it for"
yi,Add new parameter for gen_pyi.py to make it more configureable.,This will make the script more flexible for the directory where it is executed.,2024-06-12T16:59:48Z,triaged open source Stale topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/128519, for visibility.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Use computeStorageNbytesContiguous if possible," does fewer datadependent tests compared to . Therefore, use of former is more likely to succeed with dynamic shapes. This PR detects is_contiguous and dispatches to the appropriate function. This should be helpful in unblocking aot_eager for torchrec. As an aside, this is an alternative solution to the unsound solution I had first proposed in another PR).",2024-06-12T15:47:05Z,Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/128515, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `numbytescontig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout numbytescontig && git pull rebase`)", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,torch.onnx.export - `repeat_interleave` produces invalid model," üêõ Describe the bug While adding ONNX export support for Cohere models to Optimum (PR), I ran into an issue due to a single problematic `repeat_interleave` operation within `CohereRotaryEmbedding`. After further analysis, this does appear to be an issue with pytorch/onnx exporting. There is a chance this is related to ONNXRuntime, but in their issue page they link here for pytorch conversion errors. Minimal reproduction (adapted from the HF transformers repo for Cohere models):  This produces the following error:   however, if we replace the `repeat_interleave` with an equivalent op:  the model exports and runs correctly:   Versions PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.9 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.85+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.6 HIP runtime version: N/A MIOpen",2024-06-12T13:02:25Z,needs reproduction module: onnx triaged,open,0,1,https://github.com/pytorch/pytorch/issues/128505,"For completeness, I exported with `torch.onnx.dynamo_export` and that does seem to work: "
transformer,crash@sleef_tryVXE2 () while trying to run torch.compile() BERT model," üêõ Describe the bug Hi,  I have been trying to load a pretrained bert model `bertbasecased`  downloaded from HuggingFace and optimize it with torch.compile() on linuxs390x. It crashed    **`Version:`** Pytorch 2.2.0 **Reproduce:**  Download pretrained `bertbasecased` from huggingface. https://huggingface.co/googlebert/bertbasecased/tree/main run the below scripts    Versions **Version:** Pytorch 2.2.0 python  3.10.14 transformers  4.24.0 **platform**  Linuxs390x",2024-06-12T11:27:55Z,module: crash triaged module: sleef module: POWER,open,2,10,https://github.com/pytorch/pytorch/issues/128503, we have upgraded sleef library after v2.3. Could you please try to repoduce on latest daily build? ," No idea daily build if contains `s390x` OS, you can't build from source the `release/2.4` branch code also.",cc:  ,"Apparently it is crashing in the routine trying to figure out whether VXE2 is available. Being on a z14 machine (not having VXE2) a SIGILL is supposed to happen. However, Sleef installs a signal handler to catch that. The SIGILL really should not surface to the application in that case. I've extracted the code and tried it on a z15 machine to detect a z16 feature and it worked fine. Compiling the following example with ""gcc t.c o t march=z14 mzvector"" and running the program should result in an exit code of 0 on your box:  Does this work as expected for you?","> Apparently it is crashing in the routine trying to figure out whether VXE2 is available. Being on a z14 machine (not having VXE2) a SIGILL is supposed to happen. However, Sleef installs a signal handler to catch that. The SIGILL really should not surface to the application in that case. I've extracted the code and tried it on a z15 machine to detect a z16 feature and it worked fine. Compiling the following example with ""gcc t.c o t march=z14 mzvector"" and running the program should result in an exit code of 0 on your box: >  >  >  > Does this work as expected for you? Krebbel FYI: https://github.com/pytorch/pytorch/pull/123936 merged last month, you can try the latest daily build.","> Apparently it is crashing in the routine trying to figure out whether VXE2 is available. Being on a z14 machine (not having VXE2) a SIGILL is supposed to happen. However, Sleef installs a signal handler to catch that. The SIGILL really should not surface to the application in that case. I've extracted the code and tried it on a z15 machine to detect a z16 feature and it worked fine. Compiling the following example with ""gcc t.c o t march=z14 mzvector"" and running the program should result in an exit code of 0 on your box: >  >  >  > Does this work as expected for you? Krebbel Thanks ! It works fine for my system as expected with an exit code of 0. ","> Krebbel FYI: CC(s390x: use runtime detection for vectorization support) merged last month, you can try the latest daily build. Yeah, using getauxval is definitely the better way. Installing a signal handlers in a library might interfere with signal handlers used by the application. However, I'm curious to understand why it doesn't work in that particular case. The merged code change adds proper facility detection to the aten code. Wouldn't we still need to do the same for Sleef then?!",> Krebbel Thanks ! It works fine for my system as expected with an exit code of 0. Thanks for checking. I'm wondering why Sleef doing the same thing fails then :(,> sleef_tryVXE2 It seems sleef not handle the `disp_expf4_u10`'s dispatch correctly. pytorch 2.2 using the sleef that is two year's ago. I have upgrade the sleef version after pytorch 2.4. Still suggest you try the latest daily build: https://download.pytorch.org/whl/nightly/cpu ,I'll work on a PR for the issue in Sleef. See the Sleef issue for more details. Btw. the backtrace from the first comment is a red herring (and I fell for it too at first). GDB by default intercepts SIGILLs and that's what you see here in your backtrace. But in that case this is the normal operation of the feature detection in Sleef. The SIGILL is expected to happen here. The actual problem is triggered later. In order to see this you have to tell GDB not to intercept SIGILLs. This can be done with: 
transformer,InternalTorchDynamoError on converting llama-2 to onnx using torch.onnx.dynamo_export," üêõ Describe the bug Trying to convert llama2 model from HuggingFace to onnx using the new torch.onnx.dynamo_export fails.  Gives error: > InternalTorchDynamoError: 'NoneType' object has no attribute 'is_tracing' My main goal was to convert a quantized model and I was trying to use the old torch.onnx.export but I faced the problem described in CC(Export of bitwise_right_shift to ONNX needed for llama27b 8bit quantized pytorch model) so I tried to use torch.onnx.dynamo_export but it is giving this error. This is the quantized code which produces the same error.  Furthermore, adding LoRA fails with a different error message which seems to indicate that it isn't yet supported:  Error: > Unsupported: class property LoraModel getset_descriptor  Versions Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.16.1 [pip3] onnxscript==0.1.0.dev20240611 [pip3] torch==2.3.1+cu118 [pip3] triton==2.3.1 [conda] Could not collect ",2024-06-12T03:36:24Z,needs reproduction module: onnx triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/128480,"Could you test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated report if there is an error. Thanks!"
transformer,xpu: gradient checkpointing wrongly hits cuda path running on non-cuda devices,"Issue found running some examples and tests from Huggingface transformers on the system with only XPU device and pytorch built w/o CUDA. As of: * pytorch at https://github.com/pytorch/pytorch/commit/70a1e8571802c22c0f09279b77876e6e85c81325 * https://github.com/intel/torchxpuops/commit/2e6be8c46196c013610e7a53771ce0d357812179 And applying these patches to enable xpu backend for huggingface transformers: * https://github.com/huggingface/accelerate/pull/2825 * https://github.com/huggingface/transformers/pull/31238 I observe that cuda path is hit unexpectedly running some transformers examples and tests which use gradient checkpointing. See the log below. Here are my debug findings: * Issue happens on `loss.backward()` call on HF side. I checked `loss` tensor is on `xpu:0` device: https://github.com/huggingface/accelerate/blob/a9869ea0dc49652e49607d5f111caed79ed5cb67/src/accelerate/accelerator.pyL2136 * Then, inside pytorch other tensors are coming around on the following call and they are not on any device. As a result, default device path is hit which is `cuda`: https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL191 Potentially, there might be I missed something to enable xpu on HF side, but as of now I can't find what. It also might be that the there is some issue for xpu in pytorch. Need some help/guidance to debug and fix, so filing here for now. Log:  ",2024-06-12T02:25:21Z,module: checkpoint triaged module: xpu,closed,0,3,https://github.com/pytorch/pytorch/issues/128478,"This test starts to pass if `DefaultDeviceType::_default_device_type` will be changed from `cuda` to `xpu`, here: https://github.com/pytorch/pytorch/blob/02e7519ac3cd4c4b043c9a0f672464d3797c0622/torch/utils/checkpoint.pyL112 Questions I have are: 1. Where these CPUonly tensors came from on HF side? 2. Why `loss` tensor with clear xpu designation was not on the list of tensors queried for device at https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL191? 3. Is that an issue with pytorch logic with default device for checkpointing?   who from pytorch side can help advice here on further debug?","In further debug, I think I figured out where CPU tensors are coming from. These seem to be tensors returned by `torch.xpu.random.get_rng_state()` on `forward()` here: https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL185 And on `backward()` pytorch tried to get device type from these tensors which actually don't have this information, they are CPU tensors (though they correspond to `xpu` device) and as result device is assumed to be CUDA which is wrong. This happens here: https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL191 As far as I see, CUDA, XPU and MPS all return CPU tensors handling `get_rng_state()`. And they accept CPU tensors on the `set_rng_state()`. Questions: 1. What's the requirement for `getset_rng_state()`. I suspect we need 2nd case. Can pytorch maintainers, please, comment here?   can you, please, help to add relevant people to discussion? ",I have posted a PR CC(Fix device propagation for checkpointing) with the fix I suggest for this issue. It assumes that rng_state functions should work with CPU tensors. Fix is to save full device info on forward() and use it to get device type on backward().
yi,[pytorch] add folly:Indestructible,  CC([pytorch][logging] add empty wait counter implementation)  CC([pytorch] add folly:Indestructible) usingghimport Differential Revision: D58432297,2024-06-11T23:38:48Z,Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/128465,"   :x: The email address for the commit (be259c5d89974c281dfd54f2b8b6eef4036a873f) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Using PyTorch with Transformers to run inference with 'MPS' backend causes poor results.," üêõ Describe the bug When I run inference using a recent text embedding model, the tensor values and subsequent calculations are not expected values. Setup: Python version: 3.12.1 (main, Dec  7 2023, 20:45:44) [Clang 15.0.0 (clang1500.1.0.2.5)] PyTorch version: 2.2.2 Transformers version: 4.41.2 NumPy version: 1.26.4 Pandas version: 2.2.1  code and imports in a Jupyter Notebook in vscode import torch import torch.nn.functional as F from torch import Tensor from transformers import AutoTokenizer, AutoModel import numpy as np from sklearn.metrics.pairwise import cosine_similarity  Set up device for hardware acceleration device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')  Define the last token pool function def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) > Tensor:     left_padding = (attention_mask[:, 1].sum() == attention_mask.shape[0])     if left_padding:         return last_hidden_states[:, 1]     else:         sequence_lengths = attention_mask.sum(dim=1)  1         batch_size = last_hidden_states.shape[0]         return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]  Define the function to create detailed instructions def get_detailed_instruct(task_description: str, query: str) > str:     return f'Instruct: {task_description}\nQuery: {query}'  Assuming tokenizer and model are already loaded and moved to the device tokenizer = AutoTokenizer.from_pretrained('LinqAIResearch/LinqEmbedMistral') model = AutoModel.from_pretrained('LinqAIResearch/LinqEmbedMistral').to(device)  Move the model to the MPS dev",2024-06-11T18:42:51Z,triaged module: correctness (silent) module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/128435,"Hello, the issue does not appear to occur with the latest PyTorch nightly, mps returns the expected test_scores. If possible, please update to PyTorch 2.5.0 or later and let us know here if the issue still occurs: https://pytorch.org/getstarted/locally/","That sounds awesome, will try out this weekend!","Closing for now, please feel free to reopen if the issue is observed again"
yi,[pytorch] add folly:Indestructible,  CC([pytorch] add folly:Indestructible) Differential Revision: D58432297,2024-06-11T17:29:09Z,fb-exported,closed,0,4,https://github.com/pytorch/pytorch/issues/128425," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D58432297,"  :white_check_mark: login: jamesperng / name: James Perng  (c1bee3b230ee0fb33c379c74493df40acb389964, 10b062574ec801d305d3b3eeb9f6b8b42f4d3f31, 331d28aede2ac662c370fb674f60dee03764134e, 07c968c304ba54f96a911e6a695fa344e873f089, 04672015c73e96827ac5db16f740ea001f9d08c1, 7528082b15283871ecadcf55d8c16914d1deed03, 8a2c14653272d3262c141c8b75a0b2a2addb0ace) :x: The email address for the commit (7659817be1b0d642daf05fd7dbc31143a5fc460c) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D58432297
gemma,`torch.compile` with `reduce-overhead`: very long compile time + GPU memory continuously to grow," üêõ Describe the bug For code snippet, see at the end    `model.py`: very simple     `run.py`: a bit more complex but just in order to measure the memory and timing  **In short**: `torch.compile` with `reduceoverhead` takes long time and the GPU memory usage continues to grow (in the second call to each shape seen). **Although the code snippet is dummy, the same situation happens when I check with `Llama` or `Gemma` models from `HuggingFace`.** This could be seen clearly from the output sections below. Here are a few explanations to facilitate the understanding (code snippet and the outputs):  Think each (outer) iteration as a (language model) call to `generate` (up to a `max_len` steps)  Think each (inner) iteration as a (language model) decoding steps (which calls the model's `forward`)  The 1st (outer) iteration (i.e. call to `generate`) sees all possible input shape: time  taken and memory usage don't vary much    for `max_len=1024`      timing: 7.76404      Used GPU memory increased: 54.0 MB.    for `max_len=2048`      timing: 9.206949      Used GPU memory increased: 54.0 MB.    for `max_len=4096`      timing: 12.44839      Used GPU memory increased: 54.0 MB.  **The 2nd (outer) iteration**: takes much longer time and memory usage accumulate (see `outputs (with information from intermediate steps)` below)    for `max_len=1024`     ** timing: 115.606751**     ** Used GPU memory increased: 150.0 MB.**    for `max_len=2048`     ** timing: 232.851245**     ** Used GPU memory increased: 302.0 MB.**    for `max_len=4096`     ** timing: 565.084438**     ** Used GPU memory increased: 606.0 MB.**  The 3rd (outer) iteration: very fast and no me",2024-06-11T17:23:18Z,triaged module: cuda graphs oncall: pt2 module: dynamic shapes,open,0,8,https://github.com/pytorch/pytorch/issues/128424,"This is probably the same as  CC(If dynamic shapes runtime CUDA graphs never quiesces, should loudly warn / easy to diagnose) reduceoverhead is not magic fairy dust. It works by doing CUDA graphs. CUDA graphs do not work with dynamic shapes. So we CUDA graph each individual dynamic shape individually. This can end up using a lot of memory. To reduce memory usage, you will need to do some padding at multiples. Or you can rearchitect your prefill/decode so that it is CUDA graph friendly, as was done in gptfast."," Besides my support for more love for padding multiples for nestedtensor constructors (e.g.  CC([feature request] Jagged / padding version of torch.stack / torch.cat + some general nested tensor discussion)) and more inplacing/outversions (e.g. for torch.cat), but also could be cool to have somehow more introspection into the Inductor compiler cache / CUDA graph cache. E.g. if there was a way to list from Python all cached shape/dtype specializations, it would be easier to diagnose/confirm this sort of problems (e.g. it would be growing along with time) + maybe some higherlevel metric on memory fragmentation or more examples on memory allocator stats. E.g. could one enable more coarse memory allocator segment sizes without torch recompilation? (this could go along with fullyfledged support for customized/reconfigured memory allocators)","Thanks ! OK, guess we have to use the workarounds you mentioned like padding. (`rearchitect your prefill/decode so that it is CUDA graph friendly` is more complex to handle, as I work in HuggingFace `transformers` team and we try to keep the API stable). I agree what  mentioned about a way to investigate this cache stuff. (Probably it's already possible with `TORCH_LOGS`?)",One other thing: cuda has a driverlevel issue where cudagraphs take a lot of memory on device (64kb per kernel). That is fixed on cuda 12.4 and driver 550+. ,"  > Or you can rearchitect your prefill/decode so that it is CUDA graph friendly, as was done in gptfast. Confirmed that keep all tensors (not just in the arguments of the top level `forward`) in a fixed (few) number of  shapes avoid the issue. Feel free if you think we could close this issue.",Is there anyway we can save the cache if the input size is exactly same? Recompiling (even with cache) at firstrun is very slow (40s+) for GPTfast.,The issue mentioned in this issue is not related to `recompiling` and it the first run (iteration) is kind fast. I personally tried `GPTfast` (for another experimentation not related to this issue) and it works quite well for me.  `40 seconds` is quite reasonable to me however.,related: https://github.com/huggingface/transformers/pull/32227
yi,Add FloatTrueDiv and ToFloat to SYMPY_INTERP,"  CC(Add targeted unit tests for guardsrelated functions used in the codecache)  CC(Add FloatTrueDiv and ToFloat to SYMPY_INTERP) Summary: I admit I'm not 100% sure what I'm doing here. I'm hitting a bug in the FX graph cache when we try to evaluate a guards expression. We're creating guards that look like this:  It looks like we have a facility to define these operators in the SYMPY_INTERP map and we're just missing FloatTrueDiv and ToFloat. What's surprsing to me is that we're only hitting this problem with the FX graph enabled. We can create such guards, but we've never actually evaluated any? Test Plan: `TORCHINDUCTOR_FX_GRAPH_CACHE=1 python benchmarks/dynamo/torchbench.py ci accuracy timing explain inductor device cuda inference bfloat16 only detectron2_fcos_r_50_fpn`",2024-06-11T14:44:10Z,Merged ciflow/trunk release notes: fx topic: not user facing ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/128418,fyi  , do you mean this: https://github.com/pytorch/pytorch/blob/main/torch/utils/_sympy/functions.pyL12L28 Currently they don't seem to overlap that much.,No test?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,"torch._dynamo.exc.Unsupported: call_method GetAttrVariable(UnspecializedNNModuleVariable(CenterCrop), _transformed_types) __iter__ () {}"," üêõ Describe the bug Internal xref: https://fb.workplace.com/groups/1075192433118967/posts/7533980249972440/ I'm trying to export some image preprocessing codes, and got some error:   I think it's because of this line,  The source code to repro:   Versions main ",2024-06-11T14:32:40Z,triaged enhancement oncall: pt2 module: dynamo module: graph breaks dynamo-triage-june2024,closed,0,1,https://github.com/pytorch/pytorch/issues/128417,Already fixed on main by https://github.com/pytorch/pytorch/pull/130169
transformer,Disable fast path in `TransformerEncoderLayer` when there are forward (pre-)hooks attached to modules,Fixes CC(Forward hooks not called when fast path is used in TransformerEncoderLayer)  Disable fastpath if there are forward hooks or prehooks. Example failure case given in the issue.,2024-06-11T13:20:01Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,20,https://github.com/pytorch/pytorch/issues/128415,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: iibrahimli / name: Imran Ibrahimli  (943e92d8c39f59b82c049a3bf56198f7d49d0569, 2a8bb3095ef190266b8b7b3e25907c0439e8c5a3, 33332a7f7a11f18dc7dc0cb589b52acec059fac2, d20cdd180c85824f4560545123fa7f9dc871c9e6, 2b780bb69f7cd589b0a38faa8adf8ed498bf1188, 28f93fc2124bbd288be6e819656807e158923d2b, 5c17236a933ac81b544ae3a19ae24b2129f43921)",Nit: could be faster by using any to allow for short circuiting.,"> Nit: could be faster by using any to allow for short circuiting. true. changed, thanks",The failing test seems to be due to OOM? Not sure if it's related to the changes, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalcuda12.4py3.10gcc9sm86 / test (default, 4, 5, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge i ""foreach failure is unrelated"""," merge f ""foreach failure is unrelated""", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `128413addfastpathconditionforwardhooks` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 128413addfastpathconditionforwardhooks && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,"Not sure why previous lintrunner run missed it, but I applied the patch it suggested. `lintrunner m origin/main` green now locally", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Forward hooks not called when fast path is used in TransformerEncoderLayer," üêõ Describe the bug When `TransformerEncoderLayer` is run in evaluation mode and a few conditions are met, the fast path is used (which is a fused optimized implementation) instead of calling the modules like  `MultiheadAttention` (e.g. `self.self_attn`). This means any forward hooks or prehooks registered for the submodules are not called.   In this example, we would expect the `cache` to contain the output, but it does not. However, if we modify any condition for fast path selection, e.g. use an odd number of attention heads `nhead=3`, fast path is not used and the hook is called as expected.  Versions PyTorch version: 2.2.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.5 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: version 3.23.1 Libc version: N/A Python version: 3.12.0 (main, Oct  5 2023, 15:44:07) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS14.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Versions of relevant libraries: [pip3] mypy==1.8.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.26.4 [pip3] pytorchlightning==2.2.0.post0 [pip3] torch==2.2.0 [pip3] torchmetrics==1.3.1 [pip3] torchviz==0.0.2 [conda] numpy                     1.22.3           py39h64940a9_2    condaforge [conda] pytorch                   1.11.0          cpu_py39h03f923b_1    condaforge [conda] tor",2024-06-11T12:45:09Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/128413,My proposed solution would be to fall back from using fast path if there are pre/forward hooks on any submodules of the layer. I have started working on it: CC(Disable fast path in `TransformerEncoderLayer` when there are forward (pre)hooks attached to modules) 
mixtral,Compilation time for dynamic shapes for gpt-fast prefill regressed significantly.," üêõ Describe the bug  It takes 40 minutes+ to compile!  After looking at some profile results, Ed pointed me to this PR: https://github.com/pytorch/pytorch/pull/123342 cc:     Versions N/A ",2024-06-11T07:33:57Z,triaged oncall: pt2 module: dynamic shapes,closed,0,5,https://github.com/pytorch/pytorch/issues/128398,Do you have any profile or so that points to that PR as the culprit?," yeah I reverted it and the compile time drops a lot. You can replicate it by patching the diff and running `python benchmarks/gpt_fast/benchmark.py`. I also have this SVG, hopefully the link works. But you can see we spend 65% of the time in `get_implications`. !image",">  yeah I reverted it and the compile time drops a lot. You can replicate it by patching the diff and running `python benchmarks/gpt_fast/benchmark.py`. >  > I also have this SVG, hopefully the link works. But you can see we spend 65% of the time in `get_implications`. >  > !image Hi  , may I ask what profiling tool you are using here. It looks intuitive and effective. Thanks.","In PyTorch you can use it through TORCH_COMPILE_CPROFILE=1. Underneath, it uses cprofile and gprof2dot.","Thanks  , cool toolÔºÅ"
chat,_foreach_atan2," üöÄ The feature, motivation and pitch This is useful for Adamatan2, from Scaling Exponents Across Parameterizations and Optimizers, which allows skipping the `eps` parameter of Adam.  Alternatives Without it, we have to fallback to for loops.  Additional context _No response_ ",2024-06-11T06:25:37Z,module: optimizer triaged actionable module: mta,open,1,4,https://github.com/pytorch/pytorch/issues/128395,Here's where I use it:  I would replace this with: ,"6 months later, Adamatan2 is not popular; regular people continue to use default AdamW. Some optimizer folks still prefer atan2. I no longer use it because I have something better (dynamic eps).",Curiouswhat is the dynamic eps referring to here? Do you have a link to a paper/impl?,"It's just something I do internally. I can't be bothered to figure out appropriate eps for each layer, but I'm already normalizing the gradients for other purposes, so my eps happens to scale correctly with gradient size and beta2 and step count. The effect of this changed eps is a 0.000 reduction in loss. atan2 also has the disadvantage of not scaling with changing batch size, so I removed it entirely from my optimizer rather than turning it off with a flag."
transformer,[export] Failed to trace HF Llama2 model," üêõ Describe the bug The following Llama2 program used to work, but failed recently.  Error logs   Minified repro  $ python ep_llama.py strict pre Error log $ python ep_llama.py nostrict pre Error log  Versions pytorch nightly as of issue open time. ",2024-06-11T06:19:22Z,high priority triage review module: regression oncall: pt2 oncall: export,closed,0,13,https://github.com/pytorch/pytorch/issues/128394, seems like a predispatch issue? ,"To unblock, you can try torch.export._trace._export(pre_dispatch=False). ","Thanks  .  Tried pre_dispatch=False, but seems to hit the same error. (You can use the repro above, but set flag `nopre`). For example:  Cc:  ",Hit the same issue (627 nightlies) and talked with  about it. This is becoming more urgent.  This issue has been open for 3 weeks...would anyone be able to address this soon?  ,"updating  the same error blocks tracing of Llama38B, so it's continuing to block on more models.  tested with 2.5.0.dev20240630+cu121 ","Some investigations..  Loading and running with the `with torch.device(‚Äòmeta‚Äô)` does not work for me, it always runs into `RuntimeError: unsupported scalarType error`. Running the loaded model eagerly runs into the same issue `(llama(inputs[‚Äúinput_ids‚Äù]))`, even if I tried not loading the inputs on meta device. So removing that context, for the following code:  The following combinations of flags worked for me: `(pre_dispatch=False, strict=False), (pre_dispatch=False, strict=True)` On a small test case, I get the following graph:  `(pre_dispatch=True, strict=True)` results in the SpecViolationError, and produces following graph:  `(pre_dispatch=True, strict=False)` passes, resulting in the following graph, but this seems to be an incorrect behavior:  Chatted with  and we feel that the best solution is to turn the autocast context manager into a HOP, like what we do for `no_grad`.  ","> Chatted with  and we feel that the best solution is to turn the autocast context manager into a HOP, like what we do for no_grad. ?  Tugsuu is on PTO, so I'll unassign him. We should find a new owner for this","> What exactly is the problem here? If there's an autocast context manager, the predispatch (strict) graph looks something like:  But the operator `torch.amp.autocast_mode._enter_autocast` is not a valid ATen op. And we were thinking that the way we handle the autocast context manager should be the same way which we handle the no_grad HOP. Separately, the nonstrict predispatch graph seems to be incorrect, since it's missing the `_to_copy(dtype=torch.bfloat16)`, but maybe the context manager above will fix this.","For note keeping, the regression is due to https://github.com/huggingface/transformers/pull/29285/ which added the autocast context in transformers/modeling_llama.py.",Do we have a problem with the predispatch=False case? I agree in the predispatch=True case we can use a HOP to represent the context manager,"No, predispatch=False seems fine to me","Thanks  for suggesting fake tensor mode in  CC(Meta device not supported with autocast). The following script seems to work for me for export the current Llama2 model (with `autocast`).  If you would like to use `cuda` as the device while exporting the model, you can switch ""cpu"" above with ""cuda"".","With the changes in CC([export] Convert autocast to HOO), the code below can be exported (we convert autocast to HOO as discussed above). `with torch.device(""meta"")` still doesn't work (see CC(Meta device not supported with autocast)).   "
yi,[Fix] Correctly identifying arguments for sub-blocks with renaming logic during TorchScript to ExportedProgram conversion," Issue Fix two issues related to inputs lifting when there are subblocks. * Some inputs may appear in the nested subblocks, which need a recursive search to identify which arguments need to be lifted / passed in the toplevel block. * Some inputs to the subblock are intermediate results, meaning their names are only number. This will cause issue during code generation (i.e., invalid argument name). We rename those to valid names.   Test Plan * `pytest test/export/test_converter.py s k test_convert_nn_module_with_nested_if_and_param` * `test/export/test_converter.py s k test_hidden_input_name`",2024-06-11T02:56:41Z,Merged topic: not user facing ciflow/inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/128386," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[export] Llama2 export with dynamic shapes fails using Torch 2.4 nightly," üêõ Describe the bug I'm trying to export Llama27B model with dynamic shapes and encountered the following error.  This passes with torch 2.3   so it is a regression.  In the python session :  1) Please login via huggingfacecli (install it via `pip install U ""huggingface_hub[cli]""` )      https://huggingface.co/docs/huggingface_hub/en/guides/clihuggingfaceclilogin. The user access token can be accessed in your settings. Code:   Error :   Please find the full log file here :  llama2.log cc:     Versions Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240610+cu121 [pip3] torchtensorrt==2.4.0.dev0+c6f8cb464 [pip3] torchvision==0.19.0.dev20240610+cu121 [pip3] triton==2.3.1 [conda] Could not collec ",2024-06-11T02:56:05Z,oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/128385,"I wasn't able to repro this, but you could try doing this:  The `_allow_complex_guards_as_runtime_asserts` flag should also allow you to export w/o needing `torch.nn.attention.sdpa_kernel([SDPBackend.MATH])` ","> I wasn't able to repro this Is this using the reproducer provided or using the code snippet you suggested ?  Btw, the code snippet you provided seems to work fine but is it recommended to use `torch.export._trace._export` as a part of official examples ? "
transformer,[dtensor][debug] add module level tracing and readable display,"  CC([dtensor][test] test case suite for comm_mode features)  CC([dtensor][example] added MLPStacked example for printing sharding)  CC([dtensor][be] improving readability of comm_mode.py and comm_mode_features_example.py)  CC([dtensor][debug] add module level tracing and readable display) **Summary** Currently, CommDebugMode only allows displaying collective tracing at a model level whereas a user may require a more detailed breakdown. In order to make this possible, I have changed the ModuleParamaterShardingTracker by adding a string variable to track the current submodule as well as a dictionary keeping track of the depths of the submodules in the model tree. CommModeDebug class was changed by adding a new dictionary keeping track of the module collective counts as well as a function that displays the counts in a way that is easy for the user to read. Two examples using MLPModule and Transformer have been added to showcase the new changes. The expected output of the simpler MLPModule example is:  **Test Plan** torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/display_sharding_example.py ",2024-06-10T23:51:32Z,oncall: distributed Merged topic: not user facing ciflow/inductor,closed,1,0,https://github.com/pytorch/pytorch/issues/128369
rag,[Nested Tensor]fix sdpa backward for the special case with ragged second batch dim and constant length,  CC([Nested Tensor]fix sdpa backward for the special case with ragged second batch dim and constant length),2024-06-10T21:24:56Z,Merged ciflow/trunk release notes: nested tensor,closed,0,16,https://github.com/pytorch/pytorch/issues/128349, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/yuqingj/1/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/9522329915, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/yuqingj/1/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/128349`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.8gcc11 / test (distributed, 2, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/yuqingj/1/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/128349`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," cherrypick onto release/2.4 c critical fix ""Fix the missing gradient during NJT SDPA backward when the raggedness is on the second batch dim instead of the sequence length dim.""", Cherry picking CC([Nested Tensor]fix sdpa backward for the special case with ragged second batch dim and constant length) Command `git C /home/runner/work/pytorch/pytorch cherrypick x 00f675bb4c2ec02bb5ffecfc75571026e220701c` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job 
transformer,SAM breaks with torch.compile," üêõ Describe the bug transformers SAM model inference fails when model is `torch.compile`d, with  I will provide reproduction code. Model normally works when not compiled.  Error logs   Minified repro   Versions This can be reproduced with Colab's env as of now:  ",2024-06-10T12:05:41Z,oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/128326, what is `image` supposed to be here ? what are the dimensions of images supposed to be ?," sorry I didn't post it, here:  ",This no longer repros on master.
,control output type in matmul," üöÄ The feature, motivation and pitch Current matmul supports only the same input and output tensor types. As a result it can produce numerically incorrect outputs for cases when input tensors are int8 or float8 (e.g. torch.float8_e5m2): if input arguments for a matmul have type int8 the the output also will be int8 (same story with float8). I guess it is hard to extend the existing api of matmul, but it would be great to create matmul_v2(x, y, output_type)  Alternatives I see some progress in this direction: for torch.float8_e5m2 on gpu use case, I can use torch._scaled_mm() it already has out_dtype. Another option is to cast input arguments to higher precision (e.g. float32) so that the output also will have the same high precision. This approach defeats the purpose of having 8 bits types. Another option is to use custom operations. It works. Alternative frameworks such as TF, JAX support this kind of feature already.  Additional context It is useful for running large language models.",2024-06-08T05:54:16Z,triaged needs design matrix multiplication,open,3,2,https://github.com/pytorch/pytorch/issues/128283,"Marking as needs design, since we haven't reached a conclusion yet on the right API. Some discussion from triage review: alban: We could add an out= variant for matmul, would need extra work in autograd to handle it. Also, ""have a performant kernel"" is a separate step from ""expose an API for this"". Richard: there should not be a separate way to do this in eager vs compile (compile has a custom dtype HOP) nikita: we have a mixed_mm already? can maybe better advertise that this is the right API to use, there have been other similar issues, still needs design. Also from nikita: can support mixed dtypes in compile, but blas requires us to upcast for cpu (no performant kernel)","Is there any way to achieve this now (e.g., bf16 x bf16 > fp32 matmul)? I have a use case that needs exactly this"
yi,[easy][inline-inbuilt-nn-modules] Fix expected graph for control flow test,  CC([DONT MERGE][dynamo] Turn on inlining of inbuilt nn modules)  CC([easy][dynamo][inline work] Fix test with inlining inbuilt nn modules)  CC([inductor][inlining nn module] Skip batchnorm version check test for inlining)  CC([dynamo][yolov3] Track UnspecializedNNModuleVariable for mutation)  CC([dynamo] Skip inlining builtin nn modules for torch.compile inside cond)  CC([easy][inlineinbuiltnnmodules] Fix expected graph for control flow test),2024-06-07T20:01:29Z,Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/128246
transformer,nn.Transformer gives different output in torch.no_grad() context," üêõ Describe the bug The output that `nn.Transformer` gives in `eval` mode with the same inputs, is different when the forward pass is performed in a `torch.no_grad()` context:  The output I get, is: `[...]/python3.12/sitepackages/torch/nn/modules/transformer.py:384: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)   output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)`, and `tensor(False)`. The issue seems very similar to this one, created in 2022)), and likely has something to do with the fact that the model converts the encoder input into a nested tensor for some reason. When looking at the absolute difference between outputs `print(torch.abs(output1  output2).mean())`, the difference seems failry minimal; in the order of 1e7. Before, when I forgot to set a memory mask, the difference was much larger and substantially effected performance of my model during inference. Kind regards, Vincent  Versions Collecting environment information... PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Fedora Linux 39 (Workstation Edition) (x86_64) GCC version: (GCC) 13.2.1 20240316 (Red Hat 13.2.17) Clang version: Could not collect CMake version: version 3.27.7 Libc version: glibc2.38 Python version: 3.12.2 (main, Feb 21 2024, 00:00:00) [GCC 13.2.1 20231205 (Red Hat 13.2.16)] (64bit runtime) Python platform: Linux6.7.10200.fc39.x86_64x86_64withglibc2.38 Is CUDA available: True CUDA runtime version: Could not col",2024-06-07T10:46:25Z,module: nn,closed,0,3,https://github.com/pytorch/pytorch/issues/128209,"PS: just updated to 2.3.1, and it still occurs.","model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode. torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won‚Äôt be able to backprop (which you don‚Äôt want in an eval script).","from  , the delta from these two runs locally is is ~1e6, which is expected for float32. See https://pytorch.org/docs/stable/notes/numerical_accuracy.html for details."
gemma,[ROCm] TunableOp for gemm_and_bias,"Thus far TunableOp was implemented for gemm, bgemm, and scaled_mm.  gemm_and_bias was notably missing.  This PR closes that gap. This PR also fixes a regression after CC([ROCm] TunableOp improvements) disabled the numerical check by default. The env var to enable it no longer worked. CC   ",2024-06-06T16:27:05Z,module: rocm triaged open source Merged Reverted ciflow/trunk release notes: rocm ciflow/rocm,closed,0,11,https://github.com/pytorch/pytorch/issues/128143,"A lot of the failures, seem unrelated to this PR:  Could someone please restart the CI?", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `tunableop_gemm_and_bias` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout tunableop_gemm_and_bias && git pull rebase`)", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""release notes: rocm""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"This pull request has been **reverted** by 8415a4ba98f337e6d21a3c0b026917c03a19e955. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).","If we're relanding this PR, please make sure we add some unit tests to protect the tunable op numerical accuracy (in both the gemm_and_bias and gemm cases). I think this PR produces some wild results"
yi,A trivial but annoying bug in random_split," üêõ Describe the bug   Speculated cause: print(0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05+0.05) gives ""1.0000000000000002"" by plain Python in the first place.  Versions Collecting environment information... PyTorch version: 2.2.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Home GCC version: (x86_64posixsehrev0, Built by MinGWW64 project) 8.1.0 Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22631SP0 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU Nvidia driver version: 546.80 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2500 DeviceID=CPU0 Family=207 L2CacheSize=11776 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2500 Name=12th Gen Intel(R) Core(TM) i912900H ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.25.1 [pip3] torch==2.2.1+cu118 [pip3] torchaudio==2.2.1+cu118 [pip3] torchvision==0.17.1+cu118 [conda] Could not collect ",2024-06-06T06:04:37Z,needs reproduction triaged module: data,open,0,4,https://github.com/pytorch/pytorch/issues/128116,"Tried to reproduce with PyTorch from source (2.4.0a0+git662a78f), Python 3.12.3, and Cuda 12.5 resulting in:  (the same result occurred using PyTorch 2.2.1, Python 3.12.3, and pytorchcuda==12.1) I was able to reproduce this issue with Pytorch 2.2.1, Python 3.11.9, and pytorchcuda==12.1:  ","It looks like the second half of this check is the cause, https://github.com/pytorch/pytorch/blob/11f2d8e823efa8508d1f2198429b95b1e9007222/torch/utils/data/dataset.pyL455  Going back to the original issue, it seems like this was put in for backwardscompatibility reasons:  CC([TorchData] Allow random_split to use percentages when applied to map-style datasets)","Seems like this is not a problem in Python 3.12 because the algorithm that 'sum()' uses has been changed see here. (The warning for empty datasets is expected since you would be splitting 10 items into 20 sets.) As far as fixing for earlier versions of Python: In the discussion referenced by , it is mentioned that there should be no error if the list of fractions sums to less than 1, but there is a test that fails in the case that the list sums to less than 1:  https://github.com/pytorch/pytorch/blob/main/test/test_dataloader.pyL266 This test was added as a part of the pull request that resolved the aforementioned [ CC([TorchData] Allow random_split to use percentages when applied to mapstyle datasets)]( CC([TorchData] Allow random_split to use percentages when applied to map-style datasets)). Either way the problem is solved by altering https://github.com/pytorch/pytorch/blob/11f2d8e823efa8508d1f2198429b95b1e9007222/torch/utils/data/dataset.pyL455 Two options: 1. Removing the 'and' clause above (enforcing the equality constraint since math.isclose() should solve the issues that come from using floating point numbers in Python). 2. Replacing 'and' with an 'or': `if math.isclose(sum(lengths), 1) or sum(lengths) < 1:`, and removing the above test case.","~~Oh. I did speculate that it's a problem related to Python, but I didn't realize that I should also describe my python version then. Thank you all for helping.~~ Oh wait it was automatically included in that collect_env.py...... Never mind. Thank you all for helping."
rag,Use torch.ops.create_parameter_op.set_ and avoid storage resize under compile,  CC(FSDP+TP debug)  CC([Traceable FSDP2] Top of Traceable FSDP2 stack)  CC([Traceable FSDP2] Add FSDP passes to partitioner)  CC(Horace's PR 126446: Prevent partitioner from ever saving views)  CC([Brian's PR] Support out=List[Tensor] in auto_functionalize)  CC([TEST ONLY] Add tracing support for outvariant custom ops that return None)  CC(Use torch.ops.create_parameter_op.set_ and avoid storage resize under compile)  CC([Traceable FSDP2] Check hasattr('fsdp_pre_all_gather') only when not compile),2024-06-06T04:40:59Z,oncall: distributed release notes: distributed (fsdp) module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/128105
agent,Share TCPStore by default when using c10d rdzv handler,"Summary: Number of features rely on TCP store as a control plane. By default TCPStore server is started on rank0 trainer and this can create a a race condition when rank0 may exit (error and graceful exit) and any other ranks reading/writing will fail.  Solution: TCPStore server should outlive all the trainer processes. By moving the ownership TCPStore to torchelastic agent it naturally fixes the lifecycle of the server. Static rendezvous in torchelastic does already support sharing of the TCPStore server. We are extending this to more commonly used c10d rendezvous handler. Any handler would like to manage tcp store has to:  Return true on `use_agent_store` property  `RendezvousInfo`.`RendezvousStoreInfo`[`master_addr/master_port`] values refer to managed TCPStore (those are returned on `next_rendezvous` call) Note: in some instances users may want to use nonTCPStore based stores for the torchelastic rendezvous process, so the handler will need to create and hold a reference to TCPStore (as done in this change) Test Plan: `cat ~/workspace/distdemo/stores.py` ~~~ import torch import logging import sys import torch.distributed as dist import torch import os import time logger = logging.getLogger(__name__) logger.addHandler(logging.StreamHandler(sys.stderr)) logger.setLevel(logging.INFO) def _run_test(store):     if dist.get_rank() == 1:         logger.info(""Rank %s is sleeping"", dist.get_rank())         time.sleep(5)         key = ""lookup_key""         logger.info(""Checking key %s in store on rank %s"", key, dist.get_rank())         store.check([key])     else:         logger.info(""rank %s done"", dist.get_rank()) def main() > None:     use_g",2024-06-06T01:05:03Z,oncall: distributed fb-exported Merged ciflow/trunk release notes: distributed (torchelastic),closed,0,5,https://github.com/pytorch/pytorch/issues/128096,This pull request was **exported** from Phabricator. Differential Revision: D58180193,This pull request was **exported** from Phabricator. Differential Revision: D58180193,This pull request was **exported** from Phabricator. Differential Revision: D58180193," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Use torch.ops.create_parameter_op.set_ and avoid storage resize under compile,  CC(FSDP+TP debug)  CC(removed)  CC([NOT FOR LANDING] [Traceable FSDP2] Add FX passes to partitioner to avoid saving alias of primal as FWD graph output)  CC(Horace's PR 126446: Prevent partitioner from ever saving views)  CC([Brian's PR] Support out=List[Tensor] in auto_functionalize)  CC(Add tracing support for outvariant custom ops that return None)  CC([NOT USED] support inplace all_gather)  CC(Add support for register_post_accumulate_grad_hook)  CC([NOT NEEDED] Trace TensorVariable attribute mutation if call_setattr is called)  CC([WIP] Improve __bool__ access handling for UserDefinedObjectVariable)  CC([Traceable FSDP2] Workaround in nn_module_proxy())  CC([NOT NEEDED] Check hasattr before comparing source name)  CC([NOT NEEDED] Support SetVariable mutation)  CC([Traceable FSDP2] Improve FSDPManagedNNModuleVariable support)  CC([Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP)  CC([DO NOT REVIEW][NOT USED] Add queue_callback support)  CC([Traceable FSDP2] Return early from _register_post_backward_hook when compile)  CC(Use torch.ops.create_parameter_op.set_ and avoid storage resize under compile)  CC([Traceable FSDP2] Make FSDPParam._unsharded_param creation traceable)  CC([Traceable FSDP2] Use custom ops for AllGather copyin / copyout and ReduceScatter copyin)  CC([Traceable FSDP2] Check hasattr('fsdp_pre_all_gather') only when not compile)  CC([Traceable FSDP2] Dynamo support FSDP2 use_training_state context manager),2024-06-06T00:44:26Z,oncall: distributed release notes: distributed (fsdp) module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/128093
gpt,"Add model name, quantization and device to gpt_fast micro benchmark output",A small enhancement to https://hud.pytorch.org/benchmark/llms with these columns in the output.,2024-06-06T00:35:16Z,Merged ciflow/trunk topic: not user facing test-config/default ciflow/inductor-micro-benchmark test-config/inductor-micro-benchmark,closed,0,7,https://github.com/pytorch/pytorch/issues/128091, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalcuda12.4py3.10gcc9sm86 / test (default, 2, 5, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
chat,[User Empathy Day 2] non-deterministic recompiles for ChatTTS model," üêõ Describe the bug Doing inference with ChatTTS model (https://huggingface.co/2Noise/ChatTTS) will trigger nondeterministic recompiles. E.g., for the first 5 texts we do inference in the testing script (https://gist.github.com/shunting314/a904ad991a9893c0a7f3c6e190f7437e ), if no recompile happens, we get around 2x speed up. But when recompiles happens, end2end latency is slower than eager. When recomiples happens, here is the log for the failed guards: https://gist.github.com/shunting314/e4dbcaed70d743b059343b629fe1b568 The recompilation is quite nondeterministic. I've seen the following happens  no recompile  the first input triggers recompile  the third and fourth input triggers recompile  Error logs .  Minified repro _No response_  Versions . ",2024-06-05T22:10:26Z,triaged oncall: pt2 module: dynamo module: guards empathy-day dynamo-user-empathy-day dynamo-triage-june2024,closed,0,1,https://github.com/pytorch/pytorch/issues/128074,"Some update on this * No nondeterminism in recompilations now. * A lot of changes made in, which is causing more recompiles. Some of them could be removed by latest HF work. I opened an issue to encourage authors to revisit compile support  https://github.com/2noise/ChatTTS/issues/748"
transformer,Dynamo Graph break in Unsupported: call_method ConstDictVariable()," üêõ Describe the bug Model https://huggingface.co/sentencetransformers/allMiniLML6v2 The sample causes a graph break:   Repro:   Versions Collecting environment information... PyTorch version: 2.4.0a0+git49ad903 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: CentOS Stream 9 (x86_64) GCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.13) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.34 Python version: 3.11.9  (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.19.00_fbk12_zion_rc2_11583_g0bef9520ca2bx86_64withglibc2.34 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 GPU 1: NVIDIA H100 GPU 2: NVIDIA H100 GPU 3: NVIDIA H100 GPU 4: NVIDIA H100 GPU 5: NVIDIA H100 GPU 6: NVIDIA H100 GPU 7: NVIDIA H100 Nvidia driver version: 525.105.17 cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.9.2 /usr/lib64/libcudnn_adv_infer.so.8.9.2 /usr/lib64/libcudnn_adv_train.so.8.9.2 /usr/lib64/libcudnn_cnn_infer.so.8.9.2 /usr/lib64/libcudnn_cnn_train.so.8.9.2 /usr/lib64/libcudnn_ops_infer.so.8.9.2 /usr/lib64/libcudnn_ops_train.so.8.9.2 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_train.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_ops_infer.so.8 /usr/local/cuda12",2024-06-05T21:45:44Z,triaged oncall: pt2 module: dynamo empathy-day dynamo-dicts dynamo-triage-june2024,closed,0,0,https://github.com/pytorch/pytorch/issues/128067
rag,[NestedTensor] Add error checks for unbind operator coverage when ragged_idx != 1,"Summary: Add the following error checks for the `unbind` operator on `NestedTensor`s when `ragged_idx != 1`:  The current implementation allows the creation of `NestedTensor` instances from the class definition with an `offsets` tensor that applies to a dimension other than the jagged dimension. This diff ensures that `unbind` fails when the `offsets` exceed the length of the jagged dimension. Test Plan: Added the following unit tests: `test_unbind_with_lengths_ragged_idx_equals_2_bad_dim_cpu` verifies that `unbind` fails when there is a mismatch between the offsets and the jagged dimension, for `NestedTensor`s with `lengths`.  Reviewed By: davidberard98 Differential Revision: D57989082",2024-06-05T20:30:17Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/128058,This pull request was **exported** from Phabricator. Differential Revision: D57989082," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D57989082, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
chat,Make ValueRange repr less chatty by default,  CC(Make ValueRange repr less chatty by default) Signedoffby: Edward Z. Yang  ,2024-06-05T17:38:44Z,module: cpu Merged Reverted ciflow/trunk module: dynamo release notes: dynamo,closed,0,12,https://github.com/pytorch/pytorch/issues/128043,lint is complaining tho, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 2f4e8712ec1c65e60a2f1610d86e35d4d45bd8ec` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job ," merge f ""ci failures look unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c ghfirst m ""Sorry reverting because in conflict with  CC(Complete revamp of float/promotion sympy handling) which needs to be reverted""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase b main, started a rebase job onto refs/remotes/origin/main. Check the current status here,Rebase failed due to   Raised by https://github.com/pytorch/pytorch/actions/runs/9419893823
deepseek,Best approach for dynamically changing batch sizes on iterable datasets," üöÄ The feature, motivation and pitch The DeepSeek V2 paper proposed a training methodology where both the LR and the batch size were on a scheduler. Exact description is below, however essentially: * Increased total batch size from 2304 to 9216 over the first ~3% of training, or translated as a perdevice bs of 288 > 1152, starting pct of 25% > 100%, over ~2.7% of the total steps before staying constantly at 9216 * They also used warmupandstepdecay for the scheduler !image I'd like to have an approach that lets us dynamically modify the batch size on the fly (on a perbatch basis) for IterableDatasets. We can potentially do this via a wrapper `batch_sampler` (which accelerate can then wrap around to do perdevice dispatching), however we'd need an approach that works for both iterable and noniterable datasets to implement this fully.  The FR is here in ü§ó Transformers: https://github.com/huggingface/transformers/issues/31222  I know back in the day `fastai` let you just modify the `DataLoader` batch size on the fly, because of how they drew from their custom datasets. Would be really nice if we could have this in PyTorch in some capacity, to let open science flourish with all of the new ideas introduced in the DSv2 paper!  Alternatives _No response_  Additional context _No response_ ",2024-06-05T10:58:01Z,feature module: dataloader triaged module: data,open,8,1,https://github.com/pytorch/pytorch/issues/128015,"Thanks  for the issue! I think  had an idea that may work, although we need to think through different scenarios like when prefetch is enabled and whether that will give unpredictable results"
gpt,"[GPT-fast benchmark] Add MLP, gather + gemv, gemv micro benchmark",Output example: ,2024-06-05T06:13:35Z,Merged ciflow/trunk topic: not user facing ciflow/inductor-micro-benchmark,closed,0,5,https://github.com/pytorch/pytorch/issues/128002, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
chat,Problems with starting chatTTS 'native webui," üìö The doc issue I am a selftaught junior developer who ran into these problems when starting webui with chatTTS. Do you have any predecessors to help guide me. (I'm running on ubuntu) The problems are as follows:  File ""/home/agi/ChatTTS/webui.py"", line 113, in      main()   File ""/home/agi/ChatTTS/webui.py"", line 104, in main     chat.load_models()   File ""/home/agi/ChatTTS/ChatTTS/core.py"", line 62, in load_models     self._load(**{k: os.path.join(download_path, v) for k, v in OmegaConf.load(os.path.join(download_path, 'config', 'path.yaml')).items()}, **kwargs)   File ""/home/agi/ChatTTS/ChatTTS/core.py"", line 83, in _load     vocos = Vocos.from_hparams(vocos_config_path).to(device).eval()   File ""/home/agi/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1160, in to     return self._apply(convert)   File ""/home/agi/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 810, in _apply     module._apply(fn)   File ""/home/agi/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 810, in _apply     module._apply(fn)   File ""/home/agi/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 810, in _apply     module._apply(fn)   File ""/home/agi/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 857, in _apply     self._buffers[key] = fn(buf)   File ""/home/agi/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1158, in convert     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking) RuntimeError: CUDA error: out of memory CUDA kernel errors might be asynchronously reported at some other API call, so the ",2024-06-05T04:11:54Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/127997,"This means you ran out of CUDA memory during the program. As this is likely not a PyTorch library error, I would suggest raising up this problem with ChatTTS or the forum. https://discuss.pytorch.org/ Closing issuefeel free to reopen if it is actually a PyTorch issue."
yi,[dtensor][experiment] experimenting with displaying distributed model parameters and printing sharding info,  CC([dtensor][experiment] experimenting with displaying distributed model parameters and printing sharding info)  CC([dtensor][experiment] experimenting with displaying model parameters) **Summary** Example code to display distributed model parameters and verify them against ground truth. Also prints sharding information.  **Test Plan** torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/display_sharding_example.py ,2024-06-05T01:00:16Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/127987, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 14 jobs have failed, first few of them are: inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_timm, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.4py3.10gcc9sm86 / test (inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), pull / linuxdocs / builddocspythonfalse, inductor / cuda12.1py3.10gcc9sm86 / test (inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductor / cuda12.4py3.10gcc9sm86 / test (dynamic_inductor_timm, 1, 2, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""unrelated CI failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Add SinusoidalPositionalEmbedding module for use in Transformers and Diffusion models," üöÄ The feature, motivation and pitch Current state of the art algorithms in generative AI (namely transformers and diffusion algorithms) make use of positional embeddings. In pytorch, this functionality is not yet implemented as a separate module, which would be useful for developers trying to create stand alone versions of these algorithms or simply trying to use the concept of positional embeddings in general.   Alternatives There was some discussion about adding a positional embedding layer directly to the transformer model here). I believe that creating a separate module for the positional embedding would allow for its use in other models as well (such as diffusion models) ",2024-06-04T18:19:25Z,module: nn triaged needs research,open,0,0,https://github.com/pytorch/pytorch/issues/127932
yi,[Traceable FSDP2] Use custom ops for AllGather copy-in / copy-out and ReduceScatter copy-in,Making these operations into custom ops helps Inductor identify these ops and enforce the FSDP communication op ordering.    CC([Traceable FSDP2] Use custom ops for AllGather copyin / copyout and ReduceScatter copyin) ,2024-06-04T04:46:46Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/127856,"Approach here sounds good, but check lint :)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,use `OffloadPolicy`,  CC(use `OffloadPolicy`)  CC([FSDP2] enable CI for torch.compile(root Transformer)) Summary: Test Plan: Reviewers: Subscribers: Tasks: Tags: ,2024-06-04T02:46:29Z,oncall: distributed topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/127849
transformer,[FSDP2] enable CI for torch.compile(root Transformer),"  CC([FSDP2] enable CI for torch.compile(root Transformer)) This CI showcases FSDP2 works with `torch.compile` root model, since FSDP1 can do the same  compiling root Transformer without AC: `pytest test/distributed/_composable/fsdp/test_fully_shard_training.py k test_train_parity_multi_group` compiling root Transformer with AC: `pytest test/distributed/_composable/fsdp/test_fully_shard_training.py k test_train_parity_with_activation_checkpointing` ",2024-06-04T00:47:16Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,5,https://github.com/pytorch/pytorch/issues/127832, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,[FSDP2] Allowed `List[nn.Module]` as arg,"  CC([FSDP2] Allowed `List[nn.Module]` as arg)  CC([Reland][PTD] Relaxed `contract` to allow `Sequence[nn.Module]` (127773)) This PR allows `fully_shard`'s first argument to be `List[nn.Module]` instead of strictly `nn.Module`. This allows more flexible grouping of modules/parameters for communication, which can lead to memory savings and/or more efficient communication. **Approach** At a high level, we can think of a model as a tree of modules. Previously, we could only select specific module nodes in this tree as representing one FSDP parameter group. With this PR, we can select a group of module nodes, effectively becoming a single super node. To implement the runtime schedule, we define new forward hooks that run based on the following semantics:  If a module is the first to run the prehook, actually run the given prehook. Otherwise, the prehook is noop.  If a module is the last to run the posthook, actually run the given posthook. Otherwise, the posthook is a noop.  First and last are determined by scoreboarding against a set of the modules.  This set must get cleared at the end of backward in the case that >=1 module in the list is never used, in which case we still want the forward hooks to run in the next forward after this backward. Beyond these new forward hooks, everything else is some simple generalization from `Module` to `List[Module]` or `Tuple[Module, ...]`. **Examples** This PR enables wrapping Llama models more efficiently by grouping the final norm and output linear together: https://github.com/pytorch/torchtitan/pull/382. If at least one of the modules in the list does not run forward before backward, then there will ",2024-06-03T18:49:07Z,oncall: distributed Merged Reverted ciflow/trunk ciflow/inductor release notes: distributed (fsdp2),closed,0,6,https://github.com/pytorch/pytorch/issues/127786, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert c ghfirst m ""bottom pr from the stack is failing on internal error""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,Closing in favor of https://github.com/pytorch/pytorch/pull/130949
yi,[inductor] Simplify multi-kernel codegen by unifying kernel args,"  CC([TESTING] Temporarily enable multikernel)  CC([BE] Use assertEqual in MultiKernel tests)  CC([inductor] Simplify multikernel codegen by unifying kernel args)  CC([inductor] Fix splitscan interaction with multikernel) Persistent kernels are sometimes able to remove intermediate buffers that would otherwise be needed for the nonpersistent reduction kernel. This makes multi kernel's codegen more complicated as it needs to drop these extra arguments at runtime after selecting the correct kernel to run. Instead, this PR updates the persistent kernel's `must_keep_buffers` so these aren't dropped during codegen so both kernels have the same signature. ",2024-06-02T22:56:37Z,open source Merged module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/127724,Looks good overall. Can you do a perf test with multi_kernel temporarily enabled? Just to make sure there are no extra errors not captured by current unit tests.,"I've opened CC([TESTING] Temporarily enable multikernel) and it caught a few failures, one of which was this PR but the rest seem to be preexisting.   `test_evict_last_non_coalesced_loads` is failing because it counts generated kernels, not a big deal.  AOTInductor tests seem to be completely broken though. I've opened CC([inductor] MultiKernel doesn't compose with AOTInductor) to track that.", ping,letting  take this one
rag,[BE] enable UFMT for `torch/storage.py`,  CC(Resolve circular dependence between `torch.autograd` and `torch.nn.parameter`)  CC([BE][Easy] enable UFMT for `torch/nn/`)  CC([BE] enable UFMT in `torch.utils.data`)  CC([BE] sort imports in `torch.utils.data`)  CC([BE] enable UFMT for `torch/storage.py`) Part of CC(Enable UFMT on all files in PyTorch)   CC(Enable UFMT on all files in PyTorch) ,2024-06-02T12:36:54Z,module: lint open source better-engineering module: amp (automated mixed precision) Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor suppress-bc-linter,closed,0,0,https://github.com/pytorch/pytorch/issues/127706
finetuning,torch.Tensor for optimizer parameters," üöÄ The feature, motivation and pitch The LR allows torch.Tensor, see:  CC(Using torch.compile, batch training step time takes long to converge when adding a LR Scheduler)issuecomment1973390203 But the other parameters don't, such as AdamW's beta2. This causes torch.compile to fail (fallback to eager) when I compile an optimizer with changing parameters, since torch.compile does not allow nonconstant floats. Allowing changing the other (nonLR) parameters would be helpful because: 1. beta1 (momentum) needs a scheduler as the eigenvalue distribution changes 2. beta2 needs a scheduler when finetuning a model, if the optimizer states aren't loaded or if the data distribution changes. Otherwise, a long warmup is required (~2001000 steps depending on beta2). I'm currently having problems with DCP optimizer saving, so this matters to me. 3. weight decay needs a scheduler if the math is done correctly  Alternatives Either `torch.compile` working with floats or `AdamW` allowing torch.tensor params would work for me.  Additional context _No response_ ",2024-06-02T00:19:16Z,module: optimizer triaged needs design,open,0,0,https://github.com/pytorch/pytorch/issues/127699
yi,[BE][Ez]: Apply PYI059 - Generic always come last,"Generic baseclass should always be last or unexpected issues can occur, especially in nonstub files (such as with MRO). Applies autofixes from the preview PYI059 rule to fix the issues in the codebase. ",2024-06-01T14:51:36Z,open source better-engineering Merged ciflow/trunk release notes: dataloader topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/127685, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / test (default, 3, 3, macosm1stable) Details for Dev Infra team Raised by workflow job "," merge f ""preexisting flaky test"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[BE][Ez]: Enable ruff PYI019,Tells pytorch to use typing_extensions.Self when it's able to.,2024-06-01T14:28:06Z,open source better-engineering Merged ciflow/trunk release notes: onnx topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/127684, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / test (default, 2, 3, macosm1stable) Details for Dev Infra team Raised by workflow job "," merge f ""looks like an unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,module 'torch.mps' has no attribute 'device'," üêõ Describe the bug I'm getting the error `module 'torch.mps' has no attribute 'device'` trying to train a `sentencetransformer model (which uses the huggingface transformers trainer). The error is raised on this line > https://github.com/pytorch/pytorch/blob/7ef7c265d4361691dc4cf54152db083de3215fbf/torch/utils/checkpoint.pyL183 Because the mps model doesn't have a `device()` method https://github.com/pytorch/pytorch/blob/7ef7c265d4361691dc4cf54152db083de3215fbf/torch/mps/__init__.py  Versions Collecting environment information... PyTorch version: 2.3.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.5 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: Could not collect Libc version: N/A Python version: 3.10.14 (main, May 25 2024, 13:44:20) [Clang 15.0.0 (clang1500.3.9.4)] (64bit runtime) Python platform: macOS14.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Pro Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] torch==2.3.0 [conda] Could not collect ",2024-06-01T04:45:18Z,triaged actionable module: mps,open,0,4,https://github.com/pytorch/pytorch/issues/127676,I am getting the same bug under Python 3.13 and OS 14.3.1. I got it under both torch 2.2.2 and torch 2.3.,Hey! I think there are two ways we can go about this one:  Update the mps device_module to add this function to set the current device.  Change the checkpoint code to only call this function if the current device is not the same as the device we want to set (so that device/accelerator that only ever have one device don't need to worry about it). ,"For what it's worth, I only encounter this error when using CachedGISTEmbeddingLoss or CachedMultipleNegativesRankingLoss. The noncached versions of those same loss functions do not create the error.",> Hey! >  > I think there are two ways we can go about this one: >  > * Update the mps device_module to add this function to set the current device. > * Change the checkpoint code to only call this function if the current device is not the same as the device we want to set (so that device/accelerator that only ever have one device don't need to worry about it). I like option 1. We would need this method later for Triton work.. Updating current PR with it. 
transformer,Illegal memory access resulted from pointwise autotuning of a cat-like kernel," üêõ Describe the bug I ran into this issue when applying torch.compile transformer blocks in torchtitain. When running llama_70b w/ local batch_size=4, both eager mode and torch.compile works. However, when running with batch_size=8, eager works but torch.compile fails with ""illegal memory access"". Originally I thought the issue could be OOM manifest as IMA, but the issue persisted when I lowered the memory usage by reducing the number of layers. I stepped through the generated code, the issue originated from the autotuning a splitcatlike kernel. It is reproducible when isolated (see minified repro).  Error logs   Minified repro  ",2024-05-31T22:41:50Z,triaged oncall: pt2 module: inductor,closed,1,4,https://github.com/pytorch/pytorch/issues/127652,is there an easy way of running the original code ? (maybe just post the fx_graph_runnable from TORCH_COMPILE_DEBUG), that's a great idea. Here it is: ,"computesanitizer shows the following:  ========= Invalid __global__ read of size 16 bytes                                        =========     at triton_+0x5d0 in /tmp/torchinductor_root/ja/cja7fko43klibgjaatfwng6o3zjzzszdenrb2pmxdluy5j7qrote.py:66                                                           =========     by thread (3,0,0) in block (327723,0,0)                                     =========     Address 0x7efda5015830 is out of bounds                                     =========     and is 5553170384 bytes before the nearest allocation at 0x7efef0000000 of  size 939524096 bytes                                                 and the line 66 is:  `66     tmp40 = tl.load(in_ptr0 + ((2642411520) + x0 + (67108864*x1)), tmp39, other=0.0)    .to(tl.float32)`",The issue is that we're using 32 bit indexing but `2642411520` requires 64 bits. I'll look into fix
llama,[export] Errors out when unflattening TorchTitan," üêõ Describe the bug  Background TorchTitan (mimic of LLaMA) has a forward function that is similar to the mini repro below:  The above forward function is a mimic of TorchTitan's forward:  That is, a buffer is registered at root level, and passed to each layer's forward as an input argument.  Repro Here are simple steps to export and unflatten it:   Error 1: The export step works fine, but the unflatten step hits the following error:   Error 2: If we comment out the assert from torch/export/unflatten.py (which is a self check), and continue with a run, we will hit a runtime error:  This is expected because `layers.0` is expecting two inputs, but the parent module is feeding only one (i.e. the `x`).   Versions pytorch nightly Cc      Cc  Huang   ",2024-05-31T21:04:59Z,oncall: export,open,0,1,https://github.com/pytorch/pytorch/issues/127643,Proposing this PR https://github.com/pytorch/pytorch/pull/127607 to fix the above issue. Would appreciate it if export expert could take a look. The fix is needed to unblock pipelining's release in PyTorch 2.4 and TorchTitan's PP support. Thanks!
yi,[dtensor][experiment] experimenting with displaying model parameters, **Summary** Example code to display model parameters and verify them against ground truth. Also expanded on moduletracker to accomplish this.  **Test Plan** python3 torch/distributed/_tensor/examples/display_sharding_example.py  CC([dtensor][experiment] experimenting with displaying distributed model parameters and printing sharding info)  CC([dtensor][experiment] experimenting with displaying model parameters)  CC([dtensor][debug] added c10d alltoall_ and alltoall_base_ to CommDebugMode)  CC([dtensor][debug] added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing_ to CommDebugMode) ,2024-05-31T17:41:34Z,oncall: distributed Merged topic: not user facing ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/127630
agent,"Segfault, possibly due to recursion limit "," üêõ Describe the bug I am occasionally getting a segfault towards the end of my programs. When I run in GDB to see the stack trace, I notice about ~20K frames of alternating between `torch::(anonymous namespace)::CudaIPCSentDataDelete(void*)` and `torch::CudaIPCSentData::~CudaIPCSentData()`, so I think with a stack frame this large its segfaulting due to the size.  I can maybe try to get a minimal example and update this post if wanted, but I'm working on a fairly large (closed) codebase where I'm using a model with shared memory (`model.shared()`), spawning long running multiple processes using the torch multiprocessing library, and doing multiprocess inference to gather data (think like a search algorithm with a different problem instance for each process) and training in the single master process. The inference code and training works perfectly fine, I only segfault on cleanup. I am creating my Pool as such:  and I am closing the pool as such (where I think it segfaults):  EDIT: I will note that I am using a global seed for experiments, and depending on the seed I am using, I will get this crash or not. There are experiments that can complete and clean up without crashing, so it can be a bit tricky to reproduce. Here is my stacktrace when it segfaults (note that this repeats for the 28K more stack frames:   Versions  ",2024-05-31T16:12:51Z,module: crash module: multiprocessing triaged,open,0,0,https://github.com/pytorch/pytorch/issues/127622
rag,Validate tensor storage before a direct access to it,"Hi! Tried to fix an old issue with a direct access to a tensor's storage. When tensor is loaded from file, its metadata (sizes and strides) are parsed as separate values and set to tensor without checking it corresponds with a storage size. A previous fix (https://github.com/pytorch/pytorch/pull/109792) tried to validate storage allocation and sizes/strides values during unpickling (when tensor is created), so it failed to pass tests with noncontiguous and metatensors. This fix validates that storage have enough memory directly before accessing to it. Fixes CC(Heapbufferoverflow during tensor unpickling) ",2024-05-31T15:38:44Z,module: cpu triaged open source Stale release notes: quantization,closed,0,1,https://github.com/pytorch/pytorch/issues/127619,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llama,[export][unflatten] More strictly respect scope when removing inputs,"  CC([export][unflatten] More strictly respect scope when removing inputs) Code snippet from TorchTitan (LLaMa):  `self.freqs_cis` is a buffer of root module (`self`). It is also an explicit arg in the call signature of original `layer` modules. If not respecting scope  `freqs_cis`'s scope only corresponds to root  `_sink_param` can remove `freqs_cis` from `layer`'s call signature, resulting in runtime error. There are two fixes in this PR: 1. We filter out the `inputs_to_state` corresponding to the current scope, using existing code that does prefix matching. 2. We delay the removal of param inputs from `call_module` nodes' `args`, till `_sink_param` call on that submodule returns. The return now returns information on which input is actually removed by the submodule, thus more accurate than just doing:  Before the PR: !Screenshot 20240531 at 1 40 24‚ÄØAM After the PR: !Screenshot 20240531 at 1 43 41‚ÄØAM",2024-05-31T07:59:53Z,Merged,closed,0,2,https://github.com/pytorch/pytorch/issues/127607, merge f 'Minor update to add comments. All PR tests have passed in previously'," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
gpt,Disable one of the GPT2 SDPA patterns for single-thread case,"In CC([inductor][cpu]GPT2ForSequenceClassification AMP static/dynamic shape default/cpp wrapper single thread accuracy crash), it was reported that for BF16 dtype, accuracy is poor when only one thread is used for inference on CPU & Inductor uses one of the two SDPA patterns for GPT2 to offload compute to `torch.nn.functional.scaled_dot_product_attention`. Couldn't find its rootcause, so disabling this pattern for now, when only one thread is used (which probably isn't done in practice, except in some benchmarks). This issue isn't observed with multiple threads, or for FP32 datatype. It's not observed on Intel Xeon Ice Lake SP machines either, and is specific to a single thread config with processor supporting at least AVX512_BF16 ISA. Its rootcause is not known at this point. Digging. Thanks! ",2024-05-31T02:49:55Z,open source module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/127594,"Hi fangintel, the failure occurred with BF16 datatype on a Intel Xeon 4th gen machine, which has AVX512_BF16 & AMX_BF16 ISAs. Without these ISAs, on current PyTorch CI machines (Ice Lake SP or Eager Lake SP), BF16 would be converted to FP32 before computation. Xeon 4th generation CI machines were recently added but it seems no CI jobs are currently running on them. However, CC([CI] Enable amp accuracy check for inductor cpu) would enable CPU AMP CI on those machines. Thanks!",Found rootcause
gpt,"[CI] Comment hf_T5_generate, hf_GPT2 and timm_efficientnet in inductor cpu smoketest for performance unstable issue",Fixes CC(UNSTABLE inductor / linuxjammycpupy3.8gcc11inductor / test (inductor_torchbench_cpu_smoketest_perf)) ,2024-05-31T01:41:01Z,triaged open source Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,12,https://github.com/pytorch/pytorch/issues/127588, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `xiangdong/update_smoketest_target` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout xiangdong/update_smoketest_target && git pull rebase`)", rebase b main, started a rebase job onto refs/remotes/origin/main. Check the current status here,"Successfully rebased `xiangdong/update_smoketest_target` onto `refs/remotes/origin/main`, please pull locally before adding more changes (for example, via `git checkout xiangdong/update_smoketest_target && git pull rebase`)", merge," Merge failed **Reason**: Approvers from one of the following sets are needed:  ONNX exporter (BowenBao, justinchuby, liqunfu, shubhambhokare1, thiagocrepaldi, ...)  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,[GPT-fast benchmark] Merge GPT-fast and micro benchmark output as one CSV file,"Consolidate GPTfast models benchmark with microbenchmark, and save output as one CSV file with the same format as https://github.com/pytorch/pytorch/pull/126754issue2307296847.",2024-05-31T00:08:38Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/127586, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Re trying Support min/max carry over for eager mode from_float method,Summary: Original commit changeset: 2605900516c8 Original Phabricator Diff: D57977896 Test Plan: Re enabling due to prod failure Reviewed By: jerryzh168 Differential Revision: D57978925,2024-05-30T22:03:44Z,fb-exported Merged ciflow/trunk release notes: quantization release notes: AO frontend,closed,0,12,https://github.com/pytorch/pytorch/issues/127576,This pull request was **exported** from Phabricator. Differential Revision: D57978925,This pull request was **exported** from Phabricator. Differential Revision: D57978925,This pull request was **exported** from Phabricator. Differential Revision: D57978925,This pull request was **exported** from Phabricator. Differential Revision: D57978925,"meta has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D57978925,This pull request was **exported** from Phabricator. Differential Revision: D57978925," f ""All green marked but diff got messed up""", f merge," merge ""All green marked but diff got messed up""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[export] Cannot mutate tensors with frozen storage," üêõ Describe the bug  Today this will fail with ""Cannot mutate tensors with frozen storage"" error because right now we ban all kinds of mutation on converted tensors from _to_copy().  Internal post: https://fb.workplace.com/groups/1075192433118967/permalink/1438891976749009/ Will have a discussion with  about the possible options moving forward.  Versions nightly ",2024-05-30T21:30:37Z,oncall: export,open,2,2,https://github.com/pytorch/pytorch/issues/127571," CC([export] Cannot mutate tensors with frozen storage) The error ""Cannot mutate tensors with frozen storage"" occurs because recent changes prevent mutations on tensors converted with _to_copy(). To address this, we can clone the tensor before performing inplace operations to ensure it can be mutated. def test_inplace_masked_fill(self):     class Model(torch.nn.Module):         def _init_(self):             super()._init_()             self.dtype = torch.float32         def forward(self, context_mask):             mask = torch.full((tgt_len, tgt_len), torch.finfo(self.dtype).min, device=device)             mask = mask.to(self.dtype)             mask = mask.clone()   Clone the tensor to ensure it can be mutated             mask.masked_fill_(context_mask, torch.finfo(self.dtype).min)             return mask     model = Model()     ep = export(model, (torch.ones(1, 1, 1, 1),))",I got the same error.. So is it expected?
finetuning,Modeling ViT does not support quantized models,"I am trying to perform finetuning with different quantized models of ViT and it seems that modeling_vit.py does not support it. When:   I have the error:  `NotImplementedError: Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_dynamic' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].` and if I do this other quantization:   I have: ` expected_dtype = self.embeddings.patch_embeddings.projection.weight.dtype AttributeError: 'function' object has no attribute 'dtype' ` I would like to be able to train a ViT model with quantization. Thank you :)  Alternatives _No response_  Additional context _No response_ ",2024-05-30T10:51:57Z,oncall: quantization,open,0,3,https://github.com/pytorch/pytorch/issues/127521,"Related:    CC([feature request] `quantized::linear_dynamic` on CUDA/eager, and other quantized and low-level int8 operators (matmul, gemm etc) on CUDA + integrate LLM.int8 + integrate ZeroQuant?)",`quantized::linear_dynamic` is only implemented for cpu I think,for vit we have some GPU quantization example in https://github.com/pytorch/ao/blob/main/tutorials/quantize_vit/run_vit_b_quant.py
rag,[NestedTensor] Extend coverage for unbind when ragged_idx != 1,"Summary: Extend coverage for the `NestedTensor` `unbind` operator to cases in which `ragged_idx != 1`. Currently, the `unbind` operator in the `NestedTensor` class splits a tensor along the 0th dimension, where the `ragged_idx` property, which controls the jagged dimension upon which `unbind` splits, is 1. This diff extends support for `ragged_idx != 1` in `NestedTensor`s, allowing `unbind` to split a tensor along a jagged dimension greater than 0 for `NestedTensor`s with and without the `lengths` property. Test Plan: Added the following unit tests: `test_unbind_ragged_idx_equals_2_cpu`, `test_unbind_ragged_idx_equals_3_cpu`, and `test_unbind_ragged_idx_equals_last_dim_cpu` verify that `unbind` works for all jagged dimensions greater than 1, for `NestedTensor`s without `lengths`.  `test_unbind_with_lengths_cpu` and `test_unbind_with_lengths_ragged_idx_equals_1_cpu` verify that `unbind` works when the jagged dimension is 1, for `NestedTensor`s with `lengths`.  `test_unbind_with_lengths_ragged_idx_equals_2_cpu` and `test_unbind_with_lengths_ragged_idx_equals_3_cpu` verify that `unbind` works when the jagged dimension is greater than 1, for `NestedTensor`s with `lengths`.  `test_unbind_with_lengths_ragged_idx_equals_0_cpu` verifies that `unbind` fails when the jagged dimension is 0 (the batch dimension), for `NestedTensor`s with `lengths`.  `test_unbind_with_lengths_ragged_idx_equals_2_bad_dim_cpu` verifies that `unbind` fails when there is a mismatch between the offsets and the jagged dimension, for `NestedTensor`s with `lengths`.  `test_unbind_with_wrong_lengths_cpu` verifies that `unbind` fails when the lengths exceed the limitations set",2024-05-30T00:55:00Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,16,https://github.com/pytorch/pytorch/issues/127493,This pull request was **exported** from Phabricator. Differential Revision: D57942686,The committers listed above are authorized under a signed CLA.:white_check_mark: login: jananisriram  (3cbc1d3e8fc7b05a2a8372cf8d63cb86c0dae34f),/easycla,/easycla,This pull request was **exported** from Phabricator. Differential Revision: D57942686,This pull request was **exported** from Phabricator. Differential Revision: D57942686,This pull request was **exported** from Phabricator. Differential Revision: D57942686,This pull request was **exported** from Phabricator. Differential Revision: D57942686,This pull request was **exported** from Phabricator. Differential Revision: D57942686,This pull request was **exported** from Phabricator. Differential Revision: D57942686,This pull request was **exported** from Phabricator. Differential Revision: D57942686,This pull request was **exported** from Phabricator. Differential Revision: D57942686," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D57942686, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Multiplying a sparse CSR tensor by a strided vector nondeterministically fails on MacOS ARM64," üêõ Describe the bug If `T` is a sparse CSR tensor of size `(r,c)` and `v` is a dense tensor of size `c`, `torch.mv(T, v)` (or `T @ v`) nondeterministically and incorrectly produces in `NaN` for some resulting values, but only on macOS ARM64. Since this behavior is nondeterministic, it is difficult to provide a fewline reproducer. I discovered the bug with Hypothesis, which can reliably generate test cases that uncover it. It seems to be fully nondeterministic ‚Äî¬†the same inputs will sometimes produce a correct result and sometimes incorrect. I originally discovered this through another code base, but have created a Git repo that contains a minimal test case + Hypothesis example generator to reproduce the bug: https://github.com/mdekstrand/torchspmvbugrepro The failure is demonstrated (along with `collect_env.py` output) in the included GitHub Actions workflow:  This test fails only on macOS on Apple Silicon. I can reproduce the failure both on GitHub Actions and on my MacBook. The test passes on x8664 on Windows, Linux, and macOS. It also passes locally in Linux aarch64 (in Parallels on my MacBook). These tests are using the official PyTorch Conda packages for Linux, Windows, and macOS x8664, and the condaforge PyTorch packages for macOS x8664 and Linux aarch64. COO tensors seem to work correctly; the bug reproduces with CSR tensors. I have also tested CSC and seen it fail, but the current docs do not indicate CSC is supposed to work. I have also tested (in my other codebase) using CUDA on Linux, and it also passes the tests. Only CPU on macOS arm64 seems to be affected.  Versions Collecting environment information... PyTorch version: 2.3",2024-05-30T00:43:48Z,module: sparse triaged module: arm,closed,0,2,https://github.com/pytorch/pytorch/issues/127491,"I have simplified my test case to more simply generate the failing test cases, to remove scipy from the process, and to parameterize tests to better isolate failure cases for different layouts and data types. It can also reproduce the error with CSC as well, but I'm focused on CSR right now. https://github.com/mdekstrand/torchspmvbugrepro/actions/runs/9305790596",Thanks for reporting the bug! I have a quick fix for that CC([ATen][Native] fixes sparse SPMV on aarch64)
llm,"Fix torch._dynamo.exc.Unsupported: call_method GetAttrVariable(UnspecializedNNModuleVariable(CheckpointWrapper), __dict__) __contains__ [ConstantVariable()] {}",repo:  TORCHDYNAMO_INLINE_INBUILT_NN_MODULES=1 pytest ActivationCheckpointingViaTagsTests.test_distributed_utils_checkpoint_wrapper    ,2024-05-29T16:05:45Z,triaged oncall: pt2 module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/127416,This will be fixed with   work on inlining get_attr.  This is the only failure in /test/dynamo/test_activation_checkpointing.py,Fixed on main
transformer,[inductor][cpu]abnormal performance improvement and accuracy drop for huggingface suit static/dynamic quantization, üêõ Describe the bug abnormal performance improvement and accuracy drop for huggingface suit static/dynamic quantization in 20240526 nightly release accuracy drop:   model_name  1.62   f8c4c268da67e9684f3287b7468f36a5a27c6a0b textclassification+bertbasechinese !image textclassification+albertbasev1 !image ee6cb6daa173896f8ea1876266a19775aaa4f610  textclassification+bertbasechinese !image textclassification+albertbasev1 !image  Versions SW info               SW       Branch       Target commit       Refer commit                       Pytorch       nightly       549167f       bca6d8b                 Torchbench       chuanqiw/inductor_quant       ee35d764       ee35d764                 torchaudio       nightly       1980f8a       1980f8a                 torchtext       nightly       b0ebddc       b0ebddc                 torchvision       nightly       d23a6e1       d23a6e1                 torchdata       nightly       11bb5b8       11bb5b8                 dynamo_benchmarks       nightly       nightly       nightly          Repro:  Suspected guilty commit: https://github.com/pytorch/pytorch/commit/ee6cb6daa173896f8ea1876266a19775aaa4f610 textclassification+albertbasev1staticquantaccuracydrop_guilty_commit.log ,2024-05-29T15:06:11Z,high priority triaged oncall: pt2 oncall: cpu inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/127402
transformer,4 GPT2 can't run into `_scaled_dot_product_flash_attention_for_cpu` using AOTI due to export related change in https://github.com/pytorch/pytorch/pull/123732," üêõ Describe the bug  Description Before https://github.com/pytorch/pytorch/pull/123732, when running with AOTI, the SDPA pattern can be hit and `torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default` is on the `__post_grad_graphs`. However, after this PR, the below models can't run into SDPA any more when running with AOTI. This has brought performance gap between AOTI (can't hit SDPA anymore) and Inductor (which can hit SDPA). I tried on a recent main branch (669560d51aa1e81ebd09e2aa8288d0d314407d82) and the issue is still there.  Impacted models 2 models in the HF suite (DistillGPT2, GPT2ForSequenceClassification) and 2 models in the torchbench suite (hf_GPT2, hf_GPT2_large) of the dynamo benchmark are impacted.  Steps to reproduce (I didn't turn on freezing since at that moment, freezing can't work due to other issue (fixed by https://github.com/pytorch/pytorch/pull/124350 on recent main). But the graph with or without freezing both have this issue)  In the output log Using commit https://github.com/pytorch/pytorch/commit/02ed2992d94e6bb09d95fb1409883fc61cf19e13, there's `torch.ops.aten.bmm.default` but not `_scaled_dot_product_flash_attention_for_cpu` in the `__post_grad_graphs`. Using commit https://github.com/pytorch/pytorch/commit/4f29103749c5011529f1abb10b1508a682588909 (one commit before the above):  Here're some findings: Using GPT2ForSequenceClassification as an example, I printed the graph before and after https://github.com/pytorch/pytorch/pull/123732 (the graph is dumped before this line of code: link): Before the PR where SDPA can be hit:  After the PR where SDPA can't be hit: There's on extra line **before** ",2024-05-29T07:07:59Z,triaged module: regression oncall: pt2 module: aotinductor module: sdpa,closed,0,5,https://github.com/pytorch/pytorch/issues/127383,, ,Can we solve by adding extra matching patterns?,"w , is this still a problem?",It's not an issue anymore using the latest main. I checked that we have `aoti_torch_cpu__scaled_dot_product_flash_attention_for_cpu` in the output code for the 4 models reported in this issue.
yi,torch.export.export() throws out an error when dealing weighttying model. ," üêõ Describe the bug   The jit trace could trace the NN_TiedByPassing object but export could not.  The error message is shown as follow:    Versions PyTorch version: 2.3.0+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2301 DeviceID=CPU0 Family=107 L2CacheSize=3072 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=2301 Name=AMD Ryzen 5 PRO 5650U with Radeon Graphics ProcessorType=3 Revision=20480 Versions of relevant libraries: [pip3] numpy==1.26.2 [pip3] onnx==1.15.0 [pip3] onnxruntime==1.16.3 [pip3] torch==2.3.0 [conda] Could not collect ",2024-05-28T23:15:16Z,oncall: export,open,0,3,https://github.com/pytorch/pytorch/issues/127357,"Thanks for the repro! The error is because dynamo creates an extra parameter in the state dict, `self_encoder_weight`, which aliases `self.encoder.weight`, and this causes some issues downstream. It should work if you set the `strict=False` flag in the call to `torch.export`: ","Thank you, it works. Will it be fixed or should we expect to use ""strict=False"" to export weighttying model in the long run. ","We will fix it, but you can expect to use strict=False in the long run."
llm,Change default threads on Mac to one less than number of perf cores,"Changes the number of threads on Mac to one less than the number of performant cores.  Experiments with LLMs suggest this leads to better perf on M1 mac. Specifically, I see an 11% perf improvement in llama2 perf on torchchat AOTI with this change on my M1 mac (https://fb.workplace.com/groups/pytorch.edge2.team/posts/982404176348768/?comment_id=985013399421179&reply_comment_id=985082682747584).",2024-05-28T20:40:42Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/127333,"Do you have an explanation as to why this would help? I'm worried about overfitting to some implementation quirk, particularly since this is only cited as improving AOTI and llama.cpp uses one thread per performance core."," I don't have a great explanation, other than perhaps using fewer threads than cores leads to fewer context switches (https://developer.nvidia.com/blog/limitingcputhreadsforbettergameperformance/). I noticed a similar change from 8 cores to 6 cores in ExecuTorch improved llama perf on M1 mac substantially (D57416160) (although 7 also worked well). A better way to determine this would be if we had some kind of model benchmark suite to run against. Let me try more measurements using OMP rather than the c10 threadpool.","With OMP, there is less difference between 7 and 8 threads when generating 256 llama2 tokens with AOTI:   Closing this PR."
rag,Remove tensor storage_offset/storage_bytes from the cache key,Summary: We observed differences in these fields and inductor does not specialize on them so it is safe to remove them from the key. Test Plan: CI Reviewed By: masnesral Differential Revision: D57871276 ,2024-05-28T18:29:21Z,fb-exported Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,22,https://github.com/pytorch/pytorch/issues/127319,This pull request was **exported** from Phabricator. Differential Revision: D57871276, ? Do we also need to exclude parameters from this ? Please wait a moment before landing till we confirm this is sound with parameters,This pull request was **exported** from Phabricator. Differential Revision: D57871276," it's okay to generate slightly different graphs so long as the correctness is there (small fluctuations in alignment are why we don't guard in the first place). but i guess in this case, we won't actually discover if a parameter has become no longer aligned, correct ?  a couple solutions:  in the cache key, only include whether storage_offset is aligned, not full value. in repro we looked at it was unaligned for both values. I don't love that this is a little tied to the current logic and if someone changes inductor lowering wonder how it will propagate back. Still i think we could do it.  include storage_offset only for the static_input idxs  any thoughts?","responding to  : > we won't actually discover if a parameter has become no longer aligned, correct ? correct > a couple solutions: these both sound like good ideas  I guess they're both sort of tied into the current logic, but the current logic is also already sort of repeated in a few places. I guess I have a slight preference for the second one (include storage_offset only for the static_input idxs) since it seems less likely to change?","> I guess they're both sort of tied into the current logic, but the current logic is also already sort of repeated in a few places. If we were to generalize this it'd be not quite a 1) compile time guard or 2) runtime assert but 3) some sort of aligned storage offset guard  I don't think this is worth the complexity currently. Sure, the latter sgtm. so basically  we'd only want to exclude storage_offset in hash key if tensor is not in static_inputs. we could add a set of static tensors to FxGraphCachePickler then check membership in pickling to see if we want to exclude storage_offset","not a major concern: but I was thinking about this again, and guarding/caching on storage_offset alignment actually might be preferrable, for performance in the case where we start doing caching across different machines. the scenario that I'm thinking of is that we have two identical graphs coming from different models  one where the inputs are aligned, and one where the inputs are not aligned.  In the localcaching scenario, this will probably be handled correctly; machine A, running model X, will specialize on the unguarded alignment; machine B, running model Y, will do the same; and restarts/recompiles/etc. on those machines will find the correct cache entry that gives good performance. But in the globalcaching scenario, model X (say, with aligned inputs) might populate the cache first; but model Y (say, with unaligned inputs) will read the same cache entry and get an implementation that copies the inputs, which will probably have worse performance and memory usage."," I think it is pretty unlikely to have huge performance differences because of alignment for a model. You would need to generate the same exact shape/strides for all of the inputs, and have the same graph, but have radically different alignment on tensors. That seems hard to do. Almost all of the inputs we expect to be aligned, anyway. For a little context here  we were were seeing a cache miss because two tensors had slightly different alignment across ranks. One was misaligned by 3 and the other misaligned by 2 (both misaligned).  We could also do the thing I mentioned above where we include alignment in the cache key instead of the full stride. this would also generalize to parameters/static inputs."," agree, I just meant to point out that your other suggestion (""We could also do the thing I mentioned above where we include alignment in the cache key instead of the full stride"") might be slightly better than caching on storage_offset for only parameters",This pull request was **exported** from Phabricator. Differential Revision: D57871276,  could you take a look at the code changes?,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276,This pull request was **exported** from Phabricator. Differential Revision: D57871276," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,[AOTI] Update reinplace to cover mutated buffer,"Summary: Unlike JIT Inductor, AOTI currently unlifts weights and buffers from input args, so the reinplace pass didn't really work for AOTI because it only checks mutation on placeholder, which led to excessive memory copies for kv_cache updates in LLM models. This PR removes those memory copies and roughly offers a 2x speedup. In the future, we will revert the unlift logic in AOTI and make the behvior consitent with JIT Inductor. Fixes ISSUE_NUMBER ",2024-05-28T14:14:25Z,Merged topic: not user facing module: inductor ciflow/inductor module: aotinductor,closed,0,5,https://github.com/pytorch/pytorch/issues/127297," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,[AOTI] Update reinplace to cover mutated buffer,"  CC([AOTI] Update reinplace to cover mutated buffer) Summary: Unlike JIT Inductor, AOTI currently unlifts weights and buffers from input args, so the reinplace pass didn't really work for AOTI because it only checks mutation on placeholder, which led to excessive memory copies for kv_cache updates in LLM models. This PR removes those memory copies and roughly offers a 2x speedup. In the future, we will revert the unlift logic in AOTI and make the behvior consitent with JIT Inductor. ",2024-05-28T14:05:30Z,module: inductor ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/127295
yi,"Python sometimes crashes inexplicably, with ""/var/log/kernel. log"" displaying index errors"," üêõ Describe the bug I opened two python programs  on a linux machine . using the same gpu . However sometimes python programes crash , even when i just open a program , the `/var/log/kern.log` immediately display an error message . Here are some information below   I have sought help from the NVIDIA forum, but they have asked me to seek Torch's help here's the link https://forums.developer.nvidia.com/t/linuxalwayscrashwhenrunthepythonprogramusinggpu/292905  Looking forward to a reply   Versions torch version is different   one is `2.1.1`  the  another is `2.0.0` nvidiasmi  driver version :   nvcc version   ",2024-05-28T07:09:53Z,needs reproduction module: crash module: cuda triaged,open,0,3,https://github.com/pytorch/pytorch/issues/127275,Thanks for the report. Do you have a short selfcontained script that can be used to reproduce the issue?,"Crash in UVM driver strongly suggests that at least part of the problem is there, as kernel driver should never crash, even if user input is completely wrong","> Thanks for the report. Do you have a short selfcontained script that can be used to reproduce the issue? Thank you for replying .I used the bertvits2  for inferencing  The main problem comes from not knowing the location and time of the program crash. In addition, when I use BERT VITS2 to start the program, an error immediately appears in the log. But the program can work  Also I would like to provide more informations about this error . If needed , I can share my inference info about bertvits2   my env info when using bertvits2   when i tap   the log shows  "
transformer,[Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP,Test command: `pytest rA test/inductor/test_compiled_autograd.py::TestCompiledAutograd::test_trace_run_with_rng_state`   CC([Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model)  CC([Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP)  CC([Traceable FSDP2] Add autofunctionalize support for mutable list[Tensor] (copy from Brian's PR 127347); enable E2E inductor unit test for transformer model) ,2024-05-28T01:59:21Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor keep-going,closed,0,10,https://github.com/pytorch/pytorch/issues/127247,"How did you get into this situation? I thought the run_with_rng_state HOP is used for the torch.utils.checkpoint x torch.compile design, and should not show up in Dynamo itself?  ","> How did you get into this situation? I thought the run_with_rng_state HOP is used for the torch.utils.checkpoint x torch.compile design, and should not show up in Dynamo itself?   It only appears during Compiled Autograd Dynamo tracing of the AOT bwd graph (which contains the run_with_rng_state HOP)"," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
gemma,DISABLED test_large_mmaped_weights_non_abi_compatible_cuda (__main__.AOTInductorTestNonABICompatibleCuda),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_large_mmaped_weights_non_abi_compatible_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `inductor/test_aot_inductor.py` ",2024-05-27T00:57:14Z,triaged module: flaky-tests skipped oncall: pt2 module: inductor,open,0,0,https://github.com/pytorch/pytorch/issues/127202
finetuning,Tensors of the same index must be on the same device and the same dtype except step tensors that can be CPU and float32 notwithstanding., üêõ Describe the bug Run trainer return with error    Versions   ,2024-05-26T16:12:00Z,needs reproduction module: optimizer triaged actionable,open,0,16,https://github.com/pytorch/pytorch/issues/127197,  chuang   thanks for your help. The code is about huggingface Trainer fsdp training. This bug has been solved in pytorch==2.3.0 but flash_attention doesn't support pytorch==2.3.0. I have tried to patch python code torch/optim but it seems modify a lot of C++ code. Is there any minimal change to the code to solve the issue?,"flash attention doesn‚Äôt support 2.3? that‚Äôs a surprise to me ‚Äôs not a python only patch for this fix unfortunately, but newer torch versions should support flash attention‚Ä¶","PyTorch hash flash attention support built in but I think they  are saying the flash attention package does not work with 2.3. However, Tri just released a newer package so I would double check ",     I install it again and the flash_attn works well now. My new environment is:  and the basic training code error still happens ,"I am also having the same exception. Adding  inside the `_group_tensors_by_device_and_dtype` prints the following information (right before the crash):  I am kind of guessing here, but the problem is probably caused either by the empty `max_exp_avg_sqs` tensorlist or by the `state_steps` tensorlist, that uses `int64`s instead of floats.  Some further testing revealed that the problem is indeed caused by using integers for `state[""step""]`. I am pretty sure that passing in integer steps used to work previously, so it seems like a breaking change occurred at some point. Admittedly, my code is operating on a boundary of what could reasonably considered public API (manually manipulating `optimizer.state` for some dark magic).","Ah hmm this may have to do with our change that tightened what the dtype of step could be (attempting to fix  CC(Optim.Adam 'step' default setting bug.)), since our fastest CUDA fused impl only supported fp32 for step. Is there a reason step is int64 in your use case? ","  apologies for the late response, but do you also have a printout of the dtypes and devices of the Tensors when you run in to the error?","> Is there a reason step is int64 in your use case? Well, other than the fact that the number of steps is ""conceptually"" an integer  not really. I think that at some point I might have had the opposite problem (where the steps had to be an integer tensor or torch would complain), but I am honestly not sure if I am misremembering that."," ah, that may be before my time üòõ but coming back to the main pointwould it be easy for you to have step be a float as a workaround then?  We can have an action item of allowing int step for nonfused implementations, but I may not get to it for some months.","> would it be easy for you to have step be a float as a workaround then? Oh, yes, in fact I already fixed this issue in my code. It's just that the error message might not be immediately obvious (especially if the custom code that uses integers for steps is in some library instead of users code) and this technically seems to be a regression/breaking change.","Btw, here's an example of the kind of code that would manually manipulate optimizer.state (although in this particular case `step` is a regular python int, not even a tensor, which seems to have been broken previously). Either that, or code that calls the functional APIs that implement adamw directly.","Ah yes, we had deprecated the use of python Scalar for step (and other 0d state) a few years back and so now they are all scalar Tensors. This enables features like capturability (for cudagraphs and compile). Thanks for bringing the dtype narrowing BC breakingness to my awareness.",fix that worked for me: ," thank you, the below line also fixed the issue for me: > fix that worked for me: >  >  This fix sets the default device to ""cuda"", which ensures that all tensors created after this call will automatically be placed on the CUDA device (GPU) instead of the default CPU. This includes tensors created within operations, models, or even during data loading, unless explicitly overridden.",  it is advantageous to have step stay on CPU to avoid certain kernel launches. the foreach kernels will avoid erroring if step is float32>the real fix is to have step be dtype float32!,any update on this issue? 
transformer,"When training done, the mode output same result each tensor input. ( I tried many way to debug, but can't find any way to fix it, so i guess this is a bug )"," üêõ Describe the bug  those code is simple way to show your guys. one, i make sure i am using same dataset to infer. two, my model is LSTM + Transformer encoder, so i made a method to reset cell state in each train and eval. three, i tried make eval to train + no grad to test output, but still 0 chanse is the most ( by the way, the mode is made to output one of 0, 1, 2, 3 ). when i training, with backward and optimizer, the model act normal, out put 1, 2, 0... something like that. but when i not useing grad to output, the model only output me 0, this is really big problems to me. thanks for help üò≠  Versions pytorch 2.3.0 Python 3.12.2 cuda 12.1.0",2024-05-25T16:54:17Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/127177,"It's not overfitting, because is same data for infer. It's not different weight, i checked ( save and load ). it's not eval() problem, i already check with train() + no grad.","It is unclear that this is a bug in PyTorch as there are many things that can go wrong to cause a model to not train properly for these types of questions, please check out https://discuss.pytorch.org/.","It's Label leakage by optimizer. my label like 0,0,0,0... 1,1,1,1... 2,2,2,2, so when traning, optimizer tell model 2 is right, so it will contiune output 2, because label is 2,2,2,2,2..... i'm wrong, this not a bug. without optimizer, nobody tell model what label right right now, and 0 is the answer."
transformer,"RuntimeError: ""_amp_foreach_non_finite_check_and_unscale_cuda"" not implemented for 'BFloat16'"," üêõ Describe the bug i use pytorch==2.3.0 and peft to train llama3 8b , when i run my code, its raise error like:  full of my training code like:  and the all output in terminal like: ```text C:\ProgramData\miniconda3\envs\llama\python.exe D:/codes/llm_about/selfllm/zzzzz_train/llama38B/finetune_llama3_8b.py Special tokens have been added in the vocabulary, make sure the associated word embeddings are finetuned or trained. tokenizer about:   128001 128001 Map: 100% (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22631SP0 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 555.85 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin\cudnn_ops64_9.dll HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==2.3.0 [pip3] torchaudio==2.3.0 [pip3] torchvision==0.18.0 [conda] blas                      1.0                         mkl    defaults [conda] mkl                       2021.4.0                 pypi_0    pypi [conda] mklservice               2.4.0           py310h2bbff1b_0    defaults [conda] mkl_fft                   1.3.1           py310ha0764ea_0    defaults [conda] mkl_random                1.2.2           py310h4ed8f06_0    defaults [conda] numpy                     1.24.3          py310hdc03b94_0    defaults [conda] numpybase                1.24.3          py310h3caf3d7_0    defaults [conda] pytorch                   2.3.0          ",2024-05-25T16:33:16Z,triaged module: amp (automated mixed precision) module: mta,open,0,3,https://github.com/pytorch/pytorch/issues/127176,this is my cuda cudnn infos: ,some of my dataset like: ,"For BFloat16 AMP, the grad scaler wouldn't be needed as its range is equivalent to fp32's. Could you try the script without grad scaler?"
rag,Clang tidy coverage33,Fixes ISSUE_NUMBER,2024-05-25T13:18:45Z,open source release notes: jit,closed,0,0,https://github.com/pytorch/pytorch/issues/127170
rag,Lowering for the Average Pooling 3D backward operation," üöÄ The feature, motivation and pitch Although there is an existing lowering for the 2dimensional average pooling backward operation, no such lowering exists for its 3dimensional version. Implementing this lowering could improve the performance of the operation.  Alternatives _No response_  Additional context _No response_ ",2024-05-24T17:21:35Z,triaged module: pooling oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/127101," and I are currently working on this, and we will be submitting a pull request that addresses this issue in the near future."
transformer,Significant peformance regression in DDP for torch > 2.1," üêõ Describe the bug DDP is significantly slower on torch versions after 2.1.   Requirements  Machine with multiple (4+) GPUs. I am using AWS g5.12xlarge which has 4 x A10G GPUs.  Steps to reproduce  Create two environments.   Create a file called `testddp.py` with the following code (adapted from torch ddp tutorial).   Run using `torchrun`:   Observations  Environment 1 (torch 2.1.x) shows: ~3hr training time.  Environment 2 (torch 2.3.x) shows: ~5hr training time. I had originally observed this issue in `transformers` (see here) but this looks like a torch DDP issue, not really something on `transformers` side. Note: This is also an issue in `torch==2.2.x` but here I am just showing on `torch==2.3.0`.  Expected Result Training time should be the same or at least not significantly worse than <=2.1 on newer torch versions. In my actual codebase, I am seeing up to 2x worse training time on newer torch versions.  Versions  Environment 1 (`torch==2.1.2`)  ",2024-05-24T11:18:09Z,oncall: distributed triaged,closed,11,22,https://github.com/pytorch/pytorch/issues/127077,Something that could help us a lot since you already have envs setup would be to get profiler traces from the two envs to compare. Would you be able to do that and share the traces?," If you could tell me how to do that, I'll be happy to share the traces. ","I think something like the following may work:  This should just take the profiler trace on rank 0 and save it to some `.json` file. If you could share the `.json` file, then that would be great. (We would then view them in something like chrome://tracing/ to compare the difference between the envs.)","Here are the files: torch2.1.json torch2.3.json For completeness, here is the script I ran using `torchrun nprocpernode=4 profileddp.py`:  profileddp.py.txt","Thanks! I wonder if there is a regression in NCCL across the two versions.  torch2.1: allreduces take 1.309 ms and 7.944 ms  torch2.3: allreduces take 1.952 ms and 13.987 ms  Allreduce message sizes are (1054725 * 4) bytes and (7347200 * 4) bytes, respectively For torch2.1, this equates to roughly  2 * (1054725 * 4 bytes) / (0.001309 seconds) / 1e9 = 6.446 GB/s bandwidth  2 * (7347200 * 4 bytes) / (0.007944 seconds) / 1e9 = 7.399 GB/s bandwidth where the factor of 2 is from allreduce requiring two passes around the ring. More precisely, there could be an `N1/N` factor for `N=4` GPUs, but it does not matter that much. The point is that, the achieved bandwidth on these collectives is pretty low. I am not as familiar with this part, but I think that it is possible to use https://github.com/NVIDIA/nccltests to test what kind of allreduce bandwidth you should expect to see for your hardware setup. In short, the current evidence points toward there being a NCCL regression in the allreduce times for your model/setup, and your training is communication bound. This translates to slowdown in endtoend training time. cc:   if you guys have any suggestions on how to further diagnose this"," Since this might be a NCCL regression, I wonder if we can try to do the following: 1. Get the NCCL versions for both envs, e.g. via `python c ""import torch;print(torch.cuda.nccl.version())""` 2. Run a script that only does allreduce Example with profiler:  Example with CUDA events for timing without profiler:  The latter might be simpler since then we do not need to manually inspect the profiles."," torch 2.1 NCCL version: (2, 18, 1)   torch 2.2 NCCL version: (2, 19, 3)   torch 2.3 NCCL verison: (2, 20, 5)  Code: ","Regarding the micro benchmark, since you are calling `torch.cuda.Event()` and `torch.cuda.synchronize()`, the program must know which device to perform these operations on. Otherwise, these two operations would be performed on device 0 (the default one in CUDA's view). Obviously, the non0 ranks do not have CUDA kernels on device 0. So they record almost 0 time. You just need to add a line here:  Then you would see roughly equal time:  (I measure the time on a different hardware platform, so don't take the absolute number seriously.)","And also regarding NCCL perf of different versions, I did some micro benchmarks too using https://github.com/NVIDIA/nccltests.  There seems to be no difference across versions, with the sizes you provided.  See https://gist.github.com/kwen2501/89c28b6d12045b45ccf7e33816af2713. But I was testing them on 4 x A100 with NVLinks, instead of 4 x A10g, which does not rule out the chance of a platformspecific issue. That said, another possibility may be that the regression comes from torch side.","Sure, here's the result:  torch 2.1   torch 2.3 ",Thanks! Are you familiar with running nccltests? Can you run it with NCCL 2.18.5 and 2.20.5 on your platform? Thanks!,Let me type the commands here:  You can do the above separately with `v2.18.51` and `v2.20.51` for the  `` field.,"Sorry, I may have messed up my environments in the last couple of tests. Need to take a break now. Will post an update tomorrow. ","I ran some more tests.   On AWS p4d.24xlarge (8 x A100)  `torch==2.3.0 transformers==4.30.2 accelerate==0.20.3`  `testddp.py` script in the first comment: ~50min.  Minimal script in this issue: ~41hrs.  My own training script: ~8.5hrs.  `torch==2.3.0 transformers==4.40.2 accelerate==0.30.1`  `testddp.py` script in the first comment: ~50min  Minimal script in this issue: ~41hrs.  My own training script: ~13hrs.  `torch==2.0.1 transformers==4.30.2 accelerate==0.20.3`  `testddp.py` script in the first comment: ~50min  Minimal script in this issue: ~41hrs.  My own training script: ~8.5hrs.  `torch==2.1.2 transformers==4.30.2 accelerate==0.20.3`  `testddp.py` script in the first comment: ~50min  Minimal script in this issue: ~41hrs.  My own training script: ~8.5hrs. Something is off with the second env but I am not really sure if this is a torch or transformers issue.   On AWS g5.48xlarge (8 x A10G) This is a different machine than the previous one with 4 x A10Gs. Here, something is clearly going wrong with torch.  `torch==2.3.0`  `testddp.py` script in the first comment: ~16h  allreduce test:   `torch==2.0.1`  `testddp.py` script in the first comment: ~7h  allreduce test: ",Thanks for the updates.  I think it would be helpful to focus on the clearer case for now  the A10g platform. Do you mind running the nccltests on this machine? That would help isolate torch vs NCCL. Thanks.,I am trying that and running into some errors at step `make j NCCL_HOME=../nccl/build` ,"Nevermind, I fixed the issue by symlinking to `/usr/lib/libnccl.so`.  v2.18.51   v2.20.51 ",This machine actually has 8 A10Gs:  v2.18.51   v2.20.51  Weird that bandwidth is similar when testing with 4 GPUs but worse in 2.20.5 with 8 GPUs. ü§î ,"Interesting.  If you'd like further support, you can open an issue in NCCL and link to the above A10g results.  It would be also helpful for them to know the topology of your machine:  Cc NCCL experts:   ","Indeed, I'm not sure why that is. Can you open an issue on the NCCL project (https://github.com/nvidia/nccl)? Please attach the log of a run with 2.18 and 2.20, running with the following environment variables set: `NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH` Thanks!",Thanks! Opened an issue: https://github.com/NVIDIA/nccl/issues/1298,"Given the discussion in https://github.com/NVIDIA/nccl/issues/1298, I think we can close the PyTorch issue, as the root cause is more related to NCCL and the specific user hardware setup. Feel free to reopen if there are any followups."
rag,[FSDP1] fix _same_storage check for DTensor (#123617),"for FSDP (SHARD_GRAD_OP + use_orig_params) + TP, params in the backward are DTensors. However,  ``DTensor.untyped_storage().data_ptr()`` does not work in ``_same_storage``. Thus desugar to ``DTensor._local_tensor.untyped_storage().data_ptr()``  CC(FSDP + DTensor is not working with `SHARD_GRAD_OP` + use_orig_params) credit to  for the original fix. after landing, we would not need patching in mosaic composer https://github.com/mosaicml/composer/pull/3175/files Pull Request resolved: https://github.com/pytorch/pytorch/pull/123617 Approved by: https://github.com/awgu Fixes ISSUE_NUMBER ",2024-05-23T05:13:31Z,oncall: distributed release notes: distributed (fsdp) ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/126957,"Please note, CI failures are not related. here is the noop PR showing same errors: https://github.com/pytorch/pytorch/pull/126958"
rag,[FSDP1] fix _same_storage check for DTensor (#123617),"for FSDP (SHARD_GRAD_OP + use_orig_params) + TP, params in the backward are DTensors. However,  ``DTensor.untyped_storage().data_ptr()`` does not work in ``_same_storage``. Thus desugar to ``DTensor._local_tensor.untyped_storage().data_ptr()``  CC(FSDP + DTensor is not working with `SHARD_GRAD_OP` + use_orig_params) credit to  for the original fix. after landing, we would not need patching in mosaic composer https://github.com/mosaicml/composer/pull/3175/files Pull Request resolved: https://github.com/pytorch/pytorch/pull/123617 Approved by: https://github.com/awgu Fixes ISSUE_NUMBER ",2024-05-23T04:46:36Z,oncall: distributed release notes: distributed (fsdp) ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/126955
rag,Avoid accessing storage in wrapper tensor,  CC(Avoid accessing storage in wrapper tensor)  * Prevent aot autograd metadata analysis from calling `untyped_storage()` if tensor is a wrapper * Add tests to ensure that all Higher Order Ops works with `aot_eager` ,2024-05-22T15:12:33Z,open source Stale topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/126878,The original issue ( CC(`torch.compile` fails with `jacfwd` when multiplying/dividing float and tensor)) is fixed by this PR. But `jacrev` does not work due to a different bug.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Torchscript export of `Salesforce/blip2-opt-2.7b` model fails with Conv shape error, üêõ Describe the bug Export of `Salesforce/blip2opt2.7b` with the torchscript exporter fails with conv shape error:  Here's the export script I used:  Note that the same model can export successfully with dynamo exporter (by setting `dynamo_export = True` in the script.  Versions I'm using the NGC pytorch container `nvcr.io/nvidia/pytorch:24.04py3` ,2024-05-22T03:16:45Z,oncall: jit,open,0,1,https://github.com/pytorch/pytorch/issues/126843,cc: nvidia
transformer,Torchscript export of some HF models fails with indexing errors on `past_key_values`," üêõ Describe the bug Running the following export script for the two falcon models `tiiuae/falconrw1b` and `tiiuae/falcon7b` runs into indexing errors related to `past_key_values`:  The error is:  The same model can be exported with the dynamo exporter, by setting `dynamo_export = True` in the script.  Versions I'm using the NGC pytorch  container `nvcr.io/nvidia/pytorch:24.04py3` ",2024-05-22T03:07:04Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/126840,"Similar errors on `past_key_values` are also occurring for `EleutherAI/gptj6b`, `Qwen/Qwen7BChat`, `distilbert/distilgpt2`, `bigscience/bloom560m`",cc: nvidia,Closing this bug as this error can be resolved by passing in the input data differently: 
yi,[AOTI] Add back include_pytorch for specifying link paths,"  CC([AOTI][not for review] Test cpp_wrapper mode)  CC([AOTI] Add back include_pytorch for specifying link paths)  CC([AOTI] Support _CollectiveKernel in the cpp wrapper mode)  CC([AOTI][refactor] Unify val_to_arg_str and val_to_cpp_arg_str) Summary: Running dashboard with the cpp wrapper mode sometimes hit erros like ""undefined symbol: aoti_torch_empty_stride"", although it can not be reproduced locally and seems only happen on the dashboard CI. : D57911442",2024-05-21T18:23:44Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/126802," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[dynamo][flaky tests] test_conv_empty_input_*,"  CC([dynamo][flaky tests] test_conv_empty_input_*) Run CI, maybe fixes  CC(DISABLED test_conv_empty_input_cpu_float64 (__main__.TestNNDeviceTypeCPU))",2024-05-21T16:48:26Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/126790, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,FakeTensor `forward()` on `bert` attention when using FakeTensor returns a DataDependentOutputException," üêõ Describe the bug I attempted to run the How to measure memory usage from your model without running it? code, simply replacing the model with a very basic one from Hugging Face (`bertbaseuncased`), and I was left with a `torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default`.  Some advice on how to move past this would be great, as ideally I'd like to merge this functionality in with the HF model memory estimator tool once it's stable/agnostic enough if possible! Full example:     Versions Collecting environment information... PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.0 Libc version: glibc2.35 Python version: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.5.119 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU Nvidia driver version: 535.171.04 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             16 Online CPU(s) list:                015 Vendor ID:                          Genui",2024-05-21T00:55:23Z,triaged tensor subclass module: fakeTensor module: dynamic shapes,open,0,1,https://github.com/pytorch/pytorch/issues/126738,You could try enabling this by passing in a shape env to the fake tensor constructor but I'm... not that optimistic lol
llm,[RFC] Enable PyTorch XPU on Native Windows on Intel GPUs," üöÄ The feature, motivation and pitch This RFC proposes to enable PyTorch XPU on Native Windows on Intel GPUs, following [[RFC] Intel GPU Upstreaming CC([RFC] Intel GPU Upstreaming )]( CC([RFC] Intel GPU Upstreaming )) and [[RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor CC([RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor)]( CC([RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor)) which integrated building system of SYCL kernels for ATen XPU ops on Intel GPUs for Linux with a focus on torch.compile mode. This RFC outlines enabling PyTorch XPU on Intel GPUs for Native Windows. In this RFC, Windows refers specifically to Native Windows.   **1. [Stage 0] Required Windows specific changes in PyTorch** PyTorch XPU on Windows on Intel GPU will be enabled progressively in stages. Stage 0 will focus on the PyTorch XPU source build on Windows. The first step requires building SYCL kernels for ATen XPU ops on Windows. Currently, `USE_XPU` is hardcoded to support Linux only. SYCL and SYCL runtime abstracts the underlying backend, thus the building system of SYCL kernels on Windows is the same as on Linux, following [[RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor CC([RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor)]( CC([RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor)). The only major difference is the compatible host compiler ‚Äì MSVC on ",2024-05-20T21:37:11Z,module: windows triaged intel module: xpu,closed,0,3,https://github.com/pytorch/pytorch/issues/126719,"You specifically mention > Hardware: Intel Client GPU with Intel Core Ultra processors (Code Name: Meteor Lake) Does this mean just the tile GPU for the MTL SoC is the initial focus?  At what stage will you include the ""ats150m"" target? Since a lot of Windows users will be on Alchemist dGPU.","> You specifically mention >  > > Hardware: Intel Client GPU with Intel Core Ultra processors (Code Name: Meteor Lake) >  > Does this mean just the tile GPU for the MTL SoC is the initial focus? >  > At what stage will you include the ""ats150m"" target? Since a lot of Windows users will be on Alchemist dGPU. Thanks for comments. Meter Lake is just a testing HW. After native windows support is enabled, it works for Alchemist dGPU as well.","jeancho , the RFC has been concluded. Right? If so, pls. help close this RFC."
llm,Add test coverage for fp16 matrix-vector specialized kernel,"Summary: This kernel is specialcased on ARM because it's important for LLMs, so let's have test coverage. Test Plan: Ran locally and it passes. Intentionally broke fp16_gemv_trans and saw it fail, confirming it provides coverage.",2024-05-20T18:21:41Z,Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/126700, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , merge,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.",Can't merge closed PR CC(Add test coverage for fp16 matrixvector specialized kernel)
chat,Segmentation fault with torch.compile in torchchat, üêõ Describe the bug   https://github.com/pytorch/torchchat/actions/runs/9154013584/job/25163857442?pr=824   Average tokens/sec: 38.80   Memory used: 0.00 GB   + python generate.py checkpointpath checkpoints/stories15M/stories15M.pt device cpu dtype float32 temperature 1.0 sequentialprefill compile   Using device=cpu AMD EPYC 7R32   Loading model...   Time to load model: 0.02 seconds      /exec: line 35:  4135 Segmentation fault      python generate.py checkpointpath ${MODEL_PATH} device cpu dtype ${DTYPE} temperature 1.0 sequentialprefill compile  Versions Pytorch CI ,2024-05-20T06:00:08Z,needs reproduction triaged oncall: pt2 module: aotinductor,open,0,2,https://github.com/pytorch/pytorch/issues/126669,"Issue as it's filed feels unactionable to me and probably should be closed. Fact that AOTI artifact can be used from Python, but not C++ suggest problem with C++ app rather than with AOTI runtime", 
transformer,[AOTI] Using `AOTI_TORCH_CHECK` will cause performance drop on several models compared with using `TORCH_CHECK`," üêõ Describe the bug https://github.com/pytorch/pytorch/pull/119220 replaced `TORCH_CHECK` with `AOTI_TORCH_CHECK`. We found that this change caused performance drop on several models when we were working on https://github.com/pytorch/pytorch/pull/124350. The support of freezing https://github.com/pytorch/pytorch/pull/124350 is not landed yet so the below reproducer doesn't turn on the freezing flag. Steps to reproduce: ```sh  cd path_to_pytorch  install transformers pip install transformers==4.38.1  install intelopenmp and jemalloc for performance benchmark pip install intelopenmp conda install c condaforge jemalloc export LD_PRELOAD=${CONDA_PREFIX:""$(dirname $(which conda))/../""}/lib/libiomp5.so:${CONDA_PREFIX:""$(dirname $(which conda))/../""}/lib/libjemalloc.so export MALLOC_CONF=""oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:1,muzzy_decay_ms:1"" export KMP_AFFINITY=granularity=fine,compact,1,0 export KMP_BLOCKTIME=1 CORES=$(lscpu  **Update (2024531)**: hf_T5_base and hf_T5_large in the torchbench suite also meet this issue.  Versions PyTorch version: 2.4.0a0+git314ba13 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.03) Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.28 Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.16.0x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN ",2024-05-20T04:54:19Z,triaged oncall: pt2 module: aotinductor,closed,0,4,https://github.com/pytorch/pytorch/issues/126665,Something that might help is to move the boolean condition back inline, ,Will take a look. Thanks.,"w , this should have been fixed by https://github.com/pytorch/pytorch/pull/128402. Feel free to reopen if it did not."
llama,[pipelining] add back support for multi-use parameters/buffers,"  CC([pipelining] handle param aliasing)  CC([WIP][pipelining] Add param aliasing test)  CC([pipelining] add back support for multiuse parameters/buffers)  Motivation Resolves CC([pipelining] Add back support for multiuse parameters/buffers) to support TorchTitan. With this PR, we add back support for cases where a parameter or buffer is used in multiple stages. An example of such usage is in LLaMA (torchtitan), code snippet:   Solution Step 1: Remove the previous guards of `if len(node.users) == 1`. Step 2: Call `move_param_to_callee` multiple times, one for each stage (""callee""). Step 3: Delay deletion of the `get_attr` node (for getting the param) from root till this param has been sunk into each stage that uses it. The PR also cleans up the old code around this (dropping the TRANSMIT mode and supporting REPLICATE mode only).  Test Changed the `ExampleCode` model to use `mm_param1` in multiple stages. ",2024-05-19T19:46:48Z,oncall: distributed Merged ciflow/trunk release notes: distributed (pipeline),closed,0,9,https://github.com/pytorch/pytorch/issues/126653,Which titan issue was this addressing?  something with freqs_cis?,"See CC([pipelining] Add back support for multiuse parameters/buffers). I filed it against pytorch rather than titan.  But yeah, it is wrt this code block in titan:  `freqs_cis` will be used in multiple stages once we cut the model by group of `layers`.","I pulled this PR to see if it helps run torchtitan with tracer. It does get further, no longer error during tracing, so presumably the freqs_cis thing is worked out. But there is still a tracer issue with applying TP/DP iterating the transformer layers. ",i checked the fqns and they look correct to me.  So i think this PR is good to land based on fixing the immediate issue with freqs_cis.  however will need to do more work to verify e2e ,"> I pulled this PR to see if it helps run torchtitan with tracer. It does get further, no longer error during tracing, so presumably the freqs_cis thing is worked out. >  > But there is still a tracer issue with applying TP/DP iterating the transformer layers. >  >  Thanks for checking.  The error you see is basically saying: ""I want a `ModuleDict` after split to be still a `ModuleDict`, and I want `.items()` to still work on it."" But that is currently not in pippy's contract  what's broken is broken. User code needs change to support all cases, e.g. `.items()` > `.children()`.", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[pipelining] Add back support for multi-use parameters/buffers," üöÄ The feature, motivation and pitch When running tracer mode with torchtitan, the follow `NotImplementedError` was raised:  The source code that causes the multiuse is in `Transformer`'s forward function:  The support was temporarily dropped when we refactor the tracer (_IR.py) to use unflattener. We should add it back.  Alternatives _No response_  Additional context _No response_ ",2024-05-18T22:19:03Z,oncall: distributed triaged module: pipelining,closed,1,0,https://github.com/pytorch/pytorch/issues/126626
yi,Replace torch.library.impl_abstract with torch.library.register_fake,To remove the disrupting warning   ,2024-05-18T02:24:20Z,oncall: distributed open source Merged ciflow/trunk release notes: onnx topic: deprecation topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/126606, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / test (default, 1, 3, windows.4xlarge.nonephemeral) Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: trunk / winvs2019cpupy3 / test (default, 1, 3, windows.4xlarge.nonephemeral) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llama,aot_autograd: copy metadata from fw to bw nodes,"  CC(Make inductor kernel metadata comments more descriptive)  CC(aot_autograd: copy metadata from fw to bw nodes) Summary: Uses the `seq_nr` field (introduced to aot_autograd nodes in https://github.com/pytorch/pytorch/pull/103129) to map the aot_autograd fx bw nodes to the corresponding fw nodes, and copy the metadata over. I am trusting the `seq_nr` mapping in the linked PR here. I did some validation with a toy LLaMa 3 8b training run and the mapping seemed correct. I am also trusting that the forward is single threaded, since `seq_nr` is thread local.  If this isn't always true, we'll need to also plumb `thread_id` through the same machinery which is populating `seq_nr`. I'd like to use this data in a future PR to make inductor kernels easily attributable to the nn.Module path in modeling land, to make it easier to do performance debugging. Test Plan:  Reviewers: Subscribers: Tasks: Tags: ",2024-05-17T21:00:13Z,Merged Stale module: dynamo ciflow/inductor no-stale,closed,0,1,https://github.com/pytorch/pytorch/issues/126573,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Flatten out_proj in MultiHeadAttention,"The MHA has an explicit `nn.Linear` layer for output projections, which is not consistent with the rest of the implementation (s.a. input projections). In addition to that this makes the `nn.MultiHeadAttention` dependent on the linear implementation, as well as making it a nested module.  Changes: 1. Remove `MultiHeadAttention.out_proj` 2. Add `MultiHeadAttention.out_proj_weight`, `MultiHeadAttention.out_proj_bias`. Add the functional linear for forward 3. Add initialization 4. Change expected string to hide the `out_proj` 5. Adds forward compatibility to be able to load old models  Potential issues: * Initialization: `nn.Linear` initilizes its weight as uniform Kaiming, while this PR uses uniform Xavier. In addition to that, bias in the `nn.Linear` is uniform based on fanin/fanout, while here it is constant 0. This means that numerically this will be different from the original implementation.     * *Option 1: Accept current change*  this is more consistent with the rest of the implementation     * *Option 2: Duplicate initialization logic from Linear*  this is consistent with the initialization from before this PR  Tests There are no new tests, as no new logic or change in functionality is introduced. Potentially fixes CC(torch.load non backwards compatible on Transformer between 1.8.1 and 1.9.0)  ",2024-05-17T19:48:08Z,oncall: distributed triaged open source Stale release notes: distributed (fsdp),closed,0,4,https://github.com/pytorch/pytorch/issues/126568,"'t think there is a need for the `NonDynamicallyQuantizableLinear` anymore. However, it is worth checking for quantization logic changes.","This is BCbreaking, no?"," It might be for chckpoints: in case a checkpoint is created postPR, and being loaded into prePR. I can add BCcompatibility by saving the weights as `out_proj.weight` and `out_proj.bias` with a deprecation warning, but that would mean the checkpoint statedict will have structure inconsistent with the MHA, wdyt?","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llm,`torch.compiler.allow_in_graph` does not create a `call_module` op in fx.Graph in torch 2.3.0, üêõ Describe the bug I have been testing 2.3 but noticed some diverging behaviour with `allow_in_graph`   Output for torch2.2   Output for torch2.3  Is this expected?  Versions For torch2.3.  For torch2.2   ,2024-05-17T19:35:51Z,triaged oncall: pt2 module: dynamo,open,3,12,https://github.com/pytorch/pytorch/issues/126566,"Yes, this seems expected. In this case, it looks like we inlined `x + x` directly into the graph as a `call_function`. I believe this results in faster code since it foregoes the `call_module` overhead. Is there a concern with this behavior? Btw, you no longer need to add .compiler.allow_in_graph for dynamo trace this snippet in 2.3","No, this doesn't look expected to me. I assume the reason they're allowing the module in graph is because they want it to show up as is so, e.g., a custom compiler pass can pattern match on it or something.","> e.g., a custom compiler pass can pattern match on it or something. Yes, to be more precise, we need a mechanism to hide ops in certain modules for our custom backend. `allow_in_graph` only works somewhat well for that, so I'd be happy to talk about alternatives. I added some more context here:  CC(support methods for torch.compiler.allow_in_graph)issuecomment2090768401","We discussed this at triage review. If you just want to hide an op, instead of a fullblown nn.Module, please create a custom op (see  CC(support methods for torch.compiler.allow_in_graph)issuecomment2123193375) If you want to instead hide an nn.Module  we're not sure this was intended to work in the first place and we don't support that well today.","Thanks for looking into it üôè  I always interpreted `allow_in_graph` as that the annotated function or module should be part of the graph as a `call_function` or `call_module`. Here  https://pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html it's described as > The annotated callable goes as is in the TorchDynamo graph. For example, a blackbox for TorchDynamo Dynamo.nn. Note that AOT Autograd will trace through it, so the allow_in_graph is only a Dynamolevel concept. which IMO it behaved accordingly to in 2.2, but no longer in 2.3. What is the intended usage of `allow_in_graph` then? Or will it be deprecated? > If you just want to hide an op, instead of a fullblown nn.Module, please create a custom op I see how this can be used, but makes it harder when working with code from a third party library, where it's for our backend also sometimes necessary to ignore some functions/modules. (to give more context, we sometimes need to ignore those functions as they are not relevant for our backend, but they still cause issues for the backend, then we need this escape hatch. For example deepspeed, which introduces several graph breaks). For our use case ideal would be to able to easily scope all ops within a function or module with a decorator/context manager, and that information is then visible in the meta data of the op. for example  One could use the stack trace to handle that without any changes to dynamo, but here the issue is:  the `meta['stack_trace']` isn't always complete (I think that happens because of graph breaks). Then it's not clear if the function is scoped.  `meta['source_fn_stack']` only contains the last function call (at least in 2.2)",btw I traced it back the change in behaviour back to this PR https://github.com/pytorch/pytorch/pull/116312. TBH I don't quite get the motivation behind allowing torch classes to still stay as `call_module` ops in the graph but 3rd libraries are not given that option.,"> What is the intended usage of allow_in_graph then?  allow_in_graph is really just intended for PyTorch developers, not thirdparty developers. It's a lowlevel tool we use to control what goes into the graph. As the documentation says (""Note that AOT Autograd will trace through it, so the allow_in_graph is only a Dynamolevel concept.""), it doesn't completely blackbox a Python function throughout the entire torch.compile stack. I do want to understand your use case more though. Is the problem that you have calls to multiple thirdparty APIs that induce graph breaks and you wish to avoid all of those?","> I do want to understand your use case more though. Is the problem that you have calls to multiple thirdparty APIs that induce graph breaks and you wish to avoid all of those? We are using it to replace some operations in the graph with our own custom calls to run on our hardware. However, some operations we don't want to run on our hardware (for example ops that can be calculated during compile time). I am aware it is possible to implement the logic to detect those automatically, but it's nice for the user to be able to indicate that manually instead of relying the backend to correctly detect it. For that reason, we have been using `torch.compiler.disable` and `torch.compiler.allow_in_graph` to hide that from the backend, such that it doesn't have to skip those operations. I guess we could move forward by improving the skipping logic to automatically detect this in the backend itself, but having the escape hatch would definitely be valueable. Hence the proposal for being able to annotate only the meta data of the node which seems to me like it could be less intrusive. I would assume that this isn't really the main use case for torch.compiler, it's also not what we are planning to use for the longterm but more for an experimental framework for quicker itertations. Generally, the main requirement has been to get the fx graph, before we used `torch.fx.symbolic_trace`. I have seen `make_fx` https://github.com/pytorch/pytorch/blob/main/torch/fx/experimental/proxy_tensor.pyL1395, and it some tests wasn't as robust torch.compile which is why went that route. Looking into the code, my understanding is that torch.export still uses this experimental `make_fx` function. Could you comment what's the long term plan here? My understanding is that torch.export also uses dynamo, but it seems to reimplement some part of the logic compared torch.compile.","> We are using it to replace some operations in the graph with our own custom calls to run on our hardware. However, some operations we don't want to run on our hardware (for example ops that can be calculated during compile time). Ah. You don't want `allow_in_graph` for this. You want a HOP. . That being said, something like constant propagation at compile time is pretty easy for the compiler to do, and you should make your compiler do it. Remember the graphs are all straight line and functionalized (if you're using AOTAutograd), so the trivial dependency analysis will work.", is the proposal to annotate operations that they don't want to run on their hardware with a HOP?,"Yes, for that particular use case I believe a HOP is most appropriate ","Related: someone else wanted a generic way to annotate groups of ops so a backend can do something with them ( CC(Allowing topology writer to pass hints to accelerator backends)). I don't want to allow users to write their own HOPs yet (because that involves breaking open Dynamo internals), but providing a generic annotation HOP seems like it would be sufficient"
transformer,Remove activation checkpointing tag to get correct FQNs (#124698),"Cherrypick for release branch Fixes CC(Not loading optimizer state separately from checkpoint causes errors with FQNs) When setting `use_orig_params = False` and using activation checkpointing, the FQN mapping as retrieved by the `_get_fqns` function is incorrect because the prefix that is added to the name of each activation checkpointed module, `_checkpoint_wrapped_module`, can still be present. I think this is an edge case with the `_get_fqns` function that was not addressed by this previous commit CC([DCP] Removes Checkpoint Wrapped Prefix from state dict fqns). Without the change, the list of object names for an activation checkpointed module with FSDP (and `use_orig_params=False`) can be something like:  Which will incorrectly return just one FQN, `{'model.transformer.blocks.0._flat_param'}`, when all the FQNs of the parameters of the transformer block should be returned. With the change, the list of object names will now have `_checkpoint_wrapped_module` removed:  And the FQNs are correctly retrieved and returned in `_get_fqns` when this condition is satisfied. The correct FQNs are:  Pull Request resolved: https://github.com/pytorch/pytorch/pull/124698 Approved by: https://github.com/Skylion007 Fixes ISSUE_NUMBER ",2024-05-17T18:31:35Z,oncall: distributed open source module: distributed_checkpoint,closed,0,0,https://github.com/pytorch/pytorch/issues/126559
rag,Use return_and_correct_aliasing() for NJT + compatible storage setting,"  CC((WIP) to_padded_tensor() triton kernel for NJT)  CC(Use return_and_correct_aliasing() for NJT + compatible storage setting)  CC(NJT  padded dense conversions) Fixes CC([NestedTensor] chunk fails under DEBUG=1 builds) Context: `return_and_correct_aliasing()` is required for traceable wrapper subclasses so that aliasing relationships are correct. NJT has not been using this, but needs to for correct aliasing relationships, and to avoid tripping asserts when DEBUG=1 (e.g. CC([NestedTensor] chunk fails under DEBUG=1 builds)). This PR: * Uses `return_and_correct_aliasing()` in NJT * Changes how storage setting is done in `return_and_correct_aliasing()`     * Old way: use `set_.source_Storage_storage_offset()`, which has extra logic for storage resizing that we don't need     * New way: `torch.ops.aten._unsafe_set_storage_()` that shoves in a storage without this extra logic. Notably, this avoids `computeStorageNbytes()` choking on nested ints in NJT's sizes / strides",2024-05-17T17:17:49Z,Stale topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/126552," this PR breaks some NJT stuff higher up in the stack. Hacking in the storage changes the reported `nt.device` to `""meta""`, which is not correct. I think the difficulty here comes from the fact that wrapper subclasses have null storages with the desired device hacked in: https://github.com/pytorch/pytorch/blob/980f5ac0499447cd6d39b6241ae30ae30937a3e5/torch/csrc/autograd/python_variable.cppL838L846 Combine that with this scenario during tracing: * Outer tensor has ""real"" storage reporting ""cpu"" / ""cuda:0"" / etc. (albeit null) * Inner tensors are fake with meta storages Dense > subclass views mix the two and cause problems. Not sure if this is easily addressable for NJT; might be a better approach to dodge the debug asserts without the use of `return_and_correct_aliasing()`. I'm happy to break out the part that does storage setting without extra check logic to help CC([compile time] AOT Autograd is taking long time in tracing).","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,[Inductor][CPP] Leverage full bits for BF16/FP16 vectorization,  CC([Inductor][CPP] Select tiling factor for lower precision data types)  CC([Inductor][CPP] Leverage full bits for BF16/FP16 vectorization) ,2024-05-17T02:08:29Z,module: cpu open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,15,https://github.com/pytorch/pytorch/issues/126502,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Check mergeability of ghstack PR / ghstackmergeabilitycheck Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 12 jobs have failed, first few of them are: Lint / workflowchecks / linuxjob, linuxbinarylibtorchprecxx11 / libtorchcpusharedwithdepsprecxx11test / test, linuxbinarylibtorchcxx11abi / libtorchcpusharedwithdepscxx11abitest / test, linuxbinarymanywheel / manywheelpy3_9cuda12_4test / test, linuxbinarymanywheel / manywheelpy3_9cuda12_4splittest / test Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 12 jobs have failed, first few of them are: Lint / workflowchecks / linuxjob, linuxbinarylibtorchprecxx11 / libtorchcpusharedwithdepsprecxx11test / test, linuxbinarylibtorchcxx11abi / libtorchcpusharedwithdepscxx11abitest / test, linuxbinarymanywheel / manywheelpy3_9cuda12_4test / test, linuxbinarymanywheel / manywheelpy3_9cuda12_4splittest / test Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x ff394230ce64c81652f8a4a6aa1d7066b2daec46` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[FSDP2] Fixed 2D clip grad norm test,"  CC([FSDP2] Fixed 2D clip grad norm test) This fixes  CC([FSDP2][2D] test_clip_grad_norm_2d is failing on main). We change from transformer to MLP stack since transformer seems to introduce slight numeric differences when using TP. We include a sequence parallel layer norm module in the MLP stack to exercise `(S(0), R)` placement. ",2024-05-17T01:13:16Z,oncall: distributed Merged Reverted ciflow/trunk ciflow/inductor release notes: distributed (fsdp2),closed,0,18,https://github.com/pytorch/pytorch/issues/126497,switching to fsdp2 release note is really some labor work,"Failure are all inductorrelated, not FSDP2related.", merge i," Merge started Your change will be merged while ignoring the following 9 checks: inductor / cuda12.1py3.10gcc9sm86 / test (inductor, 1, 1, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_distributed, 1, 1, linux.g5.12xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (aot_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / rocm6.1py3.8inductor / test (inductor, 1, 1, linux.rocm.gpu.2), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamic_aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m ""reverting to check if might have introduced inductor cuda 12 issues"" c ""ignoredsignal""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase s, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/awgu/588/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/126497`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8clang10 / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , rebase s, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC([FSDP2] Fixed 2D clip grad norm test), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,DISABLED test_conv_empty_input_cpu_bfloat16 (__main__.TestNNDeviceTypeCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_conv_empty_input_cpu_bfloat16` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_nn.py` ",2024-05-17T01:03:03Z,module: nn triaged module: flaky-tests skipped oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/126494,looks like same as  CC(DISABLED test_conv_empty_input_cpu_float64 (__main__.TestNNDeviceTypeCPU))
rag,[FSDP1] fix _same_storage check for DTensor (#123617),"fix FSDP crash for FSDP (SHARD_GRAD_OP + use_orig_params) + TP, params in the backward are DTensors. However,  ``DTensor.untyped_storage().data_ptr()`` does not work in ``_same_storage``. Thus desugar to ``DTensor._local_tensor.untyped_storage().data_ptr()``  CC(FSDP + DTensor is not working with `SHARD_GRAD_OP` + use_orig_params) credit to  for the original fix. after landing, we would not need patching in mosaic composer https://github.com/mosaicml/composer/pull/3175/files Pull Request resolved: https://github.com/pytorch/pytorch/pull/123617 Approved by: https://github.com/awgu ",2024-05-16T22:18:13Z,oncall: distributed release notes: distributed (fsdp) ciflow/inductor,closed,1,1,https://github.com/pytorch/pytorch/issues/126464,close this PR in favor of another same PR  https://github.com/pytorch/pytorch/pull/126957
transformer,[release/2.3] Added cublasGemmAlgo_t -> hipblasGemmAlgo_t ,This PR is to add cublasGemmAlgo_t > hipblasGemmAlgo_t to cuda_to_hip_mappings.py. It is required for DeepSpeed transformer extension build on ROCm.,2024-05-16T19:54:32Z,open source,closed,0,0,https://github.com/pytorch/pytorch/issues/126448
rag,"Revert ""Improve Storage copy_ size mismatch error message (#126280)""","  CC(Revert ""[onnx.export] Avoid unnecessary copy of debug_names (123026)"")  CC(Revert ""Warn SDPA users about dropout behavior (126294)"")  CC(Revert ""Improve Storage copy_ size mismatch error message (126280)"")  CC(Revert ""[CI] Add AMP models in inductor cpu smoketest for performance (125830)"")  CC(Revert ""Remove Caffe2 python code (126035)"")  CC(Revert ""Enable UFMT on `test/test_datapipe.py` (124994)"")  CC(Revert ""Remove expected failure in `test_eager_transforms.py` (125883)"")  CC(Revert ""[optim] Fix: wrong ASGD implementation  (125440)"")  CC(Revert ""Fix triton codegen main do_bench_gpu import error (126213)"")  CC(Revert ""[dynamo] graph break on const dict KeyError (125882)"")  CC(Revert ""[dynamo] graph break on issubclass call with nonconst args (125943)"")  CC(Revert ""[dynamo] fix  CC(KeyError `shape,stack,cos` on pennylane quantum circuit) (125945)"")  CC(Revert ""[dynamo][inlineinbuiltnnmodules] Bug fix  Only unspecialized nn modules (126303)"")  CC(Revert ""[FSDP2] support fully_shard(model_on_meta, cpu_offload) (126305)"")  CC(Revert ""Add VariableTracker.debug_repr (126299)"")  CC(Revert ""Also remove compile_time_strobelight_meta frame when generating stack (126289)"")  CC(Revert ""Make propagate_real_tensor more safe (126281)"")  CC(Revert ""Switched from parameter in can_cast to from_. (126030)"")  CC(Revert ""[easy][dynamo][inlineinbuiltnnmodules] Change test to check for params (126316)"")  CC(Revert ""[Export] Allow ExportedProgram to take empty decomp table (126142)"") This reverts commit f0d34941ddef1b7fcff7be6f6e0153cd99b52183.",2024-05-16T18:39:40Z,ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/126441
transformer,[VIT Model] [perf Degradation] [X86] [ARM] torch.compile + weight prepacking results in perf degradation for VIT Transformer model," üêõ Describe the bug With Pytorch 2.3.0, when we run inferencing for VIT DL Model on CPU's both on x86 (ICELake) and ARM (Graviton3) we see a performance degradation with  torch.compile()+weight prepacking over just torch.compile(). !image Note: Here, total time taken is in seconds for 100 iterations. Only Inference time is considered here. We can clearly observe that the performance is getting worsen after applying weight prepacking optimization which shouldn't be the case. I further did deep dive analysis and figured out the root cause for this issue in the VIT model. !image From the snapshot above, we can see that, aten::addmm op is called with torch.compile() and mkldnn::_linear_pointwise (ARM)  / mkl::_mkl_linear (x86) is called with torch.compile+weight_prepacking. We see that aten::addmm is called around 4 times in VIT model execution and each of it has a different shape. For every shape there's performance improvement with weight prepacking except 1 shape which is highlighted in green/orange. **[[3072], [197, 768], [768, 3072], [], [], [197, 3072]]** This is the reason for the perf degradation. This shape scenario has to be analyzed and fixed to improve the performance on CPU (Both X86 and ARM). **Scripts to reproduce:** vit.txt vit_with_weight_prepacking.txt **How to install:** > pip3 install torch==2.3.0 > export OMP_NUM_THREADS=32  (As this was tested on 32 core machines. m7g.8xlargeGraviton3 for ARM and c6i.8xlargeIceLake for Intel.  Error logs **Model perf degradation with torch.compile()+weightprepacking both on x86 and ARM CPU's:** !image  Minified repro _No response_  Versions **For ARM:** collect_env_arm.txt requirements.",2024-05-16T11:33:46Z,oncall: pt2 module: inductor oncall: cpu inductor,closed,0,11,https://github.com/pytorch/pytorch/issues/126391, Could you help to take a look?,"Tried on SPR with 56 threads. According to the MKL verbose, the mkl_linear kernel (the highlighted shape in the issue) time has a regression starting from a certain moment:  By writing a small test case and running torch.addmm 2000 times, we could only see the kernel perf around 250us.","257.85us > 737.93us Will the regression be further investigated and fixed? The issue is really when weight prepacking is enabled with torch.compile() as highlighted in orange in the ticket. Even the shape becomes different with weight prepacking enabled compared to [[3072], [197, 768], [768, 3072], [], [], [197, 3072]] which is just torch.compile()","There are some environment problems for the previous data. With enabling tcmalloc and iomp5 (need to install intelopenmp), the performance with weight prepack is better than that without it. Tested on Xeon SPR with 56 threads. Without weight prepacking:  With weight prepacking:   Could you try with the environment parameters mentioned above? Maybe you'd better run with the PyTorch launcher https://github.com/pytorch/pytorch/blob/main/torch/backends/xeon/run_cpu.py.","I installed intelopenmp==2024.1.2 in my python environment and retried running VIT model in compile mode (with and without prepacking) on Intel ICL with 32 cores and 32 threads. I don't see much difference in the results. I do see intelopenmp brings some slight gain with few milli seconds but I still see the above anomaly. From your data above, looks like intelopenmp helps in SPR instance case with higher number of cores and threads but it doesn't seem to be applicable in the older generation of data centric CPU's on x86 platform.",I understand one of the recommendations is probably to use intelopenmp in x86 case. Can someone point how can this issue be resolved in ARM (aarch64) CPU's? (Can someone working on ARM look into this?), Have you installed tcmalloc or jemalloc?,"Thanks  for pointing it out. I have remeasured the numbers by enabling tcmalloc as well and now I see improved numbers with weight prepacking on x86 CPU. **Without weight prepacking:**                                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                                                                   Input Shapes                                             aten::addmm        24.29%     588.338ms        27.71%     671.077ms     139.808us          4800                               [[768], [197, 768], [768, 768], [], [], [197, 768]]                                             aten::addmm        23.82%     576.808ms        25.11%     608.080ms     506.733us          1200                             [[768], [197, 3072], [3072, 768], [], [], [197, 768]]                                             aten::addmm        20.36%     492.986ms        21.93%     531.105ms     442.587us          1200                            [[3072], [197, 768], [768, 3072], [], [], [197, 3072]]   **With Weight PrePacking:**                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                                                                      Input Shapes                                           mkl::_mkl_linear        26.58%     534.245ms        26.68%     536.379ms     111.746us          4800                               [[197, 768], [2900193, 1], [768, 768], [], []]                                        mkl::_mkl_linear        23.02%     462.819ms        23.07%     463.730ms     386.442us          1200                             [[197, 3072], [5259489, 1], [768, 3072], [], []]                                        mkl::_mkl_linear        21.55%     433.074ms        21.59%     433.935ms     361.613us          1200                              [[197, 768], [5259489, 1], [3072, 768], [], []]  ",Closed as the issue was caused by wrong environment config.,"The issue seems to even go away on ARM using tcmalloc (32 core, 32 thread scenario on Grv3 Instance using the latest torch 2.3.1) **Without weight prepacking:**                                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                                                                   Input Shapes                                             aten::addmm        22.56%        1.842s        26.27%        2.145s     446.891us          4800                               [[768], [197, 768], [768, 768], [], [], [197, 768]]                                             aten::addmm        14.29%        1.167s        15.79%        1.289s       1.074ms          1200                            [[3072], [197, 768], [768, 3072], [], [], [197, 3072]]                                             aten::addmm        14.07%        1.149s        15.68%        1.280s       1.067ms          1200                             [[768], [197, 3072], [3072, 768], [], [], [197, 768]]   **With weight prepacking:**                                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg                       of Calls                                                                   Input Shapes                                               mkldnn::_linear_pointwise        19.44%        1.165s        20.62%        1.237s     257.626us          4800                               [[1, 197, 768], [768, 768], [768], [], [], []]                                             mkldnn::_linear_pointwise        15.81%     947.898ms        16.19%     970.938ms     809.115us          1200                             [[1, 197, 768], [3072, 768], [3072], [], [], []]                                             mkldnn::_linear_pointwise        15.44%     925.528ms        15.86%     950.862ms     792.385us          1200                             [[1, 197, 3072], [768, 3072], [768], [], [], []]   I understand now that tcmalloc helps scale performance with highly parallel applications. I would like to know when is this recommended generally in DL space. why isn't this shared lib part of the OSS torch binaries.",> I understand now that tcmalloc helps scale performance with highly parallel applications. I would like to know when is this recommended generally in DL space. why isn't this shared lib part of the OSS torch binaries. These thirdparty caching allocators use aggressive memory pooling algorithms which would cause larger memory usages. That is why PyTorch is using the glibc allocator by default conservatively.
transformer,RAM Not Freed on CPU After Moving Model with Multiple Transformers to CUDA," Describe the bug I encountered a memory issue when moving a model to CUDA using `model.to('cuda')`. Specifically, when using `torch.nn.ModuleList` or `torch.nn.Sequential` containing multiple instances of `torch.nn.Transformer`, the model appears to move to CUDA but does not free up the corresponding CPU memory. This results in significant RAM leakage.  To Reproduce Here is a minimal example to reproduce the issue:   Expected behavior When calling `model.to('cuda')`, all parameters and buffers should be moved to CUDA and the corresponding CPU memory should be freed.  Additional Findings 1. The issue occurs with both `torch.nn.ModuleList` and `torch.nn.Sequential`. 2. If there is only one `Transformer` module in the `ModuleList`, the issue is significantly reduced, with only around 200+ MB of context left in RAM. 3. When there are more than two `Transformer` modules in the `ModuleList`, there is significant RAM leakage. 4. The issue was first discovered while using the Whisper model. Initially, it was thought to be an issue with HuggingFace's transformer library, but it was later found to occur with any complex PyTorch model. 5. The issue does not occur with sequences containing more than two `Linear` or `LSTM` layers. 6. Using `gc.collect()` does not resolve the issue. 7. `sys.getrefcount(model)` is not 1 after the model is created, indicating additional references.  Additional context It seems that the issue might be related to the way `torch.nn.ModuleList` and `torch.nn.Sequential` handle multiple `torch.nn.Transformer` instances and their memory management when moved to CUDA.  Versions PyTorch version: 2.2.1 Is debug build: False CUD",2024-05-16T08:46:54Z,needs reproduction module: nn module: memory usage triaged,open,0,1,https://github.com/pytorch/pytorch/issues/126388,+1. also for flux models. very huge leakage
transformer,DISABLED test_ring_attention_native_transformer_is_causal_True (__main__.RingAttentionTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ,2024-05-16T05:12:26Z,module: rocm triaged skipped,open,0,0,https://github.com/pytorch/pytorch/issues/126380
rag,Re-implement pin_memory to be device-agnostic by leveraging the Accelerator concept,"This PR reimplements pin memory aiming to get rid of the optional `device` argument and makes all related APIs to be deviceagnostic. We add two new abstract APIs in AcceleratorHooksInterface and redefine pin memory as: ""Pin memory is always pinned for the current accelerator device"". In detail, it uses getAcceleratorHooksInterface in pin_memory/is_pinned to get an appropriate device and invoke the corresponding overridden interfaces, instead of using BackendSelect and then dispatching to CUDA or other specific backends' implement methods. Note: For new backends who want to implement and use pin memory, just inherit AcceleratorHooksInterface and overwrite the `isPinnedPtr` and `getPinnedMemoryAllocator` methods.  Additional context: To avoid BCbreaking, this PR just preserves the `device` arg of related APIs and would throw a deprecation warning if `device` arg is passed. Another PR will be submitted to update all PT callers (`Tensor.is_pinned()`, `Tensor.pin_memory()`...) not to pass this arg based on this PR. In future, `device` arg will be actually removed. (DataLoader's pin_memory is default to CUDA if parameter pin_memory_device is not set) Relates CC(pin_memory/is_pinned API is too CUDAcentric) ",2024-05-16T03:49:10Z,oncall: distributed triaged open source Merged Reverted ciflow/trunk release notes: mps ciflow/mps module: inductor module: distributed_checkpoint ciflow/rocm ciflow/xpu,closed,2,69,https://github.com/pytorch/pytorch/issues/126376,"  Could you help to review this? If it's reasonable, I will go on for the next step to refresh all related APIs and modify the test cases.", for you, merge, Have rebased. Can we merge it now?, Are there any other problems? Maybe we can start CI to see if any problem exists., rebase,"Hi,  I have pushed new commit to handle the failed test case. Can we trigger ci now? It seems that I don't have permissions."," Do you have time to look at the newly pushed commits?  And  ,Could you help to review the code,  especially for the part of op's registration and dispatch? ( I notice that you are the code owner of pin memory",It's probably best if  finishes off the review here,> It's probably best if  finishes off the review here Thanks! Seems that he is busy. , merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `new_pin_memory` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout new_pin_memory && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,We recently ran into an issue where calling `is_pinned` on a cpu tensor would initiate a cuda context if device was != cpu. I'm curious what the expected behavior is since merging this PR? cc:   ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / build Details for Dev Infra team Raised by workflow job ", can you open an issue with a repro for this please? This is not expected for sure but we can track that independently of this PR.," Should have included in previous comment, here is the issue:  CC(Look up tensor device member inside Tensor is_pinned() implementation instead of accepting an outside input) To be specific, I believe the issue was `is_pinned` was called on a cpu tensor, but with the default device which was 'cuda'","askarov has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","> askarov has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator. cc: , I just tried import this PR and on top of it dropped the device param to our is_pinned calls in DCP. The staging memory consumption test still passes, so looks like we should be okay reverting PR128896 after the current PR gets merged.","> We recently ran into an issue where calling `is_pinned` on a cpu tensor would initiate a cuda context if device was != cpu. I'm curious what the expected behavior is since merging this PR? >  > cc:    Only cpu tensor can be pinned, the `device` that input tensor pinned memory on is not up to itself.  Currently, CUDA is the default device to pin memory if `device` arg is not passed. After merging this PR, we change the default device from CUDA to autoselected accelerator(not including CPU).  So whether initiating the cuda context when calling `is_pinned` depends on the detailed implement of isPinnedPtr. And this PR doesn't modify the internal implement of `CUDAHooks::isPinnedPtr`.  It's better to open another independent issue to track it.","> > askarov has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator. >  > cc: , I just tried import this PR and on top of it dropped the device param to our is_pinned calls in DCP. The staging memory consumption test still passes, so looks like we should be okay reverting PR128896 after the current PR gets merged. As mentioned above, this PR aims to remove the `device` arg of `is_pinned` and `pin_memory`. I will refresh APIs in torch to not passing `device` arg. Now in this PR, it's not supported to pass `device='cpu'` in `is_pinned`. So I think PR128896 should be reverted first before this PR getting merged. And another thing to confirm,  Previously, it's allowed to pass `device='cpu'` in `is_pinned` and it will return false. But in this PR, it's not allowed and will give an error 'CPU device type not an accelerator'. So whether it should be allowed to pass `device='cpu'` for BC?","Hi , this PR has a conflict with https://github.com/pytorch/pytorch/pull/129463 and https://github.com/pytorch/pytorch/pull/129205. Please make sure to note to rebase to resolve the conflicts. Sorry for your inconvenience.",", okay, PR 129972 has been merged, this should fix the distributed/checkpoint/test_utils. Can you please try to rebase and merge?", Is there any problem? Maybe we can merge it now., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[Storage_ipc] Option II: Provides IPC extensions for 3rd devices. ,"I am working on IPCrelated interfaces to support thirdparty devices.  This is another option for IPC support for third party devices. 1. The share_cuda_ interface is changed to share_device_ 2. The backend uses getAccelerator to determine the device and distribute different devices. 3. Add the share_device implementation of thirdparty devices to PrivateUse1HooksInterface In this way, thirdparty devices and CUDA will use the same interface, and the device distinction is hidden in the backend. Users also need to manually set the device type (although these are internal interfaces). another plan: https://github.com/pytorch/pytorch/pull/125122 Do you think which is feasible? Looking forward to your suggestions Fixes CC(Add support for IPC features for PrivateUse devices)  ",2024-05-16T03:30:51Z,triaged open source Stale,closed,0,2,https://github.com/pytorch/pytorch/issues/126373,"  Thank you for your review. I have refactored the code according to the above suggestions, mainly including: 1. Put IPC related methods into AcceleratorHookInterface, making it a common method for CUDA/PrivateUse1/others backend. 2. Process argparsing in StorageSharing.cpp, and match IPC methods of different devices through getAcceleratorHooksInterface. 3. Put IPC and cudart related codes into cudahooks. 4. Add interprocess communication processing of thirdparty devices in torch/multiprocessing/reductions.py. Currently tested in test_multiprocessing.py. Please review. FYI","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Dynamo ignores frame using yield," üêõ Describe the bug OptimizeContext is wrapped, and frame evaluation callback seems to be set to convert_frame. But there's no dynamo logs and the entire frame seems to run in eager.   Versions main ",2024-05-16T00:33:40Z,triaged oncall: pt2 module: dynamo,open,0,2,https://github.com/pytorch/pytorch/issues/126360,If you wrap model in a user callable I think it will work,"This is because the code is basically a `generator` and Dynamo does not support tracing through generators. The generator bytecodes are tricky.  if you can workaround that would be better. If not, let me know, I will plan for generators in H2. "
transformer,[AOTAutograd] tweak min-cut partitioner to avoid saving softmax output,"  CC([AOTAutograd] tweak mincut partitioner to avoid saving softmax output)  CC([inductor] use smaller RBLOCK for expensive reduction kernels) Right now the linear + cross entropy loss operation (usually to be the last part of a transformer model) does the following thing 1. run matmul to get softmax_input 2. load softmax_input to compute max per row. 3. load softmax_input to compute sum per row 4. load softmax_input, normalize it and save the result to softmax_output Step 4 is inefficient since a. in the fwd pass, only a small slice of the softmax_output tensor is need to compute NLLLoss. Materializing the whole tensor is an overkill b. in the backward pass, we need the whole softmax_output, but it can be recompute from softmax_input If we skip saving softmax_output, we would have perf wins since this is the largest tensor in the network. For llm.c, the size is batch_size * sequence_length * vocab_size * item_size ~= 32 * 1024 * 50257 * 2 ~= 3GB. Simply read/write such large tensor need ~2ms in A100. If we recompute softmax_output, we save 1 load for softmax_input and 1 store for softmax_output, which would result in ~4ms saving. To avoid saving the softmax_output we need make sure the min cut partitioner decides to recompute it based on softmax_input and the max/sum tensor (which is small) computed in step 2 and 3. This is not happening currently since the min cut partitioner overestimate the cost of recomputation. The fix is suggested by   to let `dist_from_bw` play a less important role.",2024-05-15T22:54:40Z,ciflow/inductor,open,0,5,https://github.com/pytorch/pytorch/issues/126348,"With the new heuristics, the backward runs slower and we end up with roughly neutral perf overall for llm.c. The reason is the kernel computing gradient of softmax input in the backward pass picks a suboptimal triton config. CC([inductor] use smaller RBLOCK for expensive reduction kernels) fixes that, and now we have the 4ms saving as estimated in the summary for llm.c: ","> Also, obviously, do a perf run. Perf run shows 5 seconds compilation time regress for TIMM link. I'll need debug where that comes from.",rerequest when ready,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llama,Runtime error while running an Inductor inference benchmark for llama_v2_7b_16h," üêõ Describe the bug Hi, With the PyTorch main branch, I tried to run an Inductor benchmark with llama_v2_7b_16h. The python version used was 3.10.2. The workload failed with a strange error when it tried to run in eager mode. I also created a new python virtual environment at my end to confirm that it's a legit issue. While I used CPU, even with CUDA, I'd expect it to fail. I'm guessing CI jobs are not covering this workload (at least with the arguments I used in the command below) I searched the logs of trunk CI jobs pertaining to benchmarks, but couldn't find it. If the CI jobs don't cover it, then it's very likely a legit issue.  Command   Error   Cause While running the benchmark, here, `benchmarks/dynamo/torchbench.py` is unpacking a dict `inputs`, which has a key `input_ids`, whose corresponding value is a tensor.  !image Because of the aforementioned unpacking, what actually got passed is the string `input_ids` (name of the key)! !image  Versions  /pytorchdevinfra     ",2024-05-15T21:48:39Z,module: ci triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/126335,Will fix it. Thanks!,"Thanks for reporting, this is indeed a model that we run only on CUDA in our CI and dashboards due to its size. The dashboard runs for this model still pass: https://tinyurl.com/54d2eb56 I would recommend doublechecking your installed `transformers` version, our benchmark repo specifies 4.38.1.","Sorry for the false alarm! On another machine with python 3.9, everything works well. I'm not sure about the rootcause, but I'm closing this issue."
transformer,DISABLED test_ring_attention_native_transformer_is_causal_False (__main__.RingAttentionTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ,2024-05-15T21:12:28Z,module: rocm triaged skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/126330
transformer,DISABLED test_ring_attention_custom_transformer (__main__.RingAttentionTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ,2024-05-15T16:35:48Z,module: rocm triaged skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/126296
rag,Improve Storage copy_ size mismatch error message,  CC(Improve Storage copy_ size mismatch error message) Signedoffby: Edward Z. Yang ,2024-05-15T13:31:48Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/126280, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[inductor][cpu]speech_transformer AMP single/multiple thread static/dynamic shape CPP/default wrapper performance regression in 2024-05-12 nightly release, üêõ Describe the bug AMP static shape default wrapper               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       speech_transformer       multiple       1       0.958469       0.030251814       0.028995425912766       69.833969       1.0       1.263245       0.023321511       0.029460782163194997       34.216882       0.76       1.02       0.77       0.49                 torchbench       speech_transformer       single       1       0.996765       0.217230587       0.21652784605105502       69.387519       1.0       1.268162       0.176525437       0.223862851236794       33.153806       0.79       1.03       0.81       0.48          AMP dyanmic shape default wrapper               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       speech_transformer       multiple       1       0.951825       0.030079704       0.0286306142598       69.78338       1.0       1.258736       0.023420975       0.029480824387600003       34.077379       0.76 ,2024-05-15T09:50:50Z,triaged oncall: pt2 oncall: cpu inductor,open,0,2,https://github.com/pytorch/pytorch/issues/126274,"Hi , according to the bisect search log, the PR CC([dynamo] Turn on guard_nn_modules) may introduce this AMP performance regression issue on CPU, could you please help to double check it?","Hi , have you got time to take a look of this issue?"
gemma,test_large_mmaped_weights_non_abi_compatible_cuda unit test failure," üêõ Describe the bug Encountered the following test failures during CI testing ==================================== RERUNS ==================================== _ AOTInductorTestNonABICompatibleCuda.test_large_mmaped_weights_non_abi_compatible_cuda _ Traceback (most recent call last):   File ""/usr/lib/python3.10/unittest/case.py"", line 59, in testPartExecutor     yield   File ""/usr/lib/python3.10/unittest/case.py"", line 591, in run     self._callTestMethod(testMethod)   File ""/usr/lib/python3.10/unittest/case.py"", line 549, in _callTestMethod     method()   File ""/usr/local/lib/python3.10/distpackages/torch/testing/_internal/common_utils.py"", line 2759, in wrapper     method(*args, **kwargs)   File ""/opt/pytorch/pytorch/test/inductor/test_torchinductor.py"", line 9818, in new_test     return value(self)   File ""/opt/pytorch/pytorch/test/inductor/test_aot_inductor.py"", line 288, in test_large_mmaped_weights     self.check_model(Model(), example_inputs)   File ""/opt/pytorch/pytorch/test/inductor/test_aot_inductor.py"", line 107, in check_model     actual = AOTIRunnerUtil.run(   File ""/opt/pytorch/pytorch/test/inductor/test_aot_inductor_utils.py"", line 107, in run     optimized = AOTIRunnerUtil.load(device, so_path)   File ""/opt/pytorch/pytorch/test/inductor/test_aot_inductor_utils.py"", line 88, in load     return torch._export.aot_load(so_path, device)   File ""/usr/local/lib/python3.10/distpackages/torch/_export/__init__.py"", line 405, in aot_load     runner = torch._C._aoti.AOTIModelContainerRunnerCuda(so_path, 1, device)   type: ignore[assignment, callarg] RuntimeError: create_func_( &container_handle_, num_models, device_str.c_str(), cubin_",2024-05-14T21:14:25Z,triaged module: aotinductor,closed,0,2,https://github.com/pytorch/pytorch/issues/126214,"It started to show up on April 20th test, possibly a regression around these days. ",Closed by https://github.com/pytorch/pytorch/pull/131000 
yi,DISABLED test_conv_empty_input_cpu_float64 (__main__.TestNNDeviceTypeCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_conv_empty_input_cpu_float64` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_nn.py` ",2024-05-14T15:40:18Z,high priority module: nn triaged module: flaky-tests skipped oncall: pt2,closed,0,7,https://github.com/pytorch/pytorch/issues/126178,Repro: , (but I don't think you should necessary be on the hook for fixing it),"Thanks for tagging me, is it likely that the failure has to do with the state not being cleaned up properly in the `finally:` block when running with `dynamo`? Adding [`([True])`](https://github.com/pytorch/pytorch/blob/main//torch/testing/_internal/common_utils.pyL1788) to the test instead of having `torch.__future__.set_swap...` in the `try, finally` and running the repro command seems to make the other test pass, but `test_swap_module_params_fails_after_forward` now fails (would have to dig more to understand why)",tlparse https://interncacheall.fbcdn.net/manifold/tlparse_reports/tree/logs/.tmpDHj0X6/index.html EDIT: pretty bugged tlparse,"To prevent a growing amount of new flaky issues from being opened due to this, I can move `test_swap_module_params_fails_after_forward` to use `` and disable it with dynamo for now, with a separate issue to track the dynamo failure in `test_swap_module_params_fails_after_forward` when decorating with `` I wonder whether there were any new changes to dynamo that caused the state in the `try: finally` to start leaking across tests (as I don't think this was happening before). edit: Actually, due to weakrefs installed by dynamo, I somewhat expect that `test_swap_module_params_fails_after_forward` should fail when it attempts to call swap_tensors and it's surprising that it was passing before","`PYTORCH_TEST_WITH_DYNAMO=1 python test/test_nn.py k test_swap_module_params_fails_after_forward` is skipping for me, but it still calls `torch.__future__.set_swap_module_params_on_conversion(True)`. Strangely, if I move it into the try block, the test no longer skips, and passes. 2 mysteries for me:  why can a test skip, after already running some code  is there something special with `torch/__future__.py`? the file itself just looks like a regular global setting config file, but the naming suggests something async", 
yi,Set dtype when copying empty tensor,Summary: Forward fix D57251348 Test Plan: `buck2 test 'fbcode//mode/dev' fbcode//executorch/kernels/test:aten_op_copy_test` Differential Revision: D57304360,2024-05-13T23:00:48Z,fb-exported Merged ciflow/trunk ci-td-distributed,closed,0,6,https://github.com/pytorch/pytorch/issues/126124,This pull request was **exported** from Phabricator. Differential Revision: D57304360,Plz let me know if there are more tests that can be added here instead of relying on the one from ET,This pull request was **exported** from Phabricator. Differential Revision: D57304360,This pull request was **exported** from Phabricator. Differential Revision: D57304360," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llava,Restore DILL_AVAILABLE for backwards compat with torchdata,  CC(Restore DILL_AVAILABLE for backwards compat with torchdata) Signedoffby: Edward Z. Yang  ,2024-05-13T20:01:31Z,open source release notes: dataloader,closed,0,0,https://github.com/pytorch/pytorch/issues/126094
yi,DISABLED test_conv_empty_input_cpu_float32 (__main__.TestNNDeviceTypeCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_conv_empty_input_cpu_float32` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_nn.py` ",2024-05-13T18:40:19Z,module: nn triaged module: flaky-tests skipped oncall: pt2 module: dynamo,closed,0,0,https://github.com/pytorch/pytorch/issues/126091
transformer,DISABLED test_transformerdecoder (__main__.TestNN),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_transformerdecoder` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_nn.py` ",2024-05-13T03:39:41Z,module: nn triaged module: flaky-tests skipped oncall: pt2 module: dynamo,closed,0,0,https://github.com/pytorch/pytorch/issues/126043
agent,Fix torch elastic test SimpleElasticAgentTest.test_restart_workers br‚Ä¶,Failure Info:  Caused by CC(Prevent rendezvous shutdown on worker restarts) . ,2024-05-11T10:48:52Z,oncall: distributed triaged open source Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/126002, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,DISABLED test_transformer_training_is_seq_parallel_True (__main__.DistTensorParallelExampleTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). Same as  CC(DISABLED test_transformer_training_is_seq_parallel_False (__main__.DistTensorParallelExampleTest)) ,2024-05-11T04:09:35Z,oncall: distributed module: rocm module: flaky-tests skipped,open,0,0,https://github.com/pytorch/pytorch/issues/125991
agent,[torch/distributed] Bugfix: wait for all child procs to exit before c‚Ä¶,"Observed Problem  When `torchrun` has finished running the main trainer function (aka entrypoint/user function) successfully, I noticed that sometimes it SIGTERMS the child processes. Then `torchrun` exits successfully.  This results in misleading warning log messages towards the end of the job like the one below:  Root Cause  I noticed that this was due to the incorrect usage of `torch.multiprocessing.ProcessContext.join()` in `torch.distributed.elastic.multiprocessing.api.MultiprocessingContext`. `torch.multiprocessing.ProcessContext.join()` does not actually wait for ALL child procs to exit, but rather waits for **atleastone** child proc to exit. If only a subset of the child procs have exited, it returns `False` and if all child procs have exited it returns `True`.  `torch.distributed.elastic.multiprocessing.api.MultiprocessingContext` was assuming that `torch.multiprocessing.ProcessContext.join()` blocks indefinitely until all child procs have exited. Fix  The fix is simple, just loop, while continuing to call `pc.join()` until it returns `True` > **NOTE**: that the indefinite blocking is NOT an issue since by the time `torch.distributed.elastic.multiprocessing.api.MultiprocessingContext` calls `pc.join()` it already did all the checking to validate that the entrypoint functions either return successfully or that one of them has failed. So we are really just waiting for the unix process to exit after running the entrypoint function. > **NOTE**: since `pc.join()` already blocks until atleastone child proc exits, there is no need to add a polling interval in the body of the loop and the debug logging will show at most `nproc_per_node`",2024-05-10T22:03:00Z,oncall: distributed triaged open source Merged ciflow/trunk release notes: distributed (torchelastic),closed,0,4,https://github.com/pytorch/pytorch/issues/125969, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[NestedTensor] multiply batch and ragged dimension to get shape of values tensor," üöÄ The feature, motivation and pitch From : > I would argue that it would be so cool to be able to do something like:  https://github.com/pytorch/pytorch/pull/124687discussion_r1596098648  Alternatives I think this is just the same as `nt.values()`.  Additional context _No response_ ",2024-05-10T17:36:09Z,triaged module: nestedtensor,open,1,2,https://github.com/pytorch/pytorch/issues/125940,'s in line with what you meant during Triage meeting saying that NestedInt should be be a first class citizen?,"Not really lol. This is an interesting request. I'm not sure it can actually be doable this way though. There are some equations that don't hold in  style fold formulation.  With fold semantics, `(B, *, E)` is represented as `(B, [s1, s2, s3, ... sb], E)`, but we already want scalar multiplication by nested int to denote scaling the nested int by that amount (because this is good for stride formulas)."
transformer,DISABLED test_transformer_training_is_seq_parallel_False (__main__.DistTensorParallelExampleTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). Broken by either CC([dtensor] improve new factory strategy) or CC([dtensor] add op support for memory efficient attention) or CC([dtensor] run transformer sdpa in dtensor). CC    ,2024-05-10T14:44:46Z,oncall: distributed module: rocm module: flaky-tests skipped,open,0,0,https://github.com/pytorch/pytorch/issues/125918
gpt,GPT-fast benchmark: remove Embedding layer from model size,Fixes ISSUE_NUMBER,2024-05-10T04:37:03Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/125901, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
chat,"RuntimeError: false INTERNAL ASSERT FAILED at ""C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp"":1538, please report a bug to PyTorch. torch.linalg.lstsq: (Batch element 0): Argument 6 has illegal value. Most certainly there is a bug in the implementation calling the backend library."," üêõ Describe the bug When I try to run the linked programÔºàhttps://github.com/KindXiaoming/pykan/blob/master/hellokan.ipynbÔºâ, the error suggests that it is a **library file bug**, I get this error and I don't know how to fix it. The code that reported the error is the penultimate line of code: model.train(dataset, opt=‚ÄúLBFGS‚Äù, steps=50);. The full code is below.  from kan import * import torch import torchvision \ create a KAN: 2D inputs, 1D output, and 5 hidden neurons. cubic spline (k=3), 5 grid intervals (grid=5). model = KAN(width=[2,5,1], grid=5, k=3, device='cpu', seed=0) \ create dataset f(x,y) = exp(sin(pi*x)+y^2) f = lambda x: torch.exp(torch.sin(torch.pi*x[:,[0]]) + x[:,[1]]**2) dataset = create_dataset(f, n_var=2) dataset['train_input'].shape, dataset['train_label'].shape \ plot KAN at initialization model(dataset['train_input']); model.plot(beta=100) \ train the model model.train(dataset, opt=""LBFGS"", steps=20, lamb=0.01, lamb_entropy=10.); model.plot() model.prune() model.plot(mask=True) model = model.prune() model(dataset['train_input']) model.plot() model.train(dataset, opt=""LBFGS"", steps=50); mode = ""auto""  ""manual"" if mode == ""manual"":     \ manual mode     model.fix_symbolic(0,0,0,'sin');     model.fix_symbolic(0,1,0,'x^2');     model.fix_symbolic(1,0,0,'exp'); elif mode == ""auto"":     \ automatic mode     lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']     model.auto_symbolic(lib=lib) model.train(dataset, opt=""LBFGS"", steps=50);   The line of code that reported the error model.symbolic_formula()[0][0]  Versions pykan                   0.0.3 matplotlib            3.8.4 numpy                 1.26.4 scik",2024-05-10T02:23:35Z,module: windows triaged module: linear algebra,closed,1,11,https://github.com/pytorch/pytorch/issues/125892,"I'm on it. Could you please share your environment versions by following commands?  I run the [code][1] and get the following result, but I use pykan 0.0.5. [1]: https://gist.githubusercontent.com/shink/ff8e666f17dd6f7f115cae2fae8e075b/raw/9d0d5e2047ac838174faa3cc626e068281bc5a84/kan.py   versions  ","It works fine on nightly version, you can install it by:  output: bash $ python torch/utils/collect_env.py Collecting environment information... PyTorch version: 2.4.0.dev20240509 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.4.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: Could not collect Libc version: N/A Python version: 3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.4.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Pro Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] torch==2.4.0.dev20240509 [pip3] torchaudio==2.2.0.dev20240509 [pip3] torchvision==0.19.0.dev20240509 [conda] numpy                     1.26.4                   pypi_0    pypi [conda] torch                     2.4.0.dev20240509          pypi_0    pypi [conda] torchaudio                2.2.0.dev20240509          pypi_0    pypi [conda] torchvision               0.19.0.dev20240509          pypi_0    pypi ``` ","The same environment, sometimes can sometimes notÔºåin hellokan.ipynb but my torch is cu118cp310 v2.3.0, Devices must be set up behind each model and data, such as CPU or CUDA: 0 and restart jupyter","Miku Hi, could you please try rerunning your code on the nightly version? This issue may have been fixed.",Is there any way to fix this? Nightly version actually made everything worse. Said fbgemm.dll is missing.,Hi... i'm also facing the same issue how to resolve it? Has anyone been able to resolve it yet? PS: currently i'm working on CPU. Does it makes any difference if one implement the model on CPU and train on GPU?,I encountered the same problem when using ComfyUI My env : , does this issue still happen on the nightly Pytorch?,I am closing this issue due to a lack of activity and since I could not reproduce it. Please reopen a new issue if this problem persists.,"ran into this error message on a simple pytest on linux ubuntu see the collect env below: Collecting environment information... PyTorch version: 2.5.1+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.5 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.8.048genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX 5000 Ada Generation Laptop GPU Nvidia driver version: 560.35.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.7 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        46 bits physical, 48 bits virtual Byte Order:                           Little Endian CPU(s):                               32 Online CPU(s) list:                  031 Vendor ID:                            GenuineIntel Model name:                           13th Gen Intel(R) Core(TM) i913950HX CPU family:                           6 Model:                                183 Thread(s) per core:                   2 Core(s) per socket:                   24 Socket(s):                            1 Stepping:                             1 CPU max MHz:                          5500.0000 CPU min MHz:                          800.0000 BogoMIPS:                             4838.40 Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities Virtualization:                       VTx L1d cache:                            896 KiB (24 instances) L1i cache:                            1.3 MiB (24 instances) L2 cache:                             32 MiB (12 instances) L3 cache:                             36 MiB (1 instance) NUMA node(s):                         1 NUMA node0 CPU(s):                    031 Vulnerability Gather data sampling:   Not affected Vulnerability Itlb multihit:          Not affected Vulnerability L1tf:                   Not affected Vulnerability Mds:                    Not affected Vulnerability Meltdown:               Not affected Vulnerability Mmio stale data:        Not affected Vulnerability Reg file data sampling: Mitigation; Clear Register File Vulnerability Retbleed:               Not affected Vulnerability Spec rstack overflow:   Not affected Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSBeIBRS SW sequence; BHI BHI_DIS_S Vulnerability Srbds:                  Not affected Vulnerability Tsx async abort:        Not affected Versions of relevant libraries: [pip3] flake8==7.1.1 [pip3] mypy==1.13.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==2.0.2 [pip3] torch==2.5.1 [pip3] torchvision==0.20.1 [pip3] triton==3.1.0 [conda] Could not collect and here is the pytest bencmark to reproduce: install torch, pytest and pytestbenchmark the benchmark pytest is as follows:     import torch     import pytest     from typing import Callable     def HermetianMatrix(batch: int = 50, dim: int = 1024) > torch.Tensor:         A = torch.rand((batch, dim, dim), dtype=torch.cfloat)         A = A + A.mH         return A     .mark.parametrize(         (""device"", ""batch"", ""dim""),         [             (""cpu"", 10, 4096),         ],     )     def test_eigh_speed(benchmark: Callable, device: str, batch: int, dim: int) > None:         A = HermetianMatrix(batch, dim).to(device=torch.device(device))         benchmark(torch.linalg.eigh, A) command to run it is `python m pytest test_eigh_crash.py benchmarkcolumns=Mean,Min,Max benchmarksort=Name` and the full output is: =========================================================================================== test session starts ============================================================================================ platform linux  Python 3.10.12, pytest8.3.3, pluggy1.5.0 benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000) rootdir: /home/sebastien.roy/src/full_stack/qcog configfile: pyproject.toml plugins: benchmark5.1.0 collected 1 item                                                                                                                                                                                            test_eigh_crash.py F                                                                                                                                                                                 [100%] ================================================================================================= FAILURES ================================================================================================= _______________________________________________________________________________________ test_eigh_speed[cpu104096] _______________________________________________________________________________________ benchmark = , device = 'cpu', batch = 10, dim = 4096     .mark.parametrize(         (""device"", ""batch"", ""dim""),         [             (""cpu"", 10, 4096),         ],     )     def test_eigh_speed(benchmark: Callable, device: str, batch: int, dim: int) > None:         A = HermetianMatrix(batch, dim).to(device=torch.device(device)) >       benchmark(torch.linalg.eigh, A) test_eigh_crash.py:20:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  test_venv/lib/python3.10/sitepackages/pytest_benchmark/fixture.py:156: in __call__     return self._raw(function_to_benchmark, *args, **kwargs) test_venv/lib/python3.10/sitepackages/pytest_benchmark/fixture.py:180: in _raw     duration, iterations, loops_range = self._calibrate_timer(runner) test_venv/lib/python3.10/sitepackages/pytest_benchmark/fixture.py:318: in _calibrate_timer     duration = runner(loops_range) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  loops_range = range(0, 1), timer =      def runner(loops_range, timer=self._timer):         gc_enabled = gc.isenabled()         if self._disable_gc:             gc.disable()         tracer = sys.gettrace()         sys.settrace(None)         try:             if loops_range:                 start = timer()                 for _ in loops_range: >                   function_to_benchmark(*args, **kwargs) E                   RuntimeError: false INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/BatchLinearAlgebra.cpp"":1538, please report a bug to PyTorch. linalg.eigh: (Batch element 0): Argument 10 has illegal value. Most certainly there is a bug in the implementation calling the backend library. test_venv/lib/python3.10/sitepackages/pytest_benchmark/fixture.py:109: RuntimeError  Captured stdout call  Intel MKL ERROR: Parameter 10 was incorrect on entry to CHEEVD. ============================================================================================= warnings summary ============================================================================================= test_venv/lib/python3.10/sitepackages/torch/_subclasses/functional_tensor.py:295   /home/sebastien.roy/src/full_stack/qcog/test_venv/lib/python3.10/sitepackages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)     cpu = _conversion_method_template(device=torch.device(""cpu""))  Docs: https://docs.pytest.org/en/stable/howto/capturewarnings.html ========================================================================================= short test summary info ========================================================================================== FAILED test_eigh_crash.py::test_eigh_speed[cpu104096]  RuntimeError: false INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/BatchLinearAlgebra.cpp"":1538, please report a bug to PyTorch. linalg.eigh: (Batch element 0): Argument 10 has illegal value. Mos... ======================================================================================= 1 failed, 1 warning in 3.67s =======================================================================================","I am running in this issue in an apple m1 pro machine. No problem when running on linux. Is it still an open issue? A simple eigh(torch.rand(2895,2895)) raises the error: RuntimeError: false INTERNAL ASSERT FAILED at ""BatchLinearAlgebra.cpp"":1538, please report a bug to PyTorch. linalg.eigh: Argument 8 has illegal value. Most certainly there is a bug in the implementation calling the backend library. If the size of the matrix is one unit smaller it works. The output of collect_env.py: "
gpt,Ingest gpt-fast benchmark results from S3 to Rockset,"A followup of https://github.com/pytorch/pytorch/pull/125450, this extends the `tools/stats/upload_dynamo_perf_stats.py` script to upload arbitrary benchmark results in CSV format. * Upload gptfast benchmarks to a new Rockset collection `benchmarks/oss_ci_benchmark`.  The file is in the following format:  * The CSV output needs to be kept in `test/testreports` directory. * Reuse the existing `.github/workflows/uploadteststats.yml` workflow  Testing Run the commands manually  Also run a sanity check on ingesting inductor benchmark results: ",2024-05-10T02:05:13Z,Merged ciflow/trunk release notes: releng test-config/default suppress-bc-linter ci-td-distributed ciflow/inductor-micro-benchmark test-config/inductor-micro-benchmark,closed,0,6,https://github.com/pytorch/pytorch/issues/125891, drci, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f 'CI only, no need to run trunk job'","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
gpt,GPT-fast benchmark: adding memory bandwidth and use A100-40GB as target,Fixes ISSUE_NUMBER,2024-05-09T23:22:28Z,Merged ciflow/trunk topic: not user facing ciflow/inductor-micro-benchmark,closed,0,3,https://github.com/pytorch/pytorch/issues/125881,"> btw, I know we're not doing this on gptfast right now, but we should probably subtract the embedding parameters from the model size computation. Yes, will send PRs to both~", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
chat,MacOS export fail with torchchat on Macos-m1-stable," üêõ Describe the bug OMPcompiled macOS export library fails on macosm1stable with PyTorch out of the box (OMP library issue): https://github.com/pytorch/torchchat/actions/runs/9006021620/job/24742618846?pr=723   ld: warning: undefined dynamic_lookup may not work with chained fixups   The generated DSO model can be found at: /Users/ec2user/runner/_work/torchchat/torchchat/pytorch/torchchat/exportedModels/stories15M.so   + python3 torchchat.py generate stories15M dsopath exportedModels/stories15M.so prompt 'Hello my name is'   OMP: Error CC(Use chainerstyle constructor for Conv2d): Initializing libomp.dylib, but found libomp.dylib already initialized.   OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/   ./runreadme.sh: line 30: 32241 Abort trap: 6           python3 torchchat.py generate stories15M dsopath exportedModels/stories15M.so prompt ""Hello my name is""   Error: Process completed with exit code 1.  Versions OMPcompiled macOS export library fails on macosm1stable: https://github.com/pytorch/torchchat/actions/runs/9006021620/job/24742618846?pr=723   ld: warning: undefin",2024-05-09T18:01:13Z,needs reproduction triaged module: macos module: openmp oncall: pt2 module: aotinductor,open,0,0,https://github.com/pytorch/pytorch/issues/125856
rag,[EZ][BE] Use `untyped_storage` in tests,"Get's rid of the following warning:  (noticed while looking at  CC(mps bug: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (6) is not less than length of dimension[0] (6)')issuecomment2101876484 ) Respective change to view ops was landed back in 2022, see https://github.com/pytorch/pytorch/pull/91414",2024-05-09T13:06:26Z,Merged topic: not user facing ciflow/mps,closed,0,2,https://github.com/pytorch/pytorch/issues/125838," merge f ""MPS and relevant parts of lint are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Do not import transformers when import torch._dynamo (#124634),"Fixes  CC(`import torch._dynamo` appears to accidentally have a runtime dep on jax/xla (and other things)) Pull Request resolved: https://github.com/pytorch/pytorch/pull/124634 Approved by: https://github.com/thiagocrepaldi, https://github.com/Chillee ghstack dependencies: CC([NJT] Inline through torch.nested.nested_tensor_from_jagged instead of graph break) Fixes ISSUE_NUMBER",2024-05-08T14:44:13Z,release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/125755
agent,[TorchElastic] Option for sharing TCPStore created by rdzv handlers,"Summary: 1. Define explicit `use_agent_store` on rdzv handlers. Handlers that set is true can share the store.  2. Instead of agent coordinating master_add/master_port values, the logic is now encapsulated by a *rdzv_handler* where `RendezvousInfo` will have `RendezvousStoreInfo` object that handlers must return.       Depending on the implementation they can either:           point to existing store (and expected to `use_agent_store` as true  point 1). Client code will rely on `TORCHELASTIC_USE_AGENT_STORE` env variable to know if the store is shared.           build args that `torch.distributed.init_process_group` can bootstrap by creating new store.  Additional points:  When TCPStore is shared, it should be wrapped in PrefixStore to qualify/scope namespace for other usecases.  `next_rendezvous` signature changed to return instance of `RendezvousInfo` instead of a (store, rank, world_size) tuple for extensibility purposes. Why:  Reduce moving parts     easier to swap implementation     improve tractability     addressing perf/debugability will benefit all usecases      Test Plan: CI Differential Revision: D57055235 ",2024-05-08T05:59:39Z,oncall: distributed fb-exported Merged ciflow/trunk release notes: distributed (torchelastic),closed,1,14,https://github.com/pytorch/pytorch/issues/125743,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235,Some of the actions are failing due module visibility issue check  addressing it.,This pull request was **exported** from Phabricator. Differential Revision: D57055235,This pull request was **exported** from Phabricator. Differential Revision: D57055235," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[DTensor][Tensor Parallel] transformer test numerical issue when `dtype=torch.float32`," üêõ Describe the bug The transformer test is at https://github.com/pytorch/pytorch/blob/eb93307dd3ad64b661759e13164535a76496031d/test/distributed/tensor/parallel/test_tp_examples.pyL190 The test passes if we use batch size = 8, sequence length = 8, ModelArgs.dim = 16. For other settings, it can easily fail due to numerical discrepancies between singleGPU and 4GPU runs. The problem disappears if we change parameter type to `torch.float64`. Note: For fp64, math attention backend will be used, and DTensor doesn't fully support it yet. (One needs to apply some trick to enable the test.) For fp32, memoryefficient attention backend will be used which is now supported by DTensor. The numerical issue persists even if we remove attention modules, so seemingly it's not caused by memoryefficient attention. ",2024-05-08T04:32:51Z,triage review oncall: distributed triaged module: dtensor,open,1,0,https://github.com/pytorch/pytorch/issues/125741
yi,"[ARM] `Vectorized<half>::loadu(x, 8)`  yields slow code if `-fno-unsafe-math-optimizations` are used "," üêõ Describe the bug `to_float` function, defined/traced as follows  results in the following code  which if compiled with `fnounsafemathoptimizations` produces following code:  But if compiled without it following  Which makes me wonder why `fnounsafemathoptimizations` was added there in the first place?  Versions nightly ",2024-05-08T01:31:46Z,module: performance module: arm oncall: pt2 oncall: cpu inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/125735,> Which makes me wonder why `fnounsafemathoptimizations` was added there in the first place? For fixing functional issues. Check this: https://github.com/pytorch/pytorch/pull/113347 And this for an example:  CC([Inductor] incorrect result of vision_maskrcnn)issuecomment1411295807," ok, do you know if it's documented somewhere (or relied upon) that `Vectoried::load(ptr, 1)` will initialize rest of the vector with zeros?","Grabbing for myself, found a way to make small modification to the codebase that allows compiler to to proper dead code elimination, stolen from avx2 codebase: ",">  ok, do you know if it's documented somewhere (or relied upon) that `Vectoried::load(ptr, 1)` will initialize rest of the vector with zeros? I'm not aware of any document or code that relies on this fact."
llm,‚òÇÔ∏è `torch.compile` generates slower code for LLMs than eager on ARM platform (M1/AARCH64)," üêõ Describe the bug On my M2 Pro, using https://github.com/pytorchlabs/gptfast/tree/malfet/setprectofloat16 and stories110M model:  yields 166 tokens/sec, but the same one with compile only 100 tokens/sec  Versions nightly ",2024-05-08T01:17:50Z,module: performance module: arm oncall: pt2 oncall: cpu inductor,open,0,0,https://github.com/pytorch/pytorch/issues/125734
transformer,AsyncCollectiveTensor: prevent wait_tensor() calls on graph inputs from getting DCEd," was seeing the loss eventually become NaN when compiling individual transformer blocks in torchtitan  with this patch I no longer see the NaN loss. The problem is the following: (1) It is possible to have graph inputs to a compiled region that are AsyncCollectiveTensors. In particular: when we compile individual transformer blocks in the llama model, the first layer (embedding layer) is run in eager mode, and it outputs an AsyncCollectiveTensor that is fed to the first transformer block (2) ideally, we would like that AsyncCollectiveTensor graph input to desugar into a `wait_tensor()` op that shows up at the beginning of the graph. (3) the way this is supposed to happen is: AOTAutograd traces through the __torch_dispatch__ of AsyncCollectiveTensor, tracing out a `wait_tensor()` call before dispatching to any of the other ops in the function we are tracing (4) however: `trigger_wait()` was getting called in a way where we would ignore its output (and return `self.elem` directly), which would cause the `wait_tensor` ops to get DCE'd.   CC(AsyncCollectiveTensor: prevent wait_tensor() calls on graph inputs from getting DCEd)  CC(AOTAutograd: use info not debug logging for ViewAndMutationMeta) ",2024-05-07T11:09:29Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,2,3,https://github.com/pytorch/pytorch/issues/125677,Very nice fix! Thank you!, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,DISABLED test_train_parity_2d_transformer_checkpoint_resume (__main__.TestFullyShard2DTraining),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ://github.com/pytorch/pytorch/commit/dba689bbfdc65cf12711ec4b4f2f7eed0fae3a59  ,2024-05-06T23:40:22Z,module: rocm triaged skipped,open,0,0,https://github.com/pytorch/pytorch/issues/125644
transformer,"Compiling with Inductor, DDP, and Dynamic Shapes Results in Errors"," üêõ Describe the bug `torch.compile` with the inductor backend errors out with dynamic shapes and DistributedDataParallel. Either a direct error `ConstraintViolationError: Constraints violated (L['x'].size()[1])!` when using `torch._dynamo.mark_dynamic`, or recompiling multiple times until the recompile limit is reached due to a ""stride mismatch at index 0"" compilation error with `dynamic=True` or `dynamic=None`. These errors occur in both PyTorch 2.3 and the latest PyTorch Nightly. I've created a replication with a simple ""transformer"" model, with just an embedding layer and linear head layer, so I can vary the shape of the sequence length in the batch. I get the same errors with a full fromscratch transformer with DDP. I inconsistently get the ConstraintViolationError when using `torch._dynamo.mark_dynamic` in a nondistributed context with PyTorch 2.3. Specifically, with the Hugging Face Transformers Llama implementation. But I have been unable to replicate it with nonHF code.  Error logs With my replication script below, compiling a DDP model for dynamic shapes with the recommended `torch._dynamo.mark_dynamic` instead of using `torch.compile(..., dynamic=True)` using the following command:  results with the following `ConstraintViolationError`   You can turn on logging with `logging`, but the dynamo logs don't appear to be that useful compared to other errors I've seen.  The same command using `torch.compile(..., dynamic=True)`  or `torch.compile(..., dynamic=None)` and relying on the compiler to detect dynamic shapes  results in a recompiles error:  The logging output also doesn't appear to verbose.  I'm happy to add more logging if w",2024-05-06T23:29:54Z,high priority oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes,closed,0,11,https://github.com/pytorch/pytorch/issues/125641,benjamin I ran locally with a nightly and this actually passes for me. Can you try out a nightly?  https://pytorch.org/getstarted/locally/," I tested my replication script with yesterday's nightly and 2.3. You can see my environment in the ""PyTorch Nightly Environment"" section. These errors are only with DDP. Single GPU compiles and trains without issue. I installed today's nightly `pytorch2.4.0.dev20240507` with both Cuda 12.4 & 12.1. Setting dynamic shapes with DDP via `torch._dynamo.mark_dynamic` using the following command still errors out with the same `ConstraintViolationError`.  And setting `torch.compile(..., dynamic=True)` or `torch.compile(..., dynamic=None)` using the following command still results with recompilations every batch until the `torch._dynamo hit config.cache_size_limit (8)` is hit. ","I am seeing the same issue this morning, running the same three commands on the replication script, on my system using CUDA 12.1. Details below:  PyTorch Nightly Environment details  ","I'm going to look into this. But my recollection is that HF added some error checking code which forces specialization, and I haven't gotten around to yelling at them to stop running this logic when being torch compiled. BTW, the two errors here are one and the same. `mark_dynamic` is yelling at you because it tried to make it dynamic, but failed due to specialization. You can use TORCH_LOGS=dynamic to find out where the specialization happened.","> I'm going to look into this. But my recollection is that HF added some error checking code which forces specialization, and I haven't gotten around to yelling at them to stop running this logic when being torch compiled. It's not just HF models which trigger this when using DDP. My replication script uses a simple twolayer model with an `Embedding` and `Linear` layer. One layer doesn't replicate this issue. It seems to have something to do with adding a second layer.  > BTW, the two errors here are one and the same. `mark_dynamic` is yelling at you because it tried to make it dynamic, but failed due to specialization. You can use TORCH_LOGS=dynamic to find out where the specialization happened. When I run my replication script with `TORCH_LOGS=+dynamic`  I get the following output for rank 0:    TORCH_LOGS=+dynamic Rank 0 Output ```text torch/fx/experimental/symbolic_shapes.py:2268] [0/0] create_env torch/fx/experimental/symbolic_shapes.py:3239] [0/0] create_symbol s0 = 977 for L['x'].size()[1] [2, 9223372036854775806] at test/replication.py:52 in forward (_dynamo/variables/builder.py:2137 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0"" torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval True == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval False == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval False == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Eq(16*s0, 16) == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Ne(Mod(16, 16*s0), 0) == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval 2048*s0 > 2048 == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval True == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Ne(s0, 1) == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Ne(16*s0, 16) == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Eq(s0, 1) == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval s0 > 1 == True [statically known] torch/fx/experimental/symbolic_shapes.py:4634] [0/0] eval 32768*s0  I'm not seeing anything about specialization, but might be misinterpreting the logs.","It's this:  Very strange though, why is this suppressed ü§î. You could get a full backtrace for this log with  TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(2048*s0, 2000896)""", could it be related to this ? https://github.com/pytorch/pytorch/pull/120523/filesdiffcb8e02fc8f37e53904ab1b151c46dd109cf50d8121bbd340834b2e976b22ebc4R74  Maybe the idiom there is not correct. We're trying update the meta strides without adding guards or specializations,"Oh yeah, this looks very very naughty. Hmmmm","As a stopgap, I guess we could prevent replacements from happening when guards are suppressed. This still seems very naughty though.....","Here's the additional backtrace with the ""Eq(2048*s0, 2000896)"" guard added:     TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(2048*s0, 2000896)""  ",I believe this is fixed
mixtral,Reduce the number of layers for mixtral moe model to adapt CI memory limitation,Fixes ISSUE_NUMBER,2024-05-06T18:01:38Z,Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/125608, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Compiling GPT2 with tensorrt backend and dynamic shapes results in guard failures., üêõ Describe the bug I'm trying to compile GPT2 model with dynamic shapes and encountered this guard failure at the end. Any suggestions on what's the exact issue and how to resolve it ?   Error logs Please find the error log here : full_gpt2.log  Minified repro   Versions Versions of relevant libraries: [pip3] flake8==6.1.0 [pip3] flake8bugbear==23.3.23 [pip3] flake8comprehensions==3.12.0 [pip3] flake8executable==2.1.3 [pip3] flake8loggingformat==0.9.0 [pip3] flake8pyi==23.3.1 [pip3] flake8simplify==0.19.3 [pip3] mypy==1.3.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.26.0 [pip3] onnx==1.15.0 [pip3] onnxgraphsurgeon==0.3.25 [pip3] onnxruntime==1.16.3 [pip3] onnxscript==0.1.0.dev20240205 [pip3] onnxsim==0.4.35 [pip3] optree==0.10.0 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] pytorchtritonrocm==2.2.0 [pip3] torch==2.3.0+cu121 [pip3] torch_tensorrt==2.3.0.dev0+6f945fa4c [pip3] torchprofile==0.0.4 [pip3] torchsurgeon==0.1.2 [pip3] torchvision==0.18.0+cu121 [pip3] triton==2.3.0 ,2024-05-06T17:29:29Z,triaged oncall: pt2 module: dynamic shapes,open,0,2,https://github.com/pytorch/pytorch/issues/125604,Dynamic shapes guards failure:   ,"This is probably  CC(Equivalent idea to size-oblivious guard for end of bounds on sizes) If you don't mind recompiling on 1024 specifically, you can fix this with `maybe_mark_dynamic` instead of `mark_dynamic`"
transformer,Issues in loading quantized weights with Version==None," üêõ Bug       Hi! I am using torch 2.0.0 and the same issue appeared. I have a custom architecture based on transformer model (Attention + FeedForward). I quantized my model and saved it successfully. However, when I tried to load the weights, it gave me the same error. (I also checked with torch==2.2.2 and torch==2.3.0) Preparing for weights loading  Weights Loading  Error:  This error was also reported in CC(Inconsistency loading quantized models when state_dict with Version==None)  Versions PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Amazon Linux 2 (x86_64) GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.117) Clang version: Could not collect CMake version: version 3.29.2 Libc version: glibc2.26 Python version: 3.10.14  (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.10.214202.855.amzn2.x86_64x86_64withglibc2.26 Is CUDA available: N/A CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration:  GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 535.161.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              48 Online CPU(s) list: 047 Thread(s) per core:  2 Core(s) per socket:  24 Socket(s):           1 NUMA node(s):        1 Vendor ID:           AuthenticAMD CPU family:          23 Model:               49 Model name:          AMD EPYC 7R32 Stepping:            0 CPU MHz: ",2024-05-06T05:37:28Z,oncall: quantization,open,0,2,https://github.com/pytorch/pytorch/issues/125564,"The error you're encountering seems to be related to the structure of the saved state dictionary not matching the structure of the model when you're trying to load the weights. This can happen if the model architecture has changed between saving and loading, or if there are inconsistencies in the naming of layers or parameters. Here are a few things you can try to resolve this issue:  Check Model Compatibility: Ensure that the model architecture you're using to load the weights matches exactly with the architecture used to save the model. Even small changes like adding or removing layers can cause compatibility issues.  Verify State Dictionary Keys: Before loading the state dictionary, inspect its keys to ensure they match the expected structure of your model. You can print the keys of the state dictionary using print(state_dict.keys()) and compare them with the names of the parameters in your model.  Strict vs NonStrict Loading: You're currently using strict=False when loading the state dictionary. This means that missing keys will not raise errors. While this can sometimes help in loading partially trained models or models with different architectures, it can also lead to unexpected behavior if the model structure has changed significantly. Try loading with strict=True to see if any errors are raised that could provide more insights into the issue.  Check for Layer Renaming: Make sure that if you've renamed any layers or parameters in your model between saving and loading, you handle these changes appropriately. You may need to manually map the keys in the state dictionary to the corresponding layers in your model.  Debugging: If none of the above steps resolve the issue, try debugging by printing out more information about the model and the state dictionary during loading. This can help identify where exactly the mismatch is occurring.","Thank you for replying so fast. Actually I have done all of this already.  I saved the model state_dict after quantization and followed the same steps to load the weights. As for the keys I have verified that they match in the `model.state_dict` and loaded `state_dict` I followed the following steps when I was performing QAT and same steps I am doing to load the weights as well **QAT:**  **Loading:**  **Model State Dict:**  **Keys in weights from memory**  Additionally, model architecture did not change because I verified, and I am the one who saved the weights after QAT and trying to load them to test them.  Also, if you look at CC(Inconsistency loading quantized models when state_dict with Version==None), the same issue existed previously. "
gpt,[Inductor] Add SDPA pattern for OOB GPT2 models,Add SDPA pattern for 2 OOB models:  tokenclassification+gpt2  textgeneration+gpt2 Note that these models have two masks: attention mask with float type and causal mask with bool type. ,2024-05-06T03:23:34Z,oncall: distributed open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,2,13,https://github.com/pytorch/pytorch/issues/125562,"Looks good, thanks for your contribution.  I would like to make sure we only make neccessary changes, and that this does not lead to any performance regressions. So I will run the benchmark suite. Until then I would hold off an approval. The benchmark run I triggered can be monitored here: https://github.com/pytorch/pytorch/actions/runs/8967381583","> Looks good, thanks for your contribution. >  > I would like to make sure we only make neccessary changes, and that this does not lead to any performance regressions. So I will run the benchmark suite. Until then I would hold off an approval. >  > The benchmark run I triggered can be monitored here: https://github.com/pytorch/pytorch/actions/runs/8967381583 Thanks for your review,  . The benchmark has 1 successful job and 2 skipped ones. Is this expected?"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", can you add a `` to the failing test?,>  can you add a `` to the failing test? Sure! Added for `TestFullyShard2DTraining(FSDPTest)  test_train_parity_2d_mlp`.,"  `Meta InternalOnly Changes Check` failed due to the code change. As it blocks the PR merge, could you help import the Pull Request? Thanks~", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Meta InternalOnly Changes Check Details for Dev Infra team Raised by workflow job ",">  has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator. It is because of this it is now linked to an internal diff and cant be landed by pytorchbot."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,"‚ùìDifferent results between normal batching and `vmap` while using lower precision (e.g., bfloat16)"," üêõ Describe the bug  Description Hi there, I would like to get persample gradients (application: transformer classifier for text). in a preliminary test, I noticed that when computing the losses in the various modes: 1. batched (typical torch) 2. manually batched 3. vmap batched I get different results when running with lower precision. When computing in higher precision, 32 or 64, the differences shrink and disappear altogether, respectively. Is this expected? Can it be avoided? Also, I noticed that the the `grad_fn` differ between 1 and 3. Specifically, in normal batched mode I get `NllLossBackward0` while with vmap I get `DivBackward0`. Would be great to understand better why vmap returns this ""Div"". Below, I report a snippet of the code that I am using. Thank you very much in advance for your attention.   Versions  Env info (same issue with torch 2.2.*) PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.35 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.2.036genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe Nvidia driver version: 550.54.14 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.7",2024-05-04T15:40:30Z,module: numerical-stability triaged module: vmap module: functorch,open,0,2,https://github.com/pytorch/pytorch/issues/125534,"I believe in this case it's that we implement the batching rule for `nll_loss` with a decomposition, which decomposes `nll_loss` into a number of other ops, which includes a division.","Hi , thank you so much for your answer. Regarding my question about different results when using lower precision tensors, I think it might be due to the order of the operations that functorch implements vs torch, right? This is to say, it is fine to close this issue. Btw, I am really glad functorch exists üöÄ  Computing persample hessianvector products for small (<500M) transformer models has been a breeze. I am not sure I would have been able to do it in torch so efficiently"
llama,Inductor generated Triton kernel spends double time from Llama2 to Llama 3," üêõ Describe the bug I'm working on enabling Llama3 on gptfast, but I found the Llama3 + int8 performs worse than expected.  I checked the generated kernel and found one of the kernels spent double time than Llama2 (from 0.066 ms to 0.11 ms). This kernel was used 34 times in the whole Llama model, so it slows down the whole model's performance a lot. This only happens on int8 quantized model (tokens/sec is 73% of Llama2), the base model (tokens/sec is 90% of Llama2) has sort of reasonable inference speed.  Llama 2:  Llama 3:  The only difference is  goes from 11008 to 14336. I'm curious if this is reasonable and what could cause the difference?  Versions N/A ",2024-05-04T01:18:03Z,triaged oncall: pt2 module: inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/125524, , would you comment with source code that would give repro ?,>  would you comment with source code that would give repro ? Updated all torch debug logs to https://drive.google.com/drive/u/1/folders/1DIpP4pN37k0vp3er0OfJ0ObwLqbw6ij,Not actively working on this right now so don't want to be assigned but I think this could be fixed by doing mm decompositions as maxautotune options,"closing as the issues has been resolved, check the perf number at https://github.com/pytorchlabs/gptfast/pull/166."
transformer,GPU stats not being reported with multi-gpu inference with torchrun," üêõ Describe the bug I wanted to run a whisper via api using multigpu inference and came up with a hacky code. The gpu usage and memory information in this case are not being reported properly.  I am running the api as follows:  I am using torchrun as follows:  When I run it in this way I am able to use multigpu inference via my api, but the gpu information including usage(which always shows 0%) and memory is not updated properly. I agree I have written the code in an uncoventional way, but regardless issues in updating gpu stats may even turn out to be a security issue.  Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.027genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 GPU 4: NVIDIA RTX A6000 GPU 5: NVIDIA RTX A6000 GPU 6: NVIDIA RTX A6000 GPU 7: NVIDIA RTX A6000 GPU 8: NVIDIA RTX A6000 GPU 9: NVIDIA RTX A6000 Nvidia driver version: 535.171.04 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Addre",2024-05-03T05:06:37Z,oncall: distributed,closed,0,0,https://github.com/pytorch/pytorch/issues/125459
yi,[optim] Merge the pyi files into py files of optimizer,Continue the work of pytorch/pytorch CC(Merge the pyi files into py files of optimizer),2024-05-03T01:07:38Z,open source Merged ciflow/trunk release notes: optim,closed,0,28,https://github.com/pytorch/pytorch/issues/125452, drci,"thanks for your review , I splited the changes about lr_scheduler into CC(add typing in torch.optim.lr_scheduler).", merge, merge, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  BC Lint / bc_linter Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , rebase b main, started a rebase job onto refs/remotes/origin/main. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/main pull/125452/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/9040977794, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  BC Lint  pull  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,"Sorry , I mistakenly thought the failures might have been fixed, so I rebased onto the main branch. However, after investigating the error message, I found that it was caused by my code, and I fixed it in https://github.com/pytorch/pytorch/pull/125452/commits/165fed5261170b85f70ae4e64365b7e77a0cea68. Could you please approve the CI again?",Should I keep the parameter name `d_p_list`? None of the affected functions are exposed to external users. !image,"> Should I keep the parameter name `d_p_list`? None of the affected functions are exposed to external users. Yes, we should keep the name. sgd is publicly exposed and the argument is sadly not positional only.","> > Should I keep the parameter name `d_p_list`? None of the affected functions are exposed to external users. >  > Yes, we should keep the name. sgd is publicly exposed and the argument is sadly not positional only. Okay, I will fix it. But wasn't `sgd` removed in torch/optim/__init__.py? Also, its only usage seems to be in SGD.step() (search for ` sgd(` using vscode) and only torch/optim/sgd.py use the name `d_p_list` (search for `d_p_list`).","The removal in optimizer `__init__.py` is the torch.optim.sgd module, not the function, which is offered so people can manage their own state. (I have yet to figure out why we do the dels there but that is not relevant for this discussion.)","> > Should I keep the parameter name `d_p_list`? None of the affected functions are exposed to external users. >  > Yes, we should keep the name. sgd is publicly exposed and the argument is sadly not positional only. done.",looks like there are merge conflicts now :/,> looks like there are merge conflicts now :/ fixed, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,Please run lintrunner locally to ensure lint passes for the next commit (and usually before pushing). Getting lintrunner locally is pretty simple: just ,> Please run lintrunner locally to ensure lint passes for the next commit (and usually before pushing). Getting lintrunner locally is pretty simple: just >  >  ok! fixed, merge
chat,[pytorch][cuda] Some speedup on depth wise convolution 2D forward,This PR does a few things:  Adds a generic implementation for `conv_depthwise2d` when the filter size is non standard. This implementation works faster because it doesn't do edge condition checks inside the innermost loops. We avoid the checks by calculating the boundaries ahead of the loop.  Hints to nvcc to minimize the register usage so that we squeeze more memory bandwidth  Adds filter size 5 as a common size where we can use the template implementation to improve unrolling and generate more efficient code  The implementation doesn't completely fix the issue described in  CC(FP32 depthwise convolution is slow in GPU). For that we need to rewrite the kernel using the suggestions described in the issue chat. This PR uses the same order of accessing the tensor as before but just removes overhead instructions in the inner loops to get the speedup. Before:  After ,2024-05-02T00:25:22Z,Merged ciflow/trunk release notes: cuda release notes: performance_as_product topic: new features ciflow/rocm,closed,0,8,https://github.com/pytorch/pytorch/issues/125362,"two old related issues:    CC(FP32 depthwise convolution is slow in GPU)    CC(Depthwise Conv1d performance (a naive CUDA kernel is 10x faster))   CC(depthwise convolution are slow on cpu ) Also, the new EfficientVIT https://arxiv.org/abs/2205.14756 also uses depthwise 5x5 convs, so maybe having EfficientVIT and various MobileNets with comparisons with ORT/TRT in standard benchmark can be useful (at last the, inverted depwthwise conv block is quite popular) In https://github.com/HazyResearch/flashfftconvshortdepthwiseconvolutions there is also an impl of fast conv1d dpethwise conv: `We also provide a fast kernel for short 1D depthwise convolutions (e.g., where the kernel length is on the order of 3/5), which runs 7 times faster than PyTorch Conv1D.`","> two old related issues: >  > * FP32 depthwise convolution is slow in GPU CC(FP32 depthwise convolution is slow in GPU)) > * Depthwise Conv1d performance (a naive CUDA kernel is 10x faster) CC(Depthwise Conv1d performance (a naive CUDA kernel is 10x faster)))) > * depthwise convolution are slow on cpu CC(depthwise convolution are slow on cpu )) >  > Also, the new EfficientVIT https://arxiv.org/abs/2205.14756 also uses depthwise 5x5 convs, so maybe having EfficientVIT and various MobileNets with comparisons with ORT/TRT in standard benchmark can be useful (at last the, inverted depwthwise conv block is quite popular) >  > In https://github.com/HazyResearch/flashfftconvshortdepthwiseconvolutions there is also an impl of fast conv1d dpethwise conv: `We also provide a fast kernel for short 1D depthwise convolutions (e.g., where the kernel length is on the order of 3/5), which runs 7 times faster than PyTorch Conv1D.` Thanks for the pointers. TBH this PR is just the outcome of a quick look after someone asked for help on an internal workflow. I didn't spend much time on research but fundamentally we can probably accelerate it even more using explicit shared memory if the reuse is large. I'll take a look on the pointers you sent.",cc:  ," label ""ciflow/rocm""",cc:  ,cc:    could you guys please take a look at this one?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,FSDP2 Memory Tracker," CC(FSDP2 Memory Tracker)  Why do we need the FSDP Memory Tracker? **Tuning Decisions** 1. What is the expected peak memory with current configuration? 2. If I change my FSDP wrapping, how much effect will it have on peak memory? 3. What is the best batch size to use? 4. What is the maximum sequence length that one can run with current configuration? 5. How does increasing/decreasing the ‚ÄúDP‚Äù world size affect peak memory? 6. How much memory do I save if I move the optimizer to the CPU? 7. Which activation checkpointing policy should I use? 8. If I have various SAC policies, How do they compare against each other? 9. What happens if I apply different SAC policies to different FSDP units? 10. If I make my gradient reduction in fp32, what effect will it have on memory? 11. If I want to use a custom mixed precision policy, how will it affect the peak memory? 12. When does it make sense to use HSDP? 13. Can I reshard to a smaller mesh without increasing peak memory substantially? 14. Can safely disable post forward reshard without causing an OOM? **Debugging** 1. Which module contributes most to activation memory? 2. Which FSDP unit is holding a lot of unsharded memory? 3. AC is not releasing memory? The FSDP2 Memory Tracker addresses all of the above. It is based on:   CC(Memory Tracker for tracking Module wise memory)    CC(Extended Module Tracker)   Example and Output:   cc:      ",2024-05-01T19:07:04Z,oncall: distributed Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,1,16,https://github.com/pytorch/pytorch/issues/125323,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: sanketpurandare  (a641063de2b5ae9f5ad7c17a009403ac67562a59, d7421d297503b2e665cdc8d37445f28137e9ff28, 685e8bf3b3c4d6fb45798f650be4c5f3ce381b96)","Highlevel comment: It might be worthwhile to document which parts are depending on FSDP2 internals, and we may be able to see how to expose things more robustly.","> This seems okay to me! I think we just need to be careful of the situation ""I changed something in FSDP2 and now it broke the memory tracker. How do we fix it?"" Yeah we need to maintain it actively till FSDP2 stabilizes.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""stuck ci winvs2019""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""stuck ci winvs2019"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," I'm seeing the new test `test_tracker_multi_group_eager` failing on ROCm distributed job https://hud.pytorch.org/pytorch/pytorch/commit/287c68c5eca2e15bf73b84fe9e39755ae3f842ba CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)45778.  Could you help take a look?  The job is only run periodically, do its signal was missed on the PR",">  I'm seeing the new test `test_tracker_multi_group_eager` failing on ROCm distributed job https://hud.pytorch.org/pytorch/pytorch/commit/287c68c5eca2e15bf73b84fe9e39755ae3f842ba CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)45778. Could you help take a look? The job is only run periodically, do its signal was missed on the PR Yeah will push a fix","Btw, I disable the test in  CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore)) to keep trunk sane.  In your fixed PR, please add ""Fixes  CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore))"" in your PR description to run the test in your PR","> Btw, I disable the test in CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore)) to keep trunk sane. In your fixed PR, please add ""Fixed CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore))"" in yout PR description to run the test in your PR Yeah it's because of Python 3.8 not supporting bitwise OR in typing"
yi,_VariableFunctions.pyi expands to invalid utf-8 encoding.," üêõ Describe the bug Torch 11.8 does not pass mypy 1.10.0 check, see https://github.com/python/mypy/issues/17189 Looks like it is generated from _VariableFunctions.pyi.in And the offset reported by mypy corresponds to line 6320 column 84 which contains these curly quotes: !image Normally these would need to be utf8 encoded, but in this case they are not and they are stored in the file as byte value 147 which is indeed an invalid utf8 encoding. When encoded as utf8 the curly braces become a 3 byte sequence 239, 191, 189. But it is a bit complicated figuring out where this is coming from  it appears this _VariableFunctions template is expanding a bunch of numpy functions, this one is the cov function. That function is in my env at D:\Anaconda3\envs\sr\Lib\sitepackages\numpy\lib\function_base.py but the line containing that comment does not have curly quotes, so I'm not sure where the curly quotes are coming from.  Versions Collecting environment information... PyTorch version: 2.3.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: version 3.28.0msvc1 Libc version: N/A Python version: 3.10.14  (main, Mar 21 2024, 16:20:14) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22631SP0 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA T1000 Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available:",2024-05-01T18:40:58Z,module: windows module: nn module: typing triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/125320,"Closing as duplicate of  CC(Certain .pyi files are not encoded as UTF-8 in Windows) Fix has been pushed to trunk, will be cherrypicked into next patch release, but further work on investigating/preventing those regressions from happening is needed"
rag,Invalidate StorageImpl instances when tensor is overwritten with cudagraphs,  CC(Invalidate StorageImpl instances when tensor is overwritten with cudagraphs) Fixes CC(torch.compiled model output gets overwritten despite tensor.detach()) ,2024-04-30T21:18:52Z,open source Merged Reverted module: cuda graphs Stale ciflow/trunk release notes: cuda oncall: pt2 module: inductor ciflow/inductor,closed,0,45,https://github.com/pytorch/pytorch/issues/125264,Will need benchmarking,Test failure is real. However it seems to be highlighting an existing bug/feature?. The following fails in current main.  This is not supposed to fail right?,"You can add `torch.compiler.cudagraph_mark_step_begin()` at the beginning of the repro to fix,  or move the `foo(m, x)` after the allclose comparison. That should fail because the foo(m, x) triggers a new invocation of the graph and overwrites existing outputs. ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/isuruf/47/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/125264`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/isuruf/47/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/125264`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/isuruf/47/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/125264`)",What are the benchmarks I should run for this change?, just doing a dashboard run with inductorcudagraphs for tb/hf/timm should be good,Perf results look good but there is an error on `test_error_on_dealloc_use2` ,, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/isuruf/47/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/125264`)", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""test test/inductor/test_cudagraph_trees.py::CudaGraphTreeTests::test_fallback_to_eager_if_recompiling_too_many_times is failing https://github.com/pytorch/pytorch/actions/runs/9933628108/job/27477785946 https://hud.pytorch.org/pytorch/pytorch/commit/1bc390c5f5ac065c156f55f4eceed267ecc67b41.  Test was introduced by fa5f5727484d78aa2ed8668c11550431bd415f5c which is before the merge base"" c landarce"," revert m ""test test/inductor/test_cudagraph_trees.py::CudaGraphTreeTests::test_fallback_to_eager_if_recompiling_too_many_times is failing https://github.com/pytorch/pytorch/actions/runs/9933628108/job/27477785946 https://hud.pytorch.org/pytorch/pytorch/commit/1bc390c5f5ac065c156f55f4eceed267ecc67b41. Test was introduced by https://github.com/pytorch/pytorch/commit/fa5f5727484d78aa2ed8668c11550431bd415f5c which is before the merge base"" c landrace Also broke ROCm CI"
rag,CI: Extending unit test coverage for aarch64 linux,"Adding core, dynamo and inductor unit tests for aarch64 linux CI runs.",2024-04-30T19:06:14Z,triaged open source Merged ciflow/trunk release notes: releng ciflow/linux-aarch64,closed,0,2,https://github.com/pytorch/pytorch/issues/125255, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Calling `get_model_state_dict/set_model_state_dict` requires forward pass for `_lazy_init`," üêõ Describe the bug The new distributed APIs `get_model_state_dict/set_model_state_dict` require running at least one forward pass in order to call `_lazy_init`. For example,  I believe get/set_model_state_dict (and maybe get/set_optim_state_dict) should call _lazy_init as well?  Versions Torch 2.3 ",2024-04-29T16:43:27Z,oncall: distributed,closed,0,8,https://github.com/pytorch/pytorch/issues/125170," label ""oncall: distributed"""," I'm a little confused by the issue, does the above code fail or is the issue that _lazy_init is not called on a submoduleof the model?",">  I'm a little confused by the issue, does the above code fail or is the issue that _lazy_init is not called on a submoduleof the model?  sorry I was not clear  I am suggesting _lazy_init should be inside `get/set_model/optimizer_state_dict` as the error otherwise is quite confusing. `_lazy_init` a private function, so the above code is not really desired", Can you show the error message? I thought FSDP.state_dict and FSDP.load_state_dict called the `_lazy_init`. I'm curious what's the error you encountered.,  ," The issues has been fixed, https://github.com/pytorch/pytorch/pull/121544. Can you check if this PR solves the issue?",  yep that looks good to me! It would be nice to include in 2.3.1, do you think we can backport for  CC([v2.3.1] Release Tracker)?
yi,Merge the pyi files into py files of optimizer,Merge the interfaces in pyi files into py files in `torch/optim`.,2024-04-29T13:52:22Z,triaged open source Merged ciflow/trunk release notes: optim,closed,0,3,https://github.com/pytorch/pytorch/issues/125153,"Hello reviewers, I am going to merge all interfaces in `torch/optim`. Currently, I have only merged one (`torch/optim/adadelta.py`). Could you please take a look at it? I will continue merging in this PR if everything is fine. Thanks.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,Add templated attention BLOCK_M & BLOCK_N default size for different head_dim,"Run different head_dims [64, 128], which are the most popular ones across major GPT models. Enumerate different  and  candidates [16, 32, 64, 128], and get the best config as default one.  Before  ",2024-04-29T04:51:33Z,Merged ciflow/trunk topic: not user facing oncall: pt2 module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/125139, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[Storage_ipc] Provides IPC extensions for 3rd devices.,This PR adds IPCrelated interfaces for thirdparty devices on the backend of storage. Thirdparty devices can implement the underlying code for interprocess communication through the hooks mechanism.  the part2 of this PR completes the IPC method registration of the thirdparty device in _generate_storage_methods_for_privateuse1_backend. Fixes CC(Add support for IPC features for PrivateUse devices)  ,2024-04-28T13:14:26Z,triaged open source Stale module: PrivateUse1,closed,0,3,https://github.com/pytorch/pytorch/issues/125122," Please review it, thank you",Thanks for the PR! I'm not very familiar with the IPC story overall so will take a few days for me to find the time to wrap my head around and give a review. Sorry for the delay.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,`torch.compile` fails with `jacfwd` when multiplying/dividing float and tensor," üêõ Describe the bug The following minimal example fails  with following last few lines in the error message (longer error log below).  Similar error if it's `return x / two`. Trying instead  also fails with a different `NotImplementedError` (full error below).   Error logs For the first example where `two = 2.0`.  For the second example where `two = torch.tensor([2.0], dtype=x.dtype, device=x.device)`.   Minified repro _No response_  Versions  ",2024-04-26T22:12:45Z,high priority triaged module: vmap oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,closed,0,9,https://github.com/pytorch/pytorch/issues/125078,"Well, it seems to be working on `torch==2.3.0.dev20240212` on `mps`.  But `torch==2.0.1+cu117` is giving error `RuntimeError: Cannot access data pointer of Tensor that doesn't have storage`",Possibly related: CC(torch_dispatch has unfaithful behavior w.r.t. wrapped numbers), ,Doesn't work on main,I'll assign this issue to me," would you mind giving an update, if there is one? (we just had an internal triage meeting where we revisit hipri issues)"," removing from release 2.4 milestone, since possible fix is not merged yet",The issue originally reported was fixed in this pull request:  There's another issue ( CC(Dynamo should prune nonlive captured variables ))) that tracks the broader problem of Dynamo not pruning nonlive captured variables.,closing as fixed
transformer,Accessing the `device` attribute of bias terms of `TransformerEncoderLayer` initialized with `bias = False` causes Attribute error," üêõ Describe the bug When performing a forward pass on `TransformerEncoderLayer` initialized with an even number of heads (2, 4, 6, ...) and without bias (`bias=False`) in eval mode (`model.eval()`), I face with an error `AttributeError: 'NoneType' object has no attribute 'device'`. Here is the stack trace:  Upon looking at the relevant code piece, I think the following lines will create the error because the bias terms are None and this line accesses the `device` attribute of None objects and thus the error.  Versions  ",2024-04-26T10:15:07Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/125015,"These lines may be troublesome as well because they access the bias terms, which might or might not exist depending on whether the layer initialized with bias or not.",Hey  can you provide code snippet where you are getting the error?,"Seems to be a duplicate of  CC(TransformerEncoderLayer raise `AttributeError: 'NoneType' object has no attribute 'device'` when `bias=False, batch_first=True`).",I just checked that the problem is fixed with the latest torch version. Feel free to close this issue.
gemma,Fix edge case in cudagraph pool detection,"  CC(Fix edge case in cudagraph pool detection) When we do cudagraph warmup, we record which outputs are in the cudagraph pool, so subsequently when we invoke a cudagraph and need to reclaim its memory we can free the prior run's outputs and make them error on access.  In warmup, we detect this by ignoring outputs which are an alias of an input that is not a prior output. We did this by checking data pointer. In very rare situations, a data pointer of a non cudagraph input might get reallocated to a cudagraph pool and causes us to ignore it.  This was happening with  gptfast error with gemma 2 when coordinate_descent_tuning was set to False. This updates so that we check aliasing with noncudagraph inputs by looking at storage pointer..  Unrelated: saw very weird behavior where an output had the same data pointer as a supposedly live input but not the same cdata ü§î  I would think that is not possible.  : D56607721",2024-04-25T23:53:46Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/124981," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",So is gptfast getting added to the benchmark suite?,The error is only reproducible in non standard configuration of gptfast. Even if it were added it would not cover this case.,"> One thing that makes me a little uneasy is the punning of StorageImpl address and data ptr address (there can be multiple StorageImpls to the same data ptr), but it is difficult to imagine a situation where this would actually cause a problem. How is this impossible ? wouldn't it imply a new allocation in which case we would want to make it a cudagraph output", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Live allocations, I mean! Obviously if it goes dead it will be reused."
llama,Fix bfloat16 serialization for ONNXProgram.save,Fixes CC(Export llama v3 to ONNX) Blocked by https://github.com/microsoft/onnxscript/issues/1471,2024-04-25T22:49:26Z,module: onnx triaged open source onnx-triaged release notes: onnx topic: bug fixes,closed,0,2,https://github.com/pytorch/pytorch/issues/124977,Maybe depends on https://github.com/microsoft/onnxscript/issues/1462,Resolved
transformer,Export llama v3 to ONNX, üêõ Describe the bug   Versions main Blocked by: * Issue https://github.com/microsoft/onnxscript/issues/1471  * PR https://github.com/pytorch/pytorch/pull/124977  * Issue https://github.com/microsoft/onnxscript/issues/1462,2024-04-25T21:46:15Z,module: onnx triaged onnx-triaged,open,0,1,https://github.com/pytorch/pytorch/issues/124973,Depends on https://github.com/microsoft/onnxscript/issues/1462
transformer,torch.compile fails on hugging face Mistral7b," üêõ Describe the bug **The following code fails:** import torch from transformers import AutoModelForCausalLM, AutoTokenizer model = AutoModelForCausalLM.from_pretrained(     ""mistralai/Mistral7Bv0.1"",     torch_dtype=torch.float16,     use_cache=True, ) model = torch.compile(model) input_ids =torch.randint(low=0, high=60000, size=(1, 32), dtype=torch.int64) attention_mask = torch.ones(1, 32, dtype=torch.int64) model(input_ids=input_ids, attention_mask=attention_mask) **It generates the following error:** Traceback (most recent call last):   File ""/work1/sleduc/torchtrials/compile_mistral7b.py"", line 13, in      model(input_ids=input_ids, attention_mask=attention_mask)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 489, in _fn     return fn(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl                                                                                                                                                                                                                                                                                                                      return self._call_impl(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn",2024-04-25T16:34:59Z,triaged oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/124946,What version of `networkx` do you have installed? It looks like this was fixed on their end quite some time ago: https://github.com/networkx/networkx/pull/4013. I suggest updating that package and trying again.,"Thanks for your answer Sorry for late reply, I was on PTO The networkx version that I was  using was 2.4, which did not have the fix you refered to I have upgraded to latest version and the issue has disappeared Thanks a lot!"
transformer,Conflict between bias=False and why_not_sparsity_fast_path in Transformer Module," üêõ Describe the bug Description: When setting the bias=false for Transformer layers, the model's biases are set to None. When the model is in evaluation mode, why_not_sparsity_fast_path becomes an empty string, allowing entry into the fast path logic. In this part of the logic, where bias is called because it is None, an error occurs. if not why_not_sparsity_fast_path:             tensor_args = (                 src,                 self.self_attn.in_proj_weight,                 self.self_attn.in_proj_bias,                 self.self_attn.out_proj.weight,                 self.self_attn.out_proj.bias,                 self.norm1.weight,                 self.norm1.bias,                 self.norm2.weight,                 self.norm2.bias,                 self.linear1.weight,                 self.linear1.bias,                 self.linear2.weight,                 self.linear2.bias,             )              We have to use list comprehensions below because TorchScript does not support              generator expressions.             _supported_device_type = [""cpu"", ""cuda"", torch.utils.backend_registration._privateuse1_backend_name]             if torch.overrides.has_torch_function(tensor_args):                 why_not_sparsity_fast_path = ""some Tensor argument has_torch_function""             elif not all((x.device.type in _supported_device_type) for x in Ôºåx.device.type throw error  Versions PyTorch: 2.1.2 Python: 3.7 Operating System: Linux ",2024-04-25T13:54:29Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/124937
rag,Tensor's storage changes computation outcome for CPU tensors.," üêõ Describe the bug  High level Tensor's storage changes the outcome of computation based on it. That is, an identical tensor residing in shared memory will yield a different `matmul` result from a counterpart residing in local memory.  Assuming that `matmul` computation is deterministic for CPU for a given hardware/software computation that means that one of the two computations is erroneous.  Detailed bug description This seems to occur any time the tensor is relocated from local memory to shared memory and then a copy of the shared memory is created. Originally the bug was found while debugging multiprocessing message passing between processes, but the following is the most barebone reproduction of the issue:  A run will result  in:  Despite the assertions around `weight` and `bias` passing, the computation based on them will produce results which are not identical. A few things to note: 1. The bug only reproduces for ""large enough"" `IN_DIM` which differs by `OUT_DIM` of the linear layer. This makes me think that the issue is in memory alignment or other related problems. 2. The bug only reproduces if exactly one tensor resides in shared memory. It does not reproduce if both tensors are in shared memory or neither tensor is in shared memory (the computations are equivalent holding the storage invariant, but differ if the storage differs) Related code: 1. Storage copy (https://github.com/pytorch/pytorch/blob/1ac402a96ca54e2d882b6daae4bf9e8c956ae03e/aten/src/ATen/StorageUtils.cppL35L54) 2. Matmul (https://github.com/pytorch/pytorch/blob/ed120b08c4828c39f116cfe1fb39195c844be485/aten/src/ATen/native/LinearAlgebra.cppL1402)  Root cause I h",2024-04-25T13:26:34Z,module: cpu triaged matrix multiplication,open,1,6,https://github.com/pytorch/pytorch/issues/124934,"A little bit more information: 1. I cannot reproduce this on `x86` machines 2. I cannot reproduce this on `arm` machines when written in pure C++, linking against a locally built `torch` 3. I can reproduce this on `arm` machines against `3.11` and `3.12` interpretters and against `Torch` version `2.2.1` and `2.3.0`","Another small piece of information: the code snippet works fine if everything is placed on `mps`. However it seems this precludes the ability to message pass with multiprocessing (which requires placement on `cpu`).  And it appears with `mps` they still share storage (?), per this link suggesting the following which returns `True`: ","What exactly is the magnitude of difference? Matrix multiply is pretty sensitive to algorithm selection, and I could definitely believe different alignment on inputs leads to different algorithms being chosen. If the result is still pretty close this may not be a bug.","~~I've found the differences to be as large as a difference between `NaN` and `nonNan`~~ (EDIT: I've attempted to reproduce that again and failed. As such, I remove the claim)  Commonly the `max` differences are around `1e7` causing `torch.allclose` to fail between the two results. The `mean` difference seems to be around `1e10` and the `median` difference around `0`. This is assuming the other side of the equation is a random distribution over 1,1. Of course the absolute magnitude is easy to blow up otherwise. Why would a different algorithm be chosen for identical inputs based on memory source?",", please let me know if the above makes sense. Would you have any more insight into what can be happening?","To confirm that two different kernels are called, do something like perf on the program and find the blas call that is doing the matmul. I think you will find it is different between the two occurences. 1e7 seems within tolerance."
transformer,"Missing description of Transformer argument ""memory_mask"" shape in 3D (including the batch dimension) case"," üìö The doc issue `nn.TransformerDecoder` seems to support 3D (N*nhead, T, S) memory_mask (since  CC([FYI] MultiheadAttention / Transformer)issue554929072), and the description of `nn.MultiheadAttention` has been updated correctly. But the description of `nn.Transformer` argument memory_mask's shape (https://pytorch.org/docs/stable/generated/torch.nn.Transformer.htmltorch.nn.Transformer, v2.3) missed 3D input case.   Suggest a potential alternative/fix Add description of ""(N*num_heads, T, S)"" for 3D memory_mask inputting case. ",2024-04-25T12:35:06Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/124931
llm,[onnx.export] Cache AllGraphInputsStatic instead of re-computing for faster export,"This PR is part of a series of PRs to significantly speed up torch.onnx.export for models with many nodes (e.g. LLM). See CC(ONNX export is unnecessarily slow (O(N^2))) for more analysis.  The inputs (dynamic inputs and constants) do not change as nodes are processed one by one and is expensive to recompute for every node.  Instead, introduce AllGraphInputsStaticWithCaching that sets a state in ConstantMap and refers to in subsequent nodes.  Move AllGraphInputsStatic out of anonymous namespace and add to header, so that it can be important and used in onnx.cpp.  Resolves (5) in CC(ONNX export is unnecessarily slow (O(N^2))). Partially fixes CC(ONNX export is unnecessarily slow (O(N^2)))",2024-04-25T04:41:10Z,release notes: onnx,closed,0,1,https://github.com/pytorch/pytorch/issues/124911,"I can't believe I accidentally put up a duplicate twice. I meant to put up a PR for (6) in CC(ONNX export is unnecessarily slow (O(N^2))) and I mistakenly put up 9 and then 5, both of which already had PRs. Sorry for the noise. Closing. (6 is coming  third times the charm)"
llm,Introduce reverse of symbol_dim_map for faster torch.onnx.export.,"This PR is part of a series of PRs to significantly speed up torch.onnx.export for models with many nodes (e.g. LLM). See CC(ONNX export is unnecessarily slow (O(N^2))) for more analysis.  Add a reverse lookup `dim_symbol_map` that is kept in parallel of `symbol_dim_map`. This avoids a linear time lookup, which creates a quadratic export time complexity.  Resolves (9) in CC(ONNX export is unnecessarily slow (O(N^2))). Partially fixes CC(ONNX export is unnecessarily slow (O(N^2)))",2024-04-25T04:28:19Z,release notes: onnx,closed,0,1,https://github.com/pytorch/pytorch/issues/124910,"Oops, I thought I hadn't pushed this yet, but I had: CC([onnx.export] Avoid linear loop over symbol_dim_map)."
llm,[onnx.export] Avoid linear look up in env for exist_in_env,"This PR is part of a series of PRs to significantly speed up torch.onnx.export for models with many nodes (e.g. LLM). See CC(ONNX export is unnecessarily slow (O(N^2))) for more analysis.  As part of torch.onnx.export, a reverse lookup is made in env. This is done for each node, and this lookup costs in proportional to the graph size, which incurs and overall O(N^2) time complexity.  A pragmatic solution is simply to keep a separate data structure to make this de facto constant time. So, this introduces a set containing all the values of env. Open to other ideas. Ideally `exist_in_env` wouldn't be needed at all, but to preserve current behavior exactly I'm not sure how that can be done.  Resolves (4) in CC(ONNX export is unnecessarily slow (O(N^2))).  This code change and the choice of py::set looks a bit more natural on top of CC([onnx.export] Avoid dict  unordered_map implicit copies), where the env is changed from a std::unordered_map to a py::dict. Partially fixes CC(ONNX export is unnecessarily slow (O(N^2)))",2024-04-25T04:18:58Z,triaged open source Merged ciflow/trunk release notes: onnx topic: improvements,closed,0,3,https://github.com/pytorch/pytorch/issues/124909,The committers listed above are authorized under a signed CLA.:white_check_mark: login: gustavla / name: Gustav Larsson  (07262461455192828170cedabec4a7fe285c1287), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,`RuntimeError: invalid dtype for bias` when use compile + autocast," üêõ Describe the bug When I tried using `torch.compile` along with `autocast` to infereance a llama's decoder block, I encountered `RuntimeError: invalid dtype for bias  should match query's dtype`.   Versions  ",2024-04-25T01:42:03Z,triaged module: amp (automated mixed precision) oncall: pt2,closed,8,5,https://github.com/pytorch/pytorch/issues/124901,is there any fix for this issue? I have the same exact error when combining autocast and compile. Thanks.,I am having this issue. Is there a fix please??,"hmm... running the repro above on a nightly, even when I turn off torch.compile I get a shape mismatch:  gives: ","> hmm... running the repro above on a nightly, even when I turn off torch.compile I get a shape mismatch: >  >  >  > gives: >  >  Sorry, the `att_mask` should be `8, 1, 2048, 2048`. I updated the repro code.",It works well with torch `2.5.0.dev20240812` !!!
yi,Certain .pyi files are not encoded as UTF-8 in Windows," üêõ Describe the bug On Windows platforms, PyTorch 2.3.0 causes mypy to fail with the following error messages:  Inspecting `_VariableFunctions.pyi` and `_VF.pyi` reveals that the files are encoded with Latin1 encoding instead of UTF8 like the rest of the pyi files. Looking deeper at the contents of the wheel obtained from `https://download.pytorch.org/whl/cu121/torch2.3.0%2Bcu121cp311cp311win_amd64.whl` shows that the files are incorrectly encoded there as well. It is important to note that PyTorch 2.2.2 does **not** have this problem. It was introduced in 2.3.0. As a workaround, the files can be reencoded to UTF8, which allows mypy to work correctly.  Versions PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Enterprise GCC version: Could not collect Clang version: 17.0.1 CMake version: version 3.27.4 Libc version: N/A Python version: 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22631SP0 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1650 Nvidia driver version: 552.22 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=4701 DeviceID=CPU0 Family=107 L2CacheSize=12288 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=4701 Name=AMD Ryzen 9 7900X 12Core Processor ProcessorType=3 Revision=24834 Versions of relevant libraries: [pip3] mypy==1.10.0 [pip3] mypyextension",2024-04-25T00:16:16Z,module: binaries module: typing triaged module: regression,closed,1,11,https://github.com/pytorch/pytorch/issues/124897,"I wonder how it happened, as I don't see the problem on trunk right now","I have no idea. I doublechecked the files in the source tree just to ensure that no changes had been made, and I doublechecked the encoding (which is set to UTF8) so it's not from the source files. My guess is it's coming from the process that fills in those files."," do you mind sharing the line around the offending characters? As at least on Mac builds all sequences are valid unicode ones  Does not fail and produces `(1137705, 1137683)` for me",Sure. I'll post them after I've had dinner. ,"Same here, I can get to a Windows machine but probably after dinner...","I dug around the file a bit and found where the characters are coming from. In the docstring for `cov(input: Tensor, *, ...)`, jump to line 6320. The offending text is this one:  The same issue is present in `_VariableFunctions.pyi` in the exact same line (6320).","Looking through the PyTorch source code, I think I know what's going on. The docstring comes from `torch/_torch_docs.py`. Reading through the docstring for `torch.cov` which starts at 2263, it appears that the issue is coming from this:  Note the doublequotes around `important`. It seems that for some reason those characters are getting mangled, which results in invalid UTF8 characters. No idea why this would be happening, but that's where the problem is coming from. Also important to note that the file itself is valid UTF8, so the issue is coming from whatever system takes that docstring and sets it in the pyi files.","Abovementioned changes were introduced by  https://github.com/pytorch/pytorch/pull/58311 and there hasn't been any significant changes to neither `torchgen/utils.py` nor to `tools/pyi/gen_pyi.py` that can warrant that change, that probably it has something to do with the build environment itself. But let's fix it two way:   Get rid of unicode characters in `_torch_docs.py`    Specify file encoding in torhcgen to be `ascii` (or utf8?)   Figure out why flake8 didn't raise the error, because torch_docs is missing encoding magic",Wondering if there's some flag for mypy to skip such files.,"Hi Torch community, does this UTF8 issue in Windows also affect version 2.3.1? ",Sorry for the late reply. I can confirm that the issue no longer appears on the latest version of torch (tested on 2.4.0)
rag,ShapeEnv canonicalization is still over-aggressive," üêõ Describe the bug I was inspecting some deferred runtime assert code and I noticed this:  The canonicalization is causing two problems: * We end up with a more verbose FX graph, because we have to have a separate FX node for the negation. This adds up. * We end up with a less good runtime error message, since the canonicalized expr is printed Maybe we can store the uncanonicalized along with canonicalized in RuntimeAssert, and then use the uncanonical for this stuff.    Versions main",2024-04-24T16:35:55Z,triaged oncall: pt2 module: dynamic shapes,closed,0,2,https://github.com/pytorch/pytorch/issues/124855,"Or cache the canonicalize function and simply call it whenever you need it, but pass the uncanonicalized expression around.","Hi  , doing issue scrapping , is this issue still relevant or it was fixed and can be closed?"
transformer,Dynamo Export Support for Qwen/Qwen-7B-Chat: Mutating module attribute _ntk_alpha_cached_list during export, üêõ Describe the bug I want to export Qwen/Qwen7BChat using Dynamo. I have a short repro script below that runs into an export error related to missing support for mutating module attributes. There are multiple models currently facing the same issue  The error is pasted below:   Versions Versions Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.16.0 [pip3] onnxgraphsurgeon==0.5.2 [pip3] onnxruntime==1.17.3 [pip3] onnxscript==0.1.0.dev20240417 [pip3] optree==0.10.0 [pip3] pytorchquantization==2.1.2 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240423+cu121 [pip3] torchtensorrt==2.3.0a0 [pip3] torchaudio==2.2.0.dev20240423+cu121 [pip3] torchdata==0.7.1a0 [pip3] torchtext==0.17.0a0 [pip3] torchvision==0.19.0.dev20240423+cu121 [pip3] triton==2.2.0+e28a256 ,2024-04-23T22:37:21Z,oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/124796
transformer,Dynamo Export Support for Google/Gemma-2B: Mutating module attribute inv_freq during export, üêõ Describe the bug I want to export google/gemma2b using Dynamo. I have a short repro script below that runs into an export error related to missing support for mutating module attributes. There are multiple models currently facing the same issue  The error is below   Versions Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.16.0 [pip3] onnxgraphsurgeon==0.5.2 [pip3] onnxruntime==1.17.3 [pip3] onnxscript==0.1.0.dev20240417 [pip3] optree==0.10.0 [pip3] pytorchquantization==2.1.2 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240423+cu121 [pip3] torchtensorrt==2.3.0a0 [pip3] torchaudio==2.2.0.dev20240423+cu121 [pip3] torchdata==0.7.1a0 [pip3] torchtext==0.17.0a0 [pip3] torchvision==0.19.0.dev20240423+cu121 [pip3] triton==2.2.0+e28a256 ,2024-04-23T22:26:35Z,triaged oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/124793
yi,[ROCm][Inductor] Disable conv cache emptying with hipgraphs,"When we warmup hipgraphs, we use cudagraph memory pool to allocate a large part of the memory. We don't necessarily execute the kernels on the GPUs. Therefore, we don't want to free up this allocated memory. However, this is conflicting with emptyCache call happening inside findAlgorithm where convolution algorithm benchmarking is happening. For benchmarking, we might use large memory allocations to cache algorithm results. As a fix, we just disable the emptyCache() call during cudagraph warmup.  As per this cuDNN PR which did the same thing for CUDA, we did not have a significant affect on memory footprint. https://github.com/pytorch/pytorch/commit/a8ff647e4233625d5f5840d46c93b00471c18fe8 ",2024-04-23T22:10:01Z,module: rocm open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor ciflow/rocm,closed,1,7,https://github.com/pytorch/pytorch/issues/124791," label ""module: inductor""  label ""ciflow/inductor""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,. extra warmup run should no longer be necessary," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 4, 5, linux.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""unrelated cuda failure; this was a rocmonly source file change; all rocm ci passing"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,"[DCP] Adds storage metadata, and passes it during the save path","This PR seeks to increase observability of save/load requests. This is accomplished with two main changes: 1. The creation of save_id and load_id:      a save_id and load_id is added to the filesystem writer. `save_id` is regenerated on every save call, and `load_id` is also regenerated on every load call.      both these ID's are stored in a new `StorageMeta` class, and saved as part of Metadata. (`load_id` is None when we save, and only set during load) 2. A new mechanism is implemented in the save path which gives the SavePlanner a chance to inspect the `storage_meta` object. The mechanism mirrors the same metadata exchange in the load path. In the load path, `storage_meta` is added to `metadata` such that the LoadPlanner can also access `storage_meta` before we begin loading. *If users now wish to access the checkpoint_id in the SavePlanner, they simple need to access the value in `storage_meta` from the `set_up_planner` call* *Additionally, users now have a generic way of passing data to the SavePlanner from the StorageWriter at the start of the save path, similar to the load path* This PR has been tested for backwards compatibility  meaning any checkpoints saved before this PR can continue being loaded after this PR. One major consideration is that there is limited forwards compatibility. If a checkpoint is generated _past_ this PR, there is no support for loading it using older torch versions. This brings up a fairly important point: since we expect the metadata object (which is saved to the disk) to continue evolving, and we want to support forwards compatibility, we explore patching `pickle` so we can at least add new members to",2024-04-23T20:11:56Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/periodic module: distributed_checkpoint suppress-bc-linter,closed,0,13,https://github.com/pytorch/pytorch/issues/124772, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: periodic / parallelnativelinuxjammypy3.8gcc11 / test (default, 1, 3, linux.2xlarge) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,why do we need a separate save_id and load_id when we have checkpoint_id already in the API?,"I also think it is an anti pattern to pass something from storage writer to save planner. Save planner is supposed to work purely on model metadata completely independent of storage specifics and pass a plan to storage writer for saving. So, it feels like we are breaking the abstractions. "
transformer,"Add scaled_dot_product_attention ""scale"" argument to nn.MultiHeadAttention "," üöÄ The feature, motivation and pitch Recently the Maximal Update Parametrization (muP, arxiv 2203.03466) is becoming prevalent in large model training becaus of its ""zeroshot hyperparameter"" transfer capabilities (i.e., grid search the best HPs on 210M params models and and train the 2B+ params with the same HPs) Refs:  FLM101B: An Open LLM and How to Train It with $100K Budget  CerebrasGPT: Open ComputeOptimal Language Models Trained on the Cerebras WaferScale Cluster  JetMoE: Reaching Llama2 Performance with 0.1M Dollars One modification involved in getting a transformer muParametrized is to modify the softmax scaling from $1/\sqrt{d}$ to $1/d$ or $1/d*\alpha$ with $\alpha$ being grid searched/learnable This can easily be done with `torch.nn.functional.scaled_dot_product_attention` using its ""scale"" argument, but nothing like that exists for torch.nn.MultiheadAttention.  Alternatives _No response_  Additional context _No response_  ",2024-04-23T09:16:32Z,triaged module: sdpa,open,0,2,https://github.com/pytorch/pytorch/issues/124718,I would be interested in working on this. ,Is there a reason why you wouldnt just using sdpa directly instead of using nn.MHA
transformer,"[dynamo] Unexpected SymBool appearing in ""is_causal"" inside scaled_dot_product_attention()", üêõ Describe the bug The error message is this:   The python program to reproduce the above is this    Versions env.txt ,2024-04-23T06:43:27Z,triaged oncall: pt2 module: dynamic shapes,closed,0,4,https://github.com/pytorch/pytorch/issues/124707,"What we need to do is force a specialization when this happens, similar to what we have been doing with int/float arguments. Shouldn't be hard.","I unsuccessfully tried to put together a smaller repro:  I'm unable to repro the error with this, so the real model is doing something different.","Dynamo might just be blindly specializing on the `> 1` test. If you modify Dynamo not to specialize immediately (which is better), then you'd hit it.","Doing issue scrapping, closing as original repro does not fail on trunk."
transformer,Remove activation checkpointing tag to get correct FQNs,"Fixes CC(Not loading optimizer state separately from checkpoint causes errors with FQNs)  When setting `use_orig_params = False` and using activation checkpointing, the FQN mapping as retrieved by the `_get_fqns` function is incorrect because the prefix that is added to the name of each activation checkpointed module, `_checkpoint_wrapped_module`, can still be present. I think this is an edge case with the `_get_fqns` function that was not addressed by this previous commit CC([DCP] Removes Checkpoint Wrapped Prefix from state dict fqns). Without the change, the list of object names for an activation checkpointed module with FSDP (and `use_orig_params=False`) can be something like:  Which will incorrectly return just one FQN, `{'model.transformer.blocks.0._flat_param'}`, when all the FQNs of the parameters of the transformer block should be returned. With the change, the list of object names will now have `_checkpoint_wrapped_module` removed:  And the FQNs are correctly retrieved and returned in `_get_fqns` when this condition is satisfied. The correct FQNs are:  ",2024-04-23T03:30:19Z,oncall: distributed open source Merged ciflow/trunk topic: not user facing module: distributed_checkpoint,closed,0,7,https://github.com/pytorch/pytorch/issues/124698,  I'm not sure why two of the checks are failing and I can't seem to trigger reruns. Could I get some assistance with this?, The failures are unrelated and won't block merging., merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,[INDUCTOR] [CPU] [GPT-FAST-MOE] large perf regression with coordinate_descent_tuning disabled," üêõ Describe the bug When the flag `coordinate_descent_tuning` disabled, GPTFASTMOE encounters a large perf regression: 52s > 1049s. The impact of disabling it on CPU is to fallback bmm and mm in decomposition.  Code snippet   Profiling coordinate_descent_tuning=True  coordinate_descent_tuning=False   Analysis According to the current analysis, there are two main reasons:  With flag disabled, bmm breaks the origin c++ kernel and some redundant calculations are generated to store uint8 to bf16 and then convert to fp32. The conversion from uint8 to bf16 takes a long time.  Many of the bmm kernels do additional contiguous in Mkldnn Matmul for noncontiguous and nontransposed format, which could not be solved by https://github.com/pytorch/pytorch/pull/122599).  Versions PyTorch: 34bce27f0d12bf7226b37dfe365660aad456701a  ",2024-04-23T03:04:08Z,oncall: pt2 oncall: cpu inductor,open,0,3,https://github.com/pytorch/pytorch/issues/124697,"With the following optimizations: 1. Add `VecConvert` for uint8 to bf16. Improvement of `graph_0_cpp_fused__to_copy_index_1`: 96.359ms > 35.693ms.  2. Extend the condition of `is_mkldnn_optimized_format` in Mkldnn Matmul: accept stride[0]=0. Input example: size=[1, 4096, 2], stride=[0, 1, 4096]. Improvement of `aten::bmm`: 60.056ms > 3.738ms. Overall profiling ","With bmm fallback, weight is converted from int8 to bf16 and Onednn uses bf16 weight. With bmm decomposition, type conversion and bmm are fused in one cpp kernel. Bmm fallback leads to the regression because the case is memory bound with batch size 1. Synced with Jiong, it is better to decompose bmm for memory bound case in lowering.","The fixed PR https://github.com/pytorch/pytorch/pull/124826 could harm the perf of LLAMA2. Hence, we need to further investigate other optimization methods for the issue."
transformer,Onnx backprop workarounds,"NB: **I have no intention of getting this merged in its current state**, but maybe it's possible to find alternative ways to address the issues I ran into here that are safer, or maybe use this as a jumping off point for supporting this kind of thing using the new dynamo_export() code path... I've been trying to export a complicated model to onnx, featuring a load of transformers, slicing operators and most importantly some backpropagation in the forward pass. This is basically so we can do inference on a diffusion model with classifier guidance, which requires differentiating through the denoiser at inference time. I don't think this is a particularly exotic use case so I feel like it's an important thing to support, and I could see it being used for other things like optimizing a latent code at runtime too. I made a post on the forums about it a while back, along with some tickets: https://devdiscuss.pytorch.org/t/exportingamodelcontainingbackpropagationtoonnx/1984  CC(Batch size can't be varied if you export self attention to onnx)  CC(Can't export an onnx with dynamic axes if it contains backpropagation)  CC(Can't differentiate through indexing operation)  CC(Problems differentiating through a transformer when exporting to onnx)  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace)  CC(Problems differentiating through certain operations when exporting to TorchScript) This branch was basically me brute forcing it so I could get it to work, and should be seen as more of a document of the issues I ran into than a literal merge request as some of the changes are a bit nasty. I did eventually get",2024-04-22T23:07:31Z,oncall: jit module: onnx open source Stale release notes: onnx ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/124677," :x:  login:  . The commit (d7247a11ebf131207c3096989c8a3f90a9c24866, d7a2b3a954cf326f270c2e40a92d838f0a7fce21, a2b24a0e59f522e30fc33f171c0333486bc41bb9, 44b152f2c1c640d36587570a4c180ed8841e0c19, cfff6c91a2ff031b81692c02674f7f046aaeb44b, b7166c34846b508ce8205f09498fb66e41665b6e, 6b305ac04a4d59bc569ca4c10b86ef8fe2274afa, 3554bee30901aa21b7dda0791c44cdea71765257, b20068d95b8455e6247b39bb87b1f4de3f7c45e7, f4ca44fa738448e95a279918ac6a923b2a9ff7f3, 615e7ccd6a4e9dff1ed4e998b5119fbd3aaf2c7f) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,[custom_op] register_autograd raises NYI on kwarg-only args,"  CC([custom_op] register_autograd raises NYI on kwargonly args)  CC([custom_op] fix schema inference for kwargonly args) It's unclear how to actually handle kwargonly args, so we raise NYI for now and hope that there aren't too many use cases for this. Also, an autograd impl (that raises error on backward) gets unconditionally registered for the op. We fix that to handle kwargonly args (assuming the user did not call register_autograd) by flattening the kwargonly args and sending them through the autograd.Function. Test Plan:  new tests",2024-04-22T18:39:58Z,ciflow/trunk release notes: composability ci-td-distributed,closed,0,0,https://github.com/pytorch/pytorch/issues/124638
transformer,Do not import transformers when import torch._dynamo,  CC([NJT] Allow construction of NJT within graph using offsets from inputs)  CC(Do not import transformers when import torch._dynamo)  CC([NJT] Inline through torch.nested.nested_tensor_from_jagged instead of graph break) Fixes  CC(`import torch._dynamo` appears to accidentally have a runtime dep on jax/xla (and other things)),2024-04-22T18:03:58Z,module: onnx Merged onnx-triaged ciflow/trunk release notes: onnx topic: bug fixes release notes: dynamo,closed,0,7,https://github.com/pytorch/pytorch/issues/124634,"https://github.com/huggingface/safetensors/pull/318 has been merged, maybe this check can be skipped at all?", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.8gcc11 / test (docs_test, 1, 1, linux.2xlarge)  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,please submit cherry pick to 2.3 branch
transformer,[NJT] Allow construction of NJT within graph using offsets from inputs,"  CC([NJT] Allow construction of NJT within graph using offsets from inputs)  CC(Do not import transformers when import torch._dynamo)  CC([NJT] Inline through torch.nested.nested_tensor_from_jagged instead of graph break) Creating symbolic nested ints within the graph is difficult. Using unbacked symints should solve the most important(?) cases in the mean time. See  CC([Nested Tensor] Support NT construction inside PT2 graph) Known gaps:  creating NJT from intermediate offsets (offsets created within the graph, as opposed to being offsets passed in as inputs)  when the same offsets is also passed in as a input to the graph. We are not smart enough to realize that the offsets from that input is the same and therefore would fail when  the sizes are compare (""s0 cannot be compared with u0"") ",2024-04-22T16:19:01Z,module: dynamo no-stale,open,0,4,https://github.com/pytorch/pytorch/issues/124624, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/soulitzer/296/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/9274613146,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llava,ERROR: No matching distribution found for torchvision==0.6.0+cu121," üêõ Describe the bug Torchvision isn't available for CUDA toolkit 12.1 and torchvision 0.6.0. Here's the bug, I am trying to run https://github.com/FanghuaYu/SUPIR. Thanks in advance.  Here's the error for SUPIR, which explains why I'm attempting to install this version.    Versions Sorry, but this doesn't seem to work either.   ",2024-04-22T03:20:34Z,module: binaries triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/124587,There's no `torchvision` **0.6.0** package for CUDA 12.1. You need to use a newer torchvision package. 0.6.0 is pretty old.
yi,`torch.func.jvp` yields different output compared to original function with `torch.nn.functional.layer_norm`," üêõ Describe the bug When I calculate a hessian vector product of layernorm with `torch.func.jvp`, the returned output from `jvp` (i.e., the value of the function, or the average gradient of the layernorm output) evaluated at the primals is different compared to calling the function directly. Code is below; I add the beginning part to ensure that the code is deterministic. In contrast, a linear layer has no difference in outputs (code for both linear layer and layernorm below).  has output:   Versions  ",2024-04-19T23:30:22Z,triaged module: functorch,closed,0,2,https://github.com/pytorch/pytorch/issues/124533,"This seems within fp precision tolerances, so this doesn't look like a bug. ","Using float64 results in 1e14 difference, so yeah, this is expected behavior."
yi,"[feature request] allow torch.compile calls with compiler options, but without modifying global dynamo options"," üöÄ The feature, motivation and pitch Originally discussed in:  CC(Case study of torch.compile / cpp inductor on CPU: min_sum / mul_sum with 1d / matmul-like with static / dynamic shapes) As I understood, any torch.compile call with passed options will modify global compiler options (and thus require `torch._dynamo.reset()` or sth like this). I suggest to introduce a ""local mode"" which applies passed options but only to a given function. Or maybe also introduce a ""compile options context""  this would be a ""semilocal"" mode  Alternatives _No response_  Additional context _No response_ ",2024-04-19T17:42:12Z,feature triaged oncall: pt2 module: dynamo,open,0,3,https://github.com/pytorch/pytorch/issues/124505,https://github.com/pytorch/pytorch/pull/111299 is this but it needs to be rewritten to be more obviously correct,"One thing that confused (and I think is confusing truly) is that the global option change was applied at the call site of the compiled function :) This is especially confusing if torch compile is used as a decorator, so the compiled functions and new options are specified in a file far, far away ","This would be very useful for having torch.compile calls in the PyTorch nn library (e.g. to have RMSNorm defined as `torch.nn.RMSNorm = torch.compile(torch.nn.RMSNorm)` in the core nn library for having fast RMSNorm code, even if the user is using eager/not compiling for the full model)  to guard them against accidental optionmodification by the user in the client code Maybe even a different decorator could be introduced like `torch.compile_scoped`"
transformer,[export] Llama3 export with dynamic shapes fails with constraint violations," üêõ Describe the bug I'm trying to export Llama38B model with dynamic shapes and encountered the following error. The same error occurs for Llama2 export as well. Any suggestions ?   The above core results in the following error   After applying the suggested fixes, I changed the line   this results in another error:  Note : Exporting model on cpu works (if I remove .eval().cuda())  Versions Versions of relevant libraries: transformers==4.40.0 [pip3] numpy==1.26.4 [pip3] torch==2.3.0+cu121 [pip3] torch_tensorrt==2.3.0.dev0+4323e3646 [pip3] torchvision==0.18.0+cu121 [pip3] triton==2.3.0 cc:   ",2024-04-19T16:55:57Z,onnx-needs-info module: dynamic shapes oncall: export,open,0,7,https://github.com/pytorch/pytorch/issues/124502,"> The exporting model on cpu works (if I remove .eval().cuda()) I'm guessing it's because some cuda kernel has some guards on this. Could you run with `TORCH_LOGS=""+export""` and share the logs?",Please find the logs here logs.txt logs after applying the fixes : logs_after_fixes.txt logs on cpu :  logs_cpu.txt,"hmm, I suspect it's cause of these `pad_last_dim` calls which are generating the guards... If you change the SDPBackend to something else, it might work... like ","Thanks   for the quick suggestion. It works and unblocks us. However, I see the attention op getting decomposed. I'll keep this bug open for now. ", you think  CC(Hybrid backed-unbacked SymInts) would have helped here?,  Any updates on this issue ? Thanks," have you tried ahead of time compilation with AOTInductor for the exported model? If so, have you been able to load the model back and do inference successfully? "
yi,Verbose log: [__aot_joint_graph] could not reconstruct view by re-applying a ViewMeta sequence. ," üêõ Describe the bug When running an internal MNIST_AUTOENCODER_TNT example model, we see this log every step. While this does not seem to affect the correctness and performance, this log is confusing and should not be a warning if it does not cause issues.  Error logs [rank0]:W0418 14:15:21.825000 140208555134976 torch/_functorch/_aot_autograd/functional_utils.py:240] [__aot_joint_graph] could not reconstruct view by reapplying a ViewMeta sequence. This error is possibly caused by dynamic shapes. Fallbacking to reconstruction using as_strided. Error message: buckout/v2/gen/fbcode/3ce39ffdd8faa9f1/caffe2/__gen_aten__/out/RegisterCUDA.cpp:17303: SymIntArrayRef expected to contain only concrete integers  Minified repro _No response_  Versions nightly ",2024-04-19T16:23:27Z,triaged oncall: pt2 module: aotdispatch,closed,0,2,https://github.com/pytorch/pytorch/issues/124499,This was added by the recent viewreplay changes in https://github.com/pytorch/pytorch/pull/121007 Warn was probably too aggressive  changing it to info as part of this forward fix: https://github.com/pytorch/pytorch/pull/124488, Can you connect also this ticket to the PR so that it will be closed?
transformer,torch.compile does not work since 2.2.1 on MacOS for some models, üêõ Describe the bug The execution hangs when first calling the model to warmup. _After_ will never be printed. This is on an Apple M3 Max. Smaller models work sometimes.  The last version that worked was torch 2.2.0.   Versions  Thanks for your help! ,2024-04-19T16:14:44Z,high priority triaged module: macos module: regression has workaround oncall: pt2 oncall: cpu inductor,closed,1,10,https://github.com/pytorch/pytorch/issues/124497,I see the same behavior on the torch 2.3.0 RC.," can you run `pip free` and share it's output here? Also, do you mind sampling hanged pytorch process in Activity Monitor and share the results? ","Can you try adding line `torch._dynamo.config.enable_cpp_guard_manager = False` in the script and see if you still see the issue? I don't have any reason to believe that it should matter, but given the timing of this issue, I want to be sure.",">  can you run `pip free` and share it's output here? Also, do you mind sampling hanged pytorch process in Activity Monitor and share the results?  and here is a gist with the sampling https://gist.github.com/maxbartel/f03722d324089b9d8c2605dbd8ac1433 > Can you try adding line `torch._dynamo.config.enable_cpp_guard_manager = False` in the script and see if you still see the issue? I don't have any reason to believe that it should matter, but given the timing of this issue, I want to be sure. I tried, but looking at https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/config.py it seems like none of the releases have this option. :confused: I tried the nightly build where this option is available and it doesn't change the behavior. The program still hangs...","Thank you for the spindump. It confirms my theory that it deadlocks because two libomp.dylibs are loaded into the runtime. From spindump:  I suspect first one is bundled with PyTorch and 2nd one comes from homebrew, that compiler is trying to load. We should put a warning against that/configure compile to use just one. In the meantime  do you mind uninstalling one from homebrew to make sure it solves the problem?","This solved the problem! üéâ I think you are right, compile should decide which one to use and stick to it. A warning is not too useful if `libomp` is a dependency for something else. Thanks for your help!"," what would be the debug option to dump the compiler invocation commands? As installing libomp from homebrew did not result in a hang for me, guess because compiler picks libomp bundled with torch", we're doing some maintenance in the triage meeting; I hope it's not presumptuous to assign the issue to you (seems like you're handling it?),Is there any update on this issue ?,Removing from 2.3.1 Milestone.  5/27 Cherrypick post deadline (End of day 5PM PST)
gemma,Fused Linear and Cross-Entropy Loss `torch.nn.functional.linear_cross_entropy`," üöÄ The feature, motivation and pitch It'd be great to have a fused linear and crossentropy function in PyTorch, for example, `torch.nn.functional.linear_cross_entropy`. This function acts as a fused linear projection followed by a crossentropy loss, e.g.  Compared to naive implementation, this fused function does not materialize the intermediate `logits`, saving a lot of VRAM. This function is quite common in models, such as LLMs, classification, and so on. When the batch size or number of classes is large, the intermediate `logits` would take up the majority of VRAM. For example, the latest LLMs, such as Llama 3 and Gemma, adopt extremely large vocabularies (128256K). Thus, the size of logits can become very large, consuming a significant proportion of VRAM. The calculation below shows the VRAM size for Llama 3 8B with a batch size of 81920 tokens: ` Therefore, a fused linear and crossentropy loss function that does not require materializing full `logits` may reduce VRAM consumption by half. Related: the same feature request for FlashAttention: https://github.com/DaoAILab/flashattention/issues/922  Alternatives _No response_  Additional context _No response_ ",2024-04-19T14:14:05Z,module: performance module: nn module: loss triaged,open,14,35,https://github.com/pytorch/pytorch/issues/124480,cc:  would be curious to hear your thoughts,"Funny thing, I have been working on exactly that very recently (for Graphcore IPU, but nothing specific about the hardware in the technique). It's definitely possible to serialize & fuse the linear + softmax + cross entropy combination, the only tradeoff being the recomputation necessary for the backward pass. Happy to help!","FWIW, I derived some formulas & wrote a (very rough) WIP prototype.  !image",: is this something we can pattern match for in compile/is it already handled by the scheduler in compile?,Update: forward finished  Output: 9.208518284318959e05 0.0 0.004878048780487805,"hi  thank you for the raising the issue, I'm suffering from the same issue and would be very happy if this feature is added to torch ! i have a question about the batch size you mentioned, 81920. is this equivalent to the number of valid tokens used for computing loss ? ( B*T where B is batch size and T is sequence length)  ",This is also relevant: https://github.com/mgmalek/efficient_cross_entropy/,"  I considered this type of optimization before  posted this issue. Similar optimization could be implemented easily with checkpoint, pseudo code:  But this implementation has a drawback: It needs to load the huge `w` multiple times, causes `num_chunks` times memory bandwidth cost. Using a custom kernel to avoid unnecessary recomputation in `checkpoint` is a clever idea!","> This is also relevant: https://github.com/mgmalek/efficient_cross_entropy/ i just tested this with llama3 8B by monkey patching huggingface transformers' model class   1x 80GB A100  `[B, T] = [1, 32768]` input size  bf16  flash attention (torch SDPA)  offloading param and gradient to CPU (because i want to scale up with FSDP after profiling)  activation checkpointing (in every layer and offload to CPU)  fused CE loss (n_loop_iters=8) i recorded GPU memory of 3 fwdbwd steps form the beginning (i didn't care warmup)  and this is for `[B, T] = [4, 20480]`   ", Could you please include the memory usage curve of the baseline (original HF transformers' model)?," of course i can, but the original model's peak memory would be greater than 12GB, because full precision logits  is already 15++ GB (`B*T*vocab*float32 = 1*32768*128256*4`)","Reasonable. I am just curious about the peak and fluctuation on the curve. Read the benchmark result in the repo, it seems that loading the weight multiple times incurs nearly 0 cost.  Ah, I forgot that matrix multiplication with large matrices will load both matrices multiple times into SRAM. As long as the block size is large enough, memory won't be a bottleneck. Following this (simplified) algorithm, `x @ w` will load `w` `M / BLOCK_SIZE_M` times if `x.shape` is `(M, K)`. Thus, as long as the chunk size is a multiple of `BLOCK_SIZE_M`, there is no extra memory read. !image In practice, the hierarchy is not only `SRAMDRAM`, it can be `Shared_MemoryL2_CacheDRAM`, but the story is similar. https://github.com/mgmalek/efficient_cross_entropy can even be implemented in pure pytorch without a custom kernel, but an inplace softmax is required to achieve the ""Optimization 1"". But if the loss needs to be computed with good numerical stability, a custom kernel will be unavoidable to reduce memory overhead (note: with inplace softmax we can get a numerical stable grad, but we cannot compute a numerical stable loss without `log_softmax`).", yeah yeah wll be uploaded!,"> > This is also relevant: https://github.com/mgmalek/efficient_cross_entropy/ >  > i just tested this with llama3 8B by monkey patching huggingface transformers' model class > and this is for `[B, T] = [4, 20480]`    Did you have to make changes to the kernel to work with batch sizes > 1?","> > > This is also relevant: https://github.com/mgmalek/efficient_cross_entropy/ > >  > >  > > i just tested this with llama3 8B by monkey patching huggingface transformers' model class > > and this is for `[B, T] = [4, 20480]` >  >  Did you have to make changes to the kernel to work with batch sizes > 1?  you can simply flatten these two axis/dim.","> > > This is also relevant: https://github.com/mgmalek/efficient_cross_entropy/ > >  > >  > > i just tested this with llama3 8B by monkey patching huggingface transformers' model class > > and this is for `[B, T] = [4, 20480]` >  >  Did you have to make changes to the kernel to work with batch sizes > 1? no i didn't.  like  said, you can flatten last hidden. but it's not optional, it's essential. see this line",">  Could you please include the memory usage curve of the baseline (original HF transformers' model)?   this is the result for the case where all settings are the same but CE loss is not fused. i changed the original code little bit like this   1x 80GB A100  `[B, T] = [1, 32768]` input size  bf16  flash attention (torch SDPA)  offloading param and gradient to CPU (because i want to scale up with FSDP after profiling)  activation checkpointing (in every layer and offload to CPU) i think fused kernel upcast logits to float32 too, so it's fair right?  added) since i recorded the first three steps (to see the change in GPU memory clearly), there may be noise in the time complexity measurement.","After reflection, I realize that chunking tokens is better than tiled softmax method for this usecase. The tiled softmax method would require a recomputation of `p` in the backward pass, but chunking tokens with a clever custom kernel doesn't.","> After reflection, I realize that chunking tokens is better than tiled softmax method for this usecase. The tiled softmax method would require a recomputation of `p` in the backward pass, but chunking tokens with a clever custom kernel doesn't. oh, i guess the performances of tiled softmax (like flash attention and your thought) and this kernel would be similar, but seems like this kernel is slightly better right? btw i have one question for this kernel.  following this line, if i see num_iters=8, it does not allow some sequences with 1510 or something.  If so, should i pad hidden with inf and labels with 100 ?   (there is nothing wrong with that in my opinion, but...) (I'm afraid i wont be able to contribute well to this discussion because i'm new to low level optimization like triton, lol)","  This kernel doesn't support `ignore_index`, currently. You can also tweak this line to the actual number of tokens in this chunk, and tweak this line to allow nonuniform chunking. The tiled softmax would require a recomputation, increase ~33% FLOPs. This kernel can be even better than https://github.com/DaoAILab/flashattention/blob/main/flash_attn/ops/triton/cross_entropy.py since it only loads the logits ONCE, while other implementations load the logits TWICE (forward + backward).","> This kernel doesn't support `ignore_index`, currently. You can tweak [this line] this line isn't sufficient to support `ignore_index`? https://github.com/mgmalek/efficient_cross_entropy/blob/049d44460051a82f58f7ff49a2ad0653ecf026d8/modules.pyL56"," Sorry, I overlooked this line!",">  ~This kernel doesn't support `ignore_index`, currently.~ You can also tweak this line to the actual number of tokens in this chunk, and tweak this line to allow nonuniform chunking. >  > The tiled softmax would require a recomputation, increase ~33% FLOPs. >  > This kernel can be even better than https://github.com/DaoAILab/flashattention/blob/main/flash_attn/ops/triton/cross_entropy.py since it only loads the logits ONCE, while other implementations load the logits TWICE (forward + backward). agree with that. what a kernel XD and like  said, it looks like supporting ignore_index too!"," I'm curious how you handle the back to back gemm in the backward pass if we don't want to materialize the large logits tensor (for better perf and for saving peak memory usage). In the backward pass, we need recompute the logits with a gemm: [BxT, D] x [D, V] = [BxT, V]. (B batch size, T sequence length, D hidden dimension; V vocabulary size). And then we need do 2 gemm to  compute gradients for weights: [BxT, D].t x [BxT, V]  compute gradients for inputs: [BxT, V] x [D, V].t Potentially, we need do two backtoback gemms here to compute gradients of inputs and weights. I'm thinking we can do similar thing as flash attention. But the difference here is the tensors here are not as 'skinny' as flash attention: in flash attention, the size of each row of the tensor (parameter d in the paper) is per head embedding dimension, while here the size of each row is the hidden dimension (which is H times larger. H for number of heads). Maybe the backtoback gemm can still be efficiently implemented. But I would like to ask if the larger row size here cause any trouble when you implement similar thing for Graphcore.",I tested mgmalek's kernel in a forked repo. https://github.com/kjslag/efficient_cross_entropy Some tests fail when I include tests that use ignore_index.,"This happened to be very relevant for a project of ours, so I spent (too much) time looking into it, recently. Just for reference, I think what this thread needs the most is a few more benchmarks, so here are some baselines in TFLOPs, memory and accuracy, when moving over N (number of tokens), V (vocab size) and H (hidden dim) for default values of N=16384, H=2048, V=131072: !fwdbwdLinear+Loss Performance over H  Defaults: N=B*S=16384, H=2048, V=131072 !fwdbwdLinear+Loss Performance over N  Defaults: N=B*S=16384, H=2048, V=131072 !fwdbwdLinear+Loss Performance over V  Defaults: N=B*S=16384, H=2048, V=131072  !fwdbwdLinear+Loss Memory Peak over H  Defaults: N=B*S=16384, H=2048, V=131072 !fwdbwdLinear+Loss Memory Peak over N  Defaults: N=B*S=16384, H=2048, V=131072 !fwdbwdLinear+Loss Memory Peak over V  Defaults: N=B*S=16384, H=2048, V=131072  !Linear+Loss Accuracy over H  Defaults: N=B*S=16384, H=2048, V=131072 !Linear+Loss Accuracy over N  Defaults: N=B*S=16384, H=2048, V=131072 !Linear+Loss Accuracy over V  Defaults: N=B*S=16384, H=2048, V=131072 `tritonzchunksinsram` is https://gist.github.com/JonasGeiping/6b724907ceb35555a6168dda9b9c4136, which is a  variation of Malek's code (linked above), but more accurate (and sometimes faster, but also with more autotune behind it). I think this variant could be even faster if the prologue is fused into each of the two backward matmuls, but that's beyond my triton abilities to implement efficiently. The torch checkpoint baseline is   (which needs a rewrite to reliably `torch.compile`)  Overall, having the logits z=Ax stored in SRAM is a bit of an unsatisfying solution to me, they need to be in float32 for precision until `softmax(z) = (z  lse).exp()` is computed, and so they introduce a lot of memory transfer, only to be immediately discarded, but I couldn't make other versions work.  The problem is, as also discussed above that is H is just too large. The forward pass is nice and fast, but in the backward pass, rematerializing z=Ax requires parallelization in (N,V) and accumulation in H. But, the backward gradients requires parallelization in (N,H) and (V,H), respectively. The other constraint that we need matrix shapes, so the gradient shapes should be at least, e.g. for dx: 16 x H. If this fits, a single operation can recompute z and immediately compute dx and dA. It doesn't fit  for normal H shapes like 4096, leading to a ton of memory load and write traffic (see failed attempts here or here). ",https://github.com/JonasGeiping/linear_cross_entropy_loss,"I have a question, do we need to compute the forward pass result? I think we can directly fuse all the things with a backward kernel during training, and thus, there is no need to save the result?","Yeah, both Malek's version and mine (both linked above), directly compute gradients during the forward pass loop over N and discard anything else. During the backward pass, nothing happens except for a scalar multiplication. ","Great! > Yeah, both Malek's version and mine (both linked above), directly compute gradients during the forward pass loop over N and discard anything else. During the backward pass, nothing happens except for a scalar multiplication."
yi,torch.compile: could not reconstruct view by re-applying a ViewMeta sequence," üêõ Describe the bug I got the following error when running a `torch.compile`'ed function: `W0418 04:04:09.562000 140622378160704 torch/_functorch/_aot_autograd/functional_utils.py:240] [__aot_joint_graph] could not reconstruct view by reapplying a ViewMeta sequence. This error is possibly caused by dynamic shapes. Fallbacking to reconstruction using as_strided. Error message: /home/weiwen/pytorchfork/build/aten/src/ATen/RegisterCPU.cpp:13111: SymIntArrayRef expected to contain only concrete integers` This is a regression. This error is caused by commit https://github.com/pytorch/pytorch/commit/e4c887fbf6fcc9b1864d10b3ab18294e5edebdfc. Before this commit, there is no error messages. With such errors, the program is still able to run but becomes much slower than before. In the real case, the program runs 7x slower than before. Expected behavior: No error messages and it has the same behavior as before this commit. Reproducer:  Please note that this producer is a simplified version of the real case.  Versions PyTorch version: 2.4.0a0+git236b0d1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (condaforge gcc 12.3.03) 12.3.0 Clang version: Could not collect CMake version: version 3.28.1 Libc version: glibc2.28 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.16.0x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: ",2024-04-18T11:15:04Z,triaged oncall: pt2 module: aotdispatch,closed,0,6,https://github.com/pytorch/pytorch/issues/124382,CC feng,"That's just a warning. You should be able to run even with that warning. The exception being raised does cause some runtime overhead, but it shouldn't be much slower. How slower does it get? What does the runtime looks like? As far as I can see, you are not doing anything computationheavy, so that may be the reason you are observing runtime difference.","The producer is a simplified version of the real case, just to trigger the error. In the real case, the function does computation. This function is called thousands of times when running the real case. The error message is printed to console every time this function is called. The program runs 7x slower than before. And it becomes even slower with larger scale of models and data.","Could you try running with 435db051d08c8a358ce43cc05670ac552e3d40c3 and e4c887fbf6fcc9b1864d10b3ab18294e5edebdfc, and report their running times?  Maybe we can bail out before actually trying to apply the `ViewMeta` sequence by checking whether we are running on dynamic shapes. What do you think?",Apparently this also affected some models perf internally. Fix: https://github.com/pytorch/pytorch/pull/124488 by  ,Yep the warn was too noisy (fix linked above)
yi,mypy.ini isn't valid,"running `mypy` gives `mypy: can't read file 'test/test_type_info.py` Upon further investigation, mypy.ini seems malformed, we're missing commas: https://github.com/pytorch/pytorch/blob/64f6ddf12c11738c3f4b1ed01cf4f699541496bf/mypy.iniL41L42 fixing this leads to some more issues. Could this be related to  CC(lintrunner-noclang job may diverge from `lintrunner` locally)? ",2024-04-17T23:40:37Z,module: typing module: lint triaged module: devx,closed,0,2,https://github.com/pytorch/pytorch/issues/124341,"One thing to note is that we don't run mypy directly anymore, we only use dmypy, and there are some differences which are material.",Fixed via CC(Fix typo in `mypy.ini`)  Can someone close this?  ?
yi,Request for modifying native allocator to use nvidia unified memory `cudaMallocManage`," üöÄ The feature, motivation and pitch I'm training models on cuda and I'm struggling with cuda OOM error. When I customized `torch.cuda.CUDAPluggableAllocator` with `cudaMallocManaged`, I solved the OOM error that the native memory allocator couldn't, however, my custom allocator didn't have many of the nice features that the native allocator had, so I would like pytorch to add support for nvidia unified memory.  Alternatives _No response_  Additional context _No response_ ",2024-04-17T15:42:36Z,module: cuda triaged needs research module: CUDACachingAllocator,open,1,5,https://github.com/pytorch/pytorch/issues/124296,". Overall, my understanding is that using unified memory is unpredictable since memory will get paged in and out of CUDA/CPU without the CUDACachingAllocator being aware. Determinism is a main reason why the CUDACachingAllocator has nice features/is able to make smart memory management moves, and not having predictability makes that a lot more difficult. If you have ideas on how you expect unified memory and our CUDACachingAllocator to integrate together, please do share!","I agree that there are indeed difficulties in this regard. But I see that your torchrec repository integrates Unified Memory and Caching, and I'd like to see this feature integrated in the pytorch library as well! https://github.com/pytorch/torchrec/blob/main/examples/sharding/uvm.ipynb","Hey! As you can see there, there is significant complexity involved in being able to use uvm in a satisfactory manner. As mentioned above, we do not want to add this to core as there are no satisfactory way to do so. For more advanced users that have enough knowledge of cuda memory model to want this feature, creating a Tensor in unified memory is feasible (as torchrec did) and I don't think there is much core can provide to make that easier.",> creating a Tensor in unified memory is feasible (as torchrec did)  can you give some code pointers to this? how do they use this feature?,"> > creating a Tensor in unified memory is feasible (as torchrec did) >  >  can you give some code pointers to this? how do they use this feature? Indeed this sounds confusing. Major LLM training frameworks (ColossalAI, Deepspeed) have multiple papers on dynamically offload/preoffload memory instead of triggering huge slow down trying to swap out pages on OOM, so it should be quite nontrivial"
transformer,SDPA + torch.compile: (*bias): last dimension must be contiguous," üêõ Describe the bug Hi, I noticed a bug with SDPA when using torch.compile, both on 2.2.2 and 2.3 RC. Reproduction:  and  Interestingly, adding  before SDPA call does not help. So the error below is not very helpful. We always have:    Versions ",2024-04-17T15:00:53Z,triaged oncall: pt2 module: inductor,closed,1,3,https://github.com/pytorch/pytorch/issues/124289, It appears https://github.com/pytorch/pytorch/pull/111721 did not fix everything,I am getting similar problems with noncompiled one,Same problem here
transformer,CUBLAS_STATUS_EXECUTION_FAILED when calling cublasGemmEx, üêõ Describe the bug I met a problem similar to CC(CUBLAS_STATUS_NOT_SUPPORTED when calling cublasDgemv) when using torch.multiprocessing  The following code should be quite easy to reproduce. All you need to do is  make sure CUDA VISIBLE DEVICES >1 .   Versions this is my environment  ,2024-04-17T05:48:22Z,module: cuda triaged module: cublas,open,0,2,https://github.com/pytorch/pytorch/issues/124262,Do you have any solutions for this issue?,"> Do you have any solutions for this issue? Not really, the issue seems  hardware related. 3090 x , while A800 ‚úÖ  ÔºöÔºâ"
finetuning,[FSDP] Unreleased reserved memory after FSDP wrap," üêõ Describe the bug When I was finetuning lLama270b on Intel GPU (64G/card, 8 cards), I met out of memory issue after FSDP wrap. Here is the printed log before and after FSDP wrap: >===memory_allocated before fsdp:  0.0 GB ===memory_reserved before fsdp:  0.0 GB ===memory_allocated after fsdp:  33.494 GB  ===memory_reserved after fsdp:  59.779  GB  The allocated memory is exactly from the base model weights (70b * 4 byte / 8), while there is 26GB reserved memory unreleased. I think that's why OOM happens.  Case verification based on one decoder layer To verify the case, I simplified the question and tested the dummy model (with only one decoder layer) on 2 cards. Here is the printed log: > all params for one decoder layer: 202383360 weight memory: 202383360 * 4 byte / 2 card / (1024)^3 = 0.377 GB =======fsdp memory_allocated before wrap:  0 GB =======fsdp memory_reserved before wrap:  **0 GB** model = FSDP (model)  wrap here =======fsdp memory_allocated after wrap:  0.381 GB =======fsdp memory_reserved after wrap:  **2.502 GB** In this case, the allocated memory is also compatible with base model weights, while there are still increased reserved memory unreleased.  I found that the unreleased memory comes from three parts: `_sync_module_params_and_buffers`, `FlatParamHandle.flatten_tensors`, and `FlatParamHandle._get_shard` in FSDP wrap: >**=== part 1: sync module** =======fsdp memory_allocated before sync module:  0.758 GB =======fsdp memory_reserved before sync module:  **0.775 GB** =======fsdp memory_allocated after sync module:  0.758 GB =======fsdp memory_reserved after sync module:  **1.367 GB** **=== part2: flat params** =======fs",2024-04-17T05:03:57Z,oncall: distributed,open,0,4,https://github.com/pytorch/pytorch/issues/124260,    varma     Huang         l   ,> `_sync_module_params_and_buffers` Could you try to run with `TORCH_NCCL_AVOID_RECORD_STREAMS=1`?,> > `_sync_module_params_and_buffers` >  > Could you try to run with `TORCH_NCCL_AVOID_RECORD_STREAMS=1`? I met the same problem when using tensor parallel all_reduce communication with megatron. I tried TORCH_NCCL_AVOID_RERORD_STREAMS=1 and it fixed. But I still have two questions as follow.  1. Will it cause some other problem like tensor being freed out of expectation? 2. The reason that tensor can not be reused under recording stream setting. Is that because the nccl stream inside cannot be controlled by users and it behaves strangely?," `TORCH_NCCL_AVOID_RECORD_STREAMS=1` changes the memory freeing behavior to be more intuitive.  Under `=1`, we hold an additional reference to the collective input and output tensors until you call `.wait()` on the collective's returned work object, preventing them from being freed. (If you run with `async_op=False`, then the `distributed_c10d` collective would call `.wait()` immediately.)      This mainly affects allgather and reducescatter (not allreduce) since allgather can free its allgather input after the allgather finishes and reducescatter can free its reducescatter input after the reducescatter finishes. If you call `.wait()` late, then the additional references to the allgather/reducescatter input would prevent those from being freed until then.      For tensor parallelism, this is generally fine since there is some dependent op ""close by"" that would require you to call `.wait()` anyway to use the result of the collective.      For data parallelism, this depends on your implementation. For example, you may not want to call `.wait()` on your reducescatters (for FSDP) until the end of backward since then you would hold all reducescatter inputs until then.  Under `=0`, the memory behavior depends on _GPU timing_, which is different from the rest of the PyTorch CUDA caching allocator. Calling a collective would call `recordStream` on the collective tensors. The caching allocator would only free their memory once the _GPU_ ops have finished. So if your CPU thread can run far ahead and issue lots of allocations, those allocations would not be able to reuse memory from tensors that had `recordStream` called on them since they are still waiting for _GPU_ ops to finish (which could take a while)."
yi,[5/x][AMD][Lowering Enablement] Hipifying aoti code_wrapper,Summary: as title Test Plan: CI & unit test patch on top of https://www.internalfb.com/phabricator/paste/view/P1214895953 to test Differential Revision: D56223917 ,2024-04-17T00:49:36Z,fb-exported Merged ciflow/trunk module: inductor ciflow/inductor,closed,0,13,https://github.com/pytorch/pytorch/issues/124241,This pull request was **exported** from Phabricator. Differential Revision: D56223917,This pull request was **exported** from Phabricator. Differential Revision: D56223917,The test failure looks real.,This pull request was **exported** from Phabricator. Differential Revision: D56223917,This pull request was **exported** from Phabricator. Differential Revision: D56223917,"Could you also run  to fix the lint issue? Otherwise, LGTM. Thanks!"," The test failure ""inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCuda::test_output_path_2_abi_compatible_cuda"" might be real. Could you take a look? Thanks!",">  The test failure ""inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCuda::test_output_path_2_abi_compatible_cuda"" might be real. Could you take a look? Thanks! Yes, it's a ROCM test, so likely relevant. Also please fix the lint error.",This pull request was **exported** from Phabricator. Differential Revision: D56223917,This pull request was **exported** from Phabricator. Differential Revision: D56223917,This pull request was **exported** from Phabricator. Differential Revision: D56223917," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
finetuning,Fix DDP no_sync when find_unused_parameters is True,"Fixes CC(DDP only syncs parameters used in most recent pass when `find_unused_parameters` is True.), CC(distributed training with c10d does not work with layerdrop in pytorch > 1.4) This PR fixes the bug introduced in CC(refactor autograd_hook) where parameters used within a `no_sync` scope are not respected when `find_unused_parameters` is set to `True`. The `local_used_map_` and `numGradHooksTriggeredMap_` variables should be updated regardless of the `no_sync` state. Tested and verified with fairseq2 and wav2vec2 ASR finetuning recipe. All gradients are correctly synced across workers as expected after applying this fix. ",2024-04-16T17:36:05Z,oncall: distributed Merged release notes: distributed (c10d),closed,0,5,https://github.com/pytorch/pytorch/issues/124193,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: kauterry / name: Kaushik Ram Sadagopan  (229dd18b2a7f84b4181b77fad3357f24df3193bd):white_check_mark: login: cbalioglu / name: Can Balioglu  (f7b8fdcff710a3f147f9d6965846fc90e1ca2d69, f85ddb2f352b61e58c3209f1e93195640855b64b)",/easycla,/easycla," merge f ""CI done"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Fakeifying a non-leaf subclass where inner tensor is noncontiguous incorrectly produces contiguous tensor.,Minified repro from internal:  ,2024-04-15T19:11:01Z,high priority triaged oncall: pt2 module: pt2-dispatcher,closed,0,7,https://github.com/pytorch/pytorch/issues/124090,"hipri for internal + potential correctness issue (although internally this manifests as an inductor stride assert. It looks like we end up with a contiguous tensor because: (1) Fakeification runs `clone()` when it sees that it needs to produce a nonautogradleaf here (2) That dispatches a clone() to the subclass's inner tensors, which faithfully clone the inner tensor (which might be arbitrarily strided, and cloning will not replicate the strides)","Running the clone feels a bit risky in general for subclasses. A few reasons I can think of: (1) FakeTensor normally can assume that the input is not a view in the `clone()` path linked above; it uses `.clone()` to mock up some autograd history so we set `fake_tensor.is_leaf` properly. But it is entirely possible to have a subclass who's outer wrapper is contiguous, but inner tensors have weird strides (that cloning will destroy) (2) calling `clone()` on a subclass can also affect the subclass's constant metadata (e.g., DTensor will run whatever sharding rule it has for `clone`). This could cause the metadata to diverge from what was passed in. One extreme option is to have a way of setting `is_leaf` on the fake tensor without running an operator on it, although I think that has its own risk (for one, it would be a pretty unsafe API).  in case you have any thoughts.","I am general in favor of adding private APIs to bash properties in, as long as we work out the safety conditions carefully. Arguably, I should have done this when I originally added fakeification, but instead I jumped through a lot of hoops to figure out how to make things work using existing APIs only.",also  ,I can't think of something that would go horribly wrong if we have a private function  (could be public tbh) like `torch.autograd.graph.forbid_in_autograd(t)` that will change t from being a leaf (t.grad_fn/grad_accumulator == None) to a nonleaf that will error out in backward (t.grad_fn == ErrorNode).,"Yeah, I think an error node seems reasonably. I can test it out and see if it makes CI unhappy",Note that the error node already exists: https://github.com/pytorch/pytorch/blob/5a735ece6b46248b6bb224bae4d0d7df24a335f0/torch/csrc/autograd/functions/basic_ops.hL15 Your code is really a python binding to c++ code that properly set the grad_fn to make_shared. You can look for other use of Error for details.
rag,[EZ][BE] Fix unknown pragma warning,By using `C10_DIAGNOSTIC_` macros instead of `pragma clang diagnostic` that puts appropriate compiler supported pragmas. Fixes following warning during the bazel build ,2024-04-15T18:35:25Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/124086, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
agent,[RFC] Support Torchrun for Out-of-the-tree Devices," üöÄ The feature, motivation and pitch  Motivation The torchrun entrypoint is awesome and convenient, and the TORCH_NCCL_ASYNC_ERROR_HANDLING environment variable preseted for worker process can timely crash the training process after detecting a stuck collective, then Agent can choose to restart training process. Our device have supported a similar environment variable like TORCH_NCCL_ASYNC_ERROR_HANDLING, and we hope torchrun can also be aware of the environment variables of outofthetree devices. Moreover, CC(fix torchrun script for custom device) has partially supported torchelastic's parameter parsing function for custom devices, but it does not work for torchrun entrypoint, because it requires explicit importing of device extensions.  Proposed Solution  Option 1 Support the feature described CC([RFC] Autoload Device Extension), then add processing for receiving environment variables from outofthetree devices at https://github.com/pytorch/pytorch/blob/7cd7a7aa8e0942da627095b23b94dc89f5a54943/torch/distributed/elastic/agent/server/local_elastic_agent.pyL294  Option 2 Add a more general environment variable like TORCH_COMM_ASYNC_ERROR_HANDLING.  Option 3 Add an argument carrying some custom environment variables to be passed to worker process for torchrun entrypoint like the rdzv_conf argumentÔºåhttps://github.com/pytorch/pytorch/blob/7cd7a7aa8e0942da627095b23b94dc89f5a54943/torch/distributed/run.pyL447L454  Alternatives _No response_  Additional context _No response_ ",2024-04-15T12:02:35Z,oncall: distributed module: elastic,open,0,0,https://github.com/pytorch/pytorch/issues/124058
yi,Compilation Error in release/1.7 due to Missing Definition for OneCycleLR in lr_scheduler.pyi," üêõ Describe the bug When using the **_release/1.7 branch_**, I encountered an error during the compilation process.  The error appears to arise from the specific content about OneCycleLR defined in torch/optim/lr_scheduler.py,  https://github.com/pytorch/pytorch/blob/release/1.7/torch/optim/lr_scheduler.py `class OneCycleLR(_LRScheduler):` yet there is a missing definition for OneCycleLR in torch/optim/lr_scheduler.pyi.  https://github.com/pytorch/pytorch/blob/release/1.7/torch/optim/lr_scheduler.pyi It seems that the .pyi stub file does not reflect the definitions in the actual .py file, leading to discrepancies that cause the compilation to fail.  And tag around 1.7.x is also missing the ""OneCycleLR"" in pyi  Error logs _No response_  Minified repro _No response_  Versions PyTorch version: 1.7.1 Is debug build: False CUDA used to build PyTorch: 10.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.1 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: Could not collect Python version: 3.7 (64bit runtime) Is CUDA available: False CUDA runtime version: 10.1.243 GPU models and configuration: GPU 0: GeForce GTX 1080 Nvidia driver version: 460.39 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] numpy==1.19.2 [pip3] torch==1.7.1 [pip3] torchaudio==0.7.2 [pip3] torchvision==0.8.2 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               10.1.243             h6bb024c_0   [conda] mkl                       2020.2                      256   [conda] mklservice               ",2024-04-15T08:50:33Z,needs reproduction module: optimizer triaged oncall: pt2 module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/124049,Does this error still occur on the latest version of PyTorch?,FYI typing is definitely a WIP and was pretty bad at the time. Also we do not backport fix so there is generally no plan to fix older versions.,> Does this error still occur on the latest version of PyTorch? NoÔºåI found only in 1.7.x,"I see, thanks. I will close this issue as  mentions, there's no plan to fix older versions.  btw I wonder if there is a reason you cannot use a later version of PyTorch?","> I see, thanks. I will close this issue as  mentions, there's no plan to fix older versions. btw I wonder if there is a reason you cannot use a later version of PyTorch? lol   cause so many 'old project' in my lab, any they do not support later version of pytorch, emmmm ,acturally, do not support later version of cuda..."
rag,Migrating from setup.py install/develop to leverage pip standards,"This PR is an attempt to migrate away from `python setup.py install` and `python setup.py develop` into a standards driven install, namely `pip install .` or `pip install e .`. These particular command variants have been deprecated for quite some time and could eventually be removed by setuptools. * https://packaging.python.org/en/latest/discussions/setuppydeprecated * https://blog.ganssle.io/articles/2021/10/setuppydeprecated.html The most notable issues of using these deprecated commands is that egg builds (.egginfo) are produced when using them. Eggs have been deprecated https://peps.python.org/pep0715/ and is notably causing issues with modern build tooling such as `uv` from being able to detect the pytorch package in docker images (e.g. `pytorch/pytorch:2.2.2`) as discussed in https://github.com/astralsh/uv/issues/2928 `setup.py` command arguments are also deprecated since they're not compatible with a standards driven install. When used for `install/develop`. For now, they were migrated to use pip configsettings which setuptools supports via their  ConfigSettingsTranslator for a couple of years. I decided not to modify `sdist/bdist_wheel` and other variants to keep the scope of the PR small, although this is a good start to eventually start migrating away. Open to feedback in case I missed an important change somewhere. ",2024-04-14T23:48:06Z,triaged open source release notes: releng module: dynamo,closed,1,5,https://github.com/pytorch/pytorch/issues/124027,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: samypr100 / name: samypr100  (ddcf65b920238dba6580dacdcbc31ec1f3c86a93, 9a04a704362beed7a4cc866f782d29cbb1480ef4, 61e267ab331b096a9d83d53dc1c885b60a14a709)", Would you be able to reapprove the runs? I've fixed the linting error and rebased from main. Thanks,Seems like the recent failed job  is a flaky one?," Thanks for the feedback > Do you mind splitting this PR into several smaller ones with a proper test plans? I.e. if CI is passing than all scripts touched by CI are good to go, but I don't want to accept changes to say utils/benchmark/examples as I'm not sure they'll still function as expected after the change I'm relying on CI to check areas where are not feasable for me to check. I only ran the primary tests on my personal system, but I don't have the hardware to go beyond that initial testing. If there's scripts that are not covered by CI, I'd be happy to remove them from this PR if you could kindly point out which ones are concerning. I do find it can lead to confusion having two separate install methods. Splitting into multiple PR's can make this change rather hard to test the system as a whole and could yield mixed artifacts (mixed legacy eggs and dists builds) which can have rather undesired sideeffects. I also wouldn't have the hardware to do so unfortunately. Do note that this PR is actually the first small step towards this and by no means we're still fully migrated to a compliant standards driven python install if this gets merged. > Also, just recently I had to deal with an issue ( ) in https://github.com/pytorchlabs/ao/pull/135 when changing an installation command resulted in pip creating some sort of isolated environment where it could not find required dependencies as setup.py is written in a way that it expects build time dependencies available by default and they are not mentioned in pyproject.toml I briefly looked at that PR and it seems you could leverage `setuptools.build_meta:__legacy__` as the build backend in the mean time to resolve the issues with finding local packages. Alternatively you can try force `pip install` not using build isolation by using `nobuildisolation` or completely disable the pyproject build backend by using `nousepep517` and rely purely on setuptools. Invoking `python setup.py install` is considered deprecated by standards, hence why I opened this PR. > I know this might sounds a bit unusual, but if one is building PyTorch in say conda environment (or using python runtime from Ubuntu), one might have setuptools package available, but not pip. Which makes me wonder, why we should force people to install pip if it might not be available on their system That's the direction python packaging is going üòÖ, this is not a recent change/development; This PR is primarily about following where python standard's are going. There's always `python m ensurepip` to bootstrap pip if it's unavailable for some reason and you can't install it at the system level or via `conda install pip`. Given the deprecation of setuptools  `install`, `develop`, etc., I wouldn't count on those always available there in the foreseable future.","Closing for now since the upstream `uv` issue has been resolved by adding legacy support, appreciate your patience during the review! If anyone with more suitable hardware to test all the required build configurations wants to pick this up again that would be awesome üëç "
transformer,[FSDP+TP] RuntimeError: 'weight' must be 2-D," üêõ Describe the bug Hi, I've changed pytorch's FSDP+TP example to HFT5 model and run on 3 nodes with 2 GPUs (total 6 GPUs)  commands `NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=INIT NCCL_IB_CUDA_SUPPORT=0 NCCL_SHM_DISABLE=1 NCCL_P2P_DISABLE=1 NCCL_SOCKET_IFNAME=ens4f0np0 NCCL_IB_DISABLE=0 NCCL_IB_SL=3 NCCL_IB_TC=96 NCCL_IB_GID_INDEX=3 torchrun nnodes 3 nprocpernode 2 noderank 0 masteraddr 10.10.10.10 masterport 8886 pytorch_transformer_2d.py batchsize=4 epochs=1` Command will be fine because another exmaples (e.g., DDP, FSDP) runs well However, problem occurs in `forward` in embedding layer.  Error log   Presumed Reason HFT53b has embedding layer of `Embedding(32128, 1024)` after `__init__` of `/opt/conda/lib/python3.10/sitepackages/torch/nn/modules/sparse.py` , `self.weigt has size of ` torch.Size([32128, 1024])`  but in `forward` of ``/opt/conda/lib/python3.10/sitepackages/torch/nn/modules/sparse.py` `self.weigt has size of ` torch.Size([32899072])`  (32899072=32128*1024) I guess somewhere of code flattens nn.Embedding and makes 2D error..  Another error if we add `""encoder.embed_tokens"": ColwiseParallel(),   `    occurs, I've find similar issues in  CC(FSDP Full Shard compatibility with BF16 AMP)issuecomment1826560494 but have no chances to make changes...  Code (pytorch_transformer_2d.py)   print HFT53b T5ForConditionalGeneration(   (shared): Embedding(32128, 1024)   (encoder): T5Stack(     (embed_tokens): Embedding(32128, 1024)     (block): ModuleList(       (0): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=1024, out_features=4096, b",2024-04-14T05:12:22Z,oncall: distributed,open,0,6,https://github.com/pytorch/pytorch/issues/124019, Do you need to pass `sharded_model` into `train` instead of `model`?,">  Do you need to pass `sharded_model` into `train` instead of `model`? Thanks!  , I missed that point.! I've changed every `model` to `sharded_model`. However 2D weight error is gone but `RuntimeError: aten.mm.default: got mixed torch.Tensor and DTensor, need to convert all torch.Tensor to DTensor before calling distributed operators!` occurs. Is it because I haven't written tensor parallelism for all layers?  Currently, only the line for the input of the first selfattention layer is written.  Current TP   Error log   Try So When I add Colwise parallelism of Embedding layer, `RuntimeError: Cannot writeback when the parameter shape changes Expects torch.Size([16449536]) but got torch.Size([0])` occurs   Error log   Try 2   Error Log  Is there any way to resolve them? Thanks for kind and quick reply!","I'm having the same issue while doing inference using model.generate (GenerateMixin from huggingface). I'm debugging it and am seeing that the shape of the embedding layer is is changed before sparse.py and after module.py (see the call stack below). Also I see that the total number of parameters changes (it becomes twice as less).    My setup: Python 3.11.5 accelerate                       0.30.1 torch                            2.4.0.dev20240515+cu121 torchaudio                       2.2.0.dev20240515+cu121 torchvision                      0.19.0.dev20240515+cu121 transformers                     4.41.2  My guess it has something to do with how parameters sharded, as FSDP from PyTorch mentions that parameters there are 1D: > use_orig_params (bool) ‚Äì Setting this to True has FSDP use module ‚Äòs original parameters. FSDP exposes those original parameters to the user via nn.Module.named_parameters() instead of FSDP‚Äôs internal FlatParameter s. This means that the optimizer step runs on the original parameters, enabling peroriginalparameter hyperparameters. FSDP preserves the original parameter variables and manipulates their data between unsharded and sharded forms, where they are always views into the underlying unsharded or sharded FlatParameter, respectively. With the current algorithm, the sharded form is always 1D, losing the original tensor structure. An original parameter may have all, some, or none of its data present for a given rank. In the none case, its data will be like a size0 empty tensor. Users should not author programs relying on what data is present for a given original parameter in its sharded form. True is required to use torch.compile(). Setting this to False exposes FSDP‚Äôs internal FlatParameter s to the user via nn.Module.named_parameters(). (Default: False)"," FSDP only allgathers parameters for `module.forward`, not custom methods like `module.generate` :( See  CC(FSDP Doesn't Work with model.generate()) for example. In our upcoming FSDP2 implementation, we have a workaround for this, e.g. see https://github.com/pytorch/pytorch/pull/125394.",">  FSDP only allgathers parameters for `module.forward`, not custom methods like `module.generate` :( See CC(FSDP Doesn't Work with model.generate()) for example. >  > In our upcoming FSDP2 implementation, we have a workaround for this, e.g. see CC([RFC][FSDP2] Added `register_fsdp_forward_method` for user fwd methods). Thanks for your comment! I've tried it like  But it fails with timeout. I will try later. This is might be the case that my model just doesn't fit or too slow. I'm running on 4 GPUs with 80GB , I'm running LLAMA 3 70B. It fails with: ","Related: *  CC(FSDP Doesn't Work with model.generate()) *  CC(torch 2.1 FSDP only some layers might not be working with training only a couple of layers) From what I've tested, as said in the above issue, wrapping `model.generate` in `torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params(fsdp_model)` works to solve the error `RuntimeError: 'weight' must be 2D`:  ALSO: if you are timing out, you should try setting `synced_gpus=True` in your call to `generate`. `generate` would hang for me in my training loop until I set it."
transformer,FSDP RuntimeError Output 0 of ViewBackward0 is a view and its base or another view of its base has been modified inplace.," üêõ Describe the bug When using FSDP to train a transformer model, I encountered the same error mentioned in CC([FSDP] RuntimeError when using FSDP with auto wrap for sequencetosequence language models such as T5, Pegasus). But I did not find a solution or workaround for this error.   Steps to reproduce the error: 1. Go to here, run the following command:  2. The following is the output of the Traceback:   3. The model architecture with FSDP wrapping can be found here  Expected Output No error and run the model.   Versions The `collect_env.py` was run on the login node which does not have GPU. I submit the job to nodes with 2 A100 GPUs.  Collecting environment information... PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Red Hat Enterprise Linux release 8.9 (Ootpa) (x86_64) GCC version: (GCC) 11.2.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.28 Python version: 3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.18.0372.9.1.el8.x86_64x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              40 Online CPU(s) list: 039 Thread(s) per core:  1 Core(s) per socket:  20 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU famil",2024-04-13T23:00:05Z,oncall: distributed,closed,0,14,https://github.com/pytorch/pytorch/issues/124017, Did you already make sure that shared parameters are either wrapped in the same FSDP module or in parentchild FSDP modules (i.e. _not_ in different FSDP modules that are not parentchild)?," Thank you for your comment. I am not really sure if they are wrapped in the same FSDP module. You can find the model architecture after FSDP wrapping here. You can see the whole model is the `_fsdp_wrapped_module`, and transformer layers are also the `_fsdp_wrapped_module` because I used the `transformer_auto_wrap_policy`.  If this file does not indicate if they are wrapped in the same FSDP module, please let me know how I can find out.","I have not had the chance to try to run your script, but if you want to check for shared parameters, you can run this before you wrap with FSDP:  Then, we can visually inspect the wrapping which you provided to know if the shared parameters violate the constraint.","Or actually, taking another look, I have another suspicion. The issue might be coming from:  FSDP only knows to allgather the parameters when you run the FSDP module's forward. Here, you are wrapping each transformer layer with FSDP plus the root module with FSDP. This should mean that the embedding is assigned to the root FSDP module, so you must run `self.model(...)` (i.e. `self.model.forward`) in order to trigger the allgather for the `tok_embeddings`.",The following is the output from the for loop:  I need some time to rewrite part of the code to put `model.module.get_embeds(q_input_ids)` into the forward call to see if getting rid of this can avoid the error.,"Okay. I found a solution. I only wrapped the transformer layers, that is `BertLayer` and `BasicTransformerBlock`, instead of the whole model plus the transformer layers. And the error disappeared! Thank you  for the help!"," If you do not wrap the whole model, I think you may not have any overlap of communication/computation ü§î  In particular, we need a parent module of the transformer layers in order to share data structures across them for overlap üò¢ .","I am curious if I wrap the `ModuleList` level which contains these transformers, will this still be an issue? ","yes :( since `ModuleList` does not implement `forward`, we should actually never wrap `ModuleList` with FSDP since then it would never run the allgather",Ahh I see.  Does this issue affect performance or efficiency? ,"If I write a `nn.Module` class with a forward call for these transformer layers, and I wrap this class with FSDP, will this avoid the not allgather issue?","I found this note: > Attempting to run the forward pass of a submodule that is contained in an FSDP instance is not supported and will result in errors. This is because the submodule‚Äôs parameters will be sharded, but it itself is not an FSDP instance, so its forward pass will not allgather the full parameters appropriately. This could potentially happen when attempting to run only the encoder of a encoderdecoder model, and the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap the submodule in its own FSDP unit. Since my model will call `word_embedding` and `lm_head` independently, can I just wrap these two linear layers with FSDP within the whole wrapped model? ","> Does this issue affect performance or efficiency? Wrapping `nn.ModuleList` affects correctness :/ Since parameters are not allgathered, you would see an error (probably a shape mismatch). > If I write a nn.Module class with a forward call for these transformer layers, and I wrap this class with FSDP, will this avoid the not allgather issue? I think so. > Since my model will call word_embedding and lm_head independently, can I just wrap these two linear layers with FSDP within the whole wrapped model? As long as if you wrap a module with FSDP, that module is used via running its `forward`, then you should be fine!",Thank you so much! I will close this issue for now 
transformer,"[2.3 dynamic shapes] backend='inductor' raised: LoweringException: AssertionError: indices must be int64, byte or bool. Got [torch.float32]"," üêõ Describe the bug I installed the final RC version of pytorch 2.3, and ran the following code, errors occurs.   Versions PyTorch version: 2.3.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Dec 19 2023, 08:15:18) [GCC 9.4.0] (64bit runtime) Python platform: Linux4.15.0189genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Quadro RTX 6000 GPU 1: Quadro RTX 6000 GPU 2: Quadro RTX 6000 GPU 3: Quadro RTX 6000 GPU 4: Quadro RTX 6000 GPU 5: Quadro RTX 6000 GPU 6: Quadro RTX 6000 GPU 7: Quadro RTX 6000 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.6 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          64 Online CPU(s) list:             063 Thread(s) per core:       ",2024-04-13T12:29:07Z,high priority triaged oncall: pt2 module: inductor,closed,0,20,https://github.com/pytorch/pytorch/issues/124006,Error logs: ,"I actually got a different error when running this code, but maybe due to some local setup issues. Someone else should try reproing this.   click to expand   ", that error means you need to clear /tmp/torchinductor cache,"Thanks, I'm able reproduce now",I test the code in the 2.3 release version and no error occurs.  ,I've just experienced the same error with the last nightly version. Do we had a regression or is it another case? ,In my case it seems originated around `if self.use_checkpoint:` ," yeah, the original test script still reproduces for me","Info on how to debug and change user code in this error would be helpful, even if not fixed","Mine, interestingly, was also near an if block about whether to use `checkpoint`. But removing the if block and checkpoint call has not fixed it. Interestingly it is also a block with a ""Shifted Window Multihead Self Attention"" call inside of it that is the last user code in the stack trace my end, though the impl is different than shown above (using mmdetection impl https://github.com/openmmlab/mmdetection/blob/main/mmdet/models/backbones/swin.py). I am on torch 2.3.1","I get  error instead when running the original repro. Seems to be related to AMP, but disabling it, I can run the script without error. Is there still a script to repro the index dtype issue?  /  ",I don't have the standalone ready for you but yes it is AMP. You can try to compile the `forward` at  https://github.com/yoxu515/aotbenchmark/blob/main/networks/encoders/swin/swin_transformer.pyL426,> You can try to compile the forward at https://github.com/yoxu515/aotbenchmark/blob/main/networks/encoders/swin/swin_transformer.pyL426 Do you mean using some random arguments to construct the module and call the compiled forward method should repro?,You could try but you know it could be not easy to rebuild all the params to correctly init the module. This is also the hard part and a lot of work required in our user ticket when you ask about minimal standalone repro. But probably you could decorate that forward with `torch.compile` and eventually try the e2e env at  CC(Pytorch 2.2 regression)issuecomment1924745100,"Also we had something similar related to AMP at  CC(AMP guards recompilation `dtype mismatch. expected Half, actual Float`)",Chatted with  about this. Can you run your command that repro this issue with TORCH_TRACE=somefolder and share us the content of that folder. We may be able to get some easier repro from TORCH_TRACE result.  ,It Is 5 months old. I was more than ready for 1 or 2 months to run this with all the flavours you want but I don't know if I have the currently the infra free to rerun this. In any case I have shared exactly the lines (with the Dockerfile) to run this training job with this code.,"We have verified that the original repro (at one point dynamic shapes, then amp) has been fixed with https://github.com/pytorch/pytorch/pull/138624.  , happy to look into other failures. please use a different issue  the fx_graph_runnable, or joint graph from TORCH_COMPILE_DEBUG / TORCH_TRACE are the easiest way to both collect and work on a repro.","> please use a different issue  the fx_graph_runnable, or joint graph from TORCH_COMPILE_DEBUG / TORCH_TRACE are the easiest way to both collect and work on a repro. Please add these best practices in the issue compiler template on Github as it is a pain to dissect a repro especially after many months when you have turned a lot of pages/activities in the meantime.",Thank you for the feedback  will do !
llama,Lowering after pointwise cat can lead to uncontiguous memory accesses," üöÄ The feature, motivation and pitch  We end up generating loads like this  This seems to be some heuristic that occurs during lowering (when we are much more flexible in changing iteration order). In fact, if we modify this line (https://github.com/pytorch/pytorch/blob/main/torch/_inductor/lowering.pyL1246) to  this boosts our performance on this kernel by 50%. In the broader kernel that  is looking at, it boosts our performance by 3x. cc:  on pointwise cat cc:  on layouts  Alternatives _No response_  Additional context _No response_ ",2024-04-13T07:01:09Z,triaged oncall: pt2 module: inductor,open,2,3,https://github.com/pytorch/pytorch/issues/124002,"Later on, might be nice to include an efficient version of pack/unpack into core with a more stable userfacing API. I also had similar pack/unpack functions in  https://github.com/pytorch/ao/issues/292, but some time ago there were problems of TorchScript handling shape manipulations",additional examples: fp6: https://discord.com/channels/1189498204333543425/1235775768756359289  bitnet: https://discord.com/channels/1189498204333543425/1240586843292958790 ,Examples are now on ao and out of cuda mode here  * https://github.com/pytorch/ao/issues/281 * https://github.com/pytorch/ao/issues/284 * https://github.com/pytorch/ao/pull/248issuecomment2123037948 And an internal only (for now) doc https://docs.google.com/document/d/1lXJa98pn2cMsgfPjzJ75jvgZrhtWfFlCrcFBZqQrDk/edit
rag,"on MPS, torch.embedding, Linear and others raise: RuntimeError: Placeholder storage has not been allocated on MPS device!"," üêõ Describe the bug When running torch/chat, get ""RuntimeError: Placeholder storage has not been allocated on MPS device!"" (Works on other devices) Repro Runs on pytorch ci @ https://github.com/pytorch/torchat/actions/runs/8670228545/job/23777955027?pr=162step:9:85         python generate.py device mps quant '{""embedding"" : {""bitwidth"": 8,\  ""group_size"": 0}}' checkpointpath ${MODEL_PATH} temperature 0 > ./output_\ eager         cat ./output_eager         python generate.py device mps quant '{""embedding"" : {""bitwidth"": 8,\  ""group_size"": 8}}' checkpointpath ${MODEL_PATH} temperature 0 > ./output_\ eager         cat ./output_eager https://github.com/pytorch/torchat/actions/runs/8670427606/job/23778418238?pr=162     File ""/Users/ec2user/runner/_work/torchat/torchat/pytorch/torchat/model.py"", line 237, in forward       h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^     File ""/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1532, in _wrapped_call_impl       return self._call_impl(*args, **kwargs)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^     File ""/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/sitepackages/torch/nn/modules/module.py"", line 1541, in _call_impl       return forward_call(*args, **kwargs)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^     File ""/Users/ec2user/runner/_work/torchat/torchat/pytorch/torchat/model.py"", line 299, in forward       q = self.wq(x)           ^^^^^^^^^^     File ""/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/sitepackages/torch/nn/module",2024-04-13T02:11:04Z,triaged module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/123995,"Can't reproduce this behavior, which hints that the problem is with the framework rather than with the op. Working example:  Example where exactly this error is printed  Grabbing for myself to print better error message","Here is my test result:  1. env  2. For the `input` and `weight`, if they are on the same device, it works as normal 1) both on CPU  2) both on MPS  3. The Runtime Error ""Placeholder storage has not been allocated on MPS device!"" occurred while `input` and `weight` are on different devices  4. I didn't encounter this issue on macOS 14.3 and before.","Got simillar issue: miniforge3/lib/python3.9/sitepackages/torch/nn/functional.py"", line 2264, in embedding     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: Placeholder storage has not been allocated on MPS device!"
transformer,FSDP Doesn't Work with model.generate()," üêõ Describe the bug I am trying to use FSDP, but for some reason there is an error when I do model.generate(). MWE below  Error below   Versions  ",2024-04-12T18:31:56Z,triaged module: fsdp,open,0,6,https://github.com/pytorch/pytorch/issues/123962,"FSDP relies on `nn.Module.forward()` to run allgather, so it will not know to allgather parameters for a non`forward` method like `generate()`. There are some known workarounds like calling `FSDP.summon_full_params(model, recurse=False)` to make sure to run the root module's allgather before inside `generate()` the submodules' `forward()` are called.","Ah I see, many thanks for the pointer! As a quick followup question, is FSDP designed to work with model.generate() like I use here (during model training)? I have heard that sample_during_eval should be disabled when using an FSDPTrainer because it becomes very slow, so I would assume the same thing applies here as well? And if that is indeed the case are there any frameworks to use for these GANstyle training schemes (generate samples during training rather than supervised learning)?","I am not sure why generation during evaluation would make things ""very slow""i f generation is effectively a forward pass. If your workload is communication bound (FSDP's allgather/reducescatter are not fullyoverlapped and are instead on the critical path), then running a generation step may just mean incurring this kind of communication boundedness an extra time per overall ""iteration"". I may need to learn more about what the constraint is causing the slowdown.","""Warning: Sampling may be very slow for FSDPTrainer"", source. The authors definitely seem to have observed this problem, and they are avoiding sampling at all costs when using the FSDPTrainer, so I would love to know what is causing the slowdown as well!"," Interestingly, the issue that they linked for FSDP ( CC(Issue with FSDP + HuggingFace generate)) is related to erroring, not slowdown, so I am not sure. Given what people are saying on the other issue about tensor parallelism, I suspect that if you have slow network bandwidth, then parallelisms like FSDP and TP might be much worse than the pipeline parallelism that they seem to otherwise support (which only requires peertopeer send/recv of activations). For setups that, for example, have NVLink, then FSDP should be fine to use for generation IMHO. Though, if the generation is using little forward compute (e.g. generating just one batch element), then TP might be more suitable, but it might be realistic to switch parallelisms for the same model.","Related: * https://github.com/huggingface/transformers/issues/30228issuecomment2350022762 > So after a lot longer than I would like to admit, I have uncovered all the gotchas of using `generate` with FSDP. >  > 1. As I mentioned above, `torch.distributed.fsdp.FullyShardedDataParallel(use_orig_params=True)` is required when instantiating your FSDP instance. Otherwise, you get the error `The tensor has a nonzero number of elements, but its data is not allocated yet`. It seems likely that `generate` is calling a `torch.compile`wrapped function. > 2. Calls to `generate` must be inside a `torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params(fsdp_model)` context, otherwise you get the error `'weight' must be 2D` from the input embeddings still being flattened by FSDP. > 3. Lastly, and most trickily, you must use `generate(synced_gpus=True)` when using differentlysized data across ranks. Otherwise, the different ranks will pause on different synchronization points leading to a deadlock. >  > Here is a minimum reproducible example to show these off: >  >  >  > **fsdp_generate.py** > "
transformer,`import torch._dynamo` appears to accidentally have a runtime dep on jax/xla (and other things)," üêõ Describe the bug We discovered this incidentally because when Jax is imported, it imports xla, which adds a process level hook that issues a warning if os.fork is used (which is is in some inductor internals). This sent us down the rabbit hole of seeing how this came to be Callstacks: https://gist.github.com/ScottTodd/981f1c8696887e0f9aef7fa57adbe84b Minimal repro showing that `import transformers` with the listed packages installed causes this. We should avoid triggering this in pytorch and also see if we can excise that probing from transformers. What is triggering this in torch (with a note it can be removed once a now closed issue is resolved): https://github.com/pytorch/pytorch/blob/main/torch/onnx/_internal/fx/patcher.pyL7 I think there is a laundry list of things that could be improved here but one thing that is causing the badness (in the onnx._internal.fx.patcher). Some other things that looked suspect while I was poking: * (torch) The PSA/warning that Jax does about os.fork is not wrong. I haven't looked into it deeply but it seems like it is in some inductor caching layer, and it may be worth doing that a different way. * (torch) If we need to be doing version probes of installed things, it is extremely costly to do this by way of `import foo`. Some of these deps are *very* heavy and make non trivial changes to the environment as part of their bootstrapping. * (transformers) Same as above but more pointed. This style of code is extremely costly and not what I would expect if doing `import transformers` https://github.com/huggingface/transformers/blob/main/src/transformers/utils/generic.pyL31 (will file this in transformers)",2024-04-12T17:07:59Z,high priority module: onnx triaged oncall: pt2 module: dynamo,closed,0,10,https://github.com/pytorch/pytorch/issues/123954,Related: https://github.com/google/jax/pull/18989discussion_r1562799747,"If we really need to do package installed probing, we should be using `find_spec`. Some notes here: https://stackoverflow.com/questions/1051254/checkifpythonpackageisinstalled Then for something like the onnx patcher, if that *really* needs to happen, I think it needs to be done only when using the onnx exporter and can probably just check `sys.modules` at that point to see if `transformers` has been *loaded* at runtime.","Some commentary at the point that is doing the fork: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codecache.pyL2780 I think this might be good to revisit how this is done (I would have expected that such backend compiler workers could be spawned and should have been very lightweight things that did not themselves pull in the world). However, the expectation set on what the state of the world is on doing this fork isn't quite right: the intention is that this get done very early in the process lifetime, but the sequencing is such that it is happening quite late (in this case, even after we've probed external deps, causing third party backends to start a bunch of threads, etc). I expect this mechanism needs a rethink.","see also  CC([`torch.compile`] When PT2 compile threads are forked after user code spawns threads, gets stuck, unbounded RAM usage) for more discussion on why the fork subprocess thing is terrible, and also why it is not so easy to get rid of","> see also CC([`torch.compile`] When PT2 compile threads are forked after user code spawns threads, gets stuck, unbounded RAM usage) for more discussion on why the fork subprocess thing is terrible, and also why it is not so easy to get rid of I believe it. Been stung by this basic issue many times. It's super easy to start using multiprocessing like that and it tends to often end badly and then is hard to back out without triggering other fallout.","FYI  in the above linked issues, the transformers devs fixed their bug where they unconditionally import jax. While this fixes the worst of it, I still think that under the principle of paying for what you use, the onnx exporter should definitely not be importing transformers as part of the main import of torch dynamo. It would be best to have no cross dependencies like that unless if the things in question are actually used (and only then if absolutely required).",as per conversation with  adding it to 2.3.1 milestone,"Follow up: when you call onnx exporter, it still imports. Is this desirable?",This is to confirm that transformers hasn't been loaded in 2.3.1.  The list of loaded module includes: ,"> Follow up: when you call onnx exporter, it still imports. Is this desirable? Not sure who you were talking to. I don't regularly invoke the onnx exporter and don't have an opinion. Kind of seems broken to have the dependencies run this way, though."
rag,[FakeTensor] calling a view op mutates the nbytes on the input's storage, prints:  I printed the storage id just to show that the storage is not getting swapped out  we are inplace changing the nbytes on the input's storage. It looks like this is happening because: (1) FakeTensor caching eventually calls `set_()` to handling aliasing when it sees a view op (code) (2) `set_`'s meta function seems to mutate the nbytes (code) ,2024-04-12T15:52:34Z,triaged oncall: pt2 module: fakeTensor,open,0,1,https://github.com/pytorch/pytorch/issues/123950,"This is a similar issue to  CC([Meta Tensor] Inplace set storage of meta tensor will alter the storage's nbytes if meta tensor's nbytes is smaller) (the root cause seems to be the same) but the problem it is showcasing is slightly different (and worse). I'll leave this issue open, although it should be fixed by https://github.com/pytorch/pytorch/pull/123880"
yi,[DCP] Adds support for non-primatives in async_save by deep copying during cpu offloading,"  CC([DCP] Remove overlapping loader in async case)  CC([DCP] Adds support for nonprimatives in async_save by deep copying during cpu offloading) Adds support for nonprimatives in async_save by deep copying during cpu offloading. If users are not type checking, the expectation in async is likely that the object is copied Differential Revision: D56065237 ",2024-04-12T15:28:33Z,oncall: distributed fb-exported Merged ciflow/trunk topic: not user facing module: distributed_checkpoint,closed,0,5,https://github.com/pytorch/pytorch/issues/123941,This pull request was **exported** from Phabricator. Differential Revision: D56065237,This pull request was **exported** from Phabricator. Differential Revision: D56065237,This pull request was **exported** from Phabricator. Differential Revision: D56065237, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[TESTING] Don't consider storage changes for incremental fake tensor update,"  CC([TESTING] Don't consider storage changes for incremental fake tensor update) Let's suppose that we are doing incremental fake tensor update after this optimization:  When we compare the fake tensor of `convert_element_type_1` with `device_put_1`, we find they are identical except that the storages are different (as `convert_element_type` generates a new tensor). The current incremental fake tensor update concludes that repropagation is necessary. However, it seems like this repropagation is... not necessary? In particular, for Inductor IR with no mutations (not true in reality, but humor me for now), we don't care about the aliasing properties, all the downstream prop would all be the same. This PR is testing if this hypothesis is true, but we also need a more robust correctness argument. Signedoffby: Edward Z. Yang  ",2024-04-12T15:07:41Z,module: inductor ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/123940,"> However, it seems like this repropagation is... not necessary? In particular, for Inductor IR with no mutations (not true in reality, but humor me for now), we don't care about the aliasing properties, all the downstream prop would all be the same. This PR is testing if this hypothesis is true, but we also need a more robust correctness argument. I don't think this is correct. Aliasing of intermediates is used by the reinplacing pass to determine whether it's safe to introduce mutations into the graph: https://github.com/pytorch/pytorch/blob/2f0fc04fa39017761a72f46ae50571e2fd35d9da/torch/_inductor/fx_passes/reinplace.pyL436 It's also used by the noop removal pass to determine if an optimization introduces a new alias between graph outputs and inputs, which is important for correctness: https://github.com/pytorch/pytorch/blob/2f0fc04fa39017761a72f46ae50571e2fd35d9da/torch/_inductor/fx_passes/post_grad.pyL658L662 I think it might be possible to have a fast path where you just copy the metadata from the existing `FakeTensor` but with a new storage, however I would have thought this was covered by the fake tensor caching that was added a while back."
yi,fix cpp path in torch/_C/_autograd.pyi,"The file `tools/autograd/init.cpp` does not exist, I think the right path is `torch/csrc/autograd/init.cpp`.",2024-04-12T08:33:11Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/123924, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[Dynamo] fix opcode `YIELD_FROM` and `SEND`,  CC([pytree] add `tree_iter` function)  CC([Dynamo] fix opcode `YIELD_FROM` and `SEND`) This PR is split from CC([pytree] add `tree_iter` function and fix opcode `YIELD_FROM` and `SEND`).  CC([pytree] add `tree_iter` function and fix opcode `YIELD_FROM` and `SEND`) ,2024-04-12T05:13:58Z,open source Merged ciflow/trunk release notes: fx module: dynamo ciflow/inductor release notes: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/123912, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[Meta Tensor] fix meta inplace set storage,Fixes CC([Meta Tensor] Inplace set storage of meta tensor will alter the storage's nbytes if meta tensor's nbytes is smaller)  ,2024-04-11T23:30:20Z,triaged open source Merged Reverted ciflow/trunk release notes: composability topic: bug fixes module: dynamo ciflow/inductor,closed,0,52,https://github.com/pytorch/pytorch/issues/123880, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 10 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8clang10 / test (default, 1, 3, linux.2xlarge)  pull / linuxfocalpy3.8clang10 / test (default, 3, 3, linux.2xlarge)  pull / linuxfocalpy3.8clang10 / test (crossref, 1, 2, linux.2xlarge)  pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 3, linux.2xlarge)  pull / linuxfocalpy3.8clang10 / test (dynamo, 3, 3, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ",Test fails seem real?," thanks for the comments! I tried to make sure symints can be compared only when they are both backed. But it looks like there is still an error like AssertionError: String comparison failed: '8*s1*s3 <= 8*s0*s1' != '\nException raised from'35337). The other error that looks less related is this one ([ AssertionError: s0 (could be from [""L['inputs'][0]._base.size()[0]""]) not in {s2: [], s1: [], s0: []}.  If this assert is failing, it could be due to the issue described in https://github.com/pytorch/pytorch/pull/90665](https://hud.pytorch.org/pr/pytorch/pytorch/123880 CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)88736)). Is there a way to fix the string comparison issue?","those are expect tests, if you're able to run the test, EXPECTTEST_ACCEPT=1 will update them", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 3 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 3, 3, macosm1stable), trunk / linuxfocalcuda12.1py3.10gcc9 / test (nogpu_AVX512, 1, 1, linux.2xlarge), trunk / linuxfocalcuda12.1py3.10gcc9 / test (nogpu_NO_AVX2, 1, 1, linux.2xlarge) Details for Dev Infra team Raised by workflow job ", thanks for taking another look! I believe now the tests are failing bcoz some of the test case expect an empty string and some expect `8*s1*s3 <= 8*s0*s1` in `test_subclasses.py::TestNestedTensor::test_inputs_to_compiled_fn_are_views`. I just need to figure out what those are. I actually cannot get a repro locally on my cpuonly build so debugging it is a bit hitormiss. Will try to address this asap.,My guess is that it's the dynamic vs not test. We have some prior art for having double expect test for this case., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 3, 3, macosm1stable) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  BC Lint  pull  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ," this pr should be ready to merge! I disabled one test in `test_subclasses.py::test_inputs_to_compiled_fn_are_views`, marked it as expected failure and fixed the linting. ","The errors look real, alas...",alas indeed. what an tangled web of failed test cases we are in LOL. I am looking into those failed test cases and will try to fix them asap.,"This PR is necessary for me to land https://github.com/pytorch/pytorch/pull/122434, so let me know how I can help move things along",Hi  thanks for the pin and sorry for the holdup! I put in one last fix in `symbolic_shapes.py` to see if CI can pass this time. If this is still failing and I wonder if there is any suggestion to have this repro locally. My local cpuonly build cannot repro any of those failures before and debug those perCIrun is less than ideal.,assigning myself to finish this off,"Hmm, this is quite troublesome. The root cause of the problem is the test for storage size is generating new guards that we didn't use to have, and these guards are in relation to the original size of the base tensor. In the past, we've suppressed these guards explicitly, but it looks like it didn't work here. A sample guard:  It actually looks like this is an interaction with caching:   ",It looks like just suppressing the guards here works,"I undid the manual test failure bashing, I also switched us to do size oblivious guards, and I turned on suppressions. Let's see what else fails."
rag,[Meta Tensor] Inplace set storage of meta tensor will alter the storage's nbytes if meta tensor's nbytes is smaller," üêõ Describe the bug My repro is as follows. Note that with `device('cpu')` it works fine.  I pinpointed the root cause of this issue to this line in TensorShape.cpp, in which unlike `set_storage_cpu_`, `set_storage_meta__symint` will not check if the `nbytes` of target tensor is less or equal to the storage that it is setting to. Therefore every time a meta tensor is being set to a storage which have a larger nbytes, the meta storage shrinks in terms of `nbytes`. The above repro should error out on a subsequent call on `timesteps[1][None]` for sharing storage, but bcoz of this line, it doesn't.  cc:     Versions Collecting environment information... PyTorch version: 2.4.0.dev20240410+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 10.5.01ubuntu1~22.04) 10.5.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.027genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.66 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 535.161.07 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train",2024-04-11T23:20:37Z,triaged module: meta tensors,closed,0,3,https://github.com/pytorch/pytorch/issues/123879, CC([Meta Tensor] fix meta inplace set storage) is up as a draft PR for the fix., is this really `module: mta` (= multitensor apply a.k.a. foreach)?,https://github.com/pytorch/pytorch/pull/123880 was reverted due to breaking inductor.
transformer,Dynamo unsupported: dynamic padding,"Dynamo raises data dependent value error if the pad size is computed by unbacked values (not sure if I used the term correctly), the issue still persists after using `torch._constrain_as_size`. However, if the pad size is computed from a tensor dim size then dynamo works. The is one of the blocking issue from CC(ONNX export fails for aten::full_like op when exporting UDOP model from transformers), where the pad size is likely unbacked. The pad size can be either positive or negative, maybe this is posing some difficulty during graph capturing. The eventual fx graph should look like the one in example 2.  Example 1: tensor.item() as pad size fails   Example 2: tensor.size(0) as pad size works   Experiment 3: tensor.item() + torch._constrain_as_size() as pad size fails  ",2024-04-11T17:26:16Z,triaged oncall: pt2 module: dynamic shapes module: dynamo,open,1,9,https://github.com/pytorch/pytorch/issues/123855,"Huh, I didn't realize we support negative pad. Does it crop in that case? If you can force the padding to be positive only, then an appropriate `torch._check(size_item > 0)` would get you past the conditional. Otherwise, I need to understand if this decomposition can be rewritten to be branchfree:  Maybe it's possible, using a `sym_min` or something?","Tried rewriting decomp with `sym_max` and `sym_min`, hit      Could not guard on datadependent expression Ne(32*u0  32*Max(0, u0) + 512, 32*Min(0, u0) + 512) during     prims.copy_to(c_output, c_input) The expression should always evaluate to False though.",,"Yeah, our Min/Max reasoning is kind of weak.  So in this case, we'd be able to reason it out by doing a case split (consider u0 > 0, consider u0 < 0), and then the individual items are statically known.  This doesn't seem too complicated to implement, though I don't know if there's potential of exponential explosion if there's a lot of min/max. .","It might also be enlightening to figure out why we are doing this equality test in the first place, maybe it isn't necessary if the inputs are contiguous or something.","If this is a common pattern, we can implement the rule `op(min(a, b), max(a, b)) = op(a, b)` whenever `op` is commutative for `+` or in general. On a side note, I think more and more that we should use just use Z3 to help us evaluate expressions, rather than implementing this... lessthangood algebra system ourselves.","There's https://github.com/pytorch/pytorch/pull/123661 which changes the decomposition for constant pad. It would then require unsafe_masked_index to support dynamic shapes (which is doable in theory, but hard to achieve with all the guard_size_oblivious scattered around the codebase)","What do you mean by ""support dynamic shapes""? Do you mean ""unbacked symints""? Also, if anything the `guard_size_oblivious` should help with the unbacked symint support. Can you elaborate?","Sorry, meant data dependent values. Using that PR and some more changes, I'm at    "
yi,[dynamo] Support numpy.iinfo/finfo,  CC([dynamo] Relax strict_mode for autograd.Function forward inputs)  CC([dynamo] Support Tuple[int] args to autograd.Function)  CC([dynamo] Emit warning to turn on capture_scalar_outputs)  CC([dynamo] Fix  on userdefined nn.Module)  CC([dynamo] Support numpy.iinfo/finfo)  CC([dynamo] Graph break on uninitialized nn.Module)  CC([inductor] Handle meta tensor ops in graph)  CC([dynamo] Improve constantprop for regex/torch.__version__) ,2024-04-11T02:20:42Z,Merged topic: not user facing module: dynamo ciflow/inductor keep-going,closed,0,0,https://github.com/pytorch/pytorch/issues/123803
transformer,Add rotary_embedding CPU operation,"**Summary**: Introduces a new rotary_embedding operation to PyTorch's CPU backend. This operation applies rotary positional embeddings to query and key tensors in attention.  The implementation supports 4D query and key tensors, and uses 2D cosine and sine embeddings for rotation. **Motivation**: Rotary positional embeddings have been shown to significantly improve the performance of transformer models by providing a more effective means of encoding positional information compared to traditional positional embeddings. This operation aims to provide PyTorch users with an efficient, native CPU implementation of RoPE, facilitating development in LLMs(NLP) and beyond. **Technical Details**: The operation is implemented as a custom ATen operation in C++ for CPU tensors. Input validation checks (using TORCH_CHECK) ensure tensors are of expected dimensions. **API Changes**: Introduces a new function signature to the torch namespace:   q, k: 4D Tensors representing queries and keys. (batch, n_heads, seq_len, head_dim // partial_rotary_factor)  cos, sin: 2D Tensors for cosine and sine rotational embeddings. (max_seq_len, n_head)  position_ids: 1D Tensor indicating the position IDs for embeddings. (seq_len)  unsqueeze_dim: Integer specifying the dimension for unsqueezing. TODO:  add tests  update documentation  benchmarking with native impl/python impl  extend to CUDA support Acknowledgments: Thanks to the PyTorch community and contributors for feedback and suggestions during the development of this feature. ",2024-04-10T17:18:11Z,triaged open source Stale module: python frontend,closed,1,3,https://github.com/pytorch/pytorch/issues/123738,:white_check_mark:login: makaveli10 / (e3a915ac32ae9c904292433c6f58e8044f7d8dee)The committers listed above are authorized under a signed CLA.,"Hey , is there an accompanying issue that this PR addresses/would you be willing to open one otherwise? For reference, issues are a good place to host community discussion for a new feature request. Which would help us determine whether this should be added to PyTorch","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Multiplying large matrices returns 0 matrix when matrices are in mps, üêõ Describe the bug Multiplying large matrices returns a zeroes matrix when matrices are in mps. Example:   Versions  Reproducible on two machines:  and:  ,2024-04-10T10:08:08Z,triaged module: correctness (silent) module: mps,closed,2,7,https://github.com/pytorch/pytorch/issues/123716,We are not able to reproduce this issue:  And the Torch version was 2.2. Can you tell us which machine you are running this on ? ," I can reproduce it on two distinct machines (the first one is the one for which I generated the versions log above):  and:  In the case this is a memory related issue, maybe you can try to reproduce it with an even larger matrix?", Have you managed to reproduce the issue?,"Hi ! We have the issue reproducing and we know where the issue is coming from. To summarize the current status:  The root issue is coming lower down the stack from Metal where allocating a buffer that exceeds a specific fraction of the system memory errors out due to a safety check in Metal drivers IIUC. On an 8GB machine with 32Kx32K matmul this gives an error of type `[_MTLCommandBuffer didCompleteWithStartTime:endTime:error:], line 1080: error 'Execution of the command buffer was aborted due to an error during execution. Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)`  Unfortunately this seems to be a silent error out on the driver side and the caller is just getting back a zero initialized buffer that ends up being the zero filled matrix we see in the output.  We are currently discussing this with the driver team to get a better understanding on what is the best long term approach here but in the meanwhile we will be addressing this issue by adding an assert on pytorch level where we detect if the requested buffer size ends up exceeding this watermark and let the user know. I'm hoping to get that PR going today.",Thanks  . This is the link: https://developer.apple.com/documentation/metal/mtldevice/2369280recommendedmaxworkingsetsize?language=objc,  Great job finding the issue. Thank you for the dedication!,Should be resolved by the fix for  CC(Data corrupted when transferring over 4GiB to MPS). Confirmed locally that the issue no longer repros with latest nightly.
rag,[CI] show doc coverage repro instructions,remind devs they can reproduce the doc coverage error locally with following msg  I spent 20min to figure out how to test locally so want to enrich the error msg  ,2024-04-10T00:24:39Z,Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/123688,"> I assume most people will already be in the pytorch folder so I dunno if it makes more sense to be cd docs or cd pytorch/docs, but seeing as you're the person who actually had to repro, I'll leave it up to you to decide thanks for review. changed to `cd docs` to be accurate", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Improve Foreach OpInfo test infra to enable meta coverage," Problem Currently, many meta tests are skipped for foreach ops. See https://github.com/pytorch/pytorch/blob/b18d8d4595aa6e0768eedd5fc7d4a4402c567181/torch/testing/_internal/common_methods_invocations.pyL9422L9429.  For inplace `add`, `mul`, `div`, `addcmul`, and `addcdiv` in particular, we only run the meta tests for complex dtypes because some of their sample inputs include complex Scalars that lead to errors when the Tensor is noncomplex. For example, you cannot inplace add:  Since the memory for `a` is not big enough to fit the complex result. Likewise the foreach version of `add` runs into the same errors, as it should! The problem occurs when we are attempting to compare the meta impl of `_foreach_add` to the actual implementation. The actual implementation will error (which is the correct behavior), and the meta test will also error as a result, see https://github.com/pytorch/pytorch/blob/main/test/test_meta.pyL542. To make tests pass, the precedent has been to say ""screw it, just skip all noncomplex dtypes"" for inplace ops but this misses out on a lot of coverage for the other sample inputs that didn't contain complex values. This was brought to my attention from our endeavor in CC(Add meta reg for addcdiv/addcmul ScalarList) with , in which we uphold the precedent but it makes me sad to.  Proposal: We can split up the foreach sample inputs into two categories: 1. those that succeed 2. those that error, which we will separate out as ErrorInputs. For the first category, we should run all our existing tests on these. For error inputs, we should just check that they error the same way as the original nonforeach versions. This way, w",2024-04-09T18:55:21Z,triaged module: testing module: mta oncall: pt2,open,1,2,https://github.com/pytorch/pytorch/issues/123663,"Hi , doing issue scrapping. I see you landed a PR, can we close this issue after that?","No, that PR doesn't solve the issue :c"
transformer,TransformerDecoderLayer fails when on cuda and batch dimension is zero (on cpu is fine)," üêõ Describe the bug The TransformerDecoderLayer craches when on cuda and batch dimension is zero. On CPU, it works as expected This issue has been discussed before ( CC(torch.nn modules should accept 0batch dim tensors.)) and a PR has been merged ( CC(Allow TransformerEncoder and TransformerDecoder to accept 0dim batch sized tensors.)), but I think the error persists on cuda.   Versions PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.12.2  (main, Feb 27 2024, 17:35:02) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.14.336257.562.amzn2.x86_64x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 535.129.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz CPU family:                         6 Model:                              85 Thread(s) per core:                 2 Cor",2024-04-09T14:43:52Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/123642,The problem seems to be related to the `torch._native_multi_head_attention`
transformer,torch.explain gives non-deterministic results on the same model with the same input," üêõ Describe the bug When running `torch._dynamo.explain` multiple times on the same model with the same input, different number of graphs and graph ops are returned. I would expect this to be the same in every run, but maybe I overlooked something in the docs. If this is expected behaviour, this nondeterminism is causing issues for the custom backend I am working on. A flag to keep it consistent would be great to have.  There's also other warnings in the error log, that I so far ignored, but maybe are the reason for the nondeterminism.  Error logs  I pasted the complete log: There are ignored exceptions, I haven't digged yet into where they are coming from.  The issue that I am focused on:   Minified repro   Versions  ",2024-04-09T10:12:39Z,triage review oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/123636,"After some more debugging to check if maybe the model isn't deterministic: When using `model.eval()`, `torch._dynamo.explain` always gives the same result. I misinterpreted `torch.inference_mode` to also do `eval`, but apparently it doesn't. I don't think there's a bug then, I'll close it. ","Hi, I'm has the same problem when I use `dynamo.explain` to get `GraphModule`. Do you have some idea to solve the AttributeError?"
rag,[FSDP1] fix _same_storage check for DTensor,"for FSDP (SHARD_GRAD_OP + use_orig_params) + TP, params in the backward are DTensors. However,  ``DTensor.untyped_storage().data_ptr()`` does not work in ``_same_storage``. Thus desugar to ``DTensor._local_tensor.untyped_storage().data_ptr()``  CC(FSDP + DTensor is not working with `SHARD_GRAD_OP` + use_orig_params) credit to  for the original fix. after landing, we would not need patching in mosaic composer https://github.com/mosaicml/composer/pull/3175/files ",2024-04-09T01:45:36Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/123617, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , do you think we can include in torch 2.3.1?   CC([v2.3.1] Release Tracker),"> for FSDP (SHARD_GRAD_OP + use_orig_params) + TP, params in the backward are DTensors. However, `DTensor.untyped_storage().data_ptr()` does not work in `_same_storage`. Thus desugar to `DTensor._local_tensor.untyped_storage().data_ptr()` CC(FSDP + DTensor is not working with `SHARD_GRAD_OP` + use_orig_params) >  > credit to  for the original fix. after landing, we would not need patching in mosaic composer https://github.com/mosaicml/composer/pull/3175/files >  > .3.1 won't include this fix. Will file a PR to see if we can make it"
transformer,Support FP16 accumulation for faster LLM inference on 4090 like GPUs," üöÄ The feature, motivation and pitch  Background Many existing Large Language Models (LLMs) utilize FP16 during inference to improve performance. Downstream inference libraries, such as vllm, rely on fundamental operators in PyTorch, like `F.Linear`. Presently, PyTorch only supports two levels of FP16 GEMM, controlled by `allow_fp16_reduced_precision_reduction`. Both levels use FP32 as the computation accumulation data type.  Opportunity Commodity NVIDIA RTX GPUs, such as the 3090 and 4090, show double throughput when doing FP16 GEMM with FP16 accumulation compared to FP32 accumulation. By benchmarking following simple perplexity test script, it can be demonstrated that using FP16 accumulation results in a 40% endtoend speedup on 4090, with a minimal perplexity increase (0.0006) in LLM serving scenarios.    Proposed Feature Introduce a new API named `torch.set_float16_matmul_precision` to offer three levels of precision: 1. ""highest"": FP16 matmul uses FP32 as all the intermediate accumulation mode, which may lead to performance regression. This aligns with setting `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction` to `False`. 2. ""high"" (default): FP16 matmul uses FP32 as the computation accumulation mode. This aligns with setting `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction` to `True`, which is also the current default behavior. 3. ""medium"": FP16 matmul uses FP16 as the computation accumulation mode. This feature enhancement aims to provide users with more control over the precision levels during FP16 matmul operations, catering to needs in inference scenarios  Alternatives _No response_  Additional",2024-04-08T13:29:38Z,module: performance triaged module: half,open,1,5,https://github.com/pytorch/pytorch/issues/123558,"As a note, I've seen a number of folks ask for this. It apparently makes a significant difference in perf for consumer cards.",From triage review: we should do it if we don't do it already.,This is related to  CC([RFC] Set Float32 Precision for CONV/RNN) ,"Thanks for the review! I've made the necessary changes to cudaBLAS.cpp locally to support this feature. I'm eager to contribute and submit a PR for PyTorch. Regarding CC([RFC] Set Float32 Precision for CONV/RNN), it seems the precisionrelated frontend APIs are still under discussion. Should I introduce the `torch.set_float16_matmul_precision` API in my PR now or wait for the conclusion in CC([RFC] Set Float32 Precision for CONV/RNN) before proceeding?"," was this ever added? if so I have been doing so much wrong for the past few months. If not, pretty please add it :) !"
yi,Instantiate VaryingShape<c10::Stride>,Fixes CC(libtorch: missing `c10::VaryingShape::merge`) As the ISSUE stated.,2024-04-08T04:20:41Z,open source Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/123542, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Support integer parameters in FullyShardedDataParallel," üöÄ The feature, motivation and pitch Libraries like Transformers, vllm and diffusers use large quantized LLMs for inference and finetuning. When running large models on several lowmemory GPUs, people often use FullyShardedDataParallel to split the model between those GPUs. Problem is, quantized models have integer parameters (e.g. uint8, int32, int64) and FSDP currently does not support integer data types. __Feature request:__ please support samedtype integer FlatParameters in FSDP. Currently, it is blocked by these two lines https://github.com/pytorch/pytorch/blob/b6201a60c57f79d081ecde66e1ce70e6614a3052/torch/distributed/fsdp/_flat_param.pyL767L768  **Edit:** I originally believed that you can remove these two lines and the code would *always* run perfectly, whereas in actuality, it causes errors in some configurations. We discuss possible workarounds below.  Alternatives bitsandbytes, a popular quantization library behind transformers & others, solves this problem as follows: https://github.com/TimDettmers/bitsandbytes/pull/970 . Their solution is to store integers as floating point tensors and reinterpret them as floats inside custom cuda kernels. However, this is still fairly difficult to use. My specific use case is for AQLM, where there is an integer parameter QuantizedWeight.codes. What I ended up doing is __simply removing these two lines, after which FSDP worked normally with integeronly modules.__ On the other hand, if you are planning to support automatically grouping FlatParameters by dtype and concatenating them together, this will also resolve this issue.  Additional context If this makes things easier, I'd be happy to cre",2024-04-07T18:11:24Z,oncall: distributed triaged,open,7,7,https://github.com/pytorch/pytorch/issues/123528, ,Removing this floatingpoint check sounds good.,"For my reference, do you have any simple/minimal examples of integer modules that we might be able to use for unit testing?","  We have an example using AQLM's QuantizedLinear but it is not *that* minimal. Currently, we use a dirty workaround by saving int32 data as float32 buffers. A simpler alternative would be to use `bitsandbytes`'s Linear4bit with the default int8 storage, but it may be too large for, say, automated tests; ( https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/nn/modules.pyL341 ) Finally, I can write a simple purepytorch example of a quantized linear layer with uint8 weights (saved as a separate module) that does not require any dependencies. Which one would you prefer?",The purepytorch example of a quantized linear layer would be much appreciated!,"   Example code  Here's an minimalistic example of a purepytorch code that imitates finetuning a quantized model with LoRA adapters (based on a popular LLM use case) Code: https://gist.github.com/justheuristic/575b053be82493e8151c775762796360 To run: `torchrun  nproc_per_node=2 demo_fsdp_lora.py` Currently, it fails   Click to see full traceback  !image  **However, it seems this check is there for a reason.** I tried simply removing the two lines that check for integer dtypes, and it only works in some cases. I got it to work on a multihost training with one GPU per host, but it fails on another server with 2x A100. I am yet to isolate out the reason why it fails. Would appreciate any tips / suggestions.  One possible solution One can easily circumvent this issue by swapping the storage type on integer parameters: Code: https://gist.github.com/justheuristic/8cc3c4df692ffa9354190c916e6ec643  ( **diff with previous gist** ) To run: `torchrun  nproc_per_node=2 fsdp_lora_solution.py` This seems to work in all setups i tried. Currently, the only way to get FSDP to work with integer parameters is to manually wrap them with IntParams (from the gist above) in the user code. Perhaps it would be possible to let users keep their integer params as is and wrap them internally, inside FullyShardedDataParallel? What do you think?","  Hi! Do you see any good solutions to integer parameters within FullyShardedDataParallel? It is, of course, possible to keep using the workaround above on the user side, but it has several problems: 1. whenever you cast the model to a different dtype (e.g. model.float()), it will irreversibly break any quantized params because pytorch thinks they are float64 2. one must wrap quantized weights as a separate module, typically rewriting the quantized model code for each library 3. direct storage manipulation may cause issues with JIT / compile and is incompatible with some backends (e.g. XLA)  If one were to, for instance, wrap integer params internally before allgathering inside FullyShardedDataParallel, it would eliminate these problems, but i am not sure if this has any other ramifications with other components. What would you suggest?"
gpt,[inductor][cpu]GPT2ForSequenceClassification AMP static/dynamic shape default/cpp wrapper single thread accuracy crash, üêõ Describe the bug   Versions SW info               name       target_branch       target_commit       refer_branch       refer_commit                       torchbench       main       d6015d42       main       d6015d42                 torch       main       781e8d2201c1e2aaeccbbc7b7b13f9322b481bc9       main       f0d461beacded34abe196c72ec4bcdb55bf01793                 torchvision       main       0.19.0a0+2c4665f       main       0.19.0a0+a0c79b3                 torchtext       main       0.16.0a0+b0ebddc       main       0.16.0a0+b0ebddc                 torchaudio       main       2.2.0a0+ea437b3       main       2.2.0a0+17a7081                 torchdata       main       0.7.1a0+0790338       main       0.7.1a0+0790338                 dynamo_benchmarks       main       nightly       main       nightly           Repro: inductor_single_run.sh bash inductor_single_run.sh single inference accuracy huggingface GPT2ForSequenceClassification amp first static/dynamic default/cpp huggingfaceGPT2ForSequenceClassificationinferenceampstaticcppsingleaccuracycrash_guilty_commit.log Suspected guilty commit: https://github.com/pytorch/pytorch/commit/5152945441b2a1387f6e9fc8fe1531bfeb6cabe4 ,2024-04-06T14:45:58Z,oncall: pt2 oncall: cpu inductor,closed,0,15,https://github.com/pytorch/pytorch/issues/123503, could you help to take a look of this issue?,"Sorry, had missed the notification. Weird that only the singlethreaded case has an accuracy issue. Will investigate, thanks!","Hi , any update for this issue?","Hi , have you got time to take a look of this issue?","Sorry, I was unable to find the rootcause of this one, since the accuracy issue only happens with singlethreaded case, and Flash Attention is a composite op, making it hard to debug. As we discussed offline, I'll add a `check_fn` to disable this SDPA pattern for the singlethread case on CPU. Thanks","Hi  fangintel, this issue seems to be happening with AMP with BF16 dtype because of MKL's `GEMM_BF16BF16F32` kernels when `AMX_BF16` ISA is used. The MKL version I used locally is 2024.1.0. It failed with 16 threads as well, but passed with 32, which is probably why the test scripts didn't catch it because it uses all cores of a socket in the multithreaded case: Command you could use locally with appropriate environment variables & numactl config:  When AMX is used (default case), this issue is reproducible. With environment variable `MKL_ENABLE_INSTRUCTIONS=AVX512_E3`, this issue doesn't occur. `AVX512_E3` here means AVX512_BF16 ISA but not AMX_BF16. Looks like correctness should be verified for these MKL kernels for those input shapes with `AMX_BF16` ISA with 16 threads (although the output is okay with 32 threads), so it's likely not a Flash Attention implementation issue in ATen. But will check one remote possibility first  baseline output (eager mode) outputs should be within tolerance bounds of each other when 16 and 32 threads are used. This will rule out any issues with the eagermode implementation, which didn't use Flash Attention, but simply `matmul > div > attn_mask > softmax > matmul`, for which oneDNN kernels leveraging AMX_BF16 ISA were used (for matmuls corresponding to MHA). Will contact our MKL team for a fix if those MKL kernels indeed have highly divergent outputs for 16 & 32 threads, and meanwhile, will try to have this SDPA fusion pattern disabled for BF16 dtype, since that's currently easier than disabling the pattern on machines with AMX. Thanks!","After also varying the maximum ISA oneDNN could use, it's now unclear as to whether MKL `GEMM_BF16BF16F32`s are actually responsible.  Example command:  **Next action item:** Since the commit responsible for this issue enabled offloading of SDPA compute to the Flash Attention kernel, a BF16 AMP UT would make things clear because currently, no CI machines with AMX ISA test for AMP variants of SDPA patterns.","> After also varying the maximum ISA oneDNN could use, it's now unclear as to whether MKL `GEMM_BF16BF16F32`s are actually responsible. >  > MKL could also use AMX_BF16 besides AVX512_BF16	MKL only used AVX512_BF16	oneDNN could also use AMX_BF16 besides AVX512_BF16	oneDNN only used AVX512_BF16	Does accuracy failure occur (with 1 to 16 threads)? > Y	N	Y	N	Y > Y	N	N	Y	N > N	Y	N	Y	N > N	Y	Y	N	N > Example command: >  >  >  > **Next action item:** Since the commit responsible for this issue enabled offloading of SDPA compute to the Flash Attention kernel, a BF16 AMP UT would make things clear because currently, no CI machines with AMX ISA test for AMP variants of SDPA patterns. Thanks for the investigation. Are these experiments all with SDPA fusion pattern 18 enabled? Comparing the the first and second experiments, it looks like something from oneDNN side causes the discrepancy which seems not aligned with the guilty commit found in this issue summary.","> Comparing the the first and second experiments, it looks like something from oneDNN side causes the discrepancy which seems not aligned with the guilty commit found in this issue summary Hi fangintel, the 3rd & 4th experiments rule out the possibility of oneDNN being an issue. It seems this issue wasn't caught with BF16 because the CI machines don't support AMX. If I add BF16 support to SDPA pattern UTs (which only test for FP32 at this point), and test on a machine that supports AVX512_BF16 or AMX, the UTs for patterns 18 & 19 fail for BF16 dtype because of exceeding tolerance bounds. e.g.   ","This issue still persists for BF16 dtype with Inductor SDPA pattern 18 that I added, and pattern 19 that  added.","Running with Maxautotune, the model passes accuracy check  One difference found w and wo maxautotune is the post op of `cpp_fused_add_mul_pow_tanh_27`. With maxautotune, these post ops fused in gemm template and directly calculated with fp32 without cvt to bf16 (mkldnn linear out) and cvt to fp32 (legalization of cpp codegen). .",AOTI also has the same issue caused by the same guilty commit.,> AOTI also has the same issue caused by the same guilty commit. Also with a single thread?,> > AOTI also has the same issue caused by the same guilty commit. >  > Also with a single thread? Both single and multiple thread have accuracy failures for GPT2ForSequenceClassification in AOTI.  could you help confirm if both single and multiple thread failures in AOTI are caused by the guilty commit in this issue?," Please help check if the issue is fixed, thanks!"
yi,"Dynamo: support proxying tensor subclass constructors, including with non-fx types","For a while, we haven't really had good support for tensor subclass constructors going **inside** of the compiled region. Two main example of that we've sort of halfhanded in the last few months are: (1) DTensor.to_local(), which is a wrapper around the constructor, that we explicitly allow_in_graph () (2) Float8Tensor.to_float8(), which is similar, and also allowed_in_graph explicitly ( / ) This PR attempts to allow **all** traceable wrapper subclass constructors to go directly in the graph, when dynamo has a chance to intercept `SubclassCtr(*args)`. To handle the DTensor case (where `Placement` and `DeviceMesh` cannot become nodes in the graph, but they are constructor inputs), I tried to add some infra on `VariableTracker` that tells us if a VT ""supports going in the FX graph"". For any inputs to the constructor that are not FX types, I close over them in the node's implementation, and convert them to their python constants (so AOT can implement them later). , a review on the dynamo bits would be much appreciated.   CC(DTensor: avoiding crashing on dynamic shapes in a few places)  CC(Dynamo: support proxying tensor subclass constructors, including with nonfx types)  CC(dynamo: use equality guards instead of id guards for Placement/DeviceMesh)  CC(AOTAutograd: force tangents to be contiguous when subclass inner tensor is noncontiguous)  CC(fix FakeTensor creation on noncontiguous subclasses)  CC(support as_python_constant on PlacementClassVariable) ",2024-04-04T15:14:29Z,oncall: distributed Stale release notes: distributed (fsdp) module: inductor module: dynamo ciflow/inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/123350,"What is the difference between ""can proxy in graph"" vs ""has an implementation of python_constant""?","A few things about this PR that make me unsure if it's worth landing. (1) I don't strictly need this PR to unblock https://github.com/pytorch/torchtrain/issues/61 /  CC([DTensor][FSDP] dynamo internal error for fully_shard(reshard_after_forward=2)) (_dynamo.disable on __new__ is enough) (2)  convinced me that one alternative to this PR would be to have dynamo specialcase subclass `__new__` and `__init__`, so we can fully + correctly inline them, all the way to their call to `_make_wrapper_subclass` call. I think there are still some details of (2) that I haven't fully internalized / aren't clear to me (we still need the torch graph to contain IR that generates the subclass so AOTAutograd can trace it), but if we can get it to work then one nice benefit is that we wouldn't have to close over any custom python classes, which would be nice to not have to worry about for  's AOTAutograd warmcache work. (counterargument  even if we go with the approach in this PR, we can probably still figure out a way to hash the torch graph for the warmstart AOTAutograd work, by e.g. putting ""nonFXallowable"" constructor arguments in a sidetable somewhere that we need to hash).","> What is the difference between ""can proxy in graph"" vs ""has an implementation of python_constant""? I was trying to represent the first to literally mean ""things that are allowed to show up as arguments to an FX node"". So I guess `torch.add(x, 1)` would showcase an example where they are different: even though `1` is technically a python constant, it is also a valid input to an FX node. And we in theory could give it the same treatment as stuff like `DeviceMesh` (close over it, and proxy a function like `dynamo_torch_add(x)` that closes over the `1`). But we probably want to minimize the number of ways that we can end up with one of these ""secretly close over arguments"" proxied functions, since they make serialization a pain.","Yes, I think I'd like to spend some time banging on design for this. Maybe we have to do it this way, but I want to explore the space some more.",We talked about this at composability sync! https://www.youtube.com/watch?v=9JoHZLSLb14,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Fakeifying views shouldnt create symbols when dynamic=False,"Fixes  CC(compile creates FakeTensors with dynamic shapes even when dynamic=False when inputs are views) I was also seeing some crashes in torchtrain due to dynamic shapes, even when I set `compile(dynamic=False)` (). This doesn't fix the underlying dynamic shape issues with compile + DTensor, but it does prevent dynamic shapes from leaking in. (make sure dynamo doesn't inline DTensor __new__ or __torch_dispatch__)  CC(Fakeifying views shouldnt create symbols when dynamic=False)  CC(fix correctness for dynamo inlining RangeVariable __contains__)  CC(compile: ban mutations on noncompositional uses of as_strided)",2024-04-04T15:14:17Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,1,3,https://github.com/pytorch/pytorch/issues/123348,"Left a comment on the issue:  CC(compile creates FakeTensors with dynamic shapes even when dynamic=False when inputs are views)issuecomment2037864731 I'm curious if it's possible to address this by just fixing up the logic for simplifying out symbols. Edit: That said, perhaps we do just want to avoid creating the symbols in the first place?", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
chat,make sure dynamo doesn't inline DTensor __new__ or __torch_dispatch__,"Fixes  CC([DTensor][FSDP] dynamo internal error for fully_shard(reshard_after_forward=2)), https://github.com/pytorch/torchtrain/issues/61 Even with the previous PR (""support DTensor/subclass constructors directly in the graph""), I still see some errors when running the repro above that start some logs showing that dynamo is inlining `__new__`. I noticed that putting `._dynamo.disable` on DTensor's `__new__` makes the entire repro pass. Why does having dynamo try to inline `Subclass.__new__` run into problems? Morally, dynamo probably shouldn't be inlining __new__ (""creating a subclass"" is a blackbox operation that AOTAutograd can trace through anyway). But concretely, we can end up with a node in the dynamo FX graph that has a ""partially initialized tensor subclass"" as its example value, because the subclass has been created but its fields have not been assigned to yet. This breaks a bunch of invariants throughout dynamo: there are many places where if we have a tensor subclass node, we want to look at its inner tensors, to see if they are FakeTensors, what their FakeTensorMode is, and if they have dynamic shapes. One option is to decide that ""uninitialized subclass"" is a firstclass thing that anyone looking at the FX node examples values on the dynamo graph needs to handle, but this seems like a lot of work when in reality we don't need dynamo to trace the __new__ at all. Hence the `torch._dynamo.disable`. I still wasn't very satisfied, since it was unclear to me **why** dynamo was inlining the `__new__` call, instead of interposing on the `DTensor()` constructor directly. After a long chat with , he explained that with code like this:",2024-04-04T15:14:11Z,oncall: distributed Merged Reverted Stale ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,1,6,https://github.com/pytorch/pytorch/issues/123347, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m=""Diff reverted internally"" c=""ghfirst"" This Pull Request has been reverted by a revert inside Meta. To reland this change, please open another pull request, assign the same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).)", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,running Kornia.morphology.opening and trying to export onnx, üêõ Describe the bug  Trouble exporting  onnx as the code throws following error :  How can i fix this? Thanks for your help :)  Versions ,2024-04-03T20:52:23Z,module: onnx triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/123287,torch.onn.export does not support unfold (aka im2col) and there is no plan to support it try using `torch.onnx.dynamo_export` isntead and let us know if that doesnt work for you
yi,libtorch: missing `c10::VaryingShape<c10::Stride>::merge`, üêõ Describe the bug libtorch_cpu doesn't export `c10::VaryingShape::merge`:  which can cause a linking error:   Versions libtorch version: 2.2.2 ,2024-04-03T09:27:00Z,module: cpp triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/123248
yi,[CI] vec_test_all tests are not run in CI," üêõ Describe the bug I've tried to compile and run `./bin/vec_test_all_types_[DEFAULTAXV512]` and noticed that it fails right now I.e. on x86 cpu:  Which is due to use of reciprocal instruction rather than a proper division for complex, see  https://github.com/pytorch/pytorch/blob/12e36dc1dfada63d81086cc1074f2294f6d04ed6/aten/src/ATen/cpu/vec/vec256/vec256_complex_float.hL377 And on ARM VecConverTests are failing as conversion between negative floats and unsigned integers are undefined   Versions CI /pytorchdevinfra wei",2024-04-02T22:34:19Z,module: cpu module: ci triaged module: regression module: intel,open,0,1,https://github.com/pytorch/pytorch/issues/123219,IMO this issue should be split into 2 or 3 separate ones:   Add test to CI (and this is what this one is about)   Fix tests or complex divisions on AVX2/AVX512   Fix conversion test to avoid undefined behavior (https://github.com/pytorch/pytorch/pull/123258 )   Unify negative float to unsigned integer conversion across scalar and vectorized converter 
yi,Reduce CPU overhead of copying inputs in CUDAGraph trees via foreach_copy,"  CC(Reduce CPU overhead of copying inputs in CUDAGraph trees via foreach_copy) I noticed that when enabling CUDA graphs in Inductor, most of the CPU time was spent issuing copies from the new inputs to the graph's input tensors. This meant that my workload was still somewhat CPU bound.  I tried to improve this situation by using the new `_foreach_copy_` operator, in order to group all the copies into one operator. There was already a comment in the code indicating that this was a desired optimization. It did indeed improve the situation substantially:  On device, the situation also improved, with the memcpys being merged into fewer larger kernels: Before:  After:  ",2024-04-02T13:09:51Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/123162,"Hey, thanks for the PR!  I had a PR earlier to use foreach_copy_ and I didn't land it because I thought it made things slower. Now that I'm looking at the PR again, I think maybe I was looking at the wrong baseline.  In any case I'm going to run a dashboard to confirm this doesn't regress perf but otherwise looks good.", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/lw/1/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/123162`)",Thanks for taking a look! > I didn't land it because I thought it made things slower I could imagine that `foreach_copy` could add some small overhead for graphs with very few inputs (which are however quite fast already) but hopefully this is outweighed by the speedup on graphs with many inputs?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,Support fp8 quantization,"This commit enables float8_e5m2 and float8_e4m3fn dtypes in fx quantization and PT2E. Motivation for using fp8 quantization instead of int8:  it works better to run inference with the same datatype the model was trained with,  fp8 can handle outliers better, which is one of the problems in LLMs activations. The numerical recipe we want to use it for is fp8 inference:  bgemms/gemms running in float8_e4m3fn,  PerTensorQuantization/Scaling,  amax observer for measurement with input_backoff and weight_backoff.",2024-04-02T12:59:20Z,triaged open source Merged ciflow/trunk release notes: quantization release notes: AO frontend,closed,0,7,https://github.com/pytorch/pytorch/issues/123161,  could you please review this PR or point me to the proper person?," , thank you for review and approval. I have fixed merge conflicts and remove unnecessary comment, but I don't see any CI run. Do I need to trigger something manually, or someone from Meta has to do it?", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,"llama-fast produces gibberish with --device mps, works with --device cpu"," üêõ Describe the bug MPS produces garbage with stories15M using llamafast: Repro: Produces gibberish when using device mps, but works correctly with device cpu:  Correct operation: git clone https://github.com/pytorchlabs/llamafast.git cd llamafast python generate.py  checkpoint_path checkpoints/$MODEL_REPO/stories15M.pt prompt ""Hello, my name is"" device cpu   Versions Collecting environment information... PyTorch version: 2.4.0a0+gitccfc87b Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.4.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: version 3.26.4 Libc version: N/A Python version: 3.11.8 (main, Feb 26 2024, 15:36:12) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.4.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Max Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] optree==0.10.0 [pip3] torch==2.4.0a0+gitccfc87b [conda] numpy                     1.26.4                   pypi_0    pypi [conda] optree                    0.10.0                   pypi_0    pypi [conda] torch                     2.4.0a0+gitccfc87b           dev_0     ",2024-04-02T04:56:35Z,triaged module: correctness (silent) module: mps,closed,0,5,https://github.com/pytorch/pytorch/issues/123151,Any insight into which ops are responsible for the wildly differing results?,> Any insight into which ops are responsible for the wildly differing results? IDK.  What's worse is that tonight it works...  Do we just close and reopen again when this happens?, any change that was done that would explain this being fixed?, I think this might also have been caused by the issue in creating scalars since it specifically shows up in MacOs14.4+. The regression was addressed here: https://github.com/pytorch/pytorch/pull/123234,Awesome! Closing this as done then. Feel free to reopen if we missed anything  
yi,Remove mypy-ignore-errors from custom_op_db,  CC([WIP] Add mangle/demangle helper functions)  CC(Add mutated_args field to custom_op)  CC(Add register_autograd to register backward formulas for custom ops)  CC(Add is_tensorlist_like_type helper)  CC(Expand is_functional_schema to work with torch._C._FunctionSchema)  CC(Remove mypyignoreerrors from custom_op_db)  CC(Add torch.library.custom_op),2024-04-01T18:16:27Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/123107, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,added one mapping used in vllm,Fixes ISSUE_NUMBER Added one mapping used in vllm in this PR https://github.com/vllmproject/vllm/pull/3140.,2024-03-29T23:20:47Z,open source ciflow/trunk rocm ciflow/rocm,closed,0,4,https://github.com/pytorch/pytorch/issues/123001, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `hip_mapping_vllm` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout hip_mapping_vllm && git pull rebase`)",close this as this seems not needed anymore
rag,[ATen-Vulkan][cleanup][ez] Remove obsolete vTensor members for buffer storage support,"Summary:  Context Support for using buffer storage for tensors in compute shaders will be reworked. For now, remove some of the obsolete mechanisms that were introduced to support buffer storage. Test Plan: CI Differential Revision: D55544884",2024-03-29T23:17:34Z,fb-exported module: vulkan ciflow/trunk release notes: vulkan ciflow/periodic,closed,0,2,https://github.com/pytorch/pytorch/issues/123000,This pull request was **exported** from Phabricator. Differential Revision: D55544884,This pull request was **exported** from Phabricator. Differential Revision: D55544884
transformer,[dtensor] run transformer sdpa in dtensor,"  CC([dtensor] run transformer sdpa in dtensor)  CC([dtensor] add op support for memory efficient attention)  CC([dtensor] improve new factory strategy) Now that efficient attention is supported in dtensor, we can modify the transformer test to use dtensor in SDPA and get rid of the manual num_head adjustments. Caveat: Efficient attention is supported only with bf16/fp32 (not fp64) and has other constraints. If any of the constraints are not satisfied, the SDPA would fall back to the math decomposed attention, which will break as it does not fully work with dtensor (it creates a `torch.Tensor` mask in the middle). I considered adding some checks like in P1202254918 but that needs to be added everywhere this Transformer is used. Is it necessary if the current CI machines can run efficient attention? Test files containing this Transformer:  `test/distributed/tensor/parallel/test_tp_examples.py`  `test/distributed/_composable/fsdp/test_fully_shard_training.py`  `test/distributed/_composable/fsdp/test_fully_shard_clip_grad_norm_.py` ",2024-03-29T22:14:20Z,oncall: distributed Merged ciflow/trunk release notes: distributed (dtensor),closed,0,2,https://github.com/pytorch/pytorch/issues/122997, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[dtensor] add op support for memory efficient attention,"  CC([dtensor] run transformer sdpa in dtensor)  CC([dtensor] add op support for memory efficient attention)  CC([dtensor] improve new factory strategy) This is a followup to flash attention. On cuda, flash attention is supported only for fp16/bf16, whereas memory efficient attention is supported for fp32 (but not fp64). With this PR, one can run SDPA and in general Transformer completely in dtensor. ",2024-03-29T22:14:14Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,2,https://github.com/pytorch/pytorch/issues/122996, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[dtensor] improve new factory strategy,"  CC([dtensor] run transformer sdpa in dtensor)  CC([dtensor] add op support for memory efficient attention)  CC([dtensor] improve new factory strategy) Previously, the new tensor out of the ""new factory"" all become replicated. With this PR, if the new tensor has the same shape as the old tensor **and** the shape can be evenly sharded, then the old spec is inherited and preferred. To accommodate this when the old tensor has sharded placements, the input args for local computation (size, stride) need to be adjusted. ",2024-03-29T22:14:09Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,2,https://github.com/pytorch/pytorch/issues/122995, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,Track the accurate check regress for DebertaForQuestionAnswering and nanogpt," üêõ Describe the bug I could repro locally.  The error message:   indicate that increasing tolerance to 0.02 can make the test pass. But on the other hand, dashboard run uses a different cuda version (12.1. Local server uses 12.0) and cause different numerical results. According to the error message on the dashboard run  we need tolerance 0.03 to pass the test. I've also done ablation test. I'm on commit 57a9a64e10b84fa8d932482ae9417aa3fc3fbf44 (clean trunk) and then revert CC(Added some checkpointing tests), CC(Forward fix for subtly breaking AC with compile in the case of stacked), CC(Made several changes to mincut partitioner that allow it to recompute     more things) in that order, than the test pass again locally. nanogpt fail for the same reasons. Update: DebertaForQuestionAnswering starts passing the accurate test from June14 2024.  Error logs .  Minified repro _No response_  Versions .  ",2024-03-29T21:31:18Z,triaged oncall: pt2 module: pt2 accuracy pt2-pass-rate-regression,open,0,1,https://github.com/pytorch/pytorch/issues/122987,> Update: DebertaForQuestionAnswering starts passing the accurate test from June14 2024. It seems to have started failing again last week
rag,Fix performance regression and memory storage handling of Flash Attention on ROCM (#122857),"This PR fixes the two major issues that was discovered after the initial merge of PR CC(Add Flash Attention support on ROCM) 1. The Flash Attention support added by has severe performance regressions on regular shapes (power of two head dimensions and sequence lengths) compared with PR CC(Readd initial Flash Attention support on ROCM). Its performance is worse than the math backend and only has numerical stability advantages. This PR fixes this problem. 2. There is a flaw of memory storage handling in PR CC(Add Flash Attention support on ROCM) which does not copy the gradients back to the designated output tensor. This PR removes the deprecated `TensorStorageSanitizer` class which is unnecessary due to the more flexible backward kernel shipped by PR CC(Add Flash Attention support on ROCM) Pull Request resolved: https://github.com/pytorch/pytorch/pull/122857 Approved by: https://github.com/jeffdaily, https://github.com/drisspg ",2024-03-29T16:58:35Z,module: rocm open source ciflow/rocm,closed,0,2,https://github.com/pytorch/pytorch/issues/122967,Do we have an issue for this and maybe a reproducible script that I could run on CUDA to confirm that there is no regression there (I assume that ROCm has been tested as I don't have have a ROCm runner to test this out)?,"> Do we have an issue for this and maybe a reproducible script that I could run on CUDA to confirm that there is no regression there (I assume that ROCm has been tested as I don't have have a ROCm runner to test this out)? Technically this change has zero impacts on CUDA side since it's only modifying code that will be compiled on ROCM. To test possible regressions, although we are using other profiling tools for performance numbers, the  the `benchmarks/transformer/sdpa.py` script under pytorch repo is sufficient to show the performance boosts on ROCM and ensure there is no regression on CUDA. I have posted performance numbers on the original PR as comments: https://github.com/pytorch/pytorch/pull/122857issuecomment2033141316"
llama,Unable to generate Llama2 pte by following the instructions," üêõ Describe the bug Following the instructions here: https://github.com/pytorch/executorch/tree/main/examples/models/llama2 I ran this command after downloading Llama2 weights: `python3 m examples.models.llama2.export_llama checkpoint /path/to/Llama27b/consolidated.00.pth params /path/to/Llama27b/params.json` I get this error: `RuntimeError: Trying to create tensor with negative dimension 1: [1, 4096]` Stacktrace:   Versions CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          AuthenticAMD Model name:                         AMD Ryzen Threadripper PRO 5955WX 16Cores CPU family:                         25 Model:                              8 Thread(s) per core:                 2 Core(s) per socket:                 16 Socket(s):                          1 Stepping:                           2 Frequency boost:                    enabled CPU max MHz:                        7031.2500 CPU min MHz:                        1800.0000 BogoMIPS:                           8000.05 Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wd",2024-03-29T09:11:16Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/122958,This looks more like an executorch issueasking in https://github.com/pytorch/executorch may be more helpful!,"Closing tentatively due to above comment, but feel free to reopen if this is indeed a pytorch/pytorch issue."
llm,[EZ] Run fp16 torch.mm/torch.mv across CPU threads,"  CC(Use QNEON register to compute the dot product)  CC([EZ] Run fp16 torch.mm/torch.mv across CPU threads) This significantly speeds up real world applications, such as LLMs Before this change llama27b fp16 inference run at 1.5 tokens per sec, after it runs at almost 6 tokens per sec ",2024-03-29T06:38:34Z,module: cpu Merged ciflow/trunk release notes: performance_as_product topic: improvements,closed,1,2,https://github.com/pytorch/pytorch/issues/122951, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Transformer Engine Checkpointing Broken on Torch 2.3," üêõ Describe the bug We've been seeing a strange checkpointing bug when using torch 2.3 and Transformer Engine.  Concretely, in `get_model_state_dict()` for torch 2.3 it seems that we aren't ignoring the `_extra_state` properly when saving checkpoints. If I'm reading the distributed FSDP checkpoint code correctly, it seems to be trying to map the keys in the state_dict into module attributes. This will work for everything that we save with `module.register_parameter(...)` or `module.register_buffer(...)`, but it does not account for the fact that` _extra_state` is not intended to be a registered attribute per PyTorch's own model saving workflow.  Composer + torch 2.3 checkpoint code path ‚ùå  https://github.com/mosaicml/composer/blob/99b86dd7b2ae4ec35c4d84c5d9431577d113180f/composer/core/state.pyL871L889  Composer + torch 2.1 checkpoint code path ‚úÖ  https://github.com/mosaicml/composer/blob/99b86dd7b2ae4ec35c4d84c5d9431577d113180f/composer/core/state.pyL891L896 Error log attached in message.  Error logs checkpoint_error_log.txt  Minified repro  Create simple FSDP Linear model but use transformer engine for linear layers  try to save FSDP checkpoint with `get_model_state_dict()` in torch 2.3.   Versions Torch 2.3  ",2024-03-29T05:52:36Z,triaged module: distributed_checkpoint,closed,0,19,https://github.com/pytorch/pytorch/issues/122946,cc:   ,"Hey, . Based on my understanding, `_extra_state` is included in `state_dict` based on the documentation. https://pytorch.org/docs/stable/generated/torch.nn.Module.html  So it would be included as part of state_dict and thereby gets saved during checkpoint. Just want to confirm if this aligns with your expectation. ","Yes that is in line with our expectation. Unfortunately, in practice, we see the error above where we are not able to get the `_extra_state` attribute successfully from the torch Linear layer. ",cc:  from Transformer Engine,"Hi   `_extra_state` is indeed part of the `state_dict` for our custom Transformer Engine modules, but it is not an attribute of the modules themselves. We're following the `torch.nn.Module` specs in the documentation and returning `_extra_state` via `get_extra_state()`. The FSDP checkpoint appears to incorrectly assume that `_extra_state` is a module attribute. If this is now expected in any `torch.nn.Module` implementation, we can update ours to match but the documentation for `torch.nn.Module` should also be updated to highlight this change. Otherwise though, FSDP checkpoint should exempt `_extra_state` from the attribute check. ",cc.  ," I'm a little confused. This is the statement from the document: `Implement this and a corresponding set_extra_state() for your module if you need to store extra state. This function is called when building the module‚Äôs state_dict().`  So the `extra_state` will be included in the `state_dict() call and `get_model_state_dict()` will also call `state_dict()`. In such a case, isn't it legit `get_model_state_dict()` to return the extra state?","  TE modules follow the `torch.nn.Module` documentation and correctly implement `get_extra_state()` to return a pickleable `_extra_state` (as well as the matching `set_extra_state()` to load the same object back into the module). The issue is that PyTorch's distributed checkpointing is trying to directly access `_extra_state` as a module attribute (i.e. `getattr(module, '_extra_state')`) instead of going through the `get_extra_state()` call.  To be more specific, `_get_model_state_dict(model, ...)` calls `_get_fqn(model, key)` on every key in the model's state dictionary, and `_get_fqn(model, key)` eventually fails at `getattr(cur_obj, cur_obj_name)` when `cur_obj_name == '_extra_state'`. Ultimately, `_get_model_state_dict(...)` is making an assumption here that every key in the state dictionary corresponds to a model attribute (like registered parameters and buffers). This is not true for `_extra_state` and the specs for `torch.nn.Module` do not specify any such requirement. If PyTorch now expects `_extra_state` to be a module attribute, this change needs to be documented in the `torch.nn.Module` specs. We can then modify the TE modules to comply with this. Otherwise, the `_get_fqn()` needs a new conditional check on `cur_obj_name == '_extra_state'` in order to use `cur_obj = cur_obj.get_extra_state()` to recover the extra states instead of `cur_obj = getattr(cur_obj, cur_obj_name)`.",Let me understand the question. So the issue you pointed out is not about the `_extra_state` should be or not be in the returned `state_dict`. The issue is about the implementation of `get_model_state_dict` should not directly access the the `_extra_state` as these states are not supposed to be the module attributes. Is my understanding correct?," Yes, that's right. We need clarification from PyTorch on whether this is intended behavior or a bug, so we can upstream a fix on our end if necessary.",I'll have a PR to fix this issue., curious if we can fix will land in for torch 2.3.1? Composer team is planning on doing a release around this for our FP8 support in Q2. ,The timeline looks possible. Can you try https://github.com/pytorch/pytorch/pull/125336 to see if that PR resolves your issue?," I looked at the above PR, but that does not seem to resolve the incompatibility between mosaic composer library's required torch and cuda versions versus llama v3's preferred ones: If I run 2.1.x+cu118 then I can get llamav3 to work nicely with transformers, but composer breaks, and if I run 2.3.0+cu121 then I can get composer to run but llamav3 won't play nice anymore.  Given that composer is working on the newest stable release of torch it seems like something that needs to be resolved from the llama side. Thoughts?","No, this is nothing to do with llama. If the model uses the extra_state in a way that torch allows, it is PTD's API should fix the issue.  can you verify if the PR fix the issue? We could only cherrypick the PR if it actually fix the issue. I can only verify through the unittest.","Hey , is this fix in PyTorch nightly?  For what it's worth, we tried to incorporate your fix by rebuilding the pytorch nightly image: `mosaicml/pytorch:2.4.0_cu121nightly20240512python3.11ubuntu20.04` but our transformer script still fails with the same error:   YAML:  ","Apologies, looking at the stack trace it seems that we monkey patch the `_get_fqn` function in `mosaic_fsdp_utils.py`.  Testing the fix there. ", we have validated that this PR fixes the TE ckpting extra state issue once monkeypatched into our composer `mosaic_fsdp_utils` . Composer monkeypatch PR here. Appreciate your work on helping us fix this issue!  has started the cherrypick for this commit into torch2.3.1 at  CC([v2.3.1] Release Tracker)issuecomment2118261677. ,Close the issue because it is fixed by https://github.com/pytorch/pytorch/pull/125336
yi,[dynamo] Bug fix for GET_YIELD_FROM_ITER,  CC([dynamo][dict] Add UnspecializedNNModuleVariable to dict keys)  CC([dynamo][3.12] Stop backend detection on the first RETURN_VALUE)  CC([dynamo][logs] Print bytecode before tracing)  CC([dynamo] Bug fix for GET_YIELD_FROM_ITER) ,2024-03-29T04:50:31Z,Merged topic: not user facing module: dynamo ciflow/inductor keep-going,closed,0,2,https://github.com/pytorch/pytorch/issues/122943,xref: CC([pytree] add `tree_iter` function and fix opcode `YIELD_FROM` and `SEND`)  CC([pytree] add `tree_iter` function and fix opcode `YIELD_FROM` and `SEND`), Just a gentle ping on this one if this slipped through the notifications.
transformer,Cannot export DETR model with torch.export," üêõ Describe the bug Unable to export the DETR model encoder with torch.export  Gives the error:  Please find in the following gist the corresponding `dynamo.explain` outputs for the same model (DETR encoder): https://gist.github.com/corehalt/dbb031cd7403bb9fb9faa4bda025adf7 For reference, other parts of the model, as the backbone for example, succeeded:   Versions Collecting environment information...                                                                                                                                                          PyTorch version: 2.2.1+cpu                                                                                                                                                                     Is debug build: False                                                                                                                                                                          CUDA used to build PyTorch: None                                                                                                                                                               ROCM used to build PyTorch: N/A                                                                                                                                                                OS: Ubuntu 22.04.3 LTS (x86_64)                                                                                                                                                                GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0                                                                                                       ",2024-03-29T02:39:03Z,export-triaged oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/122934,"Given that the model comes from Hugging Face, I wanted to highlight that there is an ongoing effort for allowing Optimum library to export the models using Dynamo: https://github.com/huggingface/optimum/pull/1712 Looking forward for `torch.export` being used everywhere. Great feature, thank you.", could you take a look?
transformer,ONNX export fails for aten::full_like op when exporting UDOP model from transformers," üêõ Describe the bug When attempting to export the UDOP model to ONNX from the transformers library, the torch.onnx.export() command fails with a RuntimeError. Below is a minimal example to reproduce this error:  This example produces the following error message:   Versions Collecting environment information... PyTorch version: 2.2.2+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.28.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.01072awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A10G Nvidia driver version: 520.61.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          16 Online CPU(s) list:             015 Thread(",2024-03-28T15:34:55Z,module: onnx triaged onnx-triaged module: dynamic shapes,open,0,9,https://github.com/pytorch/pytorch/issues/122898,"Hi marsh , can you also try with our new api, `torch.onnx.dynamo_export` and reaching out to see if that helps your export process?","Hi , thank you for looking into this. I updated the example above to use  instead of . Here is the updated code:   When running this, I encounter a number of errors. Below is the full traceback:  I have attached the report_dynamo_export.sarif file here for review (the .sarif file extension is not supported so I uploaded it in a ZIP file).  report_dynamo_export.zip I have not used  in the past so please let me know if any of these errors are down to my usage. ","Hi marsh , Thanks for the quick response. The issue is now triaged and you'll get support soon. If there are any deadlines or associated high priority, please let me know and I will get it dealt with accordingly :)  ","I ran into the exact same problem as well. Thanks to marsh for the clean reproducible example, saves me the same work. So, looking forward to the solution :)",The following script likely captures the underlying issue:  The quickest solution is to rewrite that part of model code inside huggingface/transformers like in this snippet. .,"Thanks ! I attempted this change but it did not seem to resolve the issue. I did however manage to track the issue discussed here to line 336 of transformers/models/udop/modeling_udop.py:  This issue can be corrected by replacing this line with:  However, after making that change, there is a new issue with the following traceback:  I have not been able to track down the cause of this new error so any help here would be much appreciated.  Since the issue seems to be in the UDOP model implementation within transformers, I have additionally created a topic on the HuggingFace discussion platform which can be found at: https://discuss.huggingface.co/t/exportingudoptoonnxfails/80741",marsh oh I should clarify my reply above is towards the error during `torch.onnx.dynamo_export`. ,", sorry that is my mistake. I modified the offending line as you suggested in the model code and attempted an export using  and the error did change. The new error is as follows:   It appears that this again is an issue in the model code, so I once again modified the model code to remove the python if statement and the error changed once again. Here is the new error:  It is not clear to me what is causing this issue so I am not sure how to proceed from here. Thanks!","Thanks, I found there are more issues uncovered from exporting this model. Namely the vision patcher part of code is creating some trouble during dynamo tracing. We will try to track those individual issues separately."
yi,Not consistent results when applying linear layer over sequence vs when applying the same layer only one time-step of the sequence ," üêõ Describe the bug The results different when applying a linear layer over a sequence compared to when applying the same layer over only one timestep of the same sequence.  You can check the simple code that reproduces the results below.    Versions > PyTorch version: 2.1.1 > Is debug build: False > CUDA used to build PyTorch: None > ROCM used to build PyTorch: N/A >  > OS: macOS 14.4.1 (arm64) > GCC version: Could not collect > Clang version: 15.0.0 (clang1500.3.9.4) > CMake version: version 3.26.4 > Libc version: N/A >  > Python version: 3.9.13 (main, Jan  5 2023, 09:17:42)  [Clang 13.1.6 (clang1316.0.21.2.5)] (64bit runtime) > Python platform: macOS14.4.1arm64arm64bit > Is CUDA available: False > CUDA runtime version: No CUDA > CUDA_MODULE_LOADING set to: N/A > GPU models and configuration: No CUDA > Nvidia driver version: No CUDA > cuDNN version: No CUDA > HIP runtime version: N/A > MIOpen runtime version: N/A > Is XNNPACK available: True >  > CPU: > Apple M1 Max >  > Versions of relevant libraries: > [pip3] mypy==1.5.1 > [pip3] mypyextensions==1.0.0 > [pip3] numpy==1.26.2 > [pip3] pytorchlightning==2.1.2 > [pip3] torch==2.1.1 > [pip3] torchfidelity==0.3.0 > [pip3] torchsummary==1.4.5 > [pip3] torchaudio==2.1.1 > [pip3] torchmetrics==1.2.0 > [pip3] torchvision==0.16.1 > [conda] nomkl                     3.0                           0   > [conda] numpy                     1.23.4                   pypi_0    pypi > [conda] numpydoc                  1.4.0            py39hca03da5_0  ",2024-03-28T13:36:36Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/122889,"This is expected, see https://pytorch.org/docs/stable/notes/numerical_accuracy.htmlbatchedcomputationsorslicecomputations (and use torch.allclose to compare instead of .eq)"
yi,[FSDP2] Used `_chunk_cat` for reduce-scatter copy-in,"  CC([FSDP2] Used `_chunk_cat` for reducescatter copyin)  CC(Fixed increasing CPU overhead of `RemovableHandle.__init__`)  CC(Fixed `_infer_device_type` warning in `checkpoint`) This PR uses `_chunk_cat` to fuse padding gradients on dim0, chunking into `world_size` chunks, and copying them into the reducescatter input. ",2024-03-28T13:32:28Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (fsdp2),closed,3,6,https://github.com/pytorch/pytorch/issues/122888, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ",".github/workflows/pull.yml / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) is flaky", merge i," Merge started Your change will be merged while ignoring the following 1 checks: .github/workflows/pull.yml / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
gpt,made gpt_fast benchmark run faster,  CC(Modified pointless_convert pass to only apply if it's removing truly useless conversion)  CC(turned on matrixmultiplication => matrixvector multiplication always on if reductiondim is contiguous)  CC(made gpt_fast benchmark run faster)  CC(Added some checkpointing tests),2024-03-28T06:13:30Z,Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/122872, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge i," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 2 checks: pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 3, 5, linux.4xlarge.nvidia.gpu), pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 4, 5, linux.g5.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Incorrect model configuration (possible dynamic axes issue) after torch.onnx.export," üêõ Describe the bug I have the following model:  The model trained with the following parameters:  And properly works as Pytorch model. However, when I export it to onnx:  and try to check the model output:  here is the output I get: > torch.Size([3, 222, 13]) > torch.Size([3]) > inputs > input_lenghts > logits > 20240328 10:26:00.4106034 [E:onnxruntime:, sequential_executor.cc:514 onnxruntime::ExecuteKernel] Nonzero status code returned while running Reshape node. Name:'/conformer_/conformer_layers.0/self_attn/Reshape_4' Status Message: D:\bld\onnxruntime_1710148767998\work\onnxruntime\core\providers\cpu\tensor\reshape_helper.h:45 onnxruntime::ReshapeHelper::ReshapeHelper input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{222,3,512}, requested shape:{1,24,64} >  > Traceback (most recent call last): >   File ""..\ptconformeronnxtestoutput.py"", line 68, in  >     result = session.run([label_name], { input_name: ortvalue, input2_name: ortvalue2 }) >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ >   File ""..\Conda\envs\torch\Lib\sitepackages\onnxruntime\capi\onnxruntime_inference_collection.py"", line 220, in run >     return self._sess.run(output_names, input_feed, run_options) >            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ > onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Nonzero status code returned while running Reshape node. Name:'/conformer_/conformer_layers.0/self_attn/Reshape_4' Status Message: D:\bld\onnxruntime_1710148767998\work\onnxruntime\core\providers",2024-03-28T04:35:28Z,module: onnx triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/122868,Closing as CC(torchaudio.models.Conformer incorrectly converted to onnx (only one of the axes becomes dynamic)) desribing the same issue using just the library Conformer model
rag,Fix performance regression and memory storage handling of Flash Attention on ROCM,This PR fixes the two major issues that was discovered after the initial merge of PR CC(Add Flash Attention support on ROCM)  1. The Flash Attention support added by has severe performance regressions on regular shapes (power of two head dimensions and sequence lengths) compared with PR CC(Readd initial Flash Attention support on ROCM). Its performance is worse than the math backend and only has numerical stability advantages. This PR fixes this problem. 2. There is a flaw of memory storage handling in PR CC(Add Flash Attention support on ROCM) which does not copy the gradients back to the designated output tensor. This PR removes the deprecated `TensorStorageSanitizer` class which is unnecessary due to the more flexible backward kernel shipped by PR CC(Add Flash Attention support on ROCM) ,2024-03-28T02:28:41Z,module: rocm triaged open source Merged ciflow/trunk topic: not user facing ciflow/inductor ciflow/rocm,closed,0,16,https://github.com/pytorch/pytorch/issues/122857,"  Please review this PR, it provides significant performance improvements for flash attention on ROCm. We'd like to get this landed in trunk so we can cherrypick it for release/2.3 if possible.",Can you provide some performance comparisons for this PR: https://github.com/pytorch/pytorch/blob/main/benchmarks/transformer/sdpa.py can be used ," To be more concrete about the performance numbers: on MI210 system 1. The prototype PR https://github.com/pytorch/pytorch/pull/115981 shows ~100 TFLOPS throughput (but cannot handle irregular shapes) 2. The current main can handle the irregular shapes but the throughput slows down to 30+ TFLOPS only, making the Flash attention unattractive to users who seek for performance since the math backend can provide about 60 TFLOPS 3. With this PR, the Flash attention's performance returns back to 94 TFLOPS, and can handle irregular shapes. The numbers are computed with script https://github.com/ROCm/aotriton/blob/25696604000bb29465b068afe03db2acbf525ad3/tritonsrc/performance_forward.py with following changes: 1. the script is copied to `test/` so it uses AOTriton rather than Triton. 2. the `N_CTX` (sequence length) is set to 8192 and all numbers above uses this configuration.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: .github/workflows/rocm.yml / linuxfocalrocm6.0py3.8 / test (default, 3, 6, linux.rocm.gpu.2), .github/workflows/rocm.yml / linuxfocalrocm6.0py3.8 / test (default, 6, 6, linux.rocm.gpu.2) Details for Dev Infra team Raised by workflow job ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `xinyazhang/aotriton_perf_fixupstream` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout xinyazhang/aotriton_perf_fixupstream && git pull rebase`)"," merge f ""Unrelated CI failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",Numbers on MI250X (CI GPUs) Tested with `python benchmarks/transformer/sdpa.py` under `pytorch` root directory. `torch2.4.0a0+gite6ee832` (without fix)  Doubled performance for most cases. We'll investigate `forward_time=4757.028084713966` regression in later releases., Does this fix involve only `bfloat16`? I'm asking because your benchmark results only show `bfloat16`. Thanks!,>  Does this fix involve only `bfloat16`? I'm asking because your benchmark results only show `bfloat16`. Thanks! It fixes both float16 and bfloat16 (actually our internal performance testing script only checks `fp16`). I did not modify `benchmarks/transformer/sdpa.py` so only bf16 is tested," Sorry but is this available in PyTorch 2.3.0? I understood that it was supposed to be, but trying 2.3.0 still gives me `UserWarning: 1Torch was not compiled with memory efficient attention`. Thanks a lot!",Hi   efficient attention is another thing. On CUDA the supported environment is a superset of FA but on ROCM it's another story. Efficient attention is being worked on in PR https://github.com/pytorch/pytorch/pull/124885
transformer,[dynamo][pt2d] avoid skipping modules from torch/testing/_internal,"  CC([FSDP2][PT2D] enable torch.compile for compute in CI)  CC([dynamo] avoid creating module proxy for fsdp modules)  CC([dynamo][pt2d] avoid skipping modules from torch/testing/_internal) Dynamo skips user defined modules from `torch/testing/_internal` (eg MLP, Transformer). This PR adds `torch/testing/_internal/...` to `manual_torch_name_rule_map`. It ensures FSDP CI + torch.compile are meaningfully tested unit test shows frame count = 0 before and frame count > 0 after  some FSDP unit tests actually start to compile modules with this change. add trition availability check or disable tests for now ",2024-03-28T00:32:45Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) module: dynamo ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/122851, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 4 jobs have failed, first few of them are: .github/workflows/generatedlinuxbinarylibtorchcxx11abimain.yml, trunk, .github/workflows/generatedlinuxbinarylibtorchprecxx11main.yml, .github/workflows/generatedlinuxbinarymanywheelmain.yml Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 4 jobs have failed, first few of them are: linuxbinarylibtorchcxx11abi, .github/workflows/trunk.yml, .github/workflows/generatedlinuxbinarylibtorchprecxx11main.yml, linuxbinarymanywheel Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Enabling `swap_tensors` path by default for all tensor subclasses errors on some transformer tests," üêõ Describe the bug The specific issue this tracks is https://github.com/pytorch/pytorch/pull/122755discussion_r1541541639  We see that the reference count of layernorm's weight is 3 in this test. However, the swap_tensors test for `LayerNorm`/`TransformerEncoder`does not catch this. Worth investigating whether this is due to the way the test is written or an inherent problem with `TransformerEncoder`/`LayerNorm` and the `swap_tensors` path in `nn.Module._apply`   Versions main  ",2024-03-27T17:43:21Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/122803
llama,[export] hack skip index_put_ in dce,Summary: Ideally we should do whats in the todo. Just doing this for now to unblock llama capture Test Plan: capturing llama and using pt2e to quantize it Differential Revision: D55354487,2024-03-26T19:32:18Z,open source ciflow/trunk release notes: fx,closed,0,0,https://github.com/pytorch/pytorch/issues/122721
rag,Skip storage check debug assert in view codegen when output is a subclass instance,"  CC(Skip storage check debug assert in view codegen when output is a subclass instance) Before the fix, this assert blows up in DEBUG mode for views where the input (base) is a dense tensor and the output (view) is a subclass instance.",2024-03-26T19:07:30Z,Merged ciflow/trunk topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/122718, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 3 checks: pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 3, 5, linux.g5.4xlarge.nvidia.gpu), pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge, unstable), trunk / linuxfocalrocm6.0py3.8 / build Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: GraphQL query  fragment PRReviews on PullRequestReviewConnection {   nodes {     author {       login     }     bodyText     createdAt     authorAssociation     editor {       login     }     databaseId     url     state   }   pageInfo {     startCursor     hasPreviousPage   } } fragment PRCheckSuites on CheckSuiteConnection {   edges {     node {       app {         name         databaseId       }       workflowRun {         workflow {           name         }         url       }       checkRuns(first: 50) {         nodes {           name           conclusion           detailsUrl           databaseId           title           summary         }         pageInfo {           endCursor           hasNextPage         }       }       conclusion     }     cursor   }   pageInfo {     hasNextPage   } } fragment CommitAuthors on PullRequestCommitConnection {   nodes {     commit {       authors(first: 2) {         nodes {           user {             login           }           email           name         }       }       oid     }   }   pageInfo {     endCursor     hasNextPage   } } query ($owner: String!, $name: String!, $number: Int!) {   repository(owner: $owner, name: $name) {     pullRequest(number: $number) {       closed       isCrossRepository       author {         login       }       title       body       headRefName       headRepository {         nameWithOwner       }       baseRefName       baseRefOid       baseRepository {         nameWithOwner         isPrivate         defaultBranchRef {           name         }       }       mergeCommit {         oid       }       commits_with_authors: commits(first: 100) {         ...CommitAuthors         totalCount       }       commits(last: 1) {         nodes {           commit {             checkSuites(first: 10) {               ...PRCheckSuites             }             status {               contexts {                 context                 state                 targetUrl               }             }             oid           }         }       }       changedFiles       files(first: 100) {         nodes {           path         }         pageInfo {           endCursor           hasNextPage         }       }       reviews(last: 100) {         ...PRReviews       }       comments(last: 5) {         nodes {           bodyText           createdAt           author {             login           }           authorAssociation           editor {             login           }           databaseId           url         }         pageInfo {           startCursor           hasPreviousPage         }       }       labels(first: 100) {         edges {           node {             name           }         }       }     }   } } , args {'name': 'pytorch', 'owner': 'pytorch', 'number': 122718} failed: [{'message': 'Something went wrong while executing your query. Please include `13C1:38B2A9:113C5C:17060D:660357CB` when reporting this issue.'}] Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 2 checks: pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 3, 5, linux.g5.4xlarge.nvidia.gpu), pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge, unstable) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Cannot access data pointer of Tensor that doesn't have storage when using torch.func.vmap with c++/cuda extension," üêõ Describe the bug torch.func.map reports error using cpp extension function, `constrain_make_charts` is a simple table lookup operation.  cpp code: **test.cpp**  tensor([[0., 1., 0., 1.],         [1., 1., 0., 0.],         [0., 0., 1., 1.]], dtype=torch.float64) Traceback (most recent call last):   File ""/home/zbwu/Desktop/issue/demo.py"", line 21, in      func(cond_array)   File ""/home/zbwu/soft/anaconda3/lib/python3.10/sitepackages/torch/_functorch/apis.py"", line 188, in wrapped     return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)   File ""/home/zbwu/soft/anaconda3/lib/python3.10/sitepackages/torch/_functorch/vmap.py"", line 278, in vmap_impl     return _flat_vmap(   File ""/home/zbwu/soft/anaconda3/lib/python3.10/sitepackages/torch/_functorch/vmap.py"", line 44, in fn     return f(*args, **kwargs)   File ""/home/zbwu/soft/anaconda3/lib/python3.10/sitepackages/torch/_functorch/vmap.py"", line 391, in _flat_vmap     batched_outputs = func(*batched_inputs, **kwargs) RuntimeError: Cannot access data pointer of Tensor that doesn't have storage  Collecting environment information... PyTorch version: 2.2.1 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (Arch Linux 10.5.01) 10.5.0 Clang version: 17.0.6 CMake version: version 3.29.0 Libc version: glibc2.39 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) ``` ",2024-03-26T16:32:33Z,triaged module: functorch,closed,0,5,https://github.com/pytorch/pytorch/issues/122706,"I am encountering the same issue. Look forward to a fix soon. Things I notice are:  You have to make `constrain_make_charts` a PyToch `Function` class with `forward` and `vmap` static methods. Then it let you apply `vmap` over that function. However, I got the correct result for the first batch but garbage results from 2nd batch.  This is because `constrain_make_charts` itself cannot handle batch input, applying `vmap` will give undefined behaviors.  The only thing you can do is multithread CPU/CUDA programming, which I'm not familiar with.","> I am encountering the same issue. Look forward to a fix soon. Things I notice are: >  > * You have to make `constrain_make_charts` a PyToch `Function` class with `forward` and `vmap` static methods. Then it let you apply `vmap` over that function. However, I got the correct result for the first batch but garbage results from 2nd batch. > * This is because `constrain_make_charts` itself cannot handle batch input, applying `vmap` will give undefined behaviors. > * The only thing you can do is multithread CPU/CUDA programming, which I'm not familiar with. I don't quite understand how to define `vmap staticmethod` using `torch.autograd.Function`. Could you provide an example for using this.","vmap will accept something like this but the operation is apparently wrong. The below code will blindly apply the whole batch input to the `cartpole2l_cpp.forward_dynamics` function, resulting in undefined behaviors. Probably I should use a `for loop` for correct results, but it is NOT what we want. **We need to do CUDA programming for that function.** Everyone: Please let me know if you have better ideas on this. Thanks. ","As  said, you need to provide a `vmap` staticmethod (ref) or defining `generate_vmap_rule=True` to let PyTorch autogenerate it for you.",Closing this as expected.
agent,elastic agent kill hang process when other processes timeout," üêõ Describe the bug When using multiprocess training, if process A hangs, another process B may be aborted because of timeout. The elastic agent will consequently shutdown process A after receiving the death signal from process B. In this case, how can we analyze the reason why process A hangs? Can PyTorch provide a mechanism to get the stack of process A before killing it, or just provide a debug mode that dose not kill any process?  Versions PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (aarch64) GCC version: (GCC) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.0 Libc version: glibc2.27 Python version: 3.8.18 (default, Sep 11 2023, 13:19:25)  [GCC 11.2.0] (64bit runtime) Python platform: Linux4.15.0112genericaarch64withglibc2.26 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True ",2024-03-26T07:51:05Z,oncall: distributed triaged module: elastic,open,0,7,https://github.com/pytorch/pytorch/issues/122694,cc:  ," If I understood you correctly, you could increase `last_call_timeout` as part of `rdzv_conf` options if you are using c10d based rendezvous impl (which is default) to delay cleanup stage.  Here are the docs for timeout parameters: https://pytorch.org/docs/stable/elastic/rendezvous.htmltorch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler","we are adding a feature called 'flight recorder' which can dump useful state info from each process. TORCH_NCCL_TRACE_BUFFER_SIZE=20000 TORCH_NCCL_DUMP_ON_TIMEOUT=1 this feature is still experimental, and the output file format is a pickle that you can load.  We plan to provide scripts for analysis, but if you want to poke around for now you'll have to do some extra work to parse those files and understand which rank caused the hang.  ", Thank you for your suggestion. The 'flight recorder'  is just what I need. When can we use it in a stable version? Is there any roadmap? ,  Thank you for your suggestion. I read the code about the `last_call_timeout `roughly and found it only works when initialize the rendezvous. It seems not applicable when process group is already working. ,"  > For your question: The 'flight recorder' is just what I need. When can we use it in a stable version? Is there any roadmap? Details about the feature are here:  CC(Flight Recorder - feature requests) Experimental `fliight recorder` should be available in the upcoming `2.4` release. Code is already checked into main branch. With the two ENVs above, `flight recorder` would dump a perrank file on every machine. The files can be opened to reason about the collectives that caused the hang. We have some analyzer scripts under development that will be checked in soon into the `tools` repo. Happy to share them with you sooner  if needed.",">  >  > > For your question: The 'flight recorder' is just what I need. When can we use it in a stable version? Is there any roadmap? >  > Details about the feature are here: CC(Flight Recorder  feature requests) >  > Experimental `fliight recorder` should be available in the upcoming `2.4` release. Code is already checked into main branch. With the two ENVs above, `flight recorder` would dump a perrank file on every machine. The files can be opened to reason about the collectives that caused the hang. We have some analyzer scripts under development that will be checked in soon into the `tools` repo. Happy to share them with you sooner  if needed. I used the flight recorder in PyTorch 2.4 to dump the NCCL trace file successfully. Where can I find the analyzer scripts? Additionally, I noticed that it did not dump the C++ trace. The code comments indicate that it need an internal tool for process tracing. Is it possible for us to obtain this tool?"
llm,"""torch._dynamo.exc.Unsupported: torch.* op returned non-Tensor bool call_method is_complex"" error"," üêõ Describe the bug I am seeing the following error while running a simple test. ""torch._dynamo.exc.Unsupported: torch.* op returned nonTensor bool call_method is_complex""  Error logs _No response_  Minified repro TORCH_COMPILE_DEBUG=1 python 1.py 1.py.txt  Versions Collecting environment information... PyTorch version: 2.2.0a0+git08566f1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.5 (ssh://gerrit.habanalabs.com:29418/tpc_llvm10 df95e4bd2b71c56d4042f14fa9b00af5c49c55c4) CMake version: version 3.27.7 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.091genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      43 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             12 Online CPU(s) list:                011 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Xeon(R) Gold 5220R CPU @ 2.20GHz CPU family:                         6 Model:                              85 Thread(s) per core:                 1 Core(s) per socket:                 6 Socket(s):          ",2024-03-26T06:33:47Z,triaged oncall: pt2 module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/122692,,"Hi  , can not repro this issue with nightly version 2.3.0.dev20240204+cu118","Can you please try with TORCH_COMPILE_DEBUG=1, if not done so ? I will also try on latest nightly","Hi  , I got the same error with you while adding TORCH_COMPILE_DEBUG=1","I can repro this issue if I set `fullgraph=True`. When `fullgraph=False`, this error causes a graph break and hence won't stop the program."
llama,[export] hack skip index_put_ in dce,Summary: Ideally we should do whats in the todo. Just doing this for now to unblock llama capture Test Plan: capturing llama and using pt2e to quantize it Differential Revision: D55354487,2024-03-26T04:16:05Z,fb-exported Merged ciflow/trunk release notes: fx,closed,0,7,https://github.com/pytorch/pytorch/issues/122683,This pull request was **exported** from Phabricator. Differential Revision: D55354487,This pull request was **exported** from Phabricator. Differential Revision: D55354487,This pull request was **exported** from Phabricator. Differential Revision: D55354487, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , cherrypick onto release/2.3 c regression, Cherry picking CC([export] hack skip index_put_ in dce) The cherry pick PR is at https://github.com/pytorch/pytorch/pull/122721 and it is recommended to link a regression cherry pick PR with an issue Details for Dev Infra team Raised by workflow job 
transformer,update comment of test_invalid_last_dim_stride in test_transformers.py,Fixes CC(There is a comment error in test_transformers.py)  ,2024-03-26T02:21:06Z,open source Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/122679," merge f ""Just fixing comment, should not require CI"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Deprecate `torch.nn.MultiHeadAttention` and `torch.nn.Transformer`-related modules ," üöÄ The feature, motivation and pitch This issue is WIP and is a placeholder to track discussion around the deprecation of `torch.nn.MultiHeadAttention` and `torch.nn.Transformer`related `torch.nn` modules. We observe that users would like transformer blocks to be performant and customizable. While these layers may provide an ""out of the box"" solution, they are monolithic modules.  Further, we note componentwise tools such as `torch.nn.functional.scaled_dot_product_attention`  (SDPA), `torch.nn.attention.bias` and `torch.nested` have been introduced to address the issues of customizability, composability and performance.  While there have been efforts to improve the performance and customizability of the `Transformer`/`MHA` modules, such as the addition of new masks and ""fast paths"" in these modules gated by configuration checks, we note that there have also been several issues surrounding these modules. Examples of issues around these modules include  Confusion around various arguments         CC(Inconsistent usage of `attn_mask` and `is_causal` arguments)       Definition of `attn_mask` is flipped compared to SDPA/implementation in other frameworks https://github.com/pytorch/pytorch/pull/120668        CC(Detailed documentation with an example for the causal mask in nn.TransformerEncoder)  Performance related issues        CC(Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`)       CC(nn.TransformerEncoderLayer fastpath (BetterTransformer) is much slower with src_key_padding_mask)  Bugs related to the ""fast path""       CC(TransformerEncoderLayer forw",2024-03-26T00:01:57Z,module: nn triaged,open,4,1,https://github.com/pytorch/pytorch/issues/122660, 
rag,[c10d][NCCL] Refactor coalesced storage,The `coalescedDevice_` are `coalescedComms_` used inefficiently and in case of consequent coalescing comms can cause to readbeforewrite condition. ,2024-03-25T22:20:17Z,oncall: distributed module: nccl open source Merged ciflow/trunk release notes: distributed (c10d),closed,0,6,https://github.com/pytorch/pytorch/issues/122651, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxjammypy3.8gcc11 / test (default, 2, 3, linux.2xlarge)  .github/workflows/pull.yml / linuxjammypy3.8gcc11 / test (default, 3, 3, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","The CI failures are unrelated,  merge i ", merge i," Merge started Your change will be merged while ignoring the following 3 checks: pull / linuxjammypy3.8gcc11 / test (default, 2, 3, linux.2xlarge), pull / linuxjammypy3.8gcc11 / test (default, 3, 3, linux.2xlarge), pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge, unstable) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Support fakifying method calls of torchbind object,"  CC(support tracing custom op that takes torch script object as inputs)  CC(Move predispatch handling logic into helper functions for reuse)  CC(Support fakifying method calls of torchbind object)  CC(Add torch._library.register_fake_class to fakify torchBind class)  CC(add tensor queue example)  CC([torchbind] change to parametrized tests for pre_dispatch)  CC([torchbind] redispatch call_torchbind in proxy dispatch mode) This PR adds support for fakifying torchbind object in make_fx.  Specifically, when tracing_mode is ""fake"" or ""symbolic"", we'll lookup the registered fake class for the script object and wraps the fake class's instance into AbstractScriptObject. The  AbstractScriptObject is a generic container for any fake script object. Its methods are created dynamically by wrapping its contained fake object's method with call_torchbind higher order op. This design allows  us to 1. turn all method calls of script object into call_torchbind higher order op, and 2. generically share a type across all abstract classes so that we could use it to track them as proxies. Note that this diff haven't fakified torch script objects by default in export. We need to add the fakification logic to export before entering aot. As a result, all current tests will still trace with real script object except the newly added make_fx test. We'll add the support later in this pr stack. ",2024-03-25T17:09:51Z,release notes: fx ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/122623
transformer,[Regression PyTorch 2.1 & 2.2] `torch._transformer_encoder_layer_fwd` outputs on CUDA/NestedTensorCUDA backends significantly diverge from truth for torch>=2.1 (SDP math))," üêõ Describe the bug Hi, As reported in https://github.com/huggingface/optimum/issues/1769, I noticed that the outputs of `torch._transformer_encoder_layer_fwd` significantly diverge from the correct outputs for torch>=2.1. User reports degradated predictive performance due to this. In my case, I see a mean relative difference of **more than 2%** overall. Besides, **more than 2% of values have a mean relative difference of more than 5%**. Besides, out of a 16384 values output tensor, 78 values differ by more than 20%, which is huge and seem to impact the global outputs. Note that I checked only one layer, this may accumulate on several layers. Note: Given the user report of degradated performance, I am confident that PyTorch 2.0.2 is ""truth"", but I need to confirm that This occurs only on CUDA device, not CPU. Reproduction: * `wget https://huggingface.co/fxmarty/debug_transformer_encoder_layer_fwd/raw/main/debug_bt.zip` (see https://huggingface.co/fxmarty/debug_transformer_encoder_layer_fwd/tree/main, could not upload on github directly as too large) * unzip it * Create two conda envs with torch `2.2.1+cu118` and `2.0.1+cu118` Run on each conda environment  And then run  giving   Versions PyTorch 2.2.1 env:  PyTorch 2.0.1 env:  ",2024-03-25T16:57:02Z,high priority module: nn triaged module: nestedtensor,open,0,1,https://github.com/pytorch/pytorch/issues/122617,?
llava,Restore DILL_AVAILABLE for backwards compat with torchdata,  CC(Restore DILL_AVAILABLE for backwards compat with torchdata) Signedoffby: Edward Z. Yang  ,2024-03-25T16:41:11Z,Merged ciflow/trunk release notes: dataloader topic: bug fixes module: python frontend,closed,2,14,https://github.com/pytorch/pytorch/issues/122616, merge i," Merge started Your change will be merged while ignoring the following 2 checks: pull / beforetest / llmretrieval, trunk / beforetest / llmretrieval Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / beforetest / llmretrieval Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge i," Merge started Your change will be merged while ignoring the following 3 checks: .github/workflows/pull.yml / beforetest / llmretrieval, .github/workflows/pull.yml / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge), .github/workflows/trunk.yml / beforetest / llmretrieval Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / beforetest / llmretrieval Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ," merge f ""Unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",Is there any workaround for this issue if working with torch 2.3.0?,"I would expect that you can work around this by doing the imports this way, not ideal though: ","Thanks, It worked!", cherrypick onto release/2.3 c regression fixes  CC(2.3.0 not backward compatible with torchdata), Cherry picking CC(Restore DILL_AVAILABLE for backwards compat with torchdata) The cherry pick PR is at https://github.com/pytorch/pytorch/pull/126094 and it is linked with issue  CC(2.3.0 not backward compatible with torchdata) Details for Dev Infra team Raised by workflow job ,FileNotFoundError: Cannot find DGL C++ graphbolt library at /usr/local/lib/python3.10/distpackages/dgl/graphbolt/libgraphbolt_pytorch_2.3.0.so How to solve this problem?anyone can help me in this
llm,[ez][TD] Hide errors in llm retrieval job,"The new ghstack does have a base on main anymore, so finding the base for ghstacked PRs is harder.  Something similar to https://github.com/pytorch/pytorch/pull/122214 might be needed, but then I'm worried about tokens Either way, this is a quick workaround to hide these errors for ghstack users",2024-03-25T16:14:31Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/122615," merge f ""lint passed, only job of concern passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,There is a comment error in test_transformers.py, üêõ Describe the bug https://github.com/pytorch/pytorch/blob/main/test/test_transformers.pyL1437 The comment does not match the name of test case     Versions torch version == main,2024-03-25T08:01:30Z,triaged actionable,closed,0,1,https://github.com/pytorch/pytorch/issues/122594,Feel free to send a PR removing this comment or updating it appropriately
transformer,expose transformer header in cmake and wheel,"expose transformer header in cmake and wheel, some utils functions are used in nested transformer development on IPEX side",2024-03-25T02:47:49Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,11,https://github.com/pytorch/pytorch/issues/122586, merge,  Could you pls try merge it again? thanks., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ",  rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `zhenyuan_expose_transformer_header` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout zhenyuan_expose_transformer_header && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"If we are doing something like that, it would be nice to put a warning, that those headers are subject to change "
llama,[dtensor] add op support for view_as_complex and view_as_real,  CC([dtensor] add op support for view_as_complex and view_as_real)  CC([dtensor] add backward support for scaled dot product attention (flashattention)) This PR will unblock DTensor computations for rotary embeddings used in LLaMa training. ,2024-03-24T07:06:05Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,2,https://github.com/pytorch/pytorch/issues/122569, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[Dynamo][4/N] Enable clang-tidy coverage on torch/csrc/dynamo/*,This PR enables clangtidy coverage on torch/csrc/dynamo/* and also contains other small improvements. ,2024-03-22T23:52:47Z,open source Merged ciflow/binaries ciflow/trunk topic: not user facing module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/122534, merge i, Merge started Your change will be merged while ignoring the following 1 checks: linuxbinaryconda / condapy3_11cuda11_8build / build Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: linuxbinaryconda / condapy3_8cuda11_8build / build Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 2 checks: linuxbinaryconda / condapy3_11cuda11_8build / build, linuxbinaryconda / condapy3_8cuda11_8build / build Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,DTensor: add ring attention for _scaled_dot_product_flash_attention,"Ring attention support for _scaled_dot_product_flash_attention with DTensor. This assumes the query and key/value are sharded along the sequence length dimension. See the tests for example usage with PT Transformer as well as direct usage with _scaled_dot_product_flash_attention.  Notable caveats * Numerical accuracy: The backwards pass doesn't match numerically with the nonchunked version but the forwards pass does. I assume this is due to accumulated errors. I've added a chunked version that uses autograd to verify that the distributed version matches the chunked version. * nn.Linear has incorrect behavior when running on a sharded tensor of size (bs, heads, seq_len, dim) with `Shard(2)` and does an unnecessary accumulate which requires `Replicate()` on QKV when using `nn.MultiHeadedAttention` to work around the issue. * If enabled, it forces sequence parallelism and doesn't interop with tensor parallelism.  SDPA usage   Transformer usage   Test plan  ",2024-03-22T01:21:05Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,8,https://github.com/pytorch/pytorch/issues/122460, using `sdpa_kernel` does work  that's what we do for the ContextParallel + Transformer pattern. I'm just using it directly in the sdpa tests so it's extra obvious, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxdocs / builddocspythonfalse Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Triangular matrix storage + matmul," üöÄ The feature, motivation and pitch Triangular matrixvector or matrixmatrix multiplication is a useful primitive in inference (because past and present tokens are taken into account, but not future)  in theory this saves half the FLOPs and storage. This feature exists in cuBLAS here  though I found it to be slower than GEMM in testing. It would be great to have a format to store a triangular matrix and a function that would recognize it.   Alternatives Doing normal GEMM works, but you have to store a lot of 0s and compute products with them.  Additional context _No response_ ",2024-03-22T00:21:25Z,triaged module: linear algebra,open,0,4,https://github.com/pytorch/pytorch/issues/122454,Maybe you are looking for something like https://github.com/cornelliusgp/linear_operator?,"If cublas' implmentation is not faster than the regular gemm bgemm, this request should probably go to NVIDIA, as we don't implement these kernels ourselves.","> Maybe you are looking for something like https://github.com/cornelliusgp/linear_operator? This looks great, thank you! Edit: It does seem to require first forming the full matrix and then discarding entries. Ideally you could specify it in triangular fashion initially. > If cublas' implmentation is not faster than the regular gemm bgemm, this request should probably go to NVIDIA, as we don't implement these kernels ourselves. I mentioned this originally at a meeting with NVIDIA and  asked me to open an issue. NVIDIA can solve the slowness, but it still won't be useable by pytorch. The idea was to work with both",Once they have something that's usable we can certainly look into creating bindings for it.
gemma,Gemma model is failing to run with Dynamo on CPU," üêõ Describe the bug The model: https://github.com/google/gemma_pytorch To enable Dynamo I'm adding `.compile()` (assuming cpu backend) annotation to this line. Here's how I'm running it:  It it passing with PyTorch `v2.2.1`, but fails with later versions, including top of the trunk.  Error logs   Minified repro I failed to minify the model. I got this error running minifier:  I assume it's not exactly what expected to the result of minifier run. But it might be related to the root cause, as it's also complains on the complex type, which is present in the original error (i.e. `aten.view_as_complex.default`).  Versions Collecting environment information... PyTorch version: 2.3.0a0+git36e118b Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.3.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: version 3.28.1 Libc version: N/A Python version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang1500.1.0.2.5)] (64bit runtime) Python platform: macOS14.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Max Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] optree==0.10.0 [pip3] torch==2.4.0a0+gitd131cbc [conda] Could not collect ",2024-03-21T23:08:27Z,triaged module: complex oncall: pt2 module: inductor,closed,0,10,https://github.com/pytorch/pytorch/issues/122448,Possibly related to another Gemma issue: CC(torch.export: Unsupported: call_function args: UserDefinedObjectVariable(BatchEncoding) on Gemma),"Here's reduced example:  It seems `torch.stack` lowering produces broken stride, i.e. `[256, 1, 256, 2, 1]` instead of `[256, 256, 256, 2, 1]`","The stride is constructed in make_channels_last_3d_strides_for during lowering of `aten.cat` operator. The function get `shape = [1, 1, 1, 128, 2]` and returns `[256, 1, 256, 2, 1]`. This seems incorrect.","What really happens, that triggered this regressions, is that PR https://github.com/pytorch/pytorch/pull/118453 disabled `poitwise_cat` on CPU, which worked correctly. So `aten.cat` is now lowered by ir.ConcatKernel.create(), which doesn't handle this case correctly. The logic for `ir.ConcatKernel.create()` broken, as it detects lastchannel layout, while it's just continuous. And that's what triggers execution of `make_channels_last_3d_strides_for`, which actually does incorrect thing for continuous layout.", I'd expect the strides passed to `torch.view_as_complex` to be fixed by the `require_contiguous` here: https://github.com/pytorch/pytorch/blob/26a9b05bcef33a45eb2e5d449a616e4119c5edca/torch/_inductor/lowering.pyL2040 https://github.com/pytorch/pytorch/blob/26a9b05bcef33a45eb2e5d449a616e4119c5edca/torch/_inductor/lowering.pyL1813L1817 This will either mutate the layout of the producing op (in the case of FlexibleLayout) or introduce a copy to force contiguous strides.   Though perhaps we should change that to be: https://github.com/pytorch/pytorch/blob/26a9b05bcef33a45eb2e5d449a616e4119c5edca/torch/_inductor/lowering.pyL1827 Which forces the layout to match eager mode and might be more correct. What error are you seeing with that smaller repro?,"> What error are you seeing with that smaller repro? I'm executing on macOS, so it's executed on CPU. And Dynamo crashes like this. I have exactly the same error when trying to run Google Gemma model. ","> I'd expect the strides passed to `torch.view_as_complex` to be fixed by the `require_contiguous` here I'm not an expert here, but the `stride=[256, 1, 256, 2, 1]` looks to me incorrect in the first place. It's not technically incorrect, as long as the size on ""broken"" dimension is 1 anyway (size=[1, 1, 1, 128, 1]). But the lowering seems to make assumptions that are not really correct, assuming that it's channellast 3d layout, while it's just 5d tensor with continuous layout. There are good chances that I misunderstand this though :)","The strides of the `size=1` dimensions shouldn't matter (since it is only ever multiplied by 0), though I think this is a case where the ""convert to channels last"" logic is too aggressive. This PR fixes the smaller repro: https://github.com/pytorch/pytorch/pull/123758"," I checked that CC([inductor] Disable channels_last heuristic when channels==1) solves the original problem with Gemma model, not just small reproducer.",Closing as https://github.com/pytorch/pytorch/pull/123758 was merged. Feel free to reopen if the problem persists.
rag,add case for creating storage on ort,Fixes CC(allow storage to be created for ort),2024-03-21T22:48:33Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/122446, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,allow storage to be created for ort," üöÄ The feature, motivation and pitch the goal is to enable creating a storage on the ort backend. So something like this   Alternatives _No response_  Additional context _No response_",2024-03-21T22:48:13Z,triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/122445
rag,"functionalize storage resizing, minimal ppFSDP traceable forward","More details further down, but first a more highlevel description of ""how do we functionalize storage resizing"" Today, dynamo converts `param.untyped_storage().resize_(x)` calls that it sees from fsdp into a custom op, `ops.inductor.resize_storage_bytes_(x)` So given this setup, there are 3 main cases that I think we want to handle: (1) graph input starts with a real storage size, gets resized down to zero in the graph (2) graph input starts with 0 storage size, gets resized up in the graph (3) graph input starts with 0 storage size, gets resized up and used in some compute, then resized back down to 0 For case (1) we need to emit a `resize_storage_bytes_` at the end of the graph, similar to how we emit `copy_()` for data mutations. For case (2), we need to emit a `resize_storage_bytes_` in the graph, and we **also** need to emit a `copy_()` (the input had its storage resized up, and filled in with data, which is we need to reflect as an input mutation) For case (3), the net effect is that the input had no data on entry and exit of the function, so we don't need to emit any mutable ops in the end of the graph. The main thing to call out is that: we need to write a functionalization rule for `resize_storage_byte_`, (`FunctionalTensorWrapper::storage_resize_()`) and this rule actually does very little. We would like to **not** emit any new ops in the graph (like say, a functional resize op). Instead, we should expect / rely on the fact that any resize up will be immediately followed by a `copy_()`/`foreach_copy_`/`out=` op, that will fill in the data of the tensor. So `FunctionalTensor` can temporarily live in a state where its data is inv",2024-03-21T20:56:18Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor keep-going,closed,1,44,https://github.com/pytorch/pytorch/issues/122434,I guess jansel is the main reviewer here but I couldn't tell what the overall strategy was from the PR description haha," I updated the PR with more details + some perfile changes. LMK if more detail would be helpful I also fixed some CI failures and addressed some of the feedback, although I ran into a few issues (detailed below) > Can you include some examples of the generated graph?  yep  I also added more tests in `test_aotdispatch.py` that show the 3 main cases (resize up, resize down, resize up and back down) and include the generated functional forward graph, lmk if you have any questions > You may need to track original+final size rather than just inductor_storage_resized_ to detect cases of storage going from 0>nonzero>0 (which cancel out and results in nothing). Good catch, this was done incorrectly originally (I was checking the storage size of the fake tensor, which doesn't actually get mutated by the functionalization rule). I added this explicitly instead in `FunctionalStorageImpl.h` > This PR needs some tests. You can get a few by enabling pytorch/test/inductor/test_distributed_patterns.py I flipped this config. Right now, I get a failure running `test_fake_distributed_inductor`: while functionalizing the compiled autograd backward graph, my functionalization rule is complaining that we are resizing a param to size 800, but it already have a storage size of 800. Here is the dynamogenerated torch graph of the forward (link) and the functionalized ATen graph of the forward (link). The problem: in the example test, I would have expected that we first resize our param to the allgather'd size, and then pass it into `torch._C._nn.linear` (this would allow functionalization to maintain the mapping that ""the allgathere'd tensor is the **updated** version of the original, zerosized tensor). In the dynamo graph though, we have something like:  (where the `cat()` in the test simulates ""result of the allgather) The problem is that since the result of `cat()` (the allgather'd param) is fed into the matmul, and functionalization has no link between the `cat()` and the original, zerosized param, we end up directly saving the `cat` buffer for backward (equivalent to saving the allgathered param for backward in the matmul, instead of saving the zerosized param so we can resize it later). One alternative I can imagine is that we massage the ppFSDP code/dynamo graph to look something like:  Although I'm not sure exactly what the delta is. Any thoughts are appreciated",Can we just update the meta function of torch__dynamo_create_parameter_op_tracable_create_parameter to return an alias? If I recall I made it lie and say it didn't alias to workaround functionalization bugs.,"The tests in `test_distributed_patterns.py` now test with `RESIZE=True`. Updates: (1) Killed the _nn_bind_parameter prim op, changed the code in dynamo (the custom `autograd.Function`) to call `set_()` instead. the resize_() rule functionalization is now **literally** a noop, except for the fact that it marks the tensor as having been resized, so we know if we need to emit a mutable resize_() in the graph epilogue in AOTAutograd. (1b): Unfortunately, I had to remove the restriction that `resize_()` can only be called on base tensors. The sequence of ops in the graphs looks something like:  That extra `view()` op comes from the fact that the `set_()` is called from inside of a custom `autograd.Function` (which does the gradient swizzling that we need to propagate the gradient back to `dummy_param`, and the `autograd.Function` adds an extra view op when you return an input directly. (2) Removed my (kind of crazy) `original_base_` code in `FunctionalStorageImpl`. We now end up saving the allgather'd thing for backward, and rely on a mutable `resize_(0)` in the epilogue of the graph to get good memory savings (3) made the `original_storage_size_` / `curr_storage_size_` state on `FunctionalStorageImpl` into SymInts","Actually  the mentioned tests, pass, but I don't think the graph looks right. There is a `storage_resize(0)` in the graph, but it looks like we are not resizing the actual allgathered param that we save for backward. I'll look into it some more. This is the generated, functionalized fw graph from running `python test/inductor/test_distributed_patterns.py k test_fake_distributed_inductor`: P1202196880 Another thing to note is that today, `AOTAutograd` handles `set_()` input mutations by keeping them outside of the graph, which the assumption that handling `set_()` is tricky for a backend, so it gets run in the runtime epilogue. That might make staring at the graph more confusing, so I guess we could consider keeping it in the graph (an alternative we be we do the work of tracing out the runtime wrapper into a graph and logging it).","Hmm, I think I know why we are calling `resize_(0)` on the wrong thing: (1) In the original code, we first call `x.set_(y)`, followed by `x.untyped_storage().resize_(0)`. Importantly, since the `set_()` runs first, our resize ends up deallocating **the data of `y`** (2) Since AOTAutograd keeps `set_()` input mutations outside of the graph today, we end up flipping the order: `x.untyped_storage().resize_(0)` runs on the input inside the graph, and `x.set_(y)` happens after, in the opaque epilogue. The obvious solution is to keep `set_()` in the graph, and require that compiler backends know how to implement it. This seems... probably ok? Another option might be to keep the `_nn_bind_parameter` op around, make it mutable, and effectively treat it as a more limited version of `set_()`, that has easier invariants for a backend to implement. But new op in the graph is kind of annoying. For now, I'll try to update the PR to keep `set_() in the graph","Updated, the forward graph now properly resizes the parameter before saving it for backward. Here is a paste of the ATen graph, generated with `TORCH_LOGS=""aot"" python test/inductor/test_distributed_patterns.py k test_fake_distributed_inductor`: https://www.internalfb.com/phabricator/paste/view/P1202286465 Latest changes: (1) new PR earlier in the stack where I force `set_()` input mutations to show up in the graph instead of being kept outside, which is required if we want the set_() to happen before the resize_() (2) I explicitly ban input mutations of the form `resize_()` followed by `set_()`, since we currently have a fixed ordering in the epilogue that whenever an input sees both of these types of input mutations, we emit them in a fixed order (set_() first). We can lift this restriction later if we need it, but I think this keeps things a bit simpler (3) This is enough to get the ppFSDP test with inductor running the forward successfully, and it now fails in the backward due to the backward aliasing issue I described in (8) of the description at https://github.com/pytorch/pytorch/pull/120971. I think... this is an independentenough problem that we can probably fix it separately? But I left some ideas in a comment next to the failing test case. The generated (notyetfunctional) graph from the compiledautograd backward is here: https://www.internalfb.com/phabricator/paste/view/P1202286544","> The obvious solution is to keep set_() in the graph, and require that compiler backends know how to implement it. This seems... probably ok? Another option might be to keep the _nn_bind_parameter op around, make it mutable, and effectively treat it as a more limited version of set_(), that has easier invariants for a backend to implement. But new op in the graph is kind of annoying. Note that inductor already supports `_nn_bind_parameter`, so shouldn't be that hard for a backend to implement.  I added support for that when I created these tests.","Updated the PR to effectively change inductor's `bind_nn_parameter` lowering to lower `set_` instead, so inductor properly marks mutation.aliasing for that lowering instead of using the fallback. I also kept `can_detach()` deleted, since I don't think I see any failures with it. Main remaining work though is that I still need to: (1) figure out how to fix fake tensor caching  I tried adding storage size to the cache key but that isn't enough, need to look some more (2) generally fix other random CI failures in the stack","Going to continue babysitting CI, but I think I fixed the main issues: (1) fake tensor caching (2) missing header declaration that I couldn't repro locally (it was because CI was using peroperatorheader codegen, and I didn't include `_foreach_copy.h` in `Copy.cpp`)","The latest FakeTensor caching test failures in this PR are blocked on https://github.com/pytorch/pytorch/pull/123880. That fixes a latent bug in FakeTensor, that my PR hits because I'm now including storage nbytes in the cache key.","I think https://github.com/pytorch/pytorch/pull/123880 is the only thing left for this PR to be ready, so to confirm I rebased it below this PR just to sanity check the state of CI (locally, it makes the remaining FakeTensor caching tests pass)","Updated after ignoring the failures that will be fixed by https://github.com/pytorch/pytorch/pull/123880, there were a few more failures to look into. The most interesting one is that some new dynamo tests fail when I update `create_nn_parameter` to properly alias its input and output. Previously, `create_nn_parameter` masqueraded as being functional: this can cause silent correctness, since when we compile any code that creates an `nn.Parameter` (e.g. `y = torch.nn.Parameter(x)`), we sever the aliasing relationship between `x` and `y` (I filed an issue with an example here:  CC(torch.compile + constructing an nn.Parameter + mutating it can give wrong results))) When I update `create_nn_parameter` to properly alias (by using `set_()`, it failed ~34 tests in the dynamo shard. They all have the following shape:  Previously, this code would ""run"" but be silently wrong (we would not update x to reflect the mutation in our compiled code). Now, this code raises an error during tracing:  We now avoid the silent correctness problem. However, this PR is technically BCbreaking, for any code that you torch.compile that exhibits the above behavior. , but do you think the BCchange is lowrisk enough to be acceptable? I think if we want to explore alternatives, we'd need to think of a way around that autograd.Function error.",Perhaps `torch.nn.Parameter` support is added recent enough that BCbreaking to fix the silent correctness issue is okay to do?,hmm  I'm having a hard time understanding the XLA test failure from https://github.com/pytorch/pytorch/actions/runs/8991496548/job/24702482398. Do you know what's going wrong? I see that this error is a redherring in the logs:  And below is there's an XLA stack trace:  Although I don't think I see an actual error message in the logs?,"oh, it looks like it's just a segfault. the stacktrace from the XLA logs points to the FunctionalStorageImpl constructor (which I did change). I thought my check for `TensorImpl>has_storage()` would get taken for XLA, but maybe not. I'll try updating it to explicitly not do the extra work if we detect that our TensorImpl is an XLA tensor.", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","XLA no longer segfaults, but there is still an XLA test failing this assert:  I can't easily run the XLA test locally, so I'm adding better logging to the PR ","ah  this is the printed graph from the failing test (at https://github.com/pytorch/pytorch/actions/runs/9003113930/job/24737735268, `TestDynamoBufferDonationAliasingWithCustomOp.test_manual_buffer_donation_for_inplce_op_repeat`):  It looks like XLA is setting things up such that we `copy_()` back into `dynamo_set_buffer_donor_` in the graph epilogue instead of a placeholder. This technically violates the assert, which tries to sanitycheck that we only ever `copy_()` into placeholders. I think just to unblock... I'm going to skip this assert for that specific op", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,Hmm I don't fully understand the failure but `dynamo_set_buffer_donor_` will inplace update a metadata in the XLATensorImpl. I actually don't know what's the right way for functionization to handle this op.," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,GPT2ForSequenceClassification accuracy test fails for CUDA AOT Inductor export when its SDPA pattern is mapped to the SDPA op," üêõ Describe the bug In CC(GPT2 SDPA inference patternmatching for InductorCPU), GPT2 SDPA pattern is being mapped to `torch.nn.functional.scaled_dot_product_attention`. With CUDA, `GPT2ForSequenceClassification` fails on CI for Inductor AOT export path, but not in other trunk CI checks. Disabling this pattern's replacement for CUDA, until this issue would be resolved. For attaining better performance, this issue should be fixed. Thanks!  Versions PyTorch main branch  ",2024-03-21T19:27:56Z,triaged oncall: pt2 module: aotinductor,closed,0,5,https://github.com/pytorch/pytorch/issues/122429,Can you explain a bit more what the issue is?  Why is this AOT Inductor specific?,"Sorry, I added that label because the only CI check/job that fails when that pattern's replacement is enabled for CUDA is `cuda12.1py3.10gcc9sm86 / test (aot_inductor_huggingface)`, but other CI checks/jobs do not. The CI failure is because of `GPT2ForSequenceClassification` failing an accuracy check in that particular CI job.", ,"Hi,  , I am doing issue scrapping. I see you have a landed PR for this issue, is it fixed and issue can be closed?","> Hi,  , I am doing issue scrapping. I see you have a landed PR for this issue, is it fixed and issue can be closed? Haven't get a chance to look into it. Created https://github.com/pytorch/pytorch/pull/137085 to see if the problem still exists."
transformer,fix cudnn attention check,"For CUDNN attention, besides packed QKV layout with limited support of sequence length (seq_len <= 512) and head dim requirements. Also supporting a more generic ""arbitrary sequence length with flash attention"" as stated in `Transformer Engine`: https://github.com/NVIDIA/TransformerEngine/blob/8e672ff0758033c348e263dbcd6a4b3578c01161/transformer_engine/common/fused_attn/fused_attn.cppL126 More about ""fused flash attention"" in CUDNN graph api: https://docs.nvidia.com/deeplearning/cudnn/developer/graphapi.htmlfusedflashattentionfprop",2024-03-21T07:35:50Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,16,https://github.com/pytorch/pytorch/issues/122391,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: chengzeyi / name: C  (a11ea53348372b57a6ff5c5e1d08c4a56389a964, 6f6eacd60cbc63f784b2859ef46a5635b0e254e1, baeb9f7a85f4c6a202e1b72cfd245e64cce6eb80, ced3ba54902b1347e1ce82be879f448596bbabfe, c7183204a13a24e8834a3f52acb45bc03006c011, 0bcc5e08a3e94fc7a830e8a81d9deabf75c9989a, cfe47522934918544acc5538552b4f8df7e55d61, 8c4cdd7381e94e966bc55115d940b46d15841425, 5e1b27c09d7c8f83bf40f1b236011a4e7454e07a, cb5f9ba673201100161f096e5726354cdfedfd25, 6c625fd02ba72869769ac5547b6d0ba126d8ef0a)","> Thanks for the fixes! What GPUs did you test the checks on? I work with RTX 4090, cuda 12.4 and cudnn9, üòØall the newest stuff. And according to latest cudnn doc. this meets the tensor shape and stride requirements of ""arbitrary sequence fused flash attention"" rather than the quite limited ""max 512 sequence length fused attention with packed QKV"". However, after testing it with stable diffusion v1.5. I found that in fact cudnn's implementation of flash attention is a bit slower (or not faster, just no magic happens~) than the current flash attention v2 implementation in PyTorch, which is quite frustrating. https://docs.nvidia.com/deeplearning/cudnn/developer/graphapi.htmlfusedflashattentionfprop",CC   I think this is a good start that we can merge ahead of the testing PRplease fix the merge conflicts  , Already rebased. Note that the cudnn frontend has some extra checks about the runtime version of CUDNN. But it seems that there is currently no suitable ways to place `cudnnGetVersion()` in `sdp_utils.cpp` I expect this to work properly with the new `CUDA 12.4` depsÔºåas the CUDNN version can be updated to `8.9.7` or `9.0.0`,According to https://github.com/NVIDIA/cudnnfrontend/blob/1b0b5eac540b7f8fd19b18f1e6b8427c95503348/test/python_fe/test_mhas.pyL288 The following checks should be enough: ,Seems that the version of cudnn can be got from `at::detail::getCUDAHooks().versionCuDNN()`, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cuda11.8py3 / build Details for Dev Infra team Raised by workflow job ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch push f https://github.com/siliconflow/pytorch.git pull/122391/head:dev_cudnn_attention` returned nonzero exit code 128  This is likely because the author did not allow edits from maintainers on the PR or because the repo has additional permissions settings that mergebot does not qualify. Raised by https://github.com/pytorch/pytorch/actions/runs/9021489156, would you mind trying to rebase?,>  would you mind trying to rebase? rebased, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gemma,Onnx export memory leak with qlora+deepspeed stage 2+ORT," üêõ Describe the bug In Qlora+Deepspeed stage2+ORT profile, observe memory not released after export !image Before export 8.49G After export 12.48G Note that I did not see memory leak with lora+deepspeed stage2+ORT. Pytorch and ORT memory usage is on par for lora+deepspeed stage2+ORT. I observed this with Gemma but it's probably universal for all models Repro info run_clm.zip export ORTMODULE_USE_EFFICIENT_ATTENTION=1 export ORTMODULE_ONNX_OPSET_VERSION=15 export ORTMODULE_FALLBACK_POLICY=FALLBACK_DISABLE export APPLY_4BIT=true export PYTORCH_NO_CUDA_MEMORY_CACHING=1 export ORTMODULE_DEEPCOPY_BEFORE_MODEL_EXPORT=0 export APPLY_ORT=true nsys profile trace cuda,nvtx,osrt,cublas,cudnn forceoverwrite true s cpu b fp cudabacktrace all cudamemoryusage true stats false  o profile_clm_qlora_mem_ort_nomc_nodc_1%p capturerange=cudaProfilerApi capturerangeend=stopshutdown \ python launcher.py gmon torchrun nproc_per_node 8 run_clm.py model_name_or_path google/gemma7bit dataset_name wikitext dataset_config_name wikitext2rawv1 do_train save_strategy 'no' fp16 block_size 256 gradient_accumulation_steps 1 per_device_train_batch_size 1 num_train_epochs 5 output_dir output_dir overwrite_output_dir deepspeed zero_stage_2.json \  Versions Collecting environment information... PyTorch version: 2.3.0.dev20240215+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.26.0 Libc version: glibc2.31 Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64bit runtime) Pytho",2024-03-20T23:50:15Z,module: onnx triaged,open,0,0,https://github.com/pytorch/pytorch/issues/122359
gemma,torch.export: Unsupported: call_function args: UserDefinedObjectVariable(BatchEncoding) on Gemma," üêõ Describe the bug Google's Gemma cannot be exported using `torch.export.export` due to a user defined type. Fake Tensor is used because the model is very big. A context manager is also used to workaround a bug on safetensor under fake mode  Script:   Error is as follows:  Looking [HF code, `BatchEncoding` is a custom dict:   Versions [pip3] bertpytorch==0.0.1a4 [pip3] clipanytorch==2.5.2 [pip3] CoCapytorch==0.0.12 [pip3] dalle2pytorch==1.14.2 [pip3] emapytorch==0.2.3 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.23.5 [pip3] onnx==1.15.0 [pip3] onnxconvertercommon==1.14.0 [pip3] onnxmltools==1.10.0 [pip3] onnxruntime==1.15.1 [pip3] onnxscript==0.1.0.dev20240118 [pip3] opencliptorch==2.20.0 [pip3] pytorchwarmup==0.1.1 [pip3] rotaryembeddingtorch==0.3.0 [pip3] skl2onnx==1.16.0 [pip3] torch==2.3.0.dev20240118+cpu [pip3] torchfidelity==0.3.0 [pip3] torch_geometric==2.4.0 [pip3] torchaudio==2.2.0.dev20240118+cpu [pip3] torchdata==0.6.1 [pip3] torchmetrics==1.1.2 [pip3] torchtext==0.15.2 [pip3] torchvision==0.18.0.dev20240118+cpu [pip3] triton==2.0.0 [pip3] vectorquantizepytorch==1.7.1 [conda] _anaconda_depends         2023.09             py311_mkl_1 [conda] blas                      1.0                         mkl [conda] mkl                       2023.1.0         h213fc3f_46343 [conda] mklservice               2.4.0           py311h5eee18b_1 [conda] mkl_fft                   1.3.8           py311h5eee18b_0 [conda] mkl_random                1.2.4           py311hdb19cb5_0 [conda] numpy                     1.24.3          py311h08b1b3b_1 [conda] numpybase                1.24.3          py311hf175353_1 [conda] numpydoc                  1.5.0   ",2024-03-20T20:43:35Z,triaged onnx-needs-info oncall: pt2 oncall: export,closed,0,6,https://github.com/pytorch/pytorch/issues/122340,"I think  CC(torch._dynamo.exc.Unsupported: call_function args: UserDefinedObjectVariable(EasyDict)) is similar in that dynamo doesn't handle userdefined dictionaries well.  (but he is on PTO rn) Could you try using nonstrict export? It's a version of export that doesn't go through dynamo, but might result in an unsound graph üòÖ. You can do it by calling `ep = torch.export.export(model, args=(), kwargs=input, strict=False)`","Hey  , tried that and got the same error. Would love more support here :) ","This is because the model is not called correctly. This works ~~~ with WorkAroundSafetensorsBugWithFakeTensor():      model = AutoModelForCausalLM.from_pretrained(""google/gemma7b"")      Run eager     input_ids = tokenizer(input_text, return_tensors=""pt"")     print(model(**input_ids))      Run torch.compile     input_ids = tokenizer(input_text, return_tensors=""pt"")     opt_mod = torch.compile(model, backend=""eager"", fullgraph=True)     print(opt_mod(**input_ids))      Run torch.export     input_ids = tokenizer(input_text, return_tensors=""pt"")     ep = torch.export.export(model, args=(), kwargs=input_ids) ~~~ More specifically, look at the last line `kwargs=input_ids`. Earlier, it was `kwargs=input`. With this, the compile works but export fails later at  ~~~ Traceback (most recent call last):   File ""/home/anijain/local/pytorch2/examples/gemma.py"", line 66, in      ep = torch.export.export(model, args=(), kwargs=input_ids)   File ""/home/anijain/local/pytorch2/torch/export/__init__.py"", line 174, in export     return _export(   File ""/home/anijain/local/pytorch2/torch/export/_trace.py"", line 1071, in wrapper     raise e   File ""/home/anijain/local/pytorch2/torch/export/_trace.py"", line 1043, in wrapper     ep = fn(*args, **kwargs)   File ""/home/anijain/local/pytorch2/torch/export/exported_program.py"", line 100, in wrapper     return fn(*args, **kwargs)   File ""/home/anijain/local/pytorch2/torch/export/_trace.py"", line 2080, in _export     exported_program = ExportedProgram(   File ""/home/anijain/local/pytorch2/torch/export/exported_program.py"", line 683, in __init__     self._validate()   File ""/home/anijain/local/pytorch2/torch/export/exported_program.py"", line 1104, in _validate     v().check(self)   File ""/home/anijain/local/pytorch2/torch/_export/verifier.py"", line 155, in check     self._check_graph_module(ep.graph_module)   File ""/home/anijain/local/pytorch2/torch/_export/verifier.py"", line 222, in _check_graph_module     _check_val(node)   File ""/home/anijain/local/pytorch2/torch/_export/verifier.py"", line 64, in _check_val     raise SpecViolationError(f""Node.meta {node.name} is missing val field."") torch._export.verifier.SpecViolationError: Node.meta _enter_autocast is missing val field. ~~~  for the export error, please take a look or assign someone else. Removing dynamo tags from here.","Thanks for the investigation Animesh! Will take a look into how export treats these kwargs.  is working on the autocast error, which is similar to  CC([export] Failed to trace HF Llama2 model)issuecomment2207303025", I think this error should be handled after CC([export] Convert autocast to HOO) lands.  ,"> This is because the model is not called correctly. This works >  >  >  > More specifically, look at the last line `kwargs=input_ids`. Earlier, it was `kwargs=input`. >  > With this, the compile works but export fails later at >  >  >  >  for the export error, please take a look or assign someone else. Removing dynamo tags from here. Note that we have landed a fix in https://github.com/pytorch/pytorch/pull/132677. "
transformer,Batch size can't be varied if you export self attention to onnx," üêõ Describe the bug I've been making a lot of tickets about differentiating through things lately, but here's a much more basic one: if I export a self attention layer from a transformer to on onnx, without even running any backprop stuff, the onnx appears to be broken if I try to run it with different batch sizes and sequence lengths. Execute this code to reproduce the issue:  Here's the error I get:  I don't know if this works better with the dynamo export, but that's not currently an option for me as I can mostly differentiate through things with torch.onnx.export() now and it's completely unsupported in dynamo.  Versions PyTorch version: 2.2.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: version 3.25.1 Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr 5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: True CUDA runtime version: 10.0.130 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti Nvidia driver version: 536.23 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin\cudnn_ops_train64_8.dll HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=3501 DeviceID=CPU0 Family=107 L2CacheSize=16384 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=3501 Name=AMD Ryzen Threadripper PRO 3975WX 32Cores ProcessorType=3 Revision=12544 Versions of relevant librarie",2024-03-20T16:30:27Z,module: onnx module: windows triaged,open,1,4,https://github.com/pytorch/pytorch/issues/122321,"Update: looks like it's just the batch size I can't vary. Seems to be related to the reshape operations on q, k and v in the multi_head_attention_forward function (torch/nn/functional.py) so I narrowed the test down a bit",Actually I can juggle things around in torch/nn/functional.py and make this work  here are my diffs: ,"Hi maintainers, I am encountering an issue while inferencing with the exported ONNX SegFormer model from MMSegmentation. To export the model to ONNX, I used the `onnx_export` function from MMDeploy (`from mmdeploy.apis.onnx import export as onnx_export`). However, I am facing an error with the following layer: `backbone/layers.0.1.0/attn/attn/Reshape_4`, which uses `nn.MultiheadAttention` from PyTorch. Here is the error log: Error Log   Name: 'main//backbone/layers.0.1.0/attn/attn/Reshape_4'   Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:40 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, onnxruntime::TensorShapeVector&, bool) gsl::narrow_cast(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{187,1,32}, requested shape:{160,1,32}     I have referred to the following code segment in PyTorch: https://github.com/pytorch/pytorch/blob/54f27b886e9dccebb2ebd07300c309bba3b3b30c/torch/nn/functional.pyL6129L6154 and I have added the revision as mentioned in  CC(Batch size can't be varied if you export self attention to onnx)issuecomment2010340209   Prior to writing this issue, I have attempted several methods to resolve it, particularly focusing on exporting the `nn.MultiheadAttention` module: 1. **Scripting the module and then exporting to ONNX** (`torch.jit.script` > `torch.onnx.export`):      Resulted in `RuntimeError: ScalarType UNKNOWN_SCALAR is an unexpected tensor scalar type` 2. **Torch Dynamo Export**:      Resulted in the error `Currently does not support nonfunction and super` Could you please provide guidance on how to resolve this issue or suggest alternative methods for correctly exporting the model? and will your team be considering update the codes for the `nn.MultiheadAttention` module. Thank you for your assistance. ","Relevant Issues: CC(Failing to convert torchaudio.models.Conformer to onnx after scripting of it via torch.jit.script) CC(when convert to onnx with dynamix_axis,  the Reshape op  value is always the same as static,  dynamic_axis is useless, it cant't inference right shape dynamically)"
transformer,RuntimeError: Creating a new Tensor subclass FakeTensor  && AssertionError: Mixing fake modes NYI," üêõ Describe the bug As shown below  Error logs First type:  Second type:   Minified repro ~~~python import torch from transformers import LlamaForCausalLM model = LlamaForCausalLM.from_pretrained('metallama/Llama27b',                                          torch_dtype=torch.float16,                                          use_flash_attention_2 = True,                                          device_map = 'cuda') optim_model = torch.compile(model, dynamic=True, backend='cudagraphs') def test_ktimes(model, times, bs=4, mx=512):     x = torch.randint(0, 32000, (bs, mx), device='cuda')     for _ in range(times):         y = model(x, labels=x)     return None  trigger first type with torch.no_grad():     optim_model.eval()     print(test_ktimes(optim_model, 1, bs=16, mx=512))  trigger second type x = torch.randint(0, 32000, (16, 512), device='cuda') y = optim_model(x, labels=x) ~~~  Versions PyTorch version: 2.1.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.2 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0164genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A40 GPU 1: NVIDIA A40 GPU 2: NVIDIA A40 GPU 3: NVIDIA A40 Nvidia driver version: 525.105.17 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True C",2024-03-20T14:55:20Z,triage review oncall: pt2 module: pt2-dispatcher,closed,0,6,https://github.com/pytorch/pytorch/issues/122314,"note for investigators, you need to have access to https://huggingface.co/metallama/Llama27b","cudagraphs backend does not support dynamic shapes. Use inductor with mode=""reduceoverhead""","I can't run the repro on my box, run out of memory",Another Minified Repo ,This repro is still using cudagraphs backend...,"With the latest repro, when I run it successfully prints None "
peft,some official docker images has nccl error for DDP," üêõ Describe the bug docker pull pytorch/pytorch:2.2.1cuda11.8cudnn8devel or  docker pull pytorch/pytorch:2.1.2cuda11.8cudnn8devel run some training code with deepspeed, use 4 x 8 x A800  (I test the same training code with other docker images, it works well)  error message  [rank21]: [E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or time out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data NCCL version 2.19.3  ncclRemoteERror: A call failed possibly due to a network error or a remote process exting prematurely NET/IB : Got completion from peer 192.168.xxx.xxx with error 5, opcode 0, len 5, vendor err 249 (Recv) localGid  Versions  requirements.txt   install lastest flashattention==2.5.6  pip install flashattn nobuildisolation   host machines  cuda driver version: 525.105.17  cuda version: 12.0  training code  https://github.com/haotianliu/LLaVA/blob/main/scripts/finetune.sh ",2024-03-20T09:55:26Z,oncall: distributed module: nccl module: docker,closed,0,3,https://github.com/pytorch/pytorch/issues/122301, can you please post docker image version where the same code works for you (I.e. I wonder if this could be due to the nccl version difference between the two),">  can you please post docker image version where the same code works for you (I.e. I wonder if this could be due to the nccl version difference between the two) I'm sorry, the ""worked"" docker image before also not work when finetune now, I will check other reasons from hard devices and network, thanks for your help. ","> >  can you please post docker image version where the same code works for you (I.e. I wonder if this could be due to the nccl version difference between the two) >  > I'm sorry, the ""worked"" docker image before also not work when finetune now, I will check other reasons from hard devices and network, thanks for your help. hello, I met the same question as your issue, have you found out the root reason finally?"
gpt,DISABLED test_fake_tensor_mode_huggingface_gpt2 (__main__.TORCH_NN_MODULE),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_fake_tensor_mode_huggingface_gpt2` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `onnx/test_fx_to_onnx_with_onnxruntime/TestFxToOnnxFakeTensorWithOnnxRuntime_op_level_debug_True_dynamic_shapes_True_load_checkpoint_during_init_False_export_within_fake_mode_True_model_type_TorchModelType.py` or `onnx/test_fx_to_onnx_with_onnxruntime` ",2024-03-20T03:40:45Z,module: onnx triaged module: flaky-tests skipped,open,0,0,https://github.com/pytorch/pytorch/issues/122276
transformer,Handle transposes in second batch of matrices in bmm,1. Add support for Unranked placeholders in the MPS backend.  2. PR is now fusing the Transposes into the GEMM kernel dispatches in MPS backend. This improves the performance of Transformer networks by 58%.,2024-03-19T14:25:26Z,triaged open source Merged Reverted release notes: mps ciflow/mps,closed,1,17,https://github.com/pytorch/pytorch/issues/122194,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: malfet / name: Nikita Shulga  (eabaecc4d8674ae89c22e1228df836eedb88f0dc, 2e79d284b15ab191947c89b8b0dafbcc2e56211e, 0e0f0bddb1f813469dcaa1dc1ec4f06a75a2293c, 6d0a6fe44a4794c2ed50814073f77b2e9fc5d368):white_check_mark: login: kulinseth / name: Kulin Seth  (4497328e50ba182740cf557ac704c4947c4f53cc, 1c1c2fdf2a9c69b76e9221c994fb1574159a2ff2)"," merge f ""All MPS tests are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",">  merge f ""All MPS tests are green"" Please be more careful next time, as this PR broke lint."," revert m ""Broke lint"" c ignoredsignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `dev/joona/bmm_fusion` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout dev/joona/bmm_fusion && git pull rebase`)","> Can you please provide a bit more detailed description of what this PR is trying to do. Is it fixing correctness issue or a performance one? This improves Batched matrix multiplications where the 2nd matrix is transposed. E.g consider following example:   With current code, a gather would be performed to make following `y.permute(0, 2, 1)` contiguous, then a batched matrix multiplication would be performed (where the matmul is performed `row @ column`).  With this addition,  we detect if the 2nd matrix was transposed, and if this was the case, we will not do gather anymore to make the 2nd matrix contiguous, but rather do directly a matrix multiplication where 2nd matrix is marked as transposed in MPSGraph. The big speedup comes that instead of doing a `row @ column` matmul, now it's a `row @ row` matmul (the big speedup in `row @ row` comes from the fact that the access to the 2nd matrix is not strided anymore (in trying to access the column), but rather contiguous)  this speedup will be more obvious when doing big matrix multiplications where the stride between column elements is bigger.", rebase , started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `dev/joona/bmm_fusion` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout dev/joona/bmm_fusion && git pull rebase`)"," merge f ""Lint and MPS are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",> PR is now fusing the Transposes into the GEMM kernel dispatches in MPS backend. This improves the performance of Transformer networks by 58%.  do you mind sharing a link to a benchmark that you've run?  I've used https://github.com/pytorchlabs/gptfast and alas didn't notice any perf gains before and after the change
transformer,[dort][dynamo_export] Wrong model for phi backward graph," üêõ Describe the bug The script ends with a failure in onnxruntime: ``onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: getitem_1 for the following indices``. It happens in the backward graph.   Versions Collecting environment information... PyTorch version: 2.3.0.dev20240314+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.26.4 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.10.102.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1060 Nvidia driver version: 535.98 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):    ",2024-03-19T09:27:32Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/122181
llm,[dynamo] Raise accumulated cache size limit,"  CC([dynamo] Raise accumulated cache size limit)  CC([dynamo][model_output] Do not include none for CustomizedDictVariable)  CC([dynamo][guards] Move backend match to eval_frame) Fixes CC(Raise `torch._dynamo.config.accumulated_cache_size_limit` higher, or potentially just remove it altogether.)  This was raised by IBM folks where the a LLM compile was failing because it had more than 64 layers. ",2024-03-18T21:04:05Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,2,3,https://github.com/pytorch/pytorch/issues/122130,I wonder whether we can entirely remove this guard., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Fakifying script object by wrapping python,  CC(Fakifying script object by wrapping python)  CC(Support method calls of torch script object)  CC([export] assert inline export graph module for torchbind object)  CC(Add torch._library.impl_abstract_class to fakify torchBind class),2024-03-18T16:34:31Z,release notes: jit ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/122105
yi,Error while trying to use model.compile() on WSL (ubuntu)," üêõ Describe the bug Getting a BackendCompilerFailed error.  Error logs BackendCompilerFailed: backend='inductor' raised: CalledProcessError: Command '['/bin/gcc', '/tmp/tmp5p0z5hzv/main.c', 'O3', 'I/home/denisw/.local/lib/python3.10/sitepackages/triton/common/../third_party/cuda/include', 'I/usr/include/python3.10', 'I/tmp/tmp5p0z5hzv', 'shared', 'fPIC', 'lcuda', 'o', '/tmp/tmp5p0z5hzv/triton_.cpython310x86_64linuxgnu.so', 'L/usr/lib/wsl/lib']' returned nonzero exit status 1.  Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.146.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2060 Nvidia driver version: 551.76 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             16 Online CPU(s) list:                015 Vendor ID:                          GenuineIntel Model name:       ",2024-03-18T13:26:21Z,needs reproduction triaged module: wsl oncall: pt2,closed,0,4,https://github.com/pytorch/pytorch/issues/122091,Please provide a repro, Do we know if there are known issues of torch.compile (inductor) on WSL?,"Hi 2, we are scrubbing old issues. If this issue is still relevant, could you please provide a repro?",Close since no repro. Feel free to reopen if needed.
finetuning,Is it realistic to use C++ and libtorch to train models on aarch64 devices using CPUs?," üöÄ The feature, motivation and pitch For example, if I freeze the network layer of Yolo and only train the fully connected layer for finetuning, there is only a small amount of data per training, is it realistic? I means in the edge device with low computing power, the CPU is used to train slowly to achieve a closed data loop....?  Alternatives _No response_  Additional context _No response_",2024-03-18T08:13:32Z,oncall: mobile,closed,0,1,https://github.com/pytorch/pytorch/issues/122082,I would imagine it might be tricky without Python runtime on device using existing PyTorch training flow on device.
rag,[Feature Request] add a new argument `storage_offset=None` to `empty_strided`," üöÄ The feature, motivation and pitch  Motivation Users use `empty_strided` to create a strided tensor.  If users want to create a strided tensor with storage offset, since `empty_strided` doesn't have argument `storage_offset` they have to use `empty(...).as_strided(..., storage_offset=xxx)`, which is inconvient and slower. This would also align the semantics of `empty_strided()` and `empty().as_strided()`.  Proposal The new empty_strided would looked like:   Alternatives _No response_  Additional context _No response_ ",2024-03-18T07:59:24Z,triaged module: python frontend,open,0,2,https://github.com/pytorch/pytorch/issues/122081,"Hello, would you be able to share about your usecase for this functionality please?","I am trying to create an empty tensor with specified strides, size, storage_offset and storage_size. And then benchmark performances of ops on this strided tensor. I originally intended to use  `torch.empty_strided(size=[512, 2048], stride=[1, 512], storage_offset=128, storage_size=1048576)`  but empty_strided lacks `storage_offset`.  So I use  `torch.empty(1048756).as_strided(size=[512, 2048], stride=[1, 512], storage_offset=128, storage_size=1048576)`  instead. It could be more convient to make empty_strided also support `storage_offset` and `storage_size`.  And also align the semantics of `empty_strided()` and `empty().as_strided()`.  "
transformer,"NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":830, please report a bug to PyTorch. "," üêõ Describe the bug **Python sample code:**  **error message:** > RuntimeError                              Traceback (most recent call last) >  > Cell In[1], line 4 >       1 from ultralytics.data.annotator import auto_annotate > > 4 auto_annotate( >       5 data='/app/input/signal/images', >       6 det_model='/app/runs/detect/train/weights/best.pt', >       7 sam_model='mobile_sam.pt', >       8 device=""cuda"", >       9 output_dir='/app/input/signal/labels', >      10 ) >  > File /usr/local/lib/python3.10/distpackages/ultralytics/data/annotator.py:41, in auto_annotate(data, det_model, sam_model, device, output_dir) >      39 if len(class_ids): >      40     boxes = result.boxes.xyxy   Boxes object for bbox outputs > > 41     sam_results = sam_model(result.orig_img, bboxes=boxes, verbose=False, save=False, device=device) >      42     segments = sam_results[0].masks.xyn   noqa >      44     with open(f'{str(Path(output_dir) / Path(result.path).stem)}.txt', 'w') as f: >  > File /usr/local/lib/python3.10/distpackages/ultralytics/models/sam/model.py:35, in SAM.__call__(self, source, stream, bboxes, points, labels, **kwargs) >      33 def __call__(self, source=None, stream=False, bboxes=None, points=None, labels=None, **kwargs): >      34     """"""Calls the 'predict' function with given arguments to perform object detection."""""" > > 35     return self.predict(source, stream, bboxes, points, labels, **kwargs) >  > File /usr/local/lib/python3.10/distpackages/ultralytics/models/sam/model.py:31, in SAM.predict(self, source, stream, bboxes, points, labels, **kwargs) >      29 kwargs.update(overrides) >      30 prompts = dict(bboxes=bboxes, points=",2024-03-18T02:24:12Z,module: cuda triaged module: CUDACachingAllocator,open,0,3,https://github.com/pytorch/pytorch/issues/122068,same issues 2xA5000 nvcc V: 12.1 torch                         2.2.2 Python 3.10.0,same issue with latest released pytorch,"Same Problem with:  NVIDIA A100 80GB NVIDIASMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4   torch: '2.3.0+cu121' pyhton: 3.11.6 Basically I had several notebooks opened with sam model loaded in them, so different kernels for each one in the same GPU.  I easilly solved the issue by simply opening and using one at a time. "
chat,[Incorrect Result] torch.atan(inf) returns NaN value on dtype complex128.," üêõ Describe the bug `torch.atan(inf)` returns NaN when dtype is complex128, but pi/2 on other dtypes such as float64. Please check the code below.  Also, tensorflow and numpy will give pi/2 in inf input, for corresponding `arctan` functions.  Versions PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: AlmaLinux 9.3 (Shamrock Pampas Cat) (x86_64) GCC version: (GCC) 11.4.1 20230605 (Red Hat 11.4.12) Clang version: 16.0.1 (https://github.com/llvm/llvmproject.git cd89023f797900e4492da58b7bed36f702120011) CMake version: version 3.23.2 Libc version: glibc2.34 Python version: 3.9.18 (main, Jan  4 2024, 00:00:00)  [GCC 11.4.1 20230605 (Red Hat 11.4.12)] (64bit runtime) Python platform: Linux5.14.0362.18.1.el9_3.x86_64x86_64withglibc2.34 Is CUDA available: True CUDA runtime version: 10.1.243 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 2080 Ti GPU 1: NVIDIA TITAN RTX GPU 2: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 550.54.14 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             36 Online CPU(s) list:                035 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Core(TM) i910980XE CPU @ 3.00GHz CPU family:                         6 Model:                              85 Th",2024-03-17T03:40:22Z,triaged module: complex,closed,0,2,https://github.com/pytorch/pytorch/issues/122042,jeancho please help fix this one!,It seems that this issue has been fixed on `torch 2.4.1`. Thanks for the effort! 
rag,Simplify Storage meta conversion with PyObject preservation,"  CC(Factor meta conversion through serializable MetaTensorDesc)  CC(Simplify Storage meta conversion with PyObject preservation) Thanks to https://github.com/pytorch/pytorch/pull/109039 we can rely on finalizers on Storage PyObject to handle removal from dict. Irritatingly, we still have to attach finalizer, because we don't have a weak key AND value dict (only one or the other). Signedoffby: Edward Z. Yang ",2024-03-16T04:13:34Z,Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/122018, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[MPS] Regression from macOS 14.3 to 14.4 in PyTorch 2.2.0/2.2.1," üêõ Describe the bug I've been using a pytorch model daily through transformers (https://github.com/khawhite/mangaocr) and MPS. Everything was fine with latest PyTorch and latest Transformers until the Sonoma 14.4 update which made it start crashing on startup (I was running 14.3 before and am on a M1 Max mac, for reference). Since CPU mode worked fine, I also tried older versions of PyTorch and I found anything before 2.2.0 worked fine. Bisecting it I found this commit is causing it: https://github.com/pytorch/pytorch/commit/056d6247c786f4391bc6a6f21337b009c7e9699e . I tried building latest main with that commit reverted and it's working again. I apologize but I've not been able to pinpoint this further or to write a standalone example (since I don't really know what I'm doing here). I've looked at what's happening from within transformers but not sure any of this can be of help: It seems in https://github.com/huggingface/transformers/blob/main/src/transformers/generation/beam_search.pyL318 in input_ids the first member of each tensor array differs with or without that commit: e.g. With: tensor([[3463342888,          2,       2312,       4080,       5063,       4885,               3600,       5764,       5554,       4798,       1074,       3386,               2865,       2166,       1524,       1858,       2872,       4014,               5250,       1225,       2119,       1104,       1682,       5730,               1482,       5578,         78],        [3463342888,          2,       2312,       4080,       5063,       4885,               3600,       5764,       5554,       4798,       1074,       3386,               2865,       2166,   ",2024-03-16T01:30:50Z,high priority triaged module: regression module: correctness (silent) module: mps,closed,0,7,https://github.com/pytorch/pytorch/issues/122016,"Sorry, can you please be a bit more specific of what do you mean by ""crashes at the startup""? I'm on 14.4 and everything seems to be working fine at the first glance, i.e.:  Hmm, though indeed `from manga_ocr import MangaOcr` seems to work on 14.3, but fails on 14.4","Ok, I think I have a minimal reproducer of the problem ","And following fixes the problem, which makes me wonder how this ever worked before: "," if you don't mind explaining, what does copy_and_sync and non_blocking do vs copy? Just for learning thanks! found this: https://github.com/pytorch/pytorch/blob/5030913d6aed153afe45c9d9bcb6145093eb5629/aten/src/ATen/mps/MPSStream.mmL193L207"," Looking for a good vendorneutral document on the subject but could not find one. In general, nonblocking operations, as name suggests are nonblocking, i.e. operation might still be ongoing when one returns to a function calls (see `glFlush` API description dating back to 1991 ) In practice, imagine one is running the following code:  vs  I.e. by default GPU operations are executed asynchronously( so timing it on CPU doesn't make much sense, as it will only memory time required to reserve memory and enqueue respective commands)","Thanks for the breakdown  !  Do you have an idea of why   previously had the result of [0, 3, 3]? Seem like in any asynchronous scenario, there would be at most one 3. How'd it get two 3s? ",validated with 2.3.0 
transformer,[DCP][FSDP2][Test] Add_adamW to test_train_parity_2d_transformer_checkpoint_resume,"Want to add the option of AdamW here, as currently this is the only test for 2D.  ",2024-03-15T22:55:33Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/periodic,closed,0,8,https://github.com/pytorch/pytorch/issues/122002, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxjammypy3.8gcc11 / build  .github/workflows/pull.yml / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 5, 5, linux.g5.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `add_adamW_to_test_train_parity_2d_transformer_checkpoint_resume` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout add_adamW_to_test_train_parity_2d_transformer_checkpoint_resume && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[ONNX] ONNX model has mismatching data type when Torch model and checkpoint dtype mismatches," üêõ Describe the bug The HF's Whisper model has checkpoints saved with `torch.float16`, but the user script casts it to `torch.float32`. After the export finishes, the checkpoint is used to cerate ONNX Weights, which will result in a model expecting `float32` while the weights are stored with `float16`  The checker error would be something like `onnx.onnx_cpp2py_export.shape_inference.InferenceError: [TypeInferenceError] Inferred elem type differs from existing elem type: (FLOAT16) vs (FLOAT)`  Versions main",2024-03-15T19:46:41Z,module: onnx triaged onnx-triaged release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/121986
llm,Please provide `total_repeat_length` field to PyTorch `repeat` / `repeat_interleave` API," üöÄ The feature, motivation and pitch Please provide `total_repeat_length` field to the pytorch `repeat` / `repeat_interleave` API. This feature is available on `jnp.repeat` and has valuable use cases for sparse LLMs applications. ",2024-03-14T21:04:28Z,feature triaged needs research module: tensor creation,open,0,1,https://github.com/pytorch/pytorch/issues/121933,"Since you posted the same feature request for NumPy, let me crosslink that: https://github.com/numpy/numpy/issues/26028. The answer for PyTorch is different I think: torch.repeat_interleave already has an `output_size` keyword (in addition to the other arguments which match the numpy ones). `output_size` does the same thing as `total_repeat_length` in `jnp.repeat` and looks like a nicer name, so it looks to me like there is nothing to improve there. torch.Tensor.repeat is a different function, it is like jnp.tile which doesn't have the `total_repeat_length` keyword."
dspy,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/builds/pytorch/c10/cuda/CUDACachingAllocator.cpp"":830, please report a bug to PyTorch. "," üêõ Describe the bug RuntimeError Traceback (most recent call last) Cell In[9], line 3 1 size = 30000 2 a = torch.randn(size, size, device=device) > 3 b = torch.randn(size, size, device=device) 5  Perform matrix multiplication on GPU 6 start_time = torch.cuda.Event(enable_timing=True) RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/builds/pytorch/c10/cuda/CUDACachingAllocator.cpp"":830, please report a bug to PyTorch. import torch import subprocess import time size = 30000 a = torch.randn(size, size, device=device) b = torch.randn(size, size, device=device) start_time = torch.cuda.Event(enable_timing=True) end_time = torch.cuda.Event(enable_timing=True) start_time.record() c = torch.matmul(a, b) end_time.record() torch.cuda.synchronize() elapsed_time_ms = start_time.elapsed_time(end_time) print(f""Matrix multiplication took: {elapsed_time_ms} milliseconds"")  Versions Collecting environment information... PyTorch version: 2.2.0a0+gitc3918c1 Is debug build: False CUDA used to build PyTorch: 12.2 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.6 Libc version: glibc2.35 Python version: 3.10.13  (main, Oct 26 2023, 18:07:37) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.15.091genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB   MIG 1g.10gb     Device  0: Nvidia driver version: 535.86.10 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.",2024-03-14T10:51:59Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/121899
rag,use make_storage_impl to create storages for COWStorage.,"Thanks to https://github.com/pytorch/pytorch/pull/118459Ôºå `make_storage_impl` will use the func ,which register for other backends, to create StorageImpl. `make_storage_impl` completely overwrites the `make_intrusive`, so it makes sense to replace  `make_intrusive` with `make_storage_impl` to create storage in cow.",2024-03-14T08:23:14Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,14,https://github.com/pytorch/pytorch/issues/121896, Why does the make function take a dtype? Storages don't have dtype anymore after untyped storage,">  Why does the make function take a dtype? Storages don't have dtype anymore after untyped storage I know storage will not have dtype anymore. But, in this pr, which added is `deivce` not `dtype`."," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  BC Lint  pull  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnerclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxdocs / builddocspythonfalse Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llava,[inductor][cpu] llava accuracy and performance crash," üêõ Describe the bug loading model: 0it [00:00, ?it/s] loading model: 0it [00:00, ?it/s] cpu  eval  llava                               Traceback (most recent call last):   File ""/workspace/pytorch/benchmarks/dynamo/common.py"", line 3836, in run     ) = runner.load_model(   File ""benchmarks/dynamo/torchbench.py"", line 255, in load_model     benchmark = benchmark_cls(   File ""/workspace/benchmark/torchbenchmark/util/model.py"", line 24, in __call__     obj = type.__call__(cls, *args, **kwargs)   File ""/workspace/benchmark/torchbenchmark/models/llava/__init__.py"", line 11, in __init__     super().__init__(name=""llava"", test=test, device=device, batch_size=batch_size, extra_args=extra_args)   File ""/workspace/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py"", line 98, in __init__     config = eval(class_models[name][2])   File """", line 1, in    File ""/opt/conda/lib/python3.8/sitepackages/transformers/models/auto/configuration_auto.py"", line 957, in from_pretrained     config_class = CONFIG_MAPPING[config_dict[""model_type""]]   File ""/opt/conda/lib/python3.8/sitepackages/transformers/models/auto/configuration_auto.py"", line 671, in __getitem__     raise KeyError(key) KeyError: 'llava' eager_fail_to_run  Versions SW info               name       target_branch       target_commit       refer_branch       refer_commit                       torchbench       main       1ef0a39e       main       ff42d907                 torch       main       f11f2b0d55b1aa322f73f4bb521beaf9d4563603       main       581fe26792849a80f04feaa7f8bdec69b7f41dd8                 torchvision       main       0.18.0a0+2c127da       main       0.18.0a0+2c12",2024-03-14T06:45:43Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/121888," it looks like a transformer version incompatible issue, let's try to use the pined transformers version in  https://github.com/pytorch/pytorch/blob/main/.ci/docker/ci_commit_pins/huggingface.txt again",The issue is fixed with pined transformers
llama,[inductor][cpu]llama_v2_7b_16h AMP dynamic shape single thread performance test will be killed ," üêõ Describe the bug https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c /workspace/pytorch bash inductor_single_run.sh single inference performance torchbench llama_v2_7b_16h amp first dynamic Testing with dynamic shapes. Testing with freezing on. singlethread testing.... /opt/conda/lib/python3.8/sitepackages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.   _torch_pytree._register_pytree_node( loading model: 0it [00:40, ?it/s] cpu  eval  llama_v2_7b_16h skipping cudagraphs due to skipping cudagraphs due to cpu device. Found from :    File ""benchmarks/dynamo/torchbench.py"", line 354, in forward_pass     return mod(*inputs)   File ""/workspace/pytorch/torch/nn/modules/module.py"", line 1536, in _call_impl     return forward_call(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/transformers/models/llama/modeling_llama.py"", line 1174, in forward     outputs = self.model(   File ""/workspace/pytorch/torch/nn/modules/module.py"", line 1536, in _call_impl     return forward_call(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/transformers/models/llama/modeling_llama.py"", line 1011, in forward     inputs_embeds = self.embed_tokens(input_ids) running benchmark:  78% 39/50 08:13SW info               name       target_branch       target_commit       refer_branch       refer_commit                       torchbench       main       1ef0a39e       main       ff42d907                 torch       main       41286f1505ffb214d386d72e4b72ebd680a4a475       main       581fe26792849a80",2024-03-14T05:57:50Z,triaged oncall: pt2 module: inductor oncall: cpu inductor,closed,0,13,https://github.com/pytorch/pytorch/issues/121887,May we take a look first between the difference of generated code before and after the regression ?,"For this issue, I observed memory leak after this commit https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c. Before this commit, the memory usage is !image After this commit, the memory usage is: !image I am using this script to reproduce inductor_single_run.sh My approach to collect memory usage is:   After collected memory logs, plot the log with matplotlib ","  thanks for flagging, will take a look. I wouldn't expect any codegen changes caused by https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c without `torch.cond` in the model, but will need to verify to be sure.","  I've run the repro on https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c and I don't see any memory leak. Here's the plot from `mprof`: !llamamemwithjeafterregression It stabilizes at ~50MB, like in your case, and fluctuates around there. Here's the log (I couldn't get `libjemalloc.so` working in my conda env without segfaulting: is `libjemalloc.so` crucial for the memory leak repro?):  Also, my understanding is that the torchbench run above runs `llama_v2_7b_16h` with Python wrapper, just CPU kernels instead of Triton kernels? Looking into the code modified / introduced by https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c, I can't immediately see how it could lead to memory leak in this setup, as most of the changes are related to compilation process and affect C++ wrapper (or the Python wrapper run before generating C++ wrapper). Any suggestions on how to proceed from here? Thanks!","  To add to the above, I've captured the generated Python wrapper code (with inscribed CPU kernels in C++) from the benchmark command above, separately on https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c and the parent commit https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418. The code looks identical (except a few `_frozen_param` pointers in the comments). The codegen from https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c: https://gist.github.com/aakhundov/418a22a4f3a2584adde80b918348b2ab The codegen from https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418: https://gist.github.com/aakhundov/8427ae13be0ffe6906e4838ecf80adfc This code is what actually gets executed during benchmarking. Given that it's the same, should we expect different memory behavior between the two commits?",> May we take a look first between the difference of generated code before and after the regression fangintel please see the comment above for this. There doesn't seem to be any difference except the comments.,"Hi, . Thanks for looking this  This log indicate you do not have jemalloc, a memory allocator used in CPU benchmark. You can install it by "," I've installed `jemalloc` as you suggested and tried running your (modified) inductor_single_run.sh again. Now I'm getting this:  I think, the `mprof run` command crashes before it can generate any output in `/tmp/inductor_single_test_st.csv`. This must be related to the newlyinstalled `jemalloc`, as without it I could run the script and get the results (reported above). Any idea on what may be happening here? Also, in  CC([inductor][cpu]llama_v2_7b_16h AMP dynamic shape single thread performance test will be killed )issuecomment2000782330, I've collected the generated code from https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c and the previous commit https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418, and those seem identical. Could you confirm you also see this (you can set env var TORCH_COMPILE_DEBUG=1 to dump the generated code)?","Hi, . > I think, the mprof run command crashes before it can generate any output in /tmp/inductor_single_test_st.csv. Yes and this may because the memory usage is too large to finish your process. However, you should still be able to get the memory log in `./llamamemwithjebeforeregression.log` and you can plot it. > Also, in  CC([inductor][cpu]llama_v2_7b_16h AMP dynamic shape single thread performance test will be killed )issuecomment2000782330, I've collected the generated code from https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c and the previous commit https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418, and those seem identical. Could you confirm you also see this. Yes, I also do not find difference between the output_code.py when I am trying to understand the memory leak I have observed. > It stabilizes at ~50MB The unit should be ~50MiB = 50 * 1000 MB, so it requires about 220G to finish the process with jemalloc","Hi . Re this: > Yes and this may because the memory usage is too large to finish your process. However, you should still be able to get the memory log in ./llamamemwithjebeforeregression.log and you can plot it. The script run crashes immediately, not after some time. So I don't think it's caused by too large memory usage, tbh. Also, given that the source code is the same, how can the changes in https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c lead to different memory behavior?","Hi, .  > Also, given that the source code is the same, how can the changes in https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c lead to different memory behavior? I do not have ideas on it. Here is an example about `generated code do not changed but introduced 4us overhead per iter`  CC([inductor][cpu]lennard_jones, pyhpc_equation_of_state and pyhpc_isoneutral_mixing performance regression ).  > The script run crashes immediately, not after some time. So I don't think it's caused by too large memory usage. I do not know this crash caused by jemalloc + runllamabenchamark or jemalloc + mprof + runllamabenchmark. However, I see you are able to run without jemalloc. You can change below code to disable jemalloc or use some other tools to monitor the memory usage.  Without jemalloc, we can still see memory usage diffs dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,**dynamo_peak_mem**,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles cpu,llama_v2_7b_16h,1,1.805067,4904.393957,0.797978,0.681097,2889.488794,**4242.402918**,1076,1,0,0,0,0   > after regression cpu,llama_v2_7b_16h,1,1.870527,4785.474157,0.685189,0.807869,2883.138765,**3568.821862**,1076,1,0,0,0,0   > before regression So I guess somehow your PR **introduced memory allocate for each iters** and these memory cannot be correctly released with jemalloc. Do you have any insights about it?"," I took another deeper look into the memory regression. As you've suggested, I've disabled `jemalloc` in the testing script and could, indeed, repro different memory behavior on https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c and https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418. Before regression (on https://github.com/pytorch/pytorch/commit/26740f853e10e34a7e8d2b72b950875796350418): !image After regression (on https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c): !image The main difference is that, before regression, at around 240 seconds, ~6MB of memory is released; whereas after regression those 6MB stay allocated (so the line at 240 seconds is flat). Other than that, memory profiles are basically the same. Also, I've limited the runtime above to 10 iterations, as the memory doesn't change after that. The point at 240 seconds corresponds to the end of the model compilation, which happens only once during the lifetime of the benchmarking before the first iteration. Taking a further look into https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c, I had a hunch that `V.graph` being unconditionally added and kept within `wrapper.codegened_graph_stack` may lead to the object sticking around and not being freed. So I've introduced a small refactoring to explicitly pop the parent graph from the stack in https://github.com/pytorch/pytorch/pull/122469. When this change is applied on top of https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c, I see the following memory profile: !image The above is similar to the ""before regression"" profile above, so it seems that the ~6MB of memory are freed now as before https://github.com/pytorch/pytorch/commit/3d089de851836da8b4b21495a0e1d79e05c9282c. Could you please check if https://github.com/pytorch/pytorch/pull/122469 helps with the memory regression in the `jemalloc` setup, too? Thanks!","Hi, . Thanks for looking into this, I have checked https://github.com/pytorch/pytorch/pull/122469 with `jemalloc enabled`, the memory regression is gone. !image"
gpt,Add gpt-fast as a static benchmark,Run:  It generated a cvs file  with the content like: ,2024-03-14T05:51:09Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/121886, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llava,"UNSTABLE pull / test-models-linux (cmake, llava, portable, linux.2xlarge, 90) / linux-job",> Please provide a brief reason on why you need to mark this job as unstable. Consistently fail on CI and blocks stable branch moving forward.  /pytorchdevinfra,2024-03-14T02:29:35Z,module: ci unstable,closed,0,1,https://github.com/pytorch/pytorch/issues/121884,Oh it's still submitted to pytorch instead of executorch. I thought it was enabled.
mixtral,Training fails on torch.compile + device_map.," üêõ Describe the bug I'm trying to train Mixtral MoE Model using transformers, and I'm using the device_map flag which distributes the model layers across the GPUs.  I get this error: `  File ""/home/aiscuser/transformers/src/transformers/models/mixtral/modeling_mixtral.py"", line 1171, in forward     inputs_embeds = self.embed_tokens(input_ids)   File ""/opt/conda/envs/ptca/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1527, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/opt/conda/envs/ptca/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1536, in _call_impl     return forward_call(*args, **kwargs)   File ""/opt/conda/envs/ptca/lib/python3.8/sitepackages/torch/nn/modules/sparse.py"", line 163, in forward     return F.embedding(   File ""/opt/conda/envs/ptca/lib/python3.8/sitepackages/torch/nn/functional.py"", line 2264, in embedding     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)`  Versions nightly pyotch nightly transformers ",2024-03-14T00:35:48Z,triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/121873,"This looks like more of a problem in transformers. I would expect the library to dispatch different calls per device in that schemethus, you may want to open an issue in transformers instead.","question, does torch inductor support pipeline parallel?","No, `torch.compile` has limited support with distributed training. We are working on making it better. We don't support pipeline parallelism today OOTB. ."
gpt,GPT2 SDPA inference pattern-matching for Inductor-CPU," Summary With this PR, SDPA pattern of GPT2 is being mapped to `torch.nn.functional.scaled_dot_product_attention`. While GPT2 supports both a causal mask & an attention mask, this PR considers the case of attention mask being absent. TorchBench inference workload for GPT2 also doesn't use an attentionmask. This pattern's replacement is being disabled for CUDA because CUDA AOT Inductor CI job's `GPT2ForSequenceClassification` accuracy test failed, although all other trunk CUDA Inductor CI checks had passed. Created CC(GPT2ForSequenceClassification accuracy test fails for CUDA AOT Inductor export when its SDPA pattern is mapped to the SDPA op) to track that particular issue.  CPU performance data with TorchBench  Machine  Intel(R) Xeon(R) Platinum 8468H (Xeon 4th gen Sapphire Rapids) 48 physical cores were used. Intel OpenMP & libtcmalloc were preloaded. Example command   ",2024-03-13T23:27:30Z,triaged open source Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,1,5,https://github.com/pytorch/pytorch/issues/121866,"Hi , the CI failure is unrelated, as it's a MacOS failure for `test/test_ops_fwd_gradients.py::TestFwdGradientsCPU::test_fn_fwgrad_bwgrad_nn_functional_conv_transpose3d_cpu_complex128`. I disabled the pattern replacement for CUDA & created a new GitHub issue ( CC(GPT2ForSequenceClassification accuracy test fails for CUDA AOT Inductor export when its SDPA pattern is mapped to the SDPA op)) to track `GPT2ForSequenceClassification`'s accuracy failure for the CUDA AOT Inductor export CI job. Thanks!", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,AdamW(fused=True) slower than unfused AdamW," üêõ Describe the bug 512M parameters Mostly vanilla LM transformer. FlashAttention 2.4.2, PyTorch 2.2.0. Uses both FA and FlashRotary. Dtype: bf16 Nvidia A40. singleGPU Unfused: 85 TFLOPS Fused: 68 TFLOPS  Versions PyTorch version: 2.2.0 Is debug build: False CUDA used to build PyTorch: 12.2 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.28.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.19.17coreweavex86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A40 Nvidia driver version: 525.147.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.6 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          96 Online CPU(s) list:             095 Vendor ID:                       AuthenticAMD Model name:                      AMD EPYC 7413 24Core Proce",2024-03-13T22:09:35Z,module: performance module: optimizer triaged,open,0,21,https://github.com/pytorch/pytorch/issues/121857,"hmm this is concerning. Do you mind sharing a description of the sizes of your parameters? and whether you were using AMP/needing a gradscaler? I would guess no since you‚Äôre using bf16 and not fp16, but wanted to rule out possible culprits","n_layer: 12 n_head: 12 kv_heads: 6 (GQA) hidden_dim: 1536 n_tokens: 2048 (context length) vocab_dim: 65536 activation: ""swiglu"" No AMP/gradscaler. If a profile would help, I can produce one.",a profile would be helpful yes!,"These `trace.json` files were gigantic (multiGB), so here's smaller versions without stack information and on two steps only: https://drive.google.com/file/d/1c2CQST_U_Qf6O1qgSXr0DKVorytoNgp5/view?usp=sharing https://drive.google.com/file/d/1NCi6frLbXfVhL0pzdmwoRzmYLtFKqeQ/view?usp=sharing Torch.compile mode is the default (not ""reduceoverhead"") and my TFLOPS are constant starting from the second step, so skipping just 1 step of warmup seems ok. The program is being launched by `torchrun` on a 1GPU machine.",Turns out the step count isn't an issue with stack information disabled. Here's 3 steps: Unfused: https://drive.google.com/file/d/1OKuJVC3PPK5vn6TWdi1vPD8bAoHP7h4/view?usp=sharing Fused: https://drive.google.com/file/d/1CUeKkwiOMEHRJaNZlC65utq6WuS7Fxh4/view?usp=sharing,"Oh! Wait a moment...are you comparing torch.compile(step) vs step of AdamW(fused=True)? torch.compile(step) does the vertical fusion by default so it being faster is reasonable (and the point haha)! If you're comparing eager (non compiled) step between (fused=True) and the default (), then the fused should be faster.","No: torch.compile is wrapping forward and loss, but backward and AdamW are outside the compile. So it's: torch.compile(forward+loss but not backward+AdamW) + unfused AdamW vs  torch.compile(forward+loss but not backward+AdamW) + fused AdamW ","Ah, thanks for that, I was looking for the optimizer stuff in the torch compiled region and getting confused. I did look at both the traces and you're right the fused kernel is slower. In detail: The default AdamW uses a series of foreach ops, and the part in the trace this corresponds to is:  Whereas the fused AdamW is a foreach_add (step update) followed by one single fused kernel, and the part in the trace this corresponds to is:  : 1. While the fused implementation correctly launches fewer kernels (one fused_adamw instead of a bunch of foreach_* kernels), each kernel is so much slower! Each fused_adamw kernel takes ~6.5ms whereas a single foreach_* kernel is only 0.2ms (30 times slower!) Why is the fused op so much slower? 2. Whereas there are 137 kernels launched per foreach op, there are 75 kernels launched for the fused (due to the multi_tensor_apply chunking logic). For the same number of tensor inputs, I'm surprised there are _more_ kernels launched in the smaller foreach ops. Do you know what accounts for this?","pytorch v2.2.0 doesn't seem to have CC(fused adam(w): Reduce register usage), 2.2.1, either by looking at https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/cuda/fused_adam_utils.cuh. So I guess a nightly would be worth trying",Instructions for installing nightly can be found in the toggle system of https://pytorch.org/,"My CUDA version is 12.2, and PyTorch only lists 12.1 in its nightlies. Is that still ok for me to install?","Yes, I think so","It broke when trying to run with PyTorch Nightly, mismatched CUDA versions.  Debugging this isn't your issue though. Updating from CW's stable to CoreWeave's nightly images a few days ago (http://ghcr.io/coreweave/mlcontainers/nightlytorchextras:999655bnccl2024.03.11.05cuda12.2.2ubuntu22.04nccl2.19.31torch2.3.0a0vision0.18.0a0audio2.2.0a0flash_attn2.4.2) produced NaNs in backward, but that also isn't your issue. (I also can't switch my container easily.) It seems I don't have a good way to install PyTorch nightly on my current system.","Here's PyTorch nightly profiles (CUDA 12.2, 2.3.0a0+3eb322f). torch.compile broke, so I turned it off for forward+loss. Performance is much closer; unfused is only a little bit faster than fused. https://drive.google.com/file/d/1k8zoSGeK7Pr5jst_MU4u6HJS6lhikUcl/view?usp=sharing https://drive.google.com/file/d/1weyCOEAnnte0rJ45qFEvNR1UYpYmTE0/view?usp=sharing","Thank you for getting the traces! It is surprising that the fused step is still slower than the foreach...but a look at the trace shows that we only get 67% occupancy per fused adam kernel and 100% occupancy for each foreach op kernel.  why could this be the case? Unfused, 100% occupancy  Fused, 67% occupancy  The fused kernel does use almost twice as many registers, but it might just be due to it doing more work, but it shouldn't be doing more work than the sum of the foreach ops. btw 's occupancy changes (with dynamic chunking) will help make both impls faster.","As a side note, I tried `.compile` on PyTorch's unfused AdamW, after you mentioned it. TFLOPS goes from 85 to 4, haha. It's because `torch.compile` keeps recompiling when the LR changes (which is inevitable with a learning rate scheduler).","Haha thank you for gathering the number there (though it's abysmal :() Is this true even when you wrap lr into a tensor? like torch.tensor(lr)?  There is a tracking issue for lr:  CC(Using torch.compile, batch training step time takes long to converge when adding a LR Scheduler) that is in progress  ","Using `AdamW(lr=torch.tensor(...))` with scheduler active, TFLOPS went from 85 to 88. Zoom zoom! LR scheduler is LambdaLR with regular nontensor floats.","icy œáŒ±œÑŒ≥ŒπœÅŒª:  I can reproduce the performance dropoff under torch 2.2 icy œáŒ±œÑŒ≥ŒπœÅŒª: compiling only forward + loss, enabling the fused optimizer completely destroys the performance icy œáŒ±œÑŒ≥ŒπœÅŒª: but on 2.4 nightly, enabling fused helps a tiny bit"," just a heads up there was a regression in fused optimizers in the past few days. https://github.com/pytorch/pytorch/pull/123566 should fix it. > we only get 67% occupancy per fused adam kernel and 100% occupancy for each foreach op kernel  the foreach version materializes intermediate results into global memory at every step. The fused version only materializes the final results, which requires more registers per thread and can lead to lower occupancy. I think it's normal for the fused version to be faster while having lower occupancy, because it avoids doing a lot of expensive memory I/O.","PyTorch nightly: fused AdamW 69.5 TFLOPS unfused AdamW 70.5 TFLOPS One of the two fused AdamW runs has a wobbly TFLOPS line, going up and down around 69.5.  "
llm,[TD] Add LLM retrieval + heuristic,Fixes ISSUE_NUMBER,2024-03-13T18:20:28Z,Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/121836, merge i," Merge started Your change will be merged while ignoring the following 18 checks: pull / linuxjammypy3clang12mobilebuild / build, pull / linuxjammypy3.8gcc11pch / build, pull / linuxfocalpy3.11clang10 / build, pull / linuxjammypy3clang12executorch / build, pull / linuxfocalcuda12.1py3.10gcc9 / build, pull / linuxjammycuda11.8cudnn8py3.8clang12 / build, pull / linuxfocalcuda11.8py3.10gcc9 / build, pull / linuxjammypy3.8gcc11noops / build, pull / linuxjammypy3.8gcc11mobilelightweightdispatchbuild / build, pull / linuxfocalpy3clang9mobilecustombuildstatic / build, pull / linuxfocalcuda12.1py3.10gcc9sm86 / build, pull / linuxjammypy3.8gcc11 / build, pull / linuxjammypy3.10clang15asan / build, pull / linuxfocalpy3.12clang10 / build, pull / linuxfocalpy3.8clang10 / build, pull / linuxfocalrocm6.0py3.8 / build, pull / linuxfocalpy3_8clang9xla / build, pull / linuxfocalpy3.8clang10onnx / build Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: 12 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.8gcc11pch / build  pull / linuxjammypy3clang12executorch / build  pull / linuxjammycuda11.8cudnn8py3.8clang12 / build  pull / linuxfocalcuda11.8py3.10gcc9 / build  pull / linuxfocalpy3clang9mobilecustombuildstatic / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ," merge f ""didnt realize i already had trunk, retrieval job is fine"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c weird m ""this job is constantly failing on main""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, Reverting PR 121836 failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch revert noedit 4819da60ab8749a370857460f3b3bd3e89a9bfbf` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job 
llm,[TD] LLM indexer to run daily,Run indexer daily Run indexer in docker container,2024-03-13T18:18:29Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/121835," merge f ""new code doesnt run on PR, lint passed, pull failures are due to something on main"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,The warning message in aten/src/ATen/native/transformers/sdp_utils_cpp.h::check_for_attn_mask is a bit confusing.," üêõ Describe the bug The warning message at https://github.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/transformers/sdp_utils_cpp.hL258, 'Both fused kernels do not support nonnull attn_mask,' does it mean that FlashAttention and MemEfficient do not support  nonnull attn_mask? However, at::_scaled_dot_product_efficient_attention does support  nonnull attn_mask, so should the warning message be updated?    Versions ",2024-03-13T02:54:53Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/121783,"Ahh you are right, I will send a PR Updating"
llm,Support `call_method` in DDPOptimizer,"This PR fixes Issue CC(AttributeError: 'MultiheadAttention' object has no attribute 'requires_grad'). While CC(AttributeError: 'MultiheadAttention' object has no attribute 'requires_grad') reported the issue with `MultiheadAttention`, a minimal reproduction would be:   Dynamo treats `self.linear(x)` as `call_module` while treating `self.linear.forward(x)` as a `get_attr` and a `call_method`. However, existing DDPOptimizer assumes, for a `get_attr` node, `getattr(gm, node.target)` gives a tensor with the `requires_grad` attribute. Existing DDPOptimizer also does not support `call_method` nodes. This PR adds support for `call_method` and check on `get_attr`. It also checks if a module's parameters have been added to a bucket to support multiple method calls from the same module. ",2024-03-12T23:34:38Z,oncall: distributed Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/121771, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Does PyTorch need to expose the header files in the aten/src/ATen/native/transformers directory to the client?," üöÄ The feature, motivation and pitch Now, I am adapting at::scaled_dot_product_attention to a specific type of cudalike device and encounters a problem.   Firstly, since aten/src/ATen/native/transformers/sdp_utils_cpp.h is not exposed to client, I need to write a lot of repeated checking code in my select_sdp_backend Secondly, since aten/src/ATen/native/transformers/attention.h is not exposed to client, I need to write the following code when registering _fused_sdp_choice_stub  The above issues arose in the context of planning to support PrivateUse1 at https://github.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/transformers/attention.cppL674. Following the approach at  CC(How to adapt to `at::scaled_dot_product_attention`'s routing logic for a third-party cuda-like device?) would avoid these issues, but it requires a lot of repetitive code.      Alternatives _No response_  Additional context _No response_",2024-03-12T11:20:09Z,triaged module: PrivateUse1,closed,0,2,https://github.com/pytorch/pytorch/issues/121720,"From Nikita: short term, exposing headers like this seems ok. Longer term, there should be a BE project to more clearly decide what we want to expose / not expose.",https://github.com/pytorch/pytorch/pull/122586
gemma,Complexity of output_graph.py:new_var() goes through the roof,"I'm running Google's Gemma model with Dynamo enabled and PyTorch spends around 9 minutes in the Dynamo (just Dynamo, excluding Inductor). When profiling I see that ~5:30min is spend in `new_var()`: https://github.com/pytorch/pytorch/blob/fd13a56f614effec0e731750264b92ac0a448835/torch/_dynamo/output_graph.pyL708L714 Looks like low hanging fruit for compile time reduction. ",2024-03-11T22:11:42Z,high priority triage review oncall: pt2 module: dynamo,closed,0,8,https://github.com/pytorch/pytorch/issues/121679,"With profiler disabled the savings are less. If this function is linear instead of quadratic, the savings are 85 seconds. Still worth fixing.","Thanks. I will take a look this week. If you can share repro, that would be helpful too. I can profile Dynamo more and see where rest 3:30 min go.",This logic is obviously quadratic. We don't need a repro. Just make it not quadratic.,"Yeah, I can make that happen. I am asking for a repro for other parts that takes 3.5 minutes in Dynamo (which is also very high). But a repro is definitely not needed here.",The repo is this one: https://github.com/google/gemma_pytorch To enable Dynamo I'm adding `.compile`  annotation to this line: https://github.com/google/gemma_pytorch/blob/acd24a81ecbb9ee3bb90771a0c517c7f6b6ce528/gemma/model.pyL455 And I'm running it with this command line: , Wondering if this PR helps. And whats the new compilation time you are seeing. We sent a few fixes over the past week that should help compile time. I dint get a chance to setup the model myself.," Thank for the fix, it did help. I was using tagged `v2.2.1` version for my experiments. Later versions (including current top of the trunk https://github.com/pytorch/pytorch/commit/6915a5be700c44381647ff132fae4d5cee36970a) do not work and crash with Dynamo on CPU with the following message:  Notably, it's just 1 min 38 seconds till the crash for me not (way below 9 minutes that I saw earlier), I assume it's not only the fix for `new_var`, but something else as well."," The error above looks like a regression to me. I don't understand the PyTorch internals well enough to pinpoint the problem. Should I create an issue for that? If you have an advice how to proceed with the investigation (other than triaging to the faulty commit), I can try investigating further."
llm,[torch export][serialize] create a more compact stacktrace format for serialization,"Summary:  we want fx nodes' stack trace format to be backward compatible and same as before in the program we export  however in the serialized format, we would want to show a more compact stack_trace format, otherwise the nodes attributes are dominated by stack traces  the diff implements the minimal in serialization process to dedupe node stack traces by resorting to a fileinfo_list and a filename_to_abbrev map, so we can use index to represent filenames, use lineno to represent lines. Test Plan:  llm base on D54497918  set up breakpoint after serialization/deserialization  serialize  in P1193647450 (before serialization), search for `stack_trace` in P1193604333 (after serialization), search for `stack_trace` and `co_fileinfo_ordered_list` [note: didn't do compression in this diff since the size is pretty small and it adds complexity if we do compression]  deserialize  in P1193629435, search for `stack_trace`  ads Differential Revision: D54654443 ",2024-03-11T21:22:31Z,fb-exported Merged Reverted Stale ciflow/trunk release notes: fx module: dynamo,closed,0,27,https://github.com/pytorch/pytorch/issues/121675,The committers listed above are authorized under a signed CLA.:white_check_mark: login: strisunshinewentingwang  (9431f22c13433b38cc1d5f488110568dc45d6480),This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443,This pull request was **exported** from Phabricator. Differential Revision: D54654443," merge f ‚Äúlanded internally"""," merge f ""landed internally""", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,empty_like on sparse compressed tensors fails when specifying different device,As in the title. Reproducer:  The expected result is a sparse csr tensor with indices and values having the specified device. The problematic code is https://github.com/pytorch/pytorch/blob/fc712311ce568b97f6926e48b54999d5705985a2/aten/src/ATen/native/sparse/SparseCsrTensor.cppL802L804 and a possible fix is:   PyTorch version: main ,2024-03-11T21:01:08Z,module: sparse triaged bug,closed,0,0,https://github.com/pytorch/pytorch/issues/121671
yi,Warning static library kineto_LIBRARY-NOTFOUND not found when trying build tutorial torch.compiler_aot_inductor.html, üêõ Describe the bug I observe following error when try to follow this tutorial: https://pytorch.org/docs/main/torch.compiler_aot_inductor.html Here is the output log:  However looking at the Wheel build log here: https://github.com/pytorch/pytorch/actions/runs/8229185939/job/22499908747 I do see we include kineto in the build:  Without the correct settings people won't be able to use profiling in CPP code. Our nightlies and release binaries should generate the correct settings.    Versions 2.3.0,2024-03-11T20:55:31Z,module: binaries module: cpp oncall: releng triaged topic: binaries,open,1,2,https://github.com/pytorch/pytorch/issues/121668,"> Without the correct settings people won't be able to use profiling in CPP code. Our nightlies and release binaries should generate the correct settings. This is not true, is it? One can still use profiler, without any access to the kineto. I think the right fix here would be to remove any references of kineto from libtorch CMakeLists, unless there is an example that requires one to have direct access to Kineto headers","This is still an open issue on 2.3 RC, it's good to move this to 2.3.1."
mixtral,Look at gather + gemv performance.," üöÄ The feature, motivation and pitch Motivated by https://www.thonking.ai/p/shortsupportingmixtralingptfast, `gather + gemv` is the primary kernel driving mixtral perf. See the generated code here: https://pastebin.com/Hw7QEQps Here's a microbenchmark  cc: ren  Alternatives _No response_  Additional context _No response_ ",2024-03-11T19:43:00Z,module: performance triaged oncall: pt2 upstream triton,open,0,10,https://github.com/pytorch/pytorch/issues/121661,You can put the language name next to the markdown code block to give syntax highlighting btw.    ,"For the microbenchmark, how can we rerun the perf test? I tried top of trunk pytorch, but hit an issue cannot import name 'get_cuda_stream' from 'triton.runtime.jit'",I will take a look from Triton side. Are there better implementations somewhere that perform better? That can help me figure out where/how to improve.,ren I think calm probably has faster kernels: https://twitter.com/zeuxcg/status/1765603572895604929 https://github.com/zeux/calm/blob/main/src/infer.cuL190,"I am checking the generated .py file (https://gist.github.com/manmanren/8a41917351a41c386ff0540f9f5aa1b3), wondering why we have a fixed grid that doesn't depend on XBLOCK: grid=grid(8192) Since each thread block handles [XBLOCK, RBLOCK] elements, should grid be 32768/XBLOCK? If I change how we set up grid:  I hit a pytorch issue ",ren  `grid(8192)` return a function that will compute the real grid based on block sizes.,"I am focusing on one problem size of 16384. The best block size is 1 x 2048, for which, both Compute and Memory throughput are at 75%. The warp stall from ncu is on the for loop, which is kind of weird. NCU reports: [Long Scoreboard Stalls] Est. Speedup: 23.52%, so likely we are stalling to wait for memory operations. Both software pipelining and thread occupancy can be used to cover the long memory stall. The for loop only runs for 16384/2048 iterations, so SWP may not be that efficient there. But it is worth a try. The other things I noticed are weird blocking for other block sizes. 1> Block size of 1 x 4096: sizePerThread = [1, 16], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8] for both int8 and bf16. This will make the bf16 tensor non coalesced. We may have a fix for this   2> Block size of 4 x 2048: This uses a layout of sizePerThread = [1, 16], threadsPerWarp = [1, 32], warpsPerCTA = [2, 4] (for 4 x 2048 int8) and sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8] (for 1 x 2048 bf16) This means a layout conversion. It may make sense to have the same layout for both and we will load 4 rows of the matrix and perform computation for 4 rows in one iteration. This is kind of like unrolling 4 times, maybe it can help to cover some of the memory stall. Other than hiding the memory stall, for a smaller problem size of 4096, ncu reports [FP32 NonFused Instructions] Est. Speedup: 11.84% It is not clear how we can improve this. But maybe changing precision and using fused equivalent?",">  > 1> Block size of 1 x 4096: sizePerThread = [1, 16], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8] for both int8 and bf16. This will make the bf16 tensor non coalesced. We may have a fix for this   This is being fixed by https://github.com/openai/triton/pull/3317. The new heuristics is not perfect and I'm still waiting for broader perf results. You may want to give it a local try for now. > 2> Block size of 4 x 2048: This uses a layout of sizePerThread = [1, 16], threadsPerWarp = [1, 32], warpsPerCTA = [2, 4] (for 4 x 2048 int8) and sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8] (for 1 x 2048 bf16) This means a layout conversion. It may make sense to have the same layout for both and we will load 4 rows of the matrix and perform computation for 4 rows in one iteration. This is kind of like unrolling 4 times, maybe it can help to cover some of the memory stall. >  With same layout, would CC(Matrix multiplication operator) happen, i.e, loading the bf16 tensor would result in noncoalesced memory access?","> With same layout, would  CC(Matrix multiplication operator) happen, i.e, loading the bf16 tensor would result in noncoalesced memory access? I am thinking about same layout as: sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8] so for the matrix with i8 type, it will be v2.b32 vector load, for the vector with bf16, it will be v4.b32 load. But both will be coalesced.","Tried a few things, but didn't get perf win :[ 1> https://github.com/openai/triton/pull/3317 2> forcing the same layout for block size of 4 x 2048: this gets rid of a layout conversion, but the reduction afterwards is slower with the new layout 3> enable SWP: this actually regresses performance (tried num_stages=3) "
transformer,[DTensor] Moved `Transformer` sharding to staticmethod,"  CC([no ci][WIP] Supported 2D `clip_grad_norm_`)  CC([DTensor] Moved `Transformer` sharding to staticmethod)  CC([FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh)  CC([FSDP2] Relaxed check for parent mesh) To support FSDP + TP/SP unit tests, let us factor out the canonical TP/SP sharding of `Transformer` to a staticmethod that can be called by other unit tests. Test Plan:  ",2024-03-11T19:33:38Z,oncall: distributed triaged Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/121660, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 5, 5, linux.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","Failure in pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 5, 5, linux.4xlarge.nvidia.gpu) looks unrelated: ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 5, 5, linux.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,guard on tensor parallelism test examples,This task is to add some guards to the tensor parallelism test examples to ensure we incurred the right collectives: https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.py Specifically we can follow this logic to assert on the communication collectives incurred using the CommDebugMode https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.pyL100 The test example we want to add this guard on is the Transformer e2e training example https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.pyL179 ,2024-03-11T18:21:34Z,oncall: distributed triaged pt_distributed_rampup,closed,0,0,https://github.com/pytorch/pytorch/issues/121649
transformer,Compiling SwinTransformer encoder fail with input size changes, üêõ Describe the bug Compiling SwinTransformer forward at https://github.com/yoxu515/aotbenchmark/blob/paot/networks/encoders/swin/swin_transformer.pyL684 It is going to work correctly at training time when it works on a fixed input crop. It is going to fail instead at inference when the input size change.  Error logs   Minified repro _No response_  Versions last `pytorchnightly` official docker image. ,2024-03-11T14:54:55Z,triaged ezyang's list oncall: pt2 module: dynamic shapes,closed,0,11,https://github.com/pytorch/pytorch/issues/121637,"Thanks for the report . Did you try diagnosing with the doc linked in the error message? At some point I can take a look, but the full model repro instructions would be useful.","If we follow the docs in the report and log is correct  we have  That  it seems to generate the `setitem`: `While executing %setitem : [num_users=0] = call_functiontarget=operator.setitem, slice(0, 7, None), slice(0, 7, None), slice(None, None, None)), 0), kwargs = {})` `h` and `w` could be variable at inference and they are constant in training. The docs is relative to `datadependent control flow` but here there is not an explicit control flow in the user code but probably it is in the underline generated code. > But the full model repro instructions would be useful. As this model is a good source of many compiler errors I have already shared some instruction in other tickets to setup the environment with the hope you could adopt this model more in general to stress test the compiler:  CC(Pytorch 2.2 regression)issuecomment1924745100 In this case we need to run the `eval.py` on different image sizes.",I think it may work to explicitly test with `torch._check` that the h and w are in bounds for `img_mask` before doing the index. Your simplified code looks relatively simple and can take a look later., Compiling other components of the same model I am getting this with the last nightly    As I have already different compiler open tickets on this model is there any opportunity that the compiler team could adopt it (as it is a public repo) to stress test the compiler?,/://github.com/pytorch/pytorch/issues/121504. Then we have also another open ticket same model at  CC(Pytorch 2.2 regression), Is it possible that something is missing cause how it handles slices? > I think it may work to explicitly test with torch._check that the h and w are in bounds for img_mask before doing the index. Your simplified code looks relatively simple and can take a look later.  Cause manually adding `torch._check` in the loop for W and H and transforming slices in range it is working. And then it is failing just few row later with the same bug for:  ,"Do the same trick but with the downsampled sizes. We should do better for this case, but it is difficult because when you do a slice that is out of bounds, the semantics of PyTorch is to clamp to the max value, and this is painful to generate code for (we'd much rather know that the slice range is never out of bounds). Maybe we should introduce a variant of indexing which assumes everything is in range (this will also remove annoyances with negative indices). I think I'd be happy to add this model to our overall suite. If you are interested in rolling up your sleeves to do it, open a PR to https://github.com/pytorch/benchmark and then once it's in (I can help), I can add it to our torchbenchmark benchmark runs.","In the meantime I have create a workaround to not use python slices (it would be nice if at least when we find slices we could emit a more comprehensible error). But then later in the same function  function `x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)`  ","> I think I'd be happy to add this model to our overall suite. If you are interested in rolling up your sleeves to do it, open a PR to https://github.com/pytorch/benchmark and then once it's in (I can help), I can add it to our torchbenchmark benchmark runs. As  is trying to reproduce the same environment to check other compiler issues at  CC(Custom attention recompilations) I have asked if he he is already familiar with the benchmark repository. I could help for sure but I don't think I have the bandwidth to bootstrap the PR on my side as I need to find the time to check how the benchmark standard interface are organized in the repo. But I can support for everything related to the model for sure and compiler failures reproducibility with pytorch nightly versions.","For this one, try patching the line in `torch/_prims_common/__init__.py` to `guard_size_oblivious(y != expected_stride)` Also, https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/editheading=h.44gwi83jepaj may be of interest",https://github.com/pytorch/pytorch/pull/122370 for the size oblivious here
transformer,"[BUG] Given boolean tgt_mask, TransformerDecoder produces wrong results with MPS backend"," üêõ Describe the bug Two tokens are decoded in this example. Ideally, the output feature on the first token should be the same regardless of the sequence length as a square subsequent mask is applied. Here are two ways to generate the tgt mask. One is from the official example where a float mask is generated first but converted into a boolean mask later. Another way is to generate the boolean mask directly. :heavy_check_mark: If running with `cpu` backend, the three printed values should be the same:   ‚ùå  However, if we run it on `mps` backend, the directly generated boolean mask would result in the wrong decoded feature for the first token. The result looked like this:  The bug can be reproduced by this minimal script:   Versions PyTorch version: 2.2.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: version 3.28.3 Libc version: N/A Python version: 3.11.7 (main, Dec 15 2023, 12:09:56) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Max Versions of relevant libraries: [pip3] numpy==1.26.3 [pip3] pytorchlightning==2.2.0.post0 [pip3] torch==2.2.1 [pip3] torchaudio==2.2.1 [pip3] torchdata==0.7.1 [pip3] torchmetrics==1.3.0.post0 [pip3] torchtext==0.17.1 [pip3] torchvision==0.17.1 [conda] numpy            ",2024-03-11T13:16:51Z,module: nn triaged module: mps,open,0,1,https://github.com/pytorch/pytorch/issues/121632,I've tried device='cuda' or 'cpu'. There is nothing wrong. So it's might only linked to mps. !image
transformer,[inductor] Incorrect handle of `autocast` results in type mismatch," üêõ Describe the bug   when using inductor backend. I'm not able to create a single reproducer but I bisected to the triggering commit: https://github.com/huggingface/transformers/commit/20164cc2c67c07190893933e47c8f13dc1c1f1d7, it looks to me that the nested `autocast` is not being handled correctly. Full reproducer: The issue occurs when training llama with `transformers==4.38.2` and `torch>=2.2.0`, and can be reproduced with a (not very minimized) script as: `TORCHDYNAMO_DEBUG_FUNCTION='forward' TORCH_LOGS='+dynamo' torchrun train.py model=metallama/Llama27bhf cc=inductor`  Note that `cc=eager` works fine.  Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Alibaba Cloud Linux release 3 (Soaring Falcon)  (x86_64) GCC version: (GCC) 10.2.1 20200825 (Alibaba 10.2.13.5 2.32) Clang version: Could not collect CMake version: version 3.20.2 Libc version: glibc2.32 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.10.13416.1.al8.x86_64x86_64withglibc2.32 Is CUDA available: True CUDA runtime version: 12.3.103 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10 GPU 1: NVIDIA A10 Nvidia driver version: 535.146.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              64 Online CPU(s) list: 063 Thread(s) per core:  2 Core(s) per socket:  32 Socket(s):           1 NUMA nod",2024-03-11T12:41:41Z,triaged oncall: pt2 module: inductor,open,1,2,https://github.com/pytorch/pytorch/issues/121631,For some reason `print(query.dtype)` fixes the issue,"The print is likely helping because it is inducing a graph break, and now we are no longer hitting hte compiler bug as we are no longer compiling enough"
peft,"RuntimeError: Rank 2 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank."," üêõ Describe the bug  8Âç°finetune llavainterlm2Êó∂ÈÅáÂà∞ÈîôËØØ    ÁéØÂ¢É 8Âç° l40Ôºåinternlm2 1.8BÔºå  Versions Name: torch Version: 2.2.1 Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration Homepage: https://pytorch.org/ Author: PyTorch Team Authoremail: packages.org License: BSD3 Location: /root/miniconda3/envs/xtunerenv/lib/python3.10/sitepackages Requires: filelock, fsspec, jinja2, networkx, nvidiacublascu12, nvidiacudacupticu12, nvidiacudanvrtccu12, nvidiacudaruntimecu12, nvidiacudnncu12, nvidiacufftcu12, nvidiacurandcu12, nvidiacusolvercu12, nvidiacusparsecu12, nvidiancclcu12, nvidianvtxcu12, sympy, triton, typingextensions Requiredby: accelerate, deepspeed, peft, torchvision, xtuner ",2024-03-11T02:38:51Z,oncall: distributed,open,0,0,https://github.com/pytorch/pytorch/issues/121606
transformer,[DDP] Gradient Synchronization Failure Induced by model.gradient_checkpointing_enable() in multi-task setting," üêõ Describe the bug Hello, when I am using DDP to train a model, I found that using multitask loss and gradient checkpointing at the same time can lead to gradient synchronization failure between GPUs, which in turn causes the parameters of the model on different GPUs to be inconsistent.  Environment:  A minimal example:  When I comment out `'ddp_find_unused_parameters': False,` or the peft code, We got a RuntimeError. The error message seems to be related to this issue:  I‚Äôm unsure if this is a bug or if gradient checkpointing combined with DDP is not compatible with multitask learning. When I disable the gradient checkpointing, it seems to be working normally now. But I‚Äôm worried that it might not truly be functioning correctly. I want to understand what happens in the forward and backward processes when I either disable or do not disable gradient checkpointing, and why gradient checkpointing would cause a failure in gradient synchronization.  Versions Collecting environment information... PyTorch version: 2.2.0 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.17 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01160.88.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 GPU 3: NV",2024-03-10T10:37:28Z,oncall: distributed module: ddp,open,0,1,https://github.com/pytorch/pytorch/issues/121594,I wonder if this issue could be resolved by using `torch.utils.checkpoint.checkpoint` with `use_reentrant=True`.
transformer,"RuntimeError: invalid unordered_map<K, T> key"," üêõ Describe the bug When I export the onnx model and execute it to:  Throw exception:  When defining:   When I debug to the line:   self. sense is actually  bitsandbytes. nn. modules Linear4 bit (nn. Linear)   This exception is MatMul4Bit.apply(A, B, out, bias, quant_state) ,   in file torch.autograd.function.py.apply .  Pytorch version is 2.2.1(stable) .  Versions os:  win10 cuda: 11.8 pytorch:  2.2.1 (stable) ====other===== accelerate                0.27.2 aiofiles                  23.2.1 altair                    5.2.0 annotatedtypes           0.6.0 anyio                     4.3.0 APScheduler               3.10.4 attrs                     23.2.0 backports.zoneinfo        0.2.1 bitsandbytes              0.41.0 bitsandbyteswindows      0.37.5 blinker                   1.7.0 certifi                   2022.12.7 charsetnormalizer        2.1.1 click                     8.1.7 colorama                  0.4.6 coloredlogs               15.0.1 contourpy                 1.1.1 cpmkernels               1.0.11 cycler                    0.12.1 DBUtils                   3.0.3 etxmlfile                1.1.0 exceptiongroup            1.2.0 fastapi                   0.110.0 ffmpy                     0.3.2 filelock                  3.9.0 Flask                     3.0.1 FlaskAPScheduler         1.13.1 flatbuffers               24.3.7 fonttools                 4.49.0 fsspec                    2023.10.0 gradio                    4.20.1 gradio_client             0.11.0 h11                       0.14.0 httpcore                  1.0.4 httpx                     0.27.0 huggingfacehub           0.19.4 humanfriendly             10.0 idna                      3.4 im",2024-03-09T07:21:59Z,module: onnx triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/121572,"Hi  , are you using the `torch.onnx.dynamo_export` api to export? If so, can you please share that part of the code?",Closing as there is no response in the last 30 days
gpt,GPT2 SDPA patterns mapping to ATen SDPA kernels,"  CC(Make causal + mask handiling consistent) should be merged first Graph breaks are introduced if the `inv_scale` is read with `.item()`, so not reusing its value but looked it up directly from https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.pyL182L220 It seems GPT2 can't directly use `is_causal` as `True` in `torch.nn.functional.scaled_dot_product_attention` for most cases. ",2024-03-09T05:58:38Z,open source Stale module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/121570,", pattern 20 (both causal mask & attention mask being present) seems to need more investigation. May we add it in a separate PR? I combined patterns 18 & 19 into one, and opened CC(GPT2 SDPA inference patternmatching for InductorCPU) for it instead. I'll seek your review on that PR after CI would complete. Thanks!","> , pattern 20 (both causal mask & attention mask being present) seems to need more investigation. May we add it in a separate PR? I combined patterns 18 & 19 into one, and opened CC(GPT2 SDPA inference patternmatching for InductorCPU) for it instead. I'll seek your review on that PR after CI would complete. >  > Thanks! ""pattern 20 (both causal mask & attention mask being present) seems to need more investigation."" Do you indicate the UT problem?","Sorry for my late response, ! It's the same issue that you had encountered locally with it (failing UT).","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,[DCP] Replaced `storage()` with `untyped_storage()`,  CC([FSDP] Removed clamp to `NO_SHARD` for world size 1)  CC([DCP] Replaced `storage()` with `untyped_storage()`) Let us try to remove this warning üòÑ :  ,2024-03-08T20:37:08Z,oncall: distributed Merged ciflow/trunk release notes: distributed (checkpoint) module: distributed_checkpoint,closed,0,4,https://github.com/pytorch/pytorch/issues/121538, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llava,Make unsupported error messages better in Dynamo," and I were debugging exporting llava. We ran into the following error message:  What this actually meant was: ""You tried to call iter(images), but images is of type PIL.Image so Dynamo didn't know how to handle it, thus leading to a graph break"". This was an error in the model code (`images` probably should have been a `List[Image]`instead of an `Image`), but the actual error ""torch._dynamo.exc.Unsupported: call_function BuiltinVariable(iter) [UserDefinedObjectVariable(Image)] {}"" is very obtuse. ",2024-03-08T16:48:10Z,high priority module: logging triaged months oncall: pt2 module: dynamo,open,0,4,https://github.com/pytorch/pytorch/issues/121505,"(old issue scrubbing). , is this still relevant?",The unsupported errors are still hella obtuse,Still relevant,"I want to bump the priority on this. We've had multiple users work with torch.compiling a model. The feedback we hear often (like from  recently) is ""we'd like to reduce our graph breaks to get better performance, but your graph break error messages are too obtuse"". We do have logging for the most common graph breaks that people encounter internally. To make torch.compile more usable, one thing we can do is at least audit the ""top N most common graph breaks"" and ensure that their messages are actionable. If you're looking to pick this up and think that ""rewriting error messages is boring"": the ROI on this is pretty high, from a maintenance burden pointofview. I spent a month once rewriting custom op error messages to make the whole custom ops experience more selfserve. It worked  we no longer get questions about things like ""should I use float or double as a type in the schema""? See also:  CC(Make SkipFiles graph breaks actionable)"
yi,Revert xla pin for XLA CI workflow,"XLA workflow is failing after XLA pin update to 8f913829abd9de749339d8d74b7357e1be3a7907 (from March 6th)  not sure if this is the real culprit,  The last time it worked was with Feb pin,  ",2024-03-08T16:10:54Z,triaged module: xla open source topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/121503,"there is a merge conflict in this pr, also what's the CI failure looks like?","There were a couple other prs needs newer XLA Pin, I recommend forward fix the issue by disabling the plugin"
yi,Doc updates for torch.library.impl_* APIs,Fixes CC(make it clearer (in docs) one can double decorate with torch.library.impl_* APIs) and improves examples for torch.library.impl_* APIs.,2024-03-08T02:22:27Z,triaged open source Stale release notes: composability topic: docs,closed,0,2,https://github.com/pytorch/pytorch/issues/121469,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: soxand16 / name: Sam Oxandale  (151a9e0f0247bf3fc82ee4cc802d84385a337de0, 5905d0395ab75af84985e0065f248e95a9e90330, 9272cda859210af86783c3ad54ca97b101c55c77, b0890e9c48b53898139c1f620e4803ff28193a48, d353946a9b0bb7fb369b54d87152d754fda8f3ba, d1ef5ed936174b7e8c83d5cfe858cbdd6869a5b2, 7a1788e8dccfff9a65bd4bdc3d5b30a5a28901cd)","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Doc updates for torch.library.impl_* APIs,Fixes CC(make it clearer (in docs) one can double decorate with torch.library.impl_* APIs) and improves examples for torch.library.impl_* APIs,2024-03-08T02:15:13Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/121467,"   :x: The email address for the commit (b0890e9c48b53898139c1f620e4803ff28193a48, d1ef5ed936174b7e8c83d5cfe858cbdd6869a5b2, 5905d0395ab75af84985e0065f248e95a9e90330, 151a9e0f0247bf3fc82ee4cc802d84385a337de0) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."
transformer,[RFC] PagedAttention Support," Feature request PagedAttention has been a mainstream optimization technology for generation task based on LLMs.  It has been supported by a lot of server engines, e.g., vllm, tensorrtllm and TGI. To get OOB performance with PyTorch+Huggingface, in this RFC, we plan to propose the PageAttention KV Cache design which use compatible semantic as the HF KV Cache.   Motivation KV cache is used to reduce computation for _**Decoder**_ layers, but it also brings memory overheads.  In general, the memory overheads are introduced by four parts.  1. Firstly, in the attention module, the past key/value token states should be concated with the current token to get the entire context which will introduce additional memory read/write.  2. Secondly, for the multiple sampling, e.g., beam search, the KV cache should be reordered according to latest beam index.  When the sequence is very long, the memory overheads of these two parts will be performance bottleneck.  3. Thirdly, for batching inputs, all sequence will be generally padded to the longest sequence in this batch. The native KV cache in the transformers uses contiguous memory buffer to store KV cache and the shape is (batch, head_num, past_seq_len, head_dim), it means that the memory consumption of the KV cache is decided by the longest sequence.  4. Lastly, for multiple sampling, there is only one prompts for different beams, and it can be shared across different beams, but the native implementation needs to store multiple copies of prompts tokens.  This will obviously waste a lot of memory and limits the max batch size for throughput mode.   To address the above limitations, this paper proposes ",2024-03-08T01:16:05Z,triaged module: sdpa,open,8,17,https://github.com/pytorch/pytorch/issues/121465, Chen  , ,"Hey thank you for proposing the RFC! After discussion with some other members of the PyTorch Core team we have a few points of feedback.  It seems that the `reshape_and_cache` function could instead be implemented using tensor subclasses. Although this is particular to Paged Attention, it seems that it could be made more generic and extended to represent arbitrary Logical to Physical mappings of Tensors.  To add kernels to PyTorch, a high bar is required to be met due to the build time and binary size implications. PyTorch Core is intended for very general things that can minimize maintenance burden. The world of deep learning moves very quickly, so outside repos like torchAO (https://github.com/pytorchlabs/ao) exist as places to store code for techniques that are useful to a broader audience but may undergo more rapid evolution. Given this, would it make more sense to add the paged attention kernels to torchAO instead of PyTorch Core? That would allow the technique to be available to users while providing more flexibility as it evolves. Let me know if you have any other questions or would like to discuss further. We appreciate you taking the time to submit this RFC!"," Thanks for the feedbacks. > * It seems that the `reshape_and_cache` function could instead be implemented using tensor subclasses. Although this is particular to Paged Attention, it seems that it could be made more generic and extended to represent arbitrary Logical to Physical mappings of Tensors. Abstracting the paged kv cache with tensor subclasses sounds a good idea. This can also simplify the op definition. > * To add kernels to PyTorch, a high bar is required to be met due to the build time and binary size implications. PyTorch Core is intended for very general things that can minimize maintenance burden. The world of deep learning moves very quickly, so outside repos like torchAO (https://github.com/pytorchlabs/ao) exist as places to store code for techniques that are useful to a broader audience but may undergo more rapid evolution. Given this, would it make more sense to add the paged attention kernels to torchAO instead of PyTorch Core? That would allow the technique to be available to users while providing more flexibility as it evolves. Fully understand the positioning for a general PyTorch core. I have several questions with the torchao path. 1. I noticed that the purpose of torchao is mainly for model compression (quantization and pruning) per its description: **The torchao repository contains api's and workflows for quantization and pruning gpu models.** But the paged attention optimization is general for LLMs with floating point inference too. In fact, its implementation is primarily with fp32/bf16/fp16 even for quantized models. Are you going to extend the scope of torchao to support other optimizations too? If so, how are you going to position torchao in the future? 2. Currently, torchao only contains python frontend code while what we want to propose is also about the native kernels. Would it be fine to incorporate native kernels to torchao too? 3. Is it an incubation process that we put the features outside PyTorch core first and then later they can be promoted to PyTorch core with more stability and maturity? Do you envision that things like PagedAttention can be added PyTorch core in the future?",For the questions about TorchAO I will defer to  ,Thanks . Any comments  ?,"  You're welcome to contribute to torchao, which is a repository for architecture optimization. Please see our RFC for our future plans: https://github.com/pytorchlabs/ao/issues/47 . We do also plan to ship native kernels. Stable and successful features can become prime candidates for upstreaming, if they are broadly applicable even beyond architecture optimization. Having said all of this, of course, it's not necessary for a feature to enter torchao first before it can enter core torch.",">   You're welcome to contribute to torchao, which is a repository for architecture optimization. Please see our RFC for our future plans: pytorchlabs/ao CC(Improve storage, tensor and module C error messages + fix for dl flags in nightly python) . We do also plan to ship native kernels. Stable and successful features can become prime candidates for upstreaming, if they are broadly applicable even beyond architecture optimization. Having said all of this, of course, it's not necessary for a feature to enter torchao first before it can enter core torch. Thanks  . It seems the project description of torchao needs to be updated. May I know the scope of ""architecture optimization""? Do you think PagedAttention support belongs to ""architecture optimization""? Also  ","  I agree that ""architecture optimization"" is a bit of a broad (and fluid) term, but I think the RFC on https://github.com/pytorchlabs/ao/issues/47 does a good job at outlining the scope. I do indeed see PagedAttention, as outlined in this issue, as a specialized storage layout for a Tensor cache geared towards decoding. And in particular it is for architectures that make use of the scaled dot product attention function. It seems plausible that it could be generalized further to a generic storage layout for more operators, but if I understand correctly that's not the plan in the near future. So it's an optimization of a specific (set) of architectures (attention based) and for a very specific application (decoding). We are obviously still building out torchao, so I'm eager to hear your take on this.","> I do indeed see PagedAttention, as outlined in this issue, as a specialized storage layout for a Tensor cache geared towards decoding. And in particular it is for architectures that make use of the scaled dot product attention function. It seems plausible that it could be generalized further to a generic storage layout for more operators, but if I understand correctly that's not the plan in the near future. So it's an optimization of a specific (set) of architectures (attention based) and for a very specific application (decoding). We are obviously still building out torchao, so I'm eager to hear your take on this. Thanks for clarifying the connection between PagedAttention and architecture optimization here. Yes, we indeed want to target LLM inference in the short term. I don't see any other application that can benefit from the new storage layout so far. But on the other hand, it is also worth considering to be incorporated into PyTorch upstream if an operation is important enough for key AI apps/workloads nowadays, e.g., LLM inference. The scaled dot product attention function is an example, which is also architecture specific but was added into PyTorch upstream.","  Yes, SDPA is also architecture specific, but it applies across domains (vision, speech, text and its combinations), it works for inference and training, across many devices and it works with various input tensor types. If PagedAttention eventually gets to this point too, then inclusion in torch itself makes more sense. But if we want to move fast in creating a version that works for LLMs, decoding and on CPU, then torchao is a potential place to put these kernels.","> But if we want to move fast in creating a version that works for LLMs, decoding and on CPU, then torchao is a potential place to put these kernels. Sure, I agree that we can start with torchao  we'd love to contribute to torchao. Note that PagedAttention is not specific for CPU while we (Intel) can contribute CPU kernels as a starting point and perhaps others from the community can contribute CUDA kernels too. Since torchao doesn't support native kernels yet, I guess we need more discussions and your support to incorporate native kernels into it and package them into the release binaries. > Yes, SDPA is also architecture specific, but it applies across domains (vision, speech, text and its combinations), it works for inference and training, across many devices and it works with various input tensor types. If PagedAttention eventually gets to this point too, then inclusion in torch itself makes more sense. Yes, SDPA certainly covers more usages of Transformerbased models, yet it's worth noting that decodingonly Transformer models are increasingly becoming the main area of research and deployment interest. Therefore, it's important to also consider the prominence of specific applications in determining a feature's eligibility for upstreaming.","  Yes, I'm more than happy to help support shipping this C++ kernel! We can setup an extension that allows you to add these. I'll work on this next and ping this issue once it's done.","  Also, another team you could consider talking to is https://github.com/facebookresearch/xformers  ",">   Also, another team you could consider talking to is https://github.com/facebookresearch/xformers . About the project positioning, do I understand correctly that xformers focuses more on the research while torchao is more about production? We are more interested in the product deployment in the context of PagedAttention.","xFormers has some support for PagedAttention now  but we are indeed focusing on research first, and on NVIDIA GPU acceleration almost exclusively, so it might not be the best place to put code to accelerate inference on CPUs...",  we plan to use tensor wrapper to implement the paged attention kv cache and have updated the design in this RFC. Pls help to review. 
yi,Trying to use forward AD with affine_grid_generator that does not support it because it has not been implemented yet," üöÄ The feature, motivation and pitch I would like to compute Jacobianvector products for a function relating to affine image registration. Running torch.func.jvp yielded the error message in title.  Alternatives torch.func.vjp works for my function  Additional context  NotImplementedError                       Traceback (most recent call last)  in        1  trying jvp > 2 _, jvp_out = torch.func.jvp(affine_squared_differences, (T_A, T_b), (T_A_id, T_b_id)) ~/opt/anaconda3/lib/python3.8/sitepackages/torch/_functorch/eager_transforms.py in jvp(func, primals, tangents, strict, has_aux)     914     """"""     915  > 916     return _jvp_with_argnums(func, primals, tangents, argnums=None, strict=strict, has_aux=has_aux)     917      918  ~/opt/anaconda3/lib/python3.8/sitepackages/torch/_functorch/vmap.py in fn(*args, **kwargs)      37     def fn(*args, **kwargs):      38         with torch.autograd.graph.disable_saved_tensors_hooks(message): > 39             return f(*args, **kwargs)      40     return fn      41  ~/opt/anaconda3/lib/python3.8/sitepackages/torch/_functorch/eager_transforms.py in _jvp_with_argnums(func, primals, tangents, argnums, strict, has_aux)     963                     primals = _wrap_all_tensors(primals, level)     964                     duals = _replace_args(primals, duals, argnums) > 965                 result_duals = func(*duals)     966                 if has_aux:     967                     if not (isinstance(result_duals, tuple) and len(result_duals) == 2):  in affine_squared_differences(T_A, T_b)      11         Returns a tensor of shape (1, C, Df, Hf, Wf) recording the squared differences      12     """"""         > ",2024-03-07T14:30:49Z,triaged module: forward ad module: functorch,closed,0,7,https://github.com/pytorch/pytorch/issues/121411,Reproducer: , how difficult do you think this is?,"This should be very easy since affine_grid is linear, e.g. the support can be added with `result: auto_linear` here: https://github.com/pytorch/pytorch/blob/67208f08bd5da0f89ad8b7ab6d77fb28b598a73a/tools/autograd/derivatives.yamlL279L285", tried to add auto_linear and gradcheck failed... so I'm not sure that it is actually linear. Maybe we're using gradcheck wrong?,I'll double check it later today if the gradcheck call was indeed wrong., Maybe worth checking if the input is double precision,>  Maybe worth checking if the input is double precision That was the catch. Thanks!
rag,"[Clang-tidy header][23/N] Enable clang-tidy coverage on aten/src/ATen/*.{cpp,h}",This PR finishes the works beginning with https://github.com/pytorch/pytorch/pull/120763 by enabling clangtidy on aten/src/ATen.,2024-03-07T05:10:56Z,open source Merged ciflow/trunk topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/121380, label ciflow/trunk, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,[inductor][cpu]AMP models regression and crash in 2024-03-04 nightly release, üêõ Describe the bug AMP models regression and crash in 20240304 nightly release. AMP static shape default wrapper regression               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       llama       multiple       32       1.372195       0.014102689       0.019351639332355       19.282797       32.0       2.142721       0.009058953000000002       0.019410808831113003       19.388413       0.64       1.0       0.64       1.01                 torchbench       yolov3       multiple       8       0.975399       0.09829916       0.09588090236484       17.287654       8.0       3.491892       0.02860036       0.09986936828111999       21.98618       0.28       1.04       0.29       1.27                 timm_models       convit_base       multiple       64       1.364183       0.251519204       0.34311822227033195       36.793275       64.0       1.990777       0.172512283       0.343433485213891       31.588458       0.69       1.0       0.69       0.86                 torchbench       llama       single       1       0.68803       0.061452501       0.042281164263030004       20.256173       1.0       3.788769       0.01094602       0.04147194124938       19.422561       0.18       0.98       0.18       0.96                 torchbench       yolov3       single       1       0.99071       0.28,2024-03-07T02:41:04Z,triaged oncall: pt2 module: inductor oncall: cpu inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/121377, will help to check the guilty commit manually.,"verfied the guilty commit: 1104e07 llama /workspace/pytorch bash inductor_single_run.sh multiple inference performance torchbench llama amp first static dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles cpu,llama,32,**0.999226**,19.031106,38.433693,0.927021,320.108954,345.309184,2655,5,1,1,0,0 BlenderbotForCausalLM /workspace/pytorch bash inductor_single_run.sh single inference performance huggingface BlenderbotForCausalLM amp first static ERROR:common:Backend dynamo failed in warmup() Traceback (most recent call last):   File ""/workspace/pytorch/benchmarks/dynamo/common.py"", line 2629, in warmup     fn(model, example_inputs)   File ""/workspace/pytorch/torch/_dynamo/eval_frame.py"", line 437, in _fn     return fn(*args, **kwargs)   File ""benchmarks/dynamo/huggingface.py"", line 557, in forward_pass     with self.autocast(**self.autocast_arg):   File ""benchmarks/dynamo/huggingface.py"", line 557, in torch_dynamo_resume_in_forward_pass_at_557     with self.autocast(**self.autocast_arg): TypeError: __call__() takes 2 positional arguments but 5 were given ca67938 llama /workspace/pytorch bash inductor_single_run.sh multiple inference performance torchbench llama amp first static dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles cpu,llama,32,**2.143268**,9.160565,7.426324,0.972145,320.196198,329.370829,531,1,0,0,0,0 BlenderbotForCausalLM /workspace/pytorch bash inductor_single_run.sh single inference performance huggingface BlenderbotForCausalLM amp first static dev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks,autograd_captures,autograd_compiles cpu,BlenderbotForCausalLM,1,2.482619,745.482914,30.554432,0.933324,3138.726707,3362.954854,936,1,0,0,0,0","Hi , based on the testing result from , looks like https://github.com/pytorch/pytorch/commit/1104e0798c8206e0226f2d68f6bb065645e6276f has caused the regressions reported in this issue. Could you help to take a look?","* Is  the regressed model? I run it on my end and didn't see the regression yet, but probably I'm using a different config. Can you share with me the exact command of running this model?   CC([Dynamo] Fix inspect.getattr_static doesn't work well for torch.utils._cxx_pytree.PyTreeSpec) fixed a latent bug, it may cause perf difference, but I don't know if it's reasonable before I can reproduce it.","> * Is `BlenderbotForCausalLM` the regressed model? I run it on my end and didn't see the regression yet, but probably I'm using a different config. Can you share with me the exact command of running this model? According to this issue description, there are 2 kinds regression, one is performance regression, like `llama`, another is function crash, like `BlenderbotForCausalLM`.  We used this Dockerfile  to build the test env. And can use script inductor_single_run.sh to reproduce the issue. For `BlenderbotForCausalLM` functional crash with performance mode cmd under pytorch source code root dir: `bash inductor_single_run.sh single inference performance huggingface BlenderbotForCausalLM amp first static` For `llama` performance regression can use cmd: `bash inductor_single_run.sh multiple inference performance torchbench llama amp first static`  Please try it and feel free let us know if there is any problems.","base on the latest results, this issue is fixed."
transformer,[FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh,"  CC([DTensor] Moved `Transformer` sharding to staticmethod)  CC([FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh)  CC([FSDP2] Relaxed check for parent mesh) Eventually, we should just have one unified way to check for parity between a `DTensor`sharded model and a replicated model. This PR is a small refactor to work toward that. One current gap to use this `check_sharded_parity` function for 2D is that FSDP's `(Shard(0), Shard(0))` layout differs from that of the `DTensor` APIs since FSDP shards on dim0 after TP shards on dim0. ",2024-03-06T23:19:29Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,5,https://github.com/pytorch/pytorch/issues/121357, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,"[AudioLM] Graph break: call_method UserDefinedObjectVariable(dict) get [TorchVariable(<class 'torch.Tensor'>), ConstantVariable(NoneType)]"," üêõ Describe the bug Discovered while compiling lucidrains/audiolmpytorch  This is a bit concerning; if it's actually a dict, we should have wrapped it in UserDefinedObjectVariable. This lookup is happening in einops which had some problems Full repro code: gist.github.com/ezyang/64c24c9fc5529f3afed4ee4266f6adc5  Versions main ",2024-03-06T21:49:38Z,triaged oncall: pt2 module: dynamo module: graph breaks empathy-day dynamo-triage-june2024,closed,0,6,https://github.com/pytorch/pytorch/issues/121345,"Looking over the einops code, it looks like this is a normal dict: ",I am having some similar problems with nested dicts right now. Except it's even worse as it's leading to an uncaught assestion in the constants.py (passing a Dict that should have been a ConstDict).," Do you have a small repro for your issue? I think we should find out the scenario that a regular is not wrapped as ConstDict, then it's not hard to fix it imo.",If we just add allocation stack traces to all variable trackers this will be easy to nail. If someone gives me a patch I can run it on AudioLM,"Dictionaries in dynamo have some limitations. The good part of the story is that these limitations are documented and it's super easy to extend our implementation to support more types! https://github.com/pytorch/pytorch/blob/06d2392003caa4ec4ac4259c2d0a68037a3470f2/torch/_dynamo/variables/dicts.pyL27L29 At the moment we are graphbreaking when hitting elements of dicts that are not hashable internally: https://github.com/pytorch/pytorch/blob/06d2392003caa4ec4ac4259c2d0a68037a3470f2/torch/_dynamo/variables/dicts.pyL74L75 We could throw a hard error saying ""this dict key is not supported, please file a ticket with your error to ask for support for this type of keys"".","This no longer seems to repros with da94ab0b66c and Python 3.10. I looked into the code a bit and some historical commits, but can't really see how we'd route this dict to a `UserDefinedObjectVariable`. The routing to `ConstDictVariable` is pretty straightforward and has been there before this issue was created: https://github.com/pytorch/pytorch/blob/c10eb42aa028fd3e75d06f83ab74ff740bb64bd2/torch/_dynamo/variables/builder.pyL611. Anyways, I ran into an inductor error instead, I'll create an issue for that, meanwhile can we close this one?  "
llm,[AudioLM] Graph break: call_method UserDefinedObjectVariable(_lru_cache_wrapper), üêõ Describe the bug Discovered while compiling lucidrains/audiolmpytorch  Full repro code: gist.github.com/ezyang/64c24c9fc5529f3afed4ee4266f6adc5  Versions main ,2024-03-06T21:43:08Z,triaged oncall: pt2 module: dynamo module: graph breaks internal ramp-up task empathy-day dynamo-triage-june2024,open,0,0,https://github.com/pytorch/pytorch/issues/121344
llava,Remove llava from ci_expected_accuracy as it's flaky,https://github.com/pytorch/pytorch/pull/121029 added it into the CI but the test is flaky on hud. It alternates between fail_accuracy and fail_to_run ,2024-03-06T18:19:28Z,Merged module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/121322,It's better to skip the cpu runs in https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/torchbench.yaml.,"Ah got it, I also added it to the skip list."," merge f ""inductor cpu tests passing"" "," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Problems differentiating through a transformer when exporting to onnx," üêõ Describe the bug I'm trying to export a model to onnx involving back propagation, and I've run into a number of issues. I can work around some of them, but this one seems a bit tough. You can see the kind of problem I've encountered if you run this python code:  This probably overlaps with  CC(Problems differentiating through certain operations when exporting to TorchScript) and  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace) as this is basically what I was trying to do when I found those bugs. There are some things I can work around (eg the backward pass of a gelu unit is unsupported but I can implement that myself using a torch.onnx api), but some things look a lot harder to work around. This particular export fails with an error that it's trying to insert a parameter as a constant when it requires a gradient. I've turned requires_grad off for all the model's parameters though, so I think it's erroneously trying to insert an intermediate value as a constant like it's doing in  CC(Problems differentiating through certain operations when exporting to TorchScript) (I found that bug while basically trying to strip this one down). Fixing that issue will probably reveal the layer norm problem I reported here  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace) , the fact that the backward pass for the Gelu nonlinearity isn't implemented (at least that's something I can work around) and probably some other stuff  Versions Collecting environment information... PyTorch version: 2.2.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM u",2024-03-05T23:08:10Z,module: onnx triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/121280,"Ok, I've actually managed to make this export now, with a hacked version of pytorch, a bit of torch.onnx.register_custom_op_symbolic() and overriding a couple of pytorch functions with manual python implementations. I actually haven't been able to confirm if my results were correct yet, as with randomly initialized parameters the gradients were very small, but I'll try and list all the relevant hacks I had to do here anyway: * Disabled the jit::tracer::setTracingState() stuff in tools/autograd/gen_trace_type.py for the ""matmul"" and ""linear"" operations, as if I don't do this they expand to untraced operations which cause problems in the backward pass (see the matmul case in this ticket  CC(Problems differentiating through certain operations when exporting to TorchScript)) * Hacked SavedVariable::unpack() in torch/csrc/autograd/saved_variable.cpp so it always returns the data_ member variable so the hash of the return value isn't consistent. Also saving a pointer to the original variable in the SavedVariable class, so I can use jit::tracer::setValueTrace() to tell the tracer that some of the saved output variables in the backward ops are actually the same as the original output variables (this is to fix the sqrt case in  CC(Problems differentiating through certain operations when exporting to TorchScript)) * Replaced the torch.layer_norm() function with a manual python implementation to avoid the issues mentioned here:  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace) * Replaced torch.scaled_dot_product_attention with a manual python implementation to avoid this issue:  * Implemented symbolic op graphs for aten::select_backward, aten::gelu_backward and aten::_softmax_backward_data using torch.onnx.register_custom_op_symbolic()","Update: I do appear to have made this work because I can export a larger model that differentiates through its transformer components, and the resulting onnx produces the same outputs as the original pytorch model. I can't run the onnx with different input shapes though, so the code related to the dynamic_axes argument in torch.onnx.export() appears to be broken (if I comment out the backprop part in the model the exported onnx works fine). I'll see if I can get a simple repro for that...","Hi David, torch.onnx.export (and torch.ji in general) doesn't support backpropagation and we don't have plans to support it Try torch.compile(backend=""onnxrt"") instead!"
agent,[Torchelasic] Create root log directory by default,"Summary: After refactoring in https://github.com/pytorch/pytorch/pull/120691, default behavior unintentionally was changes from creating tempdir for logging to not capturing any logs by torch Elastic Agent. Reverting the behavior to:  making tempdir when log dir is not specified  allowing nonempty root log dir      Note: in case attempt folder exists, it will be pruned here: https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/multiprocessing/api.pyL294 Differential Revision: D54531851 ",2024-03-05T19:32:40Z,oncall: distributed fb-exported Merged ciflow/trunk,closed,0,6,https://github.com/pytorch/pytorch/issues/121257,This pull request was **exported** from Phabricator. Differential Revision: D54531851,This pull request was **exported** from Phabricator. Differential Revision: D54531851, merge, Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Issue with sentence_transformers pointing to torch tensors manipulations," üêõ Describe the bug I have the following code as the first lines in my main function:  after start the following happens: 20240304 20:14:50,252:INFO:Load pretrained SentenceTransformer: allMiniLML6v2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown warnings.warn('resource_tracker: There appear to be %d ' Process finished with exit code 139 (interrupted by signal 11: SIGSEGV) It used to work well, but after recent system upgrade fails. Hardware Overview: Model Name: MacBook Pro Model Identifier: Mac14,9 Model Number: MPHE3LL/A Chip: Apple M2 Pro Total Number of Cores: 10 (6 performance and 4 efficiency) Memory: 16 GB System Firmware Version: 10151.81.1 OS Loader Version: 10151.81.1 Activation Lock Status: Disabled Also consider system crash report: Translated Report (Full Report Below) Process: Python [2020] Path: /Library/Frameworks/Python.framework/Versions/3.11/Resources/Python.app/Contents/MacOS/Python Identifier: org.python.python Version: 3.11.6 (3.11.6) Code Type: ARM64 (Native) Parent Process: pycharm [756] Responsible: pycharm [756] User ID: 501 Date/Time: 20240304 20:20:38.0394 0800 OS Version: macOS 14.3.1 (23D60) Report Version: 12 Anonymous UUID: 66A06743DA8EFC87662FA0460A49AFF9   Versions Collecting environment information... PyTorch version: 2.2.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.3.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: Could not collect Libc v",2024-03-05T15:55:59Z,high priority needs reproduction module: crash triaged module: macos,open,0,4,https://github.com/pytorch/pytorch/issues/121235,Maybe try reinstalling pytorch after the system upgrade?,"I reinstalled pytorch several times, creating new python environment, it didn't help.","This looks like a racecondition where tensor data is freed even there it's still retained by IDE. Assigning to myself in an attempt to get a repro, as usually it has nothing to do with PyCharm, but rather with wrong expectation about object lifetime, for example see reproducer to a previous crash:  CC(torch.nonzero crashes if invoked from multiple threaded)issuecomment1713015452",Same problem on almost the same hardware (but on a MacBook Pro with M3 (2*6 cores and 18 GPU cores) with 36GB. Last message was:  **20240401 15:41:56  Load pretrained SentenceTransformer: allMiniLML6v2** Anything I can do about this?
transformer,Detailed documentation with an example for the causal mask in nn.TransformerEncoder," üìö The doc issue Currently there is not much detail present as to how causal_mark=True in nn.TransformerEncoder masks the attention scores. Does one need to also pass a masking array? what should be its shape given an input of (batch_size, seq_len, d_embed)? I am currently trying to build a nanoGPT using nn.TransformerEncoder and getting really confused with this.  Suggest a potential alternative/fix Please include a documentation with a small example.  ",2024-03-05T02:54:03Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/121193
llama,The result of BF16 to FP32 is different in class LlamaRMSNorm training twice.," üêõ Describe the bug The input and weight are same absolutely, but the outputs are different sometimes.   Versions PyTorch version: 2.1.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.28.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01056azurex86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100 80GB PCIe GPU 1: NVIDIA A100 80GB PCIe GPU 2: NVIDIA A100 80GB PCIe GPU 3: NVIDIA A100 80GB PCIe Nvidia driver version: 535.154.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Byte Order:                         Little Endian Address sizes:                      48 bits physical, 48 bits virtual CPU(s):                             96 Online CPU(s) list:                095 Thread(s) per core:                 1 Core(s) per socket:                 48 Socket(s):                          2 NUMA node(s):                       4 Vendor ID:                          AuthenticAMD CPU family:                         25 Model:                              1 Model name:                         AMD EPYC 7V13 64Core Processor Stepping:                           1 CPU MHz:                            2445.438 BogoMIPS:   ",2024-03-05T02:13:19Z,needs reproduction module: numerical-stability triaged module: bfloat16,open,0,3,https://github.com/pytorch/pytorch/issues/121190,"Can you please share an input shapes/values and what error are you seeing? As floating point operations are inherently inaccurate, this might be an expected behavior or indeed a problem in the implementation.",j could you please follow this one ?,Sure.  Could you please share a simple reproducer? I can help investigate whether the issue is an expected accuracy or indeed a problem.
transformer,MPS memory leak in training," üêõ Describe the bug When using transformers Trainer on MPS hardware, after several hundred iterations, getting ""MPS out of memory"". Test code and output at: https://gist.github.com/dmahurin/fe202511a1b3314faf248d728cdb1d71 Note that when training data is of varying data lengths, the condition seems to reproduce more. While training, MPS allocated memory seems unchanged, but MPS backend memory runs out.  Versions Collecting environment information... PyTorch version: 2.3.0.dev20240122 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.2.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: version 3.28.3 Libc version: N/A Python version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang1500.1.0.2.5)] (64bit runtime) Python platform: macOS14.2.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Versions of relevant libraries: [pip3] mypy==1.7.1 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.23.5 [pip3] onnx==1.15.0 [pip3] onnxruntime==1.16.3 [pip3] opencliptorch==2.20.0 [pip3] pytorchlightning==1.9.4 [pip3] pytorchmemlab==0.3.0 [pip3] torch==2.3.0.dev20240122 [pip3] torchmlir==20240127.1096 [pip3] torchdiffeq==0.2.3 [pip3] torchmetrics==1.3.0.post0 [pip3] torchsde==0.2.6 [pip3] torchvision==0.18.0.dev20240122 [conda] Could not collect Also: [pip3] transformers==4.37.2 ",2024-03-04T05:13:43Z,high priority module: memory usage triaged module: mps,closed,0,4,https://github.com/pytorch/pytorch/issues/121113,"Note that if padding_option is set to 'max_length', the MPS gpu allocations go up, but it seems to avoid the MPS backend out of memory. So this is perhaps a nonoptimal workaround.","I'm seeing something very similar during classification with a bert based model. If I set the padding_option to 'max_length' there is no memory leak, but if not, the memory usage skyrockets quite quickly.","Hi, I haven't dug into too much details on this.. Do you know what `padding_option=max_length` do in the transformers package ? I am curious if there is dynamic slicing going on which is causing leak somewhere.","Hello, a fix for this issue will be available in a future update of MacOS"
transformer,UnsupportedOperatorError: Exporting the operator ::_transformer_encoder_layer_fwd to ONNX opset," üêõ Describe the bug when i try to export onnx model from fairseq, i encountered this error. plz help support this operator   Versions PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.27 Python version: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01062.12.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.55",2024-03-04T03:05:52Z,module: onnx triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/121109,torch.onnx.export doens't support this operator and we don't have plans to support it. Please try the new ONNX exporter and reopen this issue if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial
transformer,[ROCm] Add cublasGemmAlgo_t -> hipblasGemmAlgo_t,This PR is to add cublasGemmAlgo_t > hipblasGemmAlgo_t to cuda_to_hip_mappings.py. It is required for DeepSpeed transformer extension build on ROCm. ,2024-03-01T18:56:02Z,module: rocm open source Merged ciflow/trunk topic: not user facing ciflow/rocm,closed,0,14,https://github.com/pytorch/pytorch/issues/121030,need to update the xpass plz, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `add_hipblasGemmAlgo_t` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout add_hipblasGemmAlgo_t && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `add_hipblasGemmAlgo_t` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout add_hipblasGemmAlgo_t && git pull rebase`)"," This is a very minor change, so I'm issuing a merge command assuming CI jobs should pass fine.  merge"," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.8gcc11noops / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,OOM with mixed precision with OPT1.3B Decoder," üêõ Describe the bug I am doing simple forward pass on HF OPT1.3B model. With (1, 2048) input size, FP32 forward pass works, but same model same input with mixed precision throws OOM on 40GB A100. I tried the same on 48GB A6000 and same happens with (2, 1664)  input size. Furthermore, with (1, 1024) input size 100 forward passes  FP32 took 21 seconds using 23GB memory, fp16 4.74 seconds using 27GBs memory. Simple code to reproduce the issue.  But this works  Is it expected to see more memory usage with fp16 prediction? Thanks.  Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.4 Libc version: glibc2.35 Python version: 3.10.10  (main, Mar 24 2023, 20:08:06) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.087genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             40 Online CPU(s) list:                039 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Xeon(R) CPU ",2024-03-01T14:04:06Z,triaged module: amp (automated mixed precision),open,0,0,https://github.com/pytorch/pytorch/issues/120994
yi,torch._C._LinAlgError in torch.cholesky_inverse," üêõ Describe the bug Code:  Output:   Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.01015azurex86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 535.161.07 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      43 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             64 Online CPU(s) list:                063 Vendor ID:                          AuthenticAMD Model name:                         AMD Ryzen Threadripper 3970X 32Core Processor CPU family:                         23 Model:                              49 Thread(s) per core:                 2 Core(s) per socket:                 32 Socket(s):                          1 Stepping:                           0 Frequency boost:                    enabled CPU max MHz:                        3900.0000 CPU min MHz:                        2200.0000 BogoMIPS:                           7799.91 Flags:       ",2024-03-01T11:47:52Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/120991
yi,Include a warning in cuda get_allany_iter,"Fixes CC(The result of torch.all is inconsistent between CPU and GPU) The root cause of the discrepancy between CPU and GPU is in this line https://github.com/pytorch/pytorch/blob/63b259492a8e5519edb45bb34c5b2a275baebf63/aten/src/ATen/native/ReduceOps.cppL1536 . 1. torch.all on CPU will explicitly cast everything into bool. However, the cast is done by `Casting complex values to real discards the imaginary part`. Therefore, a pure imaginary number is treated as zero and thus false. 2. torch.all on cuda does dynamical casting, anything nonzero, including pure imaginary numbers treated as True. Example1  Example2  The CUDA dynamic is aligned with the native python, behavior below  It is the CPU version that needs to be corrected, which involves modifying the https://github.com/pytorch/pytorch/blob/498a94a7f5b4426313607313557835bf0625c268/aten/src/ATen/native/Copy.cppL300 or create an overload for the complex type which is consumed by reduction.",2024-03-01T07:12:51Z,triaged open source Stale,closed,0,5,https://github.com/pytorch/pytorch/issues/120977, mind tagging the relevant reviewers? thanks, /  ," As specified in the issue, rather than a warning, we'd accept a fix that follows python's bool conversion behavior for complex numbers (i.e. treat `0+0j` as False and everything else as True).","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",The relevant issue was fixed at https://github.com/pytorch/pytorch/pull/121803
rag,fsdp: hacky poc to functionalize storage resizing while saving allgathered weights for bw,"Not all problems are fixed yet and this PR contains many hacks, but pushing this out before I go on PTO as a starting point / proof of concept. Detailed description of the issues encountered. This PR attempts to functionalize resizes from tracing ppFSDP. Repro I've been working off of (from ): P1190991292 (I rebased this commit on top of the existing fsdp stack: https://github.com/pytorch/pytorch/pull/120442 Outputs from the repro: Dynamo forward graph: P1191009335 Functionalized forward graph: P1190991754 Backward graph from AOTAutograd (before compiled autograd runs): P1190991999 Compiled autograd graph from dynamo: P1191009635 Backward graph, after compiled autograd, after functionalization (wrong): P1190992138 **Current state** The repro generates a properly functionalized forward graph, that also includes a `param.storage_resize(0)` at the end of the graph. It fails during functionalization of the backward for reasons that I'll describe below. **Individual changes / problems ran into** I ran into many problems, but I would say that (7) and (8) below are the most interesting / hardest to solve. **(1)** `inductor.resize_storage_bytes_` is functionalized This op previously did not have alias annotations, so I gave it proper annotations, added a functional variant of the op, and wrote a functionalization rule for it. One interesting thing to note is that the functional variant has **very** strange semantics: `y = functional_resize(x, size)` returns a fresh tensor y that has the **same** size/strides as x, but a `storage().size()` of `size`. We need to properly maintain this invariant in our fake tensor logic during tracing, or else we r",2024-03-01T04:42:55Z,module: inductor module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/120971,"This PR is not really ready for review (still broken), but the description should hopefully give a good overview of the aliasing/mutation issues with tracing fsdp","(8) (a) > when fsdp closes over a param, and also saves an alias of that param for backward, are we guaranteed that there is a ""simple"" view relationship between the two? That depends on which ops are in the graph and what their derivatives save for backward Maybe for existing PyTorch ops it is generally true that the saved alias of an input is a ""simple"" view of the input? From another angle, perhaps it‚Äôs just a matter of partitioning between FWD and BWD graph, and the partitioner should always make the original input (instead of the alias) as part of the FWD graph output, and leave the view op at the beginning of the BWD graph. (Assuming view ops are cheap in general, moving it to BWD graph shouldn‚Äôt be costly.) (8) (b) > but if you e.g. perform a mutation on¬†`view1`¬†that affects the autograd metadata of¬†`view2`¬†in a user visible way, we would be on the hook for ensuring that the user gets to see¬†`view2`'s autograd metadata get mutated properly. Curious what are the potential autogradmetadatachanging mutations that user can do in their program? For inplace ops, it seems that doing `view1.add_(‚Ä¶)` will throw `a view of a leaf Variable that requires grad is being used in an inplace operation` (thus banned), and if they do `with torch.no_grad(): view.add_(‚Ä¶)` we know it won‚Äôt affect autograd. Given this, maybe it‚Äôs safe enough?","subsumed by https://github.com/pytorch/pytorch/pull/122434, see that PR descr for more details"
gpt,Capture primitive data type arguments for profiling python_function,"RECORD_FUNCTION in python_function only captures argument that is a Tensor. However, it is very common for user to use non tensor arguments in custom ops, for example, sequence length in GPT attention custom op. My previous PR tries to capture all nontensor arguments, it turned out in some cases, it is very expensive. This PR is to support primitive (or its container) arguments in RECORD_FUNCTION.",2024-02-29T22:11:58Z,fb-exported Merged ciflow/trunk release notes: jit,closed,0,6,https://github.com/pytorch/pytorch/issues/120949," , I addressed your code review comments, please take a look. Thanks!",The CPU overhead issue looks resolved now in my use case! Thanks!," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D54525618, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,Capture primitive data type arguments for profiling python_function,"RECORD_FUNCTION in python_function only captures argument that is a Tensor. However, it is very common for user to use non tensor arguments in custom ops, for example, sequence length in GPT attention custom op. My previous PR tries to capture all nontensor arguments, it turned out in some cases, it is very expensive.  This PR is to support primitive (or its container) arguments in RECORD_FUNCTION.",2024-02-29T20:11:37Z,release notes: jit,closed,0,2,https://github.com/pytorch/pytorch/issues/120935,"   :x: The email address for the commit (610174f3c9d8847e488b4fe616cc79a781a8a9fb, bd508337a1379ef47ee8c35047c6163b50a4d9c7) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.",no email 
llava,[dynamo] `CustomizedDictVariable` for HF ModelOutput behaves different when attr value is None," üêõ Describe the bug In CC([dynamo] implement custom dict variable as a general solution for HF's ModelOutput class), support for HF `ModelOutput` was moved from `DataClassVariable` to `CustomizedDictVariable`. However, the new implementation didn't consider the `ModelOutput` behavior of 'when value is None, only set it as attribute, but skip updating it in dict' as defined in their custom `__setattr__`:  Hence all items whose values are None are ignored during `to_tuple` and `__getitem__(idx: int)`. Where on the contrary dynamo traced graph will not ignore None values. This behavior was noted and covered in previous `DataClassVariable` implementation. https://github.com/pytorch/pytorch/blob/a75de6db4f5ea3173a4a549453be04267cacca7a/torch/_dynamo/variables/dicts.pyL350L354 This leads to incorrect traced graphs when user code does the combination of setting None attributes and accessing via `to_tuple` or getitem via index. This is discovered from huggingface Llava model. Repro:  > dynamo output:  None eager output:  tensor([[[[0.4403]]]])  Versions main  ",2024-02-29T18:04:23Z,high priority triaged oncall: pt2 module: dynamo dynamo-must-fix,closed,0,0,https://github.com/pytorch/pytorch/issues/120923
rag,Graph break on tensors without storage,  CC(Graph break on tensors without storage)  CC([dynamo] Graph break when faking named tensors ),2024-02-29T14:18:15Z,topic: not user facing ciflow/inductor keep-going,closed,0,0,https://github.com/pytorch/pytorch/issues/120906
transformer,FSDP: automatic instantiation on GPUs (aka zero.Init)," üöÄ The feature, motivation and pitch moving from slack discussion  How can FSDP shard a model directly to GPUs during model instantiation? This is critical for instantiating huge models.  Alternatives In Deepspeed this exists since day 0 of the framework using `zero.Init` This feature is integrated into HF Transformers' `from_pretrained` that instantiates the model directly on GPUs  here There is an emerging need to control when this should be activated and when not as we now have many situations with multiple concurrent models and this current setup no longer works well.So I thought that HF Transformers should gather at least some needs of various frameworks  Deepspeed ZeRO, FSDP and may be others where `from_pretrained` provides hooks into the model instantiation stage. The 2 needs are:     1. yes/no activate flag   2. framework provided context to instantiate in  so `zero.Init` would be just one of them The intention is that the user won't need to do anything special when they create the model. The framework would register with HF Transformers everything it needs to do it smoothly and correctly.  Additional context on slack Andrew Gu posted: > If you specify the  `device_id` arg, FSDP should move the unsharded module to GPU before sharding so that the sharding ops are fast (but still requiring CPU init and copy H2D). Otherwise, the only other way to initialize _and_ shard on GPU is to do socalled ‚Äúmanual wrapping‚Äù, which was the common approach from Fairscale. I do not think that would work with HF transformers. > For manual wrapping, you have to call `FullyShardedDataParallel(module, ‚Ä¶)` on `module` immediately after it was construct",2024-02-29T04:11:12Z,triaged needs research module: fsdp,open,21,5,https://github.com/pytorch/pytorch/issues/120878," Have you tried FSDP's (admittedly limited) support for meta device to instantiate the model directly on GPU? For finetuning use cases, you can do something like the following:  > The intention is that the user won't need to do anything special when they create the model Is it possible to offer some sort of interface / wrapper in upstream libraries to hide this complexity from the user?","varma `load_state_dict` cannot be used like this for the sharded model, it will require loading a distributed checkpoint (DCP), converting a `torch.save` checkpoint, or summoning the full params before loading (on CPU  provided you have enough RAM). But we've been very successful with this setup in our experiments. > Is it possible to offer some sort of interface / wrapper in upstream libraries to hide this complexity from the user? Since you asked, Lightning Fabric and PyTorch Lightning (disclaimer: I work on this) do this already: https://lightning.ai/docs/fabric/stable/advanced/model_init.htmlmodelparalleltrainingfsdpanddeepspeed. Including support for loading full and sharded checkpoints.",", would you like to get involved in this discussion? As it'd be HF's Accelerate's job to handle this, so your input would be most relevant  please tag others on your team as need be.  And probably opening an Issue on Accelerate's side to track.","varma, yes, this is why I'm trying to discuss this. Since HF Transformers already does a special usecase for Deepspeed ZeRO, so its sharding to GPUs is automatic for users, but as we now have other similar use cases  I think that hardcoded solution needs to be upgraded to be supportive of various usecases. So I think FSDP's need integration would be a perfect usecase. I have tagged Sourab in the comment above who took over the initial work I did at HF Transformers 3 years ago. So my intention was to spec out what FSDP needs and then take it to HF Transformers side for proper integration. as  shared above Lightning has already solved all of these cases in their frameworks. And regardless let's make sure that this usecase is well documented in the FSDP docs.","Similar to  CC(FSDP: unsharded gradients accessor)issuecomment1977140356, I wanted to share what I was thinking as an option for metadevice init in the new FSDP. The PR is https://github.com/pytorch/pytorch/pull/120351. The idea is that, by representing FSDP sharded parameters as `DTensor`s, we can rely on `DTensor`'s op dispatch to compute the correct randomness with respect to the global (unsharded) shape. As such, we can shard a metadevice model with TP and FSDP, leaving it on metadevice; materialize the sharded model on GPU (e.g. using `to_empty(device)`); and then either load from a checkpoint or randomly initialize the sharded metadevice parameters through `DTensor` dispatch. For example, taken from that PR: "
llm,[PyTorch] Add test that canEnableStaticRuntime rejects prim::CallMethod,"  CC([PyTorch] Add test that canEnableStaticRuntime rejects prim::CallMethod) Rejecting prim::CallMethod is called out in a comment in impl.cpp, but doesn't seem to be tested. Now it is. Differential Revision: D54338261",2024-02-29T00:01:58Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/120853,This pull request was **exported** from Phabricator. Differential Revision: D54338261,This pull request was **exported** from Phabricator. Differential Revision: D54338261,This pull request was **exported** from Phabricator. Differential Revision: D54338261,This pull request was **exported** from Phabricator. Differential Revision: D54338261,This pull request was **exported** from Phabricator. Differential Revision: D54338261,This pull request was **exported** from Phabricator. Differential Revision: D54338261,This pull request was **exported** from Phabricator. Differential Revision: D54338261, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ",This pull request was **exported** from Phabricator. Differential Revision: D54338261, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,about tensor.new(storage) issue," üêõ Describe the bug In pytorch 2.1.1+cu118, in this code   the out's size is not equal storage ,but in pytorch 1.13.x, they are same, I guess maybe method of tensor.new(storage) is changing or just a bugÔºåso I issue it.   Versions pytorch 2.1.1+cu118 ",2024-02-28T11:01:01Z,needs reproduction triaged module: python frontend,open,0,5,https://github.com/pytorch/pytorch/issues/120791,"Sorry, I don't quite understand what the bug report is here? Do you mind clarifying a bit  thanks! Is this related to `torch/utils/data/dataloader.py`?","Sorry, it's my fault. I mean I want use tensor to new storages in memory, but  the shap of ""storage"" is not equal with ""out"" in this code. Actually, before pytorch 1.13, they are same and this code is just normally data processing. So I guess it's coused by the method of ""tensor.new()"". I hope you could understand it and thank you!","If the behavior changed in 1.13, do you have a reproducible code snippet that can show the behavior change?",I can not reproduce it (as I understand the problem) ,"sorry, I'm busy in this term, and I capture the photo, you can see it's crazying. img1.png‚Ä¶ The left version is torch 1.13 and the right version is 2.1.1.  It's also amzing that the version of 2.1.0 is ok."
yi,MultiheadAttention returns incorrect results for windowed long causal mask for long input," üêõ Describe the bug Note, though the error is small (about 2e8), but it will be magnified by other network layers, and it will eventually become large enough.  Output (should always be True):   Versions Collecting environment information... PyTorch version: 2.1.2 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 10.5.01ubuntu1~22.04) 10.5.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.089genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 545.23.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.971 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.26.0 [pip3] pytorchlightning==2.1.4 [pip3] torch==2.1.2 [pip3] torchaudio==2.1.2 [pip3] torchmetrics==1.2.0 [pip3] torchtnt==0.2.1 [pip3] torchvision==0.16.2 [conda] blas                      1.0                         mkl   [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] libjpegturbo             2.0.0                h9bf148f_0    pytorch [conda] mkl                       2023.1.0         h213fc3f_46344   [conda] mklser",2024-02-28T09:43:19Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/120790
transformer,Tensor Parallel Inference Performance Issue with Llama-7b-chat-hf Model caused by straggler problem," üêõ Describe the bug Hello PyTorch Team, I am currently attempting to leverage Tensor Parallelism for inferencing the Llama7bchathf model. However, I've encountered a significant bottleneck related to server straggler issues, leading to an absence of expected speedup from tensor parallelization. The core of the problem seems to lie in the AllReduce operation, where all ranks are forced to wait for the completion of the slowest rank. Consequently, each AllReduce operation incurs a delay exceeding 1ms. My experimental environment consists of a server equipped with 8 V100 GPUs, interconnected via NVLink. To illustrate the issue, I am providing two nsight profile results comparing the performance of parallel inference across 4 V100 GPUs, with and without compilation. The profiles indicate a consistent issue where one rank experiences significant latency in kernel launching, thereby causing delays in the AllReduce operation for other ranks. The AllReduce operators are highlighted for ease of reference. Below is the code snippet employed for this setup. Could you please advise if there are any misconfigurations or suggest potential optimizations to mitigate this issue? !image !image Following is the code I use, Is there any wrong configuration of it?  You can run the code by following command:   Versions  ",2024-02-28T07:49:18Z,oncall: distributed,open,0,3,https://github.com/pytorch/pytorch/issues/120787,> a consistent issue where one rank experiences significant latency Is it always the same rank that's slow? Another thing to rule out is machine issues  wonder does it repro if we change to a different server machine?, wondering if you also tried torch.compile the model after applying tensor parallel?,"To be a bit clear, I think the reason why one rank is delaying a lot is because for some ranks the CPU thread is too slow to feed GPU enough work, for inference to work performantly with reasonable toks/sec (with/without Tensor Parallel), it would be preferred to use torch.compile to get rid of the CPU slowness.  You can take a look at this blogpost about many optimizations you can do to accelerate inference for llama 7B model https://pytorch.org/blog/acceleratinggenerativeai2/  "
yi,Trying doing static_quantization with Pytorch 2.1.0 torch.ao.quantization," Issue description Using torch.ao.quantization to do Posttraining Static Quantization , didn't show error but it's not change the model layer to quantized layer(ex:  nn.Conv2d is still Conv2d after running my code) Please Help Me with this problem!!!!!   Thank you!  Code example def static_quantize(weight_path, qconfig_mapping):     weights, imgsz = opt.weights, opt.img_size      load Pruned_model           device = select_device(opt.device)     device = torch.device(""cpu"")      Load model     pruned_model = attempt_load(weights, map_location=device)   load FP32 model (after pruning from other code)     stride = int(pruned_model.stride.max())   model stride     imgsz = check_img_size(imgsz, s=stride)   check img_size     pruned_model.eval()     if weight_path != '1234':   get pruned_model's weights if path is not default string""1234""          pruned_model.load_state_dict(torch.load(weight_path),False)   load pruned_model.pt       ""input""  ""output"":  Add  QuantStub , DeQuantStub     pruned_model = QuantStub()(pruned_model)     pruned_model = DeQuantStub()(pruned_model)           pruned_model = prepare(pruned_model, qconfig_mapping)     quantized_model = convert(pruned_model, inplace=False)      save model's weights     torch.save(pruned_model.state_dict(), ""static_quantized_original.pt"")      print(""Static Quantization Success!!!!"")         return quantized_model if __name__ == ""__main__"":     device = select_device(opt.device)      create static_quantization_config     qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')     qconfig_dict = {'': qconfig}     qconfig_mapping = QConfigMapping.from_dict(qconfig_dict)      operate qu",2024-02-27T11:42:21Z,oncall: quantization,open,0,2,https://github.com/pytorch/pytorch/issues/120705,"What are the contents of `pruned_model` originally and after you call `prepare` and `convert`? Can you try with a simpler example, say with one Conv2d layer and see if that works for you?","> What are the contents of `pruned_model` originally and after you call `prepare` and `convert`? >  > Can you try with a simpler example, say with one Conv2d layer and see if that works for you? (1) My Pruned_model is a YOLO model with pruned 1*1 Conv2d layer(**didn't change the model's architecture**)Ôºõor just change the activation function  (2) I have created a simple_Model with only one Conv2d layer , and Use the code below to check whether it work:  def check_quantization(model, output_file):     num_modules_to_print = 5     with open(output_file, 'w') as f:         num_printed_modules = 0         for name, module in model.named_modules():             if isinstance(module, nn.Conv2d):                 f.write(f""Module {name} is a Conv2d layer.\n"")                 f.write(f""Quantized: {isinstance(module, nnq.Conv2d)}\n\n"")             elif isinstance(module, nn.Linear):                 f.write(f""Module {name} is a Linear layer.\n"")                 f.write(f""Quantized: {isinstance(module, nnq.Linear)}\n\n"")              Add more checks for other quantized layers if needed              Print some example layers for inspection             if num_printed_modules < num_modules_to_print:                 f.write(f""+++++++++Module {name}:\n"")                 f.write(f""+++++++++{module}\n\n"")                 num_printed_modules += 1  **But I always got the outcome below:** +++++++++Module : +++++++++SimpleModel(   (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) Module conv is a Conv2d layer. Quantized: False"
transformer,add a note wrt torch.nn.functional.scaled_dot_product_attention,followup change of https://github.com/pytorch/pytorch/pull/120565   Added a note in the transformer class pointing out the mask definition is opposite to that of :attr:`attn_mask` in             torch.nn.functional.scaled_dot_product_attention.  ,2024-02-26T23:33:26Z,triaged open source Merged ciflow/trunk release notes: nn topic: docs,closed,0,5,https://github.com/pytorch/pytorch/issues/120668, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxfocalcuda12.1py3.10gcc9 / test (default, 3, 5, linux.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""foreach add test failure unrelated to doc fix"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,"2 Dynamo test are failing with ""Failed running call_method sin(*(FakeTensor(..., size=(), grad_fn=<CosBackward0"".","2 Dynamo test are failing with ""Failed running call_method sin(*(FakeTensor(..., size=(), grad_fn= Click me   ",2024-02-26T21:17:52Z,triaged oncall: pt2 module: dynamo dynamo-must-fix,open,0,1,https://github.com/pytorch/pytorch/issues/120649,"Hi , we are scrubbing old issues. Are you still working on this issue? (Feel free to unassign if not)."
rag,Leverage DLPack-based construction of PyTorch tensors to avoid costly element-by-element copy," üöÄ The feature, motivation and pitch Consider the following statement  where `other_array` is an array instance constructed by another array programming framework. If this other framework is principally designed around the DLPack array exchange protocol, something very bad happens. Basically PyTorch ignores the DLPack interface (`other_array.__dlpack__`) altogether. Furthermore, if ``other_array`` exposes the sequence protocol (``__len__`` and ``__getitem__``), PyTorch will perform a bruteforce elementwise copy potentially requiring hundreds of millions of separate PCIexpress transactions to copy individual floating point values from the GPU. To the user, it will seem that the application has crashed because nothing happens and attempting to interrupt the Python kernel doesn't work since this is all done on the C++ side. Fortunately PyTorch supports the DLPack protocol to efficiently exchange tensors with other libraries. But it only does so when the user creates the arrays in a sort of awkward way:  It's easy to forget to do this, with extremely unpleasant results. It would be very easy easy for PyTorch to _check_ if `other_array` implements the DLPack protocol and then simply to switch to this construction method. I will create a separate PR proposing a prototype of this idea.  Alternatives N/A  Additional context _No response_",2024-02-26T15:45:06Z,module: performance triaged module: dlpack,closed,0,0,https://github.com/pytorch/pytorch/issues/120614
transformer,Torchrun arguments shadowing script arguments," üêõ Describe the bug Consider a minimalistic script as follows:  Running it directly gives the following:  (see the default r value now being updated to 16) Running it using torchrun:  The reason for this is argparse's abbrevation handling which shadows any argument with the prefix of an existing torchrun argument. For example, with ""n"" which matches all torchrun flags which starts with n and so on. This behaviour exists on the current main. The fix is straightforward: set ""allow_abbrev"" to False when creating the torchrun ArgumentParser. I have tested this locally and found it to work and can open a PR if there is consensus that this is a bug and no existing workflow will be broken due to this.  Versions Collecting environment information... PyTorch version: 2.2.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Fedora release 39 (Thirty Nine) (x86_64) GCC version: (GCC) 13.2.1 20230918 (Red Hat 13.2.13) Clang version: Could not collect CMake version: version 3.27.7 Libc version: glibc2.38 Python version: 3.12.0 (main, Oct  2 2023, 00:00:00) [GCC 13.2.1 20230918 (Red Hat 13.2.13)] (64bit runtime) Python platform: Linux6.5.6300.fc39.x86_64x86_64withglibc2.38 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:     ",2024-02-26T08:55:49Z,triaged module: regression module: intel,open,1,3,https://github.com/pytorch/pytorch/issues/120601,jeancho please help debug this one!,This seems to be a nasty issue that showed up on 2.3.0 and breaks 2.2.2 scripts.,"Yeah, this is pretty bad because of the 2.3.0 changes mean you can't use torchrun on any script that use `log`, `logs` or any other prefix of `logs_spec`."
peft,[FSDP] Clean missing and unexpected keys,"  CC([FSDP] Clean missing and unexpected keys) Currently, when loading w/strict=False or w/strict=True and looking at error message, FQNs are garbled w/FSDP details such as ""_fsdp_wrapped_module"". This makes it tricky for upstream applications to validate the expected set of keys are missing / unexpected (for example with PEFT where state_dict is loaded nonstrict), and makes error message more complicated w/FSDP details. This PR cleans those prefixes by using `clean_tensor_name` in FSDP's existing post load_state_dict hooks. Currently, only full_state_dict impl is tested, can test the rest of the impls as follow up work. Differential Revision: D54182472 ",2024-02-26T08:42:16Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/120600,This pull request was **exported** from Phabricator. Differential Revision: D54182472," merge f ""CI done"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," varma sorry for hijacking this issue, but I notice that there could be some opportunities to improve the FSDP implementation when CPUoffload is enabled. Some of them are already documented in the code. Potentially if we could pipeline the GPU memory IO for loading the upcoming layers forward / backward while computing the current one, we could potentially improve the throughput of CPUoffload.  However the implementation could be fraught with synchronization complexities. I was wonedering if there are current plans to handle these things?"," Are you referring to `cpu_offload=CPUOffload(offload_params=True)` for parameter CPU offloading, or are you referring to offloading state dict states to CPU? For the former, there should be overlap already since we use a separate CUDA stream for copy the parameters hosttodevice."
llm,skip three pyhpc models with dynamic shape test,"  CC(skip three pyhpc models with dynamic shape test) As reported in  CC([inductor][cpu]6 models functionality gap between AMP dynamic shape and AMP static shape), `pyhpc_isoneutral_mixing`, `pyhpc_equation_of_state` and `pyhpc_turbulent_kinetic_energy` failed with dynamic shape testing, we propose to skip the dynamic batch size testing of these 3 models in this PR. * Error msg is  * Root Cause is   *  Benchmark code will only annotate the inputs' dim as dynamic when its size equals to batch size https://github.com/pytorch/pytorch/blob/c617e7b4076a5f968f5827040a07b013e45cd0c6/benchmarks/dynamo/common.pyL3867L3871. If it fails to find any dim equals to batch size, above error throws.   * However, for these 3 models, none of the inputs' dim will equal to input batch size since the relationship of dim sizes      * Another thing is `pyhpc_isoneutral_mixing`, `pyhpc_equation_of_state` can pass the dynamic batch size accuracy testing, because the batch size has been set to 4 in accuracy testing (https://github.com/pytorch/pytorch/blob/c617e7b4076a5f968f5827040a07b013e45cd0c6/benchmarks/dynamo/common.pyL3456) and `math.ceil(2 * size ** (1/3))` happens equaling to 4.  * Since the dim sizes of input has above relationship, running the these models in dynamic shape, we may need to annotate `dim0 = dim2 * 8`, per the discussion in  CC([PT2] Failed to capture graph in export with dynamic shape inputs for LLM models)issuecomment1897108756 , looks like we are not expressible for this case. So, I think we may need to skip the dynamic batch size testing for these 3 models. ",2024-02-26T07:59:01Z,open source Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/120599, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Functionalization of `as_strided` disregards its base tensor storage.," üêõ Describe the bug In the program below, I try to **retrieve the original tensor** by calling `as_strided`, after creating a view with **different `storage_offset`**. In theory, it should work, given that `y` is a view of `x`.   From the error above, we can clearly see that `as_strided` is only considering the storage of the input tensor. Since the lazy backend uses `FunctionalTensorWrapper`, the view aspects are controlled by it. The real tensor (wrapped one) is unaware of such a thing.  Versions PyTorch version: 2.3.0a0+gited6660e  ",2024-02-25T14:31:09Z,triaged module: functionalization,open,0,6,https://github.com/pytorch/pytorch/issues/120585,"Given all this information, I thought that the functionalization kernel of `as_strided` should redispatch, passing the base tensor (that lives in the storage of `FunctionalTensorWrapper`) as first argument (check out this draft PR). However, that brings a new problem: we have no notion of the input tensor (the view) `storage_offset`. That is because the input tensor (a view from `FunctionalTensorWrapper` perspective) is not actually a view from the backend perspective.","One solution I can think of is to possibly redispatch `as_strided` twice:  1. Using the view as first argument 2. If that fails, use the base tensor as first argument This solution would work with the example in the OP. However, there's still a problem with this solution is if the function `foo` was the following:   (1) would fail because it doesn't have enough storage space (6 elements instead of 8)  (2) would not fail, but return an incorrect result since the `storage_offset` of both the view and the base tensor is 0 That said, this solution would be an improvement over what we have now. Let me know what you think.","The ability for `as_strided` to look into the underlying storage and then make a disjoint view on it is kind of cursed and so I expect a solution to this to be kind of cursed too. I think  really needs to chime in here, but I think the way I would go about doing this properly is ensuring that functionalization is keeping track of accurate ""what the storage would have looked like in eager mode"" (which I'm not sure we do right now, to some extent we try to preserve the striding of the functionalized tensors, maybe this works); then, when we call `as_strided`, we actually perform the operation on the storage ""tensor"" (which we have been faithfully keeping track of and has all the information we need.) I don't like the hacky solution you proposed because it will generate silently incorrect results. In general, that's worse than just failing.","> is keeping track of accurate ""what the storage would have looked like in eager mode"" I thought we weren't, but after I took another look at it, I think we are doing that. That's probably what `set_sizes_strides_offset` in the functionalization backend does. Given that, the question is: why isn't the storage offset preserved. I think the answer lies in the backend (TS and PyTorch/XLA). I will take another look at it.","(belated reply)  talked to  offline. He pointed out that FunctionalTensorWrapper is already maintaining faithful eager info about sizes/strides during functionalization already, but it's failing to plumb that info to the inner tensor which is what we really want. He's going to try a PR and we can see if there's any CI fallout","Said PR: CC(Set size, stride, and offset of functional tensor.)"
transformer,Add the hyperlink of the transfomer doc,"Fixes CC(Better documentation on _mask and _key_padding_mask in transformers modules)   The shape for forward pass is clearly stated in the main transformer class  Boolean mask for _key_padding_mask is also explained in the main transformer class. Therefore, add the hyperlink to the transformer class explicitly so the user can refer back to the main class. Also, correct several symbols in the transform doc from normal text style to math style.",2024-02-24T21:35:08Z,triaged open source Merged ciflow/trunk release notes: nn topic: docs,closed,0,5,https://github.com/pytorch/pytorch/issues/120565,"> Thanks a lot! >  > re the mask discrepancy mentioned on the initial issue >  > > Also, a clarification on why one uses a boolean mask for _key_padding_mask with True values indicating there's padding there and False there's no padding which is actually the opposite of other frameworks. >  > As you mentioned, the mask definition is documented in the Transformer docs. However, would you be willing to add a `..note::` in the Transformer class that for a boolean mask, `True` means that a value is not included in the attention which is the opposite of what it means in `torch.nn.functional.scaled_dot_product_attention` please? >  > (If it is easier for you feel free to do this in a follow up) Will do."," label ""topic: docs"""," label ""release notes: nn"" ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,TransformerEncoder/Decoder: add type hints,,2024-02-24T10:00:46Z,triaged open source Merged ciflow/trunk release notes: nn topic: docs,closed,0,2,https://github.com/pytorch/pytorch/issues/120550, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,make GPT2ForSequenceClassification pass inference accuracy check,  CC(make GPT2ForSequenceClassification pass inference accuracy check) We need a higher tolerance for GPT2ForSequenceClassification since if I change bfloat16 in  to float16 or float32 it will pass the accuracy check. Adding freezing can also make the test pass for this model. I think that's may be due to different fusion output being generated (depending on if constant propagation is happening controlled by freezing) and cause some small numerical difference. ,2024-02-23T23:05:50Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/120537, merge i," Merge started Your change will be merged while ignoring the following 6 checks: pull / linuxfocalcuda12.1py3.10gcc9 / test (deploy, 1, 1, linux.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 2, 2, linux.g5.4xlarge.nvidia.gpu), inductor / linuxjammycpupy3.8gcc11inductor / test (dynamic_cpu_inductor_torchbench, 1, 2, linux.12xlarge), inductor / rocm6.0py3.8inductor / test (inductor, 1, 1, linux.rocm.gpu.2, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,torch.set_num_threads(n) is not effectively utilizing multiple cores," üêõ Describe the bug When I was Inferencing a 7B LLM model I found pytorch is slow and cannot utilize multicores of CPU. Even after setting all the parameters set_num_interop_threads(), set_num_threads(), MKL_NUM_THREADS,  OMP_NUM_THREADS  Only a single Core is being utilized at a time. I have build from source following all the linux instructions as well. Yet only one core getting utilized. `import torch import os os.environ[""OMP_NUM_THREADS""] = ""24"" os.environ[""MKL_NUM_THREADS""] = ""24"" torch.set_num_threads(24) torch.set_num_interop_threads(24) print(torch.__config__.parallel_info()) print(f""Threads: {torch.get_num_threads()}"")` !image !image  System Info Ubuntu 20.04.6 LTS torch  2.3.0a0+  Versions !image",2024-02-23T20:21:41Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/120525,I was facing this issue when I was importing the GritLM (https://github.com/ContextualAI/gritlm/tree/main?tab=readmeovfilebasic) package.  I was able to utilize all the allocated core's when I download the model from huggingface.
yi,make it clearer (in docs) one can double decorate with torch.library.impl_* APIs,,2024-02-23T18:06:22Z,module: docs good first issue triaged,open,0,1,https://github.com/pytorch/pytorch/issues/120503,"Hi  , I'd like to try and improve the doc. Can you assign it to me? Can you confirm that I'm looking at the right spot? (pytorch/docs/source/library.rst)"
transformer,Better documentation on _mask and _key_padding_mask in transformers modules," üìö The doc issue The docs here do not clearly indicate the shapes of the masks and their equivalent types that the transformer modules are expecting. The only place where one can find shapes and types of expected tensors are only in the source code docs atm.  Suggest a potential alternative/fix A couple of examples of different use cases would really help users understand what is expected what the actual effect is of each mask.  Would the transformer module operate on both of of them? Also, a clarification on why one uses a boolean mask for `_key_padding_mask` with `True` values indicating there's padding there and `False` there's no padding which is actually the opposite of other frameworks. For instance, here's an excerpt from hugging face explaining the same thing but they use 1s, i.e., `True` indicating where the model needs to attend to and 0s `False` where there's padding.  ",2024-02-23T14:14:46Z,module: docs triaged actionable,closed,0,0,https://github.com/pytorch/pytorch/issues/120488
transformer,faster communication," üöÄ The feature, motivation and pitch I noticed that with the release of the new version of nccl, a new feature appeared(https://docs.nvidia.com/deeplearning/nccl/userguide/docs/usage/bufferreg.html). This feature seems to be able to speed up transformer by combining some technologies, such as paper 'Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models'.  So I think it is interesting if pytorch could provide a highlevel API to use these technologies.  Alternatives _No response_  Additional context _No response_ ",2024-02-23T03:24:29Z,oncall: distributed triaged,open,0,2,https://github.com/pytorch/pytorch/issues/120466,"  I think we do have a plan for supporting zerocopy collectives, right? Could you help elaborate? Thanks!",adding notes from discussion with   (please correct me/add more details) 1) nccl 2.20 would be required (but not tested yet by the pytorch team) 2) zero copy would only be over the network for internode comm.  To enable intranode zerocopy allreduce you need to allocate your own memory with some flag
transformer,torch.compile() drops transformer/Qwen1.5-7B model output quality from good to unusable," üêõ Describe the bug Not sure how we can further debug this and provide more data but we are seeing drastic output quality difference between notorch.compile and torch.compile.  We are new to torch.compile(). Does torch.compile inherently cause accuracy issues? Not sure if just natural part of the beast.  model load code:  gen code and genparams are same for generation.  The difference is as if we are running a vastly different, a much more inaccurate model when torch.compile() is enabled. The difference, drop in quality, is not related to randomness of sampling.    Versions  ",2024-02-23T01:53:21Z,needs reproduction triaged oncall: pt2 module: pt2 accuracy,open,0,7,https://github.com/pytorch/pytorch/issues/120462,"I'm putting this up for triage review because there is an interesting question here, which is that torch.compile numerics are *necessarily* different from eager (we do not guarantee they are exactly the same), but what I am not sure is if we consider it a bug if you fine tune a model with eager pytorch, and then torch.compile'd version is unusable, is this a bug in torch.compile or could it be that this is expected and you needed to fine tune the model with torch.compile? That being said, since this is a transformer accuracy thing, it's possible that https://github.com/pytorch/pytorch/pull/119500 fixes the problem. I'm not sure if this one made it to 2.2, you can try a nightly.   We might be willing to take a closer look; more detailed repro instructions will help in that case.","We discussed this in triage review, and we do not believe that (modulo bugs), `torch.compile` should ever result in *worse* accuracy, particularly inference. So this sounds like a bug we'd like to fix, and we'd appreciate more detailed repro instructions.","I've also gotten much worse accuracy using torch.compile, here's a repro (for a TexttoSpeech model).  With the following changes, the final speech output is much worse. Same worse results when using other torch.compile parameters like `mode=""maxautotune""`, `mode=""maxautotunenocudagraphs""`, `fullgraph=True`, `fullgraph=False`, `dynamic=True`, `dynamic=False`, and also when torch.compile is applied to other modules of the Decoder class. https://github.com/siddhatiwari/StyleTTS2/commit/13a9f87e97af98561566b643728e01608acc1bb1","Thanks , any chance you can give instructions for how to reproduce your results on your repo?","StyleTTS2 is a model for generating speech from text. It doesn't currently use torch.compile in any capacity. I tried modifying it by adding a single torch.compile to a small part of the overall model. You can find the original repo https://github.com/yl4579/StyleTTS2 and my modified repo https://github.com/siddhatiwari/styleTTS2.  Note that you only need to clone my modified repo to generate audio with both the unmodified model (original quality) and the torch.compile model (with lower quality). The following steps explain how to set everything up, produce good outputs with the unmodified model, and bad outputs with the torch.compile model: Part 1: Setup  Run each command:  Part 2: Generating audio using unmodified model  Run:  See generated audio: 1789_142896.wav 696_92939.wav Part 3: Generating audio using torch.compile model  In ./Modules/hifigan.py (https://github.com/siddhatiwari/StyleTTS2/blob/main/Modules/hifigan.pyL436): Comment line:  Uncomment lines:  Run:  See generated audio: 1789_142896.wav 696_92939.wav You will notice that the generated audio in Part 3 is worse quality than that in Part 2.  Sample audio outputs you should get after following these steps: https://github.com/siddhatiwari/StyleTTS2/tree/main/samples"," Hey, just wanted to follow up on this. Any ideas on what I could do on my end to help you guys investigate this further? ","Hi , some basic troubleshooting ala https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html would be helpful. What I find most useful is to do an ablation: does it happen if you get rid of reduceoverhead, what about backend=""aot_eager"" or backend=""eager""?"
llm,Testing llm td changes,Fixes ISSUE_NUMBER,2024-02-22T23:10:31Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/120447
transformer,Reinitialize parameters in TransformerEncoder," üöÄ The feature, motivation and pitch I was exploring the nn.TransformerEncoder, and I noticed that one layer is initialized and the other layers are copies of this initial random state. Although this might be worth exploring, intuitively it'd make sense to initialize randomly each of the layers.  My intuition is that if a random initialization has projections associated with certain eigenvectors, repeated applications of this layer might tend to project toward vectors with large norms, which could cause numerical issues during the initial training steps.  Alternatives _No response_  Additional context _No response_ ",2024-02-22T11:17:26Z,module: nn triaged,open,1,0,https://github.com/pytorch/pytorch/issues/120396
llama,Numeric stability issue with full parameter fine tuning of Llama 2 7B model with FSDP," üêõ Describe the bug I was wondering anyone having experiences with full parameter fine tuning of Llama 2 7B model using FSDP can help: I put in all kinds of seeding possible to make training deterministic (https://pytorch.org/docs/stable/notes/randomness.html); however I still observe that the backward gradients on the first sample training vary on each run. The variation of gradients is around the scale of 1.0e8. I am using FSDP to wrap around the decoder module. I don‚Äôt have the numeric stability issue if I only fine tune an MLP classification head. The numeric instability seems to occur as soon as the decoder layers are wrapped in FSDP and require gradients. The numeric instability causes each of my training run to produce models of noticeably different qualities. Any help or suggestions would be appreciated!  Versions Collecting environment information... PyTorch version: 2.1.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.11.7  (main, Dec 23 2023, 14:43:09) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.15.120+x86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.3.107 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB Nvidia driver version: 525.125.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True ",2024-02-22T06:01:25Z,oncall: distributed,open,0,12,https://github.com/pytorch/pytorch/issues/120383,"Further experiments show that if I freeze the first 31 layers and the embedding layer (but keep the last layer and the lm_head layer require_gradient=True), the training is stable. That is the gradients on every restarted run are the same. But once I unfreeze any more parameters, the training becomes unstable, even if I set the learning rate to 1.0e8.",Seeding used for every process: ,"Could you share how you are applying FSDP (e.g. wrapping policy and args)? It seems possible that NCCL reduction collectives are not deterministic, in which case there could be slight numeric differences from run to run."," thank you very much for helping. The wrapping policy is very simple and derived from llamarecipies as:  The wrapping codes:  I was suspecting nondeterminism from NCCL as well. However: 1. The suggested NCCL_RINGS environment variable seems to be deprecated in newer versions and it did not help in my case 2. On the inference run, the results are deterministic in my case. The nondeterminism only occurs with gradients.","Inside fsdp_auto_wrap_policy, there is an if statement to choose lm_auto_wrap_policy",I made some progress in debugging. Adding the following codes into set_seed helped to stablize fp32 training:  But I see that mixed precision training with bfloat16 is still not numerically stable. Any suggestions?  ,"I did not have a chance to investigate whether the gradient reducescatter has nondeterminism or not (but if so, that would explain the issue). Taking a step back, if I am understanding correctly, the issue is that your training is not stable, and you want to stabilize it? Or are you specifically interested in producing numerically identical results after each run?","apologize for the confusion. To clarify: my interest is to produce numerically identical results with each run. Now, I am able to produce numerically identical results with FP32 training, but not yet with bfloat16 (mixed precision with FSDP). Thanks.",Maybe you can try fp32 reducescatter? Change  to  ? (`from torch.distributed.fsdp import MixedPrecision` if needed),"I experimented, and I was able to have full numerical reproducibility with the following mixed precision training. It seems that the issue is not with the reducescatter, but with the parameter type.  This is **very** undesirable in that setting `param_dtype=torch.float32` instead of `param_dtype=torch.bfloat16` eliminates the speed boost (around 2x) from the mixed precision. Hence there is little reason left for using mixed precision as opposed to using fp32.","Just to be clear, setting `reduce_dtype=torch.float32` in my case did not help numerical reproducibility.","I wonder if, because bf16 uses the tensor cores, there is nondeterminism that cannot be disabled."
rag,DISABLED test_averaged_model_all_devices_ema_False (__main__.TestSWAUtils),"Platforms: win, windows This test was disabled because it is failing on main branch (recent examples). ",2024-02-22T01:20:17Z,module: windows triaged skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/120372
llama,Switch to native functional collective by default,"  CC(Switch to native functional collective by default) This enables native functional collectives by default. After this PR:  The Python APIs remain backward compatible. Users will receive a deprecation warning if they use `(rank, tags)` as process group identifier.  Collectives will be captured as `_c10d_functional` ops in postgrad fx graphs. The change will not affect endusers, but it will impact `torchxla` which has implemented an allreduce backend based on the existing `c10d_functional` IR. This excludes the migration for `torchxla` use cases, which will be coordinated separately (see communications in CC([RFC] PT2Friendly Traceable, Functional Collective Communication APIs)).  Collectives will be lowered to and codegen'd by new Inductor collective IRs (`ir._CollectiveKernel` and `ir._WaitKernel`). This change will not affect endusers. Testing performed:  We have been running a set of representative unit tests with both the new native funcol and the old py funcol in CI. These test will continue to run with the old py funcol after this PR, so they are covered until they are removed.  Manually verified with e2e llama model training with DTensor + functional collectives (https://github.com/fairinternal/xlformers/tree/pt2_llm/pt2dcreateyourlocaldevelopmentenv). Fallback mechansim:  Introduced a temporary environment variable `TORCH_DISABLE_NATIVE_FUNCOL` that allows users to fall back to the previous implementation. We don't expect the migration to break anything; the mechanism is a safety measure to reduce potential disruption in case the PR causes unforeseen breakages. ",2024-02-22T00:51:27Z,oncall: distributed Merged Reverted ciflow/trunk topic: not user facing,closed,0,13,https://github.com/pytorch/pytorch/issues/120370, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**:  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""broke CI"" c ignoredsignal",Need to adjust a test since the flag is disabled in fbcode by default at the moment., successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,FakeTensorMode + StableDiffusion yields RuntimeError: Creating a new Tensor subclass FakeTensor but the raw Tensor object is already associated to a python object of type FakeTensor," üêõ Describe the bug Creating a FakeTensor from another FakeTensor yields an error, although it should be supported Requires https://github.com/pytorch/pytorch/pull/120261 to repro (in the process of being merged) Repro  Error   Versions main ",2024-02-21T18:07:41Z,triaged oncall: pt2 module: fakeTensor module: dynamo module: pt2-dispatcher,closed,0,2,https://github.com/pytorch/pytorch/issues/120323,This is likely the same thing as https://github.com/pytorch/pytorch/pull/119868  ,"> This is likely the same thing as CC(Do not wrap output with input device inside _to_copy) , CC(Do not wrap output with input device inside _to_copy) does fix this error"
transformer,"Second forward call of a compiled model (exact same input shapes, strides) is extremely slow due to cuda graphs"," üêõ Describe the bug When using `torch.compile(..., mode=""reduceoverhead"")` on CUDA device, the second forward call with exact same input shapes, strides, device, dtype is extremely slow, with 0% GPU usage and 100% CPU usage. When using ctrl+C (**not an error**, just forcing exit), we see that PyTorch is spending time in cudagraph `self._record(wrapped_function.model, recording_inputs)`:  This is surprising that this happens at the second forward call.  and then  Giving:  The log comes from https://github.com/huggingface/transformers/compare/a8c4e1036ac6f0f78e512235cf42c17c7d3cc762...reprobugpytorchcompilecudagraph?expand=1 A potential solution is to use `.compiler.disable` on the `_update_causal_mask` method. This removes logs as  in inductor logs. But it is not a perfect solution either as then `fullgraph=True` can not be used. It is quite surprising to me that the **second** forward call is slow. To me only the first should be.  Versions  ",2024-02-21T11:17:03Z,module: cuda graphs oncall: pt2 module: dynamic shapes,closed,0,5,https://github.com/pytorch/pytorch/issues/120309,"related  CC(If dynamic shapes runtime CUDA graphs never quiesces, should loudly warn / easy to diagnose)","> It is quite surprising to me that the second forward call is slow. To me only the first should be. This is a side effect of how we turned on cuda graph support for dynamic shapes. We hope that you do not have too many distinct sizes, and CUDA graph them all. To do this, we have to be recording on the first time you hit ever new size. What is your desired end state for your code here? Do you want fullgraph=True, but cuda graphs on only part of the graph (excluding update causal mask?) There's a potential feature we've talked about which is making reduceoverhead smarter and selectively cuda graph only compatible regions of graphs, I couldn't find if we filed an issue for it though. ","Thank you .  commented on slack: > We do two warmup calls not one because we support mutation on Parameters. The first run warms up model (things like triton autotuning etc). The second run records the graph and plays it. The third run is the fast path > > To do only one warmup run you would have to copy parameters prior to warmup otherwise you would mutate it twice, and copying parameters is not viable for memory reasons where I'm not sure which `parameters` it refers to (maybe tritonrelated found hyperparams). So it appears this is not a bug, but a design decision. > What is your desired end state for your code here? Do you want fullgraph=True, but cuda graphs on only part of the graph (excluding update causal mask?) That would be very helpful indeed. What I considered using was `.compiler.disable` for the part of the code using dynamic shapes, and compile time was drastically reduced but latency increased as well. I think for now we'll stick with feeding static shapes to the compiled model.",I found documentation in the PyTorch CUDAGraph Trees which gives a simple example with comments explaining what each invocation does: ," Thank you for the pointer, I was not aware of it!"
yi,[pytree] add `tree_iter` function and fix opcode `YIELD_FROM` and `SEND`,Reland CC([pytree] add function `tree_iter`) Fix opcode `YIELD_FROM` and `SEND` opcode for dynamo. Remove the guard return after `yield`.  to   Original comment: This PR adds a new function `tree_iter` that lazily iterates over the tree leaves. It is different than the `tree_leaves` function while the latter traversal the whole tree first to build a list of leaves.  is much more efficient than:  where `tree_leaves(tree)` is `list(tree_iter(tree))`. ,2024-02-21T08:41:55Z,triaged open source ciflow/trunk module: pytree module: dynamo ciflow/inductor release notes: dynamo,closed,0,19,https://github.com/pytorch/pytorch/issues/120300,I think you can keep tree_iter but just rewrite it as in https://github.com/pytorch/pytorch/pull/120155issuecomment1955606472," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",I'm importing the diff and will rerun the failed tests internally to confirm they are fixed.,Just to confirm that the torchrec tests pass with the current PR,> I think you can keep tree_iter but just rewrite it as in [ CC([pytree] add function `tree_iter`) (comment)](https://github.com/pytorch/pytorch/pull/120155issuecomment1955606472) I updated the PR and description. Let's see how it goes.," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `treeiter` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout treeiter && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",>  I don't have context over why this was reverted. Could you check to see if this is landable? A bunch of relevant OSS tests are still failing on the PR I think,I have fixed the implementation for opcode `YIELD_FROM` and `SEND`. Now all tests pass.,Test failures are unrelated., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Meta InternalOnly Changes Check Details for Dev Infra team Raised by workflow job ", Could you take a look at this?,Not sure what is going on here.   Somehow `Meta InternalOnly Changes Check` thinks this is linked to a phabricator diff. One fix might be creating a new version of this PR.,I split this PR into:  CC([Dynamo] fix opcode `YIELD_FROM` and `SEND`)  CC([pytree] add `tree_iter` function)
transformer,Equivalent idea to size-oblivious guard for end of bounds on sizes," üêõ Describe the bug Ref  CC(llama model failed for dynamic shape path) Sizelike SymInts currently let us definitively answer that an unbacked SymInt does not equal 0 or 1, in cases where the generic logic would generalize. It would be helpful to have an analogous version of this which says an unbacked SymInt does not equal s0, where s0 is the bound of some Tensor we're going to index with the unbacked SymInt into. The prototypical situation of this is this pattern found in transformer models:  We legitimately must know if u0 == tensor.size(0), because *only* in this case is the resulting view contiguous; when u0 < tensor.size(0), we have ""gaps"" between each batch entry and it is discontiguous. If we are aiming to compile only a single kernel, we would like to say that u0 != tensor.size(0), giving us a ""discontiguous"" tensor that Inductor can appropriately pessimize over. Let's suppose we do it with some API like `torch._check_is_bounded(u0, s0)` which is analogous to `torch._check(u0 <= s0)`. On subsequent size oblivious tests, we'd like report `u0 < s0`. It would be amazing if we could put s0 in the ValueRange for u0. But this doesn't work for two reasons: (1) ValueRanges can't take symbolic bounds, and (2) this doesn't help, you want u0 <= s0  1 to discharge conditions (but you don't actually want that). The very inefficient way to do this analysis would be to just remember u0 is bounded by s0, and then when doing size oblivious tests, use this information to answer relational queries involving u0 and s0 (we can't do the original trick of narrowing the range from min=0 to min=2. But maybe if we did some ad hoc ValueRanges analysis with",2024-02-21T04:49:01Z,high priority triaged oncall: pt2 module: dynamic shapes,open,1,8,https://github.com/pytorch/pytorch/issues/120288,"For reference, we disallow bounds to be symbolic because there are a few sympy ops like (truncation and I reckon that a couple more) that do not have a symbolic equivalent. That being said, these multivariate bounds are very brittle. It is very easy to end up with expressions of the form `s0  x0  1 < s0` which our system cannot prove that are true.","We got another example of this kind of phenomenon in https://fb.workplace.com/groups/6829516587176185/posts/7493024157492088/  The user is trying to implement padding by viewing into large and then mutating the resulting view. But this triggers a data dependent guard, as narrow(1, 0, A2) may or may not be contiguous depending on whether or not A2 == large.size(1). Our suggested workaround in this case is to use the padding operation directly, e.g., torch.nn.functional.pad","Another internal occurrence: https://fb.workplace.com/groups/6829516587176185/posts/7611851122276057 This one is interesting, because in this case they want to do a mutation on the view. Ick! Their sample program:  Actually, you can get it to work by manually functionalizing it yourself. Kids, don't try this at home: https://www.internalfb.com/intern/anp/view/?source=version_selector&id=5669466  then  and end to end ","Here is another example where it is not the narrow that is a problem per se, it is functionalizing the copy on the narrow:  Here it errors with   It's specifically failing at  So it's possible we can fix this spot specifically.",Another occurrence:  The guard is specifically triggered here:  Minimal repro:  Rewriting this to be a narrow instead works though: ,"https://fb.workplace.com/groups/6829516587176185/permalink/8388398464621315/ Yet another minimal repro:  This one is potentially also functionalization on the narrow, need to check. EDIT: Yes, it's functionalization  EDIT 2: https://github.com/pytorch/pytorch/pull/141418 works around computeStorageNbytes but you still die asking for contiguity on the tensor EDIT 3: The changevar trick works "," Do you know why does your last example work with `torch.compile(repro, dynamic=True)(*example_inputs)`, but not with export?",Your torch.compile invocation doesn't include fullgraph=True so I assume it's just graph breaking
rag,"Cloning a sparse tensor in inductor causes ""RuntimeError: Cannot access data pointer of Tensor that doesn't have storage"""," üêõ Describe the bug This fails in inductor but works in eager and aot_eager.  I believe inductor is just calling the ""normal"" clone but needs to handle the sparse tensor clone specially.  There are a couple existing issues which have the same error message but I wasn't sure if it was the same problem and this is a much simpler repro:   CC(Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile`)   CC([Bug/functorch] Cannot use `tensor.detach().numpy()` for `GradTrackingTensor`: Cannot access data pointer of Tensor that doesn't have storage)  Versions ``` Collecting environment information... PyTorch version: 2.3.0a0+gitff1beb9 Is debug build: True CUDA used to build PyTorch: 12.0 ROCM used to build PyTorch: N/A OS: CentOS Stream 9 (x86_64) GCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.13) Clang version: Could not collect CMake version: version 3.28.1 Libc version: glibc2.34 Python version: 3.11.7  (main, Dec 23 2023, 14:43:09) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.19.00_fbk12_zion_rc2_11583_g0bef9520ca2bx86_64withglibc2.34 Is CUDA available: True CUDA runtime version: 12.0.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 GPU 1: NVIDIA H100 GPU 2: NVIDIA H100 GPU 3: NVIDIA H100 GPU 4: NVIDIA H100 GPU 5: NVIDIA H100 GPU 6: NVIDIA H100 GPU 7: NVIDIA H100 Nvidia driver version: 525.105.17 cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.8.0 /usr/lib64/libcudnn_adv_infer.so.8.8.0 /usr/lib64/libcudnn_adv_train.so.8.8.0 /usr/lib64/libcudnn_cnn_infer.so.8.8.0 /usr/lib64/libcudnn_cnn_train.so.8.8.0 /usr/lib64/libcudn",2024-02-21T00:36:43Z,module: sparse triaged oncall: pt2 module: inductor,open,0,1,https://github.com/pytorch/pytorch/issues/120267,"+1 for `python test_compiled_autograd.py k test_input_buffer_accum` which uses `leaf.gather(0, ind, sparse_grad=True)` btw looks like your version snippet is hiding all the "
transformer,Stateful `int` is not updated when using `torch.compile`," üêõ Describe the bug A model instance attribute `int` is silently not correctly updated when using `torch.compile`. The model behaves as expected when using simply eager mode. Reproduce with:   Resulting in the logs:  Notice that `seen_tokens` is sometimes not correctly updated as it should be https://github.com/huggingface/transformers/blob/c9b864ba54ff09ef0eaf9b0f9f9624ef594d5e0e/src/transformers/cache_utils.pyL394 (simply incremented by one) The logs come from https://github.com/huggingface/transformers/compare/c47576ca6e699c6f8eaa8dfc4959e2e85dec0c72...reprobugpytorchcompile?expand=1 Something to consider here is that the `StaticCache` subclass is attached only AFTER the `torch.compile` call, see here and here. The first solution I found was to use a `torch.Tensor` (updated in place) instead of an int. But given that the model after torch.compile silently behaves differently than it did simply in eager mode, I believe this is a bug.  Error logs no error, silent bug.  Minified repro /  Versions  ",2024-02-20T19:28:16Z,triaged oncall: pt2 module: dynamo dynamo-must-fix,closed,0,6,https://github.com/pytorch/pytorch/issues/120248,"In the end, this was fixed in https://github.com/huggingface/transformers/pull/29114 by removing the `seen_tokens` variable altogether. In the meantime, I came across an other bug when using `seen_tokens` as a `torch.Tensor` (instead of an `int`) updated inplace, where calling `torch.compile` BEFORE the `StaticCache` subclasses initialization would also result in silently wrong computation of `seen_tokens` (but this time in the second generate call, not the first as in the above snippet).", ,(Note that the ability to use a stateful int would still be useful to us https://github.com/huggingface/transformers/pull/29221discussion_r1502454812),"This is a duplicate of  CC(torch.compile precision bug when the attr object changes) If you want to work around, you can use `torch._dynamo.config.guard_nn_modules=True`. This will incur performance penalty because TorchDynamo will insert additional guards inside the nn module hierarchy. We are investigating how we can catch this without incurring large perf penalty.",Removing the high priority label as the duplicate  CC(torch.compile precision bug when the attr object changes) is already highprio.,This is fixed now. Closing
gpt,model loaded with torch._export.aot_load does not report what file is not found during inference and Cuda driver error.," üêõ Describe the bug when I load a pt2 model exported with torch._export in one Docker container from the image `ghcr.io/pytorch/pytorchnightly:2.3.0.dev20240211cuda12.1cudnn8devel` I get a working inference.  But when I run it in another container derived from the same base image, I get a CUDA driver error. I can't track down the error because the error message doesn't give me anything to go on. I've confirmed that nvidiasmi, nvcc version, the torch version and all environment variables from `docker inspect` are the same between the two running containers. I can't identify anywhere that another torch version is installed and I can't see any other cuda versions installed in `/usr/local` that might cause a conflict.  the error is below   Versions Details for the container where inference fails  and where inference works  the only differences found by gpt 4:  resolving the triton version mismatch doesn't fix the cuda driver error. ",2024-02-19T07:12:30Z,triaged oncall: pt2 module: aotinductor,open,0,6,https://github.com/pytorch/pytorch/issues/120194,I've also confirmed that I can compile and run CUDA programs in the container where model inference fails with CUDA driver error. ,"I think this was because I had specified a path when exporting the model inadvertently. The model must be loaded from the exact same path specified in the export options  would it be possible to provide an error message that highlights exactly what file was not found (the model file)? I got confused by ""CUDA driver error"" thinking this was an issue with CUDA versions or my python environment.","After more investigating it looks like I actually just need to include all the *.cubin files along with the .so file. I thought AOTInductor would give me a single binary that included the compiled kernels, but it looks like these are in separate files.",cc:  ,"What's the work around for the problem where the generated cpp files point to an absolute path in /tmp/ for the cubin files? Is there a way to specify where the cubin files should go on AOTCompile? I want to be able to generate these assets and use them across many machines. I ran into this when moving my .so file to another machine, and my forward call resulted in:  I verified via nvidiasmi that the call that was returning the 301 code on the first cuModuleLoad call. In the generated cpp, you can see the reference to cubin is an absolute path. You can also see the paths if you use `strings model.so | grep tmp`.","For those who come after, I found that if you set a relative path in aot_compile like:  Then the Code Cache will prepend whatever is set in `TORCHINDUCTOR_CACHE_DIR`. If nothing is set there, it will prepend `/tmp/torchinductor_{username}/`. I will solve this by specifying an absolute path."
,kernel.codegen_kernel function in inductor without dependency of triton wheel," üöÄ The feature, motivation and pitch The kernel. codegen of the inductor is a text generation behavior, and currently it no longer relies on Triton. However, there are still two places where it depends on Triton: 1. gen_attr_descriptor_import : https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/triton.pyL2570 2. signature_of: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/triton_utils.pyL12 , raw function: https://github.com/openai/triton/blob/main/python/triton/runtime/jit.pyL238 There are currently two reasons to strip Triton's dependencies in the codegen process: 1. Triton currently does not support ARM architecture CPUs in pypi, nor does it have a pure CPU version, issue. Stripping off related dependencies in the codegen process can allow for certain possibilities in subsequent processes, such as cross compilation. 2. From the perspective of the software itself, stripping the dependency on the actuator in the codegen process is also a reasonable choice.  Alternatives _No response_  Additional context _No response_ ",2024-02-19T06:51:57Z,oncall: pt2,closed,0,0,https://github.com/pytorch/pytorch/issues/120192
transformer,Making Mamba first-class citizen in PyTorch," üöÄ The feature, motivation and pitch Mamba is a new SSM (State Space Model) which is developed to address Transformers‚Äô computational inefficiency on long sequences. It has attracted more attention recently due to faster inference and linear scaling in sequence length. We are exploring how to support Mamba as firstclass citizens in PyTorch.  To better understand the gaps and coordinate these ongoing effects, we created the following doc to trace the requested features and issues. Feel free to comment if you have any feedback! https://docs.google.com/document/d/1rNNByFrOjOQOBM6ZZqnOqcLGMRmdnQifKfbY_KalnM/edit?usp=sharing   ",2024-02-19T06:03:59Z,triaged needs research oncall: pt2 module: higher order operators module: pt2-dispatcher,open,10,4,https://github.com/pytorch/pytorch/issues/120189,"The document, as written, is a bit too optimistic. The current `tl.associative_scan` (and as such 's implementation in https://github.com/pytorch/pytorch/pull/119430) just supports pointwise accumulation functions. As such, we will just be able to implement SSMs for diagonal matrices, where matrix multiplication turns into pointwise multiplication. We should be able to do this once: 1. https://github.com/pytorch/pytorch/pull/119430 lands 2. We extend the current support for multiple inputs and outputs in our scan operation (this shouldn't be too difficult)."," Yes, we only discussed SSMs for diagonal matrices in the doc, but we should be able to extend to more general SSMs after this.",I hope we could include also Mamba2 coverage https://arxiv.org/abs/2405.21060 https://github.com/statespaces/mamba/issues/355,Associative scan was merged in Triton on April https://github.com/tritonlang/triton/pull/3177 A minor bug is WIP at: https://github.com/tritonlang/triton/issues/4362 What is the status on pytorch?
transformer,"The forward accuracy cannot be aligned when using  DTensor and ColwiseParallel, RowwiseParallel for TP training on the Llama2"," üêõ Describe the bug Hi, I found that the forward accuracy cannot be aligned completely when using your 'DTensor' and 'ColwiseParallel', 'RowwiseParallel', and 'Replicated' api for TP training on the Llama2.  I referred your example: https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.pyL175C9L262C1. I parallelize the embedding submodules of Llama2 by 'Replicate()', parallelize the attention and feed forward by 'ColwiseParallel' and 'RowwiseParallel', and Replicate the model.lm_head for the output, and manually adjust the number of heads and the hidden size. The following is my code:  And here is my loss comparison  between model and model_tp: !img_v3_0284_103fb69e539e427f8d3f29e7981c5f7g and my comparison code, just like you used in your example: !img_v3_0284_72d46648041d474f9004909adbe6764g It said I have 29% elements mismatched in my first step loss comparison! The gradient of model parameters in backward cannot be aligned too. !img_v3_0284_c152df3641e24b808dc270d5a3ae96bg I feel very confused and have invested the whole process, like whether the input is consistent across each rank(it is), and whether the dropout in the llama2 model could introduce randomness (Actually llama2 does not have dropout). Now I really don't know why. Could anybody help me to check this? Should I add some all_reduce for the loss or something?  Versions Package                 Version            Editable project location    abslpy                 2.1.0 accelerate              0.26.1 aiohttp                 3.9.3 aiosignal               1.3.1 astunparse              1.6.3 attrs                   23.2.0 blessed        ",2024-02-17T12:20:38Z,oncall: distributed module: dtensor,open,0,11,https://github.com/pytorch/pytorch/issues/120158," Hello, would you mind assisting me with checking the accuracy on TP training with llama2? I really don't know whether it's wrong with my code or something. I would be grateful if you could help me.","By the way, the weight of model and model_tp before forward can be alighted. So at least we can narrow down the scope of the problem to the first forward pass of the model. I think the problem probably be the all_reduce. !img_v3_0286_06138c6e460a4ba885cfbf7ad6c4cb7g !img_v3_0286_e3f1f3dca2b3441f9226fb061ff29e8g","Hi, dear developers, I have narrow down it's after the MLP of Llama2 model, the accuracy cannot be aligned totally. There is a SiLU activation in MLP, which is different from your example. I'm trying to solve it and hope it could be aligned later.","Hello, there is a correction.  I found that during the llama2 model forward pass, the precision can be aligned after **the first LlamaDecoderLayer**. However, after passing through **24 layers** of LlamaDecoderLayer class, this precision gap increases to **11%.** This is beyond my understanding.  You can try looping the transformer layer in your example multiple times instead of just one layer to see if the same precision issue occurs.",And I used torch.float32 to do the training., l can one of you take a look?,"Hello, I think I have solved the forward accuracy. It's the reason that you use the AsyncTensor to do the communication for TP. **The AsyncTensor is still dangerous now,** and can lead some errors in the compiler, as  mentioned in the comment https://github.com/pytorch/pytorch/blob/main/torch/distributed/_functional_collectives.pyL119 I use native c10d api to replace them and now the forward accuracy can be aligned, the `testcase.assertEqual(output[0], output_tp[0]) ` code can be passed now. !image But the backward of model and model_tp still cannot be aligned now, it said there are  **Mismatched elements: 7 / 4194304 (0.0%)** of attention weights between model and model_tp. :( The corresponding elements of the model and model_tp's weights looks like very close but not the same. Maybe I should check the backward of attention after backward. I am still working on finding a solution. I would greatly appreciate any suggestions you may have. !image",The loss curve still cannot be converged. üò¢ Would anyone be so kind as to lend a hand? I would be extremely grateful. Thank you!,My comment about tracing asynctensor just means the compiler starts with an unoptimized graph from timing/overlap perspective and must do its own reordering to improve performance. I don't know of an accuracy problem with FC/compile. But if you're reporting one we'll take a look. We might need some help dumping inductor IR graphs. Cc  ,"Hey , just a heads up that we landed a functional collective overhaul a few days ago (https://github.com/pytorch/pytorch/pull/120370). Can you check if your local PyTorch build contains the change? If not, I'd suggest updating your PyTorch build and rerunning your experiments with the change. It's not guaranteed, but I think there's a good chance it may address your issue. If the issue persists, please provide a minified repro. I'd be happy to look into it.","> Hey , just a heads up that we landed a functional collective overhaul a few days ago ( CC(Switch to native functional collective by default)). Can you check if your local PyTorch build contains the change? If not, I'd suggest updating your PyTorch build and rerunning your experiments with the change. It's not guaranteed, but I think there's a good chance it may address your issue. >  > If the issue persists, please provide a minified repro. I'd be happy to look into it. Thank you for your crucial reply! It really works for the forward accuracy but still can't make the backward accuracy aligned after I update my local PyTorch to the latest including CC(Switch to native functional collective by default). Sure! You can find the start.sh script at this link: start.sh. Don't worry about the model and data paths as they are all downloaded from Hugging Face. The tensor parallel part can be found in tensor_parallel.py, and the tensor parallel unit test part is available in finetune_llama2.py. Both are referenced from the tp_examples in PyTorch: test_tp_examples.py."
mistral,Add sliding window attention bias ," Summary This PR adds a new attnetionbias __torch_function__ designed to interact with SDPA. This implements sliding window and updates ""aten.sdpa_flash""  to expose the window_size_left and window_size_right arguments. With this kwarg added we can dispatch to the FAv2 impl if the necessary constraints are met.  Reviewer Highlights  One notable difference form FAv2 is that `1` is used to represent an unbounded window size, while here we accept an optional int, a value of `None` means `window_size_* = inf`.   When seqlen_q > seq_len_kv the behavior can be potentially unintuitive. If you are using the UPPER_LEFT variant, Queries  at indexes > then seq_len_q can have non masked out entries. This is because the window_size_left will allow the query at position(i) to attend to keys at position(iwindow_size_left). An analgous argument can be made for LOWER_RIGHT. See this test: https://github.com/pytorch/pytorch/pull/120143/filesdiff14009de3c2bccda8ced7e573adbed54f2a57306659b3bc14e2bfb464f1a3ae17R3523R3533. Should we ban this? or just warn?  Mask defintion The Mistral paper has a very nice visual of the bias:  The sliding window attention bias is parametrized by two values, window_size_left and window_size_right. For any given query `i` this parameters define the ""window"" or neighborhood for which the query is allowed to attend to;    Current Bias API: In order to construct a mask we require the following args:   CausalVariant The CausalVariant is used to specify the behavior of the attention mechanism when seq_len_q (the length of the query sequence) is not equal to seq_len_kv (the length of the key and value sequences). There are two possible",2024-02-17T01:15:54Z,Stale ciflow/trunk topic: new features ciflow/inductor,closed,1,4,https://github.com/pytorch/pytorch/issues/120143, Do you know what the ramifications are of the failing ABI compat test for aotinductor? In theory this should be compatible since I am adding kwarg only options to private ATen ops but not sure if there is documentation / detail on the process for doing so,">  Do you know what the ramifications are of the failing ABI compat test for aotinductor? In theory this should be compatible since I am adding kwarg only options to private ATen ops but not sure if there is documentation / detail on the process for doing so We already have a v2 `aoti_torch__scaled_dot_product_flash_attention_v2`  for some historical reason, https://github.com/pytorch/pytorch/blob/b9087f857147da37c122cb70955d791565ff1ed6/torch/csrc/inductor/aoti_torch/shim_common.cppL376. Normally, you will need to create a similar `v3` version with additional parameters you are adding here, and change the codegen rule here, https://github.com/pytorch/pytorch/blob/b9087f857147da37c122cb70955d791565ff1ed6/torch/_inductor/ir.pyL4954. But I have a recent PR touching some relevant pieces, https://github.com/pytorch/pytorch/pull/120513. So things are slightly messy here. If you rebase after my PR, it should work for OSS, but there will be issues for internals. If you think this is the only remaining blocker, I can work out a patch PR to unblock.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llm,RingAttention Support," üöÄ The feature, motivation and pitch Hello, I would like to inquire about the potential inclusion of RingAttention in `PyTorch,` which could enable training with longer sequences. The incorporation of `RingAttention` would significantly enhance the capabilities for users engaged in advanced projects involving `LLMs` and `LMMs`. I believe this feature would make `PyTorch` even more versatile and valuable for the research and development community. Thank you.  Alternatives _No response_  Additional context _No response_",2024-02-16T10:52:55Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/120074
yi,Add torch._library.impl_abstract_class to fakify torchBind class,"  CC(Fakifying script object by wrapping python)  CC(Support method calls of torch script object)  CC([export] assert inline export graph module for torchbind object)  CC(Add torch._library.impl_abstract_class to fakify torchBind class) We fakify the script object arguments of custom ops right before fake tensor mode dispatch. In order for the subsequent invocation of other custom ops on the same script object to get the same fakified script object, we cache the fakified object.  It makes use of the `__hash__` method instead of directly key on the script object itself because script object doesn't support `__eq__` by default. This is also consistent with prior art in here and here  : D54136016",2024-02-16T00:44:29Z,oncall: jit Stale ciflow/inductor release notes: AO frontend,closed,0,4,https://github.com/pytorch/pytorch/issues/120045,", can you please help pull in the right TorchScript reviewers?","re: torchscript, I'll assume you don't need my review but feel free to add me back / ping me otherwise"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Teach storage deserialization code to respect FakeTensorMode,  CC(Teach storage deserialization code to respect FakeTensorMode) Signedoffby: Edward Z. Yang ,2024-02-15T15:45:24Z,release notes: onnx,closed,0,1,https://github.com/pytorch/pytorch/issues/119990,https://github.com/pytorch/pytorch/pull/108186 was merged with these changes. Thank you!
yi,"Error on trying to run detectron2 on Mac M1 with device set to ""mps"" (Metal Performance Shaders)"," üêõ Describe the bug I am trying to run detectron2 on Mac M1 with device set to ""mps"" (Metal Performance Shaders) The problem is that every time I set up to mps the following error shows up: Error: command buffer exited with error status. 	The Metal Performance Shaders operations encoded on it may not have completed. 	Error:  	(null) 	Internal Error (0000000e:Internal Error) 	     label =       device =          name = Apple M1      commandQueue =          label =           device =              name = Apple M1      retainedReferences = 1 Anyone else with the same problem? I believe there is currently an incompatibility of ""mps"" with detectron2. Because when you set to ""cpu"" it works normally, but it is cpu and therefore slower for training.  Versions Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com) (main, Oct 26 2023, 18:09:17) [Clang 16.0.6 ] (64bit runtime) Python platform: macOS14.2.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Versions of relevant libraries: [pip3] mypyextensions==1.0.0 [pip3] numpy==1.26.4 [pip3] torch==2.2.0 [pip3] torchaudio==2.2.0 [pip3] torchvision==0.17.0 [conda] numpy                     1.26.4          py310hd45542a_0    condaforge [conda] pytorch                   2.2.0                  py3.10_0    pytorch [conda] torch ",2024-02-15T04:40:17Z,triaged module: mps,open,0,7,https://github.com/pytorch/pytorch/issues/119968,"Running into the same error when running `fasterrcnn_resnet50_fpn`, also on M1:  ","Hello, > TL;DR: It is OOM  https://github.com/google/jax/issues/19110issuecomment1868385445 I had same issue on my project. but it becomes to work when I cut down `batch size` literally, I searched the line `Error: command buffer exited with error status.` around every google, github, pytorch forum they commonly say that reducing batch size was work in some how. VRAM issue is the main reason I suspect. no one knew the actual reason but their experience have suffcient grounds. hope this would help","Same issue here, also with `fasterrcnn_resnet50_fpn`. I am using Pytorch Lightning FWIW. > they commonly say that reducing batch size was work in some how Hmm, I am running things with a batch size of 1, and my images are only `64x64`. What's strange is that despite the errors my training loop is still running. In any case, it's running extremely slowly and my entire machine is freezing up.",Same issue with `fasterrcnn_resnet50_fpn` on M1 Pro with a batch size of 2 and 800px images.  Any planned fixes or ways around this ?,same issue with  `fasterrcnn_resnet50_fpn` ,"I have a similar error for **device = ""mps""** and training **torchvision.models.detection.fasterrcnn_resnet50_fpn** network. However, when I hardly code **device = ""cpu""** it performs fast (few seconds) and there's no error message but I cannot say at this moment if the training goes successfully and is not stopped internally without errors. I use MacBook Air M1 8GB RAM 256GB HDD Sonoma 14.5, script of python run from terminal.","I am encountering the same issue while running `fasterrcnn_resnet50_fpn` with `FastRCNNPredictor` on my MacBook M1 Pro using the ""mps"" device (Metal Performance Shaders). The error appears multiple times during training with the following message:  I am using the following setup to train a Faster RCNN model with 2 classes on 800px images:  The error occurs at random points during training, typically after a few iterations, and sometimes freezes the system. Here is my environment:  Like others, I found that training on cpu works, but it‚Äôs slower. Reducing the batch size also delays the issue but doesn‚Äôt fully resolve it, slowing training process. I believe there is still some incompatibility with mps and fasterrcnn_resnet50_fpn ... Has anyone found a reliable workaround or fix for this issue?"
yi,[not ready yet] potentially fix set_() path of fakeifying for subclasses,"  CC([not ready yet] potentially fix set_() path of fakeifying for subclasses)  CC(inductor: fix for functional_collectives.wait() followed by view())  CC(DTensor: when forcing tangents to be correct, force syncing collectives at graph boundaries)  CC(AOTDispatch: allow subclasses to correct when we guess metadata of tangents incorrectly)  CC(AOTAutograd: ensure traced tangent subclass metadata takes noncontiguous outputs into account)  CC(dynamo: support placement kwargs for DTensor.to_local())  CC(dynamo: handle DTensor.device_mesh.device_type)",2024-02-15T00:45:47Z,Stale ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/119948,"I don't think this is actually needed for the internal fixes. Once the PR's below this one in the stack land, I'll confirm that internal works properly and close this PR for now (but this fix might be necessary at some point)","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llm,test change to llm indexer,Fixes ISSUE_NUMBER,2024-02-14T22:40:45Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/119931
rag,[dtensor] ops coverage tracker,"This issue is the overall tracker of the ops we are implementing currently and plan to implement soon, when we plan to add new ops we'll also try to update this issue to keep things in sync.   Current supported ops consolidation replace the existing ops that implemented in rule with op strategy to increase op sharding coverage  [x] aten.slice_backward.default,   [x] aten.bernoulli.default/aten.bernoulli_.float  [ ] aten.split.Tensor, aten.split_with_sizes.default, aten.split_with_sizes_copy.default  [x] aten.cat.default  [ ] aten.index.Tensor  [ ] aten.index_select.default  [x] aten.squeeze.default, aten.squeeze.dim  [x] aten.view.default, aten._unsafe_view.default, aten.reshape.default  [x] aten.unsqueeze.default  [x] aten.expand.default  [x] aten.permute.default  [x] aten.transpose.int, aten.t.default  [ ] aten.repeat.default  [ ] aten.convolution.default  [ ] aten.convolution_backward.default  Ops to be implemented (planned)  [x] CC([dtensor] aten._scaled_dot_product_flash_attention.default)  [x] aten._scaled_dot_product_flash_attention_backward.default  [ ] complex ops  [x] clip_grad_norm_  [x] aten.stack  [ ] non_zero  ",2024-02-14T22:22:07Z,oncall: distributed triaged module: dtensor,open,0,4,https://github.com/pytorch/pytorch/issues/119930,"Hi  , thanks for the great work! I was wondering if that is possible to add usercustomized partition rules to certain operators e.g., `torch.fft.fft ` Specifically, can we extend FFT to dtensor op without diving into the cpp implementation?  Note such implementation is provided in jax (https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html) to make it work in shared arrays. Thank you for your patience."," Yeah this is on our radar! Currently we are focusing on supporting existing torch operators, will provide a way for user to define partition/sharding rules for custom operators!"," CC(DTensor does not yet support torch.nn.init.orthogonal_)  CC(_foreach_mul(DTensor, Tensor scalar) does not work)  CC(torch._foreach_div(DTensor, float) does not work)","Hi  , we've added the support for defining op partition/sharding rules in DTensor (in https://github.com/pytorch/pytorch/pull/131108). Please feel free to try out the API and let us know any feedbacks you may have."
transformer,[dtensor] add support for loss parallel,"  CC([dtensor] add support for loss parallel) Loss parallel is the last piece of sequence parallelism to enable. It enables efficient distributed cross entropy computation when the input is sharded on the class dimension (in a classification problem with many classes). The implementation is via a context manager `loss_parallel`, after enabling which users can directly use `torch.nn.functional.cross_entropy` or `torch.nn.CrossEntropyLoss` without modifying other parts of their code. Here are the underlying rationales why we are going through these op replacements: 1. `nn.functional.cross_entropy` is the common method that OSS user is using for things like transformer training, to avoid changing user code, we want user to still use this function for loss calculation if they are already using it. 2. `nn.functional.cross_entropy` boils down into `aten.log_softmax` and `aten.nll_loss_foward/backward`, and DTensor now supports those ops already ( CC([dtensor] switch softmax forward ops to OpStrategy) CC([dtensor] switch softmax backward ops to OpStrategy) CC([dtensor] add op support for nll_loss_forward) CC([dtensor] add op support for nll_loss_backward)). They are doing computation with input *replicated* on the class dimension. 3. However when the input of this loss calculation is **sharded on the class dimension**, to run sharded computation efficiently, we need to run both `aten.log_softmax` and `aten.nll_loss_foward` with multiple allreduce collectives **in the middle of** those aten ops. This is not possible if we are just overriding these two ops, so we need to have some way to **decompose** these two ops into smaller ops to have collec",2024-02-14T07:29:34Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,4,https://github.com/pytorch/pytorch/issues/119877,"> Why is a context manager needed (and why do we need to register special replacements for the operators in dispatch table)  why can't we just add behaviors into DTensor's torch function so that when it handles these operators it calls these remap functions? Great questions! Here's just my understanding. Wanchao can say more on this. For cross_entropy related ops, if loss parallel is not needed, we've implemented them in the normal way ( CC([dtensor] switch softmax forward ops to OpStrategy) CC([dtensor] switch softmax backward ops to OpStrategy) CC([dtensor] add op support for nll_loss_forward) CC([dtensor] add op support for nll_loss_backward)). When loss parallelism is needed, we need customized and somewhat slower op implementations in this PR. There has to be a way DTensor detects the intention of doing loss parallel or not. Using context manager gives the control to users; another way to do it involves automatically detecting if input is sharded on the dimension where loss parallel can be efficient  I explored that path as well (in CC([dtensor] add support for loss parallelism)) but that requires similar amount of customized code and/or even slower computation, in addition to moderate surgery to DTensor dispatching logic.","l Please update the PR summary with more context and design on loss parallel.  loss parallel is the last piece of sequence parallelism we want to enable. The underlying rationales why we are going through this op replacements instead of just support it via sharding propagation: 1. nn.functional.cross_loss_entropy is the common method that OSS user is using for things like transformer training, to avoid changing user code, we want user to still use this function for loss calculation if they are already using it. 2. `nn.functional.cross_loss_entropy` boils down into `aten.log_softmax` and `aten.nll_loss_foward/backward`, DTensor supports those ops now already. 3. However when the input of this loss calculation sharded on the vocab_dim, to run sharded computation efficiently, we need to run both `aten.log_softmax` and `aten.nll_loss_foward/backward` with multiple allreduce collectives **in the middle of** those aten ops. This is not possible if we are just overriding these two ops, we need to have some way to decompose these two ops to smaller ops to have collectives run in the middle of these two ops 4. We explored the decomposition way, and it seems working, except that because nll_loss_backward in aten is implemented in a inefficient way, we would trigger additional collectives. Recently some user also reported similar issues  CC(Performance Concern about CrossEntropyLoss) Therefore we are doing this way of decomposition by our own currently for sequence parallelism specifically. Once we have a better decomposition in core, we can possibly take that instead of reinventing the wheels here", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,Different numerical results during training with CUDA graphs," üêõ Describe the bug I am trying to wrap a sequence of Llama decoder layers (as implemented by huggingface) with a cuda graph to speed up training. Without cuda graphs, the loss decreases normally. With cuda graphs, the loss decreases at a slower rate and does not converge to the same solution. I have some custom kernels in my linear layer that decompress quantized weights but I verified these are not the source of the issue by premanifesting the decompressed weights and just calling x.T in the linear layer.  Is there a specific way that cuda graphs need to be set up to support training? Details:  I am working on the end to end fine tuning part of quip. The latest code is in the quip repo https://github.com/CornellRelaxML/quipsharp. The fine tuning script is here https://github.com/CornellRelaxML/quipsharp/blob/main/quantize_llama/finetune_e2e_llama.py.   This script takes a llama model and shards it across multiple gpus with a simple shard script https://github.com/CornellRelaxML/quipsharp/blob/main/lib/utils/shard_model.py.   Each shard can be wrapped with a cuda graph wrapper (https://github.com/CornellRelaxML/quipsharp/blob/6e666a034b54369e2b1e0a9a7c3c5c850998a4cd/lib/utils/shard_model.pyL68, graph wrapper defined in https://github.com/CornellRelaxML/quipsharp/blob/main/lib/utils/graph_wrapper.py).   If I use L68 instead of L69, I get different loss values during training. The initial loss value before training is the same in both cases, so the forward pass is correct and there is something going on with the backward pass.  To reproduce, run `python m quantize_llama.finetune_e2e_llama base_model metallama/Llama27bhf hf_path relaxml/Ll",2024-02-14T06:02:33Z,triaged module: numerical-reproducibility module: cuda graphs,open,0,8,https://github.com/pytorch/pytorch/issues/119873,"There are some safety conditions which must be fulfilled for CUDA graphs to give valid results. Hypothetically, torch.compile on PT2 with cudagraphs should automatically test these safety conditions. If you're willing to do a detour in making your code torch.compile'able, it might tell you about what might be the problem.","I did try torch.compile in both regular and 'reduceoverhead' (cuda graphs) mode on the Shard class. In both cases, I got a rather lengthy error. I can post those later today. Is there anything obviously wrong with what I'm doing? Essentially the structure is this:  A shard wrapper model that has n shards, each of which is on a specific gpu  Each shard has its own cuda graph  Only capture the forward pass in the graph  During training, do the usual training loop but call graph.replay() on the shards sequentially to do a forward pass and do the backward pass normally.","Well, it's not clear to me how you can ""do the backward pass normally"", because ordinarily when you run a forwards pass, on CPU we setup an autograd graph that says how to do backwards. If you cudagraph replay, though, this CPU compute is skipped (the point of cudagraphs) and now we can't backward. To cudagraph, you need to cudagraph both forward and backward (which is what PT2 would do for you.) Not so sure about the multigpu interaction though.","So if I‚Äôm understanding you correctly, I need to capture both the forward and backward pass in the cuda graph for cuda graph training to work? How would this work if I need to shard my model over multiple gpus? Cuda graphs don‚Äôt support capturing over multiple gpus so I‚Äôll need to split the model over multiple graphs, but that prevents capturing the backward pass in the same graph (there‚Äôs not even one graph anymore)? From: Edward Z. Yang ***@***.***>  Sent: Sunday, February 18, 2024 7:23 PM To: pytorch/pytorch ***@***.***> Cc: Albert Tseng ***@***.***>; Author ***@***.***> Subject: Re: [pytorch/pytorch] Different numerical results during training with CUDA graphs (Issue CC(Different numerical results during training with CUDA graphs)) Well, it's not clear to me how you can ""do the backward pass normally"", because ordinarily when you run a forwards pass, on CPU we setup an autograd graph that says how to do backwards. If you cudagraph replay, though, this CPU compute is skipped (the point of cudagraphs) and now we can't backward. To cudagraph, you need to cudagraph both forward and backward (which is what PT2 would do for you.) Not so sure about the multigpu interaction though. ‚Äî Reply to this email directly, view it on GitHub  , or unsubscribe  . You are receiving this because you authored the thread.   Message ID: ***@***.*** ***@***.***> >",I know you're not going to like this answer... but if you use DDP instead of DP you will not have this problem :P,I'm not using DataParallel though? The shard wrapper thing is just a quick wrapper class I wrote to manually do sharding. I did consider FSDP  do you know if that works with cuda graphs correctly?,", it feels like it could in principle but I don't know if we've done it in practice","> Hypothetically, torch.compile on PT2 with cudagraphs should automatically test these safety conditions. If you're willing to do a detour in making your code torch.compile'able, it might tell you about what might be the problem.  Could you please point to some documentation/blog that discusses what are the capabilities of `torch.compile` in the context of CUDA Graph? Like, what are the ""safety conditions"" which get automatically tested?"
rag,[inductor] Recursivly unwrap_storage_for_input when convert_to_reinterpret_view fails,"Summary: When, during `ExternKernel.realize_input` call, underlying `ExternKernel.convert_to_reinterpret_view` fails, we currently fall back to `cls.copy_input` here: https://github.com/pytorch/pytorch/blob/31e59766e7e7b51e8dddd4a6967891ac01f4d37b/torch/_inductor/ir.pyL3805L3816 This creates a `TensorBox(StorageBox(...))` wrapped output, which causes a problem for this assertion: https://github.com/pytorch/pytorch/blob/31e59766e7e7b51e8dddd4a6967891ac01f4d37b/torch/_inductor/ir.pyL3479 Here we add a special case handling for this to unwrap `x` recursively. Test Plan: This local repro:  with this line:  https://github.com/pytorch/pytorch/blob/690f54b0f5fa911ba9f7cb6f2ef9719ec765d2d2/torch/_inductor/fx_passes/post_grad.pyL650 changed to `if cond(*args, **kwargs):` fails before and succeeds after this PR. Differential Revision: D53743146 ",2024-02-14T02:31:25Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/119867,This pull request was **exported** from Phabricator. Differential Revision: D53743146," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llama,Optimize multi_tensor_apply_for_fused_optimizer,"  CC(Optimize multi_tensor_apply_for_fused_optimizer) This PR applies the optimization introduced in https://github.com/pytorch/pytorch/pull/119764 to `multi_tensor_apply_for_fused_optimizer`.  Benchmark  Shard llama 2 70B parameters 128way and run optimizers on one shard  H100 (HBM2E, 500W)  (The benchmarks were run with bf16. Need to also run them with full precision copy)  Trace Fused Adam (baseline): fbonly link, json Fused Adam (this PR): fbonly link, json Fused Adam + amsgrad (baseline): fbonly link, json Fused Adam + amsgrad (this PR): fbonly link, json Fused AdamW (baseline): fbonly link, json Fused AdamW (this PR): fbonly link, json Fused AdamW + amsgrad (baseline): fbonly link, json Fused AdamW + amsgrad (this PR): fbonly link, json",2024-02-13T22:40:06Z,ciflow/trunk release notes: cuda ciflow/periodic,closed,1,1,https://github.com/pytorch/pytorch/issues/119842,Abandoning this for now since we are reverting the base optimization due to some regressions: https://github.com/pytorch/pytorch/pull/123763
rag,[DCP] Adds storage reader and planner classes for online loading/sharding of models in torch.save format,  CC([DCP] Adds main in format utils)  CC([DCP] deletes legacy formatting test)  CC([DCP] Adds storage reader and planner classes for online loading/sharding of models in torch.save format) as title Differential Revision: D53718041 ,2024-02-13T19:03:25Z,oncall: distributed fb-exported Merged ciflow/trunk ciflow/periodic release notes: distributed (checkpoint) module: distributed_checkpoint,closed,0,14,https://github.com/pytorch/pytorch/issues/119816,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041,This pull request was **exported** from Phabricator. Differential Revision: D53718041, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,"6 Dynamo test are failing with ""torch.utils.checkpoint: trying to save more tensors during recomputation than during the original forward pass."".","6 Dynamo test are failing with ""torch.utils.checkpoint: trying to save more tensors during recomputation than during the original forward pass."".  Repro `PYTORCH_TEST_WITH_DYNAMO=1 pytest test_autograd.py v k test_nested_checkpoint_early_stop_False` You will need to remove the skip or expectedFailure before running the repro command. This may be just removing a line in dynamo_test_failures.py  Failing tests Here's a comprehensive list of tests that fail (as of this issue) with the above message:  Click me   ",2024-02-13T15:39:13Z,triaged oncall: pt2 module: dynamo,open,0,1,https://github.com/pytorch/pytorch/issues/119794,"Making this low priority. The tests create a specific interaction, which is unlikely to happen in real usecases. The issue still exists though."
rag,Add hpu device support in storage/resize,Add hpu device to   In storage method resize_   is_supported_device for fsdp   for storage add hpu device support Fixes ISSUE_NUMBER ,2024-02-13T06:46:38Z,oncall: distributed triaged open source Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/119761,"fyi, this might landrace with https://github.com/pytorch/pytorch/pull/119671 which migrated the `THPStorage_resize_` logic, I would recommend rebasing after that lands","> fyi, this might landrace with CC(Refactor THPStorage_resize_) which migrated the `THPStorage_resize_` logic, I would recommend rebasing after that lands yes, rebased and pushed the chnages", rebased and pushed the patch. , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,SDPA Pattern Match for hf-GPT2," üöÄ The feature, motivation and pitch GPT2 SDPA pattern is not currently being matched by Inductor patternmatcher. I tried adding a corresponding pattern but it looks like `l__mod___transformer_h_0_attn_bias[(slice(None, None, None), slice(None, None, None), slice(0, 256, None), slice(None, 256, None))]` is hindering its matching. Please suggest a way to match this pattern. Thanks!  Command to run hfGPT2 with TorchBench   The portion of the graph corresponding to the SDPA pattern I'm trying to match, so that it can be replaced with a custom fusion    The pattern I'm trying to match    Alternatives Please suggest an alternative to add a new SDPA pattern corresponding to GPT2's MHA pattern, so that it can be replaced with a custom fusion on CPU. Thanks!  Additional context https://github.com/pytorch/pytorch/pull/97741 added a way to replace attention patterns with fused ops before postgrad passes. ",2024-02-13T06:38:48Z,feature triaged oncall: pt2 module: inductor inductor_pattern_match,closed,0,6,https://github.com/pytorch/pytorch/issues/119760,The first thing we do in freezing is run joint graph passes which include SDPA : https://github.com/pytorch/pytorch/blob/main/torch/_inductor/compile_fx.pyL1023.   I don't think this is a freezing issue. We are not matching this particular SDPA for other reasons.,Confirmed does not match without freezing either.  the patterns are not applied at that level of the IR. They are applied on the joint graph.,"Hi , thanks for your help! Please advise if the GPT2 MHA pattern would be more amenable to matching in some custom patternmatching pass after freezing, or if there's some way to match it (after adding its corresponding pattern in `torch/_inductor/fx_passes/fuse_attention.py`) in joint graph passes as well. Thank you! Could you please advise how I could modify the GPT2 pattern or the BERT_PyTorch SDPA pattern below  (whose matching is failing with itself, which I verified by setting a breakpoint for its replacement) to make it match with itself? That'd help me develop an intuition regarding patternmatching. Thanks again! ","Hi , the above `BERT_pytorch` pattern actually matched the SDPA pattern while running its workload. I added details in CC(Inductor patternmatching seems to be sensitive to the presence/absence of clone nodes). It looks like my test script above somehow works differently from SDPA patternmatching UTs, so I should've used those UTs instead. I'll try matching the GPT2 SDPA pattern again as well & will try replacing it with a custom fusion. If I'd succeed, then I'd update this ticket & close this issue. Thanks!","Hi  , I tried matching the hf_GPT2 SDPA pattern again (so that I could replace it with a custom fusion for CPU), but its matching failed. Please correct me if I'm wrong, as that'd help me understand how patternmatching works  I think that the following differences between the serialized pattern & the graph of `hf_GPT2` at runtime should not have mattered during matching (i.e. the lines that I commented with a leading `` character correspond to nodes that would have been outside the subgraph of the pattern I tried adding in `torch/_inductor/fx_passes/fuse_attention.py`, which should have matched the serialized pattern.   Thank you!","Sorry for the inconvenience! I was able to match the pattern by fixing a bug in it (`where` invocation was incorrect  the order of arguments had been swapped), and by having multiple outputs (also outputting `key` & `value` post permute to `(0, 2, 1, 3)`), since the `hf_GPT2` TorchBench workload also returns them."
rag,[Feature Request] Allow DTensor input when it doesn't share storage with mutated aliased inputs," üöÄ The feature, motivation and pitch Currently, if a function: (1) takes DTensor input (2) takes two aliased regular tensor inputs and mutates any of them in the graph e.g.  then compiling that function with ""aot_eager"" backend will bail out with the following error in AOTAutograd:  Throwing error here makes sense for general tensor subclass in the wild because we don't know how the alias relationship is set up within that tensor subclass (per ). However, for DTensor whose implementation we have full knowledge of, we should be able to *not* throw error for the simple case when the DTensor `._local_tensor` doesn't share storage with any of the mutated aliased input tensors, since in this case we know it's safe. We should improve AOTAutograd to not throw error in this case. This use case is encountered when tracing perparameter FSDP. Repro:   Alternatives _No response_  Additional context _No response_ ",2024-02-13T05:15:27Z,oncall: distributed feature triaged oncall: pt2 module: dtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/119755,"Hi  , we are scrubbing old issues. Is this still relevant or should we close? ",No longer an issue because right now we choose to ignore (notfunctionalize) those mutation ops and instead just make sure that they don't have aliases in the graph inputs.
rag,[inductor] Support storage resizing,  CC([inductor] Support storage resizing)  CC(Refactor THPStorage_resize_)  CC([dynamo] Capture untyped_storage().resize_()) ,2024-02-13T03:50:00Z,Merged ciflow/trunk release notes: fx module: inductor module: dynamo ciflow/inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/119749,"looks like fully general support for compiling storage resize, which is nice. but how do we reason about this in the inductor optimizations?  Are we attempting to preserve the eager FSDP's indication of when to drop/restore memory allocations, or are we free to reorder/fuse/skip them?","> looks like fully general support for compiling storage resize, which is nice. >  > but how do we reason about this in the inductor optimizations? Are we attempting to preserve the eager FSDP's indication of when to drop/restore memory allocations, or are we free to reorder/fuse/skip them? It is treated as muation op, so inductor can't reorder the storage resize with respect to other uses. What optimization are you thinking about there?","> What optimization are you thinking about there? well, for fsdp tracing the different schools of thought could be (a) ignore everything eager did except that it asked for allgathers/reducescatters.  Then rely on the partitioner and communication optimization passes to figure out good balance between communication overlap and peak memory.  and (b) being to actually respect what eager did, but that means you're locked into checkpointing, bucketing, and comm overlap decisions early on. For perparam fsdp at least, we wanted inductor to help with bucketing optimizations, though i'm not sure if that's really happening.  ","If we want to ignore these ops, we can just write a compiler pass to delete them or modify them.  This PR just tries to capture the program as written.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,[MPS] Enable more bfloat16 ops,"Introduce conveninence inlinable `mps::supportedFloatingType` function that returns true if type is Float, Half or BFloat16 Test by running LLM inference using bfloat16",2024-02-13T01:13:39Z,Merged ciflow/trunk topic: not user facing release notes: mps ciflow/mps,closed,0,5,https://github.com/pytorch/pytorch/issues/119738, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `malfet/mpsenablemovebfloat16ops` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout malfet/mpsenablemovebfloat16ops && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Refactor THPStorage_resize_,  CC([inductor] Support storage resizing)  CC(Refactor THPStorage_resize_)  CC([dynamo] Capture untyped_storage().resize_()) Moving code around to allow it to be reused in the next PR.,2024-02-12T04:38:12Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/119671, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[dynamo] Capture untyped_storage().resize_(),"  CC([inductor] Support storage resizing)  CC(Refactor THPStorage_resize_)  CC([dynamo] Capture untyped_storage().resize_()) This makes storage resizing work with `backend=eager`, the next two PRs make it work for inductor. ",2024-02-10T23:27:14Z,Merged ciflow/trunk module: dynamo ciflow/inductor release notes: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/119647, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[dynamo] Support torch.distributed.fsdp._flat_param._same_storage_size,  CC(Refactor THPStorage_resize_)  CC([dynamo] Capture untyped_storage().resize_())  CC([dynamo] Support torch.distributed.fsdp._flat_param._same_storage_size) Replaces CC([dynamo][distributed] Allow _same_storage_size in graph) ,2024-02-10T05:23:52Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/119627, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Export of bitwise_right_shift to ONNX needed for llama2-7b 8-bit quantized pytorch model," üöÄ The feature, motivation and pitch We are attempting to export a quantized llama model (from HuggingFace) to ONNX but are running into an unsupported op error for bitwise_right_shift: `torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::bitwise_right_shift' to ONNX opset version 17 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.` Would you please consider adding support for exporting this op to onnx?  Sample python to reproduce below.  Thanks!  Alternatives _No response_  Additional context ",2024-02-10T01:37:10Z,module: onnx triaged OSS contribution wanted,closed,0,2,https://github.com/pytorch/pytorch/issues/119621,"Is there any workaround for getting int8 ONNX model for llama2? Also, wanted to know approximate timeframe for resolution of this issue.",bitwise_right_shift is not supproted by torch.onxn.export and it is not planned to be Please try the new ONNX exporter and reopen this issue if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial We do export llama v2 to onnx using the new exporter :)
transformer,Fixes #105077 alternative 2,Fixes CC(torch.load fails under FakeTensorMode for GPT2 model)  Repro  Error:  ,2024-02-09T19:47:55Z,open source ezyang's list module: fakeTensor,closed,0,1,https://github.com/pytorch/pytorch/issues/119584,super seeded by https://github.com/pytorch/pytorch/pull/108186
transformer,[no ci][FSD2] Added annotations for compute compile,"  CC([no ci][FSD2] Added annotations for compute compile)  CC([FSDP2] Added pre/postallgather extensions)  CC([FSDP2] Generalized allgather outputs to >1 per parameter)  CC([FSDP2] Added CPU offloading)  CC([FSDP2] Used stream APIs for CUDA event handling) **This PR is not for landing.** **Some notes**  To disable using `DTensor` as the representation for sharded parameters, change https://github.com/pytorch/pytorch/blob/a1ff82d4a5ee90896a05533d178fbb153dd78d6f/torch/distributed/_composable/fsdp/_fsdp_param.pyL393L408 to return `tensor` directly (and it will not be wrapped with `DTensor`). I am not sure if there are any breakages from that (e.g. accesses like `self.sharded_param._local_tensor`).  This PR includes https://github.com/pytorch/pytorch/blob/a1ff82d4a5ee90896a05533d178fbb153dd78d6f/torch/distributed/_composable/fsdp/fully_shard.pyL119L121 to have Dynamo take the `FSDPManagedNNModuleVariable` path. If that is not desired, feel free to remove those lines. **Testing** There are two example unit tests that just run training on a small transformer with FSDP applied _without compile_ (compile can be added to these tests as desired):  The multithreaded one uses multithreaded process group, which is faster (e.g. 6 seconds to run that test vs. 11 seconds to run the multiprocess one). The multithreaded one should suffice for testing, but I included the multiprocess one as well just in case. Here is  's test script including compile: https://gist.github.com/yf225/34629198eb597ea80563087d6a4238d4 It may be more helpful than `test_compile_multi_thread` above.",2024-02-09T14:52:07Z,release notes: distributed (fsdp),closed,0,1,https://github.com/pytorch/pytorch/issues/119551,"    There is one interesting note: Currently, perparameter FSDP uses module forward hooks (`register_forward_pre_hook` / `register_forward_hook`). However, we could also override `nn.Module.forward()` using the `FSDP` class that we dynamically swap with. In a closed system with only FSDP, these two approaches are equivalent. However, beyond FSDP, this affects ordering (e.g., float8 and DTensor hooks), and it affects whether FSDP pre/postforward logic is included if doing `module.forward = torch.compile(module.forward)` after applying FSDP. For float8, we really want to be able to compile at least parts of the preforward. Given this, it might be preferred to override `nn.Module.forward()` so that we can include the preforward as part of the `torch.compile(module.forward)`. Otherwise, we would need some other scheme to compile like `model = torch.compile(model)` (toplevel), which has not been as stable."
agent,CUDA Memory error while having a lot of free memory," üêõ Describe the bug Hey, I'm training a reinforcement learning agent on my device. The models are rather small and the entire memory it takes (at peak usage) is around 870 MB. My GPU is NVIDIA GeForce GTX 1650 TI with 4 GB memory. I'm using CUDA V12.3 Torch V2.2.0 + cu118 My batch size is practically 1 since I'm iteratively sampling a transition, calculating the loss for it, and finally back propagating after accumulating enough transitions. After processing the transition I'm moving it back to the CPU.  I can't understand  why I'm getting the errors if the memory doesn't even get close to the capacity. here are the errors that I'm getting and a snippet of the code that handles the loss calculations: !error !update_batch !update_loss !backward !parameters_update ''' Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 2.35 GiB is free. Of the allocated memory 825.15 MiB is allocated by PyTorch, and 24.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management '''  Versions Collecting environment information... PyTorch version: 2.2.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Home GCC version: Could not collect Clang version: 14.0.0 CMake version: Could not collect Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:",2024-02-09T13:52:27Z,module: cuda module: memory usage triaged module: regression,open,0,8,https://github.com/pytorch/pytorch/issues/119547," Are the parameters in the model 870MB or have you measured previously on another machine that the _peak_ memory is 870MB? If the first, which is what I am suspecting, the optimizer being Adam will require 2X that memory for the state, and the grads will also take up 1X the parameters, meaning we'll have 4X the memory of 870MB once the first optimizer parameter update happens. This is likely the case if you're hitting the OOM on the very first optimizer step after forward and backward. Could you confirm when you see the OOM and whether the 870MB is peak memory from a previous run on a different machine vs something you know from checkpoints?","Model parameters, as they are on the disk, add to 20 MB. The model is a CNN with FC layers added up so the intermediate channels add up I guess. As I was looking at the memory usage via control panel and nvidiasmi I saw it was peaking around 1.6GB. Normal usage of the GPU consumes around 0.60.7 GB, so 870 MB does add up. I'v run the model only on my own machine. Another important note is that it started happening after upgrading my python, torch and CUDA. I'v run this model with no problems at all with python 3.7 and a much older version of python and CUDA like 2 weeks ago. Optimizer is RMSProp.","Do you mind sharing what your older torch/python/cuda versions were before this? If you have access to the optimizer (not sure if this is the ultimate culprit), can you try to see if it would still OOM if you run it with RMSProp(..foreach=False...)? One change I'm aware of in the past few years is that we now default to the faster foreach implementation for optimizers, which uses more memory. Another thing to try to debug memory is if you're able to get a memory snapshot with the tools here: https://pytorch.org/docs/stable/torch_cuda_memory.html. Could you also share the stack trace when the OOM occurs?","I think it was torch 11.3, python 3.7, don't remember cuda version... I'll try and update as it goes along and send the snapshot right before the failure. Thank you.",Was reproduced again. Here are the screenshots of the memory_viz: !trace stack !cached time !memory time,"Hm, it is strange that your memory profile is all flatI would expect intermediates and activations to build up as the process goes, which suggests that you're not wrapping the right places in code for the snapshot. Also, the forum may be a better place for this discussion: https://discuss.pytorch.org/new","I'm creating a snapshot after each episode in the reinforcement learning algorithm. I've set _record_memory_history to True. Where should I wrap it, if the places I've put them in the wrong place... I'll open a discussion on the forums with reference to this issue.","> ... I'll open a discussion on the forums with reference to this issue. Hi! Please post the forum discussion, it would be helpful."
dspy,Remove hard numpy dependency from guards.py,I'm not sure if this is the ideal behavior / best fix for this. ,2024-02-09T01:24:20Z,triaged Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,1,10,https://github.com/pytorch/pytorch/issues/119519, rebase, merge, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `cpuhrschpatch1` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout cpuhrschpatch1 && git pull rebase`)"," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 13 jobs have failed, first few of them are: pull / linuxfocalpy3.8clang10onnx / test (default, 1, 2, linux.2xlarge, unstable), pull / linuxfocalpy3.8clang10onnx / test (default, 2, 2, linux.2xlarge, unstable), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_huggingface, 1, 1, linux.12xlarge, unstable), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_timm, 1, 2, linux.12xlarge, unstable), inductor / linuxjammycpupy3.8gcc11inductor / test (cpu_inductor_timm, 2, 2, linux.12xlarge, unstable) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,Issue running squeezeLLM quantization of LLama-2-7B on VLLM," üêõ Describe the bug `RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED at ""/tmp/pipbuildenvtq9d4ekf/overlay/lib/python3.11/sitepackages/torch/include/c10/cuda/impl/CUDAGuardImpl.h"":25, please report a bug to PyTorch.` I am reporting this error here because it asked me to. I am trying to test llama2 quantized with squeezellm using vllm. I've attached the code to reproduce. bug_repro.txt  Versions Collecting environment information... PyTorch version: 2.1.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.24.4 Libc version: glibc2.35 Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.01103azurex86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe Nvidia driver version: 510.108.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.6 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits phys",2024-02-08T22:48:48Z,needs reproduction oncall: quantization module: cuda,closed,0,5,https://github.com/pytorch/pytorch/issues/119494,"Thank you for your bug report. Unfortunately, I cannot reproduce the issue with the given code on my A10 instance (output below). Maybe the problem is related to the local Python setup or the specific hardware in use.   ``` config.json: 100% 554/554 [00:00=1.17.3 and ={np_minversion} and ",Thanks for reporting this. It does look like this cannot repro so this might be due to the way the SqueezeLLM is using PyTorch constructs. Did you try reporting this issue to them directly?,"Thanks for the update! This is not a huge blocker for us. I only reported it to you all because the error message asked me to üôÇ. I will report it to SqueezeLLM since it appears to be an issue on their end. Julianne ________________________________ From: albanD ***@***.***> Sent: Monday, February 12, 2024 10:17 AM To: pytorch/pytorch ***@***.***> Cc: Author ***@***.***> Subject: Re: [pytorch/pytorch] Issue running squeezeLLM quantization of LLama27B on VLLM (Issue CC(Issue running squeezeLLM quantization of LLama27B on VLLM)) Thanks for reporting this. It does look like this cannot repro so this might be due to the way the SqueezeLLM is using PyTorch constructs. Did you try reporting this issue to them directly? ‚Äî Reply to this email directly, view it on GitHub or unsubscribe. You are receiving this email because you authored the thread. Triage notifications on the go with GitHub Mobile for iOS or Android.",Thanks a lot. I'll be closing this since there doesn't seem to be a way to move forward on our end. Feel free to reopen this issue or a new one if the SqueezeLLM reports that the issue might be on our end.,"[like]  Julianne Knott reacted to your message: ________________________________ From: albanD ***@***.***> Sent: Monday, February 12, 2024 8:14:04 PM To: pytorch/pytorch ***@***.***> Cc: Author ***@***.***>; Comment ***@***.***> Subject: Re: [pytorch/pytorch] Issue running squeezeLLM quantization of LLama27B on VLLM (Issue CC(Issue running squeezeLLM quantization of LLama27B on VLLM)) Thanks a lot. I'll be closing this since there doesn't seem to be a way to move forward on our end. Feel free to reopen this issue or a new one if the SqueezeLLM reports that the issue might be on our end. ‚Äî Reply to this email directly, view it on GitHub or unsubscribe. You are receiving this email because you authored the thread. Triage notifications on the go with GitHub Mobile for iOS or Android."
yi,Clarifying windows cosine behaviour in the documentation,"After following the discussion, I've created a PR to update the documentation clarifying the function's behaviour ( solution 1). Fixes CC(On the correctness of torch.signal.windows.cosine)",2024-02-08T06:35:53Z,open source Merged ciflow/trunk release notes: python_frontend topic: docs,closed,0,8,https://github.com/pytorch/pytorch/issues/119444, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", please fix lint errors (running lintrunner locally should be sufficient),"Thank you , all good now. ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hi , is there anything I have to do on my site? "," no, nothing, change have merged into default branch as https://github.com/pytorch/pytorch/commit/fa071a2e1b6617ba722d639747f1d885017b0e86"
llama,[inductor][cpu]6 models functionality gap between AMP dynamic shape and AMP static shape," üêõ Describe the bug below 6 models passed in AMP static shape, but failed in AMP dynamic shape.                name       accuracy       perf       suite                       hf_T5_generate       X       ‚àö       torchbench                 pyhpc_isoneutral_mixing       X       ‚àö       torchbench                 detectron2_fcos_r_50_fpn       X       X       torchbench                 llama       ‚àö       X       torchbench                 pyhpc_equation_of_state       ‚àö       X       torchbench                 pyhpc_turbulent_kinetic_energy       X       X       torchbench         SW info               SW       Nightly commit       Main commit                       Pytorch       7b9dec1       735715e                 Torchbench       /       ff42d907                 torchaudio       b2d9c3e        CC([ATen] Rename isCuda/Sparse/Distributed to is_cuda/sparse/distributed)                 torchtext       b0ebddc        CC(nn.DataParallel and GPU indexes  Buggy...)                 torchvision       a00a72b        CC(Move backtrace to its own header)                 torchdata       11bb5b8        CC(CUDA sparse operations (+ CPU improvements))                 dynamo_benchmarks       nightly       /          Repro: inductor_single_run.sh bash inductor_single_run.sh multiple inference performance/accuracy torchbench **model** amp first dynamic default ",2024-02-08T02:21:01Z,triaged oncall: cpu inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/119434,"Hi , Could you help to take a look of   hf_T5_generate  pyhpc_isoneutral_mixing  detectron2_fcos_r_50_fpn I will look into   llama  pyhpc_equation_of_state  pyhpc_turbulent_kinetic_energy","Update the failure of `pyhpc_equation_of_state` and `pyhpc_turbulent_kinetic_energy`  It also failed with CUDA BF16 dynamic case and FP32 dynamic case  Failure msg is:    RootCause is:    The default running batch size is 1048576; https://github.com/pytorch/benchmark/blob/dbc109791dbb0dfb58775a5dc284fc2c3996cb30/torchbenchmark/models/pyhpc_turbulent_kinetic_energy/__init__.pyL127    The example_inputs's size does not equal to this batch size directly but with some mathematic relationship: https://github.com/pytorch/benchmark/blob/dbc109791dbb0dfb58775a5dc284fc2c3996cb30/torchbenchmark/models/pyhpc_turbulent_kinetic_energy/__init__.pyL8L63    When running with dynamic batch size, it only annotates the dimension when its size equals to this batch size. And in this case, there is no such dimension which causes the failure; https://github.com/pytorch/pytorch/blob/e5bfdde7ba32fecb0364783115eda6e21fb3e400/benchmarks/dynamo/common.pyL3863L3877 I think maybe we can skip the dynamic testing for these 2 models.","Update the Failure of llama BF16 Dynamic size:  Failed also for FP32 dynamic shape and CUDA BF16 Dynamic shape.  Change the max bs used to generate `self.cache_k` from `32` to `33` (which will be larger than input bs as 32) can workaround this issue.   Same issue as  CC(llama model failed for dynamic shape path), please refer to it for more details."," `pyhpc_isoneutral_mixing`, `pyhpc_equation_of_state` and `pyhpc_turbulent_kinetic_energy` skipped to annotate as dynamic bs in https://github.com/pytorch/pytorch/pull/120599  `LLama` dynamic shape failure is caused by dynamo issue and tracked in   CC(llama model failed for dynamic shape path)  Transfer this issue to  for the remaining failure of `hf_T5_generate` and `detectron2_fcos_r_50_fpn`.","Update the failure of `detectron2_fcos_r_50_fpn`: * Error msg is  * Root Cause is Benchmark code will only annotate the inputs' dim as dynamic when its size equals to batch size https://github.com/pytorch/pytorch/blob/c617e7b4076a5f968f5827040a07b013e45cd0c6/benchmarks/dynamo/common.pyL3867L3871. If it fails to find any dim equals to batch size, above error throws. However, the inputs of `detectron2_fcos_r_50_fpn` are as follows:  None of the inputs' dim will equal to input batch size, so I think we may need to skip the dynamic batch size testing for this model.","Update the failure of `hf_T5_generate`: * Error msg is  * Root Cause is This error happens while creating guard for this model script line: `scores += position_bias_masked` I run it with TORCH_LOGS=""+dynamic"" and got the key line : `I0305 00:21:00.849974 140376923287424 torch/fx/experimental/symbolic_shapes.py:3963] [6/0_1] eval Eq(s0, 4) [guard added] at miniconda3/envs/pt2/lib/python3.9/sitepackages/transformers/models/t5/modeling_t5.py:561 in forward (_refs/__init__.py:403 in _broadcast_shapes)` The reason for this error is that the batch dimension of `inputs_tensor` in the dynamic batch size test is marked as dynamic shape `s0`, so the batch dimension of `scores` generated by a series of operations with `inputs_tensor` is also `s0`. However, because the function of creating `attention_mask` is not in Dynamo but in python. The batch dimension of `attention_mask` is the real shape `4`, and the batch dimension of `position_bias_masked` generated by a series of operations with `attention_mask` is also the real shape `4`, not the dynamic shape `s0`. The current line of `scores += position_bias_masked` requires creating a guard and check whether the batch dimension of `scores` is always equal to the batch dimension of `position_bias_masked`, Eq(s0, 4), the error happens. So the root cause of this error is that the function of creating `attention_mask` not in Dynamo but in python. The reason why the function of `attention_mask` not in Dynamo is that Dynamo has a graph break on this function (happened in the model script line: `is_pad_token_in_inputs = (pad_token_id is not None) and (pad_token_id in inputs)`) due to the following error: `torch._dynamo.exc.Unsupported: Tensor.item`",Close this issue as the related PRs are all landed.
llm,problematic math backend for F.scaled_dot_product_attention in ROCm 6.0 when testing using vllm for generate," üêõ Describe the bug I was testing using the following code in vllm (https://github.com/vllmproject/vllm) to generate responses for gfx1100 arch (while working on this pull request https://github.com/vllmproject/vllm/pull/2768).   The code I was testing is below (changed in `vllm/model_executor/layers/attention.py`) to comparing with `ref_masked_attention() `:  However, the result is not satisfactory when I used below prompts to compare:  The output generated by sdpa is not good. I have changed the way to send the parameters to `F.scaled_dot_product_attention()`, and it seems it does not matter the shape, or the scale. The result remained not acceptable.  Versions  ",2024-02-07T17:29:04Z,module: rocm triaged,closed,0,10,https://github.com/pytorch/pytorch/issues/119389,,"This issue may stem from a sequence of bugs related to the `warpSize` variable. For gfx10 and gfx11 targets, `warpSize` is set to 32, but there is a hardcoded value of 64 that is only applicable to gfx9 targets. As detailed in the ROCm kernel language reference: > The `warpSize` variable, an integer, denotes the number of threads per warp for the target device. Notably, Nvidia devices uniformly use a warp size of 32. In contrast, AMD devices use a warp size of 64 for gfx9, but this changes to 32 for gfx10 and newer architectures.", can you point to the hardcoded warpsize value you're referring to?,>  can you point to the hardcoded warpsize value you're referring to? I found an example at the FBGEMM repo.  I didn't look through more of the repos. I was actually investigating the issue where the float32 performance on AMD W7900 is only half of the peak performance and stumbled upon this issue.," , this was a consumer hardware (navi) specific issue wasn't it?  If so, it'll narrow the scope for the issue.",">  , this was a consumer hardware (navi) specific issue wasn't it? If so, it'll narrow the scope for the issue. On MI250, the issue is the same if I used sdpa to test (though we use flashattention for vllm on MI250 which is quite good): ","> >  can you point to the hardcoded warpsize value you're referring to? >  > I found an example at the FBGEMM repo. >  > I didn't look through more of the repos. I was actually investigating the issue where the float32 performance on AMD W7900 is only half of the peak performance and stumbled upon this issue.  is that just a random example you found, or is fbgemm warpSize the root cause of your current issue?",">  is that just a random example you found, or is fbgemm warpSize the root cause of your current issue?  just a random example. I haven‚Äôt figured out the root cause yet","I am going to close this since I realized that it was a user error by myself.  After some manipulating of input tensors and their shapes, it is fine to use the torch.nn.functional.scaled_dot_product_attention with vllm to get good generation result with llama2 models.",new generated result is as below: 
transformer,aarch64 linux: torch performance is regressed for openblas backend from torch 2.0 to 2.1+," üêõ Describe the bug on aarch64 linux platform, PyTorch inference latencies are increased on torch 2.1 and 2.2 compared to torch2.0 when openblas backend is used for multithreaded configuration. The regression is higher for larger thread counts. On AWS Graviton3, c7g.4xl, with 16 threads, the inference latency with torch2.0 is `Time elapsed: 2.777902126312256 seconds` whereas with torch 2.1 and later, it is `Time elapsed: 4.907686471939087 seconds` **Reproducer:**  gpt2large.py   Versions aarch64 linux, Ubuntu 22.04, torch 2.1 or later",2024-02-07T14:38:32Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/119374,"The fix is merged to main, and is part of today's (20240207) nightly wheel (https://download.pytorch.org/whl/nightly/cpu/torch2.3.0.dev20240207cp310cp310manylinux_2_17_aarch64.manylinux2014_aarch64.whl). with this wheel, on AWS Graviton3, c7g.4xl, the performance issue is no longer seen. Time elapsed: 2.453695774078369 seconds **how to test** ",RC fix confirmed on both t4g.2xlarge   c7g.4xl (Graviton3)  vs 
transformer,Use Dtensor and ColwiseParallel api to do Tensor Parallel for llama2 model but got communication error.," üêõ Describe the bug I read the test_transformer_training example in pytorch/test/distributed/tensor/parallel/test_tp_examples.py, and I think it's really awesome.  Then I use it to do tensor parallel for Llama2 model and now I may met a communication error. Here is my llama2 model code, simpily import them from transformers, which version is 4.37.2:  And here is my tensor parallel code:  And when I do the forward of the llama2 model after tensor shard, it is stucked in the Embedding function which is before attention in llama2 model. Seems like one rank is stucked that I can't go on, and the other three ranks are keep running. I tried it many times and it is always stucked in the same place. It doesn't have any error information or any other tints. !img_v3_027q_1540648b71e8432fba94c41ccaaed06g I have excuted your examples in test_tp_examples.py with success. !img_v3_027o_523b7183d9034e4d8c53c4ca6661ceag Could anybody help me? I really like the ColwiseParallel and the RowwiseParallel design. They are really fantastic and so incredible that I can write my tensor parallel code so concise, I really want to use them successfully.  Versions transformer: 4.37.2 pip list Package                 Version            Editable project location    abslpy                 2.1.0 accelerate              0.26.1 aiohttp                 3.9.3 aiosignal               1.3.1 astunparse              1.6.3 attrs                   23.2.0 blessed                 1.20.0 cachetools              5.3.2 certifi                 2024.2.2 charsetnormalizer      3.3.2 datasets                2.16.1 dill                    0.3.7 expecttest              0.2.1 filelock        ",2024-02-07T02:19:39Z,oncall: distributed triaged module: dtensor,open,2,4,https://github.com/pytorch/pytorch/issues/119343,"Hi, I have figured it out. The code was right but I didn't keep the input the same between different ranks, so it was stucked. Really suggest that there should be a hint or a error message.","Hey  glad that you figured out the issues, happy to know that you like the ColwiseParallel and Rowwise Parallel design! > Really suggest that there should be a hint or a error message. Thanks for the feedbacks! I think the tricky part is that each rank itself won't know other ranks tensor content, if it knows, there must be some additional communication need to be happened for us to perform this sanity check, so we didn't add this sanity check. , something we should think about how to improve sanity checkin when doing TP. We can possibly add some sanity check APIs to install hooks and check in runtime the layers are receiving replicated inputs or not.  At very least, I think we need to improve the API documentations to note that if the ColwiseParallel/RowwiseParallel is using replicated input, user would need to make sure the inputs are actually replicated before calling this module","Thank you for your kindly reply! I think the sanity checkin you mentioned is necessary as we usually randomize the input data in data parallel. So this error easily occurs when we use both TP and DP simultaneously. It seems like some kind of 'silent failure'.  By the way I have tested the result as you tested in the example, it's correct. Thank you so much! !img_v3_027r_05c1fed324034d948be1bf68ef4387ag",how to make sure the input are correctly replicated?
rag,[DCP] Adds storage reader and planner classes for online loading/sharding of models in torch.save format,"This stack adds three utilities * dcp_to_torch_save: a function for offline converting from dcp to torch save * torch_save_to_dcp: a function for offline converting from torch save to dcp * `DynamicMetaLoadPlanner` and `BroadcastingTorchSaveReader`, two classes that can be used with `dcp.load` to load models using the torch save format. After this stack is reviewed, an executable will be added to make utilizing these functions more convenient.   CC([DCP] Adds storage reader and planner classes for online loading/sharding of models in torch.save format)  CC([DCP] Adds utility for converting torch save to dcp)  CC([DCP] Adds utility for converting dcp to torch save format)  CC([DCP] Automatically set `no_dist` if distributed is unavailable) [DCP] Adds storage reader and planner classes for online loading/sharding of models in torch.save format Differential Revision: D53497593 ",2024-02-07T00:01:18Z,oncall: distributed fb-exported ciflow/trunk ciflow/periodic module: distributed_checkpoint,closed,0,1,https://github.com/pytorch/pytorch/issues/119335,Closed in favor of https://github.com/pytorch/pytorch/pull/119816 due to CI issues
chat,expect_true side path to guard_bool via has_hint is not the right thing to do sometimes," üêõ Describe the bug Internal xref: https://fb.workplace.com/chat/t/7786234988056687 chat with    Relevant logs: https://www.internalfb.com/intern/everpaste/?handle=GICWmABuPOPl_HUbAN4pgFiZsI0LbsIXAAAB In some situations, we trigger a guard on data dependent node, even though the computation in question is expect_true:  If you look in the trace, you see that we are indeed in an `expect_true` context, at which point it should be impossible to raise an error. We see this logic:  So if, somehow, a SymNode involving unbacked SymInts, has a hint, then we will try to generate a guard. But it is NEVER ok to generate a guard if it has free unbacked symbols. That's the actually correct test.  Versions main ",2024-02-06T20:29:09Z,triaged oncall: pt2 module: dynamic shapes,closed,0,0,https://github.com/pytorch/pytorch/issues/119309
rag,"Add Python binding `resizable` to class `{Untyped,Typed}Storage`",This PR exposes `resizable` method of `StorageImpl` to Python frontend to make it accessible for users. Fixes CC(Add a binding for Storage resizable) ,2024-02-06T16:45:22Z,open source Merged ciflow/trunk release notes: python_frontend topic: new features,closed,0,8,https://github.com/pytorch/pytorch/issues/119286, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `storageresizable` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout storageresizable && git pull rebase`)", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ",Forcepushed as I messed up the commit message, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,Performance Concern about CrossEntropyLoss," üêõ Describe the bug I've seem a claim floating around that PyTorch's CrossEntropyLoss operation is poorly optimized during the backward pass, and that it can be rewritten to reduce VRAM usage by a massive amount (seemingly by 3x or more!), with no impact on computation speed. This sounds like a tall claim given CrossEntropyLoss is very simple operation, but the authors of the claim has created a repo which contains, among other optimizations, an alternate CUDA kernel for CrossEntropyLoss using Triton, and some people have indeed observed significant VRAM improvement using the author's repo. My understanding is that PyTorch compiles its own CUDA kernel for CrossEntropyLoss, so there doesn't seem to be any reason why PyTorch could not implement it in the most optimal way. I'm not so sure if I should've marked the issue as a bug since I'm not certain of the veracity of this performance optimization claim, but if it's true this seems like a very big performance bug in PyTorch's CrossEntropyLoss implementation. Perhaps someone could take a look to see if it's plausible to optimize PyTorch's CrossEntropyLoss in a similar way? Given that LLM's have a very large logits tensor due to their vocabulary size, an optimization of this operation can potentially reduce training VRAM requirements of a huge number of projects.  Versions PyTorch 2.1.1 ",2024-02-06T07:22:37Z,high priority module: performance module: nn module: cuda triaged,open,1,6,https://github.com/pytorch/pytorch/issues/119261, + softmax_backward discussion we had yesterday, any core tags we can add to this issue and triage? I feel this is something we should investigate into so don't want this issue to get lost,high priority to see how far off it is in practice.,"The aforementioned kernel seems to be specialized for 3D `input`. In terms of performance, I ran a benchmark with the following script where `slow_cross_entropy_loss` is `nn.functional.cross_entropy` and `fast_cross_entropy_loss` is from the repo. The fast kernel seems to perform better on certain shapes, but not across all shapes. Still have to verify the VRAM usage. ```python import torch import torch.utils.benchmark as benchmark import itertools def benchmark_torch_function_in_microseconds(f, *args, **kwargs):     t0 = benchmark.Timer(         stmt=""f(*args, **kwargs)"", globals={""args"": args, ""kwargs"": kwargs, ""f"": f}     )     return t0.blocked_autorange().mean * 1e6 def f(ce, inp, targ):     out = ce(inp, targ)     res = out.sum().backward() if __name__ == ""__main__"":     torch.set_default_device('cuda')     Bs = [1, 128, 512]     Cs = [1, 128, 512]     Vs = [1, 128, 512]     for B, C, V in itertools.product(Bs, Cs, Vs):         inp_fast, targ_fast = torch.randn(B, C, V, requires_grad=True), torch.randint(0, C, (B, V), dtype=torch.long)         inp_slow, targ_slow = inp_fast.clone().detach().requires_grad_(True), targ_fast.clone().detach()         nn_ce_fw_bw_time = benchmark_torch_function_in_microseconds(f, slow_cross_entropy_loss, inp_slow, targ_slow)         fast_ce_fw_bw_time = benchmark_torch_function_in_microseconds(f, fast_cross_entropy_loss, inp_fast.transpose(1, 2).contiguous(), targ_fast)         print(f""{B}, {C}, {V}  ",Removing triage review as discussed at nn triage,"> The aforementioned kernel seems to be specialized for 3D `input`. In terms of performance, I ran a benchmark with the following script where `slow_cross_entropy_loss` is `nn.functional.cross_entropy` and `fast_cross_entropy_loss` is from the repo. The fast kernel seems to perform better on certain shapes, but not across all shapes. Still have to verify the VRAM usage. Wondering if there any recent results about the VRAM usage of PyTorch vs this Triton kernel? It has been some time since the release of Unsloth, and the VRAM improvements of the free version seem to be very credible by now, being reproduced by many users. According to here, a significant portion of the VRAM gains comes from the custom implementation of CrossEntropyLoss."
llm,[WIP] TD LLM retrieval,Fixes ISSUE_NUMBER,2024-02-05T22:52:32Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/119235
rag,Add a binding for Storage resizable, üêõ Describe the bug Internal xref: https://fb.workplace.com/groups/1405155842844877/posts/7973507386009657/ StorageImpl has this field  but there is no way to access it from Python. Make it accessible under a method named resizable() for both TypedStorage and UntypedStorage.  Versions main ,2024-02-05T22:00:43Z,good first issue module: python frontend,closed,0,5,https://github.com/pytorch/pytorch/issues/119233,A unit test is potentially ,"For extra credit, add logging when resizable flips from True to False. I'm not convinced we actually should do this, because TBH the original user code that had this problem is sus.",Working on this to get my feet wet for contributing to PyTorch.,"Hi, I managed to get this binding working and want to write some test code, however, the potential unit test won't work because `ndarray` doesn't have the `storage()` method. Can you teach me a way to create tensors with unresizable storage?","oops, I typoed, example is fixed now"
rag,DISABLED test_unwrap_storage_didnt_work_repro_cuda (__main__.TestInductorDynamicCUDA),"Platforms: linux, slow This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_unwrap_storage_didnt_work_repro_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `inductor/test_torchinductor_dynamic_shapes.py` ",2024-02-05T21:39:31Z,triaged module: flaky-tests skipped module: inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/119231
gpt,Fix torch.load failure under FakeTensorMode for GPT2 model,Fixes CC(torch.load fails under FakeTensorMode for GPT2 model) ,2024-02-05T20:08:59Z,module: serialization triaged open source module: meta tensors module: fakeTensor,closed,0,3,https://github.com/pytorch/pytorch/issues/119222,"I'm wondering, what would be the intended behavior of `torch.save` when it's executed inside a `FakeTensorMode` context?"," it should error, in the same way you can't actually poke at a fake tensor and get some real bytes out of the data.",super seeded by https://github.com/pytorch/pytorch/pull/108186
yi,"Reserve sizes in c10::VaryingShape::concrete_sizes(), c10::TensorType::computeStrideProps()",Summary: Costly reallocs. Test Plan: CI Reviewed By: efiks Differential Revision: D53264908,2024-02-05T13:28:59Z,fb-exported Merged,closed,0,8,https://github.com/pytorch/pytorch/issues/119189,The committers listed above are authorized under a signed CLA.:white_check_mark: login: IosifSpulber  (426fa4c1da1ed0495b48d964cd96d83abb684447),This pull request was **exported** from Phabricator. Differential Revision: D53264908,/easycla,This pull request was **exported** from Phabricator. Differential Revision: D53264908,This pull request was **exported** from Phabricator. Differential Revision: D53264908,This pull request was **exported** from Phabricator. Differential Revision: D53264908," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Can torch use make_storage_impl to replace c10::make_intrusive<c10::StorageImpl>," üöÄ The feature, motivation and pitch Same question to https://github.com/pytorch/pytorch/pull/118459 but api is ``new_qtensor``. I'm thinking if we can use ``make_storage_impl`` to replace ``c10::make_intrusive`` in everywhere? Compare to ``c10::make_intrusive``, ``make_storage_impl`` is more general and allows ``PrivateUse1`` backend to use its registered StorageImpl. https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL315 https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL350L356 So what about moving ``make_storage_impl`` to ``c10\core\StorageImpl.h`` and replace c10::make_intrusive in relative apis?  Alternatives _No response_  Additional context _No response_ ",2024-02-04T08:32:58Z,module: internals triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/119146,", same question to https://github.com/pytorch/pytorch/pull/118459, what do you think?",They don't seem directly compatible so it doesn't seem like you'd be able to use it everywhere.
rag,[storage][perf] Reduce _get_device_from_module overhead.,Using `rsplit` with maxsplit=1 is more efficient since it 1) stops traversal as soon as the first `.` from the right side is encountered 2) creates no more than 2element list This change also reuses `last_part` to avoid unnecessary repetition of a split.,2024-02-04T07:06:06Z,Merged ciflow/trunk topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/119144, merge, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,"Running inductor codegen from TORCH_LOG=""output_code"" yields a different result"," üêõ Describe the bug I'm investigating an accuracy test failure in https://github.com/pytorch/pytorch/pull/119068. It only happens with compiled autograd + inductor backend, on both cpu and cuda. I wanted to debug via executing the codegen output from TORCH_LOGS=""output_code"" with inputs hardcoded in. I was expecting the same results as the compiled autograd run. I'm using a script with hardcoded inputs and weights which I don't think is using any RNG. repro: https://gist.github.com/xmfan/622c587aeb663aa437a021c626a504be  cpu codegen results differs from compiled_result: https://gist.github.com/xmfan/f66eebca61fea1087ec892910cc251eb  cuda codegen results differs from compiled_result: https://gist.github.com/xmfan/e6cabda8e9c01ac7eb3741a026610b1a   Versions https://github.com/pytorch/pytorch/pull/119068 ",2024-02-03T05:24:30Z,triaged oncall: pt2 module: inductor module: compiled autograd,closed,0,1,https://github.com/pytorch/pytorch/issues/119121,Found the root cause of this. It's due to both accumulate_grad and inductor trying to reuse tensor storage at the same time. Turning off one or the other prevents this. See annotated cpu codegen: 
dspy,[dynamo] Fix incorrect docstring placements in _guards.py.,This makes them unavailable when using help and other tools accessing them.,2024-02-03T01:40:16Z,Merged ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/119114, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", Can you fix the lint issues (mostly spaces in blank lines as I see)?,">  Can you fix the lint issues (mostly spaces in blank lines as I see)? absolutely, sorry about this", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Catch NotImplementedError when proxying torch.* functions that return unproxyable values,  CC(Catch NotImplementedError when proxying torch.* functions that return unproxyable values) ,2024-02-02T22:56:44Z,module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/119089,oops duplicate
gemma,[ROCm] enable hipblaslt for gemm_and_bias by default,,2024-02-02T22:28:00Z,module: rocm triaged open source ciflow/rocm,closed,0,2,https://github.com/pytorch/pytorch/issues/119083,",   If this PR passes CI, we can merge.  Should help you out by making gemm_and_bias enabled by default, right?  I've only been nervous about CI failures, but let's see.","Duplicate of https://github.com/pytorch/pytorch/pull/120480, so closing"
yi,torch.bmm fails when multiplying a batch of sparse matrices by a dense tensor," üêõ Describe the bug When I try to multiply a batch of sparse matrices of shape (B, N, N) by a dense tensor of shape (B, N, 1) using , the following error is thrown:  However, no error is thrown when multiplying the batch of sparse matrices by a dense tensor of shape (B, N, D), with D>1.  Code to reproduce the first case:   Code to reproduce the second case:   Versions PyTorch version: 2.1.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: AlmaLinux 8.8 (Sapphire Caracal) (x86_64) GCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.018) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.28 Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64bit runtime) Python platform: Linux4.18.0477.21.1.el8_8.x86_64x86_64withglibc2.28 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 GPU 3: NVIDIA GeForce RTX 3090 Nvidia driver version: 535.104.05 cuDNN version: Could not collect Versions of relevant libraries: [pip3] numpy==1.26.3 [pip3] torch==2.1.2 [pip3] torch_geometric==2.4.0 [pip3] torchvision==0.16.2 [pip3] triton==2.1.0 [conda] numpy                     1.26.3                   pypi_0    pypi [conda] torch                     2.1.2                    pypi_0    pypi [conda] torchgeometric           2.4.0                    pypi_0    pypi [conda] torchvision               0.16.2                   pypi_0    pypi [conda] triton ",2024-02-02T21:50:49Z,module: sparse triaged,open,0,0,https://github.com/pytorch/pytorch/issues/119076
yi,"fakeifying DTensor fails when DTensor is non-contiguous, non-view","Attempt at minifying from an internal repro:  This failswith:  What's going on? Our fakeifying code has some interesting logic to handle the case when we are fakeifying a tensor that is both noncontiguous, and not a view (it eventually tries to call `.set_()` to smash in the correct storage / strides here: https://github.com/pytorch/pytorch/blob/main/torch/_subclasses/meta_utils.pyL607). This logic was written very carefully though, and blows up spectacularly when called on a tensor subclass. ",2024-02-02T20:59:33Z,triaged oncall: pt2 module: dtensor module: pt2-dispatcher,closed,0,1,https://github.com/pytorch/pytorch/issues/119071,"[doing issue scraping] Repro does not fail, closing issue"
gpt,make nanogpt work with both compiled autograd and _LazyGraphModule,"  CC(make nanogpt work with both compiled autograd and _LazyGraphModule)  and  reported that _LazyGraphModule ( https://github.com/pytorch/pytorch/pull/117911 ) makes nanogpt training fail with compiled autograd. We have a repro:    but it's still mysterious how to trigger the issue with a toy model. The error message for the failure is https://gist.github.com/shunting314/6402a6388b3539956090b6bc098952fb . In compile_fx we will call `detect_fake_mode`. This function will look for an active FakeTensorMode from both TracingContext and example inputs. The error is triggered because we find different FakeTensorMode from these 2 sources. Although I don't know what really causes the discrepancy of FakeTensorMode above, the fix here is to force _LazyGraphModule recompilation if we have compiled autograd enabled. This does not hurt compilation time most of the time because we anyway will call the graph module here in the backward pass when compiled autograd is enabled: https://github.com/pytorch/pytorch/blob/855d5f144efc1db50316b9fcad1e62bf37caed10/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.pyL705  Let me know if we can have a better fix. ",2024-02-02T07:57:34Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/118981,  I just added a simple enough repro. Looks like the key to repro is to do an extra compilation on the function to test., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Inconsistent usage of `attn_mask` and `is_causal` arguments," üìö The doc issue In the docs describing `F.scaled_dot_product_attention` we can read: > is_causal (bool) ‚Äì If true, assumes causal attention masking and errors if both attn_mask and is_causal are set. So if `is_causal=True` then we also need `attn_mask=None` or it will throw exception. Got it. But let's see `nn.MultiheadAttention.forward` method. > is_causal (bool) ‚Äì If specified, applies a causal mask as attention mask. Default: False. Warning: is_causal provides a hint that attn_mask is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility. Hard to understand what does it exactly mean, so lets try it. If we run it with `is_causal=True` and `attn_mask=None` it will throw exception: RuntimeError: Need attn_mask if specifying the is_causal hint. You may use the Transformer module method `generate_square_subsequent_mask` to create this mask. So now is the opposite and `is_causal=True` needs `attn_mask` to be not `None`. So what exactly this two args represent? Since multihead attention and scaled dot product are two concepts so closely related to each other, shouldn't it be more consistent?  Suggest a potential alternative/fix _No response_ ",2024-02-02T05:28:27Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/118972
rag,Support generic PyTensorType checking and support getting generic storage in function get_storage_obj.,"Third party device can do some thing like function py_bind_tensor_types, and to register their tensor type in torch._tensor_class, so maybe we need more generic way to handle tensor PyTensorType. py_bind_tensor_types: https://github.com/pytorch/pytorch/blob/main/torch/csrc/tensor/python_tensor.cppL405 1) By using torch._tensor_class from python side to check object in py_set_default_tensor_type; 2) Support more device types in function get_storage_obj. Fixes ISSUE_NUMBER   ",2024-02-01T08:37:57Z,triaged open source,closed,0,9,https://github.com/pytorch/pytorch/issues/118852,"I think I might in principle support something like this, but you need a more clear description, some examples and some tests.","> I think I might in principle support something like this, but you need a more clear description, some examples and some tests. That's ok. And do you mind to add a python API to register tensor types for custom devices, like this:   cpython: ",I can't really tell what this API is trying to do,"> I can't really tell what this API is trying to do As we know, tensor types like torch.FloatTensor, torch.cuda.FloatTensor are stored in python torch._tensor_classes, and there are initialized in function initialize_python_bindings. In c++ side, there are stored in static std::vector tensor_types. Here are some reasons: also assume custom device name is foo. 1) Custom devices can't register custom device in pytorch, because torch.foo is not initialized when import torch; 2) Custom devices also need to register tensor types to torch._tensor_classes.  and using those tensor types same as GPU, like torch.set_default_tensor_type(torch.cuda.FloatTensor); 3) Now custom devices need to copy all functions in initialize_python_bindings(all functions is in cpp file) to register their tensor types. and modify some codes, like change cuda to foo; So i think maybe we need to add a python api for custom devices to register their tensor types.",These classes are all deprecated. Even if you never create them all functionality in PyTorch will work. I would not bother ,"> deprecated > These classes are all deprecated. Even if you never create them all functionality in PyTorch will work. I would not bother Yep, i saw deprecated message from those function call. But a lot of user training scripts still using those APIs. like: 1) Set default tensor types by using set_default_tensor_type(); 2) Using torch.cuda.FloatTensor(xxx) for get a cuda tensor with float scalar type; 3) Using xxx.type == torch.cuda.FloatTensor to judge tensor type. Our device want using those APIs just like GPU, so user don't need to modify a lot training script when migrating from GPU to our device. And can we add a python api just for custom device, which is like:  ` def _register_custom_device_tensor_types():     r""""""Register tensor types to torch._tensor_classes for the specific :attr:`device_type`     supported by torch.     """"""     torch._C._register_custom_device_tensor_types() ` ` PyObject* THPModule_registerCustomDeviceTensorTypes(PyObject* _unused, PyObject* noargs) {   HANDLE_TH_ERRORS   const auto& declared_types = torch::utils::all_privateUser1_declared_types();   torch::tensors::initialize_python_bindings(declared_types);   Py_RETURN_NONE;   END_HANDLE_TH_ERRORS } `","  Hi, Yang. I have already completed the code push and would like to ask if it could be merged. Due to a large amount of training code accumulated on the user side, and our hope that users can perceive as little as possible the changes to training scripts caused by device migration, our device hopes to align as much as possible with the related functional interfaces supported by GPU training, even if they have been deprecated.",It's not difficult to port code to use the new APIs. You will have to modify them anyway to use your private device. I am not accepting this PR.,"> It's not difficult to port code to use the new APIs. You will have to modify them anyway to use your private device. I am not accepting this PR. Ok, thank you for reply.  I will remove all the APIs, and using monkey patch in our private device. But there still something need to modify, which like add private lazy init in function maybe_initialize_device, and privateUse1 in function backend_to_string. May i still using this pr to modify those?  "
llama,[executorch] Run llama in xplat,"Summary: Error running llama in xplat, where half type isnt part of c10_mobile targets. See:  D53158320 This diff:  Creates a `torch_mobile_all_ops_et` target, which is the same as `torch_mobile_all_ops`, except with a preprocessor flag (C10_MOBILE_HALF) to support Half type  Check C10_MOBILE_HALF in LinearAlgebra.cpp and include it  Use `torch_mobile_all_ops_et` for executorch, instead of `torch_mobile_all_ops`.  Considerations:  Using `torch_mobile_all_ops_et` across executorch means that our runtime binary size for xplat aten increases (see test plan for increase amount, thanks tarun292 for the pointer). This may be okay, as aten mode isn't used in production. Test Plan: Run language llama in xplat:  And in fbcode:  Test executor_runner size increase with:   Differential Revision: D53292674",2024-02-01T01:44:33Z,fb-exported Merged ciflow/trunk release notes: linalg_frontend,closed,0,15,https://github.com/pytorch/pytorch/issues/118831,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674,This pull request was **exported** from Phabricator. Differential Revision: D53292674," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,"22 Dynamo test are failing with ""Failed running call_method expand_as(*(FakeTensor(..., size=(2, 1, 2, 1)), FakeTensor(..., size=(1,))), **{}):"".","22 Dynamo test are failing with ""Failed running call_method expand_as(*(FakeTensor(..., size=(2, 1, 2, 1)), FakeTensor(..., size=(1,))), **{}):"".  Repro `PYTORCH_TEST_WITH_DYNAMO=1 pytest test_torch.py v k test_broadcast_fn_add_cpu` You will need to remove the skip or expectedFailure before running the repro command. This may be just removing a line in dynamo_test_failures.py  Failing tests Here's a comprehensive list of tests that fail (as of this issue) with the above message:  Click me   ",2024-01-31T20:30:31Z,triaged oncall: pt2 module: dynamo dynamo-must-fix,open,0,1,https://github.com/pytorch/pytorch/issues/118794,"Hi , we are scrubbing old issues. I can repro errors. Are you still working on this issue? Feel free to unassign yourself if not."
rag,Support printing storage while FakeTensorMode is enabled,  CC(Support printing storage while FakeTensorMode is enabled) Signedoffby: Edward Z. Yang ,2024-01-31T19:59:17Z,Merged ciflow/trunk release notes: composability release notes: fx topic: bug fixes,closed,0,4,https://github.com/pytorch/pytorch/issues/118780, merge," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,"Getting ""RuntimeError: Trying to backward through the graph a second time"" when calling backward on compiled unbind / split ops"," üêõ Describe the bug Since Pytorch 2.2 (until now  current PT nightly) trying to run backward() on compiled unbind or split ops, a RuntimeError is thrown. As suggested in the error message, calling backward() with retain_graph=True prevents the error from occuring. However previous PT versions (2.1) did not require that to work.  Is this an expected bahaviour or a new bug that a workaround with retain_graph=True hides?  Error logs ‚ûú  ~ python3 unbind_repro.py Traceback (most recent call last):   File ""/home/ubu/unbind_repro.py"", line 14, in      i.backward() retain_graph=True)     ^^^^^^^^^^^^   File ""/home/ubu/.local/lib/python3.11/sitepackages/torch/_tensor.py"", line 524, in backward     torch.autograd.backward(   File ""/home/ubu/.local/lib/python3.11/sitepackages/torch/autograd/__init__.py"", line 267, in backward     _engine_run_backward(   File ""/home/ubu/.local/lib/python3.11/sitepackages/torch/autograd/graph.py"", line 744, in _engine_run_backward     return Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/ubu/.local/lib/python3.11/sitepackages/torch/autograd/function.py"", line 294, in apply     return user_fn(self, *args)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/ubu/.local/lib/python3.11/sitepackages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py"", line 620, in backward     *ctx.saved_tensors,      ^^^^^^^^^^^^^^^^^ RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). ",2024-01-31T10:18:32Z,triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,open,0,4,https://github.com/pytorch/pytorch/issues/118739, (old issue scrubbing) I tried the repro and we can still get an the following error: ,Any update on this issue?,"Oh this is a bit annoying: in this particular case, there is not really a graph for us to compile: the only outputs to the function are views of inputs (outputs `unbind()`). In eager mode, if you `.backward()` through each output of the `unbind()` call individually, the autograd engine will keep the `UnbindBackward` node around  but with compile, we will still generate a `CompiledFunctionBackward` that will get freed on the first `.backward()` call. In theory, we could fix this by detecting when there is nothing to compile, and not generating a `CompiledFunction` at all (and just replaying the views at runtime). This will be a bit of a pain though.  are you running into this on a more realworld example that you can share? (in the example above, you won't really get any speedups anyway by trying to compile a function that does no actual tensor compute and just performs a view op)."," this issue is a finding from an unbind op unit test in our project, so no, I don't have any better example to share."
finetuning,torch.distributed.DistNetworkError: Connection reset by peer," üêõ Describe the bug I'm running following deepspeed command for finetuning in my venv: `deepspeed trainer_sft.py configs llama27bsftRLAIF  wandbentity tammosta  show_dataset_stats deepspeed` However, I'm getting following error:   Versions Pytorch version: 2.2.0+cu121 CUDA version: nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Mon_Apr__3_17:16:06_PDT_2023 Cuda compilation tools, release 12.1, V12.1.105 Build cuda_12.1.r12.1/compiler.32688072_0",2024-01-30T20:04:15Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/118666,issue solved by killing the inactive process,> issue solved by killing the inactive process what kind of inactive process
llama,"UNSTABLE pull / test-models-linux (cmake, llama2, portable, linux.2xlarge)",Keep failing in trunk. It will prevent promoting green commit to ET stable branch. /pytorchdevinfra,2024-01-30T19:41:50Z,module: ci unstable,closed,0,1,https://github.com/pytorch/pytorch/issues/118663,Experiment how it works and assess how it would be helpful for ET trunk job failure mitigation. I think ET should reuse the same mechanism to mark ET jobs unstable and bypass it from running. No action is needed here. Will close this issue.
transformer,TransformerEncoderLayer fast path predicts NaN when provided attention bias," üêõ Describe the bug Similar to this issue) (which is for `nn.MultiheadAttention`) and this commentissuecomment1854786904) within the same thread, `nn.TransfromerEncoderLayer` predicts `NaN` in its fast path when provided with an attention mask.  I have created a child class `MyTransformerEncoderLayer` , that is used to check the attention weights on the slow path (the fast path doesn't go through `_sa_block`).  Here is how to reproduce the problem. Based on another similar issue)) that claims dropout was the cause, dropout has been set to zero here for simplification.   Results:  Only the fast path outputs `y_enc_fast` and `y_my_enc_fast` are all `NaN`. On the slow path, none of the attention weights in `attn_weights_my_enc` are `NaN` given the same set of inputs, which suggests that the problem is likely not due to erroneous inputs creating issues with the attention scores. Note that the same results are obtained even if `src_key_padding_mask` is `None`.  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.9 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.58+x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 535.104.05 cuDNN version: Probably one of the following: /usr/lib/x86_64li",2024-01-30T08:53:34Z,module: nn triaged,open,0,2,https://github.com/pytorch/pytorch/issues/118628,I was able to reproduce this,Same issue; using only 0 and inf in my attn mask seems to make it work stably but I want to use values in between.
llm,fx: add name for call_module,"when creating graph, userdefined name is needed, currently, name can only be created by torch fx when calling call_module, this pr helps fix it,",2024-01-30T06:49:32Z,triaged open source Stale release notes: fx,closed,0,2,https://github.com/pytorch/pytorch/issues/118625,chuang can you please review it.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[no ci][RFC] Added `fuse_comm_groups` staticmethod,"  CC([no ci][RFC] Added `fuse_comm_groups` staticmethod)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([BE] Enabled mypy in `common_fsdp.py`)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Added backward prefetching)  CC([FSDP2] Added `reshard_after_forward`) This PR shows an `FSDP.fuse_comm_groups(*modules)` staticmethod.  In this example, there are now 18 allgathers/reducescatters (root, layer 0, layer 31, and layer `i`, `i+1` pairs) instead of 33 (root and 32 layers), where the pairs have 2x comm. volume.  The FSDP design has the limitation that a communication group must be defined by exactly one `nn.Module`. This means that sibling modules cannot be one group without including their parent. For transformer architectures, this yields the conventional policy where each transformer block is one group. However, fusing consecutive transformer blocks may tradeoff more efficient communication at the cost of higher memory usage. Furthermore, this fusing need not be uniform; e.g. modules can be fused more aggressively in the middle of forward. The current workaround is for users to modify their `nn.Module` definition to include a dummy parent module of the modules to fuse. To support this natively in FSDP, there are at least two approaches: 1. Allow FSDP to take in a `List[nn.Module]` in its constructor 2. Add an explicit API to fuse modules with FSDP already applied We had some discussion already on Approach 1. There some challenges around implementing the module traversal to be able to pass the `List[nn.Module]` to FSDP. For example, any graph traversal (like in aut",2024-01-29T21:54:22Z,Stale release notes: distributed (fsdp2),closed,0,2,https://github.com/pytorch/pytorch/issues/118584,It is hard to maintain this PR on top of the stack if we are not sure to land it. I am going to delete it from my local branch and preserve this PR as an artifact for future reference.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Angelayi/hf pin update,test,2024-01-29T16:56:03Z,ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/118537
rag,[WIP] Extend unit test coverage for backend onnxrt,,2024-01-29T16:50:08Z,module: onnx open source Stale onnx-triaged release notes: onnx topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/118536,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Forbid follow_imports = skip from mypy.ini,  CC(Add some type annotations to torch._inductor.codegen.wrapper)  CC(Upgrade mypy version to 1.8.0)  CC(Forbid follow_imports = skip from mypy.ini)  CC(Use strict to toggle strict options in MYPYSTRICT)  CC(Use dmypy instead of mypy in lintrunner)  CC(Remove follow_imports = skip from sympy)  CC([mypy] Remove colorama ignore_missing_imports)  CC(Enable local_partial_types)  CC(Unify MYPYINDUCTOR and MYPY)  CC(Upgrade mypy python_version to 3.11)  CC(Replace follow_imports = silent with normal) Signedoffby: Edward Z. Yang ,2024-01-28T03:33:40Z,Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/118480
rag,Modify StorageImplCreateHelper,I want to use tensor.untyped_storage()[a:b] for ``PrivateUse1`` backend but fail. The code will go into ``THPStorage_get``: https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL525L540 Here ``torch`` will create a new ``c10::StorageImpl`` but not consider about ``PrivateUse1`` backend.,2024-01-27T09:17:52Z,triaged open source Merged ciflow/trunk release notes: quantization topic: not user facing,closed,0,18,https://github.com/pytorch/pytorch/issues/118459,"Is it possible to write a test that exercises the fixed behavior? NGL, the code change here looks plausible but I am not entirely sure what it is doing.","> Is it possible to write a test that exercises the fixed behavior? >  > NGL, the code change here looks plausible but I am not entirely sure what it is doing. I think there is no need to add additional test, because we found this problem when testing test_torch.py. https://github.com/pytorch/pytorch/blob/da0635d17c8fc777010fc3a2c5efedfade499432/test/test_torch.pyL192L194 This code will check tensor.untyped_storage()[a:b]. For the second problem, both ``THPStorage_get`` and ``THPStorage_pynew`` need to make a new ``StorageImpl``, but the former doesn't consider ``PrivateUse1`` backend while the other one does.  Now ``make_storage_impl`` used in ``THPStorage_pynew`` will check if ``PrivateUse1`` backend has set ``StorageImplCreateHelper`` but it doesn't support ``StorageImpl`` constructed with ``at::DataPtr``. https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL315 https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL350L352 Therefore, we change ``make_storage_impl`` to be compatible with two apis.  ","Well, it would still be profitable to replicate the test_torch.py test, but using PrivateUse1, no?","> Well, it would still be profitable to replicate the test_torch.py test, but using PrivateUse1, no? Okey, I updated one relative test with reference to https://github.com/pytorch/pytorch/pull/100237. ","With reference to https://github.com/pytorch/pytorch/pull/100237 and  CC(Can torch use make_storage_impl to replace c10::make_intrusive<c10::StorageImpl>), I submitted a new version. Previous ``make_storage_impl`` is mainly moved from ``THPStorage_pynew``. For generality, I put ``allocator_opt`` back to ``THPStorage_pynew``, and only save code of creating ``c10::StorageImpl``. In addition, I move ``make_storage_impl`` to ``StorageImpl.h`` so it is together with ``PrivateUse1`` registration and can be used in other APIs . I think this method is more reasonable, compatible with previous functions and new ``PrivateUse1`` requirements. What do you think  ?"," , I have fixed some CI problems, can you please run again?"," , could u please take a look at this pr? Thanks!"," , what do u think of the new implemention?"," , could u please take a look at this pr?","The committers listed above are authorized under a signed CLA.:white_check_mark: login: CLiqing / name: ChenLiqing  (e815a208b0ce1568076b973189e6eba1dd12a53c, 1a6dcf7f6f6d89421dad06dda8785de487469bee, 0604e944522164e477c0cf0ef4e9c4d3debc49d1, d4d921d02fa9082d8932eb26550349b658d19fd3, e74a855c54c6eed8ae3867e104f9f2e1db561f44)","I have tranfered data_ptr==nullptr condition to the constructor.  , is there any other suggestions?"," approved the original PR that added the storage creation hook, so I am deferring this review to him."," , could u please take some time and have a look at this pr? Thanks!"," , would u plz take a look at new changes?", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `main_t` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout main_t && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Add another make_storage_impl for PrivateUse1 backend," üöÄ The feature, motivation and pitch I want to use tensor.untyped_storage()[a:b] for ``PrivateUse1`` backend but fail. The code will go into ``THPStorage_get``: https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL525L540 Here ``torch`` will create a new ``c10::StorageImpl`` but not consider about ``PrivateUse1`` backend like``make_storage_impl``. https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL315 However, existed ``make_storage_impl``'s parameters don't include ``data_ptr``. https://github.com/pytorch/pytorch/blob/bb6eba189f92790f278f1ee2840f3786da02acaa/torch/csrc/Storage.cppL293L299 So can I add a overloaded ``make_storage_impl`` with input ``data_ptr`` and use it to replace ``c10::make_intrusive`` in ``THPStorage_get``? A possible solve method: https://github.com/pytorch/pytorch/pull/118459  Alternatives _No response_  Additional context _No response_",2024-01-27T08:01:49Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/118458
rag,"[Nested Tensor] Support ragged_idx != 1 on aten::is_same_size, aten::_to_copy","  CC([Nested Tensor] Support ragged_idx != 1 on aten::is_same_size, aten::_to_copy) is_same_size is needed internally; `_to_copy` should be easy because it doesn't support new layouts.",2024-01-27T00:15:45Z,Merged ciflow/trunk topic: improvements release notes: nested tensor,closed,0,4,https://github.com/pytorch/pytorch/issues/118442, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Unify MYPYINDUCTOR and MYPY,"  CC(Remove follow_imports = skip from sympy)  CC([mypy] Remove colorama ignore_missing_imports)  CC(Enable local_partial_types)  CC(Unify MYPYINDUCTOR and MYPY)  CC(Upgrade mypy python_version to 3.11)  CC(Replace follow_imports = silent with normal) The original motivation for MYPYINDUCTOR was a faster type checking configuration that only checked a subset of files. With the removal of `follow_imports = ignore`, we are now able to use dmypy to do fast incremental typechecking, eliminating the need for this. Perhaps erroneously, when I tee'ed up this PR I elected to delete the `follow_imports = skip` designations in the mypyinductor.ini. This lead to a number of extra type error suppressions that I manually edited. You will need to review. Signedoffby: Edward Z. Yang  ",2024-01-26T22:46:24Z,Merged ciflow/trunk release notes: onnx topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/118432, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Fix RuntimeError: NYI: Named tensors are not supported with the tracer,"This PR relands CC( RuntimeError: NYI: Named tensors are not supported with the tracer) that was closed as stale due to CLA issues and also because the CI check has marked the PR as not mergeable. Repro 1:  Error:  Repro2:  Error 2:   CC(RuntimeError: NYI: Named tensors are not supported with the tracer) RuntimeError: NYI: Named tensors are not supported with the tracer CC(jit tracer doesn't work with unflatten layer) jit tracer doesn't work with unflatten layer CC(when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.) when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.     This bug was closed but exists. Multiple comments on it still showing error. This is addressed Likely fixes the following issues (but untested) CC(Named tensor in tracer) Named tensor in tracer CC(allow single nontuple sequence to trigger advanced indexing) [Bug] torch.onnx.errors.UnsupportedOperatorError when convert mask2former to onnx Fix zero dimensioned tensors when used with jit.trace They are currently assigned an empty set for names {} this is not the same as ""no name"" so jit.trace bails with   ""NYI: Named tensors are not supported with the tracer"" This happens when I am trying to save a nontrivial model as onnx but the simplest repro I have seen is 48054 above which has been added as test/jit/test_zero_dim_tensor_trace.py Test plan",2024-01-26T15:41:03Z,oncall: jit open source Merged onnx-needs-info ciflow/trunk release notes: jit topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/118393, merge,  do you know why ghstackmergeabilitycheck is failing?, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Check mergeability and dependencies for ghstack prs / ghstackmergeabilitycheck Details for Dev Infra team Raised by workflow job ","The check is broken right now, let's merge f when the rest of the tests pass."," merge f ""CI is all green, except flaky 'Check mergeability and dependencies for ghstack prs / ghstackmergeabilitycheck'"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout," üêõ Describe the bug I got the following error when running supervised fine tuning of LLAMA27b model on P4 instance. Command that I ran: `deepspeed trainer_sft.py configs llama27bsftRLAIF  wandbentity tammosta  show_dataset_stats deepspeed` This is the error I got:  I tried creating a new venv and running the command there, but got the same error. I also tried the methods suggested https://github.com/ultralytics/ultralytics/issues/1439with no luck. Any idea what's causing this?  Versions transformers version: 4.35.2 Platform: Linux5.15.01050awsx86_64withglibc2.31 Python version: 3.10.12 Huggingface_hub version: 0.20.2 Safetensors version: 0.4.1 Accelerate version: 0.26.1 Accelerate config: not found PyTorch version (GPU?): 2.1.2+cu121 (True) Tensorflow version (GPU?): not installed (NA) Flax version (CPU?/GPU?/TPU?): not installed (NA) Jax version: not installed JaxLib version: not installed ",2024-01-26T06:45:58Z,oncall: distributed,open,0,7,https://github.com/pytorch/pytorch/issues/118369,"at face value this line  `[E ProcessGroupNCCL.cpp:475] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=880, OpType=BROADCAST, NumelIn=131137536, NumelOut=131137536, Timeout(ms)=1800000) ran for 1800648 milliseconds before timing out. ` means that a particular broadcast operation took longer than 30 minutes (1800sec) and timed out.  Broadcast is a collective, meaning each rank needs to participate.  If one rank forgets to join, other ranks wait for it (and will time out). notably, Rank 0 never prints any timeout error.  This may implicate rank 0 as the one that did not join the broadcast, so you could try to approach from that angle and investigate what rank 0 is doing instead.","  When I check GPU utilization while a finetuning is running, I see this:  It looks rank 0 is not being utilized. I'm not sure why this is happening. ",you could try using pyspy to attach to rank 0's PID and see what it's up to.  you may find that it's executing a different part of the program than you expected., Could you please suggest how to debug this in details? I don't know how to do what you're suggesting.,Running into the same problem. Any idea of how to debug? I have no clue based on the error log. ,"Honestly, it's going to be hard to debug this without some familiarity with the parallelism stack inside deepspeed. At a high level a collective timeout can be caused by 2 things 1) all the ranks called the right API but somehow the network or NCCL layer broke down and didn't complete it 2) *probably more likely* for some reason, one or more ranks did not call the right API at the right time We're assuming you're facing case (2) for now until proven otherwise.  Now the issue is you have to figure out a way to instrument deepspeed to see if/why/when it maybe didn't call a collective in the right order. I will also say that we are building a new Flight Recorder feature for ProcessGroupNCCL that will make this kind of debugging a lot easier.  Its just not quite ready for general use yet.  You could actually try using it, but you'd have to update to latest nightly pytorch and plan to do some extra work writing analysis scripts for the raw dump files it produces.",Thanks!! the most annoying thing is to figure out where/when/why the timeout happens. I have no ideas how to proceed. Anyway thanks for clarification...
yi,Further simplifying guards," üêõ Describe the bug I have this function:  If I already know that x.shape[0] > 2, which in this case is from the first assertion, but in the actual case it‚Äôs a constraint, I believe that the following assertions should all be True through... math? And so I feel that the guards for the assertions afterwards should not be generated. However I still see the following guards, possibly because the symbolic reasoning is not strong enough to simplify them:    Versions main ",2024-01-25T23:41:17Z,triaged oncall: pt2 module: dynamic shapes,open,0,13,https://github.com/pytorch/pytorch/issues/118332,What is the precise x.size(0) here? I tried a few sizes and neither of them produced guards:  ,My test case was like:  I'm not sure if it's the proper way to repro what I'm running into internally: https://www.internalfb.com/phabricator/paste/view/P1077550517?lines=596,Is due to  CC(Dynamo is silently suppressing shape guards from asserts) I see the guards now.,"The modulus one is easy, we just need to have a rule that `1 % a` is guaranteed to be 1 if a > 1. The floordiv rule should already be enough juice for us to prove this. The other one is equivalent to `32 * X * X != 16 * X`. , this particular case is relatively easy to solve but in general it's not so easy to solve these equations, I suspect.  do you have any improved characterization of what kinds of equations of this form we have to solve, generally?","> do you have any improved characterization of what kinds of equations of this form we have to solve, generally? I'm not sure what I should be looking for, but I do see some guards like `2 * X * X != X` being generated, which is a simplified version of the one above.",(also ),"Hey, `a > 2` doesn't imply that `1 % (a // 2) != 0`. If `a == 3` the second formula is false (since `3 // 2 == 1` and thus `1 % 1 == 0`). In fact, `1 % (a // 2) != 0` is equivalent to `a > 3`.","For `assert 32 * (x.shape[0] // 2) ** 2  16 * (x.shape[0] // 2) != 0`, the generic way to simplify this is to: 1. CSE the expression as to find that it's of the form `y = x.shape[0] // 2 and p(y) = 0` where `p` is a polynomial of degree 2. 2. Find the roots `a1, a2` of the polynomial 3. Substitute by `y = x.shape[0] // 2 and (y == a1 or y == a2)", ," IIRC in the original internal code we were looking at that motivated this request (and as your paste confirms) you also have `x.shape[0] % 2 == 0`, am I right? In particular this would allow us to simplify reasoning about `//`. Also, since the original issue was a forced specialization by the export constraint solver I think you might be unblocked by https://github.com/pytorch/pytorch/pull/118729. ""Too complex"" constraints that force specialization are not `Ne(...)` constraints, but rather `Eq(...)` constraints. It's possible that once you can specify some of these relationships, the `Ne(...)` constraints might become simpler to check, too.", this issue was quoted as still a blocker for Seamless in the monthly execution review. This issue doesn't give me enough context on if we must fix this the complicated way or if there is something else we can do to unblock. Please advise.,"I think we just need to add a rule for ""`1 % a` is guaranteed to be 1 if `a > 1`"" which I was going to take a look at but haven't had the time yet.. and then https://github.com/pytorch/pytorch/pull/118729 should fix the 2nd complicated issue.","(scrubbing old issues). Hi , are you still working on this issue? Also ."
rag,[Nested Tensor] view: basic support for ragged_idx != 1 and _unsafe_view,"  CC([Nested Tensor] view: basic support for ragged_idx != 1 and _unsafe_view) Uses case: `_unsafe_view` is used in aot_autograd to create a view that doesn't register as a view: https://github.com/pytorch/pytorch/blob/eebe7e1d37f1baa995c694d540cc2fc98884fa18/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.pyL470L476 If a transposed nested tensor (i.e. NT with ragged_idx != 1) encounters this code path, it previously would fail for two reasons: 1) because `_unsafe_view` isn't registered, and 2) because ragged_idx != 1 is not supported. This PR adds support for `_unsafe_view` (completely reusing the implementation of `view`; this just registers `_unsafe_view` as another op using the same implementation). It also adds support for ragged_idx != 1, but only for trivial cases where inp._size == size (the use case used by aot_autograd). Tests: verify that the result of `_unsafe_view` doesn't have a `_base`, and that simple views on transposed NTs work. Differential Revision: D53096814",2024-01-25T19:29:50Z,Merged ciflow/trunk release notes: jit topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/118317," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/davidberard98/254/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/118317`)", Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,Getting error while loading LLAMA_2_7B,"NVML_SUCCESS == DriverAPI::get()>nvmlInit_v2_() INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":1123, please report a bug to PyTorch.",2024-01-25T05:14:54Z,needs reproduction triaged,open,0,1,https://github.com/pytorch/pytorch/issues/118263,"This sounds a lot like  CC(RuntimeError: NVML_SUCCESS == DriverAPI::get()->nvmlInit_v2_() INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":1123, please report a bug to PyTorch.) i.e. something is wrong with your CUDA setup"
rag,Augment create_symbol with user/infra backtrace fragment,"  CC(Report the type of a tensor in wrap_to_fake)  CC(Add stack trace to ""start tracing"" log)  CC(Augment create_symbol with user/infra backtrace fragment) Looks like this:  Signedoffby: Edward Z. Yang ",2024-01-24T20:00:15Z,Merged ciflow/trunk release notes: fx topic: not user facing ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/118215,lint needs fixing of course
transformer,[FSDP2] Added activation checkpointing tests,"  CC([no ci][RFC] Added `fuse_comm_groups` staticmethod)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added activation checkpointing tests)  CC([FSDP2] Added initial 2D and state dict tests)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Added backward prefetching)  CC([FSDP2] Added `reshard_after_forward`)  CC([FSDP2] Added pre/postbackward)  CC([FSDP2] Added reducescatter)  CC([FSDP2] Added forward unshard/wait for unshard/reshard)  CC([FSDP2] Added `_to_kwargs` root forward input cast)  CC([FSDP2] Added allgather and unsharded parameter)  CC([FSDP2] Added initial `_lazy_init` and FQNs for debugging)  CC([FSDP2] Sharded parameter in `FSDPParam`)  CC([FSDP2] Added initial `FSDPParamGroup`, `FSDPParam`, `ParamModuleInfo`)  CC([FSDP2] Added `mesh` arg, `FSDPState`, move to device)  CC([FSDP2][Reland] Introduced initial `fully_shard` frontend) This PR adds tests that `fully_shard` can use `torch.utils.checkpoint`, `_composable.checkpoint`, and `CheckpointWrapper` on a transformer. ",2024-01-24T17:17:01Z,oncall: distributed test-config/distributed release notes: distributed (fsdp2),closed,0,1,https://github.com/pytorch/pytorch/issues/118198,Squashed to save CI cost
rag,[NestedTensor] Support ragged_idx != 1 in pointwise ops,"  CC([Nested Tensor] view: basic support for ragged_idx != 1 and _unsafe_view)  CC([NestedTensor] Support ragged_idx != 1 in pointwise ops) This PR allows pointwise ops to operate on tensors with ragged_idx != 1. It does this by passing the ragged_idx metadata into the construction of the returned NestedTensor when computing pointwise ops. The assumption is that: pointwise ops can operate directly on the values tensors, and the resulting tensor should have all the same metadata properties as the input tensors. For binary ops, a test is added to verify that adding two tensors with different ragged_idx cannot be added. Previously: * unary pointwise ops would error out when performed on nested tensors with ragged_idx != 1 * binary pointwise ops would produce tensors with nonsense shapes Differential Revision: D53032641",2024-01-24T01:21:46Z,Merged ciflow/trunk topic: not user facing release notes: nested tensor,closed,0,11,https://github.com/pytorch/pytorch/issues/118157," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","> Thanks for the fix! >  > One thing I'm thinking through is ensuring the semantics are consistent with dense tensors in eager. In general, we have: >  >  >  > We should maintain this property for NT as well. >  > Previously, the only way to get `_ragged_idx != 1` was via `transpose()`, so such an NT would be necessarily noncontiguous. After this PR, we can get a NT with `_ragged_idx != 1` as the output of a unary op on such a noncontiguous NT input, and, by the property above, we expect it to be contiguous. >  > So we have to change the definition of `is_contiguous()` for jagged NT at least. >  > Another note: perfwise, SDPA assumes that the inputs have been ""pretransposed"" and that it can untranspose them very cheaply. If we're sending the output of a unary op with `_ragged_idx != 1` to SDPA, this is no longer the case. No action needed for this PR per se; just pointing this out as something to consider. Do you mean that the untranspose in SDPA won't be cheap anymore after this PR?","> Do you mean that the untranspose in SDPA won't be cheap anymore after this PR? If there is a unary op invocation in between the transpose and the SDPA call, that's right."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests,"  CC([no ci][FSD2] Added annotations for compute compile)  CC([FSDP2] Added pre/postallgather extensions)  CC([FSDP2] Generalized allgather outputs to >1 per parameter)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([BE] Enabled mypy in `common_fsdp.py`)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Replaced versionctx with `no_grad`; removed `no_grad`) This PR adds tests for autograd (mainly backward hooks), memory, overlap, and frozen parameters.  Autograd: unused forward output, unused forward module, nontensor activations (common in internal models)  Memory: expected GPU memory usage after init, forward, backward, and optimizer step  Overlap: communication/computation overlap in forward and backward  Frozen: expected reducescatter size, training parity This PR adds some initial 2D (FSDP + TP) training and model state dict tests. The only change required for model sharded state dict is to make sure parameters are sharded before save and load. This PR adds tests that `fully_shard` can use `torch.utils.checkpoint`, `_composable.checkpoint`, and `CheckpointWrapper` on a transformer. (I squashed all of these into one PR now to save CI cost.) ",2024-01-23T22:40:53Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor release notes: distributed (fsdp2),closed,0,1,https://github.com/pytorch/pytorch/issues/118136,Todo: I think we can beef up unit tests more:  Add unit test to check unshard/reshard/hook events when composing with AC  Add unit test to check FSDPmanaged memory when composing with AC  Add unit test to check FSDPmanaged memory when using mixed precision  (Lower priority) add unit test to check FSDP composes with various module backward hooks
dspy,multiple vulnerabilities in cached wheels on downloads.pytorch.org," üêõ Describe the bug When I install pytorch, then the `safety` SAC tool reports vulnerable packages on my system. https://pypi.org/project/safety/ ```console pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu $ safety check +==================================================================+                                /$$$$$$            /$$                               /$$__  $$            $$$$$$/   by safetycli.com                                        \______/ +==================================================================+  REPORT   Safety is using PyUp's free opensource vulnerability database. This data is 30 days old and limited.   For realtime enhanced vulnerability data, fix recommendations, severity reporting, cybersecurity support, team and project policy management and more sign up at https://pyup.io or email sales.io   Safety v3.0.1 is scanning for   Vulnerabilities...   Scanning dependencies in your environment:   > /home/andrew/.asdf/installs/python/3.12.1/lib/python3.12   > /home/andrew/.asdf/installs/python/3.12.1/lib/python312.zip   > /home/andrew/.asdf/installs/python/3.12.1/lib/python3.12/lib   dynload   > /home/andrew/.asdf/installs/python/3.12.1/bin   > /home/andrew/.asdf/installs/python/3.12.1/lib/python3.12/site   packages   Using opensource vulnerability database   Found and scanned 38 packages   Timestamp 20240123 15:20:28   2 vulnerabilities reported   0 vulnerabilities ignored +==================================================================+  VULNERABILITIES REPORTED +==================================================================+ > Vulnerability f",2024-01-23T21:23:56Z,module: binaries triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/118128,"We should definitely update cached version to safe ones, though I wonder why this security tool requires a registration, which feels a bit unpythonic..", do you mind updating your environment and validating that everything is green now?,"Dependencies has been update, so closing this one, but do not hesitate to reopen if you do not think it is the case."
rag,Realize inputs to DynamicScalar before unwrapping storage,"  CC(Fix constant folding bug with sym size tensor)  CC(Realize inputs to DynamicScalar before unwrapping storage)  CC(Fix several bugs related to unbacked SymInt codegen in inductor) Fixes  CC(Inductor error: assert isinstance(x, (Buffer, ReinterpretView)), x) Unfortunately, the test still fails due to an unrelated problem  CC(Sometimes SymInt shows up in Inductor sympy expressions) Signedoffby: Edward Z. Yang  ",2024-01-23T21:04:55Z,Merged ciflow/trunk topic: bug fixes module: inductor ciflow/inductor release notes: inductor,closed,0,12,https://github.com/pytorch/pytorch/issues/118125, just a dumb typo for the expectedFailure lol, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 1, 3, macosm112) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcpupy3.10gcc9bazeltest / buildandtest (default, 1, 1, linux.4xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge i," Merge started Your change will be merged while ignoring the following 2 checks: Check mergeability of ghstack PR / ghstackmergeabilitycheck, pull / linuxfocalcpupy3.10gcc9bazeltest / buildandtest (default, 1, 1, linux.4xlarge) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: 1 jobs have failed, first few of them are: Check mergeability and dependencies for ghstack prs / ghstackmergeabilitycheck Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 2 checks: Check mergeability and dependencies for ghstack prs / ghstackmergeabilitycheck, pull / linuxfocalcpupy3.10gcc9bazeltest / buildandtest (default, 1, 1, linux.4xlarge) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,`torch.cuda.is_bf16_compatible()` output inconsistent with with TorchInductor support," üêõ Describe the bug Recent change for `torch.cuda.is_bf16_compatible()` is now labeling Turing (sm_75) and Volta (sm_70) cards as compatible with `torch.bfloat16` Tensors. Meanwhile, TorchInductor supports `torch.bfloat16` only for sm_80 or higher.  This has caused Transformer Engine CI tests that would normally skip on Turing and Volta nodes to instead crash with a `BackendComputerFailed` error from TorchDynamo. We are replacing the `torch.cuda.is_bf16_compatible()` condition with explicit checks on `torch.cuda.get_device_capability()` as a workaround. However, I wanted to file this issue here regardless, just in case it was an unintentional consequence and may be considered a bug.  Versions  ",2024-01-23T20:48:10Z,high priority triaged oncall: pt2 module: inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/118122, ,"If eager works with bfloat16 data types on CUDA, I would say that `torch.cuda.is_bf16_available() == True` makes sense. If I understand this correctly, the error only occurs with inductor. Maybe we should have something similar within inductor: `torch._inductor.is_bf16_available()`.  any thoughts?","Well, imo we should fix either Inductor or triton to produce working binaries rather than crash with unsupported instructions As I have a local setup, can try working on a fix",Hi from Colab! This is causing problems for users on Colab (example). The gist is that torch.compile w/bf16 on T4 causes some semiopaque errors. e.g.  Seems like poor UX to me. Any thoughts on the appropriate fix here? Should the fix be in triton or torch? My understanding is that XLA handles this by casting to f32 (e.g. https://github.com/openxla/xla/issues/12429),Considering this affects google colab marking as high pri until we find a way to unblock,"Ok, I think we need to separate whether something is ""supported""(read emulated) for eager vs compile. Though it does not look that those two changes are related, i.e. even if `is_bf16_supported` returns `False` compile still fails to produce the correct code. Also, this is not a regression, i.e. 2.2 had the same behavior, see https://colab.research.google.com/drive/1rIy_MJSUcdV8nQu_uuFZsniLR6uhS1BR?usp=sharing Where  Results in  "
rag,[Tracker] Nested tensor op coverage requests,"This issue tracks op coverage requests for nested tensors. If you'd like a specific op to be implemented for nested tensors, please add a comment here so we can prioritize effectively. Prior requests: * Conv2d:  CC(Nested tensors fail on Conv2D) * KVcache related ops:  CC(Implement KV cached attention with NestedTensor)     * Includes ops that mutate inplace * Slicing of NTs across constant dimension:  CC(Allow slicing of Nested Tensors along constant dimensions) * EmbeddingBag:  CC(EmbeddingBag to support mini-batches with offsets) * Loss functions     * Crossentropy, log_softmax, NLL loss):  CC(More Nested Tensor Functionality (layer_norm, cross_entropy / log_softmax&nll_loss))     * Binary crossentropy:  CC(binary_cross_entropy/bce_with_logits (+ other loss functions) for nested_tensor) * `eq`, `to`, `masked_select`, `index_select`, `narrow`:  CC(Add eq, to, masked_select, index_select, narrow to nested tensors)  * LayerNorm on noncontiguous (transposed) NJT:  CC(nested_tensor LayerNorm fails with shape (*,*,j1,*) (but works with (*,j1,*,*))) ",2024-01-23T18:34:20Z,triaged enhancement module: nestedtensor,open,0,19,https://github.com/pytorch/pytorch/issues/118107,maybe a duplicate of :  CC(General NestedTensor op coverage tracking issue),"Thanks all for such a great effort here. I do a lot of work on point cloud analysis (which often involves ragged data), and it would be great to have upstream support for this sort of thing instead of having to rely on heavyweight specialized libraries (i.e. Pytorch Geometric) which come with a lot of baggage/learning curve when creating simple pytorch networks. Lots of the common point cloud analysis architectures (PointNet++, PointNeXT, PointTransformer, etc.) involve a small handful of operations which are not yet implemented in the torch.nested module. Off the top of my head: * nn.Conv{1D,2D}: Example: https://github.com/yanx27/Pointnet_Pointnet2_pytorch/blob/master/models/pointnet2_utils.pyL171 * BatchNorm{1D,2D} * MaxPool{1D,2D}: Example: https://github.com/POSTECHCVLab/pointtransformer/blob/master/model/pointtransformer/pointtransformer_seg.pyL45 I've been able to hack the conv1d/2d operations reshaping / using Linear layers, since they're usually trying to do pointwise operations. But this mechanism is not efficient. There are also various sampling/grouping operations (knearestneighbors, ""ball query"", etc), which may or may not be efficient to implement using torch primitives (I know some of the other libraries use their own cuda implementations for certain ops, like torch_cluster. If you think there might be interest/bandwidth to implement all or even a few of these operations, I'd be happy to: 1. morethoroughly list the operations which would give reasonable coverage of point clouds 2. guinneapig new operations by implementing architectures/training procedures from some of the widelyused methods, to see that it matches expected performance.","Hello   Thank you for your interest! Definitely happy to help, but can't guarantee timelines. I think we should have coverage for BatchNorm1D/2D already. For the other ops, the devil will be in the detail on how easy it is to add coverage (e.g. using reshape etc. for conv1d/conv2d via linears). How many ragged dimensions do you need? Also, do you mind opening a separate issue for this one? Then we can add all the details to that one while keeping this one tidy and focused only on tracking / lists of operators.","Great, thanks for the speedy reply :) just opened a separate issue here:  CC([NestedTensor] Support for operations found commonly in point cloud analysis architectures)",It would be awesome to support RMSNorm and similar layer normalization operators commonly found in transformerbased models. Minimal working example based off https://github.com/pytorchlabs/gptfast/blob/main/model.py  This currently returns:  ,Ditto for coverage of `repeat_interleave`:   (Also from https://github.com/pytorchlabs/gptfast/blob/main/model.py),"Hey , thanks for the requests! We'll certainly prioritize adding coverage to support this.","Thanks for this! I would like mean along the ragged dimension. Use case is a large batch with smaller batches inside of varying length. I can split the batches into a nested tensor then transpose so the variable batch dimension becomes the ragged dimension. Then I want to reduce along the ragged dimension and end up with a flat tensor again. Mean will get me off the ground, but ultimately I'm trying to do a weighted average, so multiplying by the weights then div by the sum of the weights should do the trick. I tried this and hit a few issues: 1. Mean gives:         `NotImplementedError: Could not run 'aten::mean.dim' with arguments from the 'NestedTensorCPU' backend.` 2. Sum works, then div (requires the divisor to be nested) gives:       `RuntimeError: div does not support broadcasting when given a NestedTensor` 3. Actually point 2 varies depending on how I transpose the nested tensor Ideally I could transpose the nested tensor directly, however I found this issue:  * Transpose elements first: sum works as expected  * Transpose the nested tensor: sum fails with:         `RuntimeError: NestedTensor does not support reductions when the input is noncontiguous for now.` Of course, if there is a better way to approach this, I'd love to hear suggestions :)  ",m can you use a nn.EmbeddingBag in this case? ,"Thanks so much, . I got weighted average working with embedding bag :)",Could we get some sort of checking if jagged dimensions are equal?  This binary multiplication fails on pytorch 2.2.2:  with error  even though clearly `j0 == j1`.,"Thanks for the request !  is working on improving this situation. The initial implementation requires the exact same `offsets` object for two NJTs to compare equal, but this is clearly too restrictive, as you've pointed out. In the meantime, one workaround would be: ","Awesome, thanks for the workaround. One other op I've been working around is the smooth l1 loss: 'torch.nn.functional.smooth_l1_loss'",Tracking a few more op requests here (sourceissuecomment1930959476)): * BatchNorm1d * Conv1d (but only with a kernel size of 1) * torch.max,"From offline discussion, there is a need to operate on different parts of the sequence separately. This can be broken down into: * split the NJT into two parts on the ragged dim * operate over the parts separately (do a linear projection in practice) * recombine the parts back For use cases like this, the following are useful: * `slice()` over the jagged dim * `cat()` over the jagged dim * `stack()`ing two NJTs along a new axis * `linear()` on a noncontiguous NJT with holes","When I concat two nested tensor with same shape(batch_size ,  j1  ,  dim ),  import torch  it works, but when it comes to backward , it will report CatBackward0 error say excepted exceped (batch ,j2 dim). It seems that even actually  shape is same ,but in pytorch it look them as different shape.","Hey , we are currently working on a fix to handle two nested tensors with different jvalues (e.g. `j1` / `j2`) but same conceptual ragged structure. I'll keep you updated here on the progress of that. In the meantime, one possible workaround uses `view_as()` to force one NJT to have the same jvalue as another NJT (in progress in CC(Support view_as() on NJT; allow nested int swapping)).",There's a notebook floating around showcasing nested tensors for images of varying dimensions. It uses the conv2d operator. However when I try to run the exact code with a recent pytorch (2.4.1) it errs and says it's not supported. Was nested tensor support for conv2d dropped somehow? Here's a link to the notebook: https://colab.research.google.com/github/pytorch/nestedtensor/blob/master/tutorials/notebooks/basic.ipynb,"Hi   that notebook is based on an old, out of tree version of NestedTensor. We gradually migrated that code as needed into core. The support for conv2d was relatively patchy, as in only for certain kernels was it using a specialized kernel. I think these days I'd write a triton kernel to implement this. This notebook also showcases features that didn't make it into core such as `nested_size`. We now have the `torch.jagged` layout and support for calling `.size()` on a NestedTensor with the jagged layout instead, which I think is a more principled solution."
llama,NAN value for truthfulqa_mc2 on full finetuned model TinyLlama ," üêõ Describe the bug I checked this issue has similar problem I have, however using the latest main branch doesn't solve the problem!  using lmeval to evaluate the model   Model:  Full finetune `TinyLlama/TinyLlama1.1Bstep50K105b `model using **axoltol** with **FSDP** on a completion dataset. On a single machine with two GPUs with these settings: `gradient_accumulation_steps:12,¬†microbatch:1`   Evaluation: `accelerate launch m lm_eval model hf model_args pretrained=fsdpmodel/ task truthfulqa_mc2 verbosity DEBUG`   `cat output/pretrained____fsdpmodel___truthfulqa_mc2.jsonl `   Versions torch>=1.8  ",2024-01-23T14:53:54Z,triaged module: fsdp,open,0,0,https://github.com/pytorch/pytorch/issues/118095
llava,Quantising a multimodal large language PyTorch model to run on Android," üöÄ The feature, motivation and pitch I'm working on a PoC that tries to extract as much information from an image as possible. Currently, this capability is only supported on servers/computers with powerful graphics cards, but this is not scalable due to costs. I tried to run MLLMs with a CLiP backbone through a Pytorch script to quantise them, but it looks like this is not yet supported for models with multiple modalities.  In this regard, allowing support of multimodal large language models to be run on a phone has numerous benefits. Phones like Pixel 8 Pro or IPhone 14+ have powerful chips and a large amount of memory that can be used to run basic models like https://huggingface.co/microsoft/gitbasecoco or https://github.com/rmokady/CLIP_prefix_caption?tab=readmeovfile that still provide good results but are not huge like InstructBLIP or LLaVa.   Alternatives Currently, PyTorch Android supports Resnet or YOLO architectures, but these allow for only image classification and that too on a very restricted set of items. CLiP based models are performing way better in larger computers, so it makes sense to add support for them on mobiles.   Additional context _No response_",2024-01-23T05:03:18Z,oncall: mobile,open,0,1,https://github.com/pytorch/pytorch/issues/118069,Perhaps look into `PyTorch/ExecuTorch` repo? There is an Android Demo app for Llama.
rag,DISABLED test_returned (__main__.TestAverage),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 12 workflow(s) with 12 failures and 12 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_returned` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `torch_np/numpy_tests/lib/test_function_base.py` ",2024-01-23T00:59:16Z,module: flaky-tests skipped oncall: pt2 module: dynamo,closed,0,0,https://github.com/pytorch/pytorch/issues/118050
llama,"[FSDP] Sharding params within a node, and gradient/optimizer globally"," üöÄ The feature, motivation and pitch Parameter allgathering is a very frequently called operation with zero3, but gathering across nodes can be slow, which makes zero3 much slower than zero2. When training models like llama 70B, the current HSDP implementation, which shards are things only within nodes, will cause OOM; on the other hand, the FULL_SHARD strategy causes very low throughput. Currently, we have to involve intranode 8 way tensor parallel + internode zero2 to deal with such situation, but this adds the complexity and still leaves the redundancy (e.g. the duplicate activations among tensor parallel ranks if sequence parallel is not further applied). I guess a different hybrid sharding strategy, namely to shard params within a node, and gradient/optimizer globally, may well solve such problems. For example, given 16 gpus on 2 machines, after allgathering params globally, rank0 may retain the parameter shard for rank0 and rank8 after resharding. I am not sure if I've missed anything that makse the solution not as good as I think. Thx  Alternatives _No response_  Additional context _No response_ ",2024-01-22T08:30:27Z,triaged module: fsdp,closed,0,3,https://github.com/pytorch/pytorch/issues/117968,"Ah I found it in zero++, called Hierarchical Partitioning for ZeRO (https://arxiv.org/pdf/2306.10209.pdf) Is there any plan for FSDP to support this?","Thanks for filing this issue! We are working on a new FSDP implementation ( CC([RFC] Per-Parameter-Sharding FSDP)). (I am opening PRs these days actually üòÖ .) When that is ready for broader adoption (targeting midyear 2024), it should have this feature. (Disclaimer: Since this is a rewrite, we will inevitably have to go through migration and bugfixing though.)",Thanks and respect! 
rag,Fix test_compressed_layout_conversions_coverage to check BSC format,test_compressed_layout_conversions_coverage verifies torch's conversions between different memory layouts using numpy as a reference. Since numpy doesn't support BSC format it just skipped that. Instead fake it by using a transposed BSR format.   CC(Fix dynamo failure w/ astype)  CC(Fix test_compressed_layout_conversions_coverage to check BSC format),2024-01-21T22:56:51Z,Merged topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/117951,> What's the context behind this PR? As part of removing TestSparseCSRCPU.test_sparse_to_sparse_compressed_*_cpu_float64 from the dynamo skip list I noticed that the test was missing coverage for the BSC sparse mode  so this adds a test for it. The next diff in the stack (https://github.com/pytorch/pytorch/pull/117952) then fixes the underlying issues which forced us to skip all the test_sparse_to_sparse_compressed tests on dynamo. (Basically just increasing test coverage)
,[FSDP2] Added all-gather and unsharded parameter,"  CC([no ci][RFC] Added `fuse_comm_groups` staticmethod)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([BE] Enabled mypy in `common_fsdp.py`)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Added backward prefetching)  CC([FSDP2] Added `reshard_after_forward`)  CC([FSDP2] Added pre/postbackward)  CC([FSDP2] Added reducescatter)  CC([FSDP2] Added forward unshard/wait for unshard/reshard)  CC([FSDP2] Added `_to_kwargs` root forward input cast)  CC([FSDP2] Added allgather and unsharded parameter) This PR adds the FSDP allgather (the copyin/allgather collective and the copyout) and the unsharded parameter concept to `FSDPParam`. This is to prepare for being able to run the forward pass.  We implement allgather as two functions: `foreach_all_gather` (copyin/allgather collective) and `foreach_all_gather_copy_out` (copyout).      In the future, there will be two paths: `async_op=True` in the default stream for explicit prefetching and `async_op=False` in separate streams for implicit prefetching.      In the future, we will use `torch.split_with_sizes_copy` in the copyout when it has the CUDA fast path.      We have the functions operate on `List[FSDPParam]` instead of passing the `torch.Tensor` and metadata mainly so that the `all_gather_input` can be computed under the `all_gather_copy_in_stream`. Since the two functions are specific to FSDP, I did not see motivation for avoiding this at the cost of entering/exiting the `all_gather_copy_in_stream` context twice (which incurs some CPU overhead).  The `init_all_gather_output()` and `init_unsharded_parameter()` functions m",2024-01-21T22:38:01Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,0,https://github.com/pytorch/pytorch/issues/117950
rag,Fix dynamo subgraph fragmentation over graph breaks with better reconstruction,  CC(Fix dynamo subgraph fragmentation over graph breaks with better reconstruction) ,2024-01-20T00:57:38Z,open source Stale module: dynamo ciflow/inductor,closed,0,12,https://github.com/pytorch/pytorch/issues/117906,"90% credit to Jon, still far from done tho, added a failing test and am currently looking at some of the buggier edge cases. ","> Just nits for now, I'll take a deep look at the reconstructed function soon. A lot of this doesn't work, and is not worth nitting  in particular, the tests have name ferrying issues masked by it all being x ;)","In fact  I question the entire prelude strategy as it is done here  there's a ton of sus stuff that just doesn't work with this approach. The bones of it are vaguely sound, and relatively obvious as a start: 1) sub tx (inlining) should have continuation function insts produced via create_call_resume 2) we want to pick up where we left off via combining the result of (1) carefuly However  the scope story does not work out yet... consider:  if (h) was handled as a toplevel base interpreter (not inlining)  it's continuation function would look like: (truncated for brevity)  This is basically doing `__resume_at_70_1(a)` ... but because of how prelude works, this is getting blapped into the ContinueExecutionCache of `fn`! The raw original bytecode of fn is  BLAPPING THE RESUME CALL INTO THIS IS NOT SOUND! Prior versions of this PR in chuang's tests masked the bug by having everything be called `x`! I discovered this just by reading how the prelude stuff works. I have an alternate version of this that actually grafts the lower (inlining)(sub)tracer's code_options (aka the objects to make a codetype) up the stack of tx's to make a new CodeType to replace self.f_code with. This *does* work for the most part  but the problem now arises in remapping the original interpreter source and the rewritten one  we still end up with places referencing the original code, and the whole thing just ends up feeling sus as hell. So, I think I am going way back to the drawing board on this one, here is where I am at: 1) All instruction translators know how to resume 2) create_call_resume IS necessary for subtracers 3) TBD if it needs to be the same for base vs inlining 4) We need some new structures for marshalling around scope properly  specifically, if we consider the case above we need `a` to not only exist, we need it to be in the same exact state as if we had a top level non inlining instruction translator enter on `h` and graph break! That is to say  it needs to be identical to us running our graph up to that point, which in the directtoh case is `cos(x)` or in the `fn` call case is `cos(cos(x))`. For the latter, this *should* be possible because ostensibly our stitching has provided us with a graph that is `cos(cos(x))`  the problem is now in how to properly associate that with the LOAD_FAST a  this feels like an inverse of our output codegen, wherein we produce a postamble to our userdefinedcallbackreturnedcallable that maps the outputs to variables in the user calling scope. This needs more thinking. ","A lot of the discussion in your most recent comment is around managing scope, but it's not clear to me why scope is the lynchpin of the entire affair. In particular, as you observe, if we were to have graph break in h as toplevel, we already know how to generate code that sets up the 'a' local so the subsequent bytecode accessing it has something valid. So it doesn't seem like we need anything new, we've already got what we need. Let's talk about the inner graph break from fn. Ordinarily (pre this patch), I would generate the following bytecode sequence: 1. Call FX graph for fn prefix only 2. Call resume function   a. Restore local state   b. Do the rest of the bytecode (starting off with a call to h) What do I want out of the modified sequence? First, the FX graph is now for prefix of fn and prefix of h, and has a bunch of extra outputs which will be useful for restoring h's local state. I think I would still prefer to call it up front. Then, I want to call not into a vanilla h, but a modified h which can take the extra outputs, restore local state, and go straight to the graph break point. So in the end, I have this: 1. Call FX graph for fn prefix and h prefix, returning fn and h intermediates 2. Restore local state for fn using fn intermediates 3. Call into semicompiled h, passing h intermediates   a. Restore local state for h, using h intermediates   b. Do the rest of h suffix bytecode   c. Return 4. Do rest of fn suffix bytecode","> A lot of the discussion in your most recent comment is around managing scope, but it's not clear to me why scope is the lynchpin of the entire affair. In particular, as you observe, if we were to have graph break in h as toplevel, we already know how to generate code that sets up the 'a' local so the subsequent bytecode accessing it has something valid. So it doesn't seem like we need anything new, we've already got what we need. >  > Let's talk about the inner graph break from fn. Ordinarily (pre this patch), I would generate the following bytecode sequence: >  > 1. Call FX graph for fn prefix only > 2. Call resume function >    a. Restore local state >    b. Do the rest of the bytecode (starting off with a call to h) >  > What do I want out of the modified sequence? First, the FX graph is now for prefix of fn and prefix of h, and has a bunch of extra outputs which will be useful for restoring h's local state. I think I would still prefer to call it up front. Then, I want to call not into a vanilla h, but a modified h which can take the extra outputs, restore local state, and go straight to the graph break point. So in the end, I have this: >  > 1. Call FX graph for fn prefix and h prefix, returning fn and h intermediates > 2. Restore local state for fn using fn intermediates > 3. Call into semicompiled h, passing h intermediates >    a. Restore local state for h, using h intermediates >    b. Do the rest of h suffix bytecode >    c. Return > 4. Do rest of fn suffix bytecode This is similar to my current approach as hinted at in ""this feels like an inverse of our output codegen""  I am using the subtracer output graph output as a prelude to the prelude. ","> A lot of the discussion in your most recent comment is around managing scope, but it's not clear to me why scope is the lynchpin of the entire affair. Ehh  scope was the major issue in the current PR. It is not necessarily the lynchpin of anything. ","I rewrote the bytecode stitching algorithm  it now works, partially  in the cos x 6 motivating example described in CC(Dynamo inlining should compile partial subgraphs / improve graph break with recursive calls)  it produces a solution about halfway to where we want it Before we would basically have    6 times over Now, with my latest fix, we have  And then the     3 times over! So a huge improvement in that we correctly stitch together the initial pregraph break code, BUT once we hit the graph break, we don't cascade the returns up to their parent graph  we reenter recursively allowing the returns to do their thing. It is resilient to name changes, etc. This is still a huge improvement over the status quo, imo, and a good stepping stone. Going to keep playing with it.",Current state: (1) python example.py works (2) python example2.py does not This is due to scope / nesting name issues.,> Current state: >  > (1) python example.py works (2) python example2.py does not >  > This is due to scope / nesting name issues. batsignal  !!!,"Spoke to Jansel, made some progress. This state of the PR passes all the same tests, but fails assert code.co_freevars == out_code.co_freevars  this is progress as it is no longer failing when generating the code at reconstruction creation time as we graph break. AssertionError: (""free var mismatch: old code object has free var ('g',), new code object has free var ('g', 'h')"", , None) I'm not sure this is everything we need. Hitting that exception makes sense, and its a good exception, it segfaults if we comment it out (look like attempt to deref null cause bytecode is wrong?). Going to sleep on it, and bat signal   again.",I gave some pointers via workchat  is there something you want me to look at in this PR?  It doesn't seem ready yet.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Communication Computation Overlap issue with TransformerEngine + FSDP," üêõ Describe the bug I am training a large multinode transformer model with Pytorch FSDP, and am running into some issues while using TransformerEngine. I'm not sure if this is a Pytorch issue or a TransformerEngine issue, but any advice would be very appreciated. I will also crosspost on TransformerEngine! While using FSDP and TransformerEngine, I noticed that the communication/computation overlap is very poor. Looking at the traces, it seems that the TransformerEngine LayerNorm is actually waiting for the FSDP allGather in the forward pass:  If I use custom torch linear and layernorm layers, the overlap is much better:  In terms of iteration time too, the torch linear + layernorm significantly outperforms the TransformerEngine layer. When the computation per FSDP group increases (say, if I bundle multiple layers into a single FSDP unit) this problem gets a lot worse. Thank you very much for your time!  Versions Collecting environment information... PyTorch version: 2.1.2 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.28.1 Libc version: glibc2.31 Python version: 3.10.0  (default, Nov 20 2021, 02:24:10) [GCC 9.4.0] (64bit runtime) Is CUDA available: True CUDA runtime version: 12.3.107 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H1",2024-01-20T00:00:58Z,,closed,0,5,https://github.com/pytorch/pytorch/issues/117897,"Some more information on this: The flow events suggest that the FSDP allGather is just waiting due to rate limiting, which is fine and I assume is intended behavior. The weirdness comes from the fact that the LayerNorm from two layers ago is waiting for the allGather for the current layer to finish.  Would  have insight into this? (So sorry to tag on a Friday night. No rush to answer.) Although maybe it really is more of a TransformerEngine issue. Also if it is relevant at all I am using full activation checkpointing.",By any chance is the layer norm kernel being launched as a cooperative kernel (`cudaLaunchCooperativeKernel)`?,Yes it is actually,"I think that that might be the issue. Cooperative kernels require full GPU SM occupancy, so for a NCCL allgather that requires something like 2 SMs (assuming the allgather includes intranode), the cooperative kernel will wait for the allgather to finish in order to get the full occupancy. This might be something to ask the TransformerEngine team about or at least loop them in. ",Ahh thank you so much. Okay I will follow up on the TransformerEngine post. 
transformer,Failed to torch.export phi2 model," üêõ Describe the bug When trying to `torch.export` ""microsoft/phi2"" model, a `torch._dynamo.exc.InternalTorchDynamoError: ` was raised within dynamo converter.  I've enabled both `TORCH_LOGS=""+dynamo""` and `TORCHDYNAMO_VERBOSE=1`   Versions pytorch main branch  transformers==4.36.2 ",2024-01-19T23:09:45Z,onnx-needs-info module: dynamo oncall: export,closed,0,5,https://github.com/pytorch/pytorch/issues/117891, FYI,"I'm running into this error when running the model eagerly with the given inputs, which seems to match the error from dynamo: ",Maybe a transformers version issue? I am using `tarnsformers=1.36.2` and `model(**inputs)` does succeed on my local repro ,  It seems you did `model(inputs)` instead of `model(**inputs)`," Ah, that seems to work! Since it's a kwarg then it should be passed into export as a kwarg: `torch.export.export(model, args=(), kwargs=inputs)` which seems to work for me :> "
yi,[FSDP2] Added initial `_lazy_init` and FQNs for debugging,"  CC([no ci][RFC] Added `fuse_comm_groups` staticmethod)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added activation checkpointing tests)  CC([FSDP2] Added initial 2D and state dict tests)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Added backward prefetching)  CC([FSDP2] Added `reshard_after_forward`)  CC([FSDP2] Added pre/postbackward)  CC([FSDP2] Added reducescatter)  CC([FSDP2] Added forward unshard/wait for unshard/reshard)  CC([FSDP2] Added `_to_kwargs` root forward input cast)  CC([FSDP2] Added allgather and unsharded parameter)  CC([FSDP2] Added initial `_lazy_init` and FQNs for debugging)  CC([FSDP2] Sharded parameter in `FSDPParam`)  CC([FSDP2] Added initial `FSDPParamGroup`, `FSDPParam`, `ParamModuleInfo`)  CC([FSDP2] Added `mesh` arg, `FSDPState`, move to device)  CC([FSDP2][Reland] Introduced initial `fully_shard` frontend) This PR adds the initial `_lazy_init`. Lazy initialization marks the point when the FSDP structure is finalized and is typically the beginning of the first forward. This would be after any metadevice initialization.  Lazy initialization is distinct from construction time because when processing `fully_shard(module)`, there is no way to know whether a parent of `module` will have `fully_shard` applied as well. This is a consequence of `fully_shard` having to be applied bottomup.  At lazy initialization, we now have the concept of a _root_. The root FSDP module is the one whose `forward` runs first and ends last (and hence similarly for its backward). Having a single root simplifies handling logic that should only run """,2024-01-19T21:26:41Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,5,https://github.com/pytorch/pytorch/issues/117881, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"(1) ""define fullyqualified names (FQNs) "" what doest this mean? i thought we would not be in the business of mangling FQNs anymore after ditching the wrapper class approach.  hence userprovided FQNs are the FQNs. (2) wondering about lazy init and meta init.  If we do meta init, does that run lazy init?  and then when we do real init, what do we need to do to relazy init?  would be nice to mention this in the PRdesc (even if the answer is ""we'll deal with it in later PR""","> (1) ""define fullyqualified names (FQNs) "" what doest this mean? i thought we would not be in the business of mangling FQNs anymore after ditching the wrapper class approach. hence userprovided FQNs are the FQNs. You are right. We do not mangle the FQNs anymore. However, for debugging/testing purposes, it is nice to associate each `FSDPParamGroup` and each `FSDPParam` with an FQN respectively as an identifier. We prefer to not do so directly at construction time because we do not have the _root_ module identified yet. Instead, we set these FQNs at lazy initialization time where we have finalized the application of FSDP to the overall model and can identify the root. The FQNs are prefixed starting from the FSDP root module. > (2) wondering about lazy init and meta init. If we do meta init, does that run lazy init? and then when we do real init, what do we need to do to relazy init? would be nice to mention this in the PRdesc (even if the answer is ""we'll deal with it in later PR"" Lazy init should happen after meta init since lazy init happens upon the 1st forward pass. Let me add something to the PR description.",> Lazy init should happen after meta init since lazy init happens upon the 1st forward pass. Let me add something to the PR description. yea that would have been obvious if i were not tired. :)
,Tensor Parallel + FSDP," Issue description I am trying to train large language models using FSDP and tensor parallel technology. I have explored the native capabilities of torch, I mean `parallelize_module` and `ColwiseParallel`, `RowwiseParallel`. Apparently, these methods are good when the model fits completely into the GPU memory, but they do not imply other initialization options and have problems with Attention accroding to documentation (maybe I'm wrong, but anyway). My solution was to use tensor parallel implementation from Nvidia Megatron code. And using 2D `DeviceMesh` (one dimension for data parallel group, second dimension for tensor parallel group), I could manage the data & tensor parallel groups of the GPU`s. Sharding of the part of the model corresponding to each data parallel group is performed using the process_group parameter in FullyShardedDataParallel. Thus, FSDP does not know about tensor parallel, but when executing forward and backward tensor parallel FSDP modules communicate with each other in the way it is implemented in the megatron implementation inside Parallel layers. However, judging by the saved checkpoints, an error occurs somewhere and the gradient values for duplicate layers differ for different FSDP modules corresponding to data parallel parts of the model (in order inputs and loss functions are same here). For simplicity, I started an experiment for a model with two layers  `Embedding layer` + ColumnParallel layer from Megatron with tensor parallel mode = 2. Column Parallel gathers the output, so layer`s output is exactly equal of applying 2 times larger nn.Linear layer. For two FSDP modules  the Embedding layer shape and ini",2024-01-19T10:55:33Z,triaged module: fsdp module: dtensor,closed,0,0,https://github.com/pytorch/pytorch/issues/117843
rag,[DCP] Allow users to save and load without creating storage reader and writer,  CC([DCP] Add tests to demonstrate DCP checkpoint conversion)  CC([DCP] Allow users to save and load without creating storage reader and writer)  CC([DCP][BC] Remove the dependency on _shard.TensorProperties) Right now DCP API requires users to create StorageWriter and StorageReader for every API call. This PR allows users to only pass the checkpointer_id (a path) and use it to read/write a checkpoint without creating a StorageReader and Writer. Differential Revision: D52740556 ,2024-01-18T19:45:36Z,oncall: distributed Merged ciflow/trunk ciflow/periodic module: distributed_checkpoint suppress-bc-linter,closed,1,2,https://github.com/pytorch/pytorch/issues/117772," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,llama_v2_7b_16h stopped working with torch.jit.trace," üêõ Describe the bug We've noticed this behavior on torchbench using torchscript as backend on PyTorch main branch. As we didn't identify the offending PR, my concern is if that regression was merged into PyTorch 2.2 or not. I am still working on a local repro, but it should be something similar to  The error is   Unfortunately I didn't repro locally and that is all that is informed by torchbench report. The last time we are sure the model was exporting with torch.jit.trace was around 12/16, when we locally executed torchbench https://github.com/pytorch/benchmark/pull/2121 is a proposal (not tested) for working this around, but we still need to understand   Versions pytorch main ",2024-01-18T16:11:22Z,module: third_party onnx-needs-info,closed,0,9,https://github.com/pytorch/pytorch/issues/117752,"I assume it's one of the transformers changes, which no longer correctly decompose... Though it fails the same way for me with 2.1.2 and transformer==4.36.2, but downgrading to 4.32.1 makes it work as expected",We should probably should just propose a ternary to transformer to use older path...,> We should probably should just propose a ternary to transformer to use older path... Have you discussed that with HF folks? Is there an issue we can track? I was wondering whether https://github.com/pytorch/benchmark/pull/2121 could be a fix on our end,"Issues related to SDPA have been raised three times in ONNX Runtime already. 1. https://github.com/microsoft/onnxruntime/pull/19015discussion_r1442432173 2. https://github.com/microsoft/onnxruntime/issues/19051 3. https://github.com/microsoft/onnxruntime/issues/19040 It was fixed here for LLaMA, but similar issues will likely be raised for other models. It would be helpful if the eager attention implementation is traced by default during the export of Hugging Face models.",> > We should probably should just propose a ternary to transformer to use older path... >  > Have you discussed that with HF folks? Is there an issue we can track? >  > I was wondering whether pytorch/benchmark CC(Bug fixes) could be a fix on our end   gentle ping,related discussion https://github.com/huggingface/transformers/issues/28610,> We should probably should just propose a ternary to transformer to use older path...  proposed this change: https://github.com/huggingface/transformers/pull/28823,A more proper fix will be done in part of https://github.com/huggingface/transformers/pull/27931,https://github.com/huggingface/transformers/pull/27931 has been merged this morning. We can close this one now
,Modifying parameters of FSDP-wrapped module by hand without summon_full_params context," Modifying parameters of FSDPwrapped module by hand without summon_full_params context  Issue description I am training a large language module using FSDP.  I want to store EMA weights while training, and use the EMAmodel during evaluation. To this end, I initialize EMAmodel as a deepcopy of the original one, and then update its parameters after every batch by hand. (I do deepcopy *before* wrapping both models in FSDP). Existing implementations seem to use FSDP.summon_full_params() context when updating parameters (see e.g. here). However, I do not have this option since my model is large enough that FSDP.summon_full_params() results in too high memory usage.  In principle, it seems possible to update parameters of the EMA model directly, since both the original model and EMA model are stored identically.  However, it turns out that after modifying EMA parameters, the output of `ema_model.forward()` is unchanged, suggesting that these modifications do not persist.  Code example  I apologize for not being able to provide a full code, since I use a framework where the relevant code is scattered (I use llmfoundry + composer link). The output of the last line does not change after running `update_parameters()`, although if I print out e.g. `p_averaged` when running `update_parameters()`, its value changes as expected. Is there any way to update parameters of FSDP module by hand in this situation, without using `summon_full_params()` context?  System Info Result of running `collect_env.py` on my system is here: https://gist.github.com/bluntoctopus/3edcbc83e639b2cdcad47ab0abcca623 ",2024-01-18T10:22:18Z,triaged module: fsdp,open,0,4,https://github.com/pytorch/pytorch/issues/117742,"I agree that we should be able to do EMA with FSDP without `summon_full_params()` (as long as both the main and EMA copies both shard with FSDP identically). There are 2 'gotchas' that might be happening: 1. The sharded parameters need to be exposed from `model.parameters()` when performing the EMA update. This mainly means that you should not run the EMA update after the main model's forward but before its backward. In other words, we should make sure to run the EMA update after backward or the optimizer step but before the next forward. 2. The EMA model did not free its allgathered parameters, so the next forward does not reallgather and hence uses stale parameters. For 2, we do not have a public API right now for how to force reshard the parameters, but I think the function in  CC(FSDP silently ignores a state_dict loaded between forward and backward.)issuecomment1890948734 should work.","Hello , thanks a lot for your comment. I've tried to use the method you've linked (with `FSDP._runtime_utils._reshard()`). However, it does not seem to work properly. As I understand it, the need for manual resharding arises from the fact that we use `ema_model.forward()` during validation, but do not ever use `ema_model.backward()`, which can lead to FSDP using preunsharded (and thus stale) weights. To address this problem, I put manual resharding codeissuecomment1890948734) in between validation and updating of EMA parameters. However, it fails with AssertionError ""Expects storage to be allocated"", occuring here, which I take to mean that the manual resharding was not needed? (I have also tried wrapping resharding inside `try: ... except` block, to avoid resharding the weghts when not needed, and found that it never executes without AssertionError at all). Can you please elaborate on the conditions under which we should need manual resharding, or the reason for the above AssertionError? ","I am guessing that you might be hitting the `AssertionError` since you have a mix of FSDP modules that _have_ resharded after forward and ones that have not.  On nightlies, the reshard code should become idempotent now (cc: ), so if you have access to the nightlies, then the `AssertionError` should go away.",any workarounds on this?
llm,[dynamo] Add more dynamo call_methods and getattr support or Placement,Summary: Explained by title. This fix is part of:  CC([Dynamo] Better support DTensor) Test Plan: Unit tetst and CI  Unit test: `buck2 test mode/devnosan //caffe2/test/distributed/_tensor:dtensor_compile  test_placement_compile` Differential Revision: D52863073 ,2024-01-18T06:53:41Z,oncall: distributed fb-exported Merged ciflow/trunk module: dynamo ciflow/inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/117733,This pull request was **exported** from Phabricator. Differential Revision: D52863073,This pull request was **exported** from Phabricator. Differential Revision: D52863073,This pull request was **exported** from Phabricator. Differential Revision: D52863073,This pull request was **exported** from Phabricator. Differential Revision: D52863073," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[copy][inductor] Faster C++ kernel python bindings,This is a copy of CC([inductor] Faster C++ kernel python bindings) to import into fbcode for testing there. ,2024-01-18T00:14:13Z,ciflow/trunk module: inductor module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/117709," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
transformer,[ONNX] Bump transformers in CI test,"Fixes CC([ONNX] transformers version on CI is outdated)  (1) skip dynamic tests for exported program in `test_fx_to_onnx_onnxruntime.py`, as they are not expected to pass anyway. (2) Move dolly model to runtime, since it's working in exporting, but it is blocked by nonpersistent buffers as well. (3) openai whisper has changed/regression due to modeling modifications. (4) Replace OpenLlama with Llama, because OpenLlama is deprecated.",2024-01-17T23:10:51Z,module: onnx triaged open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/117703,Looks like openai_whisper had some changes. FLAKY fail seems unusual.,PTAL, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[dynamo][distributed] Allow _same_storage_size in graph,"  CC([dynamo][distributed] handle _rank_not_in_group, _get_or_create_default_group)  CC([dynamo][distributed] Allow _same_storage_size in graph) ",2024-01-17T21:20:42Z,open source ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/117690,"Too hacky, missing shape guards, will do it properly."
transformer,[ONNX] transformers version on CI is outdated,"Currently, the transformers version on ONNX CI is still 4.32.1, while the released one is 4.36.2. There are lots of Config/Modeling changes that the outdated tests potentially make our guarding less robust, and it also increases the difficulty of developing/maintaining transformers related models of conversion. The version should be bumped.",2024-01-17T17:13:40Z,module: onnx triaged onnx-triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/117660, ,That is a good point. this is where we pin it https://github.com/pytorch/pytorch/blob/da6abaeeacedc872ada6577fa1dd0c6c8024188a/.ci/docker/common/install_onnx.shL29,"ps: when the issue is opened from onnx to onnx, and thus don't need Meta's triaging the issue, we can also add the `triaged` label :)","Should we just update it, or we want to discuss for another approach?"
transformer,Extra long allGathers when scaling up FSDP on 7b model," üêõ Describe the bug I am training a multinode 7B parameter transformer model and running into some issues with FSDP. Training starts out going pretty fast, but then slowly and up to the first validation, the MFU tank, after which it stays low:  Looking at the trace, I see that some allGathers take much longer than others, and of course the main GPU stream waits for it:  I identified that for each of the long allGathers, there is a single straggler rank that is actually doing no GPU work during the allGather:  This is what the CPU workload looks like for the straggler (the _pre_forward call when the straggler is doing no GPU operations is highlighted):  Does anyone know why this may be? I don't think it's a data issue, as it happens in the middle of the network. I suspected a memory issue as the reserved memory is at 78GB, but I have also observed this behavior with models with smaller peak reserved memory, and also I print ""num_alloc_retries"" from each rank and don't observe any meaningful pattern.  Versions  ",2024-01-16T16:07:28Z,triaged module: fsdp,closed,1,3,https://github.com/pytorch/pytorch/issues/117550,Perhaps  is the right person? Thank you very much!,You might want to try using manual Python GC.  This has been a known major cause of stragglers like this for us.,Wow that was it thank you!
transformer,[PT2] Failed to capture graph in export with dynamic shape inputs for LLM models," üêõ Describe the bug We aim to apply `pt2e quantization` for LLM models, but encounter challenges in capturing graphs with inputs of  dynamic shape despite setting constraints in `capture_pre_autograd_graph`. In LLM models, sentence generation is performed using `model.generate()`, which internally invokes `model.forward()` multiple times to generate tokens. The issue arises from the dynamic changes in input shape with each call to `model.forward()` inside `model.generate()`. Assume that the size of the prompt is `x`, batch size is `bs`, number of beams for beam search is `nb`, and input shape change rule is as follows:  Initial inputs shapes (for first token):    Input_ids: `(bs, x)`    Attention_mask: `(bs, x)`  For ith next tokens:    Input_ids: `(bs\*nb, 1)`    Position_ids: `(bs\*nb, 1)`    Attention_mask: `(bs*nb, x + i + 1)`    Past_key_values: `num_hidden_layers\*((bs\*nb, num_attention_heads, x+i, head_dim),(bs\*nb, num_attention_heads, x+i, head_dim))` As shown in the input shape change rule, some shapes need to be treated as dynamic shapes when capturing graph so that the captured graph can be applied for all tokens generation.  To make things easier, we tried to capture two graphs, one for the first token, and one for the next token. And it turns out that we can succeed to capture the graph for first token and it works fine. However, we are unable to capture expected graph for next tokens.   Unit Test Here is a unit test that reproduces the issue I met.   Error Message [20240111 17:44:58,216] [0/0] torch._guards: [ERROR] Error while creating guard: [20240111 17:44:58,216] [0/0] torch._guards: [ERROR] Name: '' [20240111 17:44:5",2024-01-15T05:48:39Z,triaged oncall: pt2 module: dynamic shapes export-triaged oncall: export,open,0,22,https://github.com/pytorch/pytorch/issues/117477,"Hi , could you kindly help to take a look of this issue? Appreciate if any suggestions for how to capture the fx graph for next token cases with input of dynamic size.","To capture a model that works for sizes 0/1, you should trace with size 2 or larger. This is because 0/1 sizes will induce specialization. If you are lucky, you will end up with a model for >= 2 that will happen to work for 0/1. When we work with sizelike unbacked symints, we never assume that they are equal to 0/1 for similar reasons."," Thanks for your suggestion. We tried changing the size (past_key_values[i][0].size()[2]) to 10, but unfortunately, we still unable to capture expected graph because **past_key_values[i][0].size()[2] was inferred to be a constant (10).** **Reproduce**: https://gist.github.com/blzheng/c890d3744913820392fa3928eb8061d3fileut **Error info**: https://gist.github.com/blzheng/c890d3744913820392fa3928eb8061d3fileerror","OK, that's progress. The next step is to examine why the size was inferred to be constant. This can be done using TORCH_LOGS=dynamic and looking for the log line corresponding to the guard on this source.",this is kv cache right?  is trying to refactor it to be the input of the model I think,"Oh, is the problem the kvcache is dynamic but also a module attribute? You know that we can change PT2 to allow for nonstatic module buffers right? You don't have to refactor unless you really don't want to touch some Dynamo code (but the policy decision here isn't even that hard...)","> Oh, is the problem the kvcache is dynamic but also a module attribute? You know that we can change PT2 to allow for nonstatic module buffers right? You don't have to refactor unless you really don't want to touch some Dynamo code (but the policy decision here isn't even that hard...) oh I don't know the details, the reason we are refactoring this for executorch is because executorch itself does not support mutable buffers anyways right now (it will be added in a few weeks). if mutable buffers is supported in dynamo already, then I think we could try to export this. in the end, we want to just export this directly I think  ","> OK, that's progress. The next step is to examine why the size was inferred to be constant. This can be done using TORCH_LOGS=dynamic and looking for the log line corresponding to the guard on this source. This is the log we captured using TORCH_LOGS=dynamic. https://gist.github.com/blzheng/c890d3744913820392fa3928eb8061d3filelog  We do not understand why `set_replacement s0 = 10` appears. The transformers version is v4.35.2.","It's the line below:  Which is  This is all looking familiar to me. Here is a similar problem in HF that I debugged, maybe you can do the same process:  CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0)issuecomment1734754159","Hi , thanks for the suggestions. I think we may know why `past_key_values[i][0].size()[2]` was inferred to be a `constant (10)` as `[20240116 17:20:36,024] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s0 + 1, 11)`:  We didn't add the constraints that `attention_mask.size(1)` to be a dynamic size. So, it has specialized to `constant(11)`.  `attn_weights + attention_mask` add the constraints that `attn_weights` and `attention_mask`(`constant(11)`) has same size. And `attn_weights` comes from https://github.com/huggingface/transformers/blob/514de24abfd4416aeba6a6455ad5920f57f3567d/src/transformers/models/gptj/modeling_gptj.pyL248, which the size of `key` comes from `example_inputs[""inputs_id""]` which is with `constant(1)`. I think we may add the constraints that `attention_mask.size(1)` to be a dynamic size to fix this issue. After that, we got some other errors:  Reproduce: https://gist.github.com/lesliefangintel/b1e5c9c5d331c1a0e82fbbd644fe967afilefileut  Error Msg: https://gist.github.com/lesliefangintel/b1e5c9c5d331c1a0e82fbbd644fe967afileerrormsg    We see 1 line from the log which looks suspicious (`Some dynamic dimensions need to be specialized because the constraints inferred for them are too complex to specify.`).    Now we have saw some hint for how to add the constraints at the end of the log file which I think we already added.","fangintel Looking at the error message, it seems like when you made some dimension dynamic, it interacted with another dynamic dimension but not in a way that is currently expressible (we can express that one dynamic shape is equal to another, but no other nontrivial relationship): here one dynamic shape = another + 1, can you confirm? Curiously we are increasingly seeing examples of such requests (e.g., `s1 = s0 + 1`, `s1 = s0 * 4`, etc.) so I'm actually working on a prototype to support it. No promises yet but I'll update when I make some progress.   ","> You don't have to refactor unless you really don't want to touch some Dynamo code (but the policy decision here isn't even that hard...)   Executorch cant handle mutable buffers yet regardless of what export policy is. My understanding is that with a lifted graph of mutable state youll see the state as IO, and then the copy back from output to input placeholder happens outside the graph. That copy back is the problem for ExecuTorch to handle right now because theres no guarantee we can dereference the memory backing the tensor. So for now we are forcing the user to lift it to IO and then figure out the copy over themselves. I dont want export to implicitly do this lifting silently, and then have us not be able to hide it as changing model io is a super negative user experience. ","> fangintel Looking at the error message, it seems like when you made some dimension dynamic, it interacted with another dynamic dimension but not in a way that is currently expressible (we can express that one dynamic shape is equal to another, but no other nontrivial relationship): here one dynamic shape = another + 1, can you confirm? >  > Curiously we are increasingly seeing examples of such requests (e.g., `s1 = s0 + 1`, `s1 = s0 * 4`, etc.) so I'm actually working on a prototype to support it. No promises yet but I'll update when I make some progress. >  >   , yean, I think that's the problem. Actually, I have tried to write some constraints as:  to workaround this issue which turns out not work also.","I just wanted to point out that it's a relatively simple code change in pytorch to disable the export constraint solver () and this won't break export, and that I know people internally have been doing this to work around constraint solver insufficiencies.","this is assigned, so adding the triaged label to remove it from the query of untriaged issues", updates on this?,"  What is the current status of this issue? BTW, I am curious about why we see such error `Some dynamic dimensions need to be specialized because the constraints inferred for them are too complex to specify` in  CC([PT2] Failed to capture graph in export with dynamic shape inputs for LLM models)issuecomment1895212019.  ","I am able to capture the graph of GPTJ with this code: https://gist.github.com/XiaWeiwen/81105535ab11bf8098e850fac7a90a34 The key part is the `dynamic_shapes`:  torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (dim2)! For more information, run with TORCH_LOGS=""+dynamic"".    Not all values of dim2 = L['past_key_values'][0][0].size()[2] in the specified range dim2 <= 2048 satisfy the generated guard Ne(2048, L['past_key_values'][0][0].size()[2] + 1).    Not all values of dim2 = L['past_key_values'][0][0].size()[2] in the specified range dim2 <= 2048 satisfy the generated guard 2 <= L['past_key_values'][0][0].size()[2] and L['past_key_values'][0][0].size()[2] <= 2047 Suggested fixes:   dim2 = Dim('dim2', max=2047) ``` I am wondering why we should use this range of `max` values. And I am still not sure if this behavior of export is expected. Are there any docs on how to set max values?   Thanks","Hi   I saw the docs for `dynamic_shapes` of `export` here: https://pytorch.org/docs/stable/export.htmlmoduletorch.export. Looks like the trialanderror method is the recommanded: (1) prepare `dynamic_shapes` with `Dim`, (2) check the error message if any, (3) modify `Dim`'s `max` according to the error message, (4) repeat until no error occurs. This works in this case but it's quite confusing. Do you plan to elaborate more on the docs of `export` with `dynamic_shapes`? Thanks.","That's not the intended workflow. The intended workflow is you *know* what sizes you're intending to send to the model, and then you trace under that range and if export fails, that's export telling you that it couldn't quite figure out if it had successfully generated a model that works for everything in the range, because there was a guard. And then, you figure out who was causing the specialization, typically by using TORCH_LOGS=dynamic","Hi  Weiwen fangintel, I assume your latest repro has been changed to https://gist.github.com/XiaWeiwen/81105535ab11bf8098e850fac7a90a34 (after we deprecate the constraints API) and the remaining question on this issue is: ""How to find the correct max value to set for Dims? "" If so, are you able to find the place causing max <= 2048 with TORCH_LOGS=dynamic?  If not, feel free to point out any particular repro/issue you want us to look at, since it's not super obvious now what is the latest repro/error pair you got after 6 months when we change a lot of dynamic shape logic internally. (btw, it shouldn't affect anything but just want to point out that capture_pre_autograd_graph is also deprecated, we recommend to use torch.export.export() moving forward) Thanks!",Thanks  for the comment.  could you help to take a try a again for this issue since the constraints API has changed?
rag,Properly unwrap_storage tensors sent to DynamicScalar,  CC(Properly unwrap_storage tensors sent to DynamicScalar) Signedoffby: Edward Z. Yang  ,2024-01-13T19:33:37Z,Merged ciflow/trunk topic: bug fixes module: inductor ciflow/inductor release notes: inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/117444, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,DISABLED test_lazy_init (__main__.TestCuda),Platforms: rocm This test was disabled because it is failing on ROCm6.0 CI upgrade PR eg.: https://hud.pytorch.org/pr/pytorch/pytorch/116270 CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)17880 ,2024-01-13T04:34:03Z,module: rocm triaged skipped,closed,0,2,https://github.com/pytorch/pytorch/issues/117430,ROCm 6.0 CI migration not complete yet.  Reopening.,ROCm 6.0 CI migration not complete yet. Reopening.
transformer,CHECKPOINT_PREFIX is not stripped when non-root module is activation checkpointed," üêõ Describe the bug When training large models, we often want to activation checkpoint something smaller than the wrap module for FSDP. For example, we might want to only activation checkpoint attention in a transformer block. Unfortunately, when calling `get_state_dict` with the new distributed checkpoint interface, the _CHECKPOINT_PREFIX from checkpoint wrapper is not properly stripped when we activation checkpoint submodules.  We have to monkeypatch torch here to strip this always.  Versions Torch 2.1.2 / Nightly for Torch 2.2 ",2024-01-12T21:38:25Z,oncall: distributed module: fsdp,closed,0,0,https://github.com/pytorch/pytorch/issues/117399
transformer,Logging when executing fx.Interpreter, üêõ Describe the bug Something like this:   Versions main ,2024-01-12T13:25:02Z,good first issue module: logging triaged oncall: pt2,open,0,10,https://github.com/pytorch/pytorch/issues/117351, Do you want to log every function?,Yes,I'll see what I can do,  How detailed should the logging be? Just when every function is called with which parameters? Or should it include more than this?,"My ideal print is what you would be the line of code as if the function call was generated to Python.  So, e.g., show me something like  It is probably unwise to actually print the contents of `x_1`, since it might be quite large."," Right now I have these log.debug statements like `log.debug(f""Execute call_function node {target} with {args} and {kwargs}"")` just like above. Is this satisfactory? p.s. It's my first time contributing to open source, hope I'm not misunderstanding or doing it really wrong lol","Well, take a look at the output; does it look good?","I tried testing it, but failed. Tried running the test_torch.py file but it just says it failed to import torch. I think this is because I have to build it first, however I tried but failed.  After running `python setup.py develop`, I get this error:  Probably because I don't have cuda, however I cannot seem to disable this. I can see in the development tips that it should be possible to skip cuda, but I don't know how.","For something like this, you don't need to build PyTorch from source, you can use tools/nightly.py to get working binaries for your dev copy.","I can't seem to either pull or checkout, It gives me a FileNotFoundError.  Conda is installed and in PATH, not sure how to fix it."
yi,Pyi doc inclusion + fix,Reland of https://github.com/pytorch/pytorch/pull/114705 with extra fix to smoothly handle when the modules we're trying to load are not available (and thus the pyi won't contain the docs in this case). Tested locally that it works properly in fbcode.,2024-01-11T15:49:00Z,Merged ciflow/trunk release notes: python_frontend topic: docs,closed,0,4,https://github.com/pytorch/pytorch/issues/117267, merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `pyi_doc_fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout pyi_doc_fix && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[inductor][cpp] apply simplify_index_in_vec_range to vector store and vector transpose,"  CC([inductor][cpp] apply simplify_index_in_vec_range to vector store and vector transpose)  CC([inductor][cpp] apply simplify_index_in_vec_range in select_tiling_indices to enable more contiguous vec load)  CC([inductor][cpp] improve vector contiguous checks for FloorDiv and ModularIndexing) As the title, this PR extends the `simplify_index_in_vec_range` to store and transpose. ",2024-01-11T14:36:40Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/117263, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x a852522640b2594007523847cca7bf7c45bae390` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[inductor][cpp] apply simplify_index_in_vec_range in select_tiling_indices to enable more contiguous vec load,  CC([inductor][cpp] apply simplify_index_in_vec_range to vector store and vector transpose)  CC([inductor][cpp] apply simplify_index_in_vec_range in select_tiling_indices to enable more contiguous vec load)  CC([inductor][cpp] improve vector contiguous checks for FloorDiv and ModularIndexing) For the one of the kernels in the UT `test_vec_contiguous_ModularIndexing`: Before:  After:  This PR also further speeds up the model `swin_base_patch4_window7_224` from 1.25x to 1.28x. ,2024-01-11T13:50:06Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/117260, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
peft,_IncompatibleKeys returned from nn.Module.load_state_dict,"Hello all, While loading multiple adapters with `peft`, I get an `_IncompatibleKeys` object with many `missing_keys`, but no `incompatible_keys`. I have raised an issue on `peft` here: huggingface/peft CC(Concatenate directly into shared memory when constructing batches). However, the answer that I got was that: > The _IncompatibleKeys comes up because of this line in set_peft_model_state_dict : > `load_result = model.load_state_dict(peft_model_state_dict, strict=False)` > Here, peft_model_state_dict is the state dict containing our new adapter weights (and ONLY the new adapter weights). Internally, PEFT > will rename the state dict entries to use the new adapter name adapter2. So in my case, it looks like: > `['base_model.model.transformer.h.0.attn.c_attn.lora_A.adapter2.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.adapter2.weight', ......]` > When you call load_adapter, a new adapter is initialized first using add_adapter here. Next, when > we load the state dict for the new adapter using model.load_state_dict, PyTorch finds that all the entries for the new adapter are present, but the entries for the base model and the old adapter are not present. Hence you get a _IncompatibleKeys load result with a long list of missing keys (and no unexpected keys). This means that the issue was cased by PyTorch not finding the keys for the base model and old adapter in the new state dict, which contains only entries for the new adapter's state dict. Therefore, it reports that there are missing keys. However, I note that the `missing_keys` attribute contains entries from both adapters. This means that the `missing_keys` are not simpl",2024-01-11T13:34:45Z,module: nn triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/117258,"From the thread on the other issue, it seems like some transformations are happening on the `peft` end that are causing this and this is not a PyTorch issue.  > Here, peft_model_state_dict is the state dict containing our new adapter weights (and ONLY the new adapter weights). Internally, PEFT will rename the state dict entries to use the new adapter name adapter2. So in my case, it looks like: 'base_model.model.transformer.h.0.attn.c_attn.lora_A.adapter2.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.adapter2.weight', ....... When you call load_adapter, a new adapter is initialized first using add_adapter here. Next, when we load the state dict for the new adapter using model.load_state_dict, PyTorch finds that all the entries for the new adapter are present, but the entries for the base model and the old adapter are not present. Hence you get a _IncompatibleKeys load result with a long list of missing keys (and no unexpected keys). Closing as complete, but feel free to reopen if you still feel that this is a PyTorch issue :)"
yi,Submit a bug when trying to assign arg 'out=' in api torch._foreach_add_," üêõ Describe the bug When I try to use aten.\_foreach_add.Scalar_out by assign 'out=' in torch.\_foreach_add_ api like thisÔºö  Error occurs:  And if I follow the Error and change the param from `(a,)` to `a`, another error occursÔºö  It seems like ops generated by keyword `autogen` in native_functions.yaml could not  be called by torch api.  For example, in native_functions.yaml, foreach_add.Scalar_out belongs to keyword `autogen`, and I could not call foreach_add.Scalar_out by torch.foreach_add_ or torch._foreach_add, just like the above error. Is this a bug and will this be fixed in future version? Thanks a lot   Versions PyTorch version: 2.2.0.dev20231208+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: Could not collect CMake version: version 3.27.1 Libc version: glibc2.31 Python version: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.026genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.1.66 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A800SXM480GB GPU 1: NVIDIA A800SXM480GB GPU 2: NVIDIA A800SXM480GB GPU 3: NVIDIA A800SXM480GB GPU 4: NVIDIA A800SXM480GB GPU 5: NVIDIA A800SXM480GB GPU 6: NVIDIA A800SXM480GB GPU 7: NVIDIA A800SXM480GB Nvidia driver version: 525.105.17 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Litt",2024-01-11T08:18:55Z,triaged module: mta module: python frontend,open,0,7,https://github.com/pytorch/pytorch/issues/117233,"Currently inplace foreach functions don't return values due to the limitation of https://github.com/pytorch/pytorch/blob/28bb31e4a588f3fa198e927439fe09c871201aab/torchgen/model.pyL1412L1434 and write out the results into `self` argument. So I'm afraid I don't think `torch._foreach_add_(self=(a,), other=c, out=c)` makes sense as `c` is a list of python scalar. I however get a bit surprised that list of one python scalar is handled. I'm not sure if we would like to relax this constraint specifically for inplace foreach functions so that they can return a list of tensors (which isn't compatible with method chaining as we can't do `list[Tensor].op_()`."," Thanks for your apply. But I'm still not clear on how to use torch api to call aten.ops' out variants. For example, I want to call `torch.ops.aten._foreach_add.Scalar_out`ÔºåI know I can call this op directly. But for some reason, I want to call this op through torch api `torch._foreach_add`. I tried various combinations of inputs and it  stills not work.  And the signature  of `torch._foreach_add` provided by torch shows it doesn't support Scalar_out variants (just like below code).  So do you think this is a bug?",I'd say we would need to add the support of `out` argument in the end. Firstly I'd need to work on  CC([RFC] Let in-place foreach functions return a list of Tensors).,"> for some reason, I want to call this op through torch api `torch._foreach_add`. I tried various combinations of inputs and it stills not work. Why wouldn't `torch._foreach_add` be sufficient for this case?",> I'd say we would need to add the support of `out` argument in the end. Firstly I'd need to work on CC([RFC] Let inplace foreach functions return a list of Tensors). OK. Thanks a lot.,> Why wouldn't `torch._foreach_add` be sufficient for this case?  I mean I can't call `torch.ops.aten._foreach_add.Scalar_out` through `torch._foreach_add`. Because I want to do some statistics on  the mapping relationship between torch api and  torch.ops.aten.xx.,"Hi , I tried to add `=out` overloads to foreach ops, but currently it seems like there's a check disallowing passing a list/tuple of Tensors as `out`  https://github.com/pytorch/pytorch/blob/86a2d67bb9db7dae8ff4589930dd505a6c5b4ec6/torchgen/model.pyL1434L1456"
transformer,"torch.onnx.export the torch,nn.TransformerEncoderLayer, the dynamic_axes work not right"," üêõ Describe the bug  the bug description When I use the tensor shape like this (batch_size, seq_len, embedding_size) and i put the batch_size, seq_len in dynamic_axes to generate onnx model, I cannot use the onnx model to handle data with different seq_len, as you can see, the *code* is as follows:   the *error message* is like this:   Versions I found it maybe one bug in the `multi_head_attention_forward` function in the `torch/nn/functional.py`, as I change the   and  in the `multi_head_attention_forward` into   and  The bug will fix and I can get the seq_len changable. I guess the torch.onnx.export changes the shape[0] to a static item or the v, k shape[0] may not be as same as tgt_len so the changes I made were wrong. Thanks for your help!",2024-01-11T03:23:36Z,module: onnx triaged,open,4,3,https://github.com/pytorch/pytorch/issues/117209,I am experiencing the same problem when trying to `onnx.export` with `torch==2.2.1` and `onnx==1.15.0`. Minimal repro below:  Without `dynamic_axes`:  With `dynamic_axes`: ,This seems to be a dynamic shape issue Please try the new ONNX exporter and let us know if that works for you: quick torch.onnx.dynamo_export API tutorial,"Sorry, but the Dynamo export API throws an error, and the log is literally empty! Any updates on this issue? "
llm,Add uint1 to uint7 dtypes," * __>__ pytorch/pytorch CC(Add uint1 to uint7 dtypes) Summary: These dtypes are added since we see more demand for these sub byte dtypes, especially with the popularity of LLMs (https://pytorch.org/blog/acceleratinggenerativeai2/step4reducingthesizeoftheweightsevenmorewithint4quantizationandgptq2021toks) Note these are just placeholders, the operator support for these dtypes will be implemented with tensor subclass. e.g. torch.empty(..., dtype=torch.uint1) will return a tensor subclass of uint1, that supports different operations like bitwsise ops, add, mul etc. (will be added later) Also Note that these are not quantized data types, we'll implement quantization logic with tensor subclass backed up by these dtypes as well. e.g `Int4GroupedQuantization(torch.Tensor)` will be implemented with torch.uint4 Tensors (see https://github.com/pytorchlabs/ao/pull/13 as an example) Test Plan: CIs python test/test_quantization.py k test_uint1_7_dtype  Reviewers: Subscribers: Tasks: Tags:",2024-01-11T03:01:10Z,Merged ciflow/trunk release notes: quantization,closed,0,8,https://github.com/pytorch/pytorch/issues/117208,"Should there also be opaque dtypes for 2bit/3bit/4bit subbytes? I think, the quantization efforts now go even there... Regarding 1bit, there also exists ""Onebit Adam"" (I think even implemented somewhere in Meta experimental optimizer repos) which use the 1bit  can be a good showcase for uint1 or for bitmap/bittensor dtype in the related:  https://github.com/pytorch/ao/issues/292 One nasty thing is that if you google uint1 or uint4, 1byte and 4byte exotic namings come out: https://people.montefiore.uliege.be/boigelot/research/lash/man/uint.html which is not very nice... What is the presupposed indexing semantics for these subbyte types? (and if some standardized indexing is supposed at all?) e.g. uint1tensor[3] should retrieve the 3rd bit of the first byte? or the third encompassing byte?","> Should there also be opaque dtypes for 2bit/3bit/4bit subbytes? yeah we have these in the PR, this PR adds all dtypes from uint1 to uint7 > One nasty thing is that if you google uint1 or uint4, 1byte and 4byte exotic namings come out: people.montefiore.uliege.be/boigelot/research/lash/man/uint.html which is not very nice... do you mean the lashdtype naming? > What is the presupposed indexing semantics for these subbyte types? (and if some standardized indexing is supposed at all?) e.g. uint1tensor[3] should retrieve the 3rd bit of the first byte? or the third encompassing byte? I don't think we want to support sub byte indexing or nonbyte aligned indexing. but we can support byte aligned indexing by unpacking, see: https://github.com/pytorchlabs/ao/pull/13/filesdiff109a7f01577eb57b0d9facb5e1c17c23158f544b7203cda513075487a389b2f6R160R165","Yeah, the indexing / virtual vs actual byte shape is important for the usecase like BitTensor/BitMap where semantically they want to be like compressed BoolTensor Regarding the naming, I mean that uint1/uint4 probably would google badly, as currently there exist some trash mentions in other contexts where 1/4 stand for byte and not bit, and uint4_t does not exist in C context. Regarding the uint1, I would also suggest to have some alias or a subclass in core like torch.bit or torch.bitmap or torch.bitset or similar which suggest also a higherlevel usage  the compressing BoolTensor might a relatively frequent highlevel usecase, also with pack/unpack and RoaringBitmaplike ops, and then maybe some classical morphological / binary image processing ops, also maybe with some LSH/hashing ops  ","> Regarding the naming, I mean that uint1/uint4 probably would google badly, as currently there exist some trash mentions in other contexts where 1/4 stand for byte and not bit, and uint4_t does not exist in C context. >  I think uint1 to uint7 is consistent with C/C++ naming of uint8, uint16, uint32 dtypes > Regarding the uint1, I would also suggest to have some alias or a subclass in core like torch.bit or torch.bitmap or torch.bitset or similar which suggest also a higherlevel usage  the compressing BoolTensor might a relatively frequent highlevel usecase, also with pack/unpack and RoaringBitmaplike ops, and then maybe some classical morphological / binary image processing ops, also maybe with some LSH/hashing ops can you write some quick code example of what you want to do (what is higherlevel usage)? I think these may be built in tensor subclass in general","> Yeah, the indexing / virtual vs actual byte shape is important for the usecase like BitTensor/BitMap where semantically they want to be like compressed BoolTensor For nonaligned indexing, I think the most plausible implementation strategy in this direction is an introduction of a 1/8 SymInt (similar to SingletonSymNode we use to represent ragged dimension). Then, if I have a 8 element uint1 tensor, the storage offset of the 1index element is 1/8, 2index is 2/8, and so forth. TBH,  and co are not that interested in the packed bool tensor use case, so someone else is probably going to have to implement it. > Regarding the uint1, I would also suggest to have some alias or a subclass in core like torch.bit or torch.bitmap or torch.bitset or similar which suggest also a higherlevel usage  the compressing BoolTensor might a relatively frequent highlevel usecase, also with pack/unpack and RoaringBitmaplike ops, and then maybe some classical morphological / binary image processing ops, also maybe with some LSH/hashing ops My vote is for torch.bit","> what is higherlevel usage e.g. as   pack/unpack for compressing BoolTensor  set ops (RoaringBitmaps style)  binary image processing / local binary patterns,   Hadamard matrices, Haar wavelet basis matrices  probably not worth saving space on the analysis matrices, but in general representing 0/1 or 1/1 or 1/0/1matrices efficiently can be cool (which consist only of 1/1 or 1/0/1  although stretching to 1 is maybe taking it too far (e.g. b'1' could mean 1s and b'0' could mean 1s=absence of 1| and for 2bit case b'01' could mean 1s, b'00' could mean 0s and b'11' could mean 1s)  :)) etc :)  Further on  maybe for some binary neural nets in the future  intersecting/competing with 1bit quantiztion in some limit extreme cases I guess", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,"WSL2 RTX A6000 , CUDA out of memory."," üêõ Describe the bug I finetuned and inferred Qwen14BChat using LLaMA Factory. I constantly encounter outofmemory issues in WSL2, and it can only run in a Windows environment. The following is the screen output during inference: (base) wxyFEGFEAL:/mnt/d/LLaMAFactory$ conda activate llmfactory (llmfactory) wxyFEGFEAL:/mnt/d/LLaMAFactory$ sh web_demo_qwen.sh [INFO trainable%: 0.0000 01/11/2024 08:51:34  INFO  llmtuner.model.loader  This IS expected that the trainable params is 0 if you are using model for inference only. Traceback (most recent call last):   File ""/mnt/d/LLaMAFactory/src/web_demo.py"", line 11, in      main()   File ""/mnt/d/LLaMAFactory/src/web_demo.py"", line 5, in main     demo = create_web_demo()            ^^^^^^^^^^^^^^^^^   File ""/mnt/d/LLaMAFactory/src/llmtuner/webui/interface.py"", line 59, in create_web_demo     engine = Engine(pure_chat=True)              ^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/d/LLaMAFactory/src/llmtuner/webui/engine.py"", line 20, in __init__     self.chatter = WebChatModel(manager=self.manager, demo_mode=demo_mode, lazy_init=(not pure_chat))                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/d/LLaMAFactory/src/llmtuner/webui/chatter.py"", line 30, in __init__     super().__init__()   File ""/mnt/d/LLaMAFactory/src/llmtuner/chat/chat_model.py"", line 31, in __init__     self.model = dispatch_model(self.model)                  ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/mnt/d/LLaMAFactory/src/llmtuner/model/utils.py"", line 46, in dispatch_model     return model.to(device=get_current_device())            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/hom",2024-01-11T01:21:01Z,module: windows module: cuda module: memory usage triaged,open,0,0,https://github.com/pytorch/pytorch/issues/117197
transformer,DISABLED test_gradgrad_nn_Transformer_cuda_float64 (__main__.TestModuleCUDA),Platforms: linux This test was disabled because it is failing on main branch (recent examples). The test is too slow and is causing the test suite to timeout running in slow gradcheck mode https://hud.pytorch.org/pytorch/pytorch/commit/8bcdde5058658cc193c94a7f1eb16660553dc35a ,2024-01-10T19:57:25Z,module: autograd triaged skipped actionable,open,0,2,https://github.com/pytorch/pytorch/issues/117140,,"This test has a lot of samples (32) and for each sample, the number of input elements is (48  63). We should disable slow gradcheck for this test."
transformer,Skip test_modules.py::TestModuleCUDA::test_gradgrad_nn_Transformer_cuda_float64,Takes 30+ minutes while doing slow grad check,2024-01-10T17:18:09Z,topic: not user facing ciflow/slow,closed,0,0,https://github.com/pytorch/pytorch/issues/117123
transformer,_dynamo fails with abc.__file__," üêõ Describe the bug Hello, I was working on deploying a Huggingface model to a Vertica database instance and stumbled across the following error: Exception: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback): Failed to import transformers.integrations.peft because of the following error (look up to see its traceback): module 'abc' has no attribute '__file__' Traceback: Traceback (most recent call last):   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/transformers/utils/import_utils.py"", line 1382, in _get_module     return importlib.import_module(""."" + module_name, self.__name__)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/vertica/oss/python3/lib/python3.11/importlib/__init__.py"", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File """", line 1206, in _gcd_import   File """", line 1178, in _find_and_load   File """", line 1149, in _find_and_load_unlocked   File """", line 690, in _load_unlocked   File """", line 940, in exec_module   File """", line 241, in _call_with_frames_removed   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/transformers/integrations/peft.py"", line 29, in      from accelerate import dispatch_model   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/accelerate/__init__.py"", line 3, in      from .accelerator import Accelerator   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/accelerate/accelerator.py"", line 35, in      from .checkpointing import load_accelerator_state,",2024-01-10T14:00:31Z,triaged oncall: pt2 module: dynamo dynamo-must-fix,closed,0,0,https://github.com/pytorch/pytorch/issues/117109
transformer,torch.compile leads to OOM with different prompts.," üêõ Describe the bug Given a transformer language model compiled with:  where the bulk of the task is computing next token logits for different prompts (MMLU), memory usage grows until reaching OOM. Without `torch.compile`, there is no OOM. I think this happens because different prompts are different lengths, so `torch.compile` records different paths for each different size, which eventually leads to an OOM. It's a Llama2 model with a static kvcache (the model from https://github.com/pytorchlabs/gptfast/blob/main/model.py). I call `model.setup_caches(max_batch_size=1, max_seq_length=model.config.block_size)` so the caches are not being adjusted ever. Is there a way to set up the prompt to always be `model.config.block_size` tokens long, then mask out irrelevant tokens, so that there is only one path through model.forward? Or should I avoid torch.compile and setup batching to achieve a speedup?  Error logs N/A  Minified repro N/A  Versions Collecting environment information... PyTorch version: 2.3.0.dev20240109+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.11.1 (main, Mar  8 2023, 10:58:11) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0169genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 GPU 4: NVIDIA RTX A",2024-01-10T13:28:11Z,needs reproduction module: memory usage triaged oncall: pt2,closed,0,7,https://github.com/pytorch/pytorch/issues/117106,Does it OOM without reduceoverhead?,Yep.,Do you have a runnable repro script that shows the OOM?,"Which Llama2 model are you using, and which command did you run gptfast with?  I see that your system has multiple RTX A6000s, you can also try enabling tensor parallel: https://github.com/pytorchlabs/gptfast/?tab=readmeovfiletensorparallelism1. > Is there a way to set up the prompt to always be model.config.block_size tokens long, then mask out irrelevant tokens, so that there is only one path through model.forward? Or should I avoid torch.compile and setup batching to achieve a speedup? This might be best answered on the gptfast repo.","Looks like we missed marking this as triaged, so doing that now."," I'm helping the team scrub old issues and trying to come up with a repro here, but have so far failed (with gptfast). Is this still an issue for you? If so, could you provide the command?","I'm going to assume this is fixed, but please reopen if it's still an issue."
llava,Semi-Structured Sparsity Causing Memory Overflow and Unsupported Operations in LLaVA Model Inference on A100 GPU," Issue description I am following the method of this blog Accelerating Generative AI with PyTorch: Segment Anything, Fast to accelerate LLaVA  inference. When I experimented with the Semistructured sparsity solution, I encountered a problem. First, when I apply semistructured sparse to the entire model, the memory explodes (cuda out of memory). But my A100 has 40960 MiB of video memory, which logically shouldn't be the case. LLaVA is composed of Llama + CLIP, and the model structure is as follows.  So I performed semistructured sparse on vision_tower alone, but encountered another error. !image The cause of this error has not been found for the time being, so I chose to only perform semistructured sparse on the first layer of Llama, that is, apply_sparse(model.model.layers[0]). It did not burst the video memory, but the following error was reported. !image In the Llama model structure of LLaVA, the Linear layer of the attention layer does not contain bias. It is speculated that the above error may be caused by the lack of bias. In addition, in the PyTorch documentation, semistructured sparse only supports the following operations, and bias is also required in Linear. !image https://pytorch.org/docs/stable/sparse.htmlsparsesemistructuredtensors In LLaVA, except Vision Tower, only the Linear of mm_projector contains bias. Sure enough, `apply_sparse(models.model.mm_projector)` succeeded, but the inference speed did not increase. Performing semistructured sparse on mm_projector, memory usage: 28194 MiB, timeconsuming: 149.9 ms (average of 200 pictures). Without semistructured sparse mm_projector, memory usage: 28034 MiB, time consumption: 15",2024-01-10T10:26:01Z,module: sparse triaged,open,0,10,https://github.com/pytorch/pytorch/issues/117101, ,Can you provide the exact code that you've encountered problems with?,"> Can you provide the exact code that you've encountered problems with? Sorry, my code is on the server and cannot be copied. I can provide a screenshot and a description. !image Among them, from load_pretrained_model function from https://github.com/haotianliu/llava?tab=readmeifquickstartwithHuggingface other code ","It's hard to reason about the issues without the code being available. As far as use of GPU memory during the conversion to sparse semistructured concerned:  Yes, some steps of conversion require significant amount of GPU memory (up to several times the size of the original matrix), but this memory is released as soon as conversion completed.  So if conversion performed as in `apply_sparse()` method you provided, with dense weight matrices being replaced with sparse semistructured ones, then the GPU memory used should not be a problem, and it should be actually smaller after the conversion of all weight matrices completed.  However, if references to original weight matrices kept somewhere else, then the memory usage will indeed increase along the way, and could exceed the amount of memory available on your GPU. As far as problem reported by `two_four_sgemm_cutlass` method, it is just probably as written in the error message, i.e. that the number of inputs provided is not divisible by 8.  You may wish to try a nightly build of PyTorch, as there were some relevant changes in the meantime. As far as problem with `bias` argument not supplied concerned, you can just pass `None` for the bias value.","Yes the nightlies have added padding support ( CC(Add padding support for dense matrices in (semistructured sparse ) matmul)), which should fix the `two_four_sgemm_cutlass` issue. Can you try rerunning your code with them and seeing if the error goes away  ? ",": Shall we change the `torch.ops.aten.linear.default` operator implementation in `semi_structured.py`, so that it checks for number of arguments and set `bias` to `None` if corresponding argument not supplied?","To solve the problem of no bias, I set the bias to None by modifying the source code, but after the modification, the error ""Number of columns of dense matrix must be divisible by 8"" appeared again. Continue to follow the reply, saying that the PyTorch nightly version can be automatically filled in. After I upgraded to torch2.3.0.dev20240111+cu118cp39cp39linux_x86_64, since Flash Attention only supports PyTorch 2.2, I cannot continue the experiment. What is strange is this error. Each Linear layer in LLaVA, whether in_features or out_features, is a multiple of 8, but why is the error must be divisible by 8 reported?","Again, it's hard to tell without a reproducer...  The code first checks the divisibility (by 8 in this case), and then reports an error if the divisibility condition is not satisfied, so if an error is reported then probably the condition is not satisfied.  Would it be possible for you to find `semi_structured.py` file in your PyTorch installation, and then before each `torch._sparse_semi_structured_linear()` call, to put a `print()` call to print the input tensor shape (for example, for the first instance of this call, the print statement would be `print(input_A.shape)`, and so on)?  This way, we could see actual sizes of tensors that get checked.","> : Shall we change the torch.ops.aten.linear.default operator implementation in semi_structured.py, so that it checks for number of arguments and set bias to None if corresponding argument not supplied? Yes, let's add a test for this at the very least, I think it's possible the changes I added in 2.2 might fix this but I'm not sure.  > What is strange is this error. Each Linear layer in LLaVA, whether in_features or out_features, is a multiple of 8, but why is the error must be divisible by 8 reported? Can you try with 2.2 then? I think that the padding changes have made it into that branch cut. To answer your question, this is because there are shape constraints on **both** the dense and sparse matrix when doing sparse @ dense matmul. In this case, it does not matter if in_features and out_features are both multiples of 8 if the dense matmul shapes are not (which is what is going on here as far as I can tell, you would expect to see a different error if the sparse matrix had the wrong shapes).","> > : Shall we change the torch.ops.aten.linear.default operator implementation in semi_structured.py, so that it checks for number of arguments and set bias to None if corresponding argument not supplied? >  > Yes, let's add a test for this at the very least, I think it's possible the changes I added in 2.2 might fix this but I'm not sure. >  Indeed  calling linear operator without specifying bias seems to be working fine now."
rag,Add function to materialize COW storages,"Summary: From Kurt Mohler, see https://github.com/pytorch/pytorch/pull/113396 (manually imported due to ghimport problems) Test Plan: sandcastle, OSS CI Differential Revision: D52610522 ",2024-01-09T19:19:55Z,fb-exported Merged ciflow/trunk release notes: mps ciflow/mps module: inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/117053,This pull request was **exported** from Phabricator. Differential Revision: D52610522," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Auto-applying labels if one-or-more Labels are applied?,"For the PT2 oncall, we have labels that are sublabels of each other. For example, the ""module: aotdispatch"" label is a sublabel of ""module: pt2dispatcher"". It would cool if if someone labels an issue as ""module: aotdispatch"", a bot comes along and automatically labels it as ""module: pt2dispatcher"" (and ""oncall: pt2""). /pytorchdevinfra  any ideas on how to go about doing this?",2024-01-09T18:44:24Z,triaged module: devx,closed,0,6,https://github.com/pytorch/pytorch/issues/117051,"For complex labeling logic, we have https://github.com/pytorch/testinfra/blob/main/torchci/lib/bot/autoLabelBot.ts to implement them.  One way to implement this IMO is to define the relationship among these labels on https://github.com/pytorch/pytorch/blob/main/.github/pytorchprobot.yml.  The bot can then read the file and attach necessary labels accordingly.",I also want to remove `not user facing` if any other topic labels is applied,Proposal: 1. we have some dictionary that represents a hierarchical label structure. For example:   2. logic that consumes this dictionary and determines which labels to apply.  was the suggestion to toss the dictionarylike hting into pytorchprobot.yml?,"Also, unassigning because I don't think either of us are actively working on this at the moment.", what is the state of creating this fancier labeling system? Should we do it by hand for the ones we need now and migrate once the relationship system is implemented?,"Done in https://github.com/pytorch/pytorch/pull/125042, feel free to add rules in the file added by this pr.  Only ""any"" and ""all"" are supported right now, please let us know if you want more specific stuff The topic not user facing is a bit more difficult, made a specific issue just for that https://github.com/pytorch/testinfra/issues/5144 so I don't get confused every time I see the title"
rag,Code Typo fix in code_coverage/readme.md,Closes CC(Wrong with code_coverage/readme.md),2024-01-09T15:59:38Z,triaged open source Stale topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/117040,/pytorchdevinfra is code_coverage actually used anywhere or should it be deleted?,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Docs: fix docstring errors in model_averaging,pydocstyle check averagers.py Pre  /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:1 at module level:         D100: Missing docstring in public module /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:20 in public method `__init__`:         D107: Missing docstring in __init__ /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:27 in public method `average_parameters`:         D102: Missing docstring in public method /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:84 in public method `__init__`:         D107: Missing docstring in __init__ /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:106 in public method `average_parameters`:         D205: 1 blank line required between summary line and description (found 0) /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:106 in public method `average_parameters`:         D400: First line should end with a period (not '`') 6 Post /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:1 at module level:         D100: Missing docstring in public module /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:20 in public method `__init__`:         D107: Missing docstring in __init__ /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:27 in public method `average_parameters`:         D102: Missing docstring in public method /workspaces/pytorch/torch/distributed/algorithms/model_averaging/averagers.py:84 in public method `__init__`:         D107: Missing docstring in _,2024-01-09T15:40:10Z,oncall: distributed triaged open source Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/117038," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.",Huang feel free to reassign to a proper owner if you are not the one, merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `Testing1` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout Testing1 && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llama,"[Dynamo, ONNX] Run llama attention with onnxrt and dynamic shapes","As title. This PR enables dynamic shapes for running llama with ORT. Both forward and backward are captured as a single graph with this PR. Summary of changes:  Test llama attention, llama decoder, llama model to ensure (1) no graph breaks (2) models exported with dynamic shapes with onnxrt dynamo backend  Reshape SymInt to tensor with shape (1,) to align with the cast done for int in fx_onnx_interpreter.py  Create an util function to map Python types (e.g., float) to ONNX tensor element type (e.g., onnx.TensorProto.FLOAT).  Return `hint` for torch.Sym* in type promotion pass.  Remove _replace_to_copy_with_to since exporter supports aten::_to_copy it now.  Modify _get_onnx_devices to return CPU device for torch.Sym*.  Introduce _adjust_scalar_from_fx_to_onnx (e.g., change 0 to tensor(0)) and _adjust_scalar_from_onnx_to_fx (e.g., change tensor(0) to 0) for adjusting scalars when passing values to and receive values from ORT.  Now, ValueInfoProto of graph inputs (i.e., input_value_infos) are stored and used as `ORTexpected type` when calling `_adjust_scalar_from_fx_to_onnx`.",2024-01-09T02:53:52Z,module: onnx triaged open source Merged onnx-triaged ciflow/trunk release notes: onnx,closed,0,3,https://github.com/pytorch/pytorch/issues/117009,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: wschin / name: WeiSheng Chin  (94539e1928d151d4dbbe4f130fa06dba2320f164, 49da336eddb320ff1159e2f9d87c0f452fb75945, 847838ee125e99cb07580b947708b46354d8cb11, d5f1ddb04b39b2a67a6765e149e8d597e44f7cdd, 6aec602a338f4e1306770b6edf9f9794c59c2529, 4b80cb7c9902e959dac0790c2f153fb8caf66cd7, 6df2c1346d9bff1fd055cec8e65e196cd5f73dfa, adce921600d3af1deee06792b122c353cd3a3dea, 78bccdc7affc96d56bd347413fd7cf57fcd80bee, a70388e9e5c261891cd3c19256f3d02f8b13ee9b, cfbfc0af42e000220f54a23617f3f05d1de2a163, 7c8db681de621a5fbb0c546631c5e85c8e0a7308, ae51ba265d5352de08ccc0ccb0e1f45b299e7983, e3b6b65fe763d1a11d73dedf15531b44d60f988a, 6ce3d283ed0ae84fd254cc56881784fe3f94da2f, 011092ef44b4dde55cbf1c0754ecba2698b9d60c, 9de24291e7b51987d7d07451c7e89dc9888f2004, 0b345300173ac74ec0aea6b31d00ac00fc035677, b9d1f3172a36f277ff28716d66cb3b9e92216f97, 0dec5b5dc22c8c0e0ccf1753eb9176db71ee39ad):white_check_mark: login: xadupre / name: Xavier Dupr√©  (f0f59db7d08df6fc7a49818a3ac398f7c921521e)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Stop unconditionally applying hermetic mode,"  CC(Stop unconditionally applying hermetic mode) When originally authored, it was not necessary to unconditionally apply hermetic mode, but I chose to apply it in eager mode to help catch bugs. Well, multipy is kind of dead, and hermetic mode is causing real implementation problems for people who want to do fancy Python stuff from the dispatcher.  So let's yank this mode for now. Signedoffby: Edward Z. Yang ",2024-01-08T22:46:59Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/116996, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Add warning when trying to use nccl version while not compiled with nccl,Trying to get nccl version on windows return an exception:  Here is error:  Test: ,2024-01-08T20:49:10Z,Stale with-ssh,closed,0,3,https://github.com/pytorch/pytorch/issues/116989," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.", Done. Applied your comment. Please review the PR again,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Wrapping multiple layers in Pytorch FSDP," üöÄ The feature, motivation and pitch Is it possible to wrap multiple nn.Modules (e.g., every two Transformer layers) with FSDP API to control communication and memory overlap at a more granular level?  cc:   ",2024-01-08T19:55:03Z,triaged module: fsdp,closed,2,10,https://github.com/pytorch/pytorch/issues/116986,"Thanks for the issue! This is a good ask. The TL;DR is that we probably cannot support it with existing FSDP, but we may be able to support it in our upcoming FSDP rewrite in the future ( CC([RFC] Per-Parameter-Sharding FSDP)). I will leave another comment with more indepth discussion on this a bit later. I am finishing up some other work.","Thanks  for the prompt response. Do you have a suggested approach for this? I guess I can wrap my layers in another nn.Module, but it feels very hacky. I really like the proposal you linked, particularly the support for Zero++ and  improved state_dicts! Super curious if you have time estimates there! I will try leave comments once I have more time to read it. "," The approach I have seen for advanced users that own the model code is what you said  wrap every `k` transformer blocks into a parent module that acts like a 'sequential' over its `k` children. It affects `state_dict` compatibility though (since the keys change), which may block its viability depending on your use case.","For timeline, we are goaling internally on having the rewrite as a prototype feature by the end of this half (so ~6 months). We should land core components in the next month or two. I will provide a more formal update on the issue later. The main immediate work items are around: (1) breaking up the implementation into PRs for review/landing, (2) formalizing the design for how to extend FSDP (e.g. fp8 allgather, QLoRA), and (3) landing custom kernels for fast allgather copyout/reducescatter copyin. ",Sounds good! Thanks again. ,"Sorry for the delay. Here are some additional thoughts: One way to address this issue is to allow `FSDP(List[Module])` instead of only `FSDP(Module)`. A concern is how will the user express/configure this. FSDP offers auto wrapping (`auto_wrap_policy`) as a syntactic sugar for manual wrapping (i.e. directly calling `FSDP` on each module). Auto wrapping applies FSDP using a given predicate (the wrap policy) following a depthfirst traversal over the root module. If we allow for `FSDP(List[Module])`, it is not obvious how:  Auto wrapping would accommodate this tractably since we need a traversal that, in the most general case, applies the predicate over every combination of modules (one option could be to consider every consecutive `k` modules, where ""consecutive"" follows `root_module.modules()` order)  Manual wrapping users would translate from a config file to actually passing in the `List[Module]` to FSDP Another way to address this is to expose some new API to allow for merging existing FSDP instances. This is also another option but also complicates the API and testing. We should be confident that such an API is necessary if we add this. (Finalizing the FSDP configuration at construction time helps with reasoning about it and debugging, which is one reason that we are not a big fan of rebucketing at runtime.)"," Do you have any preference on whether we allow `FSDP(List[Module])` vs. having a separate API like `FSDP.fuse(*modules)` to fuse already initialized FSDP modules? See https://github.com/pytorch/pytorch/pull/118584 for an example. I was leaning toward the latter (though it would introduce some more complexity internally to FSDP) because it allows the initial ""wrapping"" policy to be fixed while the fusing scheme could be tuned. Moreover, the separate API could mainly be targeting expert users without confusing general users as to why FSDP can take in a `List[Module]`.","> Another way to address this is to expose some new API to allow for merging existing FSDP instances. This is also another option but also complicates the API and testing. We should be confident that such an API is necessary if we add this. (Finalizing the FSDP configuration at construction time helps with reasoning about it and debugging, which is one reason that we are not a big fan of rebucketing at runtime.) Hello, Is there any updates? I would like to wrap multiple `T5Block` with single FSDP unit, but cannot find any solutions..","Hi ! We do not have this functionality yet, but I am experimenting with this again. I am leaning back toward allowing users to pass in a `List[nn.Module]` to FSDP, and we can wrap them together (e.g. your `T5Block`). The draft is in https://github.com/pytorch/pytorch/pull/127786.","> Hi ! We do not have this functionality yet, but I am experimenting with this again. >  > I am leaning back toward allowing users to pass in a `List[nn.Module]` to FSDP, and we can wrap them together (e.g. your `T5Block`). The draft is in CC([FSDP2] Allowed `List[nn.Module]` as arg). I see!, thanks for quick and kind reply."
transformer,Compiled Transformer AOT Autograd graph Accuracy Failed," üêõ Describe the bug This is a WIP version of https://github.com/lucidrains/xtransformers that has zero graph breaks. It converges as expected when not compiled, but when compiled the loss curve is much worse. Repro fails accuracy as expected with random data so I didn't upload the checkpoint. repro.py.txt Edit: Smaller repro file: https://github.com/pytorch/pytorch/files/13851673/repro.py.txt The above fails, but I cannot minify further due to:  The issue seems to go away (only tested for a minute or so) when I turn off 16bit mixed precision and only use fp32, but of course everything is half speed due to limited memory bandwidth.  Error logs [20240106 18:15:32,526] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph [20240106 18:15:46,128] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs [20240106 18:15:46,910] torch._dynamo.utils: [ERROR] Accuracy failed: allclose not within tol=0.001 [20240106 18:15:46,912] torch._dynamo.repro.after_aot: [WARNING] Accuracy failed for the AOT Autograd graph model__19_backward_57 [20240106 18:15:47,003] torch._dynamo.repro.after_aot: [WARNING] Writing checkpoint with 4892 nodes to themodel/torch_compile_debug/run_2024_01_06_18_15_47_002351pid_2494068/minifier/checkpoints/4892.py themodel/env/lib/python3.11/sitepackages/torch/_dynamo/eval_frame.py:412: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect Epoch 0/2 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/ 0:04:29 ‚Ä¢ :: 0.00it/s v_num: epro train/loss: 0",2024-01-06T23:29:26Z,high priority needs reproduction triaged ezyang's list oncall: pt2 module: inductor module: pt2-dispatcher,closed,0,64,https://github.com/pytorch/pytorch/issues/116935,The repro as uploaded fails for me with: ,I tried running the repro script above to confirm and it fails in eager mode too.  can you double check your repro?,> The repro as uploaded fails for me with: >  >  Weird. Which version of PyTorch are you running this with? I know they changed the Flash Attention implementation to 2 within the last few months. ,"I""m using a recent nightly  do you mind trying it out on a nightly as well? https://pytorch.org/getstarted/locally/","> I""m using a recent nightly  do you mind trying it out on a nightly as well? https://pytorch.org/getstarted/locally/ I tested with a recent nightly before I submitted this bug to make sure the issue hadn't been fixed already. I can test again with different attention heads/etc but can't today because plane internet can't seem to handle an SSH connection. Maybe find and replacing all the flash attention calls to the fallback would work?","Also, the model isn't using multi query attention or anything that would change the number of heads for the different matrices."," I looked at your repro in more detail. It looks like the minified repro was bungled: I extracted out the first ~300 lines of the repro, and some of the intermediate tensors start to produce inf/nan values. It's entirely possible that these nans/infs were initially caused due to a torch.compile bug, but given that the repro script itself produces nans/infs (even when run in eager mode), that repro script doesn't look like it'll be enough to root cause some more. repro pasted here (you can see where I print before/after the last op in the graph  there's an `aten.mm` call where the input is a `fp16` tensor with a very large value (37984), and running the fp16 matmul returns a tensor with infs in it): https://gist.github.com/bdhirsh/061727e285d3f5b106d1358170df077a (prints the below)  do you have a link to the original source code where you're seeing numeric differences? (preferably with repro instructions :) )",">  I looked at your repro in more detail. It looks like the minified repro was bungled: I extracted out the first ~300 lines of the repro, and some of the intermediate tensors start to produce inf/nan values. It's entirely possible that these nans/infs were initially caused due to a torch.compile bug, but given that the repro script itself produces nans/infs (even when run in eager mode), that repro script doesn't look like it'll be enough to root cause some more. >  > repro pasted here (you can see where I print before/after the last op in the graph  there's an `aten.mm` call where the input is a `fp16` tensor with a very large value (37984), and running the fp16 matmul returns a tensor with infs in it): >  > https://gist.github.com/bdhirsh/061727e285d3f5b106d1358170df077a (prints the below) >  >  >  > do you have a link to the original source code where you're seeing numeric differences? (preferably with repro instructions :) ) I could share the code (which is currently private but nothing too fancy/secret) but not the dataset. The weirdest thing about this process for me was I tried taking just the model and running a forward/backward pass with random inputs and for one iteration everything matched up between compiled/noncompiled. Also, during training the model performed significantly worse but the loss didn't go NaN or infinite as would be expected if what you mentioned were happening during training. What might have gone wrong with the repro and how should I resolve it? Could it have been using a cached compiled model from another PyTorch version or something? Also, could the fact that it's failing for eager and compiled imply it's actually a torch dynamo bug?",  repro.py.txt Here is another repro run from a completely fresh A100 instance running the latest nightly.," Further investigation implies the problem goes away when I disable PyTorch's Flash Attention (the accuracy checker spins for much longer... maybe forever?). This aligns with what I said before about how disabling mixed precision also fixed it (since FA 2 requires either FP16 or BF16 tensors so PyTorch's version prob falls back to no FA). EDIT: However, casting Q, K, and V to fp16 before sending them into the PyTorch attention function doesn't solve the accuracy issue. Here's are my modifications to the transformer I'm using: https://github.com/Sonauto/xtransformers You can funnel random data through it like this: ","Hmm. I installed your `xtransformers` package locally and ran that script, but I modified it slightly to set randomness state and compare the output between compile and eager:  This prints:  A delta of 1e6 doesn't seem... too bad. Is the latest snippet you posted enough to reproduce the fact that your model fails to converge? (Maybe if you use try to train your `ContinuousTransformerWrapper` over many iterations?)","Hmm... although when I run with `backend=""aot_eager""`, I get no difference:  While running with `compile(backend=""aot_eager_decomp_partition"")` gives me a small difference:  Which means that one of the inductor decomps is a possible culprit",Looks like disabling inductor's `native_layer_norm` decomp brings the difference from eager back to zero: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 Maybe we're forgetting to treat some intermediates in higher precision in the decomp,> Looks like disabling inductor's `native_layer_norm` decomp brings the difference from eager back to zero: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 >  > Maybe we're forgetting to treat some intermediates in higher precision in the decomp When I ran the little test script I sent you for one iteration I got the same results as you (also checked gradients but didn't include that). The divergence only gets crazy when training over many iterations. I inserted an intentional graph break (using unsupported context manager) in the call to the PyTorch Flash Attention function and the training no longer diverges (but the tradeoff is as many graph breaks as there are attention layers which makes the efficiencyobsessed part of me sad).," maybe just for sanity  do you think you could try running your E2E script while also commenting out this line from inductor (effectively removing the decomp), and seeing if that ""fixes"" the convergence issue? https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 That will at least tell us if this decomp is the root cause and needs a careful look",">  maybe just for sanity  do you think you could try running your E2E script while also commenting out this line from inductor (effectively removing the decomp), and seeing if that ""fixes"" the convergence issue? https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 >  > That will at least tell us if this decomp is the root cause and needs a careful look I commented this line, and removed the intentional graph break workaround I added earlier around PyTorch's Flash Attention and it still diverges. I ran `torch._dynamo.reset()` before staring the training script and instantiating the models to ensure it wasn't using a cached compiled version, but lmk if there are any other methods I should use to ensure it isn't using a cached model (which it shouldn't in this case anyway as this was a fresh A100 VM). I also ensured the comment was actually taking effect by printing before it was executed (so I didn't edit the wrong environment or something). Repro: repro.py.txt", I figured out how to reproduce it in one step with the above script! Needed to use AMP:  **note:** you must use commit db394f7 from my xtransformers fork. The later commit includes the workaround.,"Thanks for the updated repro  your right  this one gives a much larger difference. From some digging, it looks like this is still an issue with the `native_layer_norm` decomposition. When I comment it out in the inductor code, I no longer see an accuracy difference. I'll keep digging","> Thanks for the updated repro  your right  this one gives a much larger difference. >  >  >  > From some digging, it looks like this is still an issue with the `native_layer_norm` decomposition. When I comment it out in the inductor code, I no longer see an accuracy difference. I'll keep digging With native_layer_norm disabled do you still see a divergence in the gradient? I can double check myself within an hour, but it seems weird in this case that disabling the native_layer_norm decomposition didn't solve my divergence issue with the actual training run.","> Thanks for the updated repro  your right  this one gives a much larger difference. >  > From some digging, it looks like this is still an issue with the `native_layer_norm` decomposition. When I comment it out in the inductor code, I no longer see an accuracy difference. I'll keep digging I just tried to reproduce this fix myself and wasn't able to (which is a good thing as that means I'm probably just doing something wrong!). This is the only code segment in decomposition.py I changed:  Was I supposed to change something else?","Hmm nope you're right, I'm seeing that too. When I comment out the decomp, I now see good accuracy with `backend=""aot_eager_decomp_partition""`, but I still see an accuracy difference with the inductor backend. I'll keep looking around.",I don't yet understand the inner workings of inductor all that deeply but is there some way I can disable optimization of the flash attention call (since I suspect that's where the issue is based on its disappearance when I disable it) without creating a graph break? ,Cc  ,"Try not using the context manager and instead use these functions directly: https://pytorch.org/docs/stable/backends.htmltorch.backends.cuda.enable_flash_sdp And set this to false, I think that this doesn't cause graph breaks ","> Try not using the context manager and instead use these functions directly: >  > https://pytorch.org/docs/stable/backends.htmltorch.backends.cuda.enable_flash_sdp >  > And set this to false, I think that this doesn't cause graph breaks This would remove the graph break but losing flash attention is a much bigger perf/VRAM hit than the graph breaks. When I said ""disable optimization of the flash attention call"" I meant any further optimization Inductor is doing on top of the eagermode version that I suspect is breaking things, but not disabling it entirely. Thanks for the info anyway, though!", Has enough data been collected where I can set my repo private again?,TBH this seems to be more of a PT2 question rather than directly attributable to flashattention. Do you see numeric issues when in eager? Otherwise I think  might be best to answer here,"> TBH this seems to be more of a PT2 question rather than directly attributable to flashattention. Do you see numeric issues when in eager? Otherwise I think  might be best to answer here No *known* issues in eager because we're using it as the reference. Also I tried turning off flash attention in the FP16 minimal repro script I sent before and the 0.002 divergence still exists, which implies there's even more to this than I thought. The question then is why did my training runs start working when I put the context manager around SDPA and compiled? To summarize: 1. No compile AMP fp16: Trains fine 2. Compile fullgraph=True AMP fp16: Converges much more slowly, fails accuracy checker (talking about the builtin one that generates minified repros, not my repro script) 3. Compile fullgraph=False AMP fp16 with context manager around SDPA: Trains fine and **doesn't fail accuracy checker**, but the repro script I wrote earlier fails(??).","I've been staring at the previous repro the last couple days (why we're seeing `.02` delta between eager mode and inductor). Latest progress: I manually bisected the autocast rules and turned several of them off, and ~~I'm able avoid the numeric issue when I specifically disable the autocast rule for sdpa~~ (nvm, just disabling for attention still gives me the divergence  it might be multiple rules). Still digging  I'm looking into why exactly the FP16 impl for SDPA diverges with inductor. For anyone interested, here's the script I was using to disable various autocast rules from python: ","Hey , quick update: I'm no longer convinced that the above repro is enough to show training divergence. The repro is comparing the difference between compiled vs. eager with amp enabled. But to really know if compile is causing problems, we need to compare **both** eager and compile to a reference implementation with amp turned off. If `abs(compiled_amp  eager_fp64) >> abs(eager_amp  eager_fp64)`, then that would be a numerical issue with compile. But if both compiled AMP and eager AMP are off by roughly the same amount, then that would be expected behavior with compile (not guaranteed to be bitwise identical to eager, but promising not to be ""worse"" than eager compared to a reference implementation). With this updated patch:  I get:  (so compiled isn't ""worse"" than eager)"
transformer,"Mistral works on (2.0.1+cu117) but ""CUDA out of memory"" on (2.1.2+cu121)"," üêõ Describe the bug I pip upgraded torch from 2.0.1 to 2.1.2 and without any code changes I now run out of CUDA memory loading Mistral7B on NVIDIA GeForce RTX 3060 TI.  From 2.0.1 to 2.1.2. Did memory allocators or max_split_size_mb change? Does torch reserves more memory?   Any steps I can do to further debug this issue? From Traceback: I see failing to allocate 20MB and 205MB reserved by unused by PyTorch. Does 2.1.2 reserves more memory? ""this process has 17179869184.00 GiB memory"" is this GiB or bytes?   Traceback (most recent call last):   File ""/home/bpevangelista/projects/kfastml/test.py"", line 12, in      model = AutoModelForCausalLM.from_pretrained(   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/models/auto/auto_factory.py"", line 566, in from_pretrained     return model_class.from_pretrained(   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/modeling_utils.py"", line 3706, in from_pretrained     ) = cls._load_pretrained_model(   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/modeling_utils.py"", line 4091, in _load_pretrained_model     state_dict = load_state_dict(shard_file)   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/modeling_utils.py"", line 510, in load_state_dict     return safe_load_file(checkpoint_file)   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/safetensors/torch.py"", line 310, in load_file     result[k] = f.get_tensor(k)   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/torch/utils/_device.py"", line 77, in __torch_function__     return func(*args, **kwargs) torch.cuda.OutOfMemory",2024-01-06T18:54:36Z,high priority module: cuda module: memory usage triaged module: regression,open,0,11,https://github.com/pytorch/pytorch/issues/116928," how much memory does your 3060 has? (I have an old 2080 I can try this one on) Also, do you mind trying to use torch2.1.2+cu118 to see if this would work or exhibit the same OOM Also, can you try setting `PYTORCH_NO_CUDA_MEMORY_CACHING` environment variable to disable caching allocator (it can negatively affect the perf, but just curious)"," The 3060 has 8GB, I imagine there's some virtualmemory/paging happening as the model should have ~16GB? I tried what you asked, results:  torch2.1.2+cu118   Tried PYTORCH_NO_CUDA_MEMORY_CACHING, which asked me to also use CUDA_LAUNCH_BLOCKING.   Went back to 2.0.1 but with cu118 instead of cu117, so we can isolate out CUDA. "," facing the same error for the same specs, any resolution found? Is it the shortage of 8GB of memory that is causing this issue?", I reverted back to 2.0.1 for the mean time.,Is transformer version the same for your torch2.0 and torch2.1 setups?,I cannot reproduce the issue and see the expected OOMs in all releases when limiting the memory to 8GB. Running the script without using `torch.cuda.set_per_process_memory_fraction` shows a memory requirement of ~24GB:  using `torch.cuda.set_per_process_memory_fraction` shows:  in `2.1.2+cu121` and  in `2.0.1+cu118`.," The transformers version is the same, only libraries that changed are pytorch and triton.  If I force GPU memory to 8GB the sample I provided will not work on both. The 7B 16bparam model needs close to 15GB, plus KV cache and scratch memory. Correct me if I'm wrong but the way pytorch+cuda works is that you reserve memory and then mmap it to GPU. Thus, an 8GB GPU could see as much memory as my RAM (32GB). Below is my GPU memory on 2.0.1 showing 14.5GB on a 8GB GPU.   I saw some 3mo old changes on CUDA allocator related to pinning memory instead of mapping, it appears disabled by default but wondering if that was part of the issue.","> Correct me if I'm wrong but the way pytorch+cuda works is that you reserve memory and then mmap it to GPU. Thus, an 8GB GPU could see as much memory as my RAM (32GB). Below is my GPU memory on 2.0.1 showing 14.5GB on a 8GB GPU. No, PyTorch will use a caching mechanism to reuse memory, but will not offload GPU memory to the host by default via e.g. managed memory. Thus, I still don't understand how you can allocate more than 8GB on your device without changing the memory allocations. Are you also able to create a 14GB tensor on this device? ","  Sorry, I forgot to reply. Yes, I can allocate a 14GB tensor on GPU ""cuda:0"" and I can allocate 14x1GB Tensors as well.  On both cases, Torch shows ~14GB reserved GPU memory while nvidiasmi would show close to 8BG/8GB memory. Torch does appear to fail to allocate close to 3x my VRAMsize. Thanks","Same issue, with WSL & RTX 3060 Ti, Pytorch  2.1.2 doesn't work (same error) and 2.0.1 works.",I have the same error with PyTorch 2.1.2.
transformer,torch.compile fullgraph=True is failing for GPTJ model for toy_backend," üêõ Describe the bug  Error Unsupported                               Traceback (most recent call last) [](https://localhost:8080/) in ()      21 torch._dynamo.reset()      22 fn = torch.compile(backend=toy_backend, dynamic=True,fullgraph=True)(model.generate) > 23 out = fn(input_ids, do_sample=True, temperature=0.9, max_length=200)      24       25  out = fn(**inputs) 60 frames /usr/local/lib/python3.10/distpackages/torch/_dynamo/exc.py in unimplemented(msg)     170 def unimplemented(msg: str):     171     assert msg != os.environ.get(""BREAK"", False) > 172     raise Unsupported(msg)     173      174  Unsupported: call_function BuiltinVariable(str) [UserFunctionVariable()] {} from user code:    File ""/usr/local/lib/python3.10/distpackages/torch/_dynamo/external_utils.py"", line 17, in inner     return fn(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py"", line 115, in decorate_context     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/transformers/generation/utils.py"", line 1460, in generate     self._validate_model_class()   File ""/usr/local/lib/python3.10/distpackages/transformers/generation/utils.py"", line 1193, in _validate_model_class     if not self.can_generate():   File ""/usr/local/lib/python3.10/distpackages/transformers/modeling_utils.py"", line 1244, in can_generate     if ""GenerationMixin"" in str(cls.prepare_inputs_for_generation) and ""GenerationMixin"" in str(cls.generate): Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information You can suppress this exception and fall back to eager by setting:     import torch._dynamo     torch._dynamo.config.",2024-01-05T04:47:35Z,feature triaged oncall: pt2 module: dynamo dynamo-triage-june2024,closed,0,7,https://github.com/pytorch/pytorch/issues/116835,we can in principle support this but lol this code haha ( ),"Haha, what is the funny about this code, may I know ü§£ Get Outlook for Android ________________________________ From: Edward Z. Yang ***@***.***> Sent: Saturday, January 6, 2024 8:58:42 AM To: pytorch/pytorch ***@***.***> Cc: Tirupathi Rao Baggu ***@***.***>; Author ***@***.***> Subject: Re: [pytorch/pytorch] torch.compile fullgraph=True is failing for GPTJ model for toy_backend (Issue CC(torch.compile fullgraph=True is failing for GPTJ model for toy_backend)) Caution: This email originated from outside of the organization. Please take care when clicking links or opening attachments. When in doubt, contact your IT Department we can in principle support this but lol this code haha ( ) ‚Äî Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***>",Just that the code appears highly dynamic and will likely stretch the capabilities of our JIT :P ,"Hoo ,FYI same code with full graph=False ,works fine both toybackend and inductor backend, here the dynamo splitting  big graph into 13 subgraphs, I think each subgrqph execute and send output back to CPU then it execute the next graph Get Outlook for Android ________________________________ From: Michael Suo ***@***.***> Sent: Sunday, January 7, 2024 9:14:12 AM To: pytorch/pytorch ***@***.***> Cc: Tirupathi Rao Baggu ***@***.***>; Author ***@***.***> Subject: Re: [pytorch/pytorch] torch.compile fullgraph=True is failing for GPTJ model for toy_backend (Issue CC(torch.compile fullgraph=True is failing for GPTJ model for toy_backend)) Caution: This email originated from outside of the organization. Please take care when clicking links or opening attachments. When in doubt, contact your IT Department Just that the code appears highly dynamic and will likely stretch the capabilities of our JIT :P ‚Äî Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***>","Specifically, what is funny is stringifying a... method I guess? And then doing a string submatch on it. It's the sort of thing you'd expect to work if you `make_fx` but it's a lot of trouble for Dynamo because we need to identify that all of this stuff can be constant folded away.",Assigning to myself. Failing with  ,"The issue mentioned in the  original post is fixed on main. There are two graph breaks not under the scope of this issue 1) Graph break on copy.deepcopy (covered at  CC([dynamo] Graph breaks from copy.deepcopy)) 2) Graph break on tensor.item In general, tensor.item is unavoidable because `generate` is very data dependent function. Closing this for now."
yi,Fix test_decomp test for ops with py_impl(CompositeImplicitAutograd),  CC(Fix test_decomp test for ops with py_impl(CompositeImplicitAutograd)),2024-01-05T03:14:08Z,open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/116832,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",The second part of the test added here resulted in  any ideas on how to fix this?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Enables private_use_one lazy_init by PrivateUse1HooksInterface,"Fixes CC(In the func Tensor.to, how can I make privateuse lazy init) ) I want to use `PrivateUse1HooksInterface` to implement lazy_init for `PrivateUse1`. In addition, my team found that `torch.load` without `lazy_init ` will also results in the same error because of follow codes:  https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL319L321 https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL334L335 ",2024-01-05T01:42:50Z,triaged open source module: backend ciflow/trunk topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/116826," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.", , merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `lazyinit` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout lazyinit && git pull rebase`)"," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch commit author=""feifan "" m Enables private_use_one lazy_init by PrivateUse1HooksInterface ( CC(Enables private_use_one lazy_init by PrivateUse1HooksInterface)) Fixes CC(In the func Tensor.to, how can I make privateuse lazy init) ) I want to use `PrivateUse1HooksInterface` to implement lazy_init for `PrivateUse1`. In addition, my team found that `torch.load` without `lazy_init ` will also results in the same error because of follow codes:  https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL319L321 https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL334L335 Pull Request resolved: https://github.com/pytorch/pytorch/pull/116826 Approved by: https://github.com/ezyang ` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job "
transformer,Fix TransformerEncoderLayer for bias=False,"Fixes  CC(TransformerEncoderLayer raise `AttributeError: 'NoneType' object has no attribute 'device'` when `bias=False, batch_first=True`) Don't call `torch._transformer_encoder_layer_fwd` when `bias=False` `bias=False` was not something that `torch._transformer_encoder_layer_fwd`  was meant to work with, it was my bad that this wasn't tested as I approved https://github.com/pytorch/pytorch/pull/101687. `bias=False` was causing the `tensor_args` in `TransformerEncoder`/`TransformerEncoderLayer` to contain `None`s and error on checks for the fastpath like `t.requires_grad for t in tensor_args`. Alternative fix would be to 1) Pass `torch.zeros_like({*}.weight)` to the kernel when `bias=False` and filter `tensor_args` as appropriate 2) Fix `torch._transformer_encoder_layer_fwd` to take `Optional` for biases and fix the kernels as appropriate Let me know if these approaches are preferable   CC(Fix TransformerEncoderLayer for bias=False)",2024-01-04T04:15:28Z,Merged ciflow/trunk release notes: nn topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/116760, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/mikaylagawarecki/172/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/7405594863, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,support wrapper tensor to shard local_tensor storage,"Fixes CC(DTensor tensor subwrapper not be handle with apex) hi , i fix this with given the wrapper tensor the data_ptr to share it's own storage, then it can be call in any custom op ",2024-01-04T03:01:43Z,oncall: distributed triaged open source Stale ciflow/inductor release notes: distributed (dtensor),closed,0,4,https://github.com/pytorch/pytorch/issues/116757,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: JsBlueCat / name: Cery D  (4a7429c9d5c33bb3def21c165fe885c247e0fd03, e5d58e607c2c88b96845a0041ee0c929fc86fb2b, 0c5e3cf9d8c4bde3b31661c7e8f3f45725c67ae2, 2f1206def29c3ca76c65485ddc36deca55369ddd, 29f8704ff22037176719bd803ef3f9d140f03257, 71cde9413437c11dadfa5c8475e061855eb59550, 5bc12cab32dc0c603efb4fa51cd6f6f2db955656, 7c9634412f05fa20ef2394f91cfbfe1791102824)"," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","  hi, can you help, why the storage change the meta device? origin code is also use the given device, but it not throw error ...","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[codemod] markDynamoStrictTest test_transformers,  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest test_sort_and_select)  CC([codemod] markDynamoStrictTest test_stateless)  CC([codemod] markDynamoStrictTest test_subclass)  CC([codemod] markDynamoStrictTest test_tensor_creation_ops)  CC([codemod] markDynamoStrictTest test_tensorboard)  CC([codemod] markDynamoStrictTest test_testing)  CC([codemod] markDynamoStrictTest test_transformers)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest torch_np/numpy_tests/core/test_scalarmath)  CC([codemod] markDynamoStrictTest torch_np/numpy_tests/core/test_shape_base),2024-01-03T23:49:45Z,Merged topic: not user facing keep-going,closed,0,1,https://github.com/pytorch/pytorch/issues/116735,"hi,  . Do you have a quick method to list all UT of `TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu`? I would like to do the similar thing for `TestSDPACPU.test_scaled_dot_product_fused_attention_mask_vs_math_cpu` to remerge https://github.com/pytorch/pytorch/pull/115913."
rag,(wip)[inductor] Support SliceView in as_storage_and_layout,Differential Revision: D52446781 ,2024-01-03T23:40:18Z,Stale ciflow/trunk module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/116727,This pull request was **exported** from Phabricator. Differential Revision: D52446781," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", is this PR obsolete now?
llm,[export] Add val to call_module unflattened nodes,Fixes  CC(torch.export.unflatten loses node.meta['val']),2024-01-03T22:28:23Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/116720
transformer,Decomp various private upsample ops," üöÄ The feature, motivation and pitch Operators like `_upsample_bilinear2d_aa` and `_upsample_bicubic2d_aa` show up in export results ( CC(ONNX export error when exporting Vision Transformer model from PyTorch to ONNX format)), are not part of the core IR (https://pytorch.org/docs/master/ir.html), and do not have decomp implemented. It would be helpful to implement decomp for the various private upsample operators.  Alternatives _No response_  Additional context _No response_ ",2024-01-03T17:53:39Z,module: decompositions oncall: export,closed,0,1,https://github.com/pytorch/pytorch/issues/116706,Closed as some private ops are used as core ops. 
yi,[BE]: Improve typing to respect ruff PYI058,Tried out rule PYI058 and it flagged one typing recommendation in our codebase that would be better to fix.,2024-01-01T17:40:47Z,open source better-engineering Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/116588, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.8gcc11 / test (docs_test, 1, 1, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
mixtral,[Inductor] Decompose bmm if batch2's last dim size is 1 and coordinate_descent_tuning is enabled,We found this perf optimization opportunity at https://github.com/pytorchlabs/gptfast/pull/71. This would bring 5%+ perf gain for Mixtral 8x7B on gptfast. ,2024-01-01T01:06:46Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/116582, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
