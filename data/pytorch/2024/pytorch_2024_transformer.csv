transformer,Scaled Dot-Product Attention Invalid Configuration Error on Large batch size," Scaled DotProduct Attention Invalid Configuration Error on Large batch size  Summary The `torch.nn.functional.scaled_dot_product_attention` (sdpa) function is not working as expected when the batch size is large. It causes a `RuntimeError: CUDA error: invalid configuration argument`. This problem affects also the `torch.nn.TransformerEncoderLayer` as it relies on the `scaled_dot_product_attention` function.  Reproducing the error The following scripts are run with:  However, the same results can be obtained without setting `CUDA_LAUNCH_BLOCKING=1`.  SDPA Example This code will raise the error:    TransformerEncoderLayer Example This code will raise the error:    Cause of The Error I searched a little bit for the cause of the error. Here is what I found: The error is caused in file `pytorch/aten/src/ATen/native/transformers/cuda/attention.cu`. In particular the `launchKernel` lambda inside `_efficient_attention_forward` is responsible for the error. To my understanding this is the code snippet that triggers the invalid configuration:  It appears that, with a large batch size this kernel is run with a number of blocks that exceeds the maximum number of kernel blocks, `65,535`. This triggers the invalid configuration error. As a matter of fact, running the snippet  will not raise the error. Doing the same with a batch size of `65536` will raise the error. Note that also the number of heads contributes to the number of cuda blocks. So also an increased number of heads will trigger the error.   SDPA Workaround The error can be avoided by simply removing the number of heads dimension in the input tensor. For example,  Note that, the pytorch d",2024-12-06T10:54:13Z,,open,0,0,https://github.com/pytorch/pytorch/issues/142228
transformer,torch.export.export fails to export a model with dynamic shapes for a custom type, üêõ Describe the bug torch.export.export fails to keep a dynamic dimension. The script is using a custom class. Is it a bug or did I forget something when I registed the custom class?   Versions ,2024-12-05T19:27:45Z,,open,0,0,https://github.com/pytorch/pytorch/issues/142161
transformer,[export] NotImplementedError: No registered serialization name for <class 'diffusers.models.modeling_outputs.Transformer2DModelOutput'>, üêõ Describe the bug Flux model has been compiled using TorchTensorRT and I'm trying to export it using `torch.export.export` and see the following error   Here's the full script to reproduce the error   cc:    Versions torch.version '2.6.0.dev20241202+cu124' trt.version '10.6.0.post1' ,2024-12-04T01:54:18Z,oncall: pt2 oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/142025,"We need to go here and update their code to also include the serialized type name, similar to what I did here",Thanks  that got me forward and now it results in the  CC([export] Saving models over 4GiB requires pickle protocol 4 or higher ). Any suggestions on this ? 
transformer,[export] Saving models over 4GiB requires pickle protocol 4 or higher ," üêõ Describe the bug For large models compiled with TorchTensorRT, when we try to save them we get an error as follows   Two models have this issue flux schnell and laion2b_s34b_b82k_augreg_soup  from open_clip Is there a way to pass the pickle protocol to torch.export.save similar to torch.save ?  cc:   Here's a script to reproduce the flux error    Versions  torch.__version__ '2.6.0.dev20241202+cu124'  trt.__version__ '10.6.0.post1' ",2024-12-03T22:08:24Z,oncall: pt2 oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/142004, Any updates on this issue ?, See https://github.com/pytorch/pytorch/pull/142253
transformer,Forward from pytorch/executorch: export error with Macbook pro M4 ," üêõ Describe the bug Original Issue: https://github.com/pytorch/executorch/issues/7127  when I export .pte from Llama3.2 1B, it is enterrupt. the error is blow: python m examples.models.llama.export_llama checkpoint ""/Users/qitmac001443/.llama/checkpoints/Llama3.18B/consolidated.00.pth"" params ""/Users/qitmac001443/.llama/checkpoints/Llama3.18B/params.json"" kv use_sdpa_with_kv_cache X d bf16 metadata '{""get_bos_id"":128000, ""get_eos_ids"":[128009, 128001]}' output_name=""llama3_1.pte"" INFO:root:Applying quantizers: [] INFO:root:Loading model with checkpoint=/Users/qitmac001443/.llama/checkpoints/Llama3.18B/consolidated.00.pth, params=/Users/qitmac001443/.llama/checkpoints/Llama3.18B/params.json, use_kv_cache=True, weight_type=WeightType.LLAMA INFO:root:model.to torch.bfloat16 INFO:root:Loading custom ops library: /Users/qitmac001443/Desktop/workspace/executorch/.venv/lib/python3.12/sitepackages/executorch/extension/llm/custom_ops/libcustom_ops_aot_lib.dylib INFO:root:Model after source transforms: Transformer( (tok_embeddings): Embedding(128256, 4096) (layers): ModuleList( (031): 32 x TransformerBlock( (attention): Attention( (wq): Linear(in_features=4096, out_features=4096, bias=False) (wk): Linear(in_features=4096, out_features=1024, bias=False) (wv): Linear(in_features=4096, out_features=1024, bias=False) (wo): Linear(in_features=4096, out_features=4096, bias=False) (kv_cache): KVCache() (SDPA): SDPA( (kv_cache): KVCache() ) (apply_rotary_emb): RotaryEmbedding() ) (feed_forward): FeedForward( (w1): Linear(in_features=4096, out_features=14336, bias=False) (w2): Linear(in_features=14336, out_features=4096, bias=False) (w3): Linear(in_feature",2024-12-02T18:30:55Z,oncall: export,open,0,4,https://github.com/pytorch/pytorch/issues/141893,Forwarded here as Pytorch compiler asked me to do this if we got user export issues in executorch repo. ,Feels like it belongs in ET,Could they try using nonstrict?,"And providing the TORCH_LOGS=""+export"" with the original code? "
transformer,Can not convert Llama-3.2-11B-Vision-Instruct by torch.jit.script / torch.jit.trace," üêõ Describe the bug I am trying to convert the `*.pth` file to `*.pt` file for using libtorch to load the model in C++. But both `torch.jit.script` or `torch.jit.trace` will emit a not support type error. I encountered this error, but I'm not sure if I misused the function, if it's an unimplemented feature, or if it's an upstream issue. Thank you for your guidance.    Versions  ",2024-12-02T08:35:22Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/141855
transformer,running my facebook/bart-base for summarization task :  MPS does not support cumsum op with int64 input," üêõ Describe the bug ''' Summarization implementation ''' from domain.interfaces import SummarizationServiceInterface from transformers import AutoTokenizer, AutoModelForSeq2SeqLM import torch class SummarizationService(SummarizationServiceInterface):     """"""Summarizes text using a pretrained model.""""""     def __init__(self, model_name: str = ""facebook/bartbase"", device: str = ""mps""):         """"""         Initializes the summarization service.         Args:             model_name (str): Name of the pretrained model to load.             device (str): Device to run the model on ('cpu' or 'mps').         """"""         self.device = torch.device(device)         self.tokenizer = AutoTokenizer.from_pretrained(model_name)         self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)     def summarize(self, texts: list[str]) > str:         """"""         Summarizes a list of texts.         Args:             texts (list[str]): A list of text to summarize.         Returns:             str: The summarized text.         """"""         combined_text = "" "".join(texts)         inputs = self.tokenizer(             combined_text, max_length=1024, return_tensors=""pt"", truncation=True         ).to(self.device)         summary_ids = self.model.generate(             inputs[""input_ids""], max_length=150, min_length=40, num_beams=1, early_stopping=True         )         return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)         The error message is:         Summarizing Cluster 231 with 153 reviews... /Users/..../Code/clustersummreviews/venv/lib/python3.9/sitepackages/transformers/generation/configuration_utils.py:638: UserWarn",2024-11-29T01:13:41Z,triaged module: mps,open,0,1,https://github.com/pytorch/pytorch/issues/141786,"This is likely not a problem with PyTorch itself, as downstream is trying to use `cumsum` with `torch.int64`, which is not supported with the MPS backend. If the error originates in hugging face code I can contrib a fix in their code base. Can you please provide a minimal reproduction example that can be run topdown to get the same error that you got?  label ""module: mps"""
transformer,Lint: switch oncall owner for test_transformers,  CC(Lint: switch oncall owner for test_transformers),2024-11-27T21:09:28Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/141722," merge f ""Lint is green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,xpu: running Huggingface hiera with initializer_range=1e-10 renders Nan output,"With: * PyTorch: https://github.com/pytorch/pytorch/commit/f2d388eddd25b5ab5b909a8b4cbf20ff34c6f3f2 * https://github.com/intel/torchxpuops/commit/15f6d654685a98b6bbf6b76ee094258b16dd21ca * https://github.com/huggingface/transformers/commit/1f6b423f0ce0c6e7afd300b59e5fd1a816e4896c * https://github.com/huggingface/accelerate/commit/e11d3ceff3a49378796cdff5b466586d877d5c60 Running Huggingface `hiera` model with nondefault `initializer_range=1e10` (default is `0.02` and it works) on Pytorch XPU backend (I used Intel PVC platform) fails  output tensor contains `Nan` values. This behavior was first found running the following Huggingface Transformer tests for `hiera` model:  Instead of running Huggingface Transformers, it's possible to reproduce the issue on the following script which is my attempt to extract essential behavior from HF test:  Issue appears when `initializer_range=1e10`. That's what HF Transformers are doing in the tests (see tests/test_modeling_common.pyL149). Output on XPU will be:  And running with default `initializer_range=0,02` on XPU you will get reasonable results (named HF hiera tests will pass with this value, but other tests form hiera will start to fail):  **Note that CUDA does handle `initializer_range=1e10` in a different way** and does not return `Nan` tensors. However, what CUDA returns is suspiciously ""good"":  And for complete picture on CUDA, that's what it returns on `initializer_range=0,02`:  CC:        ",2024-11-27T01:27:25Z,triaged module: xpu,open,0,8,https://github.com/pytorch/pytorch/issues/141642,"My guesses for the issue: 1. XPU backend calculates somewhere with the lower precision than CUDA 1. CUDA has different handling for corner cases when calculations step into precision limits Considering that CUDA did return too good results for `initializer_range=1e10`, I think that 2nd variant might be the case.",deng is looking into this issue. ,Registering a hook to dump outputs from each layer:  First time `inf/nan` appear on one of the `LayerNorm` layers. After that they waterfall in calculations in the following layers. Below output is output of layers from the very first till the `LayerNorm` where `inf/nan` appear: ,"Ok, so this seems to be an issue with XPU implementation of `LayerNorm`. Slightly modifying the hook to dump layer outputs, it's easy to dump an input of the first `LayerNorm` which has nan/inf values. I did that on my side and uploaded captured input here: https://github.com/dvrogozh/pytorch/blob/for_141642/PT_141642.pt. After that using the following script you can reproduce the issue:  Output will be:  And modifying `device` in the script to be `cpu`, you will get:  So, outputs for LayerNorm operation are different between cpu and xpu with xpu having nan/inf in some values (not in all values).","> My guesses for the issue: >  > 1. XPU backend calculates somewhere with the lower precision than CUDA > 2. CUDA has different handling for corner cases when calculations step into precision limits >  > Considering that CUDA did return too good results for `initializer_range=1e10`, I think that 2nd variant might be the case. As for 1: Current implementation for Layernorm in XPU is on the top of the onepass algorithm instead of Welford algorithm, which may lead some precision drop.","Sync with  , we will try to implement the Welford algorithm and then check if it can help. Besides, For  's guess 2, we will check the implementation for precision handling in this case.  ","Thank you, deng, !  Do we know which algorithm is implemented for CPU and CUDA? Welford? Can we just convert CUDA kernel to SYCL kernel to get the implementation?","> Thank you, deng, ! >  > Do we know which algorithm is implemented for CPU and CUDA? Welford? >  > Can we just convert CUDA kernel to SYCL kernel to get the implementation? Welford is the common solution and our changes are currently in progress"
transformer,xpu: implement aten::_thnn_fused_lstm_cell for XPU backend,"The following aten operator is not currently implemented for XPU backend and does not allow fallback with `PYTORCH_ENABLE_XPU_FALLBACK=1`:  [ ] `aten::_thnn_fused_lstm_cell`, https://github.com/intel/torchxpuops/pull/926 With: * https://github.com/huggingface/transformers/commit/bdb29ff9f3b8030772bd4be037d061f253c0e928 * https://github.com/huggingface/accelerate/commit/e11d3ceff3a49378796cdff5b466586d877d5c60 * https://github.com/pytorch/pytorch/commit/f2d388eddd25b5ab5b909a8b4cbf20ff34c6f3f2 Can be reproduced running Huggingface Transformers `encodec` model tests:  Unfortunately that's one of those aten operators for which running with `PYTORCH_ENABLE_XPU_FALLBACK=1` does not help. If ran with it getting this error:  CC:      ",2024-11-26T02:25:49Z,triaged module: xpu,open,0,1,https://github.com/pytorch/pytorch/issues/141539,Related PR: https://github.com/intel/torchxpuops/pull/926
transformer,[export] A node has no users in the exported program," üêõ Describe the bug The following example produces a node with no user.  The graph contains the following line:  Version of 11/17 works, version of 11/21 fails.  Versions  ",2024-11-22T19:57:00Z,oncall: pt2 oncall: export,closed,0,6,https://github.com/pytorch/pytorch/issues/141373,"Removing the assert, the code runs for me with no errors. I had to change the call line ",But it does seem like `copy_` was no longer used.,"I took some time to investigate more, the line at stake is this one https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.pyL978. !image When I compare the two graphs, I see a dead end for this specific line. ","Here is another example not using transformers. The exported program returns ``clone`` when it should return ``copy_``. However, its execution returns the correct result but it is unexpected. ",The graph produces by 11/17 version is using slice_scatter. ,I'll close this issue. It seems the default behaviour for setitem is not to decompose it into slice_scatter anymore.
transformer,Updating from torch 2.0 to 2.1/2.2/2.3/2.4/2.5 results in gradient explosion with SDPA and bf16 ," üêõ Describe the bug We are training a huggingface transformer using pytorch native SDPA and the huggingface Trainer. We recently upgraded from torch 2.0, and found that **this change alone** caused our training runs to diverge.  We cannot upload any images and unfortunately are unable to share code since it is internal/proprietary. The training loss is almost identical across torch versions for over half the run and then they diverge. The diverges look similar to this issue  CC(CUDNN sdp attention causes loss explosion), however grad_norm can sometimes get up to values near 100. Here are some comparisons before and after different changes (all with grad clipping and max grad norm of 1.0): Torch 2.0 + bf16 + SDPA + lr=5e5 + gradient accum = 2> clean loss curve :white_check_mark: Torch **2.1/2.2/2.3/2.4/2.5** + bf16 + SDPA + lr=5e5 +  gradient accum = 2 > exploding gradients :x: Torch 2.4 + bf16 + SDPA + lr=5e5 + **gradient accum = 1** >  exploding gradients :x: Torch 2.4 +  **fp16** + SDPA + lr=5e5 + gradient accum = 2 > clean loss curve :white_check_mark: Torch 2.5 +  fp16 + SDPA + **lr=1e4** + gradient accum = 2 >exploding gradients :x: Torch 2.4 + bf16 + No SPDA (eager) + lr=5e5 + gradient accum = 2 > clean loss curve :white_check_mark:  We also tried what this issue) reported as a solution (`torch.backends.cuda.enable_cudnn_sdp(False)`) but this also resulted in exploding gradients. We are unsure whether this is an issue with SDPA after 2.0 or just finicky pretraining. Is there anything that changed post torch 2.0 that could be related to this issue? Seems odd that upgrading torch alone would cause this shift in behavior. Any help wou",2024-11-21T14:59:59Z,triaged module: sdpa,open,2,0,https://github.com/pytorch/pytorch/issues/141241
transformer,`CrossEntropyLoss` much slower with class probabilities vs. class indices target," üêõ Describe the bug Hi, I noticed this when training ViTS/16 on the ImageNet1k dataset, with and without MixUp. In short: torchvision's `v2.MixUp`, just like other common implementations, converts the class indices to onehot class probabilities before linearly interpolate them, so the same call to `CrossEntropyLoss()` goes to different kernels with vs. without MixUp. In a less optimized setting (specific commit), I found that with vs. without MixUp makes the difference between **0.198s vs. 0.120s** per step (excluding data loading) with batch size 1024 on a 8x A100SXM440GB Lambda instance. In further iterations, I included `CrossEntropyLoss` in the model so they compile together. I then tested a workaround in which the custom MixUp returns both targets and the weight scalar lam(bda):  and the model calculates the cross entropy loss by using the class indices twice:  See https://github.com/EIFY/mupvit/compare/normalprefetch...lossoptim. I found that this is faster (**0.184s** per step) than just calling `CrossEntropyLoss()` with class probabilities even with `torch.compile(...,mode=""maxautotunenocudagraphs"")` (**0.191s** per step), excluding data loading. Is this expected? Intuitively I can see that the sparsity helps forward & backward pass in loss calculation, but backward pass beyond that should be the same and it's hard to understand the observed second/step difference with a 12layer transformer. To replicate the timing result, you can run the code with fake data:  with either the normalprefetch or the lossoptim branch.  Versions The output below shows PyTorch version: 2.3.1 and torchvision==0.18.1, but I am actually running  I don't ",2024-11-20T22:32:58Z,module: nn module: loss triaged,open,1,0,https://github.com/pytorch/pytorch/issues/141177
transformer,"Cannot view a tensor with shape torch.Size([2, 1, 8, 32]) and strides (32, 512, 64, 1) as a tensor with shape (2, 256)! for transfomer MHA with permute, view"," üêõ Describe the bug When exporting my transformer model with torch.onnx.export(... dynamo=True, ...), any sequence length >= 2 gives the error. Sequence length 1 works but does not support dynamic shapes in inference. When dynamic_shapes is off, I get the above error, and when dynamic_shapes is set and seq_len >=2, I get an error about guard lengths similar to CC(torch.export.export() fails with dynamic shapes when more than one shape is dynamic). I tried latest nightly (9 PM PST on 11/19) but didn't work. Also noticed this issue is similar to CC([export] Cannot view a tensor with shape torch.Size([1, 512, 32, 128]) and strides (2097152, 128, 65536, 1) as a tensor with shape (1, 512, 4096)) and CC([export] `run_decomposition` fails for permute>view sequence). Thanks in advance for the help! onnx_export_20241119_210853216442_conversion.md **Repro**   Versions Collecting environment information... PyTorch version: 2.6.0.dev20241112+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 24.04.1 LTS (x86_64) GCC version: (Ubuntu 13.2.023ubuntu4) 13.2.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.39 Python version: 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.39 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU Nvidia driver version: 560.94 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available:",2024-11-20T05:16:40Z,needs reproduction module: onnx oncall: pt2 oncall: export,open,1,2,https://github.com/pytorch/pytorch/issues/141107, I think you may use this as a repro,
transformer,graph break when training LoRA," üêõ Describe the bug In torchtune, we offer LoRA training, which consists of freezing transformer layers and adding just a few matrices that are trainable. In this setting,  the input to the first transformer doesnt require gradients. However, the intput to the second transformer does. You can see the loop here. We compile each transformer block independently, to avoid long compilation times. Because of that, i get a graph break:   Is there a mark_dynamic equivalent for required_grad? tlparse: https://interncacheall.fbcdn.net/manifold/tlparse_reports/tree/logs/.tmptwNd6e/index.html To reproduce:    Error logs _No response_  Versions Tried it with both: 2.5.1 and 2.6.0.dev20241119+cu124 ",2024-11-19T16:34:06Z,oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/141039,"Oh  the recompile is because: (1) you are compiling two individual layers (2) on the first layer, your input `x` does not require_grad, and so we don't need to compute gradients for it (3) on the second layer, the input to that layer is an activation, which we **do** need to compute gradients for. We need to recompile for the first layer vs the second, because we need to do less compute in the backward of the first layer. This should cause at most one recompile though  is that a problem for you? Altenratively, you could try compiling both layers together into a single graph to avoid the recompiles","not really a problem, just figuring out ways to improve it. If there isnt an easy flag, then thats ok. Thank you for taking a look!"
transformer,xpu: implement aten::_linalg_eigvals for XPU backend (affecting HF Transformers v4.46.0 and later),"Recent changes in Huggingface Transformers (https://github.com/huggingface/transformers/commit/cdee5285cade176631f4f2ed3193a0ff57132d8b and https://github.com/huggingface/transformers/commit/4a3f1a686fcda27efc19b8b3a87b62a338c2ad86) introduced usage of `torch.linalg.eigvals()` which is not implemented for XPU backend (missing `aten::_linalg_eigvals`) and it's not registered for automated CPU fallback as well. To use this operator with XPU you need to use `PYTORCH_ENABLE_XPU_FALLBACK=1`. Transformers version v4.46.0 and later are affected. Can this operator, please, be added?  [ ] `aten::_linalg_eigvals` With: * https://github.com/huggingface/transformers/commit/e80a65ba4fbbf085fda2cf0fdb0e8d48785979c8 * https://github.com/huggingface/accelerate/commit/8ade23cc6aec7c3bd3d80fef6378cafaade75bbe * https://github.com/pytorch/pytorch/commit/e429a3b72e787ddcc26ee2ba177643c9177bab24 Can be reproduced running Huggingface Transformers tests for a number of models (such as albert, mpt, gemma2). Here is example for one of the model:  CC:      ",2024-11-18T18:50:50Z,triaged enhancement module: xpu,open,1,2,https://github.com/pytorch/pytorch/issues/140965,"Regarding the missed operation, we will support it on top of mkl library. Currently, the timeline is supposed to be PT 2.8 or PT 2.7.", : can we add `aten::_linalg_eigvals` for automated CPU fallback while we are waiting for the actual implementation of `aten::_linalg_eigvals` for XPU in PT2.7/2.8?
transformer,[export][quant] AttributeError: 'FunctionalTensor' object has no attribute '_quantized_linear_op'," üêõ Describe the bug I'm trying to export a model to use w8/a8 quantisation. In my example, I'm using BERT.  However running `torch.export` fails with `AttributeError: 'FunctionalTensor' object has no attribute '_quantized_linear_op'` A traceback is here: Traceback      My code is here:   Versions  ",2024-11-18T12:11:49Z,oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/140943
transformer,TypeError: Type parameter +RV without a default follows type parameter with a default in _inductor/utils.py," üêõ Describe the bug env:  python 3.9,  torch.__version__='2.5.1'  also observed in 2.4.1 when importing transformers and torch I get error triggered by typing problem in this line:  `class CachedMethod(Protocol, Generic[P, RV]):` https://github.com/pytorch/pytorch/blob/99014a297c179862af38ee86bac2051434d3db41/torch/_inductor/utils.pyL459C1L459C46 `TypeError: Type parameter +RV without a default follows type parameter with a default` suggegsted solution:  replace  `class CachedMethod(Protocol, Generic[P, RV]):` with `class CachedMethod(Protocol, Generic[RV, P]):` in https://github.com/pytorch/pytorch/blob/99014a297c179862af38ee86bac2051434d3db41/torch/_inductor/utils.pyL459C1L459C46 possibly related to https://github.com/pytorch/pytorch/pull/127685  ERROR STACK BELOW:   Versions python 3.9 torch.__version__='2.5.1' ",2024-11-17T21:23:06Z,module: typing triaged actionable oncall: pt2,open,3,1,https://github.com/pytorch/pytorch/issues/140914,"I also met this problem with PyThon 3.12 and PyTorch 2.5.1, and it led to a fatal error in my project. Hope it can be resolved soon ü§©."
transformer,xpu: CTCLossLogAlphaKernelFunctor hangs on HF hubert test_retain_grad_hidden_states_attentions,With: * (pytorch) https://github.com/pytorch/pytorch/commit/33191bb664fef338d94586a05bf1f14d17a00340 * https://github.com/huggingface/transformers/commit/a32c4e6c7b7fd79eb0c94c6c6ce9446de0877ded * https://github.com/huggingface/accelerate/commit/c0552c9012a9bae7f125e1df89cf9ee0b0d250fd Huggingface Transformers `test_retain_grad_hidden_states_attentions` test for the `hubert` model hangs. Can be reproduced with the below command line. Using https://github.com/intel/ptigpu/tree/master/tools/unitrace tool with combination of NEO driver flags we can see that `CTCLossLogAlphaKernelFunctor` kernel hangs:  Root cause is likely similar to root cause of CC(xpu: huggingface levit test_retain_grad_hidden_states_attentions test hangs on exit on PVC): `CTCLossLogAlphaKernelFunctor` kernel has early returns while barrier is used in the kernel. There are 2 return calls: * https://github.com/intel/torchxpuops/blob/b75da0cba01fadafca9df95b1df88f0ed24ed312/src/ATen/native/xpu/sycl/LossCTCKernels.cppL50 * https://github.com/intel/torchxpuops/blob/b75da0cba01fadafca9df95b1df88f0ed24ed312/src/ATen/native/xpu/sycl/LossCTCKernels.cppL58 And here is a barrier: * https://github.com/intel/torchxpuops/blob/b75da0cba01fadafca9df95b1df88f0ed24ed312/src/ATen/native/xpu/sycl/LossCTCKernels.cppL114 CC:       ,2024-11-15T03:29:03Z,triaged module: xpu,closed,0,3,https://github.com/pytorch/pytorch/issues/140781,We will update the torchxpuops to fix it., Shall we close this issue? As the relevant PR has been merged.,>  Shall we close this issue? As the relevant PR has been merged. Yes. This one can be closed since the change got propagated to PyTorch: * https://github.com/intel/torchxpuops/commit/f9c7682530f4259339b6f3cd4b24fa63518cfeb8  a fix in torchxpuops * https://github.com/pytorch/pytorch/commit/81ab2cc757bab1770433c97a713fd07b59a1d80f  update of torchxpuops commit pin in pytorch This fix trends to be available in PT 2.6.
transformer,`torch._transformer_encoder_layer_forward` usage of SDPA attention instead of native_MHA," üöÄ The feature, motivation and pitch Discussion about attention usage of: https://github.com/pytorch/pytorch/blob/02d0c43c3243f967856798fa8622a8727f9d36a3/aten/src/ATen/native/transformers/transformer.cppL109C1L122C19 It is not clear to me what kind of attention version is dispatched with _transformer_encoder_layer_forward (transformer encoder layer)  it seems that `at::_native_multi_head_attention` is used, which could potentially open up a optimization opportunity in case this not the SDPA backend. I could not find the code for `at::_native_multi_head_attention` in the Github repo. Potentially, triton_multi_head_attention dispatches SDPA or multi_head_attention_cuda, which make use of a fused attention: https://github.com/pytorch/pytorch/blob/217d328764be9a8a4c5f846f762e83100a595905/aten/src/ATen/native/transformers/attention.cppL1008  Alternatives _No response_  Additional context _No response_ ",2024-11-15T00:12:22Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/140768,we dispatch to sdpa when it is using the fused version: https://github.com/pytorch/pytorch/blob/bf78a0fa968103cff9e2652316071d6cdb0e6296/aten/src/ATen/native/transformers/cuda/attention.cuL572
transformer,[AOTI] AOT Compile NaViT - AttributeError: 'int' object has no attribute 'node'," üêõ Describe the bug NaViT (Native Resolution Vision Transformer) is a variant of the ViT. It can be draftexported but failed when running `aoti_compile_and_package` To repro:  1. Install vitpytorch  2. Run the following:  Draft mode export works, but  AOTI failed with the following error   Versions main ",2024-11-13T22:49:27Z,high priority triage review oncall: pt2 module: dynamic shapes oncall: export module: aotinductor empathy-day,closed,0,6,https://github.com/pytorch/pytorch/issues/140625,cc:  ,This is similar to this internal issue https://fb.workplace.com/groups/1075192433118967/posts/1545164526121753 But it looks like this is an easier repro.,"There's a bug that ought to be fixed but I think the problem can be worked around by editing navit code, specifically:  The shapes should not get cast into a tensor, this is totally unnecessary.",Minimized repro: ,"I think this is some sort of constant propagation edge case, and different from the internal example",Yup  this makes the test pass
transformer,Non-actionable error message when output type is not pytree-able for aoti artifact,"From user empathy day 111324  üêõ Describe the bug Script to generate aoti artifact:  Script to run the artifact and repro error:  error: https://gist.github.com/mlazos/b459671d7a481631cc87351f7ac72366 In this case the solution is to import transformers, but also perhaps sharing the registration API for users would be useful here as well if they return their own data structures.  Versions e754611d190b323e53c5d17db0dc39a96687513c ",2024-11-13T21:36:54Z,oncall: pt2 oncall: export,open,0,1,https://github.com/pytorch/pytorch/issues/140615,",  Looks like we just need to improve the error message to point to an API where we register custom datatype. "
transformer,Bad error message for malformed args during export,"Found during user empathy day 111324  üêõ Describe the bug The below repro yields a cryptic dynamo error: `torch._dynamo.exc.Unsupported: Observed exception`  see full error: https://gist.github.com/mlazos/697b326977dad55c5298d74e8d8df103 This was tricky as I had to guess what the exception is. It looks like we just need to add more information to `ExceptionVariable` to render properly and give more information to the user about which exception was thrown. I was able to guess my way out of this, but our users would definitely be confused by this.   Versions e754611d190b323e53c5d17db0dc39a96687513c ",2024-11-13T20:53:03Z,oncall: pt2 module: dynamo oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/140607
transformer,[export][non-strict] Passing in kwargs torch.export fails at non_strict_utils.fakify()," üêõ Describe the bug  Repro   Error   Notes  If using strict=True, then it's fine.  If using strict=False and args instead of kwargs, then it's fine.  If using strict=False and use kwargs, then it fails.  Versions n/a ",2024-11-13T19:44:03Z,module: bootcamp triaged oncall: pt2 oncall: export empathy-day,open,0,2,https://github.com/pytorch/pytorch/issues/140596,Error suggests that you need to register a pytree function for this type. The error message could be improved tho. ,"what is weird is that `transformers.image_processing_base.BatchFeature` had the same functionality as a dictionary, but it didn't subclass `Dict`. So, I'm guessing if they had subclassed `Dict`, then the pytree function for `Dict` would be fine."
transformer,[Export] AssertionError in replace_autocast_with_hop_pass,Repro: //  See https://huggingface.co/nvidia/NVEmbedv1 for installation instructions //  Basically `pip install transformers` and make sure you have huggingface access  Error:  ,2024-11-13T19:12:04Z,oncall: pt2 oncall: export empathy-day,closed,0,4,https://github.com/pytorch/pytorch/issues/140589," , assigning to you since you are the author of replace_autocast_with_hop_pass",">  , assigning to you since you are the author of replace_autocast_with_hop_pass I'll take a look, thanks for reporting!","The problem is that the current pass doesn't work well with nested autocast. I'll push a fix soon. Update: I fixed the bug with nested autocast, but there's another bug with mixing set_autograd and autocast, working on a fix.","I fixed the HOP pass error in https://github.com/pytorch/pytorch/pull/141169 and https://github.com/pytorch/pytorch/pull/141065, but now I got a new error:  Here: https://github.com/pytorch/pytorch/blob/9bc9d4cdb4355a385a7d7959f07d04d1648d6904/torch/export/_trace.pyL374  This seems like a different bug now, and unrelated to the HOP pass now.  Do you know what's wrong here? "
transformer,RuntimeError in `_group_tensors_by_device_and_dtype` (`torch/optim/optimizer.py`) when using `torchrun` on N>1 GPUs. ," üêõ Describe the bug I'm trying to finetune a model using HuggingFace's `Trainer` class on multiple GPUs using `torchrun`. If I run the following code with a single process, or without torchrun, it works fine. However, if I increase `nproc_per_node`, I get `RuntimeError: Tensors of the same index must be on the same device and the same dtype except `step` tensors that can be CPU and float32/64 notwithstanding`.  This error has already been discussed here, or more recently in issue CC(Tensors of the same index must be on the same device and the same dtype except step tensors that can be CPU and float32 notwithstanding.). However, in this case, the error seems to stem from somewhere else, as the `state_steps` parameter already uses floats (which seems to be the resolution of the original issue). I reproduced the error in a standalone file with a dummy dataset, provided below. I am not sure if this is a HuggingFace or a PyTorch issue. I'm leaning on the PyTorch side since it only happens when using `torchrun` with two or more GPUs.  Output Error   Original Code   Command Command that triggers the error (considering the previous code is in a file called `bug.py`)    Debugging steps  * Running this with a single proc works as expected (the training takes place) * I was able to reproduce the same error on two servers * Using the default alpaca training code leads to the same error, both with Llama and OPT models.  * This error is triggered during the optimization step. Specifically, it is triggered by this snippet of code in `torch/optim/adamw.py:480`:  * Using `adamw_fused` triggers the same error. * I printed the datatype and device of each e",2024-11-13T00:26:30Z,oncall: distributed module: optimizer triaged module: fsdp,closed,0,7,https://github.com/pytorch/pytorch/issues/140471,"Thank you for the detailed investigation. You are correct that it's the bf16 fp32 gap between param and grad at that point, so it's weird the grads became bf16 then. ?","It happens to me as well, only get this error for FSDP training, not the DDP training, revert to transformers==4.45.0 resolve the issue, can train with FSDP ","If reverting the transformers can solve the issue, can this be an integration bug in the transformers library?",I met this problem too with fsdp. And I double check different versions. The problem was introduced in transformers==4.46.2,"I can confirm this error does not occur with transformers==4.45.0. This seems to indicate this is a huggingface issue as  suggested. I opened an issue on their GitHub. I don't know if there still is something to fix on the PyTorch side, so feel free to close this issue, thanks for your help! ","I encountered the same issue with transformers version 4.46.2, while 4.45 seems to work fine.","Closing for now, but feel free to reopen if it is indeed a PyTorch issue"
transformer,Auto SAC - Automated SAC (Selective Activation Checkpointing) Policy Construction and Wrapping ,"Endtoend workflow for automating Selective Activation Checkpointing to enable model training under memory constraints. Under the hood, this PR leverages several tools from `torch.distributed._tools`, namely `RuntimeEstimator`, `MemTracker`, `SACEstimator`, and `ILP for AutoSAC`. It first extracts the memory, runtime and activation checkpointing tradeoff statistics. Using these it solves an ILP (Integer Linear Program) to determine the modules to AC and the memory budget for the selected modules. For each of the selected modules, it obtains a checkpointing policy (optimal or greedy) that determines whether to save or recompute the operators. Finally, the modules are wrapped using the policies. Further, this PR adds the following: 1. Adds a Greedy Policy for determining whether to save or recompute an op using the ratio of (memory/runtime) as the greedy metric. 2. Adds support for estimating runtime by specifying a GPU type in `RuntimeEstimator` and `SACEstimator`. 3. Adds optimizer state memory, mandatorily saved memory, and upper bound for discarded memory in the SAC ILP. For FSDP composability, it estimates FSDP prefetch buffer, shared param, grad, and optimizer state memory. 4. Corrects the objective function of the SAC ILP to be bounded by total forward pass runtime.  CC(Memory Tracker for tracking Module wise memory)   CC(Runtime Estimator for estimating GPU compute time)   CC(SAC Estimator (Selective Activation Checkpointing) for estimating memory and recomputation time tradeoffs.)   CC(ILP for Auto SAC (Selective Activation Checkpointing))  Endtoend example with FSDP2:  Output:  cc:      ",2024-11-12T15:46:05Z,oncall: distributed triaged open source topic: not user facing,open,2,0,https://github.com/pytorch/pytorch/issues/140410
transformer,ILP for auto FSDP wrapping,"  CC(ILP for auto FSDP wrapping) This PR presents a mixed integer linear programming (MILP) formulation that can be utilized to determine, under a memory budget, which modules to wrap as FSDP units. Similar to the auto SAC MILP introduced in https://github.com/pytorch/pytorch/pull/137908, the MILP uses information collected from MemTracker, Runtime Estimator, and SAC Estimator, introduced in these PRs: * https://github.com/pytorch/pytorch/pull/124688 * https://github.com/pytorch/pytorch/pull/134243 * https://github.com/pytorch/pytorch/pull/135208 Endtoend example and its sample output:   ",2024-11-11T19:04:07Z,oncall: distributed Merged Reverted ciflow/trunk topic: not user facing ci-no-td,open,0,6,https://github.com/pytorch/pytorch/issues/140298,  merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""to allow CI pass for PRhttps://github.com/pytorch/pytorch/pull/140410"""," revert m ""for other PR"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
transformer,DDP disables torch.compile dynamic shape behaviour," üêõ Describe the bug When running a torch.compiled model with DDP with inputs that has growing sequence length, the recompilations happens every time the input shape changes. After 8 recompilations, the cache size limit is reached. Very similar to this). version 2.5.1+cu124  Notes :  works without DDP  `torch.compile` with `dynamic=None` or `dynamic=True` have the same behavior  if the model has only one layer, it works  if it's the batch size that's growing, it works **Minimal code to reproduce :**  Run with `torchrun standalone nproc_per_node=1 repl_ddp.py` You will see that the model gets recompiled each step (because the seq len changes each step), and after 8 compilations cache limit is hit. (here, the ""sequence length"" is just a second batch dimension. In my setup where this first occurred (transformer) it is really a sequence length)  Error logs Here are the relevant excerpts of the output when ran with `TORCH_LOGS=""+dynamic""`. **Without DDP (works, compiles only two times):** this get displayed during the 2nd compilation (len was 16, is now 18): `[0/1] create_symbol s0 = 18 for L['x'].size()[1] [2, int_oo]` [...] `[0/1] eval 12288*s0  [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/symbolic_shapes.py:5106] [0/1]     logits = model(inputs) [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/symbolic_shapes.py:5106] [0/1]   File ""/usr/local/lib/python3.11/distpackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/symbolic_shapes.py:5106] [0/1]     return self._call_impl(*args, **kwargs) [rank0]:I1110 16:29:04.502000 41799 torch/fx/experimental/",2024-11-10T16:36:19Z,oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes pt2d-triage-nov2024,closed,0,4,https://github.com/pytorch/pytorch/issues/140229, isn't this the stride specialization thing in DDP?,looking,"Fix here: https://github.com/pytorch/pytorch/pull/140751, more details in the description",Thank you!
transformer,[onnx] [njt] [feature request] Export NJT-enabled SDPA / MHA ops to ORT's PackingMode Attention," üöÄ The feature, motivation and pitch I found that some support for NJTenabled SDPA / MHA exists in onnxruntime: https://github.com/microsoft/onnxruntime/issues/22764 as ""PackedAttention"" https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/convert_to_packing_mode.pyL317 NJTenabled SDPA also used to exist in FasterTransformer, known as ""Effective Transformer kernels"": https://github.com/NVIDIA/FasterTransformer/blob/main/docs/bert_guide.mdstandardbertandeffectivefastertransformer I wonder if in the long term it would be good to have some example of exporting NJT ops to ORT and mapping NJTenabled SDPA to this PackingMode directly at export time.  Alternatives _No response_  Additional context _No response_  ",2024-11-08T10:18:00Z,module: onnx triaged,open,0,1,https://github.com/pytorch/pytorch/issues/140130,"If onnx export in PyTorch supports setting up some transforms or plugins, maybe this proposal can be realized in form of example for this functionality : how to set up onnx export to make use of PackingAttention plugin in ORT"
transformer,DTensor support for fused qkv matmul," üöÄ The feature, motivation and pitch For transformer architecture (for example https://github.com/pytorchlabs/gptfast/blob/main/model.pyL195L211) it tends to be most performant to merge the qkv matrices together. If you try to shard this concatenated tensor then the subsequent SDPA op won't be shared correctly since you need each column of q sharded with the corresponding columns of  k and v [q1,k1,v1,...], but by default the sharding will be [q1, q2, q3...] When not using DTensor this is relatively easy to get to work: https://github.com/pytorchlabs/gptfast/blob/main/tp.pyL73 but for DTensor the way to enable this is really unclear. is there a way to handle this type of operation with DTensor parallelization or should we just stick to normal tensor parallel support and figure out how to get it to work with our APIs? This is currently blocking tensor parallel support in torchAO so i wanted to centralize discussion to a single location.  Alternatives don't use DTensor for tensor parallel  Additional context _No response_ ",2024-11-08T00:12:56Z,oncall: distributed module: dtensor,open,0,1,https://github.com/pytorch/pytorch/issues/140069,Depends on DTensor Strided Sharding:   CC([RFC] PyTorch DistributedTensor)issuecomment2081838245  CC([WIP][RFC][DTensor] DTensor Strided Sharding: A More Flexible Way To Shard Tensors)
transformer,`torch.export.export` infers dynamic_shape as constant," üêõ Describe the bug Install most recent transformers and pytorch.   Repro    Error  Collecting environment information... PyTorch version: 2.6.0.dev20241106+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.0121genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 550.120 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.1.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        39 bits physical, 48 bits virtual Byte Order:                           Little Endian CPU(s):                               4 Online CPU(s) list:                  03 Vendor ID:                            GenuineIntel Model name:                           Intel(R) Core(TM) i57640X CPU @ 4.00GHz CPU family:                           6 Model:                                158 Thread(s) per core:                   1 Core(s) per socket:                   4 Socket(s):                            1 Stepping:                             9 CPU max MHz:                          4200.0000 CPU min MHz:                          800.0000 BogoMIPS:",2024-11-07T13:38:15Z,oncall: pt2 module: dynamic shapes oncall: export,open,0,6,https://github.com/pytorch/pytorch/issues/140011, do you have time to have a look?," Could you try rerunning export with `TORCH_LOGS=""+dynamic""`? That should log while running, and somewhere in the log there's likely a guard that looks like `Eq(s*, 6)` that's responsible for this specializing to a constant.  Then you can rerun additionally with `TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s*, 6)""`, and that should produce a stack trace that likely points to where in model code the specialization happens. If you could post that that would be very helpful.",">  Could you try rerunning export with `TORCH_LOGS=""+dynamic""`? That should log while running, and somewhere in the log there's likely a guard that looks like `Eq(s*, 6)` that's responsible for this specializing to a constant. >  > Then you can rerun additionally with `TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s*, 6)""`, and that should produce a stack trace that likely points to where in model code the specialization happens. If you could post that that would be very helpful. Sure!  Here are the stack traces:  export TORCH_LOGS=""+dynamic""  Loading checkpoint shards: 100% 2/2 [00:01), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:00.782000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:00.787000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s1 = VR[2, 128] (update) I1118 13:45:00.787000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s1 = 6 for L['attention_mask'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s1"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:00.791000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s2 = VR[2, 128] (update) I1118 13:45:00.791000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s2 = 6 for L['position_ids'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s2"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:00.827000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:00.829000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:00.834000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 1) == False [statically known] V1118 13:45:00.839000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s0, 1) == False [statically known] V1118 13:45:00.842000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:00.844000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s1, 1) == True [statically known] V1118 13:45:00.874000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s0 = VR[6, 6] (update) I1118 13:45:00.875000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5793] [0/0] set_replacement s0 = 6 (range_refined_to_singleton) VR[6, 6] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] runtime_assert Eq(s0, 6) [guard added] causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(1, 1)   transformers/models/llama/modeling_llama.py:1091 in _prepare_4d_causal_attention_mask_with_cache_position (_refs/__init__.py:425 in _broadcast_shapes) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] User Stack (most recent call last): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   (snipped, see stack below for prefix) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/transformers/models/llama/modeling_llama.py"", line 915, in forward I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     causal_mask = self._update_causal_mask( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/transformers/models/llama/modeling_llama.py"", line 1024, in _update_causal_mask I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/transformers/models/llama/modeling_llama.py"", line 1091, in _prepare_4d_causal_attention_mask_with_cache_position I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(1, 1) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] Stack (most recent call last): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/aot_llama_test.py"", line 96, in  I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     compile_path = aot_compile('llama3.pt2', model, **inputs1) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/aot_llama_test.py"", line 58, in aot_compile I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     exported_program = torch.export.export( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/__init__.py"", line 368, in export I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _export( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1004, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ep = fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/exported_program.py"", line 122, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1957, in _export I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     export_artifact = export_func(   type: ignore[operator] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1251, in _strict_export I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _strict_export_lower_to_aten_ir( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1279, in _strict_export_lower_to_aten_ir I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level = _export_to_torch_ir( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level, _ = torch._dynamo.export( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     result_traced = opt_f(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._torchdynamo_orig_callable( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     guarded_code = compile_inner(code, one_graph, hooks, transform) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile_inner(code, one_graph, hooks, transform) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return function(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 743, in _compile_inner I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     out_code = transform_code_object(code, transform) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1348, in transform_code_object I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     transformations(instructions, code_options) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 233, in _fn I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 662, in transform I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2914, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     super().run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1738, in CALL_FUNCTION I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, {}) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 410, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1828, in CALL_FUNCTION_KW I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 294, in impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn_var.call_function(self, self.popn(nargs), {})) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 998, in call_function I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return handler(tx, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 974, in _handle_insert_op_in_graph I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy(tx, proxy) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2100, in wrap_fx_proxy I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2166, in wrap_fx_proxy_cls I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _wrap_fx_proxy( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2262, in _wrap_fx_proxy I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2247, in get_fake_value I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ret_val = wrap_fake_exception( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 1795, in wrap_fake_exception I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn() I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2248, in  I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     lambda: run_node(tx.output, node, args, kwargs, nnmodule) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2362, in run_node I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return node.target(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/utils/_stats.py"", line 21, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1271, in __torch_dispatch__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.dispatch(func, types, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1813, in dispatch I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._cached_dispatch_impl(func, types, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1381, in _cached_dispatch_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     output = self._dispatch_impl(func, types, args, kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 2297, in _dispatch_impl I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     r = func(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_ops.py"", line 723, in __call__ I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._op(*args, **kwargs) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_meta_registrations.py"", line 3568, in meta_binop_inplace I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     check_inplace_broadcast(self.shape, other.shape) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_meta_registrations.py"", line 90, in check_inplace_broadcast I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     broadcasted_shape = tuple(_broadcast_shapes(self_shape, *args_shape)) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_refs/__init__.py"", line 425, in _broadcast_shapes I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     torch._check( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1615, in _check I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     _check_with(RuntimeError, cond, message) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1578, in _check_with I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     if expect_true(cond): I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 1302, in expect_true I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return a.node.expect_true( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/sym_node.py"", line 515, in expect_true I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.shape_env.defer_runtime_assert( I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/recording.py"", line 263, in wrapper I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return retlog(fn(*args, **kwargs)) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6453, in defer_runtime_assert I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self._log_guard(""runtime_assert"", orig_expr, forcing_spec=False) I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6097, in _log_guard I1118 13:45:00.878000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.log.info( V1118 13:45:00.886000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 9223372036854775807) == False [statically known] V1118 13:45:00.890000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s1 > 1 == True [statically known] V1118 13:45:00.955000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 1) == False [statically known] V1118 13:45:00.958000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 9223372036854775807) == False [statically known] V1118 13:45:00.961000 1300466 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s2 What is a compiler? A compiler is a program that translates source code written Traceback (most recent call last):   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir     gm_torch_level, _ = torch._dynamo.export(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1584, in inner     raise constraint_violation_error   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner     result_traced = opt_f(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn     return fn(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__     return self._torchdynamo_orig_callable(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__     return _compile(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile     guarded_code = compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner     return _compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function     return function(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 840, in _compile_inner     check_fn = CheckFunctionManager(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 2183, in __init__     guard.create(builder)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_guards.py"", line 298, in create     return self.create_fn(builder, self)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 1766, in SHAPE_ENV     code_parts, verbose_code_parts = output_graph.shape_env.produce_guards_verbose(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 5072, in produce_guards_verbose     raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=""+dynamic"".    Not all values of seq_len = L['input_ids'].size()[1] in the specified range seq_len "," export TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s1, 6) ``` Loading checkpoint shards: 100% 2/2 [00:01), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:23.237000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:23.242000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s1 = VR[2, 128] (update) I1118 13:45:23.242000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s1 = 6 for L['attention_mask'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s1"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:23.246000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s2 = VR[2, 128] (update) I1118 13:45:23.246000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4320] [0/0] create_symbol s2 = 6 for L['position_ids'].size()[1] [2, 128] (_dynamo/variables/builder.py:2785 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s2"" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=""0"" V1118 13:45:23.282000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:23.283000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6398] [0/0] runtime_assert True == True [statically known] V1118 13:45:23.288000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 1) == False [statically known] V1118 13:45:23.293000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s0, 1) == False [statically known] V1118 13:45:23.296000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s0, 1) == True [statically known] V1118 13:45:23.299000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Ne(s1, 1) == True [statically known] V1118 13:45:23.328000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s0 = VR[6, 6] (update) I1118 13:45:23.328000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5793] [0/0] set_replacement s0 = 6 (range_refined_to_singleton) VR[6, 6] I1118 13:45:23.329000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] runtime_assert Eq(s0, 6) [guard added] causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(1, 1)   transformers/models/llama/modeling_llama.py:1091 in _prepare_4d_causal_attention_mask_with_cache_position (_refs/__init__.py:425 in _broadcast_shapes), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s0, 6)"" V1118 13:45:23.336000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s1, 9223372036854775807) == False [statically known] V1118 13:45:23.341000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s1 > 1 == True [statically known] V1118 13:45:23.406000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 1) == False [statically known] V1118 13:45:23.408000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval Eq(s2, 9223372036854775807) == False [statically known] V1118 13:45:23.412000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6228] [0/0] eval s2  I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     compile_path = aot_compile('llama3.pt2', model, **inputs1) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/aot_llama_test.py"", line 58, in aot_compile I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     exported_program = torch.export.export( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/__init__.py"", line 368, in export I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _export( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1004, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ep = fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/exported_program.py"", line 122, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1957, in _export I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     export_artifact = export_func(   type: ignore[operator] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1251, in _strict_export I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _strict_export_lower_to_aten_ir( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 1279, in _strict_export_lower_to_aten_ir I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level = _export_to_torch_ir( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     gm_torch_level, _ = torch._dynamo.export( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     result_traced = opt_f(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._call_impl(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return forward_call(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._torchdynamo_orig_callable( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     guarded_code = compile_inner(code, one_graph, hooks, transform) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _compile_inner(code, one_graph, hooks, transform) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return function(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 743, in _compile_inner I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     out_code = transform_code_object(code, transform) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1348, in transform_code_object I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     transformations(instructions, code_options) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 233, in _fn I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 662, in transform I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2914, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     super().run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1828, in CALL_FUNCTION_KW I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/nn_module.py"", line 442, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1816, in CALL_FUNCTION_EX I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, argsvars.items, kwargsvars) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 410, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1816, in CALL_FUNCTION_EX I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, argsvars.items, kwargsvars) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/nn_module.py"", line 442, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1816, in CALL_FUNCTION_EX I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, argsvars.items, kwargsvars) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 410, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 640, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return inner_fn(self, inst) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1738, in CALL_FUNCTION I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.call_function(fn, args, {}) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 967, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn.call_function(self, args, kwargs))   type: ignore[argtype] I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 349, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return super().call_function(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/functions.py"", line 125, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 973, in inline_user_function_return I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3129, in inline_call I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return cls.inline_call_(parent, func, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 3257, in inline_call_ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     tracer.run() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1120, in run I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     while self.step(): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1032, in step I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.dispatch_tableinst.opcode I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 294, in impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.push(fn_var.call_function(self, self.popn(nargs), {})) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 998, in call_function I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return handler(tx, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builtin.py"", line 974, in _handle_insert_op_in_graph I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy(tx, proxy) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2100, in wrap_fx_proxy I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2166, in wrap_fx_proxy_cls I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return _wrap_fx_proxy( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 2262, in _wrap_fx_proxy I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2247, in get_fake_value I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     ret_val = wrap_fake_exception( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 1795, in wrap_fake_exception I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn() I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2248, in  I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     lambda: run_node(tx.output, node, args, kwargs, nnmodule) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 2362, in run_node I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return node.target(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/utils/_stats.py"", line 21, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return fn(*args, **kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1271, in __torch_dispatch__ I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.dispatch(func, types, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1813, in dispatch I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self._cached_dispatch_impl(func, types, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 1381, in _cached_dispatch_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     output = self._dispatch_impl(func, types, args, kwargs) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_tensor.py"", line 2183, in _dispatch_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs)) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_impls.py"", line 879, in fast_binary_impl I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     final_shape = infer_size(final_shape, shape) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_subclasses/fake_impls.py"", line 833, in infer_size I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     torch._check( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1615, in _check I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     _check_with(RuntimeError, cond, message) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/__init__.py"", line 1578, in _check_with I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     if expect_true(cond): I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 1302, in expect_true I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return a.node.expect_true( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/sym_node.py"", line 515, in expect_true I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return self.shape_env.defer_runtime_assert( I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/recording.py"", line 263, in wrapper I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     return retlog(fn(*args, **kwargs)) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6453, in defer_runtime_assert I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self._log_guard(""runtime_assert"", orig_expr, forcing_spec=False) I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 6097, in _log_guard I1118 13:45:23.500000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0]     self.log.info( V1118 13:45:23.546000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5632] [0/0] _update_var_to_range s1 = VR[6, 6] (update) I1118 13:45:23.547000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:5793] [0/0] set_replacement s1 = 6 (range_refined_to_singleton) VR[6, 6] I1118 13:45:23.547000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6097] [0/0] eval Eq(s1, 6) [guard added] causal_mask = causal_mask[:, :, :, : key_states.shape[2]]   transformers/models/llama/modeling_llama.py:589 in forward (_dynamo/utils.py:2362 in run_node), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s1, 6)"" V1118 13:45:23.548000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:6217] [0/0] eval 6 [trivial] I1118 13:45:26.643000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4429] [0/0] produce_guards V1118 13:45:26.643000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].size()[0] 1 None V1118 13:45:26.643000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].size()[1] 6 StrictMinMaxConstraint(warn_only=False, vr=VR[1, 128]) V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].stride()[0] 6 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].stride()[1] 1 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['input_ids'].storage_offset() 0 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].size()[0] 1 None V1118 13:45:26.644000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].size()[1] 6 StrictMinMaxConstraint(warn_only=False, vr=VR[1, 128]) V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].stride()[0] 6 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].stride()[1] 1 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['attention_mask'].storage_offset() 0 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].size()[0] 1 None V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].size()[1] 6 StrictMinMaxConstraint(warn_only=False, vr=VR[1, 128]) V1118 13:45:26.645000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].stride()[0] 6 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].stride()[1] 1 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['position_ids'].storage_offset() 0 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['cache_position'].size()[0] 6 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['cache_position'].stride()[0] 1 None V1118 13:45:26.646000 1300510 venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py:4631] [0/0] track_symint L['cache_position'].storage_offset() 0 None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] Error while creating guard: E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] Name: '' E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Source: shape_env E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Create Function: SHAPE_ENV E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Guard Types: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Code List: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Object Weakref: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     Guarded Class Weakref: None E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] Traceback (most recent call last): E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_guards.py"", line 298, in create E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     return self.create_fn(builder, self) E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 1766, in SHAPE_ENV E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     code_parts, verbose_code_parts = output_graph.shape_env.produce_guards_verbose( E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 5072, in produce_guards_verbose E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]     raise ConstraintViolationError( E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=""+dynamic"". E1118 13:45:26.647000 1300510 venv/lib/python3.10/sitepackages/torch/_guards.py:300] [0/0]    Not all values of seq_len = L['input_ids'].size()[1] in the specified range seq_len What is a compiler?? A compiler is a program that translates source code Traceback (most recent call last):   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/export/_trace.py"", line 660, in _export_to_torch_ir     gm_torch_level, _ = torch._dynamo.export(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1584, in inner     raise constraint_violation_error   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1539, in inner     result_traced = opt_f(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 556, in _fn     return fn(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1736, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1747, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 1423, in __call__     return self._torchdynamo_orig_callable(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 549, in __call__     return _compile(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 977, in _compile     guarded_code = compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 708, in compile_inner     return _compile_inner(code, one_graph, hooks, transform)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_utils_internal.py"", line 95, in wrapper_function     return function(*args, **kwargs)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 840, in _compile_inner     check_fn = CheckFunctionManager(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 2183, in __init__     guard.create(builder)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_guards.py"", line 298, in create     return self.create_fn(builder, self)   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/_dynamo/guards.py"", line 1766, in SHAPE_ENV     code_parts, verbose_code_parts = output_graph.shape_env.produce_guards_verbose(   File ""/home/samu/venv/lib/python3.10/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 5072, in produce_guards_verbose     raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (seq_len)! For more information, run with TORCH_LOGS=""+dynamic"".    Not all values of seq_len = L['input_ids'].size()[1] in the specified range seq_len ","Let's debug the `Eq(s0, 6)`, since that seems upstream of and might be causing the other 2 specializations. The user stack trace seems helpful:  The work now is to look into this call, and possibly trace back, to follow shape propagation rules on how these tensors are constructed, and how they're related to `s0`. For example, during fake tensor propagation, which is where we handle symbolic shapes, if cache_position has shape `s0` (i.e. is derived from `input_ids.size(1)` somehow), and if `target_length` is just a plain int (6), then we'll reason from this elementwise operation that `s0 == 6` and specialize. The solution is to make sure all the computation leading up to here is such that both `target_length` and the size of `cache_position` use `s0` (`input_ids.size(1)`), inplace of the static value 6, so this call doesn't trigger specialization. Would you have some idea how this might be happening?","I realized that my intention was not to define any custom inputs for the model, so after simplifying inputs like this:  and dynamic shape definitions to take into account only inputs and attention masks:  the constraint error on sequence length changes to   . Then the suggested fix and the issue looks very similar to this one  CC([export] Llama3 export with dynamic shapes fails with constraint violations) . Following the hints from that issue and adding `with torch.nn.attention.sdpa_kernel([SDPBackend.MATH]):` as a context manager solves the constraint error and model gets exported and compiled. "
transformer,Inductor vs. Liger Performance Track," üêõ Describe the bug Recently, we did some benchmarking on custom operators in liger kernels compared with inductor compiled kernels. Inductor is worse on some  cases. Here is the operator and config list we need to improve.   List  Format For each operator, the data format in the following task list is:  [ ] **Operator Name**, (20th percentile speedup, 50th percentile (median), 80th percentile)  Speedup Calculation The speedup numbers are computed as follows: $$ \text{inductor\\_vs\\_liger} = \frac{\text{speedup\\_inductor}}{\text{speedup\\_liger}} = \frac{\frac{\text{latency\\_eager}}{\text{latency\\_inductor}}}{\frac{\text{latency\\_eager}}{\text{latency\\_liger}}} = \frac{\text{latency\\_liger}}{\text{latency\\_inductor}} $$ Since each operator has multiple inputs, there are multiple speedup numbers. We use the 20th, 50th, and 80th percentiles to better represent the results. If the number is >1, it means inductor's results are faster. The GPU peak memory usage is determined using the same process. We need to improve inductor's performance on cases that less than 1.  For FP32 forward_backward latency:  [ ] kl_div, (0.65, 0.65, 1.28)  [ ] rms_norm, (0.77, 1.09, 1.29)   [ ] CC(rope perf improvement)  [ ] geglu, (0.98, 1.00, 1.00)  [ ] embedding  For FP32 peak gpu memory usage:  [ ] cross_entropy, (0.75, 0.75, 0.75)  [ ] fused_linear_cross_entropy, (0.47, 0.58, 0.75)  [ ] fused_linear_jsd, (0.71, 0.72, 0.81)  [ ] geglu, (0.89, 0.90, 0.94)  [ ] kl_div, (0.86, 0.86, 1.00)  [ ] rope, (0.54, 0.56, 0.75)  [ ] swiglu, (0.90, 0.91, 0.92) For FP32 forward only latency:  [ ] embedding, (0.40, 0.47, 0.50)  CC(Embedding forward performance analysis",2024-11-06T18:37:02Z,triaged oncall: pt2 module: inductor,open,0,0,https://github.com/pytorch/pytorch/issues/139908
transformer,`torch.compile` error on `scaled_dot_product_attention` in `transformers.LlamaForCausalLM` when providing `attention_mask`: `RuntimeError: (*bias): last dimension must be contiguous`," üêõ Describe the bug Related: *  CC(SDPA + torch.compile: (*bias): last dimension must be contiguous) *  CC(""RuntimeError: (*bias): last dimension must be contiguous"" with F.scaled_dot_product_attention + torch.compile)  cc:   Versions PyTorch version: 2.6.0.dev20241030+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 OS: Ubuntu 22.04.5 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.0124genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.4.131 GPU 0: NVIDIA H100 80GB HBM3 Nvidia driver version: 550.90.12 Versions of relevant libraries: [pip3] pytorchtriton==3.1.0+cf34004b8a [pip3] torch==2.6.0.dev20241030+cu124 [conda] pytorchtriton            3.1.0+cf34004b8a          pypi_0    pypi [conda] torch                     2.6.0.dev20241030+cu124          pypi_0    pypi ",2024-10-31T19:50:47Z,triaged oncall: pt2 module: dynamo,open,0,10,https://github.com/pytorch/pytorch/issues/139424, I can't repro the failure. Would you mind doing the ablation described here to help me understand which layer is contributing to the failure: https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0heading=h.f8o4pwe4uv1h,">  I can't repro the failure. Would you mind doing the ablation described here to help me understand which layer is contributing to the failure: https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0heading=h.f8o4pwe4uv1h It fails with the same error with `backend` set to `""eager""`, `""aot_eager""`, or `""aot_eager_decomp_partition""`. The deepest I can go into the model to still achieve this error is the `LlamaModel` of `LlamaForCausalLM`. Running with `TORCHDYNAMO_REPRO_AFTER=aot`, I got this script that also repros (for me):  The tail of the error message I see: ",Can you repro with the minifier script or still not?, can you check if https://github.com/pytorch/pytorch/pull/139787 fixes it for you ?,"No, it didn't seem to fix it. Also, I don't know if I posted the right repro script previously... Does this reproduce for you? ",Your original repro works for me on my pr.,ü§î  Are you building torch from source when you are testing your branch? I'm trying to consider what the differences between our environments might be... I've simply been modifying the nightly wheel installed in my sitepackages with your change in `torch/_inductor/lowering.py` and I am still seeing the error persist on my original repro.,"Can repro now that I am on `transformers==4.46.2`.  , we have this argument which is being expanded inside inductor.   We are viewing a tensor of size `[2, 1, 8192, 8]` as a tensor of size `[2, 32, 8192, 8192]`.  Can we loosen the checks inside `scaled_dot_product_attention` to allow stride 0 ? this will still be 16byte aligned. The alternative is we have to materialize this much larger tensor in memory, which will be slow.", https://github.com/pytorch/pytorch/pull/139752 this should do it right?,"That just allows size 1 but it doesnt allow stride 0. The input to the sdpa op will be size `[2, 32, 8192, 8192]`"
transformer,[export] Failure to export Mixtral-8x7B-Instruct-v0.1," üêõ Describe the bug I'm experiencing an issue exporting the `Mixtral8x7B` model. I originally discussed the problem in CC(Failure to export Mixtral8x7BInstructv0.1 as onnx), but this is more about `torch.export`.  My ultimate goal is to go to StableHLO. It raises the error `error torch._dynamo.exc.Unsupported: hasattr ConstDictVariable to.` I've seen a similar issue here)), is this something that we'd need to add a custom handler for? Script to reproduce problem     I also tested with `torch.onnx.export(..., dynamo=True, report=True)` on the latest torchnightly (`2.6.0.dev20241030+cpu`), I get the following: (Markdown error report in this gist).   You both have commits to `torch.export`, any recommendations?  Versions  ",2024-10-31T15:19:22Z,oncall: pt2 export-triaged oncall: export,open,0,4,https://github.com/pytorch/pytorch/issues/139401,"I'd highly encourage you to try exporting with `strict=False`, which avoids Dynamo coverage issues such as this.","Thanks!  For LLama2 exports I've been using:  as recommended as a workaround hereissuecomment2161601418). However, trying this with Mixtral gives:  This also happens when adding `strict=False` to the normal `torch.export.export()`.",You can use the public API with `strict=False`. Could you report if Mixtral goes through with that?,"> You can use the public API with `strict=False`. Could you report if Mixtral goes through with that? Yes, I got the same behaviour with the public `torch.export.export()` API."
transformer,[MPS] Torch 2.5.x and Nightlies are using  50% more memory and are 60% slower than 2.4.1 run Stable Diffusion," üêõ Describe the bug I've seen a lot of people mentioning in various forums but not seen an issue raised here so here we go. I'm reporting this as an MPS issue as I've not been able to test on NVIDIA or AMD etc. Running various Stable Diffusion and new transformer DiT models using 2.5.x and nightly releases of PyTorch shows a very significant downgrade in performance compared to running them in in the same environment using pytorch 2.4.1. For example a standard SDXL run via Diffusers using 2.4.1, the python binary reports using 9.5Gb and runs at 5.7 seconds per iteration, under 2.5.1 or nightly it reports 14.9 GB and runs at 8.5 s/i SD3.5 goes from running without setting  PYTORCH_MPS_HIGH_WATERMARK_RATIO  on a 24Gb M3 to failing if its not set to 0.0 and using 37Gb compared to 27.7GB when they first start iterating (2.5.1 takes ages to start iterating they'll use more when it kicks in properly). Here is the diffusers SDXL script I used as that'll be the smallest and fastest model. Its a little messy as was a test script of something else  but I wanted to give you the exact same script, so you'll have to forgive the unused imports and variables :)  environment id a straight forward diffusers venv  torch version was switch around br doing an uninstall and reinstall  or   Versions  python lib/python3.11/sitepackages/torch/utils/collect_env.py  Collecting environment information... PyTorch version: 2.5.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 15.0.1 (arm64) GCC version: Could not collect Clang version: 16.0.0 (clang1600.0.26.4) CMake version: version 3.29.5 Libc version: N/A Python ve",2024-10-31T12:25:28Z,high priority module: memory usage triaged module: mps,closed,0,10,https://github.com/pytorch/pytorch/issues/139389, do you mind posting references to some of the forum threads here?,"Struggling to find them again, crappy discord,  actually I keep finding people mentioning 2.4.0 period nightlies compared to 2.3.1 but try the example  it's hard to miss the slowdown and extra memory usage.","I've modified script a bit  It reports that on my M2 Pro 2.4.1 finishes generation in 50 sec using 14Gb of memory, while 2.5.1 needs 64 sec and 16Gb RAM, so 30% slower and 15% more RAM, which is pretty bad",,The PR that caused at least of of the regressions Is https://github.com/pytorch/pytorch/pull/131863,Let's keep it open for a bit until nightly is available and someone can confirm that this is working for them.  can you build PyTorch from source using https://github.com/pytorch/pytorch/commit/68ef445c330d465d3d20f253583c2370c5df0139 or prefer to wait until nightly build is available?,"As I'm off to bed in a few minutes it'll probably have to wait until the nightlies are built, what time is that normally ?", I just ran with ToT and the memory utilization + runtime is back in line with 2.4.1,"Just tried a nightly.  speed is back, if fact its improved,  memory practically there ~ 3.4% .","Cool thank you all for validating, closing now ,but need to figure out how we can monitor for something like that...."
transformer,Plan to support ‚Äúdiscrete‚Äù dynamic dimension on torch.export," üöÄ The feature, motivation and pitch Is there any plan for the `torch.export` team to support the setbased dynamism in addition to rangebased dynamism? Below is a small repro code for discrete `Dim` failure mode. The `torch._check` is supposed to work within discrete sets, but it fails.   Alternatives _No response_  Additional context The conventional use of the dynamic dimension on `torch.export` is to use the continuum of the variables like `Dim(min=3, max=12)`, which is suited for accepting dynamic batch size for example. Our team is trying to support some level of ‚Äúdiscrete‚Äù dynamism on the neural network, for providing ‚Äúimage‚Äù mode and ‚Äúvideo‚Äù mode for the vision transformer. To show some idea behind this, here is a pseudocode for doing the operation. The specifics are not same in detail.  The `torch.export` immediately fails, because the second `lambda` function inside `torch.cond` is not compatible with the length other than 12. Since there is no way to enforce the model to accept the ‚Äúset‚Äù based constraints other than the continuum of the shapes, our team has no way other than exporting two almostidentical models with sharing large sets of common transformer weights. ",2024-10-30T18:44:31Z,oncall: pt2 module: dynamic shapes export-triaged oncall: export,open,1,7,https://github.com/pytorch/pytorch/issues/139307,I also wonder if it's possible to parse native Python `assert` statements (e.g. supporting constraints on shape elements and enforcing eq/lt relations between shapes of different tensors) and transform them into torch.compile constraints?   CC(Record shaping assertions and use them for tracing / scripting optimization and codegen),Error message: ,"If the graphs end up differing between the discrete values there's nothing you can do, you have to torch.cond your way to victory or export multiple graphs.","> If the graphs end up differing between the discrete values there's nothing you can do, you have to torch.cond your way to victory or export multiple graphs. Understood. Currently the dynamic dimension is in the continuum of the shapes, so it is really challenging to capture this branch in a single graph. For example one of the shapes can be 2 or 4 and not in between, there is no way to enforce this as a dynamic dimension and the graph capturing will not be run as intended. Is there any plan to support this discrete dynamic dim in the future? Or is it not possible or appropriate thing to support because of the inherent design of dynamic graph capturing?",The problem is specifically around capturing and preserving branching behavior in the set of computations you do at runtime. This is only supported through torch.cond. There may be some auxiliary problems where you are unable to refine shape symbols temporarily inside cond but those we should fix.," Following this thread, can you add a bit more color to your response? In this modified toy example above,   the assert `torch._check(x.shape[0] in {2, 3})` forces `torch.export` to infer `x` has a constant shape of 3. That seems odd, especially given that shape guards can check for x in a range (e.g. `torch._check(2 <= x.shape[0] <= 3)` is completely fine). In principle, couldn't there be a `torch.export.Dim` object that took `values={...}` as a param and applied a disjunctive shape guard, e.g.  which would then pass ","We technically support disjunction but it is not exposed in a user facing way because it's.... not very useful ü§£ . We don't currently do any reasoning based off of the information, so it would basically be equivalent to not having the check at all. I suppose that in principle discrete values could be used to statically reason about unbacked SymInts, as you could enumerate all disjunct values to see if a given expression is statically known in all situations... but that's pretty obscure. When you write `x.shape[0] in {2, 3}`, there is an additional problem that we must recognize that this is something that we can generate a disjunction on, as opposed to guarding. Without Dynamo, this would be impossible to do as the only thing the reasoning system would see is the equality test on 2 and then the test on 3 (or, I suppose in this case, you would try to hash the SymInt, which doesn't work.) With Dynamo, in principle, we could write a special case for test of membership in a set which is known to be constant...... I guess I would accept a patch that does this, but it seems pretty niche."
transformer,torch.compile'ing individual linears for torchtitan debug model + FSDP2 leads to errors," üêõ Describe the bug When I change torchtitan's torch.compile logic to compile individual linear layers instead of transformer blocks, I see an error:   There is a full repro here: https://github.com/pytorch/torchtitan/pull/661  you can check out that PR against torchtitan and run the test plan on a machine with at least 2 GPUs to see the error. Note that this seems similar to  CC(inductor error with PT Lightning + FSDP + torchao.float8 + torch.compile), but this repro is on FSDP2.  Versions Pytorch version 2.6.0.dev20241023+cu121 ",2024-10-29T21:19:49Z,oncall: distributed triaged module: fsdp oncall: pt2 module: dynamo module: guards pt2d-triage-nov2024,open,0,4,https://github.com/pytorch/pytorch/issues/139222," ,  ","hmm... this does seem eerily similar to https://github.com/pytorch/pytorch/pull/138819, I'll try to confirm if that fixes this for FSDP2 as well later this week","Confirmed locally that this is also fixed by https://github.com/pytorch/pytorch/pull/138819. I still need to look more into the CI failures on that PR, with some help from Animesh. With that change, the repro runs E2E for me. I do see many recompiles though, which are probably worth looking into"," these are the recompiles I get with the above patch:  And this sort of makes sense: we have 3 linear layers, where each layer has a parameter with a difference shapes. compile treats parameter shapes as static by default, so we recompile for each distinct layer's weight shapes. You could try the suggestion in the error message (dont force compile to treat parameter shapes as static  although specializing on param shape might give better matmul perf)"
transformer,"When I load the model data of torch_model.bin, it will go directly to GPU loading, and cannot load the whole process on CPU"," üêõ Describe the bug When I load the model data of torch_model.bin, it will go directly to GPU loading, and cannot load the whole process on CPU. Below is my code  source fileÔºötorch/_utils.py:_cuda:113   Versions Collecting environment information... PyTorch version: 2.3.0 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.0119genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 4090 GPU 1: NVIDIA GeForce RTX 4090 GPU 2: NVIDIA GeForce RTX 4090 GPU 3: NVIDIA GeForce RTX 4090 Nvidia driver version: 550.107.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        43 bits physical, 48 bits virtual Byte Order:                           Little Endian CP",2024-10-26T05:24:06Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/138977,"Use `map_location=""cpu""`?",Please ask the question on how to use PyTorch on https://discuss.pytorch.org and use issues to report unexpected behaviour/feature requests. `map_location` option is indeed a solution here
transformer,"[ROCm] ""No available kernel"" when running EFFICIENT_ATTENTION sdpa",Hit this error when running https://huggingface.co/genmo/mochi1preview. Repro script as followed.   Environment:  ,2024-10-24T23:42:49Z,module: rocm triaged,open,0,4,https://github.com/pytorch/pytorch/issues/138864, ,"  This is expected, ME on ROCM only support head dimension <= 256. The example needs head dimension == 512.",Hi  will there be plan to support this in ROCm?,"> Hi  will there be plan to support this in ROCm? There is no solid plan for arbitrary head dimension support right now, since it requires major rework of the kernel. "
transformer,[export] `run_decompositions` fails with pytree error on `Llama-3.2-vision`,The attached code fails with the following error when `run_decompositions` is run. **It is a regression from torch 2.5.** This affects the ONNX exporter.  Error   Code   Env   ,2024-10-24T18:41:19Z,triaged oncall: pt2 oncall: export,closed,0,9,https://github.com/pytorch/pytorch/issues/138839,  bump on this, ,"> bump on this  not sure I see what the ask is here. I've passed it to the export team, they are looking into it. If you think this is a high priority issue, please add a label and it'll be discussed during the next triage review ","> > bump on this >  >  not sure I see what the ask is here. I've passed it to the export team, they are looking into it. If you think this is a high priority issue, please add a label and it'll be discussed during the next triage review Thanks, just looking for an initial response as it's been around for some time now. Hopefully it's resolved in a week or two as it's blocking high pri model export. Would this time frame require a high pri tag?",I will take a look today. ,"Could you patch this before export:  The issue is not run_decompositions, it is more like export failed to recognize the user subclass dict input as a proper dict and it accidentally worked. I think we need better detection mechanism in export to error out early on. ",Thank you!," If you got unblocked, can i close this issue?","Yes, thanks!"
transformer,"There is no header file ""torch/torch.h""file int the include folder.I doubt if that file was removed in the latest version.","**_I download the  release binary files from the https://pytorch.org/  and ,my version is CUDA 12.4._** However,after I unziped the release,I discover that when I use cmake to contain the Libtorch to my build system,I connot find the key header file ""Torch/torch.h"" in the include folder. You can see it below.I install both Debug and Release version for that,so ...I doubt if the header was removed from libtorch,and I cannot query any information about it... Can anyone help me slove this question,thanks a lot...  ",2024-10-24T15:25:49Z,needs reproduction module: cpp triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/138822,"And there is a few files  exists in the torch folder such as ""torch/script.h""...What can I say...","Not sure which binaries you are trying to test against (and what this folder graph is about), but `torch.h` is present in 2.5.0 binaries. Taking windows cpu one: ","> Not sure which binaries you are trying to test against (and what this folder graph is about), but `torch.h` is present in 2.5.0 binaries. Taking windows cpu one: >  thank you a lot ÔºåI would try this package later.","Closing, but please do not hesitate a new issue if you run into a new problem with PyTorch"
transformer,AOT eager accuracy regression in Segformer in 2.5.0 release," üêõ Describe the bug I noticed a qualitative regression in the inference quality when compiling a segformerb0 model from the `transformers` library. This regression was introduced in the 2.5.0 release  previously, the model output was qualitatively identical. The code below segments an example image. In **eager mode**, the segmentation looks like this: !eager_2 4 1 Previously, in 2.4.1, the AOTinductor output was identical: !aot_2 4 1 Now, in 2.5.0, in the AOTinductor output the predicted masks corresponding to sidewalk, car, tree, and lamppost have subtle artifacts: !aot_2 5 0 The code I used to create the visualization is below. Please let me know if a more minimal or quantitative example is required. Is this a genuine regression, or just a change in the default autotune config? Is there a way I can configure AOT Inductor to produce identical results to eager mode? Thank you!   Versions For 2.5.0 model:  For 2.4.1 model:  ",2024-10-22T22:32:43Z,high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,open,0,9,https://github.com/pytorch/pytorch/issues/138652,"A small repro will always be helpful. The first thing I will check is if torch.compile causes a similar regression. That will help us to triage if this is a torch.compile (Inductor) issue, or a AOT Inductor specific issue.","Oh, good idea. Here's a more minimal and quantitative example. Using this example, I think that the issue is in Inductor.  **Output for 2.4.1:**  **Output for 2.5.0**  To me, this suggests that: 1. For both versions of Pytorch, 2.4.1 and 2.5.0, the absolute and relative error was similar for `torch.compile` and `torch._export.aot_compile`. 2. There is a much larger error between eager mode and the Inductor product in 2.5.0 than in 2.4.1. In case it helps to reproduce it, I've been running these examples in Docker with a workflow similar to: ","I think this issue actually has to do with `nn.functional.interpolate`, which is used in the Decode Head of the segformer implementation in `transformers` here. I manually minified this bug a bit and determined that setting `align_corners=True` in this function in `transformers` reduces the error by several orders of magnitude. Maybe there has been some recent change to the CUDA implementation of `interpolate` causing a regression in this usage? My intuition says that the decode head should be using `align_corners=True` in the first place, so maybe it has to do with that.","After a bit more debugging, I have unfortunately been unable to reproduce this bug in a minified example. However, I'm fairly confident that it involves an interaction of `interpolate` with `align_corners=False` and strided memory. That is because the `SegformerDecodeHead::forward()` function does some ambitious `flatten` and `transpose` operations inside of a helper class, and I discovered that adding a call to `contiguous` after one of them fixes the bug. Here's my hack that I have found fixes the issue on my machine:  I was unable to reproduce this bug in a reduced file with only `torch` dependencies, but from my experience with strided memory operations in Inductor I really suspect there's an issue in Pytorch code here. Do you have any advice  for how I could narrow down the search for a minimal, reproducible example?", ,"This fails with me on aot_eager  note, not aot_eager with decomp partition.    gives "," does this fail with both aot_eager, and aot_eager_decomp_partition, or just one of them?",It fails with both for me. Slightly smaller repro: ,"  sorry, yea, meant to say, ""it fails on aot_eager, even without decompositions"""
transformer,[ONNX] Export Phi3.5 onnx graph multiple slice nodes missing starts or ends attribute," üêõ Describe the bug  In the exported graph, multiple slides nodes are missing starts and/or ends attributes !image !image Because starts and ends attributes are required. When running inference session  , I get below error onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Nonzero status code returned while running Slice node. Name:'/Slice' Status Message: slice.cc:195 FillVectorsFromInput Starts must be a 1D array  Versions PyTorch version: 2.4.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.24.0 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.164.11.cm2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.6.68 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 550.54.15 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_engines_precompiled.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_engines_runtime_compiled.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_graph.so.9.4.0 /usr/lib/x86_64linuxgnu/libcudnn_heuristic.so.9.4.0 /usr",2024-10-22T20:35:47Z,module: onnx triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/138637,"It is working for me with ``torch.onnx.export(model, inputs, ""repro.onnx"", dynamo=True)`` (nightly build). Which version of transformers are you using?","I used stable version pytorch and without dynamo=True. Let me try that For transformers, I used transformers             4.45.2",It looks like they are just dynamic from the outputs of other nodes?,Yes it's dynamic based on shape of a tensor.,Sounds good. The issue is resolved I guess?
transformer,ILP for auto FSDP wrapping,"  CC(ILP for auto FSDP wrapping) This PR presents a mixed integer linear programming (MILP) formulation that can be utilized to determine, under a memory budget, which modules to wrap as FSDP units. Similar to the auto SAC MILP introduced in https://github.com/pytorch/pytorch/pull/137908, the MILP uses information collected from MemTracker, Runtime Estimator, and SAC Estimator, introduced in these PRs: * https://github.com/pytorch/pytorch/pull/124688 * https://github.com/pytorch/pytorch/pull/134243 * https://github.com/pytorch/pytorch/pull/135208 Endtoend example and its sample output:   ",2024-10-22T20:31:46Z,oncall: distributed Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,1,13,https://github.com/pytorch/pytorch/issues/138635,thanks for moving fast on this, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: linuxbinarylibtorchprecxx11 / libtorchcpusharedwithdepsprecxx11build / build, linuxbinarylibtorchcxx11abi / libtorchcpusharedwithdepscxx11abibuild / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / workflowchecks / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""revert"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, Reverting PR 138635 failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch revert noedit e92208c919cf1a491f1f0ccdf4b14139308f68f1` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job ,"Clicked on the ""squash and merge"" button :( Reopened PR here  https://github.com/pytorch/pytorch/pull/140298"
transformer,`(*bias): last dimension must be contiguous` when running compiled SDPA on length 1 tensors," üêõ Describe the bug When using `torch.compile` on `torch.nn.functional.scaled_dot_product_attention` with length 1, a RuntimeError occurs during the backward pass:   The code is heavily modified from `BertSDPASelfAttention`, available from the transformers library: https://github.com/huggingface/transformers/blob/b54109c7466f6e680156fbd30fa929e2e222d730/src/transformers/models/bert/modeling_bert.pyL357L455 I suspect that the combination of `view`, `permute`, and `attention_mask` is to blame here. Modifying any one of these eliminates the bug. Tested on CUDA 12.4 w/ PyTorch `2.4.0`, `2.4.1`, `2.5.0`, and nightly `2.6.0.dev20241017+cu124`.  Versions PyTorch version: 2.6.0.dev20241017+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.5 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU Nvidia driver version: 565.90 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                  ",2024-10-18T07:40:36Z,triaged oncall: pt2 module: inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/138317,Error only happens with inductor backend.,"Hi guys, I could repro this on my local, is anyone work on this issue now, if there is not, I would like to take a look, thanks!",Nobody as far as I'm aware of  feel free to work on it and submit a PR!,"I tried to investigate on this issue, IIUC there is some unexpected behavior from `inductor/triton`. From the log I saw the `bias` tensor for the attention backward calculation looks like:  Its shape and strides is `size=[8, 1, 1, 1], stride=[8, 8, 8, 1]`, and then the compiler tried to reinterpret it as `size=[8, 8, 1, 1], stride=[8, 0, 0, 0])`. In the `attention_backward` method, it checks for the `""last stride""` for the `bias` tensor is `1`, which means the tensor is continuous. I though in such a case, if the `size` of last dim is `1`, we didn't have to check for the stride, so I drafted a PR to do it. I'm not clear about why `inductor/triton` would like to reinterpret it as `size=[8, 8, 1, 1], stride=[8, 0, 0, 0])`, and I didn't know is it an expected behavior, if this behavior didn't meet our expectation, I thought we could try to fix this issue from this side too, I'm happy to take a further study in these components. What do you think of this method? If there is anything I did wrong, please feel free to correct me, thank you!",cc'ing inductor people  ,". i am also going to make similar pr for inductor.  If you look at the generated output code we are doing a view on an input, which causes us to change stride. ","Thank you for the quick reply! Your PR fixed the issue well! And It is also a good entry point for me to study `inductor`. IIUC, there are still some redundant check in the kernel, I leaved a comment in my PR about it, how did you think of them, shall we remove them? PTAL, thanks!"
transformer,Torch 2.5.0 vs 2.4.1: torch nested in BetterTransformer fastpath implementation, üêõ Describe the bug Thanks for the new torch release!  I am using torch nested in the bettertransfromer implementation. https://pytorch.org/blog/abettertransformerforfasttransformerencoderinference/ Here is the exact code where torch compile breaks: https://github.com/huggingface/optimum/blob/1e5014e70f17e0437c4b0a7f4e65e170688d8ab0/optimum/bettertransformer/models/encoder_models.pyL203 and https://github.com/huggingface/optimum/blob/1e5014e70f17e0437c4b0a7f4e65e170688d8ab0/optimum/bettertransformer/models/encoder_models.pyL146   Versions Torch=2.5.0 Torch=2.4.1 ,2024-10-17T22:23:40Z,needs reproduction triaged module: nestedtensor oncall: pt2,open,0,10,https://github.com/pytorch/pytorch/issues/138274,Maybe related:   CC(torch.compile does not support strided NestedTensor),Do you have a repro?,The reproduction (i pushed two docker images for you)  docker on infinity torch2.4 image with all deps:  torch2.5. poetry lock with just torch 2.4.1 to 2.5.0 bumped. all other dependencies are frozen.   Minimal repro Interactive ssh into container:  ,"   Reproduction above, can you update the triage status?"," I'm not the maintainer, so can't update the status...",Can you check if you still repro on nightly PyTorch? And can the repro only run one setting combination of bettertransform/compile? ,"It's still is the same error on the `torch2.6.0.dev20241112+cu121`. bettertransform + compile: breaks with error above Only bettertransform (torch.sdpa, mask, no nested): works Only compile: works No bettertransform, no compile: works","Hi , sorry for the delay on this. BetterTransformer currently uses an older version of nested tensor (AKA NST or ""nested strided tensor"") which is not compatible with torch.compile. This is in contrast with the newer NJT AKA ""nested jagged tensor"" that is supported within torch.compile. I suggest disabling the BetterTransformer fast path when utilizing torch.compile for now. This flag can be used to do so: `torch.backends.mha.set_fastpath_enabled(False)`. We could swap out use of NST for NJT within BetterTransformer when compiling; is this something that would be useful to you?"," btw is the difference between NST and NJT layouts explained in the docs anywhere? in https://pytorch.org/docs/stable/nested.html i found no detailed explanation besides notes that two layouts are supported it also suggests that `torch.layout` instance is expected, but the link https://pytorch.org/docs/stable/tensor_attributes.htmltorch.layout does not mention jagged",NJT is the normal layout you would expect. NST is a weird one where we maintain explicit size/stride metadata for each inner tensor (lol). NST predates NJT though
transformer,Torch Dynamo support for Flux T5 model," üêõ Describe the bug I'm using the script below to export the Flux T5 model to ONNX using torch.onnx.dynamo_export(). However, I run into an error due to missing support for `fused_layer_norm_cuda.PyCapsule.rms_forward_affine`. The script below can be used to reproduce the issue:  The error is pasted below:   Versions transformers               4.42.2 diffusers                  0.31.0.dev0 torch                      2.5.0a0+b465a5843b.nv24.9 (Nvidia NGC 24.09 PyTorch container)",2024-10-17T10:42:38Z,module: onnx triaged,open,0,2,https://github.com/pytorch/pytorch/issues/138196,"Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated report if there is an error. Thanks!","I was able to successfully export your model with ``torch.onnx.export(model, (dict(input_ids=inputs),), dynamo=True)``. You should use the nightly build."
transformer,Torch Dynamo support for Flux Transformer model," üêõ Describe the bug I'm using the script below to export the Flux Transformer model to ONNX using torch.onnx.dynamo_export(). However, I run into a TypeError relating to an attribute type.  The script below can be used to reproduce the issue:  The error is pasted below:   Versions transformers              4.42.2 torch                     2.6.0.dev20241016+cu124 diffusers                 0.31.0.dev0",2024-10-17T10:23:08Z,module: onnx triaged,open,0,14,https://github.com/pytorch/pytorch/issues/138195,"Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated report if there is an error. Thanks!", Is fake mode exposed through this integrated API? I don't see a flag for it. If not what's the recommended way to use it now?,It should. You may refer to https://pytorch.org/docs/main/onnx_dynamo.htmltorch.onnx.enable_fake_mode,"Thanks for the pointer  . As per v2.4 docs, ""A ONNXFakeContext object that must be passed to dynamo_export() through the ExportOptions.fake_context argument."" This statement is missing in v2.5 docs. I'm guessing this means that using the context manager with `torch.onnx.export` is sufficient and the ONNXFakeContext need not be passed as an argument during export. Hope my understanding is accurate",That's right!,Sorry there may be a change to this. FakeMode is by default enabled for tensor propergation. Could you explain what your main goal is when using fake mode?,The main goal is to reduce VRAM usage during ONNX export. How can I enable fake mode for this objective?,"Does the VRAM usage happen during model initialization, or model export? We are going to create a tutorial on how to reduce memory usage when the model is being initialized so that the weights are not loaded; the export process itself is already running on fake tensors and requires minimal memory space.","The Flux Transformer is 23GB but the peak usage during the export step is 46GB. I haven't captured the usage for model load and model export separately yet.  > the export process itself is already running on fake tensors and requires minimal memory space. Does this apply to the torchscriptbased exporter by default, too, or is there an arg to enable this for TSbased export? Looking forward to the docs for optimizing memory usage during model load as well. Please feel free to share snippets here for quicker turnaround ","Only the torch.onnx.export(, dynamo=True) option will have an optimized memory usage for now. There is no plan to enable it for the torch script based export (the models are traced differently).",Sounds good. Please share the usage for loading models without the weights even if the tutorial is not ready yet. We can try it and provide feedback,"> Sounds good. Please share the usage for loading models without the weights even if the tutorial is not ready yet. We can try it and provide feedback Here is a simply example, with latest torchnightly version, to help calling the latest dynamo enabled exporter to export your model. This latest exporter will leverage FakeTensorMode internally to reduce the memory usage.  After execute above code, you will get a file like ""dynamo_exporter_xxxx.pickle"" which will save the cuda memory uage during the export process. Launch this page and drag and drop this .pickle file on this web page, you will see the details of memory usage.","Thanks for the example z . Just to clarify, the model is still initialized with random weights. However, the weights aren't loaded from disk.  Is this what you meant in  CC(Torch Dynamo support for Flux Transformer model)issuecomment2427244659  ?  We still will have the weights in memory, but the randomly initialized ones?","This may not impact my original use case of Flux transformer as `low_cpu_mem_usage` is True by default during `model = FluxTransformer2DModel.from_pretrained(model_dir, subfolder=""transformer"", torch_dtype=torch.float16).to(device)` . This only loads the pretrained weights and does not initialize the weights"
transformer,DISABLED test_transformer_backend_inductor_fullgraph_True (__main__.TestFullyShardCompile),This test was disabled because it is failing in CI machine upgrade (recent examples). ,2024-10-17T01:25:37Z,oncall: distributed skipped,closed,0,2,https://github.com/pytorch/pytorch/issues/138147, seems need to guard this on the availability of bf16?,Closing because I am doing wholeclass skip in test file. See  CC(DISABLED __main__.TestFullyShardCompile)issuecomment2418441754
transformer,Error while exporting wav2vec2 speech emotion recognition, üêõ Describe the bug Code to reproduce the problem  Error Logs   Versions  ,2024-10-16T21:26:44Z,oncall: pt2 oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/138120,This is resolved by adding  `hidden_states = hidden_states.clone()` in  https://github.com/huggingface/transformers/blob/main/src/transformers/models/wav2vec2/modeling_wav2vec2.pyL1112,"Hi, this is likely caused by the following: currently in export, we do not allow mutation to the output of `aten.to` and dropout (or any of its alias). This is because the aliasing behavior of them depends on runtime (for example for aten.to, it's sometimes a copy, sometimes an alias).  You fix of adding a clone is the right thing to do now. We are working on relaxing this restriction for aten.to, and I can let you know after we landed the fix to relax this restriction."
transformer,Error while exporting BLIP, üêõ Describe the bug Steps: 1) `pip install timm==0.4.12 fairscale transformers` 2) `git clone https://github.com/salesforce/BLIP.git` Code for reproducing the problem  Error   Versions  ,2024-10-16T20:41:07Z,oncall: pt2 oncall: export,open,0,9,https://github.com/pytorch/pytorch/issues/138111,This is similar to  CC([export] Cannot mutate tensors with frozen storage) This is resolved by adding `text.input_ids = text.input_ids.clone()` in https://github.com/salesforce/BLIP/blob/main/models/blip.pyL111,"Hi , I seem to run into a number of similar problems that you have during your investigation, including this issue and  CC(Error while exporting depth estimation model `intel-isl/MiDaS`). We both seem to be trying to run nonstrict export on a large set of models. Is this part of an effort to make nonstrict torch.export more stable? Discover broken edgecases? Or is there another motivation? I'm trying to gauge how much effort my team should dedicate to patching over these kinds of issues on our side. I'm also wondering if there is some public issue that is tracking the status of these explorations and the maturity of torch.export.",Hi  Would love to understand what your team is working on and see how we can collaborate. Will reach out to you,"Sounds good, we would love to collaborate!","Hi, this is likely caused by the following: currently in export, we do not allow mutation to the output of `aten.to` and dropout (or any of its alias). This is because the aliasing behavior of them depends on runtime (for example for aten.to, it's sometimes a copy, sometimes an alias).  Your fix of adding a clone is the right thing to do now. We are working on relaxing this restriction for aten.to, and I can let you know after we landed the fix to relax this restriction."," Hi, any progress on this matter? It is preventing us from exporting many xnnpack quantized models :sweat_drops:  This PR is the one with more details about the problem:  CC(Cannot export a quantized ResNet-18 model)","> RuntimeError: cannot mutate tensors with frozen storage `While executing %add_ : [num_users=1] = call_functiontarget=torch.ops.aten.add_.Tensor, kwargs = {})` I have same message with an export from pytorch `ao`","  We're still working on it, but it might take some time before the constraint is relaxed. For now, you probably can bypass the error by adding a clone somewhere. This is similar to this issueissuecomment2417997285).", Sometimes it is hard to find the offending tensor that needs to be cloned. Can you take a look at https://github.com/pytorch/ao/issues/1381issuecomment2521475118 ?
transformer,ILP for Auto SAC (Selective Activation Checkpointing),"  CC(ILP for Auto SAC (Selective Activation Checkpointing)) This PR presents a mixed integer linear programming (MILP) formulation that can be utilized to determine, under a memory budget, which modules to apply activation checkpointing (AC) and the amount of activation memory that should be discarded for each module. The MILP uses information collected from MemTracker, Runtime Estimator, and SAC Estimator, introduced in these PRs: * https://github.com/pytorch/pytorch/pull/124688 * https://github.com/pytorch/pytorch/pull/134243 * https://github.com/pytorch/pytorch/pull/135208 Endtoend example and its sample output:   ",2024-10-14T17:03:54Z,oncall: distributed Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,3,8,https://github.com/pytorch/pytorch/issues/137908, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.12clang10 / test (default, 4, 4, linux.4xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalrocm6.2py3.10 / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Fixed issue with nn.Transformer().generate_square_subsequent_mask(),Fixed issue where nn.Transformer().generate_square_subsequent_mask() doesn't respect set_default_device() and set_default_dtype(). Fixes CC(`nn.Transformer().generate_square_subsequent_mask()` doesn't care `set_default_device()` and `set_default_dtype()`),2024-10-09T22:00:00Z,open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/137654," label ""topic: not user facing""",Apologies if I have done anything wrong in this pull request.  I am new to open source and am happy to make changes as needed :), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Fixes issue where nn.Transformer().generate_square_subsequent_mask() doesn't respect set_default_device() and set_default_dtype(),Fixes CC(`nn.Transformer().generate_square_subsequent_mask()` doesn't care `set_default_device()` and `set_default_dtype()`),2024-10-09T20:50:53Z,open source topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/137640,:white_check_mark:login: FinlaySanders / (b37d1bcd92fabb72b8c34c92a7f41926a46d2c17)The committers listed above are authorized under a signed CLA.," label ""topic: not user facing"""
transformer,RuntimeError:  Expression of type - cannot be used in a type expression: __torch__.transformers_modules.code-5p-110m-embedding.modeling_codet5p_embedding.___torch_mangle_1368.CodeT5pEmbeddingModel ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE," üêõ Describe the bug I use the model: https://huggingface.co/Salesforce/codet5p110membedding and with `torch.jit.load`, it makes error:  the error message is:   Versions Collecting environment information... PyTorch version: 2.1.2 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 3.28.4 Libc version: glibc2.17 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.10.112005.ali5000.al8.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA L20 Nvidia driver version: 535.161.08 cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.9.0 /usr/lib64/libcudnn_adv_infer.so.8.9.0 /usr/lib64/libcudnn_adv_train.so.8.9.0 /usr/lib64/libcudnn_cnn_infer.so.8.9.0 /usr/lib64/libcudnn_cnn_train.so.8.9.0 /usr/lib64/libcudnn_ops_infer.so.8.9.0 /usr/lib64/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] torch==2.1.2 [pip3] torchdct==0.1.6 [pip3] torchaudio==2.1.2 [pip3] torchvision==0.16.2 [pip3] triton==2.0.0 [conda] blas                      1.0                         mkl   [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] libjpegturbo             2.0.0                h9bf148f_0    pytorch [conda] mkl                       2023.1.0         h213fc3f_46344   [c",2024-10-03T04:58:33Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/137252
transformer,Setting `nn.TransformerEncoder()` and `nn.TransformerDecoder()` to `nn.Transformer()` doesn't get any errors with `nn.Transformer()`'s arguments having wrong type values, üêõ Describe the bug Setting nn.TransformerEncoder() and nn.TransformerDecoder() to nn.Transformer() doesn't get any errors with `nn.Transformer()`'s arguments having wrong type values as shown below:   Versions  ,2024-10-02T17:28:55Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/137192
transformer,`nn.Transformer().generate_square_subsequent_mask()` doesn't care `set_default_device()` and `set_default_dtype()`," üêõ Describe the bug nn.Transformer().generate_square_subsequent_mask() doesn't care set_default_device() and set_default_dtype(), getting `cpu` and `float32` instead of `cuda` and `float64` as shown below. *I know I can set `device` and `dtype` manually to `nn.Transformer().generate_square_subsequent_mask()`:  In addition, torch.tensor() cares `set_default_device()` and `set_default_dtype()` as shown below:   Versions  ",2024-10-02T15:58:24Z,high priority triage review good first issue module: correctness (silent),closed,0,4,https://github.com/pytorch/pytorch/issues/137186,Marking hipri tentatively for silent incorrectness,Shouldn't be too hard to fix probably,I'd love to contribute.  Will have a look :),Have found the problem.  Will submit a pull request shortly.
transformer,`batch_first` argument of `nn.Transformer()` should be `True` by default because it's not convenient to get a warning with the default settings," üêõ Describe the bug nn.Transformer() with the default settings gets the warning as shown below because `batch_first` argument is `False` by default:  > UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)   warnings.warn(f""enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}"") So `nn.Transformer()` with `batch_first=True` doesn't get the warning as shown below:  My suggestion is `batch_first` should be `True` by default because it's not convenient to get the warning with the default settings.  Versions  ",2024-10-02T11:30:17Z,module: nn triaged,open,2,1,https://github.com/pytorch/pytorch/issues/137173,"thank you by this !!! batch_first=True is necessary to avoid the warning, God bless you"
transformer,Can not translate Llama model to MLIR, üêõ Describe the bug I use the following script to translate Llama27bhf to MLIR. It failed in the translation to torchscript.  backtrace   Versions  ,2024-09-29T10:19:54Z,oncall: pt2 export-triaged oncall: export,open,0,6,https://github.com/pytorch/pytorch/issues/136948,"According to the backtrace, it seems the bug happended when translating python code to torchscript."," I haven't reproed yet, but are you sure the model runs with sample inputs `torch.randn(1, 8)` in eager mode? The error sounds like the sample input being a float tensor is crashing the embedding op, which should take in token ids, which are integer/long tensors?", Sorry for the late reply. I had a vacation last week. comes from the following python code.  I dump the shape of the inputs and hard code it when export Llama model.,  I change the input tensor type to torch.long. But it still failed.  Here is the backtrace. ,I can reproduce with the following code. ,
transformer,Bias gradient calculation for NJT linear backward,  CC(Bias gradient calculation for NJT linear backward) Previously NYI   needs it for Transformers. Fixes CC([NJT] Gradients for bias do not get populated for nn.Linear),2024-09-25T18:49:54Z,Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/136660, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/jbschlosser/179/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/136660`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,torch.export support for the latest transformers `DynamicCache` as input,Hugging Face `transformers` is moving to use the `DynamicCache` class as part of the model inputs for the kv cache values. Currently `torch.export` will complain that it is not a tensor. So all models that take `DynamicCache` as input will not be exportable. This affects torch.onnx and other exporters dependent on torch.export alike.   Will raise an error  ,2024-09-24T23:28:48Z,oncall: pt2 export-triaged oncall: export,open,0,1,https://github.com/pytorch/pytorch/issues/136582," The same issue is addressed in transformers repo already. See https://github.com/huggingface/transformers/pull/32830, where we create an integration point to ""Export to ExecuTorch"". Basically, it's hiding the cache object from the graph I/O to make it exportable. For ExecuTorch, we only support StaticCache atm as there is no way to resize the size when putting the exported artifacts ondevice. However, I believe it can work with DynamicCache as well if you have a usecase where you do want to export with DynamicCache"
transformer,FSDP2 error loading `PreTrainedModel`'s `torch.Tensor` state_dict into `DTensor`," üêõ Describe the bug I'm trying to follow the instructions to efficiently load Hugging Face models from `torchtitan`'s docs for FSDP1 > FSDP2: MetaDevice Initialization. When I get to the last step of actually loading the model's weights, I get an error about trying to copy `torch.Tensor` into the FSDP2 `DTensor`s.   Coming from: https://github.com/pytorch/pytorch/blob/172ecf78b75f55948f9102c85e72314d1f8f1a22/torch/distributed/tensor/_dispatch.pyL458L473 `accelerate.load_checkpoint_and_dispatch` is just calling `load_state_dict` under the hood. How are users expected to initialize from pretrained weights in this scenario?     Versions Collecting environment information... PyTorch version: 2.4.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.0119genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 535.183.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(",2024-09-19T21:48:49Z,triaged module: fsdp,open,0,5,https://github.com/pytorch/pytorch/issues/136336,"I will be out today, but I will try to look at this beginning of next week.","For terminology, FSDP considers two kinds of state dicts:  Full state dict: values are unsharded `torch.Tensor`s  Sharded state dict: values are sharded `DTensor`s (sharded on dim0) FSDP1 allows saving and loading these two kinds of state dicts configured via `StateDictConfig`. FSDP2's parameter representation directly matches the sharded state dict.  Calling `model.state_dict()` on an FSDP2sharded model returns a sharded state dict without any computation or communication.  Conversely, FSDP2 only supports loading a sharded state dict. All this means is that the conversion between full and sharded state dicts should happen outside of FSDP2. One way to do this conversion is to load the full state dict on rank 0 and iteratively broadcast and shard it on all ranks; this is to avoid loading the same full state dict in CPU RAM multiple times per host. Here is an example of how to do this: https://github.com/pytorch/pytorch/blob/3bc073d7280d36eec2af3b1969c165b336a8ecae/test/distributed/_composable/fsdp/test_fully_shard_init.pyL701 You may customize this logic further to support specific constraints. For example, torchtune has this logic (code) with some special support for `NF4Tensor`s.",It makes sense to use the sharded meta tensors of the model to inform how to distribute a loaded nonsharded state dict. But how should I distribute loaded nonsharded optimizer state tensors? The optimizer state starts out empty so I don't have any sharded meta tensors to use to inform how to shard.,"For optimizer state dict, if you want to roll your own solution, then you could base it off the model's sharding. Otherwise, I think distributed state dict provides utilities for this. cc:   for example API usage","  For both model state and optimizer state (optimizer state initialization), you can use distributed state dict. `get_optimizer_state_dict` would initialize the states for you. `set_optimizer_state_dict` would load the optim state dict into the optimizer.  Here is a test case(https://github.com/pytorch/pytorch/blob/main/test/distributed/_composable/test_composability/test_2d_composability.pyL410) for loading a full state dict into a 2D parallelized (FSDP2+TP) model. Usage should be identical.  For more details, you could refer to the doc here: https://pytorch.org/docs/stable/distributed.checkpoint.htmltorch.distributed.checkpoint.state_dict.get_state_dict"
transformer,Flex Attention Extremely Slow," üêõ Describe the bug I try to use flex attention in huggingface transformer, only to find it very slow. Compared to the sdpa implementation, flex attention is about 45 times slower, but it does save the CUDA memory. Tested on RTX3090, A6000 and A100. Here is the example code: https://gist.github.com/whyinShanghaitech/8b8205f98568c6741a2e38dfcdb9d362 I have no idea what is happening. Is this normal? Can anyone reproduce this? Or this problem is related to huggingface transformers?  Versions  ",2024-09-18T11:51:57Z,oncall: pt2 module: higher order operators module: pt2-dispatcher module: flex attention,closed,0,2,https://github.com/pytorch/pytorch/issues/136261,"It looks like you are using flex_attention in eager mode. Please use compilation mode instead. i.e., `flex_attention = torch.compile(flex_attention)` ","> It looks like you are using flex_attention in eager mode. Please use compilation mode instead. i.e., `flex_attention = torch.compile(flex_attention)` Thank you so much! This exactly solves the problem."
transformer,Performance regression in torch.compile," üêõ Describe the bug Hi fangintel  The compiled model's generation latency is slower than the eager mode. Regression happens from torch 0806  torch 0807. transformers version: 4.44.2 Script:   Versions Collecting environment information...                                                                                                                                                                                 PyTorch version: 2.5.0.dev20240806+cpu                                                                                                                                                                                Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Rocky Linux release 8.10 (Green Obsidian) (x86_64) GCC version: (condaforge gcc 12.3.013) 12.3.0 Clang version: Could not collect CMake version: version 3.27.9 Libc version: glibc2.28 Python version: 3.10.14  (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64bit runtime) Python platform: Linux4.18.0553.5.1.el8_10.x86_64x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              224 Online CPU(s) list: 0223 Thread(s) per core:  2 Core(s) per socket:  56 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU family:          6 Model:               143 Model name:          In",2024-09-18T06:01:53Z,oncall: pt2 oncall: cpu inductor,open,0,16,https://github.com/pytorch/pytorch/issues/136254,"Suspected guilty commit is https://github.com/pytorch/pytorch/commit/de00c7958301ce81b9716bdef5731ed40d4d14ca. In this model, it will cause recompilations.  Could you please help check this ?", https://github.com/pytorch/pytorch/pull/136516 might fix this. I will work on it tomorrow., I tried https://github.com/pytorch/pytorch/pull/136516. The recompilation is fixed.,Hi . It seems https://github.com/pytorch/pytorch/pull/136516 can not fix this issue when it is merged into main branch. Applying your PR on https://github.com/pytorch/pytorch/commit/de00c7958301ce81b9716bdef5731ed40d4d14ca can solve recompilation., I can take a look. But can you elaborate? You mention that applying it on the above commit solves recompile. But that commit is already on main.," Ah, were you suggesting that you want to be at 2.5 release branch and manually apply the changes? In that we might need both commits. Let me check if I can backport these fixes."," Just to be sure I am writing down my analysis 1) The base commit (or the commit de00c79) that you mentioned. We don't need that in 2.5 release candidate. That PR was reverted. And we relanded that in https://github.com/pytorch/pytorch/pull/134136. This PR/commit is already present in the 2.5 release branch !image 2) Now, I am sending a backport for https://github.com/pytorch/pytorch/pull/136516 in the release branch here  https://github.com/pytorch/pytorch/pull/137025. If that is merged, then you won't see extra recompiles with 2.5 release branch.", Thanks for your fixing. Sorry for the late reply. I'm on vacation. There may be commits on the main branch that caused the regression before your fix https://github.com/pytorch/pytorch/pull/136516. We will verify it next.,"Hi  , please reopen this issue as it's not been resolved in the main branch. Thanks.","Can you share TORCH_LOGS=guards,recompiles for the  fast and slow case? "," It seems that even with the fix https://github.com/pytorch/pytorch/pull/136516, the commit 042b733ddd7d7eda5aa01e8f919b2847a7fb6692  still cause the recompilation.  "," can you share TORCH_LOGS=guards,recompiles for the old and new case. You can use backend=eager to speed it up.", We collected the logs and sent them to you via email.,"Hi  . Do you have any updates? BTW, `backend=eager` no helps.","I wonder if this is related, I am seeing a massive performance falloff (90% slowdown) in sunoai/bark model: https://github.com/sunoai/bark/issues/609", Is there any update on this issue ? Thanks
transformer,[BE] Make `NestedTensorTransformerFunctions.cu` compilable without warnings,Before the change compilation produced following warnings:  after it compiled without a warning Fixes ISSUE_NUMBER,2024-09-17T20:34:28Z,Merged release notes: cuda topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/136222," merge f ""builds + lint are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,"[inductor] gradient numeric discrepency when fusing Transfromer (Embedding, LayerNorm, Linear)"," üêõ Describe the bug In Transformer, we have 3 basic modules (Embedding, LayerNorm, Linear) (see `EmbNormLinear` below). Graident numerics are different betwen `torch.compile(backend='inductor')` and eager The numerics are important because I want to compare float8 numerics with high precision numerics. If high precision numerics itself is not accurate, it would be hard to attribute numeric loss repro: `TORCH_LOGS=""output_code"" CUBLAS_WORKSPACE_CONFIG=:4096:8 TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 python test_emb_numerics.py` * bisecting on PT2 stack, backend='aot_eager' have exact numerics. Just backend='inductor' yield different numerics * bisecting on code, removing 1 of the modules will yield exact numerics * bisecting on forward/backward, forward has exact numerics, just backward is different. There is no optimizer.step    Versions `python c ""import torch;print(torch.__version__)""`: 2.5.0a0+git94d2471 ",2024-09-15T23:25:14Z,oncall: pt2 module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/136121, do you think this deserves to be hipri (and discussed at triage review)?,torch.compile will not produce bitwise equal numerics. when switched to torch.testing.assert_allclose the repro passes. The diverence here is very small: `AssertionError: i=0 774.6301879882812 vs 774.6292114257812`
transformer,xpu: set of aten ops are missing for Huggingface Transformers,"With: * https://github.com/pytorch/pytorch/commit/cd472bb1e368a711a2bd34d5671c77dab336d312   * Plus this applied: https://github.com/pytorch/pytorch/pull/135567 * https://github.com/intel/torchxpuops/commit/c6981a238cdaf93774b1c6a3550a436f530f4736 * https://github.com/huggingface/accelerate/commit/4b4c036933f7c50fe3a7027b0380fcec53c6975e * https://github.com/huggingface/transformers/commit/d70347726577e9823e35c11883e98c5b2c520b37 I was running Huggingface transformers models tests, i.e.:  Overall: * **Few aten ops not implemented for XPU (this issue is about this)** * `PYTORCH_ENABLE_XPU_FALLBACK=1` was not needed at all (which is good) *  CC(xpu: huggingface levit test_retain_grad_hidden_states_attentions test hangs on exit on PVC)  hang happens in levit tests on Intel PVC (only in levit, other models ran fine) * https://github.com/huggingface/transformers/pull/33485  few HF tests were not enabled for HW devices (cuda/xpu wise), this PR adds support The following aten ops are not yet implemented for XPU and affect HF tests. Please, implement:  [x] `aten::_ctc_loss`, https://github.com/intel/torchxpuops/pull/925  [x] `aten::_ctc_loss_backward`, https://github.com/intel/torchxpuops/pull/925  [ ] `aten::_fft_c2c`, https://github.com/intel/torchxpuops/pull/526  [ ] `aten::_fft_c2r`  [ ] `aten::_fft_r2c`  [x] `aten::index_copy.out`, https://github.com/intel/torchxpuops/pull/793  [x] `aten::linspace.out`, https://github.com/intel/torchxpuops/pull/850  [x] `aten::take`, https://github.com/intel/torchxpuops/pull/868  [x] `aten::upsample_bicubic2d_backward.grad_input`, https://github.com/intel/torchxpuops/pull/914  [x] `aten::xlogy.OutTensor`, h",2024-09-14T00:04:50Z,triaged module: xpu,open,4,4,https://github.com/pytorch/pytorch/issues/136065,All remaining missing ones are on PT2.6 task list.," Please refresh the issue status, if any updates.","As of https://github.com/pytorch/pytorch/commit/d08dbd0436637a04098151b0e03c69dcdab3f44e, the following ops are not yet available in pytorch build:  [ ] `aten::_fft_c2c`, https://github.com/intel/torchxpuops/pull/526  [ ] `aten::_fft_c2r`  [ ] `aten::_fft_r2c` The rest of ops are implemented and available and should appear in PT 2.6 all except `aten::upsample_bicubic2d_backward.grad_input` which made it into PT 2.5.",Huggingface Transformers v4.46.0 introduced usage of `aten::_linalg_eigvals` via a call to `torch.linalg.eigvals` which is not implemented for XPU and requires `PYTORCH_ENABLE_XPU_FALLBACK=1`. I filed separate CC(xpu: implement aten::_linalg_eigvals for XPU backend (affecting HF Transformers v4.46.0 and later)) request for this operator to be added.
transformer,Add _addmm_activation to lower precision cast policy on AutocastCPU,"Fixes CC(autocast to float16/bfloat16 fails on transformer encoder (in `eval` mode)). Add `_addmm_activation` to lower precision cast policy on AutocastCPU. `_addmm_activation`  https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/transformer.cppL39 of `transformer_encoder_layer_forward` may throw `RuntimeError: mat1 and mat2 must have the same dtype, but got BFloat16 and Float` when autocast is enabled, as `_native_multi_head_attention` is put in lower data type cast policy https://github.com/pytorch/pytorch/pull/107674 and `_addmm_activation` may encounter mixed data types. ",2024-09-13T03:34:15Z,triaged open source module: amp (automated mixed precision) Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/135936, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,LLaMA v3.1 on MPS backend breaks in BinaryOp mps::add_sub_lerp_template," üêõ Describe the bug Error: `failed assertion `[MPSNDArray initWithDevice:descriptor:isTextureBacked:] Error: total bytes of NDArray > 2**32'` Requires similar tiling approach to BinaryOp that was done for the batch matmul op here: PR CC(Enable batch matmul for result sizes > 2**32 the tensor can be split along batch axis) Reproduces with running LLaMA v3.1 on the current nightly PyTorch. Script to repro   Versions Collecting environment information... PyTorch version: 2.5.0.dev20240909 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 15.0 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: version 3.30.2 Libc version: N/A Python version: 3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Ultra Versions of relevant libraries: [pip3] numpy==2.0.1 [pip3] torch==2.5.0.dev20240909 [pip3] torchaudio==2.5.0.dev20240909 [pip3] torchvision==0.20.0.dev20240909 [conda] numpy                     2.0.1                    pypi_0    pypi [conda] torch                     2.5.0.dev20240909          pypi_0    pypi [conda] torchaudio                2.5.0.dev20240909          pypi_0    pypi [conda] torchvision               0.20.0.dev20240909          pypi_0    papi ",2024-09-10T18:25:32Z,high priority triaged module: mps,open,0,2,https://github.com/pytorch/pytorch/issues/135598, do you know how much memory one would need to reproduce it? I've tried to repro it machine with 32Gb of memory and all I got was: ,Hi  ! This was reproduced this on a 128GB machine but since its a 8B model it should run on a 64GB at least. TBH I was kinda assuming it would have worked on a 32GB machine as well but evidently that's not the case here.
transformer,Discussion about the testing methods in test_transformers.py after math backend always uses fp32," üêõ Describe the bug The current method to estimate the numerical errors in test_transformers.py is based on the difference b/w results from current precision and higher precision tensors https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL3107L3108 https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL3118L3122 https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL132L133 https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/test/test_transformers.pyL141L145 However, with PR 128922, the math backend always casts the input tensors to fp32 from 16bit datatypes for better accuracy: https://github.com/pytorch/pytorch/blob/bc1b8f094d24de27432f4c29f0729e85a6b5ba63/aten/src/ATen/native/transformers/attention.cppL787L801 Although PR 128922 fixed the tests on NV GPUs by raising the fudge factors, the assumptions of current numerical error estimation method do not hold anymore because `ref` and `_lp` tensors are fundamentally doing the same computation, and the errors between the two tensors are probably not measuring something meaningful for the purpose of estimating the SDPA's numerical errors. More concretely. I have inspected the output tensors of UT `TestSDPACudaOnlyCUDA.test_flash_attention_vs_math_ref_grads_batch_size_8_seq_len_q_2048_seq_len_k_2048_head_dim_8_is_causal_False_dropout_p_0_0_float16_scale_l1_enable_gqa_False_n_heads1_cuda_float16` The difference b/w the gold and reference is `4.76837158203125e07` for `grad_query`, and `3.0510127544403076e05`",2024-09-10T17:15:07Z,triaged module: sdpa,open,0,3,https://github.com/pytorch/pytorch/issues/135590,I wrote up my thoughts on the testing strategy here: https://github.com/pytorch/pytorch/pull/128922discussion_r1695568715 I think the goal/concencus from the sdpa_math change was  to provide a decomposed func that should more closely align to the casting behavior of the fused kernels.  I dont really understand why the AMD tests are orders of magnitude higher in terms of devotion  ,"> I think the goal/concencus from the sdpa_math change was to provide a decomposed func that should more closely align to the casting behavior of the fused kernels With new sdpa_math, the testing method should be improved as well because the only difference b/w `out` and `out_lp` is when the input tensors are casted from low precision to fp32 (note the golden is never fp64 unless the input is fp32). `out` does it in Python through `query_key_value_clones` and `out_lp` does it in sdpa_math's C++ code. Actually it's more surprising that `out` and `out_lp` are not matching precisely.","> Actually it's more surprising that out and out_lp are not matching precisely. This is the crux of it for me. Ideally every fudge factor should be 1. And potentially the fudge factor might need to be bumped up slightly due to fp operation reordering and iterative softmax, but that is not the case. I want to drill down on why that is "
transformer,"BUG: `torch.cuda.is_available()` returns `False` in certain torch, CUDA and driver version"," üêõ Describe the bug Hi, I'm trying to create a Docker container with the following (**minimal reproducible**) CUDA `12.4.1` Dockerfile (host info: Driver Version: `550.107.02`     CUDA Version: `12.4`):  This just create a basic `nvidia/cuda:12.4.1cudnndevelubuntu22.04` image and install `conda` and `pip`. Then, I run the container with the following command:  Then, inside the container, I install the latest stable torch (`2.4.1`) by:  After that, I run the simplest torch cuda test by:  What I got is:  This is quite strange, since if I simlply turn to use the base image `nvidia/cuda:12.5.1cudnndevelubuntu22.04`, I can got the correct result that `torch.cuda.is_available()` returns `True`. Any advice will be sincerely appreciated, thx!  Versions  ",2024-09-09T17:18:26Z,module: binaries module: cuda triaged module: docker,open,0,5,https://github.com/pytorch/pytorch/issues/135508,"From your error message ""Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)"" it looks like there's a mismatch between the CUDA version in your container and the NVIDIA driver version on your host machine. To confirm, could you please check the NVIDIA driver version on your host by running: ",Did you run `collect_env` inside or outside of your container? It feels like container setup is somehow wrong in one of the case. Can you compile and run simple hello.cu in that container (or run `nvidiasmi` and share the output here) ,"> From your error message ""Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)"" >  > it looks like there's a mismatch between the CUDA version in your container and the NVIDIA driver version on your host machine. >  > To confirm, could you please check the NVIDIA driver version on your host by running: >  > `nvidiasmi` The result of `nvidiasmi` on the host is as follows: ","> Did you run `collect_env` inside or outside of your container? It feels like container setup is somehow wrong in one of the case. Can you compile and run simple hello.cu in that container (or run `nvidiasmi` and share the output here) >  >  The `collect_env.py` script is executed inside the container. I have compile and run the `hello.cu` and the result is as follows:  Nothing outputs. I've further inspect the error type by:  And the output is as follows:  It seems that something wrongs on the CUDA and driver version, but in my settings the container has CUDA 12.4 and the driver version on the host is 550, still strange...","Thank you guys, I have found the reason and solution: The root reason is that: > If hosts have new enough driver then there is no need to use compat lib. By removing `/usr/local/cuda/compat` inside the envvar `LD_LIBRARY_PATH`, the mismatch error won't occur. Thanks again! Ref: https://github.com/NVIDIA/nvidiadocker/issues/1256"
transformer,AssertionError: Dict types must use ConstDictVariable," üêõ Describe the bug I am trying to execute following code                  However, I am getting following error.   Error logs   Minified repro _No response_  Versions  ",2024-09-06T06:52:09Z,high priority triage review oncall: pt2 module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/135329,"triage review to discuss if this should be hipri, given it is a crash on a transformers model","This is because of  is empty dict by default, and it hits a Dynamo bug when tracing the . I just submitted CC([Dynamo] Fix Huggingface PretrainedConfig get non const attr) to fix it. BTW, I recommend to just wrap  with . The model constructors and print calls should be outside of compile region."
transformer,SAC Estimator (Selective Activation Checkpointing) for estimating memory and recomputation time trade-offs.,"This PR adds a Selective Activation Checkpointing (SAC) Estimator, built on top of the `Runtime Estimator`, for estimating memory and recomputation time tradeoffs. It provides a `TorchDispatchMode` based context manager that estimates the memory and runtime tradeoffs of functions or `torch.nn.Modules` for SAC, using the `Runtime Estimator` CC(Runtime Estimator for estimating GPU compute time)  under the hood to support two estimation modes: 'operatorlevelbenchmark' and 'operatorlevelcostmodel' (roofline model). The SAC Estimator provides detailed statistics and metadata information for operators of each module, including greedy order for selecting operators to be recomputed/checkpointed and permodule tradeoff graphs. This estimator is designed to be used under FakeTensorMode and currently supports estimation of compute time and memory usage."" It's inspired from: XFormers SAC by   Endtoend example:    Example AC Stats for one of the transformer layers: !Screenshot 20241011 at 10 09 13‚ÄØPM Example AC Tradeoff for one of the transformer layers: !Screenshot 20241011 at 10 09 58‚ÄØPM Example AC TradeOff graph one of the transformer layers: !Transformer layers 3 cc:    ",2024-09-05T12:01:09Z,oncall: distributed open source Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,1,5,https://github.com/pytorch/pytorch/issues/135208, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalcuda12.4py3.10gcc9sm86 / test (default, 1, 5, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ","  merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Undocumented fast pass behavior in nn.TransformerEncoderLayer causes failures in test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA)," üêõ Describe the bug This issue is related to  CC(DISABLED test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA)) Unit test test_transformerencoderlayer_cuda_float32 (in `test_nn.py`) performs the following tests: https://github.com/pytorch/pytorch/blob/a8611da86f42a442c3ab891a038af55440ccd8d0/test/test_nn.pyL12426L12450 This test expects all NaN output tensor if the `TransformerEncoderLayer` not using fast path. However, the determining factor of all NaN output is if the `SDPBackend.FLASH_ATTENTION` or `SDPBackend.EFFICIENT_ATTENTION` is used by the underlying SDPA operator within TransformerEncoderLayer However, the nonfast path of `TransformerEncoderLayer` does not explicitly disables the FA/MEFF attention: https://github.com/pytorch/pytorch/blob/a8611da86f42a442c3ab891a038af55440ccd8d0/torch/nn/modules/transformer.pyL896L927 Therefore, the actual backend selection logic depends on three parts of the whole codebase 1. The fast path/nonfast path logic in `TransformerEncoderLayer` 2. bool can_use_flash_attention(sdp_params const& params, bool debug) 3. bool can_use_mem_efficient_attention(sdp_params const& params, bool debug) 2 and 3 are undocumented, and FA may be used even if `TransformerEncoderLayer` should use nonfast path according to its document.  Suggested solution When calling `TransformerEncoderLayer._sa_block`, explicitly disables FA/MEFF backend by enclosing `x = self.self_attn` with `with sdpa_kernel([SDPBackend.MATH]):`  Versions  ",2024-09-04T21:32:27Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/135150,The Transformerencoderlayer takes 1 of two paths: 1.) Path that leads to SDPA 2.) FastPath: https://github.com/pytorch/pytorch/blob/96880148204fffd2f309b075672e8dfacbc21c90/aten/src/ATen/native/native_functions.yamlL14721 If it goes to 1.) there shouldnt be NaNs if it goes to 2 there might be NaNs depending on if it calls into SDPA or not I updated the efficient attention kernel on cuda to provide the updated masked out row semantic but did not make the same change to the rocm kernel. I think a better fix would be to update the ROCM kernel. Example of the reasoning can be found in the readme here: https://github.com/pytorch/pytorch/pull/133882
transformer,"[inductor][cpu] AlbertForMaskedLM, DebertaV2ForMaskedLM and timm_vision_transformer_large AMP AOT inductor crash issue"," üêõ Describe the bug AMP static shape default wrapper               suite       name       thread       accuracy       perf       reason(reference only)                       huggingface       AlbertForMaskedLM       multiple       X       X       AlbertForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 huggingface       DebertaV2ForMaskedLM       multiple       pass_due_to_skip       X       DebertaV2ForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 torchbench       timm_vision_transformer_large       multiple       pass_due_to_skip       X       timm_vision_transformer_large, Error: weights_offset must be aligned to 16K boundary                 huggingface       AlbertForMaskedLM       single       X       X       AlbertForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 huggingface       DebertaV2ForMaskedLM       single       pass_due_to_skip       X       DebertaV2ForMaskedLM, Error: weights_offset must be aligned to 16K boundary                 torchbench       timm_vision_transformer_large       single       pass_due_to_skip       X       timm_vision_transformer_large, Error: weights_offset must be aligned to 16K boundary           Versions SW info               name       target_branch       target_commit       refer_branch       refer_commit                       torchbench       main       23512dbe       main       23512dbe                 torch       main       f9f85bfc0b5b63274fa3fdd22afb0a456abf53f4       main       920ebccca2644881ece4f9e07b4a4b4787b8f2b1                 torchvision       main       0.19.0a0+d23a6e1       main    ",2024-09-03T16:14:47Z,oncall: pt2 oncall: cpu inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/135027," when you create those issue, do you mind adding respective oncall labels?", ,">  when you create those issue, do you mind adding respective oncall labels? Hi , could you please help to add the permission to ? He don't have the permission now. Thanks.",verified the fix in https://github.com/pytorch/pytorch/pull/135205
transformer,"Error ""attn_bias is not correctly aligned"" When Offloading torch.nn.functional.scaled_dot_product_attention Using torch.autograd.graph.save_on_cpu()"," üêõ Describe the bug I'm encountering an issue while training a Llama2 model when I use torch.autograd.graph.save_on_cpu() to offload activation tensors to CPU memory. The problem arises specifically when I attempt to offload the attention layer. The implementation of self_attn is based on the LlamaSdpaAttention class from the transformers library. I have pinpointed the issue to the torch.nn.functional.scaled_dot_product_attention operator. If I offload this operation as follows:  I get the following error:  However, if I offload other parts of the LlamaSdpaAttention class without offloading this specific operation, the issue does not occur. What could be causing this misalignment error when offloading the attention layer, specifically with torch.nn.functional.scaled_dot_product_attention, and how can I resolve it?  Versions Collecting environment information... PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.30.1 Libc version: glibc2.31 Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0187genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 535.183.0",2024-09-03T07:32:42Z,triaged module: sdpa,open,0,0,https://github.com/pytorch/pytorch/issues/134995
transformer,"Bert model training is not running on cpu (X86/ARM) with torch.compile() mode, generating value error."," üêõ Describe the bug I was trying to run Bert model training on ICELAKE CPU with torch.compile mode then it is giving a value error, but when i am running it with eager mode then it is running fine without any error. this is the error which i am getting when running in compile mode. !image this is script which i am running. !image the similar behavior is observed in Graviton machine also.   Versions using torch==2.4, the requirement file which i am using. requirement.txt Code: ` import os, time from transformers import AutoTokenizer from transformers import DataCollatorWithPadding import evaluate import numpy as np import torch from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer from torch.profiler import profile, record_function, ProfilerActivity from datasets import load_dataset imdb = load_dataset(""imdb"",split=['train[:5]','test[:5]']) tokenizer = AutoTokenizer.from_pretrained(""bertbaseuncased"") def preprocess_function(examples):     return tokenizer(examples[""text""], truncation=True,padding=True) tokenized_imdb_tr = imdb[0].map(preprocess_function, batched=True) tokenized_imdb_ts = imdb[1].map(preprocess_function, batched=True) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) accuracy = evaluate.load(""accuracy"") def compute_metrics(eval_pred):     predictions, labels = eval_pred     predictions = np.argmax(predictions, axis=1)     return accuracy.compute(predictions=predictions, references=labels) id2label = {0: ""NEGATIVE"", 1: ""POSITIVE""} label2id = {""NEGATIVE"": 0, ""POSITIVE"": 1} model = AutoModelForSequenceClassification.from_pretrained(     ""bertbaseuncased"", num_labels=2, id2label=id2",2024-09-02T10:30:41Z,triaged oncall: pt2 module: dynamo,open,0,5,https://github.com/pytorch/pytorch/issues/134950,here's the formatted repro script for anyone who wants to copypaste. I can repro it locally: ,"This also repros with `compile(..., backend=""eager"")` for me, which looks like a dynamo bug",(triage review to assess priority  it's a hard crash not a graph break),"This is happening because of unfortunate interaction of this line  https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.pyL818 `torch.compile` wrapper on the nn.Module changes the codepath for the above line. To solve this issue, you can use inplace nn.Module compile wrapper, something like  This fails further in the tool chain but resolves atleast the weird error.",Removing high prio because of a workaround. devang please confirm if this helps.
transformer,Pytorch Distributed DataParallel hanging, üêõ Describe the bug Basically I have 2 T4 GPU's and I am using kaggle when i try and load it in It shows some outputs that were put there for debugging but nothing else  Input   Output  Then it hangs and the CPU goes down (GPU doesn't change) and makes it not able to interrupt the execution It is super frustrating having 2 GPU's and only being able to use one  Versions N/A as using Kaggle ,2024-09-01T07:38:38Z,oncall: distributed triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/134920, May I ask how you are running your script? Are you using torchrun?,The output log you show seems to suggest that there is only 1 process being launched. Could that be the case and reason for the hang?,"Hey sorry, I got really busy,  I forgot about it for a while came back and eventually got it fixed, you can close this now, sorry for wasting your time On Mon, Sep 16, 2024 at 5:31‚ÄØPM Ke Wen ***@***.***> wrote: > The output log you show seems to suggest that there is only 1 process > being launched. Could that be the case and reason for the hang? > > ‚Äî > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >"
transformer,DISABLED test_transformer_runtime (__main__.TestRuntimeEstimator),Platforms: rocm Failure introduced by https://github.com/pytorch/pytorch/pull/134243 This test was disabled because it is failing on main branch (recent examples). ,2024-08-30T04:18:10Z,module: rocm triaged skipped,open,0,3,https://github.com/pytorch/pytorch/issues/134824,"? If these are to be skipped more permanently, it would be better to migrate this to an incode skip.","Yes, we will be trying to resolve this within the next two weeks.",Cc  amd    I am disabling the accuracy check on the time estimation **in the test code** because it breaks when we upgrade CI GPUs from M60 to T4. 
transformer,hf_T5_generate torch.complie() failed with latest transformers==4.44.2,"Latest `transformers=4.44.2` packages from HuggingFace uses `torch.isin()` which prevents graph break in certain control flows. It also adds the condition branch to avoid `copy.deepcopy` which is not supported by `dynamo` See this PR: https://github.com/huggingface/transformers/pull/30788 However, when I update to `transformers==4.44.2`. I got the error when running with the following command. **Repro**  **Error log**   ",2024-08-29T20:29:26Z,,closed,0,5,https://github.com/pytorch/pytorch/issues/137133,Is this the only model having this issue?,Can you try to add a `decoder_start_token_id=None` in here? https://github.com/pytorch/benchmark/blob/e6251f1838701030872e276f94987b7a24a3d6c8/torchbenchmark/util/framework/huggingface/model_factory.pyL162,upstream issue https://github.com/huggingface/transformers/issues/30892,Opened the issue in HuggingFace transformers repo: https://github.com/huggingface/transformers/issues/33283,"This is more an issue for huggingface, as you have opened, or potentially torchbench. I'm going to close this issue feel free to open an issue in torchbench."
transformer,Fusion Benchmarking records 0.000ms," üêõ Describe the bug I was trying out the fusion benchmarking option in the inductor (`torch._inductor.config.benchmark_fusion = True`) and noticed that the triton benchmark of operators (unfused and fused) takes 0.000ms:  This seems to come from this benchmarking code:  source: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/triton.pyL3105L3121 Running the generated kernel does take > 0ms:  There are also other examples where the difference is starker (it reports 0.000ms for 0.03ms). A step through with the debugger suggests this discrepancy is due this line:  I can't really follow the motivation from the comments in the code. Can someone please clarify if this is really correct or a bug? Attached a minimal code example (compilation of HF BERT), generated kernel for the above.  Error logs **main.py**  **cretgkc37eour7kyivicvw23qmm2fa2degqjm3dggag5n3u5ysye.py**   Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.4.0+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.14 (main, Jun  3 2024, 16:55:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.5.044genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Quadro GV100 GPU 1: Quadro GV100 GPU 2: Quadro GV100 GPU 3: Quadro GV100 GPU 4: Quadro GV100 GPU 5: Quadro GV100 GPU 6: Quadro GV100 ",2024-08-29T15:21:25Z,triaged oncall: pt2 module: inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/134768, 
transformer,DISABLED test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA),Platforms: rocm Broken by https://github.com/pytorch/pytorch/pull/133331 ,2024-08-28T16:08:18Z,module: rocm triaged skipped,open,0,5,https://github.com/pytorch/pytorch/issues/134687,Is there going to be a followup fix?,", I believe you will be filing a github issue to discuss the proposed fix?"," amd I'm not very familiar with the process, should I start an issue, or file a PR directly if I thought I found a solution?",Feel free to file an issue to discuss the fix and link to this one.,> Feel free to file an issue to discuss the fix and link to this one. Filed as  CC(Undocumented fast pass behavior in nn.TransformerEncoderLayer causes failures in test_transformerencoderlayer_cuda_float32 (__main__.TestNNDeviceTypeCUDA)) amd   
transformer,MiniCPM-V 2.6 transformer model returns nonsense tokens when running on MPS backend vs CPU backend.," üêõ Describe the bug When I run MiniCPMv2.6 model on my MacBook, the outputs look fine when using CPU backend, but they tend to contain nonsense English tokens or foreign language tokens when running on MPS backend. Here's the code to reproduce the issue:  Here are some example generations with MPS backend:  Here are example generations with CPU backend for the exact same input.   Versions  ",2024-08-27T00:31:53Z,triaged module: mps,open,0,0,https://github.com/pytorch/pytorch/issues/134534
transformer,Make the arguments in `torch.func.functional_call` optional," üöÄ The feature, motivation and pitch > torch.func.functional_call(module, parameter_and_buffer_dicts, args, kwargs=None, *, tie_weights=True, strict=False) I do not see why `args` is a strictly required argument. In huggingface transformers, the dataloader will load the data in a dictionary, and usually we call the model forward function like this:  However, we cannot directly do this using `torch.func.functional_call`. I think it is more reasonable to make `args` an optional argument with default value `None`. If necessary, we could add a postcheck to ensure `args` and `kwargs` are not both `None`. I am not sure whether it has specific reasons to make `args` not optional, so I raise an issue instead of a PR.  Alternatives Currently, I am using the following workaround:  Use an empty tuple to serve as a placeholder of `args`.  Additional context This issue was raised when I was adapting the codes from the ACL 2024 best paper ""Why are Sensitive Functions Hard for Transformers?"" ",2024-08-25T02:25:50Z,triaged module: functorch,closed,1,1,https://github.com/pytorch/pytorch/issues/134408,"Seems reasonable, we'd accept a PR fixing this."
transformer,[torch.compile] Graphs differ between 2.4 and 2.5," üêõ Describe the bug Within TorchTensorRT, we use aot_export_joint_simple to get aten level graph for TensorRT compilation. This used to return a graph with weights registered as `get_attr` nodes in the graph in Pytorch 2.4. However, this behavior seems to be changed in 2.5. All the constants are now registered as placeholders. We observed this to potentially impact performance of models.  To reproduce:   Please find the attached gm_2.4.txt and gm_2.5.txt to notice the differences.  Is there a setting to disable this behavior ?  cc:     Error logs _No response_  Minified repro _No response_  Versions Pytorch 2.4 and 2.5.0.dev20240822+cu124 ",2024-08-23T20:50:00Z,triaged oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/134369
transformer,Runtime Estimator for estimating GPU compute time,"This PR adds a basic Runtime Estimator for singledevice models. It estimates the GPU runtime in milliseconds using various estimation methods under the ``FakeTensorMode``. It provides a ``TorchDispatchMode`` based context manager that can estimate the eager runtime of PyTorch functions. It supports two estimation modes, benchmarking (`operatorlevelbenchmark`) and roofline cost modeling (`operatorlevelcostmodel`). For modules executed under this context manager, it agggregates the forward and backward operation runtimes and records their execution orders.      ",2024-08-22T17:00:33Z,oncall: distributed triaged open source Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,0,8,https://github.com/pytorch/pytorch/issues/134243,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: sanketpurandare  (ff46f47d012d27a199586d22d48fe0fc2b80984c, 9ce025489c5c4c0c56527106d737eb1e906201df, 5d0945123cef3675ed6727c219522734cfbe76c8, 5742c250f4b823c5c952bed6efdccd72bb145426, df96f5c0ab87582b27d6d1a493cd8e83ff947ca9, 5a6fa9a19443ece79113747154c903403d34cf70, 395d32eda34f4fbc42b4925cb13f71cb92b3241b)",the core looks good and left minor comments. look forward to unit test for santity checking. then we should be good, this PR is in draft mode now. Are you planning on changes? asking to make sure I am not blocking you ,>  this PR is in draft mode now. Are you planning on changes? asking to make sure I am not blocking you Added description and now the PR is ready for full review.,any chance to test on smaller tensors?  CI error is `distributed/_tools/test_runtime_estimator.py::TestRuntimeEstimator::test_conv_model_runtime  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.52 GiB. GPU 0 has a total capacity of 7.43 GiB of which 3.07 GiB is free`,> happy to stamp once we use less memory for conv test Fixed test params. , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,torch.utils.flop_counter.FlopCounterMode broke with torch-2.4," üêõ Describe the bug We have been successfully using `torch.utils.flop_counter.FlopCounterMode` up to torch2.4 and now it breaks and is impossible to use. It either warns: `The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch` or crashes with: `The Module hierarchy tracking is wrong. Report a bug to PyTorch` The relevant part of the trace is:  here is how we use it:  This happens with any HF transformers model I tried  Bert, Lllama, Mistral  clearly their models are perfectly fine. Rolling back to 2.3.1 restores the functionality. Questions: 1. what is the workaround to unblock us using FlopCounterMode with pt2.4+ 2. what is the longterm solution Suggestion: If I may suggest the warning/error is meaningless to the user. What does ""messed up mean""?  In particular this one:  https://github.com/pytorch/pytorch/blob/3c5b246d3c6461ef59fa38e8c4265b2c6b223412/torch/distributed/_tools/mod_tracker.pyL175C10L177C72 `""The module hierarchy tracking maybe be messed up. Please file a bug to PyTorch, if it is the case"" `how can a user tell if ""it is the case""?  Versions the problem happens on multiple setups  the only common ground is pt2.4.0 ,   ",2024-08-22T16:59:43Z,high priority triage review module: regression module: flop counter,closed,1,9,https://github.com/pytorch/pytorch/issues/134242,"Adding triage review as it feels like this belong to `oncall: profiler`, but it's not..",  can you please take a look? The module tracker warning is affected by your recent PR which is in the correct regression timeframe: https://github.com/pytorch/pytorch/commit/2e5366fbc04f819c62612e8c56fb786b43c1c67d,>   can you please take a look? The module tracker warning is affected by your recent PR which is in the correct regression timeframe: 2e5366f  https://github.com/pytorch/pytorch/blob/058302494cdf65fc59a1784fb9cea6c61e2be3f7/torch/utils/flop_counter.pyL5 Th FlopCounter does not use the `ModTracker` in `torch.distributed._tools`. It uses the one by   in `torch.utils`? https://github.com/pytorch/pytorch/blob/main/torch/utils/module_tracker.py cc:   ,"Thank you for clarifying,  that they are 2 very similar but different copies of code  I was just flagging that at the end of the OP that the error message isn't userfriendly or actionable  and that one is in torch.dist To repeat the specifics: ""The module hierarchy tracking maybe be messed up. Please file a bug to PyTorch, if it is the case"" how is the user to know ""if it is the case""? what does ""may be messed up"" mean?  so this could probably be improved as well to tell the user where the problem is or how to identify it or some such guidance.  In other words, I ended up asking for 2 related things in this OP 1. overcoming regression 2. while helping users to make sense of these cryptic warnings and errors Thank you.","Just to add a small voice of support here. We're also using the FlopCounter on nightly (2.5) and it's a great tool! But, I can confirm the same problem, concerning the warning `The module hierarchy tracking seems to be messed up.Please file a bug to PyTorch`.  For what it's worth, the counted FLOPs seem correct, even with the warning? EDIT: Additional Info: For us, the warning (but not error) is always caused by turning on activation checkpointing (for which it would be amazing to count flops correctly)","In https://github.com/pytorch/pytorch/blob/main/torch/utils/module_tracker.py there is no warning at the moment. A similar error message is ""print""ed only when you enter the same Module multiple times. We should definitely update that message to say this. The is a (not much more helpful) error thrown when you exit the Module during the forward pass (ignored during the backward): https://github.com/pytorch/pytorch/blob/75c22dd8bf8801a6eaf9bbe30e08cf8c05ded6a1/torch/utils/module_tracker.pyL112 These messages are because tracking entry/exit of module is quite hard and brittle and we wanted to provide as much suggestion there. We can also remove the print/error if you think that they are not helpful. For this particular error, it will depend on the model and what is done. One known case (and why there is no error in backward) is during the backward when the input doesn't require gradients. I think there are issues with calling the same Module recursively and I know there are issues when reusing the same input for multiple modules (I need to prepare a PR for this but it is tricky if we want to catch Tensors inside data structures for inputs).","FWIW, I found the cause  it's gradient_checkpointing (`torch.utils.checkpoint`) that triggers it. When activated, the modules will be reentered and it should be normal/expected. Weirdly I lost the use case where this lead to the exception, at the moment I can only reproduce the warning repeated on all ranks.",", thank you for the quick fix  and adding it to 2.4.1 milestone!",Validated the fix seems to work for 2.4.1
transformer,[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend,"Description: 1. Quantize Linear Layer Weights to 4bits: Quantize the weights of the Linear layer to 4 bits, using symmetric quantization. Pack two 4bit weights into one uint8 container. Choose a quantization scheme (channelwise or groupwise), with the group size being a multiple of 32. 2. Prepare Quantized Weights, Scales, and Optional Bias: After quantizing, obtain the quantized_weights, scales, and groupsize. If the original Linear layer has a bias, prepare it as well. 3. Pack the Weights Efficiently: Use torch.ops.aten._dyn_quant_pack_4bit_weight to optimally pack the weights, scales, and optional bias. packed_weights = torch.ops.aten._dyn_quant_pack_4bit_weight(weight, scales_and_zeros, bias, groupsize, in_features, out_features) Input parameters should include: in_features and out_features (the same as the Linear layer‚Äôs corresponding parameters). 4. Perform Dynamic Quantized Matrix Multiplication: Use torch.ops.aten._dyn_quant_matmul_4bit to perform matrix multiplication with quantized weights. output = torch.ops.aten._dyn_quant_matmul_4bit(input, packed_weights, scales_and_zeros, bias, groupsize, in_features, out_features) Inputs required include: The input tensor, packed_weights, scales, bias, groupsize, and the in_features and out_features. Model Perf : 7B Transformer model: Prefill : 340 t/s Decode  : 40  t/s 2B Transformer model Prefill : 747 t/s Decode  : 80  t/s Tests: python test/test_linalg.py k test__dyn_quant_pack_4bit_weight Ran 1 test in 0.016s OK python test/test_linalg.py k test__dyn_quant_matmul_4bit Ran 8 tests in 0.077s OK python test/test_linalg.py k test_compile_dyn_quant_matmul_4bit Ran 8 tests in 11.454s Chang",2024-08-21T17:42:15Z,module: cpu triaged open source module: arm release notes: linalg_frontend module: dynamo ciflow/inductor ciflow/linux-aarch64,open,0,24,https://github.com/pytorch/pytorch/issues/134124,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: ng05 / name: Nikhil Gupta  (80d263f5343806d3169f5c180f23cfe975264bdf, 1c0ef38138d8621ab4a044860404fbf01c7504a6)"," Attention! native_functions.yaml was changed If you are adding a new function or defaulted argument to native_functions.yaml, you cannot use it from preexisting Python frontend code until our FC window passes (two weeks).  Split your PR into two PRs, one which adds the new C++ functionality, and one that makes use of it from Python, and land them two weeks apart.  See https://github.com/pytorch/pytorch/wiki/PyTorch'sPythonFrontendBackwardandForwardCompatibilityPolicyforwardscompatibilityfc for more info.  _Caused by:_   aten/src/ATen/native/native_functions.yaml"," label ""ciflow/linuxaarch64"" ""module:arm"""," label ""module:arm"""," label ""module: arm"""," label ""ciflow/linuxaarch64""", can you please help in merging this PR. ,">  can you please help in merging this PR. I believe prerequisite for merging is passing build and test for specific target and this PR clearly fails aarch64 build right now, see https://github.com/pytorch/pytorch/actions/runs/10616020253/job/29425421107?pr=134124  Perhaps it's just a matter or rebase, but in general, I would strongly advice against merging a relatively large change if one could not get a clear signal from the platform it targets. For comparison, here is result of the build/test for the recent trunk commit: https://github.com/pytorch/pytorch/actions/runs/10636148674/job/29487324878","Hello  , We are in the process to refactor the PR after your valuable inputs. We are planning to : 1. keep all the files , kernel interface and kernel implementation as it is in  aten/src/ATen/native/kleidiai/* 2. Plug kleidiai int4 matmul kernel with _weight_int4pack_mm_cpu() and modify  the signature of _weight_int4pack_mm_cpu() to fit our requirements 3. Plug kleidiai int4 weght pack kernel with _convert_weight_to_int4pack_cpu() and modify the signature of _convert_weight_to_int4pack_cpu() to fit our requirements 4. Register a new op in torchao for kai_pack_rhs_size() kernel and use it in torchao for quantization and packing step. The implementation will still be in pytroch but op will be registered in torchao 5. Add / Reuse Symmetric_quantization() in torchao . This will be used for functioning of  _weight_int4pack_mm_cpu() and _convert_weight_to_int4pack_cpu() ops 6. Modify/Add the existing tests for _weight_int4pack_mm_cpu() to accomodate kleidiai kernel and use quantization scheme from torchao directly Please let me know your thoughts on this and if this addresses all your concerns regarding PR.",The current CI failure are not observed on our local machine but it seems we are unable to detect sve vector lenght on the CI environment `Error in cpuinfo: prctl(PR_SVE_GET_VL) failed` to reproduce run :  `python c 'import torch; print(torch.__config__.show())' ` or `python bb  test_multiprocessing.py shardid=1 numshards=1 v vv rfEX p no:xdist usepytest x reruns=2 importslowtests importdisabledtests` We are observing these failures in other PRs (119571) as well: https://github.com/pytorch/pytorch/actions/runs/10777650193/job/29891036346?pr=119571step:20:6914 Source : https://github.com/pytorch/cpuinfo/pull/255 cc:    ,> The current CI failure are not observed on our local machine but it seems we are unable to detect sve vector lenght on the CI environment `Error in cpuinfo: prctl(PR_SVE_GET_VL) failed` to reproduce run : `python c 'import torch; print(torch.__config__.show())' ` or `python bb test_multiprocessing.py shardid=1 numshards=1 v vv rfEX p no:xdist usepytest x reruns=2 importslowtests importdisabledtests` >  > We are observing these failures in other PRs (119571) as well: https://github.com/pytorch/pytorch/actions/runs/10777650193/job/29891036346?pr=119571step:20:6914 >  > Source : pytorch/cpuinfo CC(Fix batch_first in AutogradRNN) >  > cc:   There's a bug using this new API  cpuinfo_get_max_arm_sve_length() on the HW which doesn't have SVE support. It fails on HW like Graviton2. A simple fix was required and there's already a PR in place for this. https://github.com/pytorch/cpuinfo/pull/258,"> A simple fix was required and there's already a PR in place for this. Merged, please update this PR to include new cpuinfo. Thanks","> > A simple fix was required and there's already a PR in place for this. >  > Merged, please update this PR to include new cpuinfo. Thanks I will make a separate PR https://github.com/pytorch/pytorch/pull/135857",We have added a new 4 bit matmul operator along with its reference implementation to work across all CPU architectures. The new Operator   dynamically quantizes the input tensor from fp32 to int8 and matmuls it with prepacked 4 bit groupwise weights. A group can be of a full channel as well. Operator internal details can be found here and here  : ,"Hi  , We have also provided the reference kernels here https://github.com/pytorch/pytorch/pull/134124/filesdiff3dcbe3f22985e9eb2b5ce8fbfb3c7222cef9a7ad1de4a44494995c98d4b4f77cR816 and https://github.com/pytorch/pytorch/pull/134124/filesdiff3dcbe3f22985e9eb2b5ce8fbfb3c7222cef9a7ad1de4a44494995c98d4b4f77cR975","Hi  , Thanks for the comment. We are looking into it."," I Internally rebased this PR to pytorch latest and observed big regression in the torch.compile() . I constantly see below warning when I compile the model  ``W1121 14:33:31.913000 20422 torch/_inductor/ir.py:6377] [0/0] aten._dyn_quant_matmul_4bit.default is missing a cshim implementation, using proxy executor as fallback`` Any idea what is going wrong here or if pytorch refactored something around aten ops + compile","> observed big regression in the torch.compile() What is the baseline? And did you profile where is time being spent? > ""is missing a cshim implementation, using proxy executor as fallback"" Hmm.. not super familiar with it, but per the comment at the raise site, I guess you already added this op in `torchgen/aoti/fallback_ops.py`?","> > observed big regression in the torch.compile() >  > What is the baseline? And did you profile where is time being spent? >  > > ""is missing a cshim implementation, using proxy executor as fallback"" >  > Hmm.. not super familiar with it, but per the comment at the raise site, I guess you already added this op in `torchgen/aoti/fallback_ops.py`? I profiled end to ende execution of 4 bit matmul op before and after. This data is average of multiple runs. matmul shape [M=1 , N = 4096 , K = 4096] `Compile mode` Before : 129 us  After :    232 us `Eager Mode` Before : 79 us After :    80 us I used the torch profiler to get a breakdown.  The actual 4 bit matmul operation is taking exactly the same time before and after (55 us) but the overall execution of the compiled graph is slower in compile mode in latest pytorch.  I have not added the my 4 bit matmul ops to `torchgen/aoti/fallback_ops.py` . I ran the `python torchgen/gen.py updateaoticshim` to update the list but cant see anything changing. Can you please point me to relevant PR/code to correctly enable this change?  I am unsure why this change is needed but I could see some recent development and PRs around this fallback mechanism. UPDATE: Added to fallback_ops.py and the issue seems to be fixed.", > The actual 4 bit matmul operation is taking exactly the same time before and after (55 us) but the overall execution of the compiled graph is slower in compile mode in latest pytorch. I am not sure how you ran it. Does the profile say where the time outside matmul is spent? Can we amortize any static costs those by running a bunch of times? .,  Please merge this PR. There are two unsuccessful checks but they are not related to this PR IMO.,">   Please merge this PR. There are two unsuccessful checks but they are not related to this PR IMO. 05 I believe we had this discussion a while back:   Why does is have to go into core vs AO?   PR title/description are very confusing to the users: this is indeed a new feature, so title should be something like `Add 4 bit dynamic quantization matmuls` and description should mention how to use new API and perf numbers that dynamic quantization delivers on supported platforms vs say static 4 bit quantization","> >   Please merge this PR. There are two unsuccessful checks but they are not related to this PR IMO. >  > 05 I believe we had this discussion a while back: >  > * Why does is have to go into core vs AO? > * PR title/description are very confusing to the users: this is indeed a new feature, so title should be something like `Add 4 bit dynamic quantization matmuls` and description should mention how to use new API and perf numbers that dynamic quantization delivers on supported platforms vs say static 4 bit quantization   I think we had an agreement on 1st point with meta team already?","> > * Why does is have to go into core vs AO? >   I think we had an agreement on 1st point with meta team already? Yes, when  reviewed PR early in November he was happy with new ATen operation for dynamic quantization to go to core as long as there is a reference implementation for setup without KleidiAI which was added by 05 now."
transformer,fix for fp16,"This PR is a replacement for https://github.com/pytorch/pytorch/pull/133085 for pushing a quick fix for RMSNorm. The original author is  Previous PR summary: Since FP16 has quite small dynamic range it is very easy to overflow while computing `at::pow(input, 2)` , and it happens in real world computation. I've tried to use `nn.RMSNorm` fused implementation instead of `LlamaRMSNorm` inside `transformers` implementation of Llama (`src/transformers/models/llama/modeling_llama.py`). It started to give wrong answers in Fp16 while still giving good in FP32. I figured out happens due to overflow while computing square of the input tensor. Original `LLamaRMSNorm` implementation upcasts input to fp32 to prevent this and give better numerical stability.  Proposed commit fixed the issue. FP16 in RMSNorm has to be treated in special way, to be usable in real world implementations. ",2024-08-21T13:41:15Z,oncall: distributed module: cpu triaged module: mkldnn open source Merged ciflow/trunk release notes: quantization topic: not user facing module: inductor module: dynamo,closed,0,32,https://github.com/pytorch/pytorch/issues/134106, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: trunk / macospy3arm64mps / test (mps, 1, 1, macosm113), trunk / macospy3arm64mps / test (mps, 1, 1, macosm114) Details for Dev Infra team Raised by workflow job ", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  BC Lint  pull  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , can you merge this? I dont think the failing test is coming from my PR, pinging again for quick resolution for merge. Unsure about failing tests. pretty sure they are unrelated to this PR, merge r ,"The failing tests look related to me `test_modules.py::TestModuleMPS::test_forward_nn_RMSNorm_mps_float16`, rebasing to check", started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `fixrmsnorm` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout fixrmsnorm && git pull rebase`)","This PR updates submodules third_party/cudnn_frontend, third_party/ideep If those updates are intentional, please add ""submodule"" keyword to PR title/description.", seems like 3 fails compared to 5 after the rebase," There are third_party changes (ideep and CuDNN) in this PR. Could you please clarify whether the changes are needed? If not, please sync the submodule when doing the rebase.",looks like a botched merge?,j  I have not done the rebasing. I think  has been trying something out. Can you fix  ?, can you fix the rebase?, I have fixed the blotched rebase., Perhaps you could just modify the reference function in the sample inputs function to match the semantic you implemented for fp16. I wonder why the test only fails on MPS though...,"> nit: could we use `OpMath` rather than hardcoding the `half`, `float` case here? e.g., >  > https://github.com/pytorch/pytorch/blob/bf7db4e4f9f07765e63d9ce1c98c5962bafe4608/aten/src/ATen/OpMathType.hL16  I dont understand how to use this. Can you give an example?",  I am done with the changes and all cases pass now. I am not sure how to use the OpMathType. Can one of you push changes for that?,  pinging again,guys can we get this merged?,"> > nit: could we use `OpMath` rather than hardcoding the `half`, `float` case here? e.g., > > https://github.com/pytorch/pytorch/blob/bf7db4e4f9f07765e63d9ce1c98c5962bafe4608/aten/src/ATen/OpMathType.hL16 >  >  I dont understand how to use this. Can you give an example? see e.g., https://github.com/pytorch/pytorch/blob/3daca187aa1a536e0b28449c6381dc3f7a4a3ae8/aten/src/ATen/native/cpu/ReduceUtils.hL213","  my knowledge of C++ isnt great  this doesn't compile, can you push a fix?", figured it out I think its fixed now., pinging again,failing tests seem unrelated   , any updates on this?
transformer,DTensor sharding propagation for `scaled_dot_product_efficient_attention` should be more conservatively cached," üêõ Describe the bug The current `DTensor` sharding propagation caching policy for  `aten.scaled_dot_product_efficient_attention` (default) can result in silently incorrect gradients or trigger an IMA after cuda kernel launch (bt below) in mixed `require_grad` configurations. Testing a variety of TP `requires_grad` patterns (validating mechanistic interpretability probing use cases among others) revealed that if the first `aten.scaled_dot_product_efficient_attention` sharding propagation has `compute_log_sumexp=False` (e.g. first Transformer block attention layer has `require_grad=False`), subsequent `aten.scaled_dot_product_efficient_attention` ops may reuse the sharding propagation cache inappropriately (e.g., for any later Transformer block attention layer that may have `requires_grad=True`) resulting in the aforementioned silent gradient incorrectness or IMA. For instance, the silent correctness scenario can be observed with the test referenced below using a simple twodevice mesh and the default `n_heads=4` Transformer config. It occurs because:   The corrupted cache triggers an extra sharding operation. `scaled_dot_product_efficient_attention_strategy` sets the `logsumexp` `DTensorSpec` to `Replicate()` and shape `[bs, n_heads, 0]`, triggering an unintended redistribute and `_replicate_to_shard` before the local backward op.  Using the `num_heads_dim_sharding` strategy, the shard placement `mesh_dim` equals `sqrt(n_heads)` so the extra sharding operation yields a `local_tensor_args` tensor that will not will not raise an IMA when passed to our cuda kernel     Alternatively, if `mesh_dim sizes()[0] $13 = (const long &) : 8 (gdb) p log",2024-08-20T21:13:57Z,oncall: distributed module: dtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/134050,"cc: , l "," Thank you for helping identify the bug! We appreciate the report and the proposed solution. I agree to fix the bug we should simply adding `RuntimeSchemaInfo` for the two ops (efficient attention fwd, flash attention fwd). I'd suggest submitting a separate PR for this issue, both because it seems to be an independent problem, and because we need more time to digest the other issue. I can help review your PR. Since personally I believe this is a simple fix, it's up to you whether you'd like to merge without further testing, or modify the existing test cases/file for the two attentions in https://github.com/pytorch/pytorch/blob/main/test/distributed/_tensor/test_matrix_ops.py."
transformer,PyTorch torch.compile inductor fails to compile yolo.," üêõ Describe the bug Reproducer:  Error:  This model compiles successfully with torch<=2.3.1.  Versions Collecting environment information... PyTorch version: 2.4.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 20.0.0git (https://github.com/llvm/llvmproject.git 0795ab4eba14b7a93c52c06f328c3d4272f3c51e) CMake version: version 3.29.3 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A3000 12GB Laptop GPU Nvidia driver version: 536.45 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 46 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 24 Online CPU(s) list: 023 Vendor ID: GenuineIntel Model name: 12th Gen Intel(R) Core(TM) i712850HX CPU family: 6 Model: 151 Thread(s) per core: 2 Core(s) per socket: 12 Socket(s): 1 Stepping: 2 BogoMIPS: 4838.39 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnow",2024-08-17T10:37:36Z,triaged oncall: pt2 module: dynamic shapes module: inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/133759
transformer,Enable DTensor sharding propagation of `native_layer_norm_backward` to more fully accommodate optional args,"Fixes CC(DTensor sharding propagation of `native_layer_norm_backward` does not fully accommodate optional args)  The issue Testing a variety of TP `requires_grad` patterns (validating maximally flexible finetuning) revealed `DTensor` sharding propagation of `aten.native_layer_norm_backward` (default) fails with an `IndexError` for certain `requires_grad` patterns (pattern 1) (e.g. `output_mask` `[True, False, False]`) and an `AssertionError` for others (pattern 2) (e.g. output mask `[False, True, *]`). Please see issue CC(DTensor sharding propagation of `native_layer_norm_backward` does not fully accommodate optional args) for a full description of the observed failure patterns along with reproduction.  Use Cases and Remediation Failure pattern 1 is potentially problematic for a variety of finetuning scenarios. Though failure pattern 2 is really an xfail right now since it's not fully supported, IMHO there are use cases (e.g. especially wrt to mechanistic interpretability research, but certain finetuning scenarios too potentially) that justify supporting this output mask (especially since supporting it is fairly straightforward I think). In this PR I propose some modest changes that:    * Address the aforementioned failure modes.   * Add a couple tests that I'm hopeful will help ensure `DTenso`r op dispatch (which is so well implemented and such a pleasure working with btw! :rocket: :tada:) accommodates a wide variety of (potentially unanticipated) `requires_grad` patterns as it evolves. To address both failure modes, I'm proposing the following changes: 1. To `torch.distributed._tensor.ops._math_ops.layer_norm_bwd_strategy`:    Refactor",2024-08-14T20:17:11Z,oncall: distributed triaged open source better-engineering Merged ciflow/trunk topic: not user facing ciflow/inductor module: dtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/133502," merge f ""lint error is not related to this PR"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,DTensor sharding propagation of `native_layer_norm_backward` does not fully accommodate optional args," üêõ Describe the bug Testing a variety of TP `requires_grad` patterns (validating maximally flexible finetuning) revealed `DTensor` sharding propagation of `aten.native_layer_norm_backward` (default) fails with the following `IndexError` (traceback below) with certain `requires_grad` patterns (e.g. `output_mask` `[True, False, False]`):  Pattern 1   Pattern 2 Additionally, the op fails (expectedly according to this comment) with the following `AssertionError` (traceback below) for the currently unsupported output mask `[False, True, *]`.  Though failure pattern 2 is not fully supported, IMHO there are usage patterns (e.g. especially wrt to mechanistic interpretability research, but certain finetuning scenarios too potentially) that justify supporting this output mask (especially since supporting it is fairly straightforward I think). In a PR I will submit shortly, I propose some modest changes that:  1. Address the aforementioned issues 2. Add a couple tests that I'm hopeful will help ensure `DTensor` op dispatch (which is so well implemented and such a pleasure working with btw! :rocket: :tada:) accommodates a wide variety of (potentially unanticipated) `requires_grad` patterns as it evolves.  To reproduce To reproduce the two failure patterns listed above as manifest with a variety of `output_mask` patterns, run the following test provided in the PR I'm submitting shortly associated with this issue. You should observe `8/20` subtests failing with the relevant subtest configurations provided:   Additionally, to reproduce a specific failure patterns with the standard `DTensor` test Transformer, one can run the following new test parameter",2024-08-14T20:06:40Z,oncall: distributed triaged module: dtensor,closed,2,5,https://github.com/pytorch/pytorch/issues/133499,"Thanks a lot for the report and the PR! We appreciate them! We happen to be planning on some changes to how layer norm does sharding propagation in DTensor. Specifically we might need to create candidate sharding options (like we do for matmul ops), instead of propagating along inputs sharding. Let us carefully study your report and PR and see if it aligns with / contributes to the new strategy we need. cc:  ","> Thanks a lot for the report and the PR! We appreciate them! We happen to be planning on some changes to how layer norm does sharding propagation in DTensor. Specifically we might need to create candidate sharding options (like we do for matmul ops), instead of propagating along inputs sharding. >  > Let us carefully study your report and PR and see if it aligns with / contributes to the new strategy we need. >  > cc:  Happy to contribute! The distributed team's code is always a pleasure to work with, shrewdly designed and adroitly implemented. üéâ üöÄ  I definitely appreciate the API is still evolving but whatever portions of the PR could be merged (even the test refactor) would be helpful while the candidate sharding option is being explored (agreed something akin to `gen_einsum_strategies` could be useful!).  I'm the primary maintainer of a downstream package (FineTuning Scheduler) and am currently adding support for the DTensor APIs and FSDP2 etc. I'm also planning to experiment with using DTensor dispatch for some mechanistic interpretability research functionality so if the team wants to solicit thoughts from the user base on anticipated usage patterns beyond the typical RFCs I'd be glad to provide any feedback that could be useful.  Anyway, if you think about it, please tag me once the candidate sharding refactor work gets going. Thanks again for your work!!","Thanks  for reporting the issue and categorizing it into 2 patterns which largely help us understand the problem. 1. for pattern 1, this is a mistake in `layer_norm_bwd` implementation. I misunderstood that when `output_mask[1] = False` and `weight` is not `None, the desired spec should still be added to the list. Your fix in PR should address this. 2. for pattern 2, your fix in PR complements the missing case support.  The PR looks good to me on these 2 parts, but my question is on the decision on filtering `None` from `args_spec` or `desired_specs` because they should contain no `None` objects.","> Thanks  for reporting the issue and categorizing it into 2 patterns which largely help us understand the problem. >  Thanks for all your great contributions, they're a pleasure working with! > 1. for pattern 1, this is a mistake in `layer_norm_bwd` implementation. I misunderstood that when `output_mask[1] = False` and `weight` is not `None, the desired spec should still be added to the list. Your fix in PR should address this. > 2. for pattern 2, your fix in PR complements the missing case support. >  > The PR looks good to me on these 2 parts, but my question is on the decision on filtering `None` from `args_spec` or `desired_specs` because they should contain no `None` objects. I just had a chance to review your PR suggestions this morning. I had more conservatively refactored `layer_norm_bwd_strategy`, always adhering to the full op schema for the target input specs (i.e. including `None` for optional args) to ensure alignment with `op_schema.args_spec` but your suggestion that it should be safe to rely on the presence of `(weight|bias)_strategy` allowed me to push a `layer_norm_bwd_strategy` refactor that obviates the need for the referenced `None` filtering and it's now been removed. Thanks again for your thoughtful collaboration! ","close this issue as https://github.com/pytorch/pytorch/pull/133502 has been merged. Again, really appreciate  for the detail issue report and quick fix!! "
transformer,[FSDP2] Added eager fast-path for fp32->bf16 param cast,"  CC([FSDP2] Added eager fastpath for fp32>bf16 param cast)  CC([FSDP2] Set `ctx.set_materialize_grads(False)` for postbackward) Some recommendation models have a high number of `nn.Parameter`s. This exacerbates pertensor CPU overheads in FSDP2 compared to FSDP1. This PR adds a fastpath for the common bf16/fp32 mixed precision case for the casting the parameters from fp32 to bf16 to reduce CPU overhead and possibly have more efficient copy.  Old: `for` loop + `.to(torch.bfloat16)`, incurring dispatcher overhead per parameter  New: `torch.empty` + `torch.split` + `torch._foreach_copy_`, incurring three dispatches  Example on Llama38B which does not have many `nn.Parameter`s (compared to recommendation models): (Old) on Llama38B (0.46 ms CPU overhead for allgather): !Screenshot 20240813 at 6 19 39‚ÄØPM (New) on Llama38B (0.37 ms CPU overhead for allgather): !Screenshot 20240813 at 6 20 32‚ÄØPM  Same example as above but now with float8 allgather: (Old) on Llama38B with float8 (0.996 ms CPU overhead for allgather): !Screenshot 20240815 at 11 27 46‚ÄØAM (New) on Llama38B with float8 (1.014 ms CPU overhead for allgather): !Screenshot 20240815 at 11 26 33‚ÄØAM The times are relatively comparable for float8 with the new one possibly slightly slower, but this is mainly because for Llama's transformer blocks, there are only two norm weights that need to cast to bf16. These screenshots are mainly to show that the optimization still works in the mixed case. : D61236983",2024-08-13T22:15:14Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (fsdp2),closed,0,5,https://github.com/pytorch/pytorch/issues/133369," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","cc:  depending on how many weights use fp8, the fp8 casting CPU overhead, even with the precompute, might become a problem", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,"[Torch.Export] [OpenELM] Failed to Export OpenELM: ""Pop from Empty List"""," üêõ Describe the bug When I was trying to export OpenELM  I was hit by   Versions Tried both torch `2.3.0` and `2.4.0`, both fails Transformers version is `4.38.2` ",2024-08-12T18:45:29Z,export-triaged oncall: export,closed,0,3,https://github.com/pytorch/pytorch/issues/133252," Sorry we just got to this, the embedding weights seem to be not tracked properly, I'll look into this."," if you're able to use nonstrict mode export `export(strict=False)` I would recommend it while we work on this, nonstrict seems to export successfully.",Also putting up https://github.com/pytorch/pytorch/pull/134500 to fix this
transformer,[torch.compile] Integers stored on nn.Modules as dynamic causing errors," üêõ Describe the bug reporduce code  The above code is fine on 2.3, but it will throw an error on 2.4 and the main branch. What is error looks like:  This seems to be a problem with inductor or triton, but it is actually because *self.num_heads* is dynamic. This phenomenon occurred after this commit:  CC([torch.compile] nn.Module int/float attributes caused re-compile) The problem is that torch made all ints in nn.Module dynamic, but this is actually not necessary. I understand that there will be an automatic dynamic mechanism for this in the future, but for the moment if I can have a simple way to avoid the problems.  Versions  ",2024-08-10T13:47:51Z,high priority triaged actionable oncall: pt2 module: dynamic shapes,open,1,5,https://github.com/pytorch/pytorch/issues/133166,pytorch 2.4 release error infomation: !image,:  CC([torch.compile] nn.Module int/float attributes caused re-compile) ,"Adding insult to injury there's no way to force the integer to be static lol. Well, I guess you could do something like `tensor_num_heads = torch.empty((num_heads, 0))` and then `num_heads = tensor_num_heads.size(0)` LOL",It looks like this is causing many errors internally,"I actually circumvent this problem by using warmup. I don't specifiy dynamic=True, but use default setting. Then I warmup with inputs of different lengths. Eventually, the dimension of the sequence length become dynamic, while num_heads is still static, which serves my purpose."
transformer,Fixing Pytorch RMS norm implementation,"Since FP16 has quite small dynamic range it is very easy to overflow while computing `at::pow(input, 2)` , and it happens in real world computation. I've tried to use `nn.RMSNorm` fused implementation instead of `LlamaRMSNorm` inside `transformers` implementation of Llama (`src/transformers/models/llama/modeling_llama.py`). It started to give wrong answers in Fp16 while still giving good in FP32. I figured out happens due to overflow while computing square of the input tensor. Original `LLamaRMSNorm` implementation upcasts input to fp32 to prevent this and give better numerical stability.  Proposed commit fixed the issue. FP16 in RMSNorm has to be treated in special way, to be usable in real world implementations.",2024-08-09T10:02:21Z,triaged open source,closed,0,7,https://github.com/pytorch/pytorch/issues/133085," :x:  login:  / name: Karol Kontny . The commit (c79577d3d55483360ac339b14f605656d3f30c42, 6458021f9eaa4e13d3bb63d08df361286c424d65) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.",> Could you sign CLA in  CC(Fixing Pytorch RMS norm implementation) (comment) please I have to process the CLA through the company legal. Unfortunately it will be probably done early next week.," would you mind if I open this PR again instead? Its blocking me as well, want to get it in as soon as possible",">  would you mind if I open this PR again instead? Its blocking me as well, want to get it in as soon as possible  Please go on, I tried to hurry them, but with no effect. It takes much more time than it should...", I have created a PR here: https://github.com/pytorch/pytorch/pull/134106, I think we can close this. this has been fixed now in https://github.com/pytorch/pytorch/pull/134106,Fixed via CC(fix for fp16) 
transformer,Optimize test_transformers.py," Reduced number of skipped test cases  Merged redundant test cases **Benchmark:**  _These are approximate numbers from running test_transformers.py on a single H100, and can change based on the device._",2024-08-08T23:31:50Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/133049, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Optimize test transformers,Fixes ISSUE_NUMBER,2024-08-08T23:20:11Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/133046
transformer,Exclude test_transformers and unit tests which require recent GPU arch,This PR is to exclude test_transformers on ROCm temporarily and skip some unit tests which require recent GPU arch. ,2024-08-07T16:56:32Z,oncall: distributed triaged open source Merged topic: not user facing ciflow/periodic ciflow/inductor rocm rocm priority keep-going ciflow/rocm ciflow/inductor-rocm,closed,0,38,https://github.com/pytorch/pytorch/issues/132895, label ciflow/rocm, label keepgoing, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `0807_test_results_info` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 0807_test_results_info && git pull rebase`)", label ciflow/trunk
transformer,[Memory] snapshot wrong with optimizer_state and gradient - ü§ó Transformers," üêõ Describe the bug  Training GPT2XL with ü§ó Transformers and ü§ó Accelerate  Using _record_memory_history and _dump_snapshot for profiler Training GPT2XL with ü§ó Transformers, optimizer set to adamw_torch. Since we are using adamw, the memory of optimizer_state should be 2 times of gradient if fp32training, or 6 times of gradient if fp16 or bf16training. However, from the snapshot, the optimizer_state is always the 2 times of gradient, which is obviously impossible. Reproduce  model: GPT2XL  batch size: 4  sequence length: 1024  single GPU  optimizer: adamw_torch  bf16  !Trace_gpt2xldataset_alpacagpus1bs4seq1024lr0 0004linearbf16_step_3_to_5  Versions PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04 LTS (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0155genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.2.91 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A800SXM480GB GPU 1: NVIDIA A800SXM480GB GPU 2: NVIDIA A800SXM480GB GPU 3: NVIDIA A800SXM480GB GPU 4: NVIDIA A800SXM480GB GPU 5: NVIDIA A800SXM480GB GPU 6: NVIDIA A800SXM480GB GPU 7: NVIDIA A800SXM480GB Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn.so.9.1.0 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv.so.9.1.0 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn.so.9.1",2024-08-07T10:04:12Z,module: optimizer triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/132855, (not sure what label to add for memory snapshot),"Why should optim state take up 6 times of gradient in fp16 or bf16training? If the optimizer was created with fp16/bf16 parameters and there's no additional logic to store copies of the params, then the optimizer being 2x the size of the params is expected. Also I believe this may be a better question for the huggingface transformers repo https://github.com/huggingface/transformers as it is likely setup related.","Sorry I made a mistake, the 2x size is expected. > then the optimizer being 2x the size of the params is expected. Closing this issue."
transformer,Forward Hooks in Transformer Not Working When `model.eval()` and `torch.no_grad()`," üêõ Describe the bug For `TransformerEncoder` network, when I created a forward hook, it does not get called if there are `model.eval()` and with `torch.no_grad():` at the same time. Here is a code to reproduce:  It gives ‚ÄúHook for * is set‚Äù as output, but it does not print ‚ÄúHook working‚Äù, (and it does not actually call the hook). I observed that when there is only model.eval(), it works. Also, when there is only with torch.no_grad(), it also works. Somehow using both of them make it not working. And also, I only observed this with TransformerEncoder (not happens in `nn.Linear`) I first shared this in discuss.pytorch, and as a first impression ptrblck stated that "" 'faster transformer' pass is used as described here which does not seem to support forward hooks.""  Versions Collecting environment information... PyTorch version: 2.3.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.30.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.85+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib",2024-08-05T21:26:43Z,triaged module: sdpa,open,0,3,https://github.com/pytorch/pytorch/issues/132700,Could you try setting `torch.backends.mha.set_fastpath_enabled(False)`,"Thanks, it works after your suggestion. If this was an intentional design choice, I think there should be at least some note about this in the documentation because it took a lot of my time to figure out where is the problem","Hi, I was using a forward hook to extract intermediate layers, which breaks in eval mode due to this behaviour.  What is the recommended way to extract intermediate layer feature maps when fastpath is enabled?"
transformer,`torch.distributed.pipelining` hang and timeout in CPU gloo backend," üêõ Describe the bug When executing the pippy_bert.py example with cpu gloo backend:  I got the following hang and timeout error:  BTW, pippy_gpt2.py works but pippy_llama.py is also timeout.  Versions Collecting environment information... PyTorch version: 2.5.0.dev20240805+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.092genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      43 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          AuthenticAMD Model name:                         AMD Ryzen 9 3950X 16Core Processor CPU family:                         23 Model:                              113 Thread(s) per core:                 2 Core(s) per socket:                 16 Socket(s):                          1 Stepping:                           0 Frequency boost:                    enabled CPU max MHz:                        3500.00",2024-08-05T09:40:07Z,oncall: distributed triaged module: pipelining,open,0,4,https://github.com/pytorch/pytorch/issues/132644,cc: Huang  ,"Hi , we don't have testing for `pippy` (now under `torch.distributed.pipelining`) for CPU and gloo. We would accept improvements and fixes in this area. Please take a look under https://github.com/pytorch/pytorch/tree/main/torch/distributed/pipelining if you are interested in contributing.","Hi Huang, what are the e2e models you are testing after `pippy` has been merged into `torch.distributed.pipelining`? Or do you mean that `torch.distributed.pipelining` support and testing are only for GPU? ","> torch.distributed.pipelining support and testing are only for GPU Yes we are only testing for GPU. The e2e model tests have not been upstreamed from `pippy` to `torch`, we would also accept contributions there"
transformer,autocast to float16/bfloat16 fails on transformer encoder (in `eval` mode)," üêõ Describe the bug Continuing from https://github.com/LightningAI/pytorchlightning/issues/19980 Autocasting a `TransformerEncoder` to `bfloat16` works in training time, but not in eval time:  Yields:   Versions  ",2024-08-04T13:02:16Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/132613, please take a look at this one.
transformer,Change owners of test/test_transformers.py to module: multi-headed-attention,So flaky tests get tagged with `module: multiheadedattention` instead of `module: nn`   CC(Change owners of test/test_transformers.py to module: multiheadedattention),2024-08-02T15:51:23Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/132519, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,dynamic tensor shape of vmap dimention," üêõ Describe the bug I've have a model that internally consists of multiple data streams, each of which having their own unique parameters. To optimize for performance I had stacked different parameters for all the streams together and issued a single call for linear operations with help of `torch.vmap`. This approach worked fine back in torch 2.3, but ever since upgrading to torch2.4 the compile operation fails if I mark the vmap dimension as dynamic. I've stripped all the details from the code down to the smallest repro script:  It seems the constraints created because of vmap are enforcing constant guards rather than dynamic/relative ones. In my searches I've seen that similar issues have previously been reported but nothing was explicitly addressing vmap transforms. I additionally noted that in 3.3 this was issue was bypassed since the graph broke automatically when hitting function transformers, but the default behavior has changed in 3.4. I have captured the following logs: Hopefully it helps:   Versions Collecting environment information... PyTorch version: 2.4.0+cu124 Is debug build: False CUDA used to build PyTorch: 12.4 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.153.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 GPU",2024-08-01T05:32:55Z,actionable module: functorch module: dynamic shapes export-triaged oncall: export,open,0,10,https://github.com/pytorch/pytorch/issues/132381,"Thanks for the detailed report. In the programming model for export, shapes of any state in a module cannot be dynamic. Here `self.weight_streams` is specialized to the value `2` provided as `n_streams` during module init. Then in the forward, `self.weight_streams` is interacting with `stream_idx`. I'm pretty sure this is causing the dynamic dim to be specialized to `2`, which is always an export error. The suggested fix given as part of the error message asks to make this dim be `2` instead of dynamic.","I have tried this with any dimension for the `self.weight_streams`. It doesn't cause any issues.  For example if we create the `StreamedLinear(20, 3, 4, False)` but with the same `stream_idx = [0,1]` the code still fails with specialized value set to `2`. Any operation (such as addition) doesn't cause issues either.  From my understanding, the constraints emitted when processing `vmap` are too restrictive. In essence `vmap` always emits static shape contraints regardless of whether the other parameter is dynamic or not.  Effectively instead of emitting `X[0] = S1 & Y[0]= S1` the `X[0] = 2 & Y[0] = 2` constraints are being emitted.","I also have to mention that similar to export, compiling this module also fails to properly make it dynamic and results in static shapes. which in turn leads to recompilation and finally omitting the module from compilation.","OK, looks like this deserves a deeper look. I'll report back what I find."," you're right, here are the relevant logs for the vmap specialization using `TORCH_LOGS=""export"" TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(s0, 2)"" ...` ",  do you know if we can do better here? Looks like `_vmap_increment_nesting` is specializing on its input `batch_size` (which here is `s0`).,This code in particular is causing the specialization: https://github.com/pytorch/pytorch/blob/1962f9475fbc289eee35c1fc24bf71e43f5dbba8/torch/_dynamo/variables/torch.pyL285L289,We should be able to thread the batch size through as dynamic,"I have to mention, just a naive approach of removing guard and returning `x.sym_num` in case of `SymNodeVariable` doesn't cause any additional bugs (to the best of my knowledge) but still will down the line result in specialization. It seems vmap will at some point call `__index__` for the batch dimension when calling into c++ function `_maybe_remove_batch_dim` which will result in guarding and specialization again. ",In triage review this was acknowledged as something we could fix but it remains unassigned. Feel free to send a PR.
transformer,[inline_inbuilt_nn_modules] [BUG] C++ error in `torch.compile` when `dynamic=True`," üêõ Describe the bug This commit f44446e851 breaks the usage of dynamic=True, it can be produced by the following script:   Error is too long, here is just a few part of it: !image  Versions Env:  ",2024-08-01T03:19:44Z,high priority needs reproduction oncall: pt2 oncall: cpu inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/132373,Thanks for opening the issue. I will take a look tomorrow and keep it high prio.,"feng Is it possible for you to check it this stack works for you? https://github.com/pytorch/pytorch/pull/132334 I can try more but on my machine, it does not fail. But it take a very long time to finish. UPDATE  I tried to repro the error on `main` branch, but the error does not repro. Can you share your transformers version? Otherwise, please try the above pull request to see it that resolves the problem.","> feng Is it possible for you to check it this stack works for you? CC([dynamo] Track params/buffers and mark them as static) >  > I can try more but on my machine, it does not fail. But it take a very long time to finish. >  > UPDATE  I tried to repro the error on `main` branch, but the error does not repro. Can you share your transformers version? Otherwise, please try the above pull request to see it that resolves the problem. Hi  . The error could be reproduced with the parameter `export TORCHINDUCTOR_FREEZING=1`.","This also looks like a normal C++ codegen problem that should be fixed (when generating initializer lists, we need to ensure we cast every inner element to a consistent type, because you can't mix SymInt and integer literals)  ","feng Even with the flag, I can't repro. So, please provide some more information around conda/pip packages. Removing my assignment for now. ","It has been fixed in the latest main branch, thanks! "
transformer,`torch.nn.transformer.forward` returns incorrect value inside `torch.no_grad()` blocks., üêõ Describe the bug `torch.nn.transformer` returns incorrect value inside `torch.no_grad()` blocks. A minimal example is available. You could also find the same code in Colab.   Versions  ,2024-07-30T12:31:15Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/132136, shouldn't this indeed be part of `oncall: transformer` as it uses `torch.nn.transformer`?
transformer,UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. ," üêõ Describe the bug I get this error:  after all my print statements and training loop properly run. I am on MacOS Ventura 3.4 and have installed the latest version of Pytorch 2.4.4 /opt/anaconda3/envs/andrej_gpt2/lib/python3.10/sitepackages/torch/_tensor_str.py:138: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:335.)   nonzero_finite_vals = torch.masked_select( tensor(0.0029, device='mps:0', grad_fn=) Following along Andrej Kaparthy's tutorial:   Versions Collecting environment information... PyTorch version: 2.4.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.14  (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ] (64bit runtime) Python platform: macOS13.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Versions of relevant libraries: [pip3] numpy==2.0.1 [pip3] torch==2.4.0 [pip3] torchaudio==2.4.0 [pip3] torchvision==0.19.0 [conda] numpy                     2.0.1           py310h52bbd9b_0    condaforge [conda] torch                     2.4.0                    pypi_0    pypi [conda] torchaudio   ",2024-07-30T02:13:36Z,triaged module: mps,closed,0,1,https://github.com/pytorch/pytorch/issues/132110,The natively supported macOS version for nonzero op was lifted to 14.0 due to the previous implementation being flawed. I proposed a pr to correct the error message.
transformer,All-reduce followed by division does not correctly divide data," üêõ Describe the bug When sharing weights between two layer of a model (such as tying input and output embeddings in a transformer model), an allreduce sum followed by averaging by world_size doesn't average the weights. Dividing the .data works. For example, you can run the following script:   Versions PyTorch version: 2.1.1 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0144genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA H800 GPU 1: NVIDIA H800 GPU 2: NVIDIA H800 GPU 3: NVIDIA H800 GPU 4: NVIDIA H800 GPU 5: NVIDIA H800 GPU 6: NVIDIA H800 GPU 7: NVIDIA H800 Nvidia driver version: 550.54.15 cuDNN version: Probably one of the following: /lib/x86_64linuxgnu/libcudnn.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_",2024-07-29T10:15:32Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/132027,"`param = param / world_size` creates a new tensor object for the lefthandside `param` compared to the righthand side `param`, but this new lefthandside `param` is never registered back to the module. In other words, you are doing something like `new_param = param / world_size` but then throwing away the result of `new_param`.",Oh can't believe I missed this. Closing now
transformer,[FSDP2] root moduel parameters stays unsharded after forward before backward," üìö The doc issue this question comes up from torchtune. after forward before backward, root module parameters are unsharded. want to confirm if this is by design. I know it's performant to keep root unsharded because they will be used in backward immeidately. but want to confirm TLDR repo  full repro   Suggest a potential alternative/fix _No response_",2024-07-26T23:18:39Z,triaged release notes: distributed (fsdp2),closed,0,3,https://github.com/pytorch/pytorch/issues/131965,This is by design.,"If people find this kind of implicit behavior confusing, we could put the burden on the user to pass `reshard_after_forward=False` to the root module. I would be okay with that. For now, the user can call `root_module.reshard()` if they want to shard the root module after forward.","> If people find this kind of implicit behavior confusing, we could put the burden on the user to pass `reshard_after_forward=False` to the root module. I would be okay with that. >  > For now, the user can call `root_module.reshard()` if they want to shard the root module after forward. thanks. confirmed with torchtune that they only ask this for understanding purpose"
transformer,Optimize test transformers," Reduced number of skipped test cases  Merged redundant test cases **Benchmark:**  _These are approximate numbers from running test_transformers.py on a single H100, and can change based on the device._ ",2024-07-26T18:00:02Z,oncall: distributed topic: not user facing module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/131919, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/131919/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/10308563271
transformer,xpu: add check for supported devices to xpu initialization and torch.xpu.is_available(),"As of now, https://github.com/pytorch/pytorch/commit/bf6aae1468659d611959d94c314639e402edc3fe, XPU backend supports limited set of GPU devices, PVC, Meteor Lake. However, XPU backend does not check for supported device types and backend will be reported to be supported on a lot more Intel GPU devices, for example, on Alder Lake. However, trying to run backend on such devices will actually fail. On Alder Lake I get:  Can XPU backend, please, start to check whether it can actually run on the underlying Intel GPU, fail to initialize and report `False` in `torch.xpu.is_available()` if it can't? That's important because sometimes there are device agnostic use cases. For example  consider below Huggingface example which specifies just `device_map=auto`. User's intent might be to run on CPU or on CUDA, but it might get dispatched to unsupported Intel GPU and fail.  CC:      ",2024-07-25T19:33:25Z,triaged module: intel module: xpu,open,0,9,https://github.com/pytorch/pytorch/issues/131799,"Yes, we mentioned it in our discussion. But I might think there was no such error log or not informative log on Alder lake. The error log makes sense to me.  Return false from calling `torch.xpu.is_available` is, 1. Not reasonable, if there are iGPU (unsupported) and dGPU (E.g. Arc, supported) installed on the system. 2. Not aligned with semantics of other PyTorch backends. Return true, only if CUDA source is compiled and device count doesn't equal to 0. Like,  Options in my mind, 1. There is not that big difference between bypassing Alder lake but going into CPU/CUDA and raising runtime error without the gating, when people tend to use Alder lake capability only. So we should enhance the error log with explicit platform name or adding other informative log. 2. Find a new API, like `get_device_capability`, `get_device_properties`, not sure if it works. And add it HF framework for checking  workability of the current platform. I might think CUDA has the same issue. As for NV, they just consider dGPU only. Archs of iGPU and dGPU may have big difference before for us, requiring different implementation in driver and compiler. The panic of compatibility is much slighter than us?","Having the case with iGPU and dGPU to handle is a valid point. I don't see contradictions here though. Can logic be modified as follows? 1. `device_count()` should count only supported devices. If there is iGPU + dGPU and both are supported it should return =2, if iGPU is not supported it should return =1 (counting only dGPU). And it will return =0 if there are no supported devices. 2. `is_available()` don't need modifications. It will return true or false depending on the cases described above 3. `xpu:0` will mean first supported intel device instead of first found (and potentially unsupported) device  this also don't need any modifications I think. I just note the behavior. Will this work?","LGTM. The feature should be about XPU device enumeration in XPU device pool. XPU device pool should only include devices supported by current driver and compiler. So you mention me there seems showing up a trick or a contradiction in SYCL runtime/driver. We query a GPU device from SYCL runtime, which should mean driver commits its workability, but operations on the device fails due to driver runtime error. Two options, 1. Walk around in PyTorch to maintain a list saying what platforms are supported. But I think it is not compatible or hard to maintain since user cases are various. 2. Require driver to return a valid GPU device basing on current driver version and hardware platform.   Any thoughts?","From my perspective, this doc clarifies the hardware prerequisites. And the user can get the device name via `torch.xpu.get_device_name` or `torch.xpu.get_device_properties`.  It seems enough to let the users judge if their GPU is supported.","> From my perspective, this doc clarifies the hardware prerequisites. And the user can get the device name via torch.xpu.get_device_name or torch.xpu.get_device_properties. It seems enough to let the users judge if their GPU is supported No, it's not enough. This will lead to ~~disaster~~ bad user experience. Software must report what it supports and nothing else. And it must actually work on what it supports. The doc you refer to further refers to intel side documentation which clearly states that limited set of Intel GPUs is supported depending on PyTorch release. For 2.4 that's PVC, for 2.5 that's PVC, ATSM, DG2, MTL. Do you see AlderLake in this list? no. Why it's reported by torch.xpu then? is it after all supported or not? This is not right to offload lists of supported/not supported hardware to end user applications   such lists will need to be for each application, will run out of sync pretty soon, etc., etc. And I believe that's definitely a concern for any packaging of pytorch with XPU backend: you get a package which you must be extremely careful with and install only on appropriate environment and if you will install it on a wrong environment you risk to break something. For example, consider what user should install in a system with iGPU (ADL) +dGPU (DG2)? what if user wants to run both CPU and XPU workloads? what if user use oneAPI without pytorch with ADL, but wants pytorch with DG2? We have multiple variants of usage here, I highlighted few, but there are much more combinations. With such variety of choices it will be a nightmare for the user to install correct versions of pytorch/oneapi if these versions attempts to automatically recognize GPUs they are not supposed to work on.","> So you mention me there seems showing up a trick or a contradiction in SYCL runtime/driver.  XPU backend in pytorch declared support (via doc) for the subset of GPUs (PVC, ATSM, DG2, MTL) supported by SYCL/runtime driver which supports much more. According to at least the following docs they support TGL+: * https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapidpcppsystemrequirements.html * https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapibasetoolkitsystemrequirements.html  Setup doc provides repackaging of oneAPI components. How these packages were built? and how they differ from original oneAPI packages available via different setup instruction? Were supported platforms fused to the actual list exposed for pytorch (PVC, ATSM, DG2, MTL)? I believe there was no fuse and that's just repackaging putting packages in another location. As a result we actually get oneAPI capable for all oneAPI platforms (TGL+) which reports exactly that  we support TGL. I agree with your outline for 2 options above.. Some additional thoughts on top of them: 1. (list of supported devices in PyTorch) Realistically, I think that's the only viable option in the short/middle term (on the span of ~1 year). I would add such a list on pytorch side + added environment variable to bypass the list (might be useful to try out new platforms). 2. Here Intel needs to decide which GPUs it supports for pytorch and how this maps to oneAPI platforms. On our technical level the thing we can consider is this. Can we query for any sycl device properties which will tell us whether XPU backend will be able to run on such device? I afraid this might be nontrivial thing to do. Basically someone will need to debug platforms which fail like ADL and extract corresponding property characteristic.  Problem is that most likely there is no such characteristic and it actually should work on TGL and ADL, but there are bugs preventing this. And this might be not rational thing to invest in fixing such bugs. If this will be the case, then 1st will be the only viable option  restrict reported list of supported GPUs somewhere. With all above, I want to add that as of now ball to restrict reported list of supported platforms is on torchxpuops side. That's because you actually already have such list in a form of dpcpp command line flags. Here: https://github.com/intel/torchxpuops/blob/bbd200abe2d3a3e290c45e04127723f65293ed7e/cmake/BuildFlags.cmakeL87 You can just make this list available to you at runtime and check available devices against it.","> https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapidpcppsystemrequirements.html > https://www.intel.com/content/www/us/en/developer/articles/systemrequirements/inteloneapibasetoolkitsystemrequirements.html Actually, https://dgpudocs.intel.com/devices/hardwaretable.html, UMD (LTS or Rolling) and KMD (OS kernel + kernel drivers) decide which hardware works on current SW and HW platform. In the moment, it is hard to say oneAPI package can provide correct behavior or correct claim independently. In my experience, these things are decoupled, 1. Be able to compile. oneAPI (SYCL compiler/runtime compiler) commits a platform only if which is listed in compilation targets list. But probably, our IGC (included in UMD) doesn't work. 2. Be able to compute. oneAPI (SYCL runtime) returns a valid device only it is able to be probed in driver. But probably, the current kernel driver (KMD) doesn't work (https://dgpudocs.intel.com/devices/hardwaretable.html) when some computations occur. So I prefer oneAPI claims a max support list. Users have to check the matrix in driver Docs and find what they could actually get. I think it is confused to Intel GPU users. The best practice in my mind is, 1. Re Docs, we should not have a separate support list both in oneAPI and drivers, when we have a page to couple them together. 2. Re runtime, I m not sure of if there is a bug on ADL or the ADL SKU is not supported on your kernel driver. SYCL runtime/driver should always return a valid device only if it is workable (be able to compile and compute) in computation subsystem (SYCL/L0/KMD), not it is able to be probed only. > With all above, I want to add that as of now ball to restrict reported list of supported platforms is on torchxpuops side. That's because you actually already have such list in a form of dpcpp command line flags. Here: https://github.com/intel/torchxpuops/blob/bbd200abe2d3a3e290c45e04127723f65293ed7e/cmake/BuildFlags.cmakeL87 You can just make this list available to you at runtime and check available devices against it. It's a list of AOT build, not the support list. Devices not on the list still work with JIT build only if both oneAPI and driver support them. AOT build list aligns CUDA implementation in PyTorch supports the most recent and popular platforms. They are performance consideration. The white list we are talking about is functionality."," : please, decide which GPUs are going to be supported with XPU backend. Once decided, you can treat this issue either as the issue to fix the reporting list (if you will chose to restrict supported platforms to something, as of now DG2+) or as an issue to fix bugs to make full range of reported platforms work (as of now ADL fails).",WIP in https://github.com/intel/torchxpuops/issues/664
transformer,[INDUCTOR] Performance regression when batch size = 1 in text generation," üêõ Describe the bug The performance is much worse when batch size = 1 compared to batch size > 1 Profile when batch size = 1 (500ms) !image Profile when batch size = 4 (60ms) !image Command:     Versions Collecting environment information...                                                                                                                                                                              PyTorch version: 2.5.0.dev20240722+cpu                                                                                                                                                                             Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Rocky Linux release 8.10 (Green Obsidian) (x86_64) GCC version: (GCC) 12.2.0 Clang version: Could not collect CMake version: version 3.27.9 Libc version: glibc2.28 Python version: 3.8.19  (default, Mar 20 2024, 12:47:35)  [GCC 12.3.0] (64bit runtime) Python platform: Linux4.18.0553.5.1.el8_10.x86_64x86_64withglibc2.10 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              224 Online CPU(s) list: 0223 Thread(s) per core:  2 Core(s) per socket:  56 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU family:          6 Model:               143 Model name:          Intel(R) Xeon(R) Plati",2024-07-25T02:39:59Z,oncall: cpu inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/131734,fangintel ,"sync with feng, this is a regression issue which only for BS = 1. He will help to narrow down the nightly version which causes this regression.",The regression happens from `2.5.0.dev20240613+cpu` to `2.5.0.dev20240614+cpu`,"The rootcause is onednn has hit the ref path, for input of size [1, 1, 4096] and view it to [1, 4096], we get the stride of [0, 1]. In this case, oneDNN uses ref implementation."
transformer,Projeto liliti stk 3.6.9 intelig√™ncia artificial multimidal fase 5 , **1. Simula√ß√£o de Viagem no Tempo (Virtual)** **Objetivo:** Simular cen√°rios futuros baseados em dados hist√≥ricos.   **2. Interface de Realidade Aumentada e Virtual** **Objetivo:** Implementar uma interface b√°sica para RA/RV.   **3. Computa√ß√£o Qu√¢ntica** **Objetivo:** Implementar uma simula√ß√£o b√°sica para algoritmos qu√¢nticos.   **4. Reconhecimento Facial e An√°lise Emocional** **Objetivo:** Detectar e analisar express√µes faciais.   **5. Assistentes Pessoais Hologr√°ficos** **Objetivo:** Implementar um assistente hologr√°fico simples.   **6. Algoritmos de Previs√£o Temporal** **Objetivo:** Implementar algoritmos para previs√µes temporais.   **7. Gera√ß√£o de Conte√∫do Criativo com IA** **Objetivo:** Gerar conte√∫do criativo com IA.   **8. Interfaces de C√©rebroComputador** **Objetivo:** Implementar uma interface b√°sica c√©rebrocomputador.   **9. Simula√ß√£o de Realidades Paralelas** **Objetivo:** Criar e explorar realidades alternativas. ,2024-07-24T19:55:42Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/131692,"Closing, as unclear what this has to do with PyTorch, if you have a more concrete example please do not hesitate to open a new issue"
transformer,[dtensor][debug] adding new noise level which allows users to only print operations with dtensors,"  CC([dtensor][debug] adding new noise level which allows users to only print operations with dtensors) **Summary** I have added a new noise level between the existing levels of 1 and 2, such that the noise level controls are now:           0. prints modulelevel collective counts           1. prints dTensor operations not included in trivial operations (new noise level)           2. prints operations not included in trivial operations           3. prints all operations This gives the user more flexibility in controlling what information they want to use. The noise levels are used both for creating the console/file log and the json dump. In the example file, I have changed the module_tracing examples to noise level 0 and have changed my transformer examples to show off the new noise level.  **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_json_dump 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing 3. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_module_tracing 4. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_module_tracing ",2024-07-24T00:13:42Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/131592, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""CI timed out"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[torch.compile][HF] torch.compile issue tracker for torch.compile on forward method of Meta Llama model ," üêõ Describe the bug Script ~~~ from transformers import AutoTokenizer, AutoModelForCausalLM import torch import os os.environ[""TOKENIZERS_PARALLELISM""] = ""false""   To prevent long warnings :) torch._dynamo.config.inline_inbuilt_nn_modules = True tokenizer = AutoTokenizer.from_pretrained(""metallama/MetaLlama38B"") model = AutoModelForCausalLM.from_pretrained(""metallama/MetaLlama38B"", device_map=""auto"", torch_dtype=torch.float16) model.generation_config.cache_implementation = ""static"" model.forward = torch.compile(model.forward, mode=""reduceoverhead"")  backend=""eager"") input_text = ""The theory of special relativity states "" input_ids = tokenizer(input_text, return_tensors=""pt"").to(model.device) outputs = model.generate(**input_ids) print(tokenizer.batch_decode(outputs, skip_special_tokens=True)) ~~~ There are 2 issues I found * Recompilation because of stride change  We can probably build on top of https://github.com/pytorch/pytorch/pull/130232 and update `mark_dynamic` to work with strides as well. I think HF team is ok with the source code change to avoid this single recompilation as well. ~~~ DEBUG:torch._dynamo.guards.__recompiles:Recompiling function forward in /home/anijain/local/transformers/src/transformers/models/llama/modeling_llama.py:1021     triggered by the following guard failure(s):      0/0: tensor 'L['input_ids']' stride mismatch at index 0. expected 9, actual 1 ~~~ * Cudgraph skipping  ~~~ skipping cudagraphs due to mutated inputs (64 instances). Found from :    File ""/home/anijain/local/transformers/src/transformers/models/llama/modeling_llama.py"", line 1069, in forward     outputs = self.model(   File ""/home/anijain/loc",2024-07-19T23:28:50Z,triaged oncall: pt2 module: dynamo,open,0,3,https://github.com/pytorch/pytorch/issues/131265, (dynamic strides)   (cudagraph)  for static cache related issues,"Hmm, we handle two cases today  nn module buffers and parameters. These should be marked as static if https://github.com/pytorch/pytorch/pull/130391 is included in your runs (I assume it is because it relanded after a revert ~5 days ago) It's possible that these cache tensors are being handled in a nonstandard way that was papered over by the noninlining implementation. I can take a look Monday  I should be able to move the static marking from your PR into dynamo","Took a look, it appears that that the cache here is actually an input that is getting mutated: https://github.com/huggingface/transformers/blob/96a074fa7e2c04b904f72d9e827398d4c5f90f25/src/transformers/models/llama/modeling_llama.pyL582 We can mark it as a static input to resolve this, or no longer make it an input. , I think in the noninlining impl, this just wasn't an input (if it was instantiated as an attribute of an NN module higher in the hierarchy)"
transformer,Masked Attention has no effect in ``TransformerEncoderLayer``," üêõ Describe the bug  Problem description The forward method of `TransformerEncoderLayer` provides an argument to pass in a mask to zero specific attention weights. However, the latter has no effect. Here is a minimal script to reproduce. Notice that  the code doesn‚Äôt raise an error, though I set on purpose the mask to a wrong shape (it must be `(len_seq, len_seq)`)  changing the values of the mask (also its shape) has no effect on the output   Preliminary debugging The `TransformerEncoderLayer` uses `MultiheadAttention` and always calls its forward with ``need_weights=False`` https://github.com/pytorch/pytorch/blob/4aef5a1134b2735b43d41eaeb328103deb97e43c/torch/nn/modules/transformer.pyL918L926 Hence, because of the this `if` statement,  https://github.com/pytorch/pytorch/blob/4aef5a1134b2735b43d41eaeb328103deb97e43c/torch/nn/functional.pyL6021L6025 the attention mask is always set to `None`  Comments I would be happy to open a PR to fix the issue. Also pinging   Versions  ",2024-07-19T21:32:42Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/131254
transformer,Missing float8 storage," üöÄ The feature, motivation and pitch I'm trying to use float8 to try mistral nemo which was trained in a quantization aware way, to do inference in FP8. Ref:  https://mistral.ai/news/mistralnemo/  https://huggingface.co/mistralai/MistralNemoBase2407transformers  Alternatives _No response_  Additional context Environment:  macOS 14.5  Python 3.12.4  pytorch 2.3.1  transformers at 43ffb785c0b2f24948d2011883d40dccb609d341 Repo:  Failure:  When I try with float8_e5m2, the exception is Float8_e5m2Storage ",2024-07-19T15:40:09Z,triaged module: float8,open,0,8,https://github.com/pytorch/pytorch/issues/131196,`torch.set_default_dtype` seems to be broken with Float8  I think the error is here  https://github.com/pytorch/pytorch/blob/c64ad2403c0954a0c4a36c720f0c87b3d284d0c1/torch/csrc/tensor/python_tensor.cppL217L236 where it should be using `UntypedStorage` perhaps?,"  I think issue is that its mentioned that environment is a macOS environment, but I think macOS systems dont support float8 natively, could this be the problem?",any update on this guys?,"I had the same problem on ubuntu 22, the graphics card is 2080ti","I believe only Nvidia H100 and the like support fp8 right now, correct?",same issue on torch 2.4 on 4090,"Float8 support in PyTorch so far has focused on target hardware which supports the float8 gemm, such as NVIDIA machines with CUDA capability 8.9 and above.  We would welcome community contributions to make float8 work as a storage type on Mac OS!","same issue here, Ubuntu 22, 4090 GPUs, latest verison of most libraries"
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_5_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_5_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T12:50:44Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131179
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_2_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_2_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T06:49:01Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131146
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 18 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T03:43:43Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131123
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_5_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T03:43:35Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131120
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 15 failures and 5 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T01:06:15Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131107
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 4 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_2_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T01:04:21Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131095
transformer,DISABLED test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_0_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 12 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_3D_input_dim_no_attn_mask_dropout_p_0_0_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-19T01:04:09Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/131086
transformer,Runtime error when running huggingface pretrained model with torch-xla," üêõ Describe the bug Encountered `RuntimeError: isDifferentiableType(variable.scalar_type())` during the forward pass of the training script of a hugging face wav2vec2 conformer model. This error happens with a CPU device.  Full trace of the error is    Reproduction example Please find the reproducing script in the following section. This script is modified from https://github.com/huggingface/transformers/blob/main/examples/pytorch/speechpretraining/run_wav2vec2_pretraining_no_trainer.py, to include only the forward part of the training loop.  To run the script, invoke with the following command in a venv satisfying the requirement:  Or similarly    Versions The result include:  Additional packages include:   ",2024-07-17T21:04:12Z,triaged module: xla,open,0,0,https://github.com/pytorch/pytorch/issues/130985
transformer,BertForSequenceClassification.from_pretrained broken when using FSDP," üêõ Describe the bug The following code successfully loads the model checkpoint if ran using `python foo.py` or `accelerate launch foo.py` but not with FSDP enabled `accelerate launch use_fsdp foo.py`.  This seems like a bug where we say in `PreTrainedModel.from_pretrained` that `pretrained_model_name_or_path` can be None ""if you are both providing the configuration and state dictionary"", which I do here. But then if `is_fsdp_enabled()` is True, we set `low_cpu_mem_usage = True` and thus in turn `state_dict = None`, which causes the loading to fail.  Error message:   Versions 20240716 16:34:29  https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com):443... connected. HTTP request sent, awaiting response... 200 OK Length: 23357 (23K) [text/plain] Saving to: ‚Äòcollect_env.py‚Äô collect_env.py                                                                                    100%[===========================================================================================================================================================================================================================================================>]  22.81K  .KB/s    in 0.002s   20240716 16:34:29 (9.20 MB/s)  ‚Äòcollect_env.py‚Äô saved [23357/23357] Collecting environment information... PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4",2024-07-16T23:35:20Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/130875,"At the first glance, it does not look like a PyTorch issue, but rather a bug in `accelerate` framework(as backtrace suggestion) or in your code snippet. (perhaps you need to download checkpoint first and put it in a specific location?)  Have you tried asking this question on https://discuss.pytorch.org ?  Closing, but please do not hesitate to open a new issue if you can narrow it down to be a problem with PyTorch APIs",Opened an accelerate issue here
transformer,Training on M1 MBP: Placeholder storage has not been allocated on MPS device," üêõ Describe the bug Trying to do a simple training isn't working as expected with the following error:  I've been Googling around for a long time now and I I've done everything I can think of that might be my fault... I can't get the below to work on my MBP via MPS or CPU.... Reproduce Script:   Versions PyTorch version: 2.5.0.dev20240715 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.6.7 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: version 3.27.8 Libc version: N/A Python version: 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.6.7arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Max Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] onnx==1.16.1 [pip3] onnxconvertercommon==1.14.0 [pip3] onnxruntime==1.18.0 [pip3] skl2onnx==1.17.0 [pip3] torch==2.5.0.dev20240715 [pip3] torchaudio==2.4.0.dev20240715 [pip3] torchvision==0.20.0.dev20240715 [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.1.0.dev20230416          pypi_0    pypi [conda] torchaudio                2.1.0.dev20230416          pypi_0    pypi [conda] torchvision               0.16.0.dev20230416          pypi_0    pypi ",2024-07-16T00:27:58Z,needs reproduction triaged module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/130790,"Note this seems similar to CC(on MPS, torch.embedding, Linear and others raise: RuntimeError: Placeholder storage has not been allocated on MPS device!) which was closed, but it's uncertain why exactly it was closed to me?","Please note, that you somehow get both torch2.1.0 and torch2.5.0 installed in your environment, which can potentially cause conflicts","`Placeholder storage has not been allocated on MPS device` error means that one of the tensors passed to the operator has been allocated on CPU, while other is on MPS, which indicates problem in the training code rather than in PyTorch.  CC(on MPS, torch.embedding, Linear and others raise: RuntimeError: Placeholder storage has not been allocated on MPS device!) has been closed, because it was not a PyTorch error, though in 2.4 an improved message will be printed indicating which of the passed tensors has been on CPU instead of MPS device. Closing, as this does not look like a PyTorch issue, but rather a great topic for discussion on https://discuss.pytorch.org/"
transformer,PyTorch 2.4 windows performance regression compared with 0410 nightly," üêõ Describe the bug We are measuring performance on windows and observed that Windows whl performance regressed on rls/2.4 prereleased whl. In my local env, dev20240410 nightly whl was downloaded few month ago and we can see windows performance improved which might related to optimization PR by  . We are get our best to search which nightly whl caused this regression, but the oldest nightly whl is 0513 which is already regressed.  Hardware: 13th Gen Intel Core i713700H 2.4GHz OS: Windows 11 23H2 22631.3593  Versions How to reproduce: https://github.com/WeizhuoZhangintel/win_benchmarks/blob/main/torchvision_models.py     ",2024-07-12T14:01:15Z,high priority module: binaries module: windows triaged module: regression,closed,0,22,https://github.com/pytorch/pytorch/issues/130619,"Thanks for intel 's effort.  Here are two key PRs on the timeline:  1. https://github.com/pytorch/pytorch/pull/118980 merged on Mar 31. This PR enabled AVX2/AVX512 on pytorch Windows. So `Pytorch 0410` has perf improvememt to `Pytorch 0314`.  2. https://github.com/pytorch/builder/pull/1798 merged on April 26. This PR changes: a. switch mkl to static link. b. change mkl version. We can download the earliest nightly build was `0514` : https://download.pytorch.org/whl/nightly/torch/ The earlier nightly of `0514` was deleted. I plan to pull `release/2.4` code to debug https://github.com/pytorch/builder/pull/1798, Wish it has some progress.","Hi intel ,  give us the URL can download the nightly build before `0514`: https://pytorch.s3.amazonaws.com/whl/nightly/cpu/torch2.4.0.dev20240420%2Bcpucp311cp311win_amd64.whl Please do the binary search between `0410` to `0514`, and locate the issue date. Thanks.",Test with   on Pytorch 2.3.1:  on Pytorch 2.4.0:  Pytorch 2.1.0  Pytorch nightly from 0420  ,Tests:  Pytorch 2.4.0 (wheels)  Pytorch 2.4.0 (conda)  2.3.1  2.1.0  Pytorch 0420:  Pytorch 0424:  Pytorch 0428 ," could you please test the 0428, which after mkl changed?", We can fork from `release/2.4` and then: revert https://github.com/pytorch/builder/pull/1798 and https://github.com/pytorch/pytorch/pull/129493 Let's check if static link mkl cause the performance regression.,"If there are no regression between 2.3 and 2.4, we don't need to make any reverts",Rerun of 2.3.1 ,"> If there are no regression between 2.3 and 2.4, we don't need to make any reverts Hi  , This a regression since `0425` and it before the `release/2.4` branch cut. The data shows it seems not a regression to `2.3.1`. The reason is https://github.com/pytorch/pytorch/pull/118980 boost the performance and fill the `mklstatic` regression's performance gap. I think we need revert them for better performance.",I think the challenge here is that it's a bit late(and too risky) in the release cycle to do something like that.,"> I think the challenge here is that it's a bit late(and too risky) in the release cycle to do something like that. I known your concern, but the revert only impact on Windows. We can only rebuild the Windows binary.",hi  we would need to do complete rebuild of the rc. I agree with  we should target this change for 2.4.1 release,> hi  we would need to do complete rebuild of the rc. I agree with  we should target this change for 2.4.1 release release 2.4.1 is good for me.  we can revert them in the `main` branch. We need to fix and confirm them in nightly build. And then I will involve Intel MKL team look into the issue.,I suspect one can solve it by using wholelib flag when linking with the library,> I suspect one can solve it by using wholelib flag when linking with the library What flag? could you please show me the detailed information?,"Hi   Maybe we can try to PR fix build options for `main` branch, and fix `mklstatic`.  https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkllinklineadvisor.htmlgs.c3oeg2","MKL build options comparsion: Linux:  Windows:  It seems Windows MKL cmake file, miss some configuration. I will try to figure out it.","Hi  ,   I have submited a PR to fix Windows version `mklstatic` build options issue: https://github.com/pytorch/pytorch/pull/130697 . Please take a look. After this PR merged, intel we can test the new nightly build, and check if the perf regression issue would be fixed.",We verified the latest 0716 nightly whl of windows and this performance issue has been fixed.  ,"Hi  , We have verified PyTorch v2.4.1 RC build whl: https://download.pytorch.org/whl/test/cpu/torch2.4.1%2Bcpucp38cp38win_amd64.whl. And this performance issue was fixed.    ",Closing this one since fixed in Nightly and cherrypicked to RC,Confirmed final rc 2.4.1: 
transformer,"RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'/Reshape_5' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:45 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, onnxruntime::TensorShapeVector&, bool) input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,32,512}, requested shape:{1,1,8,64}"," üêõ Describe the bug I am trying to convert a pytorch transformer model to onnx. My model architecture consists of multiple nn.modules, so I am converting each to onnx separately. I am having to use a combination of torch.onnx.export() and torch.onnx.dynamo_export(), because some module conversions do not support dynamo_export yet. I am able to convert all the modules to onnx. However, when I run an inference session through the decoder module, I get the mentioned error. For reference, here is my Decoder class   Here is my code for onnx conversion of the module   Here is my inference code in onnx   I have marked the line which throws the error above. Here is the full error   Any help in resolving this issue would be appreciated. Thanks.  Versions PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.9 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.85+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linux",2024-07-11T14:08:18Z,module: onnx triaged,open,0,1,https://github.com/pytorch/pytorch/issues/130539,"Could you try `torch.onnx.export(..., dynamo=True, report=True)` with nightly? And attach the error report please."
transformer,[dtensor][debug] adding new noise level which allows users to only print operations with dtensors,"  CC([dtensor][debug] adding new noise level which allows users to only print operations with dtensors)  CC([debug][dtensor] implemented activation checkpointing differentiation)  CC([dtensor][debug] changed which module tracker I inherited from to fix bug with activation checkpointing) **Summary** I have added a new noise level between the existing levels of 1 and 2, such that the noise level controls are now:           0. prints modulelevel collective counts           1. prints dTensor operations not included in trivial operations (new noise level)           2. prints operations not included in trivial operations           3. prints all operations This gives the user more flexibility in controlling what information they want to use. The noise levels are used both for creating the console/file log and the json dump. In the example file, I have changed the module_tracing examples to noise level 0 and have changed my transformer examples to show off the new noise level.  **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_json_dump 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing 3. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_module_tracing 4. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_module_tracing  ",2024-07-11T03:56:50Z,oncall: distributed ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/130518,PR does not adapt to inserted PRs
transformer,[Feature Request] Lazy upcasting for mmap'd state dicts,"In transformers as a rule we load models always in as `float32` for stability, even if the weights are in `bfloat16`. As a result, loading `llama38B` can't be done lazily via mmap, since we have to upcast all the values in the `state_dict` immediately leading to some pretty slow timings. It'd be nice if there were an API exposed that could take a lazyloaded `state_dict` and hook into it to where once we request something from that state dict (before it's read from disk etc), we convert that parameter to `.to(float32)` automatically (or whatever precision you may want). transformers PR where this behavior would be *very* useful: https://github.com/huggingface/transformers/pull/31771discussion_r1672525371  ",2024-07-10T16:33:05Z,module: nn module: serialization triaged needs design,open,0,11,https://github.com/pytorch/pytorch/issues/130480,"Or, if this exists in PyTorch core I don't know, a hook when loading in the model weight via mmap to perform some operation on it? ",Actually thinking about this more but maybe we should discuss this on pytorch/pytorch since  and  can give a better answer than me,"This seems like a potential use case for `torch.Tensor.module_load`, which can hook into the `nn.Module.load_state_dict` call.  If the model definition contains a `__torch_function__` tensor subclass for all parameters, and the state_dict was loaded via mmap, `YourTensorSubclass.module_load` could be defined via `__torch_function__` to do the transformation from bfloat16 to float32 when each individual parameter is being loaded. Will look into this further!","That sounds perfect , looking forward to it!",My other fear with this is how easy it is to scale (e.g. can it be generic enough to apply via inheritance? and then all `transformer`'s models can make use of this functionality if configured). A toy example to see how `__torch_function__` would work in this case would be exceedingly helpful :) ,"Hey , does this snippet achieve what you are looking for? "," close but not quite! We still wind up loading the state dict in there, rather than delaying it as far as we can (aka an input goes through to the model). I'm not sure if what I want there actually *can* be possible.  As this is not really different from just going through and doing `.to(dest.dtype)` manually after loading in the state dict ourselves no? ","Oh, I see what you mean, hmmm I am slightly surprised that the decrease in first pass throughput is so small when lazy loading when the first input is passed to the model as compared to the model init time before :o Curious, for the throughput numbers for the first pass on https://github.com/huggingface/transformers/pull/31771issue2388519274 is the page cache cleared each time before mmaping the checkpoint?","*Probably* not, I'm not sure how to do that so some advice would be nice! :) However the hardware I'm working with (M.2 drives) can read up to 14.5GB/s, so it's not unreasonable to read llama8B's data so fast it seems almost instantaneous ","I'm not sure if there's a way to do it from python, but on linux I think it would be `sudo sysctl vm.drop_caches=1`"," destroyed the cache between runs.   New timings (llama38B in bfloat16 on CPU)  Model init: W/o lazy loading: 3.025 seconds W/ lazy loading: 0.319 seconds  First pass: W/o lazy loading: 2.353 tok/s (total time == 8.499s) W/ lazy loading: 2.020 tok/s (total time == 9.903s) At ~16gb for the weights, that's ~11.4GB/s (which makes sense for the speed of my m.2)  Second pass: W/o lazy loading: 2.444 tok/s (total time == 8.182s) W/ lazy loading: 2.434 tok/s (total time == 8.218s)"
transformer,[SDPA] Clean up `print` in `test/test_transformers.py` ,"Left this in CC([cuDNN][SDPA] Remove `TORCH_CUDNN_SDPA_ENABLED=1`, enable cuDNN SDPA by default on H100 and 2nd on other archs >= sm80), oops...",2024-07-09T00:54:05Z,open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/130302, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Failure to export Mixtral-8x7B-Instruct-v0.1 as onnx," üêõ Describe the bug trying to export loaded torch model as onnx: Simple python script here: `from transformers import AutoModelForCausalLM, AutoTokenizer import torch mistral_models_path = ""/home/ceti/models/textgen/Mixtral8x7BInstructv0.1"" tokenizer = AutoTokenizer.from_pretrained(mistral_models_path) model = AutoModelForCausalLM.from_pretrained(mistral_models_path, device_map=""auto"") onnx_program = torch.onnx.dynamo_export(model)` UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.   warnings.warn( Traceback (most recent call last):   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/exporter.py"", line 1428, in dynamo_export     ).export()   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/exporter.py"", line 1171, in export     graph_module = self.options.fx_tracer.generate_fx(   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/fx/dynamo_graph_extractor.py"", line 213, in generate_fx     graph_module, graph_guard = torch._dynamo.export(   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 1311, in inner     result_traced = opt_f(*args, **kwargs)   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 451, in _fn     return fn(*args, **kwargs)   File ""/home/ceti/miningarch/src/infra/triton/.env/lib/python3.10/sitepackages/torch/onnx/_internal/fx/dynamo_gr",2024-07-08T21:16:48Z,module: onnx triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/130274,Can you make sure `torch.export.export()` succeeds?,"I've experienced a similar issue with the `Mixtral8x7B` model. A more reproducible script is:  `torch.export.export` raises the error `torch._dynamo.exc.Unsupported: hasattr ConstDictVariable to`. I've seen a similar issue here)), is this something that we'd need to add a custom handler for?","Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated markdown report if there is an error. Thanks!","> Please test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated markdown report if there is an error. Thanks! Cheers!  My specific goal is to use `torch.export`, since I'm trying to use the StableHLO export system. However, with the latest torchnightly (`2.6.0.dev20241030+cpu`), I get the following: Markdown error report   PyTorch ONNX Conversion Error Report  Error message:    ",  you may want to open a new issue and tag the torch export team
transformer,[codemod] Fix deprecated dynamic exception in pytorch/FasterTransformer/FasterTransformer/tests/unittests/unittest_utils.h +5 (#129471),Summary: LLVM has detected a violation of `Wdeprecateddynamicexceptionspec`. Dynamic exceptions were removed in C++17. This diff fixes the deprecated instance(s). See Dynamic exception specification and noexcept specifier. Test Plan: Sandcastle Reviewed By: dmmfb Differential Revision: D58953076,2024-07-08T18:37:20Z,fb-exported ciflow/trunk topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/130263,This pull request was **exported** from Phabricator. Differential Revision: D58953076
transformer,"RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, int, NoneType, Device, bool,"," üêõ Describe the bug I have been trying to JIT trace Whisper, using this code:  However it fails with:  I did see  CC(RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, NoneType, NoneType, Device, bool, ), but while the error message looks similar, I was not doing an ONNX export, and also provided a reproduction. I did see lots of warnings like the following, but I don't believe they are relevant to this issue.   Versions  ``` ",2024-07-08T04:32:36Z,oncall: jit,open,2,9,https://github.com/pytorch/pytorch/issues/130229,", I only noticed today that this issue had been assigned to me :(",  any chance you have looked into this?  I am kind of blocked on my project until this is fixed..  Thanks in advance.,"I faced the same issue. Tried different versions of Pytorch: 2.4.0, 2.3.1, 2.3.0, 2.2.2","I found the solution. Changing the source code helped me. For example, the original code:  I changed to: ","sitsky, sorry, I had missed the notification! I haven't had a chance to work on it due to the approaching PyTorch v2.5 code freeze (Sep 6). I'll work on it after Sep 6. Just curious, BTW  why are you not using `torch.compile` to leverage Inductor? It's supposed to support a lot more models without requiring modifications in their sourcecode. TorchScript has been deprecated in the sense that its support is still present in PyTorch, but no new features would be added (except bug fixes, as in this case).","  thanks for your reply.  The purpose of this script is to generate a torchscript model for use with DJL, as described here: https://github.com/deepjavalibrary/djl/blob/master/examples/docs/whisper_speech_text.mdtracethemodel.",So is the solution is to change rewrite `torch.full` operation ?," This error seems to be happening because https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/ir/ir.cppL1035 returned false for various `aten::full` schemas for argument 1, which is a bool. Snippet of code to reproduce:  , please advise as to whether `bool` should be considered a subtype of `Scalar` for `torch::jit`. Thanks!","> So is the solution is to change rewrite `torch.full` operation ? In general, TorchScript supports fewer OOB models than torch.compile, and may require modifications in sourcecode to be supported. If modifying the model is feasible at your end, please go ahead with a workaround."
transformer,Memory leak when exporting hf model to onnx ," üêõ Describe the bug I want to export models from HuggingFace to ONNX format, but I encountered a memory leak issue with torch.onnx.export after performing multiple rounds of exports. Here is a quick code snippet to reproduce the problem  1. **When I commented out 'torch2onnx'**, I found that the peak memory usage for each iteration was around 30GB and was very stable. 2.  **When I added torch2onnx**, I discovered that the peak memory usage increased with each iteration, from 36GB to 48GB to 64GB.  Versions ",2024-07-04T12:09:02Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/130115
transformer,[TS2EP] Failing on longformer," üêõ Describe the bug I am testing the logic on huggingface longformer. This exact repro is a little complex because I have changed some source code in transformers, but the error stack may give you some ideas?   Versions main  ",2024-07-03T01:42:12Z,module: dynamic shapes oncall: export,open,0,3,https://github.com/pytorch/pytorch/issues/130008,torch._check_is_size(pad_idx) or whatever u0 is  another reason to implement your stack walker lol,Was this change needed in `torch/_refs` or in user code? Sorry I am not yet familiar with `_check_is_size`,User code. I guess it's also possible that maybe some framework code could use a check_is_size but if pad_idx is only passed into the indexing operation we can't do that because negative indices are supported there.
transformer,torch.distributed in different groups at the same time leads to `socketStartConnect: Connect to xxx.xxx.xxx.xxx<xxxx> failed : Software caused connection abort`," üêõ Describe the bug ```python import ray import torch import torch.distributed as dist from ray.air.util.torch_dist import (     TorchDistributedWorker,     init_torch_dist_process_group,     shutdown_torch_dist_process_group, ) from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy .remote(num_gpus=1) class TestWorker(TorchDistributedWorker):     def __init__(self):         super().__init__()     def run(self):         rank = torch.distributed.get_rank()         dev = f""cuda:{ray.get_gpu_ids()[0]}""         tensor = torch.tensor([rank]).to(dev)          case 1: whole group, part workers => blocking         if rank > 1:             group = dist.new_group([0, 1, 2, 3])             dist.broadcast(tensor, 2, group)          case 2: part group, all workers => success         group = dist.new_group([2, 3])         dist.broadcast(tensor, 2, group)          case 3: part group, part workers => success         if rank > 1:             group = dist.new_group([2, 3])             dist.broadcast(tensor, 2, group)          case 4: different groups, all workers => error         if rank  20240702 10:33:31,362 INFO worker.py:1749  Started a local Ray instance. > (TestWorker pid=2123894) [W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt) > (TestWorker pid=2123892) [W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt) > (TestWorker pid=2123891) [W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use",2024-07-02T02:41:34Z,oncall: distributed,closed,0,1,https://github.com/pytorch/pytorch/issues/129926,I fixed this problem by replacing `new_group` with `new_subgroups_by_enumeration`. This issue can be closed.
transformer,DISABLED test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_0_cuda (__main__.TestTransformersCUDA),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 6 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_scaled_dot_product_attention_4D_input_dim_no_attn_mask_dropout_p_0_0_cuda` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_transformers.py` ",2024-07-01T03:41:42Z,triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/129853
transformer,Implementation of a numerically stable log(1 - softmax) function in PyTorch," üöÄ The feature, motivation and pitch Hello, I am working on editing knowledge in transformers which, in the case of knowledge deletion, requires the minimization of the likelihood of targeted sequences of tokens, and therefore, I need in my work, a numerically stable log(1softmax) function. Details are below. The maximization of the likelihood of correct tokens is extensively used when training LLMs, and it requires an efficient and numerically stable implementation of the log(softmax(x)) function (to compute the CrossEntropy Loss) which was available in Pytorch for as long as I can remember. On the other hand, the minimization requires the implementation of a stable log(1softmax(x)) function **which is not available at the moment**. The naive implementation of log(1softmax) using torch.log and torch.softmax is highly unstable when the softmax(x) is close to 1 for one element of its output. I used it myself and I found that my loss sometimes equaled ""inf."" which consequently, produced ""nan"" values when backpropagating. I consider the log(1softmax) to be an important function in my usecase but also more generally in classification tasks. One could assume that someone would like to maximize some classes while minimizing others in some usecase. After asking for a numerically stable log(1softmax) function in a Pytorch forum, a user named KFrank proposed an elegant solution:  I generalized it to multidimensional tensors:  Finally I tested its numerical stability ; the methodology and results are explained in the previous Pytorch forum.  Request Is it possible to add this function to the list of native functions of torch, such that it can be u",2024-06-27T14:36:42Z,module: numerical-stability triaged module: python frontend,open,3,2,https://github.com/pytorch/pytorch/issues/129657,I'm curious why we need a native implementation for this? Would it be more performant than the python implementation suggested above? Also curious if using `torch.compile()` on this python implementation works well. That would remove the need to have to write a fast kernel by hand!," maybe the question is on having a shortcut for this in the core (as maybe for `log(1sigmoid(x))`), and the premise is that these are needed fairly frequently (and tricky to come about this impl handling numerics correctly) if you judge these are fairly frequently needed, I think it's great that if torch.compile produces fast fused codegen, then more of such functions can be added in core just as:  and maybe CI tests could exist to track regressions or memory consumption of produced generated code (in the meanwhile, it's a great workaround to have it pasted in user code as is of course)"
transformer,`torch.compile()` failing with PyTorch 2.3 for tupled inputs," üêõ Describe the bug One of our Torch compile tests was running successfully with PyTorch 2.2 but isn't anymore with PyTorch 2.3.1. Have tried with the nightlies but no luck.  Error logs   Minified repro I am not sure if running the minifier would be sensible here. So, going to provide all the steps to reproduce: 1. `git clone https://github.com/huggingface/diffusers/` 2. `cd diffusers && pip install e .` 3. `cd benchmarks && mkdir benchmark_outputs` 4. `pip install transformers accelerate` 5. `python benchmark_ip_adapters.py run_compile`  Versions  ",2024-06-27T07:33:19Z,triaged oncall: pt2 module: dynamo,open,0,0,https://github.com/pytorch/pytorch/issues/129637
transformer,[dtensor][be] Reduced redundant LOC by creating functions to set up models used in example,"  CC([dtensor][debug] added deviceMesh for relevant operations and module parameter sharding and module fqn)  CC([dtensor][debug] Added functionality to convert log into a json file)  CC([dtensor][be] Reduced redundant LOC by creating functions to set up models used in example)  CC([dtensor][debug] Added forward and backward differentiation for module level tracing) **Summary** As the CommModeFeature example file grew, there were to many LOC that was repeated for setting up the models used. I created two functions, one to handle MLP and MLPStacked models and the other for transformer models. The output of the examples will not have changed. **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_distributed_sharding_display 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLPStacked_distributed_sharding_display 3. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_module_tracing 4. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_module_tracing 5. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_operation_tracing 6. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing ",2024-06-26T23:28:11Z,oncall: distributed better-engineering Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129613, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model,  CC([Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model)  CC([Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP)  CC([Traceable FSDP2] Add autofunctionalize support for mutable list[Tensor] (copy from Brian's PR 127347); enable E2E inductor unit test for transformer model) ,2024-06-26T23:16:59Z,oncall: distributed Stale topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/129611,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[CP] Make CP loss curve on par with TP,"  CC([CP][Experimental] Introduce enable_context_parallel() and context_parallel_buffers())  CC([dtensor][experimental] distribute_function)  CC([CP] Refactor the code)  CC([CP] Make CP loss curve on par with TP)  CC([CP] Fix the incorrect ring schedule in the fwd and bwd)  CC([dtensor] Add dtensor to TORCH_LOGS) Summary: This PR changes two implementations to make CP (CP8) lose curve be on par with TP (TP8). 1. Making key and value contiguous before doing ring attention. It is unclear why this is a requirement as SDPA does not have this requirement. 2. Use the out, grad_out, softmax_lse passed by autograd to do the backward. This implementation is similar to the implementation in transformer engine. The original implementation reruns the SDPA to get the output and logsumexp and uses that reculcated results to infer the corrected softmax_lse. But that implementation does not give a better accuracy or lose curve. Instead, that implementation converges slower. ",2024-06-25T21:42:51Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129515, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[Traceable FSDP2] Add auto-functionalize support for mutable list[Tensor] (copy from Brian's PR #127347); enable E2E inductor unit test for transformer model,Copy of Brian's PR: https://github.com/pytorch/pytorch/pull/127347 with additional changes to support mutable `List[Tensor]` in Inductor. Also enable E2E inductor unit test for Traceable FSDP2 + transformer model. Test commands:  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_trace_fsdp_set_`  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_simple_mlp_fullgraph_backend_aot_eager`  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_simple_mlp_fullgraph_backend_inductor`  `pytest rA test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_transformer_fullgraph_backend_aot_eager`  `pytest rA test/dynamo/test_misc.py::MiscTests::test_auto_functionalize_tensorlist`  `pytest rA  test/inductor/test_torchinductor.py::GPUTests::test_fallback_mutable_op_list_cuda`   CC([Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model)  CC([Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP)  CC([Traceable FSDP2] Add autofunctionalize support for mutable list[Tensor] (copy from Brian's PR 127347); enable E2E inductor unit test for transformer model) ,2024-06-25T20:33:07Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing module: inductor module: dynamo ciflow/inductor keep-going,closed,0,8,https://github.com/pytorch/pytorch/issues/129502,"Ok, we found that we can't actually functionalize `fsdp.split_with_sizes_copy` without also functionalizing `fsdp.set_`, and we already know we can't functionalize the latter because we want to avoid AOTAutograd input dedup struggle. So we prefer to also defunctionalize `fsdp.split_with_sizes_copy` instead.",Closing in favor of https://github.com/pytorch/pytorch/pull/129422, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macospy3arm64 / test (default, 1, 3, macosm1stable) Details for Dev Infra team Raised by workflow job ","CI shows error in `test_ci_sanity_check_fail.py::TestCISanityCheck::test_env_vars_exist` macospy3arm64 / test (default, 1, 3, macosm1stable), which I don't believe is related to this PR. I will proceed to land this PR."," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[codemod] Fix deprecated dynamic exception in pytorch/FasterTransformer/FasterTransformer/tests/unittests/unittest_utils.h +5,Summary: LLVM has detected a violation of `Wdeprecateddynamicexceptionspec`. Dynamic exceptions were removed in C++17. This diff fixes the deprecated instance(s). See Dynamic exception specification and noexcept specifier. Test Plan: Sandcastle Reviewed By: dmmfb Differential Revision: D58953076,2024-06-25T13:24:28Z,fb-exported topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/129471,This pull request was **exported** from Phabricator. Differential Revision: D58953076,This pull request was **exported** from Phabricator. Differential Revision: D58953076
transformer,[PT2][fp8][FSDP2] compile the function that pre-computes fp8 amax ," üöÄ The feature, motivation and pitch share repro for  ,  on the gaps of torch.compile for FSDP2 fp8 allgather for FSDP2 fp8 allgather, it's criticial to precompute amax for `model.parameters()`  However, `compute_amaxes_compile` is slower than `compute_amaxes_eager` Sanpshot of cpu overhead  snapshot of gpu time  repro * pytorch with nested tensor subclass support, otherwies IMA during torch.compile https://github.com/pytorch/pytorch/pull/127431 * torchao: https://github.com/pytorch/ao * command `torchrun standalone nproc_per_node=2 compile_precompute.py`     Alternatives _No response_  Additional context _No response_ ",2024-06-25T07:00:52Z,oncall: distributed triaged module: fsdp oncall: pt2 pt2d-triage-nov2024,open,0,10,https://github.com/pytorch/pytorch/issues/129457,"Also, I think that _foreach_max was added in :   https://github.com/pytorch/pytorch/pull/127187"," I think we are already using `_foreach_norm(ord=math.inf)` in the eager path, which can compute both the abs and the max together. (If we used `_foreach_max`, then we still need to run abs first separately.)","Yeah, I was mainly wondering about the compile path... Currently it has a list comprehension for computing a max...","Findings so far: Eager version contains 1 nccl:all_reduce operation when it handles torch.clamp Compiled version contains a lot of nccl:all_reduce ( I guess one for every split item). They are coming from generating backward and keeping that backward inside forward. (I suspect mutations were registered.) E.g. https://gist.github.com/IvanKobzarev/245e088b449885af54eaf992050d6928 is the default graph (with training) If to use   Compiled duration is x2 faster than eager (used torch.no_grad() for compiled and eager)   , if you use it only for inference, potentially to unblock for compilation will be to add `with torch.no_grad()`: With no_grad subclasses overhead is pretty feasible on the profiling is about 300us for wrap and 300us unwrap when compute_amaxes_compiled region execution is 400us In total 1ms. Debugging it.","> Findings so far: Eager version contains 1 nccl:all_reduce operation when it handles torch.clamp >  > Compiled version contains a lot of nccl:all_reduce ( I guess one for every split item). They are coming from generating backward and keeping that backward inside forward. (I suspect mutations were registered.) is it possible for compiler to have 1 nccl:all_reduce? there can be N parameters (say N = 100), and we have to do them together for perf reasons I do not need backward so `no_grad` is feasible. just the entering no_grad and exiting no_grad might degrade the perf? ","> is it possible for compiler to have 1 nccl:all_reduce? there can be N parameters (say N = 100), and > we have to do them together for perf reasons no_grad compiled version has 1 all_reduce, the same as eager. Additional all_reduce show up only when grad_enabled(). > I do not need backward so no_grad is feasible. just the entering no_grad and exiting no_grad  > might degrade the perf? There should not be any degradation in entering no_grad.  It will instruct aot_autograd to not generate backward and any autograd tracking. So it will not present in any graph behind the dynamo graph.","Checked the unwrap/wrap logic, have not found any special. The model has 20 fp8 weights. They become the input arguments. Unwrap/wrap of each of them takes 14us. As a result we get 20 * 14us. The problem is that compiled kernel will be optimized to be smaller and smaller, but this overhead of wrap/unwrap is proportional to number of weights. ","If you need only inference, we can fuse all wrap/unwrap of weights into the graph using inductor freezing.  Results in compiled to be x6 faster than eager ","Found that specifying `__slots__` a bit helps with the cost of `flatten`, on my profile it i s `14us > 11us` per fp8 weights. For 20 weights in this example this makes 300us > 260us. https://github.com/pytorch/ao/pull/1211 ","> I do not need backward so no_grad is feasible. just the entering no_grad and exiting no_grad might degrade the perf?  another reason to use `no_grad()` (even if you are worried about a few microseconds of python overhead) is that without no_grad, compile will (pessimistically) assume that you need to train on that function, and potentially compute and saveforbackward extra activations that you don't actually need."
transformer,[no ci][FSDP2] Squash for ghimport,"  CC([no ci][FSDP2] Squash for ghimport)  CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out) ",2024-06-24T14:44:17Z,oncall: distributed release notes: distributed (fsdp2),closed,0,1,https://github.com/pytorch/pytorch/issues/129377,Still `patch does not apply` üò¢ 
transformer,[test only] TORCH_LOGS_RANKS,"  CC([test only] TORCH_LOGS_RANKS)  CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out)  CC([Traceable FSDP2] Fix support for CUDA resize_storage_bytes_)",2024-06-21T22:37:06Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/129283
transformer,fix test_fake_distributed_aot_eager,"  CC(fix test_fake_distributed_aot_eager)  CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out)  CC([Traceable FSDP2] Fix support for CUDA resize_storage_bytes_) ",2024-06-21T19:55:19Z,oncall: distributed release notes: distributed (fsdp) module: inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/129263
transformer,Release corresponding CUDA deps of `pytorch-nightly::pytorch-cuda` along with `pytorch-nightly::pytorch`," üöÄ The feature, motivation and pitch When compiling PyTorch with BuildExtension, it is common to encounter issues related to mismatched CUDA versions between different dependencies. This feature aims to address and resolve such issues efficiently. Consider the following example command to setup an environment: `conda create n dev python=3.12 pytorch torchvision torchaudio transformers scipy numpy rich ninja pytorchcuda cuda c pytorchnightly c nvidia y` In this case, the latest available version of CUDA is 12.5, and the pytorchnightly::pytorch package also aligns with this version at 12.5. However, the pytorchnightly::pytorchcuda package remains at version 12.4. This misalignment can cause compatibility issues during the build process, leading to potential failures or suboptimal performance.  Alternatives _No response_  Additional context _No response_ ",2024-06-21T11:24:53Z,module: binaries triaged enhancement needs design,open,0,4,https://github.com/pytorch/pytorch/issues/129230,"Do I understand correctly, that ask here is to build PyTorch using cuda12.5? I don't think that's the case (or the plan) right now.  If  there is an issue already that asks for more frequent CUDA updates, it should be closed as duplicate. Otherwise, let's keep this one as such issue.  IMO the goal is worthy, but in practice it's too problematic, as PyTorch might not be fully compatible with each version of CUDA released. But perhaps we can relax the restrictions, at least for the pytorchnightly channel  what do you think about it?"," , thx for your reply. I suggest to release the `nightly` version of `deps` of pytorch, so that we can perform testing withouting struggling to guess the `cuda` related environment setup. For example, if `pytorchnightly::pytorch` is built on `cuda12.5.x`, it would be best to release the `pytorchnightly::pytorchcuda` on the same `cuda12.5.x` to match with each other. Also, if u check the `pytorchnightly::pytorch` install, u will find it was already built on `cuda12.5`, see https://anaconda.org/pytorchnightly/pytorch","> For example, if pytorchnightly::pytorch is built on cuda12.5.x, it would be best to release the pytorchnightly::pytorchcuda on the same cuda12.5.x to match with each other. ius this is already the case, isn't it? I.e. `conda install c pytorchnightly c nvidia pytorch pytorchcuda=12.4` will install PyTorch built against 12.4, and if you change the version to 12.1, it will downgrade all the dependencies to 12.1, wouldn't it? > Also, if u check the pytorchnightly::pytorch install, u will find it was already built on cuda12.5, see https://anaconda.org/pytorchnightly/pytorch Where do you see 12.5 binaries? There are 11.8, 12.1 and 12.4, aren't there?","> IMO the goal is worthy, but in practice it's too problematic, as PyTorch might not be fully compatible with each version of CUDA released. But perhaps we can relax the restrictions, at least for the pytorchnightly channel PyTorch itself (i.e. the code in `upstream/main`) should always be compatible with the latest CUDA release, as we upstream needed changes and are also releasing a TOT PyTorch builds with the latest CUDA stack each month in our NGC containers.  The binary build process, however, is not trivial and is currently described here. We are working on improving this workflow as it's currently too slow for quick CUDArelated updates ( already has PRs open to unify `pytorch/builder` with `pytorch/pytorch`, which would hopefully improve the update speed). > For example, if pytorchnightly::pytorch is built on cuda12.5.x I don't see any binaries built with CUDA 12.5 and the link also does not show any. If you've found some, maybe a community member created these (the build would not be maintained by us then). > But perhaps we can relax the restrictions, at least for the pytorchnightly channel  Yes, we could try to relax some versions in the nightly binaries and might want to test these in CI in the same PR to see if any conflicts would be raised. The stable binaries might need to stick to a `MAJOR.MINOR` tag. "
transformer,model.generate(..) slow and huge GPU memory consumption," üêõ Describe the bug Hi, my setup  When calling the following code with `torch==2.3.0` or `torch==2.3.1`  it seems to load the model a second time to the GPU instead of just performing inference. Observed with `watch n 1 d nvidiasmi`. When using `torch==2.2.2` inference runs smoothly within seconds. Thank you very much and best regards  Versions ",2024-06-21T10:44:25Z,needs reproduction triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/129226,"Hey , I'm unable to run your repro code to try reproducing the problem.  If you are able to provide a runnable repro, that would help us help you  thanks!","> Hey , I'm unable to run your repro code to try reproducing the problem. >  >  >  > If you are able to provide a runnable repro, that would help us help you  thanks! Ah yes, sorry. So I was using the following model: https://huggingface.co/casperhansen/llama370binstructawq Download/clone it and adjust the path to your local model path, but I guess you know that. Or do you need it in some other form?",Just want to bring this comment to your attention which may be relevant: https://github.com/casperhansen/AutoAWQ/issues/523issuecomment2195365651,"oh, alright thank you very much   I close this issue then"
transformer,[Traceable FSDP2] Fix support for CUDA resize_storage_bytes_,"Currently if `x` is a CUDA tensor, calling `x.untyped_storage().resize_()` seems to always go into the `built without cuda` branch of `resize_storage_bytes_()` regardless of whether PyTorch is built with CUDA. I suspect this is because `inductor_ops.cpp` is only included in `libtorch_cpu.so` thus doesn't have the `USE_CUDA` information or ability to link to CUDArelated functions. This PR moves `resize_storage_bytes_()` related custom op functions out of `inductor_ops.cpp` into its standalone file `resize_storage_bytes.cpp` to be included in `libtorch_python.so` instead. This mimics the setup for `StorageMethods.cpp`. This way, `resize_storage_bytes_()` can have access to the CUDArelated functions, which passes the CUDA unit test.   CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out)  CC([Traceable FSDP2] Fix support for CUDA resize_storage_bytes_) ",2024-06-21T05:53:37Z,Merged topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129215," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,"[Traceable FSDP2][Brian's PR #128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out","This is a copy of Brian's PR https://github.com/pytorch/pytorch/pull/128754, with some changes in the test_distributed_patterns.py unit tests to more closely reflect FSDP2 patterns. Also disabled two tests `test_input_mutation_storage_resize_up_down` and `test_input_mutation_storage_resize_not_supported` in test_aotdispatch.py until we figure out the right behavior for them.   CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out) ",2024-06-21T04:25:50Z,oncall: distributed Merged release notes: distributed (fsdp) topic: not user facing module: inductor module: dynamo ciflow/inductor keep-going,closed,0,2,https://github.com/pytorch/pytorch/issues/129203," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model,"This PR adds Traceable FSDP2 `aot_eager` backend E2E tests for simple MLP as well as transformer model.   CC([Traceable FSDP2] Add `aot_eager` backend E2E tests for transformer model)  CC([Traceable FSDP2][Brian's PR 128754] Use torch.ops.fsdp.set_ for FSDP2 storage resize; dont functionalize resize_, set_, split_with_sizes_copy.out) ",2024-06-20T19:04:29Z,oncall: distributed Merged topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/129157," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Regression in loading optimizer learning rate," üêõ Describe the bug Hey PyTorch team! Thanks for your hard work on the distributed checkpointing, big fan of the work! There seems to have been a regression between `2.2.2` and `2.3.0` that resulted in the optimizer learning rate being loaded incorrectly. It seems like the `lr` key in params group is not loaded from the checkpoint, resulting in the first step having the initialisation learning rate instead of the checkpointed one. This was not the case in `2.2.2` and only occurs after `2.3.0`.    debug_dcp.py    Torch 2.2.2     scratch222.txt    cont222.txt    Torch 2.3.0    scratch230.txt   cont230.txt      Versions PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 Nvidia driver version: 535.86.10 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      43 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:    ",2024-06-19T18:41:18Z,oncall: distributed triaged module: distributed_checkpoint oncall: distributed checkpointing,closed,0,5,https://github.com/pytorch/pytorch/issues/129079,Did a git bisect and found that f518cf811d8645b801de773c8d3f44fc00d9af1e is the first bad commit  ,"Narrowed down the root cause to the `tree_map_only` used by `_init_state_dict`. https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/planner_helpers.pyL297L303  Doing the `tree_map_only` outofplace and then assigning it this way only maintains references for the leaves but not the branches. Which I guess breaks some assumptions made by the reading code. Using the inplace `tree_map_only_` seems to resolve the issue, but im not sure if it breaks something else as suggested by  in the comment.   "," For now, you can use `dcp_state_dict[""optimizer""]` to ensure getting the correct loaded state_dict.  I think our test cases do not catch this use case where we access the state_dict through the original reference."," It seems like `dcp_state_dict[""optimizer""]` doesnt work either. ", I put up a fix for this. https://github.com/pytorch/pytorch/pull/129398
transformer,[inductor] don't materialize the large sparse matrix in CE bwd,"  CC([inductor] don't materialize the large sparse matrix in CE bwd) Inductor currently materialize a large sparse matrix in the backward pass for CrossEntropyLoss and load that to compute gradients of Softmax input. If we could fuse the sparse matrix computation to the consumer sides, we gonna have both perf and memory usage wins. The Fx graph snippets that construct this aforementioned sparse matrix looks like:  Leveraging the following observations:  the scatter is applied upon a all zero (or more generally a const tensor)  the index tensor for the scatter has a single element on the scatter dimension. In this case it's the label tensor allow us to lower this 'scatter_upon_const_tensor' pattern to a pointwise kernel that can be easily fused with downstream kernels:   Test result on microbenchmark For the microbenchmark added as `test_cross_entropy_loss`, we improve latency from 47.340ms to 42.768ms, memory footprint from 10.524GB to 7.227GB on A100. (on H100, we improve latency from 27.54ms to 23.51ms, memory footprint from 10.574GB to 7.354GB). The saving matches the backofenvelope calculation. We avoid storing a BF16 tensor with shape [30K, 50K] which is about 3GB in size. On A100, avoid loading and storing such a tensor can roughly save 3GB x 2 / 1.5TBGS = 4ms  Test result on llm.c We also test this on llm.c and the saving is much larger especially for memory footprint. The reason is due to autotuning that allocates extra memory for benchmarking. (Check  CC(Move Memory Allocation for Autotuning out of the critical path) and https://github.com/pytorch/pytorch/pull/129399 for more details). For llm.c PyTorch implementation on A100, w",2024-06-19T06:19:10Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/129043,What's the graph we're matching here?,> What's the graph we're matching here?  : Here is the whole backward graph https://gist.github.com/shunting314/2082ee22613848b937ef992cc8717bd7 for the microbench I run. And the matched part is: ,"Horace gave me a nice suggestion offline to pattern match only the scatter+allzero part and let inductor figure out the rest itself. It works well. This makes code much simpler and more general. And now I don't need to duplicate the patterns to make it work for llm.c (which does not have the convert element type node). For llm.c, I see larger gain then I expected. I was only expecting a saving of 6GB memory (a [30k, 50k] fp32 tensor). But the benchmark results shows the memory usage drops from 33556 MiB > 18595 MiB.  I would like to dig more on this", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[dtensor][debug] add operation tracing to comm_mode,"  CC([dtensor][be] Reduced redundant LOC by creating functions to set up models used in example)  CC([dtensor][debug] Added forward and backward differentiation for module level tracing)  CC([dtensor][debug] add operation tracing to comm_mode) **Summary** I have added an even more detailed module tracker that now includes the collective counts and operations that happen in each submodule making it easier for users to debug. The tracing now includes the operation's DTensor arguements' input shape and sharding. Like the module collective tracing, the user also has the option to log the tracing table to output.txt file. I have decided not to include the example output for transformer as it is too many lines. The expected output for the MLP_operation_tracing is shown below:   **Test Plan** 1. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e MLP_operation_tracing 2. torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/comm_mode_features_example.py e transformer_operation_tracing ",2024-06-19T00:21:48Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/129017,let's convert this PR to Draft since it's still WIP.,Curious of what is your decision of printing out the mesh for the DTensor., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[inductor][cpu]transformers models static/dynamic quant performance/accuracy crash in 2024-06-17 nightly release, üêõ Describe the bug   Versions SW info               SW       Branch       Target commit       Refer commit                       Pytorch       nightly       8410bf5       963d450                 Torchbench       chuanqiw/inductor_quant       ee35d764       ee35d764                 torchaudio       nightly       b829e93       1980f8a                 torchtext       nightly       b0ebddc       b0ebddc                 torchvision       nightly       d23a6e1       d23a6e1                 torchdata       nightly       11bb5b8       11bb5b8                 dynamo_benchmarks       nightly       fea73cb       fea73cb           Repro:  Suspected guilty commit: https://github.com/pytorch/pytorch/commit/2229884102ac95c9dda0aeadbded1b04295d892e textclassification+albertbasev1staticquantaccuracycrash_guilty_commit.log ,2024-06-18T06:03:53Z,oncall: pt2 module: dynamic shapes oncall: cpu inductor,open,0,8,https://github.com/pytorch/pytorch/issues/128933,"Hi , could you kindly help to take a look? Prepare the script to reproduce this issue: https://gist.github.com/lesliefangintel/696041fa7e7352ecb985b04a5e1188de and it starts to fail since https://github.com/pytorch/pytorch/commit/2229884102ac95c9dda0aeadbded1b04295d892e Here are the version of transformer I used `pip install ""git+https://github.com/huggingface/transformers""` in case needed.",vision_maskrcnn and detectron2_fcos_r_50_fpn AMP/float32 single/multiple thread static/dynamic shape default/cpp wrapper  meet `TypeError: Invalid NaN comparison` https://gist.github.com/zxd1997066/5f1fc727ced62f4ae82df88ea232f863 And they have the same guilty commit https://github.com/pytorch/pytorch/commit/2229884102ac95c9dda0aeadbded1b04295d892e bisect log: torchbenchvision_maskrcnninferencefloat32staticdefaultmultipleaccuracycrash_guilty_commit.log Repro: inductor_single_run.sh ,"Running this test with `TORCH_LOGS=""+dynamic""` We can find the guard difference before and after this commit:  Previously, we can statically known `s0 != 9223372036854775807`  However, after this commit, we have to add the guard which causes the failure. ","Further looking into the why we can't statically known `s0 != 9223372036854775807` after this commit:  Before regression    Here the upper of `vr` is `9223372036854775806` and `offset` is `1` which make `add` returning `9223372036854775805`      https://github.com/pytorch/pytorch/blob/cac6f99d41baa8418e6b31834eb7829257acb24c/torch/fx/experimental/symbolic_shapes.pyL4533     Here comparing `b.lower` `9223372036854775807` and `a.upper` `9223372036854775806`, we can known they are not equal statically .      https://github.com/pytorch/pytorch/blob/17d1723aeeadbfc6d33c02ab56c5aacb8c671876/torch/utils/_sympy/value_ranges.pyL468   After regression:    Here the upper of `vr` is `int_oo` and `offset` is `1` which make `add` returning `int_oo`        https://github.com/pytorch/pytorch/blob/cac6f99d41baa8418e6b31834eb7829257acb24c/torch/fx/experimental/symbolic_shapes.pyL4533     Here comparing `b.lower` `9223372036854775807` and `a.upper` `int_oo`, we can't known they are not equal statically.      https://github.com/pytorch/pytorch/blob/17d1723aeeadbfc6d33c02ab56c5aacb8c671876/torch/utils/_sympy/value_ranges.pyL468 ","Not sure how to make the correct fix. If `b.lower` is larger than `sys.maxsize1` and `a.upper` is `int_oo`, can we say they are not equal in `SymPyValueRangeAnalysis`? ","But that guard sounds reasonable to me, no? It's asking that `s0` should be representable in `int64`. I'm not sure how the points above are related to the failure, and it's difficult to know without more context. Looking at the error in  CC([inductor][cpu]transformers models static/dynamic quant performance/accuracy crash in 2024-06-17 nightly release)issuecomment2179004620, it might suggest that our `safe_mul` is not as safe as it should be. In particular, it might be doing something like `0 * sympy.oo` and it's returning a `NaN`. In that case, we should probably treat in that operation `0 * sympy_oo` (and same with `sympy_oo`) as 0, as this formula is equivalent to the limit `lim_{x>inf} 0 * x = 0`.  this shows a larger issue that's lurking with the inf treatment: Our bounds are inclusive... unless one of the ends is `oo`, in which case they are not...","> But that guard sounds reasonable to me, no? It's asking that s0 should be representable in int64. I'm not sure how the points above are related to the failure, and it's difficult to know without more context. Yean, any suggestions for how to further debug why the guard failed? I am just listing out the difference before and after this commit and maybe there is another potential issue which fails the guard :(  Update for why the new added guard fail   By previous debug, we will create a new guard as `Ne(s0, 9223372036854775807)`  And I see we will evaluate this again here which returns result of None. It actually hit the lru_cache, but I think it will follow same analysis in  CC([inductor][cpu]transformers models static/dynamic quant performance/accuracy crash in 2024-06-17 nightly release)issuecomment2184408260 even it didn't hit the lru_cache.    https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/fx/experimental/symbolic_shapes.pyL4107    Then we `record_constraint_violation` in `constraint_violations` here since there is a constraint with instance of `StrictMinMaxConstraint`        https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/fx/experimental/symbolic_shapes.pyL4090  When `constraint_violations` is not empty, we raise the error here https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/fx/experimental/symbolic_shapes.pyL4180","This is sort of expected, but what we probably can do is make the constraint violation error more tolerant for this case. The big question I had to answer in https://github.com/pytorch/pytorch/pull/127693/ was what I should do if there legitimately was different behavior when s0 == sys.maxsize.  Previously, I simply assumed this couldn't happen, because who makes sys.maxsize type tensors. But with int_oo modeling, ""just assuming"" it doesn't happen is not so convenient. But it's also not a big deal, you just get a guard testing that the int is not maxsize, nbd. Except for the constraint stuff. The constraint violation says ""if there is ANY guard, error out"". But we can probably make it softer, e.g., a guard that the value is not maxsize shouldn't trigger this."
transformer,Unable to export Phi-3-vision model to exported program," üêõ Describe the bug Repro:  Error message:   Versions Collecting environment information... PyTorch version: 2.4.0.dev20240412+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4080 Nvidia driver version: 535.161.08 cuDNN version: Probably one of the following: /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.9.3 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_ops_train.so.8.9.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                   ",2024-06-17T22:45:52Z,module: dynamic shapes oncall: export,open,1,7,https://github.com/pytorch/pytorch/issues/128906,"To fix `RuntimeError: shape '[1, s0]' is invalid for input of size 2`, could you try the following code? It seems the repro code uses a wrong order of inputs.  After using correct `args/kwargs`, I found another error `torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from datadependent expression u0 (unhinted: u0).  (Sizelike symbols: u0).` It comes from `if len(positions.tolist()) > 0: ...`. One fix could be specializing on one branch by updating the code. Full trace for new error: ", Thanks for your suggestion. I tried the code snippet you provided and got the exactly same error as yours. Do you have any ideas to fix it?, Do you have any updates for the bug?,"> It comes from if len(positions.tolist()) > 0: .... One fix could be specializing on one branch by updating the code. I removed `if len(positions.tolist()) > 0:` as  suggested, but unfortunately, this fix doesn't work for me. Here's the new error:  ",Using strict export will fix the problem. You can also rewrite the the `sub_img[:B_]` to use narrow instead which will avoid the specialization forced by Python slice," Thanks for the reply. I tried `strict=True` and `sub_img = sub_img.narrow(0, 0, B_)` and it works for this line, but afterwards, when hitting `sub_img.reshape(B_,H,H,C)`, got the error:  It seems there's a similar open issue here:  CC(Torch.onnx.dynamo_export stuck at reshape)issuecomment1751919505 Do you have any idea for this?",You should read https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/editheading=h.44gwi83jepaj This one looks like it would work if u1 was marked as size like. Find where you did the item() call and torch._check_is_size the result
transformer,`RuntimeError: invalid dtype for bias - should match query's dtype` when using torch.compile + FSDP + hf transformer," üêõ Describe the bug I was trying to use torch.compile + FSDP + huggingface transformer. I was able to make it work on one GPU, however, on 8 A100 GPUs, I ran into the following errors. I made a reproduce repo here: https://github.com/ByronHsu/torchcompilefsdperror.   Versions  ",2024-06-16T23:12:25Z,oncall: distributed triaged module: fsdp oncall: pt2 pt2d-triage-nov2024,open,2,3,https://github.com/pytorch/pytorch/issues/128798, ,Does anyone solved it?,"I got the same error on `GH200` + `HF trainer` + `compile` when training on **FP16** or **BF16**. **Container**: `nvcr.io/nvidia/pytorch:24.08py3`  Without model compile, it is all alright."
transformer,Change index_put on GPU to accept FP8 inputs,"As the title says, this PR changes the dispatcher for the CUDA index_put_ kernel to accept FP8 inputs. This is useful for Transformers models where the KV cache is FP8 and has been preallocated. ",2024-06-14T23:55:20Z,module: cpu triaged open source Merged ciflow/trunk release notes: cuda,closed,0,15,https://github.com/pytorch/pytorch/issues/128758,"Thanks, looks good after CI passes! Can we also add a test to test/quantization/core/experimental/test_float8.py?","> Thanks, looks good after CI passes! Can we also add a test to test/quantization/core/experimental/test_float8.py? actually, sorry, even better would be to add testing to `test/test_torch.py` instead of `test/quantization/core/experimental/test_float8.py`.  We should move the float8 tests in `test/quantization/core/experimental/test_float8.py` as well in a future PR.","ok, I'll add the test and check what's going on with CI tests!","added the test, but seems like CI is currently really broken...", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `index_put_fp8` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout index_put_fp8 && git pull rebase`)"," merge f ""unrelated test failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge i ""unrelated test failures""", merge i," Merge failed **Reason**: Approvers from one of the following sets are needed:  superuser (pytorch/metamates)  Core Reviewers (mruberry, lezcano, Skylion007, ngimel, peterbell10)  Core Maintainers (soumith, gchanan, ezyang, dzhulgakov, malfet) Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[torch.compile] Llama2 failure using dynamic shapes with Torch 2.4 nightly," üêõ Describe the bug torch.compile fails with the following error on Llama2. The error seems to occuring here : https://github.com/pytorch/TensorRT/blob/main/py/torch_tensorrt/dynamo/backend/backends.pyL90L97  Please run the following instructions to reproduce: 1) Please login via huggingfacecli (install it via pip install U ""huggingface_hub[cli]"" ) https://huggingface.co/docs/huggingface_hub/en/guides/clihuggingfaceclilogin. The user access token can be accessed in your settings. 2) Please install transformers via `pip install transformers=4.41.2`. You can install torch_tensorrt nightly via `pip install torch_tensorrt indexurl https://download.pytorch.org/whl/nightly/cu121`  3) Run the following script  The whole log can be found here  llama2_tc.log cc:   Any suggestions here ? Thanks   Error logs _No response_  Minified repro _No response_  Versions [pip3] numpy==1.26.4 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240610+cu121 [pip3] torchtensorrt==2.4.0.dev0+a8a079715 [pip3] torchvision==0.19.0.dev20240610+cu121 [pip3] triton==2.3.1 transformers==4.41.2 ",2024-06-12T20:03:40Z,triaged oncall: pt2 module: dynamic shapes,open,0,1,https://github.com/pytorch/pytorch/issues/128548,Confirmed that it fails without tensorrt. 
transformer,torch.onnx.export - `repeat_interleave` produces invalid model," üêõ Describe the bug While adding ONNX export support for Cohere models to Optimum (PR), I ran into an issue due to a single problematic `repeat_interleave` operation within `CohereRotaryEmbedding`. After further analysis, this does appear to be an issue with pytorch/onnx exporting. There is a chance this is related to ONNXRuntime, but in their issue page they link here for pytorch conversion errors. Minimal reproduction (adapted from the HF transformers repo for Cohere models):  This produces the following error:   however, if we replace the `repeat_interleave` with an equivalent op:  the model exports and runs correctly:   Versions PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.9 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.85+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.6 HIP runtime version: N/A MIOpen",2024-06-12T13:02:25Z,needs reproduction module: onnx triaged,open,0,1,https://github.com/pytorch/pytorch/issues/128505,"For completeness, I exported with `torch.onnx.dynamo_export` and that does seem to work: "
transformer,crash@sleef_tryVXE2 () while trying to run torch.compile() BERT model," üêõ Describe the bug Hi,  I have been trying to load a pretrained bert model `bertbasecased`  downloaded from HuggingFace and optimize it with torch.compile() on linuxs390x. It crashed    **`Version:`** Pytorch 2.2.0 **Reproduce:**  Download pretrained `bertbasecased` from huggingface. https://huggingface.co/googlebert/bertbasecased/tree/main run the below scripts    Versions **Version:** Pytorch 2.2.0 python  3.10.14 transformers  4.24.0 **platform**  Linuxs390x",2024-06-12T11:27:55Z,module: crash triaged module: sleef module: POWER,open,2,10,https://github.com/pytorch/pytorch/issues/128503, we have upgraded sleef library after v2.3. Could you please try to repoduce on latest daily build? ," No idea daily build if contains `s390x` OS, you can't build from source the `release/2.4` branch code also.",cc:  ,"Apparently it is crashing in the routine trying to figure out whether VXE2 is available. Being on a z14 machine (not having VXE2) a SIGILL is supposed to happen. However, Sleef installs a signal handler to catch that. The SIGILL really should not surface to the application in that case. I've extracted the code and tried it on a z15 machine to detect a z16 feature and it worked fine. Compiling the following example with ""gcc t.c o t march=z14 mzvector"" and running the program should result in an exit code of 0 on your box:  Does this work as expected for you?","> Apparently it is crashing in the routine trying to figure out whether VXE2 is available. Being on a z14 machine (not having VXE2) a SIGILL is supposed to happen. However, Sleef installs a signal handler to catch that. The SIGILL really should not surface to the application in that case. I've extracted the code and tried it on a z15 machine to detect a z16 feature and it worked fine. Compiling the following example with ""gcc t.c o t march=z14 mzvector"" and running the program should result in an exit code of 0 on your box: >  >  >  > Does this work as expected for you? Krebbel FYI: https://github.com/pytorch/pytorch/pull/123936 merged last month, you can try the latest daily build.","> Apparently it is crashing in the routine trying to figure out whether VXE2 is available. Being on a z14 machine (not having VXE2) a SIGILL is supposed to happen. However, Sleef installs a signal handler to catch that. The SIGILL really should not surface to the application in that case. I've extracted the code and tried it on a z15 machine to detect a z16 feature and it worked fine. Compiling the following example with ""gcc t.c o t march=z14 mzvector"" and running the program should result in an exit code of 0 on your box: >  >  >  > Does this work as expected for you? Krebbel Thanks ! It works fine for my system as expected with an exit code of 0. ","> Krebbel FYI: CC(s390x: use runtime detection for vectorization support) merged last month, you can try the latest daily build. Yeah, using getauxval is definitely the better way. Installing a signal handlers in a library might interfere with signal handlers used by the application. However, I'm curious to understand why it doesn't work in that particular case. The merged code change adds proper facility detection to the aten code. Wouldn't we still need to do the same for Sleef then?!",> Krebbel Thanks ! It works fine for my system as expected with an exit code of 0. Thanks for checking. I'm wondering why Sleef doing the same thing fails then :(,> sleef_tryVXE2 It seems sleef not handle the `disp_expf4_u10`'s dispatch correctly. pytorch 2.2 using the sleef that is two year's ago. I have upgrade the sleef version after pytorch 2.4. Still suggest you try the latest daily build: https://download.pytorch.org/whl/nightly/cpu ,I'll work on a PR for the issue in Sleef. See the Sleef issue for more details. Btw. the backtrace from the first comment is a red herring (and I fell for it too at first). GDB by default intercepts SIGILLs and that's what you see here in your backtrace. But in that case this is the normal operation of the feature detection in Sleef. The SIGILL is expected to happen here. The actual problem is triggered later. In order to see this you have to tell GDB not to intercept SIGILLs. This can be done with: 
transformer,InternalTorchDynamoError on converting llama-2 to onnx using torch.onnx.dynamo_export," üêõ Describe the bug Trying to convert llama2 model from HuggingFace to onnx using the new torch.onnx.dynamo_export fails.  Gives error: > InternalTorchDynamoError: 'NoneType' object has no attribute 'is_tracing' My main goal was to convert a quantized model and I was trying to use the old torch.onnx.export but I faced the problem described in CC(Export of bitwise_right_shift to ONNX needed for llama27b 8bit quantized pytorch model) so I tried to use torch.onnx.dynamo_export but it is giving this error. This is the quantized code which produces the same error.  Furthermore, adding LoRA fails with a different error message which seems to indicate that it isn't yet supported:  Error: > Unsupported: class property LoraModel getset_descriptor  Versions Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.16.1 [pip3] onnxscript==0.1.0.dev20240611 [pip3] torch==2.3.1+cu118 [pip3] triton==2.3.1 [conda] Could not collect ",2024-06-12T03:36:24Z,needs reproduction module: onnx triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/128480,"Could you test with `torch.onnx.export(..., dynamo=True, report=True)` using the latest torchnightly. Attach the generated report if there is an error. Thanks!"
transformer,xpu: gradient checkpointing wrongly hits cuda path running on non-cuda devices,"Issue found running some examples and tests from Huggingface transformers on the system with only XPU device and pytorch built w/o CUDA. As of: * pytorch at https://github.com/pytorch/pytorch/commit/70a1e8571802c22c0f09279b77876e6e85c81325 * https://github.com/intel/torchxpuops/commit/2e6be8c46196c013610e7a53771ce0d357812179 And applying these patches to enable xpu backend for huggingface transformers: * https://github.com/huggingface/accelerate/pull/2825 * https://github.com/huggingface/transformers/pull/31238 I observe that cuda path is hit unexpectedly running some transformers examples and tests which use gradient checkpointing. See the log below. Here are my debug findings: * Issue happens on `loss.backward()` call on HF side. I checked `loss` tensor is on `xpu:0` device: https://github.com/huggingface/accelerate/blob/a9869ea0dc49652e49607d5f111caed79ed5cb67/src/accelerate/accelerator.pyL2136 * Then, inside pytorch other tensors are coming around on the following call and they are not on any device. As a result, default device path is hit which is `cuda`: https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL191 Potentially, there might be I missed something to enable xpu on HF side, but as of now I can't find what. It also might be that the there is some issue for xpu in pytorch. Need some help/guidance to debug and fix, so filing here for now. Log:  ",2024-06-12T02:25:21Z,module: checkpoint triaged module: xpu,closed,0,3,https://github.com/pytorch/pytorch/issues/128478,"This test starts to pass if `DefaultDeviceType::_default_device_type` will be changed from `cuda` to `xpu`, here: https://github.com/pytorch/pytorch/blob/02e7519ac3cd4c4b043c9a0f672464d3797c0622/torch/utils/checkpoint.pyL112 Questions I have are: 1. Where these CPUonly tensors came from on HF side? 2. Why `loss` tensor with clear xpu designation was not on the list of tensors queried for device at https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL191? 3. Is that an issue with pytorch logic with default device for checkpointing?   who from pytorch side can help advice here on further debug?","In further debug, I think I figured out where CPU tensors are coming from. These seem to be tensors returned by `torch.xpu.random.get_rng_state()` on `forward()` here: https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL185 And on `backward()` pytorch tried to get device type from these tensors which actually don't have this information, they are CPU tensors (though they correspond to `xpu` device) and as result device is assumed to be CUDA which is wrong. This happens here: https://github.com/pytorch/pytorch/blob/219da29dfd8fd39b783b0a25aef693e25bbe6c8a/torch/utils/checkpoint.pyL191 As far as I see, CUDA, XPU and MPS all return CPU tensors handling `get_rng_state()`. And they accept CPU tensors on the `set_rng_state()`. Questions: 1. What's the requirement for `getset_rng_state()`. I suspect we need 2nd case. Can pytorch maintainers, please, comment here?   can you, please, help to add relevant people to discussion? ",I have posted a PR CC(Fix device propagation for checkpointing) with the fix I suggest for this issue. It assumes that rng_state functions should work with CPU tensors. Fix is to save full device info on forward() and use it to get device type on backward().
transformer,Using PyTorch with Transformers to run inference with 'MPS' backend causes poor results.," üêõ Describe the bug When I run inference using a recent text embedding model, the tensor values and subsequent calculations are not expected values. Setup: Python version: 3.12.1 (main, Dec  7 2023, 20:45:44) [Clang 15.0.0 (clang1500.1.0.2.5)] PyTorch version: 2.2.2 Transformers version: 4.41.2 NumPy version: 1.26.4 Pandas version: 2.2.1  code and imports in a Jupyter Notebook in vscode import torch import torch.nn.functional as F from torch import Tensor from transformers import AutoTokenizer, AutoModel import numpy as np from sklearn.metrics.pairwise import cosine_similarity  Set up device for hardware acceleration device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')  Define the last token pool function def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) > Tensor:     left_padding = (attention_mask[:, 1].sum() == attention_mask.shape[0])     if left_padding:         return last_hidden_states[:, 1]     else:         sequence_lengths = attention_mask.sum(dim=1)  1         batch_size = last_hidden_states.shape[0]         return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]  Define the function to create detailed instructions def get_detailed_instruct(task_description: str, query: str) > str:     return f'Instruct: {task_description}\nQuery: {query}'  Assuming tokenizer and model are already loaded and moved to the device tokenizer = AutoTokenizer.from_pretrained('LinqAIResearch/LinqEmbedMistral') model = AutoModel.from_pretrained('LinqAIResearch/LinqEmbedMistral').to(device)  Move the model to the MPS dev",2024-06-11T18:42:51Z,triaged module: correctness (silent) module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/128435,"Hello, the issue does not appear to occur with the latest PyTorch nightly, mps returns the expected test_scores. If possible, please update to PyTorch 2.5.0 or later and let us know here if the issue still occurs: https://pytorch.org/getstarted/locally/","That sounds awesome, will try out this weekend!","Closing for now, please feel free to reopen if the issue is observed again"
transformer,Disable fast path in `TransformerEncoderLayer` when there are forward (pre-)hooks attached to modules,Fixes CC(Forward hooks not called when fast path is used in TransformerEncoderLayer)  Disable fastpath if there are forward hooks or prehooks. Example failure case given in the issue.,2024-06-11T13:20:01Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,20,https://github.com/pytorch/pytorch/issues/128415,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: iibrahimli / name: Imran Ibrahimli  (943e92d8c39f59b82c049a3bf56198f7d49d0569, 2a8bb3095ef190266b8b7b3e25907c0439e8c5a3, 33332a7f7a11f18dc7dc0cb589b52acec059fac2, d20cdd180c85824f4560545123fa7f9dc871c9e6, 2b780bb69f7cd589b0a38faa8adf8ed498bf1188, 28f93fc2124bbd288be6e819656807e158923d2b, 5c17236a933ac81b544ae3a19ae24b2129f43921)",Nit: could be faster by using any to allow for short circuiting.,"> Nit: could be faster by using any to allow for short circuiting. true. changed, thanks",The failing test seems to be due to OOM? Not sure if it's related to the changes, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalcuda12.4py3.10gcc9sm86 / test (default, 4, 5, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge i ""foreach failure is unrelated"""," merge f ""foreach failure is unrelated""", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `128413addfastpathconditionforwardhooks` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout 128413addfastpathconditionforwardhooks && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,"Not sure why previous lintrunner run missed it, but I applied the patch it suggested. `lintrunner m origin/main` green now locally", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Forward hooks not called when fast path is used in TransformerEncoderLayer," üêõ Describe the bug When `TransformerEncoderLayer` is run in evaluation mode and a few conditions are met, the fast path is used (which is a fused optimized implementation) instead of calling the modules like  `MultiheadAttention` (e.g. `self.self_attn`). This means any forward hooks or prehooks registered for the submodules are not called.   In this example, we would expect the `cache` to contain the output, but it does not. However, if we modify any condition for fast path selection, e.g. use an odd number of attention heads `nhead=3`, fast path is not used and the hook is called as expected.  Versions PyTorch version: 2.2.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.5 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: version 3.23.1 Libc version: N/A Python version: 3.12.0 (main, Oct  5 2023, 15:44:07) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS14.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Versions of relevant libraries: [pip3] mypy==1.8.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.26.4 [pip3] pytorchlightning==2.2.0.post0 [pip3] torch==2.2.0 [pip3] torchmetrics==1.3.1 [pip3] torchviz==0.0.2 [conda] numpy                     1.22.3           py39h64940a9_2    condaforge [conda] pytorch                   1.11.0          cpu_py39h03f923b_1    condaforge [conda] tor",2024-06-11T12:45:09Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/128413,My proposed solution would be to fall back from using fast path if there are pre/forward hooks on any submodules of the layer. I have started working on it: CC(Disable fast path in `TransformerEncoderLayer` when there are forward (pre)hooks attached to modules) 
transformer,[export] Failed to trace HF Llama2 model," üêõ Describe the bug The following Llama2 program used to work, but failed recently.  Error logs   Minified repro  $ python ep_llama.py strict pre Error log $ python ep_llama.py nostrict pre Error log  Versions pytorch nightly as of issue open time. ",2024-06-11T06:19:22Z,high priority triage review module: regression oncall: pt2 oncall: export,closed,0,13,https://github.com/pytorch/pytorch/issues/128394, seems like a predispatch issue? ,"To unblock, you can try torch.export._trace._export(pre_dispatch=False). ","Thanks  .  Tried pre_dispatch=False, but seems to hit the same error. (You can use the repro above, but set flag `nopre`). For example:  Cc:  ",Hit the same issue (627 nightlies) and talked with  about it. This is becoming more urgent.  This issue has been open for 3 weeks...would anyone be able to address this soon?  ,"updating  the same error blocks tracing of Llama38B, so it's continuing to block on more models.  tested with 2.5.0.dev20240630+cu121 ","Some investigations..  Loading and running with the `with torch.device(‚Äòmeta‚Äô)` does not work for me, it always runs into `RuntimeError: unsupported scalarType error`. Running the loaded model eagerly runs into the same issue `(llama(inputs[‚Äúinput_ids‚Äù]))`, even if I tried not loading the inputs on meta device. So removing that context, for the following code:  The following combinations of flags worked for me: `(pre_dispatch=False, strict=False), (pre_dispatch=False, strict=True)` On a small test case, I get the following graph:  `(pre_dispatch=True, strict=True)` results in the SpecViolationError, and produces following graph:  `(pre_dispatch=True, strict=False)` passes, resulting in the following graph, but this seems to be an incorrect behavior:  Chatted with  and we feel that the best solution is to turn the autocast context manager into a HOP, like what we do for `no_grad`.  ","> Chatted with  and we feel that the best solution is to turn the autocast context manager into a HOP, like what we do for no_grad. ?  Tugsuu is on PTO, so I'll unassign him. We should find a new owner for this","> What exactly is the problem here? If there's an autocast context manager, the predispatch (strict) graph looks something like:  But the operator `torch.amp.autocast_mode._enter_autocast` is not a valid ATen op. And we were thinking that the way we handle the autocast context manager should be the same way which we handle the no_grad HOP. Separately, the nonstrict predispatch graph seems to be incorrect, since it's missing the `_to_copy(dtype=torch.bfloat16)`, but maybe the context manager above will fix this.","For note keeping, the regression is due to https://github.com/huggingface/transformers/pull/29285/ which added the autocast context in transformers/modeling_llama.py.",Do we have a problem with the predispatch=False case? I agree in the predispatch=True case we can use a HOP to represent the context manager,"No, predispatch=False seems fine to me","Thanks  for suggesting fake tensor mode in  CC(Meta device not supported with autocast). The following script seems to work for me for export the current Llama2 model (with `autocast`).  If you would like to use `cuda` as the device while exporting the model, you can switch ""cpu"" above with ""cuda"".","With the changes in CC([export] Convert autocast to HOO), the code below can be exported (we convert autocast to HOO as discussed above). `with torch.device(""meta"")` still doesn't work (see CC(Meta device not supported with autocast)).   "
transformer,[export] Llama2 export with dynamic shapes fails using Torch 2.4 nightly," üêõ Describe the bug I'm trying to export Llama27B model with dynamic shapes and encountered the following error.  This passes with torch 2.3   so it is a regression.  In the python session :  1) Please login via huggingfacecli (install it via `pip install U ""huggingface_hub[cli]""` )      https://huggingface.co/docs/huggingface_hub/en/guides/clihuggingfaceclilogin. The user access token can be accessed in your settings. Code:   Error :   Please find the full log file here :  llama2.log cc:     Versions Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240610+cu121 [pip3] torchtensorrt==2.4.0.dev0+c6f8cb464 [pip3] torchvision==0.19.0.dev20240610+cu121 [pip3] triton==2.3.1 [conda] Could not collec ",2024-06-11T02:56:05Z,oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/128385,"I wasn't able to repro this, but you could try doing this:  The `_allow_complex_guards_as_runtime_asserts` flag should also allow you to export w/o needing `torch.nn.attention.sdpa_kernel([SDPBackend.MATH])` ","> I wasn't able to repro this Is this using the reproducer provided or using the code snippet you suggested ?  Btw, the code snippet you provided seems to work fine but is it recommended to use `torch.export._trace._export` as a part of official examples ? "
transformer,[dtensor][debug] add module level tracing and readable display,"  CC([dtensor][test] test case suite for comm_mode features)  CC([dtensor][example] added MLPStacked example for printing sharding)  CC([dtensor][be] improving readability of comm_mode.py and comm_mode_features_example.py)  CC([dtensor][debug] add module level tracing and readable display) **Summary** Currently, CommDebugMode only allows displaying collective tracing at a model level whereas a user may require a more detailed breakdown. In order to make this possible, I have changed the ModuleParamaterShardingTracker by adding a string variable to track the current submodule as well as a dictionary keeping track of the depths of the submodules in the model tree. CommModeDebug class was changed by adding a new dictionary keeping track of the module collective counts as well as a function that displays the counts in a way that is easy for the user to read. Two examples using MLPModule and Transformer have been added to showcase the new changes. The expected output of the simpler MLPModule example is:  **Test Plan** torchrun standalone nnodes=1 nprocpernode=4 torch/distributed/_tensor/examples/display_sharding_example.py ",2024-06-10T23:51:32Z,oncall: distributed Merged topic: not user facing ciflow/inductor,closed,1,0,https://github.com/pytorch/pytorch/issues/128369
transformer,SAM breaks with torch.compile," üêõ Describe the bug transformers SAM model inference fails when model is `torch.compile`d, with  I will provide reproduction code. Model normally works when not compiled.  Error logs   Minified repro   Versions This can be reproduced with Colab's env as of now:  ",2024-06-10T12:05:41Z,oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/128326, what is `image` supposed to be here ? what are the dimensions of images supposed to be ?," sorry I didn't post it, here:  ",This no longer repros on master.
transformer,nn.Transformer gives different output in torch.no_grad() context," üêõ Describe the bug The output that `nn.Transformer` gives in `eval` mode with the same inputs, is different when the forward pass is performed in a `torch.no_grad()` context:  The output I get, is: `[...]/python3.12/sitepackages/torch/nn/modules/transformer.py:384: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)   output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)`, and `tensor(False)`. The issue seems very similar to this one, created in 2022)), and likely has something to do with the fact that the model converts the encoder input into a nested tensor for some reason. When looking at the absolute difference between outputs `print(torch.abs(output1  output2).mean())`, the difference seems failry minimal; in the order of 1e7. Before, when I forgot to set a memory mask, the difference was much larger and substantially effected performance of my model during inference. Kind regards, Vincent  Versions Collecting environment information... PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Fedora Linux 39 (Workstation Edition) (x86_64) GCC version: (GCC) 13.2.1 20240316 (Red Hat 13.2.17) Clang version: Could not collect CMake version: version 3.27.7 Libc version: glibc2.38 Python version: 3.12.2 (main, Feb 21 2024, 00:00:00) [GCC 13.2.1 20231205 (Red Hat 13.2.16)] (64bit runtime) Python platform: Linux6.7.10200.fc39.x86_64x86_64withglibc2.38 Is CUDA available: True CUDA runtime version: Could not col",2024-06-07T10:46:25Z,module: nn,closed,0,3,https://github.com/pytorch/pytorch/issues/128209,"PS: just updated to 2.3.1, and it still occurs.","model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode. torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won‚Äôt be able to backprop (which you don‚Äôt want in an eval script).","from  , the delta from these two runs locally is is ~1e6, which is expected for float32. See https://pytorch.org/docs/stable/notes/numerical_accuracy.html for details."
transformer,Dynamo Graph break in Unsupported: call_method ConstDictVariable()," üêõ Describe the bug Model https://huggingface.co/sentencetransformers/allMiniLML6v2 The sample causes a graph break:   Repro:   Versions Collecting environment information... PyTorch version: 2.4.0a0+git49ad903 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: CentOS Stream 9 (x86_64) GCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.13) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.34 Python version: 3.11.9  (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.19.00_fbk12_zion_rc2_11583_g0bef9520ca2bx86_64withglibc2.34 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 GPU 1: NVIDIA H100 GPU 2: NVIDIA H100 GPU 3: NVIDIA H100 GPU 4: NVIDIA H100 GPU 5: NVIDIA H100 GPU 6: NVIDIA H100 GPU 7: NVIDIA H100 Nvidia driver version: 525.105.17 cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.9.2 /usr/lib64/libcudnn_adv_infer.so.8.9.2 /usr/lib64/libcudnn_adv_train.so.8.9.2 /usr/lib64/libcudnn_cnn_infer.so.8.9.2 /usr/lib64/libcudnn_cnn_train.so.8.9.2 /usr/lib64/libcudnn_ops_infer.so.8.9.2 /usr/lib64/libcudnn_ops_train.so.8.9.2 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_cnn_train.so.8 /usr/local/cuda12.1/targets/x86_64linux/lib/libcudnn_ops_infer.so.8 /usr/local/cuda12",2024-06-05T21:45:44Z,triaged oncall: pt2 module: dynamo empathy-day dynamo-dicts dynamo-triage-june2024,closed,0,0,https://github.com/pytorch/pytorch/issues/128067
transformer,Add SinusoidalPositionalEmbedding module for use in Transformers and Diffusion models," üöÄ The feature, motivation and pitch Current state of the art algorithms in generative AI (namely transformers and diffusion algorithms) make use of positional embeddings. In pytorch, this functionality is not yet implemented as a separate module, which would be useful for developers trying to create stand alone versions of these algorithms or simply trying to use the concept of positional embeddings in general.   Alternatives There was some discussion about adding a positional embedding layer directly to the transformer model here). I believe that creating a separate module for the positional embedding would allow for its use in other models as well (such as diffusion models) ",2024-06-04T18:19:25Z,module: nn triaged needs research,open,0,0,https://github.com/pytorch/pytorch/issues/127932
transformer,use `OffloadPolicy`,  CC(use `OffloadPolicy`)  CC([FSDP2] enable CI for torch.compile(root Transformer)) Summary: Test Plan: Reviewers: Subscribers: Tasks: Tags: ,2024-06-04T02:46:29Z,oncall: distributed topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/127849
transformer,[FSDP2] enable CI for torch.compile(root Transformer),"  CC([FSDP2] enable CI for torch.compile(root Transformer)) This CI showcases FSDP2 works with `torch.compile` root model, since FSDP1 can do the same  compiling root Transformer without AC: `pytest test/distributed/_composable/fsdp/test_fully_shard_training.py k test_train_parity_multi_group` compiling root Transformer with AC: `pytest test/distributed/_composable/fsdp/test_fully_shard_training.py k test_train_parity_with_activation_checkpointing` ",2024-06-04T00:47:16Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,5,https://github.com/pytorch/pytorch/issues/127832, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,module 'torch.mps' has no attribute 'device'," üêõ Describe the bug I'm getting the error `module 'torch.mps' has no attribute 'device'` trying to train a `sentencetransformer model (which uses the huggingface transformers trainer). The error is raised on this line > https://github.com/pytorch/pytorch/blob/7ef7c265d4361691dc4cf54152db083de3215fbf/torch/utils/checkpoint.pyL183 Because the mps model doesn't have a `device()` method https://github.com/pytorch/pytorch/blob/7ef7c265d4361691dc4cf54152db083de3215fbf/torch/mps/__init__.py  Versions Collecting environment information... PyTorch version: 2.3.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.5 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.3.9.4) CMake version: Could not collect Libc version: N/A Python version: 3.10.14 (main, May 25 2024, 13:44:20) [Clang 15.0.0 (clang1500.3.9.4)] (64bit runtime) Python platform: macOS14.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Pro Versions of relevant libraries: [pip3] numpy==1.26.4 [pip3] torch==2.3.0 [conda] Could not collect ",2024-06-01T04:45:18Z,triaged actionable module: mps,open,0,4,https://github.com/pytorch/pytorch/issues/127676,I am getting the same bug under Python 3.13 and OS 14.3.1. I got it under both torch 2.2.2 and torch 2.3.,Hey! I think there are two ways we can go about this one:  Update the mps device_module to add this function to set the current device.  Change the checkpoint code to only call this function if the current device is not the same as the device we want to set (so that device/accelerator that only ever have one device don't need to worry about it). ,"For what it's worth, I only encounter this error when using CachedGISTEmbeddingLoss or CachedMultipleNegativesRankingLoss. The noncached versions of those same loss functions do not create the error.",> Hey! >  > I think there are two ways we can go about this one: >  > * Update the mps device_module to add this function to set the current device. > * Change the checkpoint code to only call this function if the current device is not the same as the device we want to set (so that device/accelerator that only ever have one device don't need to worry about it). I like option 1. We would need this method later for Triton work.. Updating current PR with it. 
transformer,Illegal memory access resulted from pointwise autotuning of a cat-like kernel," üêõ Describe the bug I ran into this issue when applying torch.compile transformer blocks in torchtitain. When running llama_70b w/ local batch_size=4, both eager mode and torch.compile works. However, when running with batch_size=8, eager works but torch.compile fails with ""illegal memory access"". Originally I thought the issue could be OOM manifest as IMA, but the issue persisted when I lowered the memory usage by reducing the number of layers. I stepped through the generated code, the issue originated from the autotuning a splitcatlike kernel. It is reproducible when isolated (see minified repro).  Error logs   Minified repro  ",2024-05-31T22:41:50Z,triaged oncall: pt2 module: inductor,closed,1,4,https://github.com/pytorch/pytorch/issues/127652,is there an easy way of running the original code ? (maybe just post the fx_graph_runnable from TORCH_COMPILE_DEBUG), that's a great idea. Here it is: ,"computesanitizer shows the following:  ========= Invalid __global__ read of size 16 bytes                                        =========     at triton_+0x5d0 in /tmp/torchinductor_root/ja/cja7fko43klibgjaatfwng6o3zjzzszdenrb2pmxdluy5j7qrote.py:66                                                           =========     by thread (3,0,0) in block (327723,0,0)                                     =========     Address 0x7efda5015830 is out of bounds                                     =========     and is 5553170384 bytes before the nearest allocation at 0x7efef0000000 of  size 939524096 bytes                                                 and the line 66 is:  `66     tmp40 = tl.load(in_ptr0 + ((2642411520) + x0 + (67108864*x1)), tmp39, other=0.0)    .to(tl.float32)`",The issue is that we're using 32 bit indexing but `2642411520` requires 64 bits. I'll look into fix
transformer,[inductor][cpu]abnormal performance improvement and accuracy drop for huggingface suit static/dynamic quantization, üêõ Describe the bug abnormal performance improvement and accuracy drop for huggingface suit static/dynamic quantization in 20240526 nightly release accuracy drop:   model_name  1.62   f8c4c268da67e9684f3287b7468f36a5a27c6a0b textclassification+bertbasechinese !image textclassification+albertbasev1 !image ee6cb6daa173896f8ea1876266a19775aaa4f610  textclassification+bertbasechinese !image textclassification+albertbasev1 !image  Versions SW info               SW       Branch       Target commit       Refer commit                       Pytorch       nightly       549167f       bca6d8b                 Torchbench       chuanqiw/inductor_quant       ee35d764       ee35d764                 torchaudio       nightly       1980f8a       1980f8a                 torchtext       nightly       b0ebddc       b0ebddc                 torchvision       nightly       d23a6e1       d23a6e1                 torchdata       nightly       11bb5b8       11bb5b8                 dynamo_benchmarks       nightly       nightly       nightly          Repro:  Suspected guilty commit: https://github.com/pytorch/pytorch/commit/ee6cb6daa173896f8ea1876266a19775aaa4f610 textclassification+albertbasev1staticquantaccuracydrop_guilty_commit.log ,2024-05-29T15:06:11Z,high priority triaged oncall: pt2 oncall: cpu inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/127402
transformer,4 GPT2 can't run into `_scaled_dot_product_flash_attention_for_cpu` using AOTI due to export related change in https://github.com/pytorch/pytorch/pull/123732," üêõ Describe the bug  Description Before https://github.com/pytorch/pytorch/pull/123732, when running with AOTI, the SDPA pattern can be hit and `torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default` is on the `__post_grad_graphs`. However, after this PR, the below models can't run into SDPA any more when running with AOTI. This has brought performance gap between AOTI (can't hit SDPA anymore) and Inductor (which can hit SDPA). I tried on a recent main branch (669560d51aa1e81ebd09e2aa8288d0d314407d82) and the issue is still there.  Impacted models 2 models in the HF suite (DistillGPT2, GPT2ForSequenceClassification) and 2 models in the torchbench suite (hf_GPT2, hf_GPT2_large) of the dynamo benchmark are impacted.  Steps to reproduce (I didn't turn on freezing since at that moment, freezing can't work due to other issue (fixed by https://github.com/pytorch/pytorch/pull/124350 on recent main). But the graph with or without freezing both have this issue)  In the output log Using commit https://github.com/pytorch/pytorch/commit/02ed2992d94e6bb09d95fb1409883fc61cf19e13, there's `torch.ops.aten.bmm.default` but not `_scaled_dot_product_flash_attention_for_cpu` in the `__post_grad_graphs`. Using commit https://github.com/pytorch/pytorch/commit/4f29103749c5011529f1abb10b1508a682588909 (one commit before the above):  Here're some findings: Using GPT2ForSequenceClassification as an example, I printed the graph before and after https://github.com/pytorch/pytorch/pull/123732 (the graph is dumped before this line of code: link): Before the PR where SDPA can be hit:  After the PR where SDPA can't be hit: There's on extra line **before** ",2024-05-29T07:07:59Z,triaged module: regression oncall: pt2 module: aotinductor module: sdpa,closed,0,5,https://github.com/pytorch/pytorch/issues/127383,, ,Can we solve by adding extra matching patterns?,"w , is this still a problem?",It's not an issue anymore using the latest main. I checked that we have `aoti_torch_cpu__scaled_dot_product_flash_attention_for_cpu` in the output code for the 4 models reported in this issue.
transformer,[Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP,Test command: `pytest rA test/inductor/test_compiled_autograd.py::TestCompiledAutograd::test_trace_run_with_rng_state`   CC([Traceable FSDP2] Add inductor E2E unit test for FSDP2+SAC for transformer model)  CC([Traceable FSDP2] Add Dynamo support for run_with_rng_state HOP)  CC([Traceable FSDP2] Add autofunctionalize support for mutable list[Tensor] (copy from Brian's PR 127347); enable E2E inductor unit test for transformer model) ,2024-05-28T01:59:21Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor keep-going,closed,0,10,https://github.com/pytorch/pytorch/issues/127247,"How did you get into this situation? I thought the run_with_rng_state HOP is used for the torch.utils.checkpoint x torch.compile design, and should not show up in Dynamo itself?  ","> How did you get into this situation? I thought the run_with_rng_state HOP is used for the torch.utils.checkpoint x torch.compile design, and should not show up in Dynamo itself?   It only appears during Compiled Autograd Dynamo tracing of the AOT bwd graph (which contains the run_with_rng_state HOP)"," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""unrelated failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,"When training done, the mode output same result each tensor input. ( I tried many way to debug, but can't find any way to fix it, so i guess this is a bug )"," üêõ Describe the bug  those code is simple way to show your guys. one, i make sure i am using same dataset to infer. two, my model is LSTM + Transformer encoder, so i made a method to reset cell state in each train and eval. three, i tried make eval to train + no grad to test output, but still 0 chanse is the most ( by the way, the mode is made to output one of 0, 1, 2, 3 ). when i training, with backward and optimizer, the model act normal, out put 1, 2, 0... something like that. but when i not useing grad to output, the model only output me 0, this is really big problems to me. thanks for help üò≠  Versions pytorch 2.3.0 Python 3.12.2 cuda 12.1.0",2024-05-25T16:54:17Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/127177,"It's not overfitting, because is same data for infer. It's not different weight, i checked ( save and load ). it's not eval() problem, i already check with train() + no grad.","It is unclear that this is a bug in PyTorch as there are many things that can go wrong to cause a model to not train properly for these types of questions, please check out https://discuss.pytorch.org/.","It's Label leakage by optimizer. my label like 0,0,0,0... 1,1,1,1... 2,2,2,2, so when traning, optimizer tell model 2 is right, so it will contiune output 2, because label is 2,2,2,2,2..... i'm wrong, this not a bug. without optimizer, nobody tell model what label right right now, and 0 is the answer."
transformer,"RuntimeError: ""_amp_foreach_non_finite_check_and_unscale_cuda"" not implemented for 'BFloat16'"," üêõ Describe the bug i use pytorch==2.3.0 and peft to train llama3 8b , when i run my code, its raise error like:  full of my training code like:  and the all output in terminal like: ```text C:\ProgramData\miniconda3\envs\llama\python.exe D:/codes/llm_about/selfllm/zzzzz_train/llama38B/finetune_llama3_8b.py Special tokens have been added in the vocabulary, make sure the associated word embeddings are finetuned or trained. tokenizer about:   128001 128001 Map: 100% (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22631SP0 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 555.85 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin\cudnn_ops64_9.dll HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==2.3.0 [pip3] torchaudio==2.3.0 [pip3] torchvision==0.18.0 [conda] blas                      1.0                         mkl    defaults [conda] mkl                       2021.4.0                 pypi_0    pypi [conda] mklservice               2.4.0           py310h2bbff1b_0    defaults [conda] mkl_fft                   1.3.1           py310ha0764ea_0    defaults [conda] mkl_random                1.2.2           py310h4ed8f06_0    defaults [conda] numpy                     1.24.3          py310hdc03b94_0    defaults [conda] numpybase                1.24.3          py310h3caf3d7_0    defaults [conda] pytorch                   2.3.0          ",2024-05-25T16:33:16Z,triaged module: amp (automated mixed precision) module: mta,open,0,3,https://github.com/pytorch/pytorch/issues/127176,this is my cuda cudnn infos: ,some of my dataset like: ,"For BFloat16 AMP, the grad scaler wouldn't be needed as its range is equivalent to fp32's. Could you try the script without grad scaler?"
transformer,Significant peformance regression in DDP for torch > 2.1," üêõ Describe the bug DDP is significantly slower on torch versions after 2.1.   Requirements  Machine with multiple (4+) GPUs. I am using AWS g5.12xlarge which has 4 x A10G GPUs.  Steps to reproduce  Create two environments.   Create a file called `testddp.py` with the following code (adapted from torch ddp tutorial).   Run using `torchrun`:   Observations  Environment 1 (torch 2.1.x) shows: ~3hr training time.  Environment 2 (torch 2.3.x) shows: ~5hr training time. I had originally observed this issue in `transformers` (see here) but this looks like a torch DDP issue, not really something on `transformers` side. Note: This is also an issue in `torch==2.2.x` but here I am just showing on `torch==2.3.0`.  Expected Result Training time should be the same or at least not significantly worse than <=2.1 on newer torch versions. In my actual codebase, I am seeing up to 2x worse training time on newer torch versions.  Versions  Environment 1 (`torch==2.1.2`)  ",2024-05-24T11:18:09Z,oncall: distributed triaged,closed,11,22,https://github.com/pytorch/pytorch/issues/127077,Something that could help us a lot since you already have envs setup would be to get profiler traces from the two envs to compare. Would you be able to do that and share the traces?," If you could tell me how to do that, I'll be happy to share the traces. ","I think something like the following may work:  This should just take the profiler trace on rank 0 and save it to some `.json` file. If you could share the `.json` file, then that would be great. (We would then view them in something like chrome://tracing/ to compare the difference between the envs.)","Here are the files: torch2.1.json torch2.3.json For completeness, here is the script I ran using `torchrun nprocpernode=4 profileddp.py`:  profileddp.py.txt","Thanks! I wonder if there is a regression in NCCL across the two versions.  torch2.1: allreduces take 1.309 ms and 7.944 ms  torch2.3: allreduces take 1.952 ms and 13.987 ms  Allreduce message sizes are (1054725 * 4) bytes and (7347200 * 4) bytes, respectively For torch2.1, this equates to roughly  2 * (1054725 * 4 bytes) / (0.001309 seconds) / 1e9 = 6.446 GB/s bandwidth  2 * (7347200 * 4 bytes) / (0.007944 seconds) / 1e9 = 7.399 GB/s bandwidth where the factor of 2 is from allreduce requiring two passes around the ring. More precisely, there could be an `N1/N` factor for `N=4` GPUs, but it does not matter that much. The point is that, the achieved bandwidth on these collectives is pretty low. I am not as familiar with this part, but I think that it is possible to use https://github.com/NVIDIA/nccltests to test what kind of allreduce bandwidth you should expect to see for your hardware setup. In short, the current evidence points toward there being a NCCL regression in the allreduce times for your model/setup, and your training is communication bound. This translates to slowdown in endtoend training time. cc:   if you guys have any suggestions on how to further diagnose this"," Since this might be a NCCL regression, I wonder if we can try to do the following: 1. Get the NCCL versions for both envs, e.g. via `python c ""import torch;print(torch.cuda.nccl.version())""` 2. Run a script that only does allreduce Example with profiler:  Example with CUDA events for timing without profiler:  The latter might be simpler since then we do not need to manually inspect the profiles."," torch 2.1 NCCL version: (2, 18, 1)   torch 2.2 NCCL version: (2, 19, 3)   torch 2.3 NCCL verison: (2, 20, 5)  Code: ","Regarding the micro benchmark, since you are calling `torch.cuda.Event()` and `torch.cuda.synchronize()`, the program must know which device to perform these operations on. Otherwise, these two operations would be performed on device 0 (the default one in CUDA's view). Obviously, the non0 ranks do not have CUDA kernels on device 0. So they record almost 0 time. You just need to add a line here:  Then you would see roughly equal time:  (I measure the time on a different hardware platform, so don't take the absolute number seriously.)","And also regarding NCCL perf of different versions, I did some micro benchmarks too using https://github.com/NVIDIA/nccltests.  There seems to be no difference across versions, with the sizes you provided.  See https://gist.github.com/kwen2501/89c28b6d12045b45ccf7e33816af2713. But I was testing them on 4 x A100 with NVLinks, instead of 4 x A10g, which does not rule out the chance of a platformspecific issue. That said, another possibility may be that the regression comes from torch side.","Sure, here's the result:  torch 2.1   torch 2.3 ",Thanks! Are you familiar with running nccltests? Can you run it with NCCL 2.18.5 and 2.20.5 on your platform? Thanks!,Let me type the commands here:  You can do the above separately with `v2.18.51` and `v2.20.51` for the  `` field.,"Sorry, I may have messed up my environments in the last couple of tests. Need to take a break now. Will post an update tomorrow. ","I ran some more tests.   On AWS p4d.24xlarge (8 x A100)  `torch==2.3.0 transformers==4.30.2 accelerate==0.20.3`  `testddp.py` script in the first comment: ~50min.  Minimal script in this issue: ~41hrs.  My own training script: ~8.5hrs.  `torch==2.3.0 transformers==4.40.2 accelerate==0.30.1`  `testddp.py` script in the first comment: ~50min  Minimal script in this issue: ~41hrs.  My own training script: ~13hrs.  `torch==2.0.1 transformers==4.30.2 accelerate==0.20.3`  `testddp.py` script in the first comment: ~50min  Minimal script in this issue: ~41hrs.  My own training script: ~8.5hrs.  `torch==2.1.2 transformers==4.30.2 accelerate==0.20.3`  `testddp.py` script in the first comment: ~50min  Minimal script in this issue: ~41hrs.  My own training script: ~8.5hrs. Something is off with the second env but I am not really sure if this is a torch or transformers issue.   On AWS g5.48xlarge (8 x A10G) This is a different machine than the previous one with 4 x A10Gs. Here, something is clearly going wrong with torch.  `torch==2.3.0`  `testddp.py` script in the first comment: ~16h  allreduce test:   `torch==2.0.1`  `testddp.py` script in the first comment: ~7h  allreduce test: ",Thanks for the updates.  I think it would be helpful to focus on the clearer case for now  the A10g platform. Do you mind running the nccltests on this machine? That would help isolate torch vs NCCL. Thanks.,I am trying that and running into some errors at step `make j NCCL_HOME=../nccl/build` ,"Nevermind, I fixed the issue by symlinking to `/usr/lib/libnccl.so`.  v2.18.51   v2.20.51 ",This machine actually has 8 A10Gs:  v2.18.51   v2.20.51  Weird that bandwidth is similar when testing with 4 GPUs but worse in 2.20.5 with 8 GPUs. ü§î ,"Interesting.  If you'd like further support, you can open an issue in NCCL and link to the above A10g results.  It would be also helpful for them to know the topology of your machine:  Cc NCCL experts:   ","Indeed, I'm not sure why that is. Can you open an issue on the NCCL project (https://github.com/nvidia/nccl)? Please attach the log of a run with 2.18 and 2.20, running with the following environment variables set: `NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH` Thanks!",Thanks! Opened an issue: https://github.com/NVIDIA/nccl/issues/1298,"Given the discussion in https://github.com/NVIDIA/nccl/issues/1298, I think we can close the PyTorch issue, as the root cause is more related to NCCL and the specific user hardware setup. Feel free to reopen if there are any followups."
transformer,Torchscript export of `Salesforce/blip2-opt-2.7b` model fails with Conv shape error, üêõ Describe the bug Export of `Salesforce/blip2opt2.7b` with the torchscript exporter fails with conv shape error:  Here's the export script I used:  Note that the same model can export successfully with dynamo exporter (by setting `dynamo_export = True` in the script.  Versions I'm using the NGC pytorch container `nvcr.io/nvidia/pytorch:24.04py3` ,2024-05-22T03:16:45Z,oncall: jit,open,0,1,https://github.com/pytorch/pytorch/issues/126843,cc: nvidia
transformer,Torchscript export of some HF models fails with indexing errors on `past_key_values`," üêõ Describe the bug Running the following export script for the two falcon models `tiiuae/falconrw1b` and `tiiuae/falcon7b` runs into indexing errors related to `past_key_values`:  The error is:  The same model can be exported with the dynamo exporter, by setting `dynamo_export = True` in the script.  Versions I'm using the NGC pytorch  container `nvcr.io/nvidia/pytorch:24.04py3` ",2024-05-22T03:07:04Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/126840,"Similar errors on `past_key_values` are also occurring for `EleutherAI/gptj6b`, `Qwen/Qwen7BChat`, `distilbert/distilgpt2`, `bigscience/bloom560m`",cc: nvidia,Closing this bug as this error can be resolved by passing in the input data differently: 
transformer,FakeTensor `forward()` on `bert` attention when using FakeTensor returns a DataDependentOutputException," üêõ Describe the bug I attempted to run the How to measure memory usage from your model without running it? code, simply replacing the model with a very basic one from Hugging Face (`bertbaseuncased`), and I was left with a `torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default`.  Some advice on how to move past this would be great, as ideally I'd like to merge this functionality in with the HF model memory estimator tool once it's stable/agnostic enough if possible! Full example:     Versions Collecting environment information... PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.0 Libc version: glibc2.35 Python version: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.5.119 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU Nvidia driver version: 535.171.04 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             16 Online CPU(s) list:                015 Vendor ID:                          Genui",2024-05-21T00:55:23Z,triaged tensor subclass module: fakeTensor module: dynamic shapes,open,0,1,https://github.com/pytorch/pytorch/issues/126738,You could try enabling this by passing in a shape env to the fake tensor constructor but I'm... not that optimistic lol
transformer,[AOTI] Using `AOTI_TORCH_CHECK` will cause performance drop on several models compared with using `TORCH_CHECK`," üêõ Describe the bug https://github.com/pytorch/pytorch/pull/119220 replaced `TORCH_CHECK` with `AOTI_TORCH_CHECK`. We found that this change caused performance drop on several models when we were working on https://github.com/pytorch/pytorch/pull/124350. The support of freezing https://github.com/pytorch/pytorch/pull/124350 is not landed yet so the below reproducer doesn't turn on the freezing flag. Steps to reproduce: ```sh  cd path_to_pytorch  install transformers pip install transformers==4.38.1  install intelopenmp and jemalloc for performance benchmark pip install intelopenmp conda install c condaforge jemalloc export LD_PRELOAD=${CONDA_PREFIX:""$(dirname $(which conda))/../""}/lib/libiomp5.so:${CONDA_PREFIX:""$(dirname $(which conda))/../""}/lib/libjemalloc.so export MALLOC_CONF=""oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:1,muzzy_decay_ms:1"" export KMP_AFFINITY=granularity=fine,compact,1,0 export KMP_BLOCKTIME=1 CORES=$(lscpu  **Update (2024531)**: hf_T5_base and hf_T5_large in the torchbench suite also meet this issue.  Versions PyTorch version: 2.4.0a0+git314ba13 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.03) Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.28 Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.16.0x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN ",2024-05-20T04:54:19Z,triaged oncall: pt2 module: aotinductor,closed,0,4,https://github.com/pytorch/pytorch/issues/126665,Something that might help is to move the boolean condition back inline, ,Will take a look. Thanks.,"w , this should have been fixed by https://github.com/pytorch/pytorch/pull/128402. Feel free to reopen if it did not."
transformer,[pipelining] Add back support for multi-use parameters/buffers," üöÄ The feature, motivation and pitch When running tracer mode with torchtitan, the follow `NotImplementedError` was raised:  The source code that causes the multiuse is in `Transformer`'s forward function:  The support was temporarily dropped when we refactor the tracer (_IR.py) to use unflattener. We should add it back.  Alternatives _No response_  Additional context _No response_ ",2024-05-18T22:19:03Z,oncall: distributed triaged module: pipelining,closed,1,0,https://github.com/pytorch/pytorch/issues/126626
transformer,Flatten out_proj in MultiHeadAttention,"The MHA has an explicit `nn.Linear` layer for output projections, which is not consistent with the rest of the implementation (s.a. input projections). In addition to that this makes the `nn.MultiHeadAttention` dependent on the linear implementation, as well as making it a nested module.  Changes: 1. Remove `MultiHeadAttention.out_proj` 2. Add `MultiHeadAttention.out_proj_weight`, `MultiHeadAttention.out_proj_bias`. Add the functional linear for forward 3. Add initialization 4. Change expected string to hide the `out_proj` 5. Adds forward compatibility to be able to load old models  Potential issues: * Initialization: `nn.Linear` initilizes its weight as uniform Kaiming, while this PR uses uniform Xavier. In addition to that, bias in the `nn.Linear` is uniform based on fanin/fanout, while here it is constant 0. This means that numerically this will be different from the original implementation.     * *Option 1: Accept current change*  this is more consistent with the rest of the implementation     * *Option 2: Duplicate initialization logic from Linear*  this is consistent with the initialization from before this PR  Tests There are no new tests, as no new logic or change in functionality is introduced. Potentially fixes CC(torch.load non backwards compatible on Transformer between 1.8.1 and 1.9.0)  ",2024-05-17T19:48:08Z,oncall: distributed triaged open source Stale release notes: distributed (fsdp),closed,0,4,https://github.com/pytorch/pytorch/issues/126568,"'t think there is a need for the `NonDynamicallyQuantizableLinear` anymore. However, it is worth checking for quantization logic changes.","This is BCbreaking, no?"," It might be for chckpoints: in case a checkpoint is created postPR, and being loaded into prePR. I can add BCcompatibility by saving the weights as `out_proj.weight` and `out_proj.bias` with a deprecation warning, but that would mean the checkpoint statedict will have structure inconsistent with the MHA, wdyt?","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Remove activation checkpointing tag to get correct FQNs (#124698),"Cherrypick for release branch Fixes CC(Not loading optimizer state separately from checkpoint causes errors with FQNs) When setting `use_orig_params = False` and using activation checkpointing, the FQN mapping as retrieved by the `_get_fqns` function is incorrect because the prefix that is added to the name of each activation checkpointed module, `_checkpoint_wrapped_module`, can still be present. I think this is an edge case with the `_get_fqns` function that was not addressed by this previous commit CC([DCP] Removes Checkpoint Wrapped Prefix from state dict fqns). Without the change, the list of object names for an activation checkpointed module with FSDP (and `use_orig_params=False`) can be something like:  Which will incorrectly return just one FQN, `{'model.transformer.blocks.0._flat_param'}`, when all the FQNs of the parameters of the transformer block should be returned. With the change, the list of object names will now have `_checkpoint_wrapped_module` removed:  And the FQNs are correctly retrieved and returned in `_get_fqns` when this condition is satisfied. The correct FQNs are:  Pull Request resolved: https://github.com/pytorch/pytorch/pull/124698 Approved by: https://github.com/Skylion007 Fixes ISSUE_NUMBER ",2024-05-17T18:31:35Z,oncall: distributed open source module: distributed_checkpoint,closed,0,0,https://github.com/pytorch/pytorch/issues/126559
transformer,[FSDP2] Fixed 2D clip grad norm test,"  CC([FSDP2] Fixed 2D clip grad norm test) This fixes  CC([FSDP2][2D] test_clip_grad_norm_2d is failing on main). We change from transformer to MLP stack since transformer seems to introduce slight numeric differences when using TP. We include a sequence parallel layer norm module in the MLP stack to exercise `(S(0), R)` placement. ",2024-05-17T01:13:16Z,oncall: distributed Merged Reverted ciflow/trunk ciflow/inductor release notes: distributed (fsdp2),closed,0,18,https://github.com/pytorch/pytorch/issues/126497,switching to fsdp2 release note is really some labor work,"Failure are all inductorrelated, not FSDP2related.", merge i," Merge started Your change will be merged while ignoring the following 9 checks: inductor / cuda12.1py3.10gcc9sm86 / test (inductor, 1, 1, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_distributed, 1, 1, linux.g5.12xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (dynamic_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (aot_inductor_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu, unstable), inductor / rocm6.1py3.8inductor / test (inductor, 1, 1, linux.rocm.gpu.2), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu), inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamic_aot_eager_torchbench, 1, 2, linux.g5.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m ""reverting to check if might have introduced inductor cuda 12 issues"" c ""ignoredsignal""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase s, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/awgu/588/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/126497`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8clang10 / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , rebase s, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC([FSDP2] Fixed 2D clip grad norm test), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[release/2.3] Added cublasGemmAlgo_t -> hipblasGemmAlgo_t ,This PR is to add cublasGemmAlgo_t > hipblasGemmAlgo_t to cuda_to_hip_mappings.py. It is required for DeepSpeed transformer extension build on ROCm.,2024-05-16T19:54:32Z,open source,closed,0,0,https://github.com/pytorch/pytorch/issues/126448
transformer,[VIT Model] [perf Degradation] [X86] [ARM] torch.compile + weight prepacking results in perf degradation for VIT Transformer model," üêõ Describe the bug With Pytorch 2.3.0, when we run inferencing for VIT DL Model on CPU's both on x86 (ICELake) and ARM (Graviton3) we see a performance degradation with  torch.compile()+weight prepacking over just torch.compile(). !image Note: Here, total time taken is in seconds for 100 iterations. Only Inference time is considered here. We can clearly observe that the performance is getting worsen after applying weight prepacking optimization which shouldn't be the case. I further did deep dive analysis and figured out the root cause for this issue in the VIT model. !image From the snapshot above, we can see that, aten::addmm op is called with torch.compile() and mkldnn::_linear_pointwise (ARM)  / mkl::_mkl_linear (x86) is called with torch.compile+weight_prepacking. We see that aten::addmm is called around 4 times in VIT model execution and each of it has a different shape. For every shape there's performance improvement with weight prepacking except 1 shape which is highlighted in green/orange. **[[3072], [197, 768], [768, 3072], [], [], [197, 3072]]** This is the reason for the perf degradation. This shape scenario has to be analyzed and fixed to improve the performance on CPU (Both X86 and ARM). **Scripts to reproduce:** vit.txt vit_with_weight_prepacking.txt **How to install:** > pip3 install torch==2.3.0 > export OMP_NUM_THREADS=32  (As this was tested on 32 core machines. m7g.8xlargeGraviton3 for ARM and c6i.8xlargeIceLake for Intel.  Error logs **Model perf degradation with torch.compile()+weightprepacking both on x86 and ARM CPU's:** !image  Minified repro _No response_  Versions **For ARM:** collect_env_arm.txt requirements.",2024-05-16T11:33:46Z,oncall: pt2 module: inductor oncall: cpu inductor,closed,0,11,https://github.com/pytorch/pytorch/issues/126391, Could you help to take a look?,"Tried on SPR with 56 threads. According to the MKL verbose, the mkl_linear kernel (the highlighted shape in the issue) time has a regression starting from a certain moment:  By writing a small test case and running torch.addmm 2000 times, we could only see the kernel perf around 250us.","257.85us > 737.93us Will the regression be further investigated and fixed? The issue is really when weight prepacking is enabled with torch.compile() as highlighted in orange in the ticket. Even the shape becomes different with weight prepacking enabled compared to [[3072], [197, 768], [768, 3072], [], [], [197, 3072]] which is just torch.compile()","There are some environment problems for the previous data. With enabling tcmalloc and iomp5 (need to install intelopenmp), the performance with weight prepack is better than that without it. Tested on Xeon SPR with 56 threads. Without weight prepacking:  With weight prepacking:   Could you try with the environment parameters mentioned above? Maybe you'd better run with the PyTorch launcher https://github.com/pytorch/pytorch/blob/main/torch/backends/xeon/run_cpu.py.","I installed intelopenmp==2024.1.2 in my python environment and retried running VIT model in compile mode (with and without prepacking) on Intel ICL with 32 cores and 32 threads. I don't see much difference in the results. I do see intelopenmp brings some slight gain with few milli seconds but I still see the above anomaly. From your data above, looks like intelopenmp helps in SPR instance case with higher number of cores and threads but it doesn't seem to be applicable in the older generation of data centric CPU's on x86 platform.",I understand one of the recommendations is probably to use intelopenmp in x86 case. Can someone point how can this issue be resolved in ARM (aarch64) CPU's? (Can someone working on ARM look into this?), Have you installed tcmalloc or jemalloc?,"Thanks  for pointing it out. I have remeasured the numbers by enabling tcmalloc as well and now I see improved numbers with weight prepacking on x86 CPU. **Without weight prepacking:**                                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                                                                   Input Shapes                                             aten::addmm        24.29%     588.338ms        27.71%     671.077ms     139.808us          4800                               [[768], [197, 768], [768, 768], [], [], [197, 768]]                                             aten::addmm        23.82%     576.808ms        25.11%     608.080ms     506.733us          1200                             [[768], [197, 3072], [3072, 768], [], [], [197, 768]]                                             aten::addmm        20.36%     492.986ms        21.93%     531.105ms     442.587us          1200                            [[3072], [197, 768], [768, 3072], [], [], [197, 3072]]   **With Weight PrePacking:**                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                                                                      Input Shapes                                           mkl::_mkl_linear        26.58%     534.245ms        26.68%     536.379ms     111.746us          4800                               [[197, 768], [2900193, 1], [768, 768], [], []]                                        mkl::_mkl_linear        23.02%     462.819ms        23.07%     463.730ms     386.442us          1200                             [[197, 3072], [5259489, 1], [768, 3072], [], []]                                        mkl::_mkl_linear        21.55%     433.074ms        21.59%     433.935ms     361.613us          1200                              [[197, 768], [5259489, 1], [3072, 768], [], []]  ",Closed as the issue was caused by wrong environment config.,"The issue seems to even go away on ARM using tcmalloc (32 core, 32 thread scenario on Grv3 Instance using the latest torch 2.3.1) **Without weight prepacking:**                                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                                                                   Input Shapes                                             aten::addmm        22.56%        1.842s        26.27%        2.145s     446.891us          4800                               [[768], [197, 768], [768, 768], [], [], [197, 768]]                                             aten::addmm        14.29%        1.167s        15.79%        1.289s       1.074ms          1200                            [[3072], [197, 768], [768, 3072], [], [], [197, 3072]]                                             aten::addmm        14.07%        1.149s        15.68%        1.280s       1.067ms          1200                             [[768], [197, 3072], [3072, 768], [], [], [197, 768]]   **With weight prepacking:**                                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg                       of Calls                                                                   Input Shapes                                               mkldnn::_linear_pointwise        19.44%        1.165s        20.62%        1.237s     257.626us          4800                               [[1, 197, 768], [768, 768], [768], [], [], []]                                             mkldnn::_linear_pointwise        15.81%     947.898ms        16.19%     970.938ms     809.115us          1200                             [[1, 197, 768], [3072, 768], [3072], [], [], []]                                             mkldnn::_linear_pointwise        15.44%     925.528ms        15.86%     950.862ms     792.385us          1200                             [[1, 197, 3072], [768, 3072], [768], [], [], []]   I understand now that tcmalloc helps scale performance with highly parallel applications. I would like to know when is this recommended generally in DL space. why isn't this shared lib part of the OSS torch binaries.",> I understand now that tcmalloc helps scale performance with highly parallel applications. I would like to know when is this recommended generally in DL space. why isn't this shared lib part of the OSS torch binaries. These thirdparty caching allocators use aggressive memory pooling algorithms which would cause larger memory usages. That is why PyTorch is using the glibc allocator by default conservatively.
transformer,RAM Not Freed on CPU After Moving Model with Multiple Transformers to CUDA," Describe the bug I encountered a memory issue when moving a model to CUDA using `model.to('cuda')`. Specifically, when using `torch.nn.ModuleList` or `torch.nn.Sequential` containing multiple instances of `torch.nn.Transformer`, the model appears to move to CUDA but does not free up the corresponding CPU memory. This results in significant RAM leakage.  To Reproduce Here is a minimal example to reproduce the issue:   Expected behavior When calling `model.to('cuda')`, all parameters and buffers should be moved to CUDA and the corresponding CPU memory should be freed.  Additional Findings 1. The issue occurs with both `torch.nn.ModuleList` and `torch.nn.Sequential`. 2. If there is only one `Transformer` module in the `ModuleList`, the issue is significantly reduced, with only around 200+ MB of context left in RAM. 3. When there are more than two `Transformer` modules in the `ModuleList`, there is significant RAM leakage. 4. The issue was first discovered while using the Whisper model. Initially, it was thought to be an issue with HuggingFace's transformer library, but it was later found to occur with any complex PyTorch model. 5. The issue does not occur with sequences containing more than two `Linear` or `LSTM` layers. 6. Using `gc.collect()` does not resolve the issue. 7. `sys.getrefcount(model)` is not 1 after the model is created, indicating additional references.  Additional context It seems that the issue might be related to the way `torch.nn.ModuleList` and `torch.nn.Sequential` handle multiple `torch.nn.Transformer` instances and their memory management when moved to CUDA.  Versions PyTorch version: 2.2.1 Is debug build: False CUD",2024-05-16T08:46:54Z,needs reproduction module: nn module: memory usage triaged,open,0,1,https://github.com/pytorch/pytorch/issues/126388,+1. also for flux models. very huge leakage
transformer,DISABLED test_ring_attention_native_transformer_is_causal_True (__main__.RingAttentionTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ,2024-05-16T05:12:26Z,module: rocm triaged skipped,open,0,0,https://github.com/pytorch/pytorch/issues/126380
transformer,[AOTAutograd] tweak min-cut partitioner to avoid saving softmax output,"  CC([AOTAutograd] tweak mincut partitioner to avoid saving softmax output)  CC([inductor] use smaller RBLOCK for expensive reduction kernels) Right now the linear + cross entropy loss operation (usually to be the last part of a transformer model) does the following thing 1. run matmul to get softmax_input 2. load softmax_input to compute max per row. 3. load softmax_input to compute sum per row 4. load softmax_input, normalize it and save the result to softmax_output Step 4 is inefficient since a. in the fwd pass, only a small slice of the softmax_output tensor is need to compute NLLLoss. Materializing the whole tensor is an overkill b. in the backward pass, we need the whole softmax_output, but it can be recompute from softmax_input If we skip saving softmax_output, we would have perf wins since this is the largest tensor in the network. For llm.c, the size is batch_size * sequence_length * vocab_size * item_size ~= 32 * 1024 * 50257 * 2 ~= 3GB. Simply read/write such large tensor need ~2ms in A100. If we recompute softmax_output, we save 1 load for softmax_input and 1 store for softmax_output, which would result in ~4ms saving. To avoid saving the softmax_output we need make sure the min cut partitioner decides to recompute it based on softmax_input and the max/sum tensor (which is small) computed in step 2 and 3. This is not happening currently since the min cut partitioner overestimate the cost of recomputation. The fix is suggested by   to let `dist_from_bw` play a less important role.",2024-05-15T22:54:40Z,ciflow/inductor,open,0,5,https://github.com/pytorch/pytorch/issues/126348,"With the new heuristics, the backward runs slower and we end up with roughly neutral perf overall for llm.c. The reason is the kernel computing gradient of softmax input in the backward pass picks a suboptimal triton config. CC([inductor] use smaller RBLOCK for expensive reduction kernels) fixes that, and now we have the 4ms saving as estimated in the summary for llm.c: ","> Also, obviously, do a perf run. Perf run shows 5 seconds compilation time regress for TIMM link. I'll need debug where that comes from.",rerequest when ready,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,DISABLED test_ring_attention_native_transformer_is_causal_False (__main__.RingAttentionTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ,2024-05-15T21:12:28Z,module: rocm triaged skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/126330
transformer,DISABLED test_ring_attention_custom_transformer (__main__.RingAttentionTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ,2024-05-15T16:35:48Z,module: rocm triaged skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/126296
transformer,[inductor][cpu]speech_transformer AMP single/multiple thread static/dynamic shape CPP/default wrapper performance regression in 2024-05-12 nightly release, üêõ Describe the bug AMP static shape default wrapper               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       speech_transformer       multiple       1       0.958469       0.030251814       0.028995425912766       69.833969       1.0       1.263245       0.023321511       0.029460782163194997       34.216882       0.76       1.02       0.77       0.49                 torchbench       speech_transformer       single       1       0.996765       0.217230587       0.21652784605105502       69.387519       1.0       1.268162       0.176525437       0.223862851236794       33.153806       0.79       1.03       0.81       0.48          AMP dyanmic shape default wrapper               suite       name       thread       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       torchbench       speech_transformer       multiple       1       0.951825       0.030079704       0.0286306142598       69.78338       1.0       1.258736       0.023420975       0.029480824387600003       34.077379       0.76 ,2024-05-15T09:50:50Z,triaged oncall: pt2 oncall: cpu inductor,open,0,2,https://github.com/pytorch/pytorch/issues/126274,"Hi , according to the bisect search log, the PR CC([dynamo] Turn on guard_nn_modules) may introduce this AMP performance regression issue on CPU, could you please help to double check it?","Hi , have you got time to take a look of this issue?"
transformer,DISABLED test_transformerdecoder (__main__.TestNN),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 9 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_transformerdecoder` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Sample error message   Test file path: `test_nn.py` ",2024-05-13T03:39:41Z,module: nn triaged module: flaky-tests skipped oncall: pt2 module: dynamo,closed,0,0,https://github.com/pytorch/pytorch/issues/126043
transformer,DISABLED test_transformer_training_is_seq_parallel_True (__main__.DistTensorParallelExampleTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). Same as  CC(DISABLED test_transformer_training_is_seq_parallel_False (__main__.DistTensorParallelExampleTest)) ,2024-05-11T04:09:35Z,oncall: distributed module: rocm module: flaky-tests skipped,open,0,0,https://github.com/pytorch/pytorch/issues/125991
transformer,DISABLED test_transformer_training_is_seq_parallel_False (__main__.DistTensorParallelExampleTest),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). Broken by either CC([dtensor] improve new factory strategy) or CC([dtensor] add op support for memory efficient attention) or CC([dtensor] run transformer sdpa in dtensor). CC    ,2024-05-10T14:44:46Z,oncall: distributed module: rocm module: flaky-tests skipped,open,0,0,https://github.com/pytorch/pytorch/issues/125918
transformer,Do not import transformers when import torch._dynamo (#124634),"Fixes  CC(`import torch._dynamo` appears to accidentally have a runtime dep on jax/xla (and other things)) Pull Request resolved: https://github.com/pytorch/pytorch/pull/124634 Approved by: https://github.com/thiagocrepaldi, https://github.com/Chillee ghstack dependencies: CC([NJT] Inline through torch.nested.nested_tensor_from_jagged instead of graph break) Fixes ISSUE_NUMBER",2024-05-08T14:44:13Z,release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/125755
transformer,[DTensor][Tensor Parallel] transformer test numerical issue when `dtype=torch.float32`," üêõ Describe the bug The transformer test is at https://github.com/pytorch/pytorch/blob/eb93307dd3ad64b661759e13164535a76496031d/test/distributed/tensor/parallel/test_tp_examples.pyL190 The test passes if we use batch size = 8, sequence length = 8, ModelArgs.dim = 16. For other settings, it can easily fail due to numerical discrepancies between singleGPU and 4GPU runs. The problem disappears if we change parameter type to `torch.float64`. Note: For fp64, math attention backend will be used, and DTensor doesn't fully support it yet. (One needs to apply some trick to enable the test.) For fp32, memoryefficient attention backend will be used which is now supported by DTensor. The numerical issue persists even if we remove attention modules, so seemingly it's not caused by memoryefficient attention. ",2024-05-08T04:32:51Z,triage review oncall: distributed triaged module: dtensor,open,1,0,https://github.com/pytorch/pytorch/issues/125741
transformer,AsyncCollectiveTensor: prevent wait_tensor() calls on graph inputs from getting DCEd," was seeing the loss eventually become NaN when compiling individual transformer blocks in torchtitan  with this patch I no longer see the NaN loss. The problem is the following: (1) It is possible to have graph inputs to a compiled region that are AsyncCollectiveTensors. In particular: when we compile individual transformer blocks in the llama model, the first layer (embedding layer) is run in eager mode, and it outputs an AsyncCollectiveTensor that is fed to the first transformer block (2) ideally, we would like that AsyncCollectiveTensor graph input to desugar into a `wait_tensor()` op that shows up at the beginning of the graph. (3) the way this is supposed to happen is: AOTAutograd traces through the __torch_dispatch__ of AsyncCollectiveTensor, tracing out a `wait_tensor()` call before dispatching to any of the other ops in the function we are tracing (4) however: `trigger_wait()` was getting called in a way where we would ignore its output (and return `self.elem` directly), which would cause the `wait_tensor` ops to get DCE'd.   CC(AsyncCollectiveTensor: prevent wait_tensor() calls on graph inputs from getting DCEd)  CC(AOTAutograd: use info not debug logging for ViewAndMutationMeta) ",2024-05-07T11:09:29Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,2,3,https://github.com/pytorch/pytorch/issues/125677,Very nice fix! Thank you!, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,DISABLED test_train_parity_2d_transformer_checkpoint_resume (__main__.TestFullyShard2DTraining),Platforms: rocm This test was disabled because it is failing on main branch (recent examples). ://github.com/pytorch/pytorch/commit/dba689bbfdc65cf12711ec4b4f2f7eed0fae3a59  ,2024-05-06T23:40:22Z,module: rocm triaged skipped,open,0,0,https://github.com/pytorch/pytorch/issues/125644
transformer,"Compiling with Inductor, DDP, and Dynamic Shapes Results in Errors"," üêõ Describe the bug `torch.compile` with the inductor backend errors out with dynamic shapes and DistributedDataParallel. Either a direct error `ConstraintViolationError: Constraints violated (L['x'].size()[1])!` when using `torch._dynamo.mark_dynamic`, or recompiling multiple times until the recompile limit is reached due to a ""stride mismatch at index 0"" compilation error with `dynamic=True` or `dynamic=None`. These errors occur in both PyTorch 2.3 and the latest PyTorch Nightly. I've created a replication with a simple ""transformer"" model, with just an embedding layer and linear head layer, so I can vary the shape of the sequence length in the batch. I get the same errors with a full fromscratch transformer with DDP. I inconsistently get the ConstraintViolationError when using `torch._dynamo.mark_dynamic` in a nondistributed context with PyTorch 2.3. Specifically, with the Hugging Face Transformers Llama implementation. But I have been unable to replicate it with nonHF code.  Error logs With my replication script below, compiling a DDP model for dynamic shapes with the recommended `torch._dynamo.mark_dynamic` instead of using `torch.compile(..., dynamic=True)` using the following command:  results with the following `ConstraintViolationError`   You can turn on logging with `logging`, but the dynamo logs don't appear to be that useful compared to other errors I've seen.  The same command using `torch.compile(..., dynamic=True)`  or `torch.compile(..., dynamic=None)` and relying on the compiler to detect dynamic shapes  results in a recompiles error:  The logging output also doesn't appear to verbose.  I'm happy to add more logging if w",2024-05-06T23:29:54Z,high priority oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes,closed,0,11,https://github.com/pytorch/pytorch/issues/125641,benjamin I ran locally with a nightly and this actually passes for me. Can you try out a nightly?  https://pytorch.org/getstarted/locally/," I tested my replication script with yesterday's nightly and 2.3. You can see my environment in the ""PyTorch Nightly Environment"" section. These errors are only with DDP. Single GPU compiles and trains without issue. I installed today's nightly `pytorch2.4.0.dev20240507` with both Cuda 12.4 & 12.1. Setting dynamic shapes with DDP via `torch._dynamo.mark_dynamic` using the following command still errors out with the same `ConstraintViolationError`.  And setting `torch.compile(..., dynamic=True)` or `torch.compile(..., dynamic=None)` using the following command still results with recompilations every batch until the `torch._dynamo hit config.cache_size_limit (8)` is hit. ","I am seeing the same issue this morning, running the same three commands on the replication script, on my system using CUDA 12.1. Details below:  PyTorch Nightly Environment details  ","I'm going to look into this. But my recollection is that HF added some error checking code which forces specialization, and I haven't gotten around to yelling at them to stop running this logic when being torch compiled. BTW, the two errors here are one and the same. `mark_dynamic` is yelling at you because it tried to make it dynamic, but failed due to specialization. You can use TORCH_LOGS=dynamic to find out where the specialization happened.","> I'm going to look into this. But my recollection is that HF added some error checking code which forces specialization, and I haven't gotten around to yelling at them to stop running this logic when being torch compiled. It's not just HF models which trigger this when using DDP. My replication script uses a simple twolayer model with an `Embedding` and `Linear` layer. One layer doesn't replicate this issue. It seems to have something to do with adding a second layer.  > BTW, the two errors here are one and the same. `mark_dynamic` is yelling at you because it tried to make it dynamic, but failed due to specialization. You can use TORCH_LOGS=dynamic to find out where the specialization happened. When I run my replication script with `TORCH_LOGS=+dynamic`  I get the following output for rank 0:    TORCH_LOGS=+dynamic Rank 0 Output ```text torch/fx/experimental/symbolic_shapes.py:2268] [0/0] create_env torch/fx/experimental/symbolic_shapes.py:3239] [0/0] create_symbol s0 = 977 for L['x'].size()[1] [2, 9223372036854775806] at test/replication.py:52 in forward (_dynamo/variables/builder.py:2137 in ), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=""s0"" torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval True == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval False == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval False == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Eq(16*s0, 16) == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Ne(Mod(16, 16*s0), 0) == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval 2048*s0 > 2048 == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval True == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Ne(s0, 1) == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Ne(16*s0, 16) == True [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval Eq(s0, 1) == False [statically known] torch/fx/experimental/symbolic_shapes.py:4719] [0/0] eval s0 > 1 == True [statically known] torch/fx/experimental/symbolic_shapes.py:4634] [0/0] eval 32768*s0  I'm not seeing anything about specialization, but might be misinterpreting the logs.","It's this:  Very strange though, why is this suppressed ü§î. You could get a full backtrace for this log with  TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(2048*s0, 2000896)""", could it be related to this ? https://github.com/pytorch/pytorch/pull/120523/filesdiffcb8e02fc8f37e53904ab1b151c46dd109cf50d8121bbd340834b2e976b22ebc4R74  Maybe the idiom there is not correct. We're trying update the meta strides without adding guards or specializations,"Oh yeah, this looks very very naughty. Hmmmm","As a stopgap, I guess we could prevent replacements from happening when guards are suppressed. This still seems very naughty though.....","Here's the additional backtrace with the ""Eq(2048*s0, 2000896)"" guard added:     TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=""Eq(2048*s0, 2000896)""  ",I believe this is fixed
transformer,Compiling GPT2 with tensorrt backend and dynamic shapes results in guard failures., üêõ Describe the bug I'm trying to compile GPT2 model with dynamic shapes and encountered this guard failure at the end. Any suggestions on what's the exact issue and how to resolve it ?   Error logs Please find the error log here : full_gpt2.log  Minified repro   Versions Versions of relevant libraries: [pip3] flake8==6.1.0 [pip3] flake8bugbear==23.3.23 [pip3] flake8comprehensions==3.12.0 [pip3] flake8executable==2.1.3 [pip3] flake8loggingformat==0.9.0 [pip3] flake8pyi==23.3.1 [pip3] flake8simplify==0.19.3 [pip3] mypy==1.3.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.26.0 [pip3] onnx==1.15.0 [pip3] onnxgraphsurgeon==0.3.25 [pip3] onnxruntime==1.16.3 [pip3] onnxscript==0.1.0.dev20240205 [pip3] onnxsim==0.4.35 [pip3] optree==0.10.0 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] pytorchtritonrocm==2.2.0 [pip3] torch==2.3.0+cu121 [pip3] torch_tensorrt==2.3.0.dev0+6f945fa4c [pip3] torchprofile==0.0.4 [pip3] torchsurgeon==0.1.2 [pip3] torchvision==0.18.0+cu121 [pip3] triton==2.3.0 ,2024-05-06T17:29:29Z,triaged oncall: pt2 module: dynamic shapes,open,0,2,https://github.com/pytorch/pytorch/issues/125604,Dynamic shapes guards failure:   ,"This is probably  CC(Equivalent idea to size-oblivious guard for end of bounds on sizes) If you don't mind recompiling on 1024 specifically, you can fix this with `maybe_mark_dynamic` instead of `mark_dynamic`"
transformer,Issues in loading quantized weights with Version==None," üêõ Bug       Hi! I am using torch 2.0.0 and the same issue appeared. I have a custom architecture based on transformer model (Attention + FeedForward). I quantized my model and saved it successfully. However, when I tried to load the weights, it gave me the same error. (I also checked with torch==2.2.2 and torch==2.3.0) Preparing for weights loading  Weights Loading  Error:  This error was also reported in CC(Inconsistency loading quantized models when state_dict with Version==None)  Versions PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Amazon Linux 2 (x86_64) GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.117) Clang version: Could not collect CMake version: version 3.29.2 Libc version: glibc2.26 Python version: 3.10.14  (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64bit runtime) Python platform: Linux5.10.214202.855.amzn2.x86_64x86_64withglibc2.26 Is CUDA available: N/A CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: N/A GPU models and configuration:  GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 535.161.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              48 Online CPU(s) list: 047 Thread(s) per core:  2 Core(s) per socket:  24 Socket(s):           1 NUMA node(s):        1 Vendor ID:           AuthenticAMD CPU family:          23 Model:               49 Model name:          AMD EPYC 7R32 Stepping:            0 CPU MHz: ",2024-05-06T05:37:28Z,oncall: quantization,open,0,2,https://github.com/pytorch/pytorch/issues/125564,"The error you're encountering seems to be related to the structure of the saved state dictionary not matching the structure of the model when you're trying to load the weights. This can happen if the model architecture has changed between saving and loading, or if there are inconsistencies in the naming of layers or parameters. Here are a few things you can try to resolve this issue:  Check Model Compatibility: Ensure that the model architecture you're using to load the weights matches exactly with the architecture used to save the model. Even small changes like adding or removing layers can cause compatibility issues.  Verify State Dictionary Keys: Before loading the state dictionary, inspect its keys to ensure they match the expected structure of your model. You can print the keys of the state dictionary using print(state_dict.keys()) and compare them with the names of the parameters in your model.  Strict vs NonStrict Loading: You're currently using strict=False when loading the state dictionary. This means that missing keys will not raise errors. While this can sometimes help in loading partially trained models or models with different architectures, it can also lead to unexpected behavior if the model structure has changed significantly. Try loading with strict=True to see if any errors are raised that could provide more insights into the issue.  Check for Layer Renaming: Make sure that if you've renamed any layers or parameters in your model between saving and loading, you handle these changes appropriately. You may need to manually map the keys in the state dictionary to the corresponding layers in your model.  Debugging: If none of the above steps resolve the issue, try debugging by printing out more information about the model and the state dictionary during loading. This can help identify where exactly the mismatch is occurring.","Thank you for replying so fast. Actually I have done all of this already.  I saved the model state_dict after quantization and followed the same steps to load the weights. As for the keys I have verified that they match in the `model.state_dict` and loaded `state_dict` I followed the following steps when I was performing QAT and same steps I am doing to load the weights as well **QAT:**  **Loading:**  **Model State Dict:**  **Keys in weights from memory**  Additionally, model architecture did not change because I verified, and I am the one who saved the weights after QAT and trying to load them to test them.  Also, if you look at CC(Inconsistency loading quantized models when state_dict with Version==None), the same issue existed previously. "
transformer,"‚ùìDifferent results between normal batching and `vmap` while using lower precision (e.g., bfloat16)"," üêõ Describe the bug  Description Hi there, I would like to get persample gradients (application: transformer classifier for text). in a preliminary test, I noticed that when computing the losses in the various modes: 1. batched (typical torch) 2. manually batched 3. vmap batched I get different results when running with lower precision. When computing in higher precision, 32 or 64, the differences shrink and disappear altogether, respectively. Is this expected? Can it be avoided? Also, I noticed that the the `grad_fn` differ between 1 and 3. Specifically, in normal batched mode I get `NllLossBackward0` while with vmap I get `DivBackward0`. Would be great to understand better why vmap returns this ""Div"". Below, I report a snippet of the code that I am using. Thank you very much in advance for your attention.   Versions  Env info (same issue with torch 2.2.*) PyTorch version: 2.3.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.4 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.35 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.2.036genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe Nvidia driver version: 550.54.14 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.7 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.7",2024-05-04T15:40:30Z,module: numerical-stability triaged module: vmap module: functorch,open,0,2,https://github.com/pytorch/pytorch/issues/125534,"I believe in this case it's that we implement the batching rule for `nll_loss` with a decomposition, which decomposes `nll_loss` into a number of other ops, which includes a division.","Hi , thank you so much for your answer. Regarding my question about different results when using lower precision tensors, I think it might be due to the order of the operations that functorch implements vs torch, right? This is to say, it is fine to close this issue. Btw, I am really glad functorch exists üöÄ  Computing persample hessianvector products for small (<500M) transformer models has been a breeze. I am not sure I would have been able to do it in torch so efficiently"
transformer,GPU stats not being reported with multi-gpu inference with torchrun," üêõ Describe the bug I wanted to run a whisper via api using multigpu inference and came up with a hacky code. The gpu usage and memory information in this case are not being reported properly.  I am running the api as follows:  I am using torchrun as follows:  When I run it in this way I am able to use multigpu inference via my api, but the gpu information including usage(which always shows 0%) and memory is not updated properly. I agree I have written the code in an uncoventional way, but regardless issues in updating gpu stats may even turn out to be a security issue.  Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.027genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 GPU 4: NVIDIA RTX A6000 GPU 5: NVIDIA RTX A6000 GPU 6: NVIDIA RTX A6000 GPU 7: NVIDIA RTX A6000 GPU 8: NVIDIA RTX A6000 GPU 9: NVIDIA RTX A6000 Nvidia driver version: 535.171.04 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Addre",2024-05-03T05:06:37Z,oncall: distributed,closed,0,0,https://github.com/pytorch/pytorch/issues/125459
transformer,FSDP2 Memory Tracker," CC(FSDP2 Memory Tracker)  Why do we need the FSDP Memory Tracker? **Tuning Decisions** 1. What is the expected peak memory with current configuration? 2. If I change my FSDP wrapping, how much effect will it have on peak memory? 3. What is the best batch size to use? 4. What is the maximum sequence length that one can run with current configuration? 5. How does increasing/decreasing the ‚ÄúDP‚Äù world size affect peak memory? 6. How much memory do I save if I move the optimizer to the CPU? 7. Which activation checkpointing policy should I use? 8. If I have various SAC policies, How do they compare against each other? 9. What happens if I apply different SAC policies to different FSDP units? 10. If I make my gradient reduction in fp32, what effect will it have on memory? 11. If I want to use a custom mixed precision policy, how will it affect the peak memory? 12. When does it make sense to use HSDP? 13. Can I reshard to a smaller mesh without increasing peak memory substantially? 14. Can safely disable post forward reshard without causing an OOM? **Debugging** 1. Which module contributes most to activation memory? 2. Which FSDP unit is holding a lot of unsharded memory? 3. AC is not releasing memory? The FSDP2 Memory Tracker addresses all of the above. It is based on:   CC(Memory Tracker for tracking Module wise memory)    CC(Extended Module Tracker)   Example and Output:   cc:      ",2024-05-01T19:07:04Z,oncall: distributed Merged ciflow/trunk topic: not user facing release notes: distributed (tools),closed,1,16,https://github.com/pytorch/pytorch/issues/125323,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: sanketpurandare  (a641063de2b5ae9f5ad7c17a009403ac67562a59, d7421d297503b2e665cdc8d37445f28137e9ff28, 685e8bf3b3c4d6fb45798f650be4c5f3ce381b96)","Highlevel comment: It might be worthwhile to document which parts are depending on FSDP2 internals, and we may be able to see how to expose things more robustly.","> This seems okay to me! I think we just need to be careful of the situation ""I changed something in FSDP2 and now it broke the memory tracker. How do we fix it?"" Yeah we need to maintain it actively till FSDP2 stabilizes.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""stuck ci winvs2019""","The merge job was canceled or timed out. This most often happen if two merge requests were issued for the same PR, or if merge job was waiting for more than 6 hours for tests to finish. In later case, please do not hesitate to reissue the merge command  For more information see pytorchbot wiki."," merge f ""stuck ci winvs2019"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," I'm seeing the new test `test_tracker_multi_group_eager` failing on ROCm distributed job https://hud.pytorch.org/pytorch/pytorch/commit/287c68c5eca2e15bf73b84fe9e39755ae3f842ba CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)45778.  Could you help take a look?  The job is only run periodically, do its signal was missed on the PR",">  I'm seeing the new test `test_tracker_multi_group_eager` failing on ROCm distributed job https://hud.pytorch.org/pytorch/pytorch/commit/287c68c5eca2e15bf73b84fe9e39755ae3f842ba CC(Êú™ÊâæÂà∞Áõ∏ÂÖ≥Êï∞ÊçÆ)45778. Could you help take a look? The job is only run periodically, do its signal was missed on the PR Yeah will push a fix","Btw, I disable the test in  CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore)) to keep trunk sane.  In your fixed PR, please add ""Fixes  CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore))"" in your PR description to run the test in your PR","> Btw, I disable the test in CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore)) to keep trunk sane. In your fixed PR, please add ""Fixed CC(DISABLED test_tracker_multi_group_eager (__main__.TestTrackerFullyShard1DTrainingCore))"" in yout PR description to run the test in your PR Yeah it's because of Python 3.8 not supporting bitwise OR in typing"
transformer,Accessing the `device` attribute of bias terms of `TransformerEncoderLayer` initialized with `bias = False` causes Attribute error," üêõ Describe the bug When performing a forward pass on `TransformerEncoderLayer` initialized with an even number of heads (2, 4, 6, ...) and without bias (`bias=False`) in eval mode (`model.eval()`), I face with an error `AttributeError: 'NoneType' object has no attribute 'device'`. Here is the stack trace:  Upon looking at the relevant code piece, I think the following lines will create the error because the bias terms are None and this line accesses the `device` attribute of None objects and thus the error.  Versions  ",2024-04-26T10:15:07Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/125015,"These lines may be troublesome as well because they access the bias terms, which might or might not exist depending on whether the layer initialized with bias or not.",Hey  can you provide code snippet where you are getting the error?,"Seems to be a duplicate of  CC(TransformerEncoderLayer raise `AttributeError: 'NoneType' object has no attribute 'device'` when `bias=False, batch_first=True`).",I just checked that the problem is fixed with the latest torch version. Feel free to close this issue.
transformer,Export llama v3 to ONNX, üêõ Describe the bug   Versions main Blocked by: * Issue https://github.com/microsoft/onnxscript/issues/1471  * PR https://github.com/pytorch/pytorch/pull/124977  * Issue https://github.com/microsoft/onnxscript/issues/1462,2024-04-25T21:46:15Z,module: onnx triaged onnx-triaged,open,0,1,https://github.com/pytorch/pytorch/issues/124973,Depends on https://github.com/microsoft/onnxscript/issues/1462
transformer,torch.compile fails on hugging face Mistral7b," üêõ Describe the bug **The following code fails:** import torch from transformers import AutoModelForCausalLM, AutoTokenizer model = AutoModelForCausalLM.from_pretrained(     ""mistralai/Mistral7Bv0.1"",     torch_dtype=torch.float16,     use_cache=True, ) model = torch.compile(model) input_ids =torch.randint(low=0, high=60000, size=(1, 32), dtype=torch.int64) attention_mask = torch.ones(1, 32, dtype=torch.int64) model(input_ids=input_ids, attention_mask=attention_mask) **It generates the following error:** Traceback (most recent call last):   File ""/work1/sleduc/torchtrials/compile_mistral7b.py"", line 13, in      model(input_ids=input_ids, attention_mask=attention_mask)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 489, in _fn     return fn(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl                                                                                                                                                                                                                                                                                                                      return self._call_impl(*args, **kwargs)   File ""/work1/sleduc/.python/lib/python3.10/sitepackages/torch/nn",2024-04-25T16:34:59Z,triaged oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/124946,What version of `networkx` do you have installed? It looks like this was fixed on their end quite some time ago: https://github.com/networkx/networkx/pull/4013. I suggest updating that package and trying again.,"Thanks for your answer Sorry for late reply, I was on PTO The networkx version that I was  using was 2.4, which did not have the fix you refered to I have upgraded to latest version and the issue has disappeared Thanks a lot!"
transformer,Conflict between bias=False and why_not_sparsity_fast_path in Transformer Module," üêõ Describe the bug Description: When setting the bias=false for Transformer layers, the model's biases are set to None. When the model is in evaluation mode, why_not_sparsity_fast_path becomes an empty string, allowing entry into the fast path logic. In this part of the logic, where bias is called because it is None, an error occurs. if not why_not_sparsity_fast_path:             tensor_args = (                 src,                 self.self_attn.in_proj_weight,                 self.self_attn.in_proj_bias,                 self.self_attn.out_proj.weight,                 self.self_attn.out_proj.bias,                 self.norm1.weight,                 self.norm1.bias,                 self.norm2.weight,                 self.norm2.bias,                 self.linear1.weight,                 self.linear1.bias,                 self.linear2.weight,                 self.linear2.bias,             )              We have to use list comprehensions below because TorchScript does not support              generator expressions.             _supported_device_type = [""cpu"", ""cuda"", torch.utils.backend_registration._privateuse1_backend_name]             if torch.overrides.has_torch_function(tensor_args):                 why_not_sparsity_fast_path = ""some Tensor argument has_torch_function""             elif not all((x.device.type in _supported_device_type) for x in Ôºåx.device.type throw error  Versions PyTorch: 2.1.2 Python: 3.7 Operating System: Linux ",2024-04-25T13:54:29Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/124937
transformer,"Missing description of Transformer argument ""memory_mask"" shape in 3D (including the batch dimension) case"," üìö The doc issue `nn.TransformerDecoder` seems to support 3D (N*nhead, T, S) memory_mask (since  CC([FYI] MultiheadAttention / Transformer)issue554929072), and the description of `nn.MultiheadAttention` has been updated correctly. But the description of `nn.Transformer` argument memory_mask's shape (https://pytorch.org/docs/stable/generated/torch.nn.Transformer.htmltorch.nn.Transformer, v2.3) missed 3D input case.   Suggest a potential alternative/fix Add description of ""(N*num_heads, T, S)"" for 3D memory_mask inputting case. ",2024-04-25T12:35:06Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/124931
transformer,`RuntimeError: invalid dtype for bias` when use compile + autocast," üêõ Describe the bug When I tried using `torch.compile` along with `autocast` to infereance a llama's decoder block, I encountered `RuntimeError: invalid dtype for bias  should match query's dtype`.   Versions  ",2024-04-25T01:42:03Z,triaged module: amp (automated mixed precision) oncall: pt2,closed,8,5,https://github.com/pytorch/pytorch/issues/124901,is there any fix for this issue? I have the same exact error when combining autocast and compile. Thanks.,I am having this issue. Is there a fix please??,"hmm... running the repro above on a nightly, even when I turn off torch.compile I get a shape mismatch:  gives: ","> hmm... running the repro above on a nightly, even when I turn off torch.compile I get a shape mismatch: >  >  >  > gives: >  >  Sorry, the `att_mask` should be `8, 1, 2048, 2048`. I updated the repro code.",It works well with torch `2.5.0.dev20240812` !!!
transformer,Dynamo Export Support for Qwen/Qwen-7B-Chat: Mutating module attribute _ntk_alpha_cached_list during export, üêõ Describe the bug I want to export Qwen/Qwen7BChat using Dynamo. I have a short repro script below that runs into an export error related to missing support for mutating module attributes. There are multiple models currently facing the same issue  The error is pasted below:   Versions Versions Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.16.0 [pip3] onnxgraphsurgeon==0.5.2 [pip3] onnxruntime==1.17.3 [pip3] onnxscript==0.1.0.dev20240417 [pip3] optree==0.10.0 [pip3] pytorchquantization==2.1.2 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240423+cu121 [pip3] torchtensorrt==2.3.0a0 [pip3] torchaudio==2.2.0.dev20240423+cu121 [pip3] torchdata==0.7.1a0 [pip3] torchtext==0.17.0a0 [pip3] torchvision==0.19.0.dev20240423+cu121 [pip3] triton==2.2.0+e28a256 ,2024-04-23T22:37:21Z,oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/124796
transformer,Dynamo Export Support for Google/Gemma-2B: Mutating module attribute inv_freq during export, üêõ Describe the bug I want to export google/gemma2b using Dynamo. I have a short repro script below that runs into an export error related to missing support for mutating module attributes. There are multiple models currently facing the same issue  The error is below   Versions Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.16.0 [pip3] onnxgraphsurgeon==0.5.2 [pip3] onnxruntime==1.17.3 [pip3] onnxscript==0.1.0.dev20240417 [pip3] optree==0.10.0 [pip3] pytorchquantization==2.1.2 [pip3] pytorchtriton==3.0.0+45fff310c8 [pip3] torch==2.4.0.dev20240423+cu121 [pip3] torchtensorrt==2.3.0a0 [pip3] torchaudio==2.2.0.dev20240423+cu121 [pip3] torchdata==0.7.1a0 [pip3] torchtext==0.17.0a0 [pip3] torchvision==0.19.0.dev20240423+cu121 [pip3] triton==2.2.0+e28a256 ,2024-04-23T22:26:35Z,triaged oncall: pt2 oncall: export,open,0,0,https://github.com/pytorch/pytorch/issues/124793
transformer,"Add scaled_dot_product_attention ""scale"" argument to nn.MultiHeadAttention "," üöÄ The feature, motivation and pitch Recently the Maximal Update Parametrization (muP, arxiv 2203.03466) is becoming prevalent in large model training becaus of its ""zeroshot hyperparameter"" transfer capabilities (i.e., grid search the best HPs on 210M params models and and train the 2B+ params with the same HPs) Refs:  FLM101B: An Open LLM and How to Train It with $100K Budget  CerebrasGPT: Open ComputeOptimal Language Models Trained on the Cerebras WaferScale Cluster  JetMoE: Reaching Llama2 Performance with 0.1M Dollars One modification involved in getting a transformer muParametrized is to modify the softmax scaling from $1/\sqrt{d}$ to $1/d$ or $1/d*\alpha$ with $\alpha$ being grid searched/learnable This can easily be done with `torch.nn.functional.scaled_dot_product_attention` using its ""scale"" argument, but nothing like that exists for torch.nn.MultiheadAttention.  Alternatives _No response_  Additional context _No response_  ",2024-04-23T09:16:32Z,triaged module: sdpa,open,0,2,https://github.com/pytorch/pytorch/issues/124718,I would be interested in working on this. ,Is there a reason why you wouldnt just using sdpa directly instead of using nn.MHA
transformer,"[dynamo] Unexpected SymBool appearing in ""is_causal"" inside scaled_dot_product_attention()", üêõ Describe the bug The error message is this:   The python program to reproduce the above is this    Versions env.txt ,2024-04-23T06:43:27Z,triaged oncall: pt2 module: dynamic shapes,closed,0,4,https://github.com/pytorch/pytorch/issues/124707,"What we need to do is force a specialization when this happens, similar to what we have been doing with int/float arguments. Shouldn't be hard.","I unsuccessfully tried to put together a smaller repro:  I'm unable to repro the error with this, so the real model is doing something different.","Dynamo might just be blindly specializing on the `> 1` test. If you modify Dynamo not to specialize immediately (which is better), then you'd hit it.","Doing issue scrapping, closing as original repro does not fail on trunk."
transformer,Remove activation checkpointing tag to get correct FQNs,"Fixes CC(Not loading optimizer state separately from checkpoint causes errors with FQNs)  When setting `use_orig_params = False` and using activation checkpointing, the FQN mapping as retrieved by the `_get_fqns` function is incorrect because the prefix that is added to the name of each activation checkpointed module, `_checkpoint_wrapped_module`, can still be present. I think this is an edge case with the `_get_fqns` function that was not addressed by this previous commit CC([DCP] Removes Checkpoint Wrapped Prefix from state dict fqns). Without the change, the list of object names for an activation checkpointed module with FSDP (and `use_orig_params=False`) can be something like:  Which will incorrectly return just one FQN, `{'model.transformer.blocks.0._flat_param'}`, when all the FQNs of the parameters of the transformer block should be returned. With the change, the list of object names will now have `_checkpoint_wrapped_module` removed:  And the FQNs are correctly retrieved and returned in `_get_fqns` when this condition is satisfied. The correct FQNs are:  ",2024-04-23T03:30:19Z,oncall: distributed open source Merged ciflow/trunk topic: not user facing module: distributed_checkpoint,closed,0,7,https://github.com/pytorch/pytorch/issues/124698,  I'm not sure why two of the checks are failing and I can't seem to trigger reruns. Could I get some assistance with this?, The failures are unrelated and won't block merging., merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Onnx backprop workarounds,"NB: **I have no intention of getting this merged in its current state**, but maybe it's possible to find alternative ways to address the issues I ran into here that are safer, or maybe use this as a jumping off point for supporting this kind of thing using the new dynamo_export() code path... I've been trying to export a complicated model to onnx, featuring a load of transformers, slicing operators and most importantly some backpropagation in the forward pass. This is basically so we can do inference on a diffusion model with classifier guidance, which requires differentiating through the denoiser at inference time. I don't think this is a particularly exotic use case so I feel like it's an important thing to support, and I could see it being used for other things like optimizing a latent code at runtime too. I made a post on the forums about it a while back, along with some tickets: https://devdiscuss.pytorch.org/t/exportingamodelcontainingbackpropagationtoonnx/1984  CC(Batch size can't be varied if you export self attention to onnx)  CC(Can't export an onnx with dynamic axes if it contains backpropagation)  CC(Can't differentiate through indexing operation)  CC(Problems differentiating through a transformer when exporting to onnx)  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace)  CC(Problems differentiating through certain operations when exporting to TorchScript) This branch was basically me brute forcing it so I could get it to work, and should be seen as more of a document of the issues I ran into than a literal merge request as some of the changes are a bit nasty. I did eventually get",2024-04-22T23:07:31Z,oncall: jit module: onnx open source Stale release notes: onnx ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/124677," :x:  login:  . The commit (d7247a11ebf131207c3096989c8a3f90a9c24866, d7a2b3a954cf326f270c2e40a92d838f0a7fce21, a2b24a0e59f522e30fc33f171c0333486bc41bb9, 44b152f2c1c640d36587570a4c180ed8841e0c19, cfff6c91a2ff031b81692c02674f7f046aaeb44b, b7166c34846b508ce8205f09498fb66e41665b6e, 6b305ac04a4d59bc569ca4c10b86ef8fe2274afa, 3554bee30901aa21b7dda0791c44cdea71765257, b20068d95b8455e6247b39bb87b1f4de3f7c45e7, f4ca44fa738448e95a279918ac6a923b2a9ff7f3, 615e7ccd6a4e9dff1ed4e998b5119fbd3aaf2c7f) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Do not import transformers when import torch._dynamo,  CC([NJT] Allow construction of NJT within graph using offsets from inputs)  CC(Do not import transformers when import torch._dynamo)  CC([NJT] Inline through torch.nested.nested_tensor_from_jagged instead of graph break) Fixes  CC(`import torch._dynamo` appears to accidentally have a runtime dep on jax/xla (and other things)),2024-04-22T18:03:58Z,module: onnx Merged onnx-triaged ciflow/trunk release notes: onnx topic: bug fixes release notes: dynamo,closed,0,7,https://github.com/pytorch/pytorch/issues/124634,"https://github.com/huggingface/safetensors/pull/318 has been merged, maybe this check can be skipped at all?", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.8gcc11 / test (docs_test, 1, 1, linux.2xlarge)  Lint / lintrunnernoclang / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,please submit cherry pick to 2.3 branch
transformer,[NJT] Allow construction of NJT within graph using offsets from inputs,"  CC([NJT] Allow construction of NJT within graph using offsets from inputs)  CC(Do not import transformers when import torch._dynamo)  CC([NJT] Inline through torch.nested.nested_tensor_from_jagged instead of graph break) Creating symbolic nested ints within the graph is difficult. Using unbacked symints should solve the most important(?) cases in the mean time. See  CC([Nested Tensor] Support NT construction inside PT2 graph) Known gaps:  creating NJT from intermediate offsets (offsets created within the graph, as opposed to being offsets passed in as inputs)  when the same offsets is also passed in as a input to the graph. We are not smart enough to realize that the offsets from that input is the same and therefore would fail when  the sizes are compare (""s0 cannot be compared with u0"") ",2024-04-22T16:19:01Z,module: dynamo no-stale,open,0,4,https://github.com/pytorch/pytorch/issues/124624, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/soulitzer/296/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/9274613146,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[export] Llama3 export with dynamic shapes fails with constraint violations," üêõ Describe the bug I'm trying to export Llama38B model with dynamic shapes and encountered the following error. The same error occurs for Llama2 export as well. Any suggestions ?   The above core results in the following error   After applying the suggested fixes, I changed the line   this results in another error:  Note : Exporting model on cpu works (if I remove .eval().cuda())  Versions Versions of relevant libraries: transformers==4.40.0 [pip3] numpy==1.26.4 [pip3] torch==2.3.0+cu121 [pip3] torch_tensorrt==2.3.0.dev0+4323e3646 [pip3] torchvision==0.18.0+cu121 [pip3] triton==2.3.0 cc:   ",2024-04-19T16:55:57Z,onnx-needs-info module: dynamic shapes oncall: export,open,0,7,https://github.com/pytorch/pytorch/issues/124502,"> The exporting model on cpu works (if I remove .eval().cuda()) I'm guessing it's because some cuda kernel has some guards on this. Could you run with `TORCH_LOGS=""+export""` and share the logs?",Please find the logs here logs.txt logs after applying the fixes : logs_after_fixes.txt logs on cpu :  logs_cpu.txt,"hmm, I suspect it's cause of these `pad_last_dim` calls which are generating the guards... If you change the SDPBackend to something else, it might work... like ","Thanks   for the quick suggestion. It works and unblocks us. However, I see the attention op getting decomposed. I'll keep this bug open for now. ", you think  CC(Hybrid backed-unbacked SymInts) would have helped here?,  Any updates on this issue ? Thanks," have you tried ahead of time compilation with AOTInductor for the exported model? If so, have you been able to load the model back and do inference successfully? "
transformer,torch.compile does not work since 2.2.1 on MacOS for some models, üêõ Describe the bug The execution hangs when first calling the model to warmup. _After_ will never be printed. This is on an Apple M3 Max. Smaller models work sometimes.  The last version that worked was torch 2.2.0.   Versions  Thanks for your help! ,2024-04-19T16:14:44Z,high priority triaged module: macos module: regression has workaround oncall: pt2 oncall: cpu inductor,closed,1,10,https://github.com/pytorch/pytorch/issues/124497,I see the same behavior on the torch 2.3.0 RC.," can you run `pip free` and share it's output here? Also, do you mind sampling hanged pytorch process in Activity Monitor and share the results? ","Can you try adding line `torch._dynamo.config.enable_cpp_guard_manager = False` in the script and see if you still see the issue? I don't have any reason to believe that it should matter, but given the timing of this issue, I want to be sure.",">  can you run `pip free` and share it's output here? Also, do you mind sampling hanged pytorch process in Activity Monitor and share the results?  and here is a gist with the sampling https://gist.github.com/maxbartel/f03722d324089b9d8c2605dbd8ac1433 > Can you try adding line `torch._dynamo.config.enable_cpp_guard_manager = False` in the script and see if you still see the issue? I don't have any reason to believe that it should matter, but given the timing of this issue, I want to be sure. I tried, but looking at https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/config.py it seems like none of the releases have this option. :confused: I tried the nightly build where this option is available and it doesn't change the behavior. The program still hangs...","Thank you for the spindump. It confirms my theory that it deadlocks because two libomp.dylibs are loaded into the runtime. From spindump:  I suspect first one is bundled with PyTorch and 2nd one comes from homebrew, that compiler is trying to load. We should put a warning against that/configure compile to use just one. In the meantime  do you mind uninstalling one from homebrew to make sure it solves the problem?","This solved the problem! üéâ I think you are right, compile should decide which one to use and stick to it. A warning is not too useful if `libomp` is a dependency for something else. Thanks for your help!"," what would be the debug option to dump the compiler invocation commands? As installing libomp from homebrew did not result in a hang for me, guess because compiler picks libomp bundled with torch", we're doing some maintenance in the triage meeting; I hope it's not presumptuous to assign the issue to you (seems like you're handling it?),Is there any update on this issue ?,Removing from 2.3.1 Milestone.  5/27 Cherrypick post deadline (End of day 5PM PST)
transformer,SDPA + torch.compile: (*bias): last dimension must be contiguous," üêõ Describe the bug Hi, I noticed a bug with SDPA when using torch.compile, both on 2.2.2 and 2.3 RC. Reproduction:  and  Interestingly, adding  before SDPA call does not help. So the error below is not very helpful. We always have:    Versions ",2024-04-17T15:00:53Z,triaged oncall: pt2 module: inductor,closed,1,3,https://github.com/pytorch/pytorch/issues/124289, It appears https://github.com/pytorch/pytorch/pull/111721 did not fix everything,I am getting similar problems with noncompiled one,Same problem here
transformer,CUBLAS_STATUS_EXECUTION_FAILED when calling cublasGemmEx, üêõ Describe the bug I met a problem similar to CC(CUBLAS_STATUS_NOT_SUPPORTED when calling cublasDgemv) when using torch.multiprocessing  The following code should be quite easy to reproduce. All you need to do is  make sure CUDA VISIBLE DEVICES >1 .   Versions this is my environment  ,2024-04-17T05:48:22Z,module: cuda triaged module: cublas,open,0,2,https://github.com/pytorch/pytorch/issues/124262,Do you have any solutions for this issue?,"> Do you have any solutions for this issue? Not really, the issue seems  hardware related. 3090 x , while A800 ‚úÖ  ÔºöÔºâ"
transformer,[FSDP+TP] RuntimeError: 'weight' must be 2-D," üêõ Describe the bug Hi, I've changed pytorch's FSDP+TP example to HFT5 model and run on 3 nodes with 2 GPUs (total 6 GPUs)  commands `NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=INIT NCCL_IB_CUDA_SUPPORT=0 NCCL_SHM_DISABLE=1 NCCL_P2P_DISABLE=1 NCCL_SOCKET_IFNAME=ens4f0np0 NCCL_IB_DISABLE=0 NCCL_IB_SL=3 NCCL_IB_TC=96 NCCL_IB_GID_INDEX=3 torchrun nnodes 3 nprocpernode 2 noderank 0 masteraddr 10.10.10.10 masterport 8886 pytorch_transformer_2d.py batchsize=4 epochs=1` Command will be fine because another exmaples (e.g., DDP, FSDP) runs well However, problem occurs in `forward` in embedding layer.  Error log   Presumed Reason HFT53b has embedding layer of `Embedding(32128, 1024)` after `__init__` of `/opt/conda/lib/python3.10/sitepackages/torch/nn/modules/sparse.py` , `self.weigt has size of ` torch.Size([32128, 1024])`  but in `forward` of ``/opt/conda/lib/python3.10/sitepackages/torch/nn/modules/sparse.py` `self.weigt has size of ` torch.Size([32899072])`  (32899072=32128*1024) I guess somewhere of code flattens nn.Embedding and makes 2D error..  Another error if we add `""encoder.embed_tokens"": ColwiseParallel(),   `    occurs, I've find similar issues in  CC(FSDP Full Shard compatibility with BF16 AMP)issuecomment1826560494 but have no chances to make changes...  Code (pytorch_transformer_2d.py)   print HFT53b T5ForConditionalGeneration(   (shared): Embedding(32128, 1024)   (encoder): T5Stack(     (embed_tokens): Embedding(32128, 1024)     (block): ModuleList(       (0): T5Block(         (layer): ModuleList(           (0): T5LayerSelfAttention(             (SelfAttention): T5Attention(               (q): Linear(in_features=1024, out_features=4096, b",2024-04-14T05:12:22Z,oncall: distributed,open,0,6,https://github.com/pytorch/pytorch/issues/124019, Do you need to pass `sharded_model` into `train` instead of `model`?,">  Do you need to pass `sharded_model` into `train` instead of `model`? Thanks!  , I missed that point.! I've changed every `model` to `sharded_model`. However 2D weight error is gone but `RuntimeError: aten.mm.default: got mixed torch.Tensor and DTensor, need to convert all torch.Tensor to DTensor before calling distributed operators!` occurs. Is it because I haven't written tensor parallelism for all layers?  Currently, only the line for the input of the first selfattention layer is written.  Current TP   Error log   Try So When I add Colwise parallelism of Embedding layer, `RuntimeError: Cannot writeback when the parameter shape changes Expects torch.Size([16449536]) but got torch.Size([0])` occurs   Error log   Try 2   Error Log  Is there any way to resolve them? Thanks for kind and quick reply!","I'm having the same issue while doing inference using model.generate (GenerateMixin from huggingface). I'm debugging it and am seeing that the shape of the embedding layer is is changed before sparse.py and after module.py (see the call stack below). Also I see that the total number of parameters changes (it becomes twice as less).    My setup: Python 3.11.5 accelerate                       0.30.1 torch                            2.4.0.dev20240515+cu121 torchaudio                       2.2.0.dev20240515+cu121 torchvision                      0.19.0.dev20240515+cu121 transformers                     4.41.2  My guess it has something to do with how parameters sharded, as FSDP from PyTorch mentions that parameters there are 1D: > use_orig_params (bool) ‚Äì Setting this to True has FSDP use module ‚Äòs original parameters. FSDP exposes those original parameters to the user via nn.Module.named_parameters() instead of FSDP‚Äôs internal FlatParameter s. This means that the optimizer step runs on the original parameters, enabling peroriginalparameter hyperparameters. FSDP preserves the original parameter variables and manipulates their data between unsharded and sharded forms, where they are always views into the underlying unsharded or sharded FlatParameter, respectively. With the current algorithm, the sharded form is always 1D, losing the original tensor structure. An original parameter may have all, some, or none of its data present for a given rank. In the none case, its data will be like a size0 empty tensor. Users should not author programs relying on what data is present for a given original parameter in its sharded form. True is required to use torch.compile(). Setting this to False exposes FSDP‚Äôs internal FlatParameter s to the user via nn.Module.named_parameters(). (Default: False)"," FSDP only allgathers parameters for `module.forward`, not custom methods like `module.generate` :( See  CC(FSDP Doesn't Work with model.generate()) for example. In our upcoming FSDP2 implementation, we have a workaround for this, e.g. see https://github.com/pytorch/pytorch/pull/125394.",">  FSDP only allgathers parameters for `module.forward`, not custom methods like `module.generate` :( See CC(FSDP Doesn't Work with model.generate()) for example. >  > In our upcoming FSDP2 implementation, we have a workaround for this, e.g. see CC([RFC][FSDP2] Added `register_fsdp_forward_method` for user fwd methods). Thanks for your comment! I've tried it like  But it fails with timeout. I will try later. This is might be the case that my model just doesn't fit or too slow. I'm running on 4 GPUs with 80GB , I'm running LLAMA 3 70B. It fails with: ","Related: *  CC(FSDP Doesn't Work with model.generate()) *  CC(torch 2.1 FSDP only some layers might not be working with training only a couple of layers) From what I've tested, as said in the above issue, wrapping `model.generate` in `torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params(fsdp_model)` works to solve the error `RuntimeError: 'weight' must be 2D`:  ALSO: if you are timing out, you should try setting `synced_gpus=True` in your call to `generate`. `generate` would hang for me in my training loop until I set it."
transformer,FSDP RuntimeError Output 0 of ViewBackward0 is a view and its base or another view of its base has been modified inplace.," üêõ Describe the bug When using FSDP to train a transformer model, I encountered the same error mentioned in CC([FSDP] RuntimeError when using FSDP with auto wrap for sequencetosequence language models such as T5, Pegasus). But I did not find a solution or workaround for this error.   Steps to reproduce the error: 1. Go to here, run the following command:  2. The following is the output of the Traceback:   3. The model architecture with FSDP wrapping can be found here  Expected Output No error and run the model.   Versions The `collect_env.py` was run on the login node which does not have GPU. I submit the job to nodes with 2 A100 GPUs.  Collecting environment information... PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Red Hat Enterprise Linux release 8.9 (Ootpa) (x86_64) GCC version: (GCC) 11.2.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.28 Python version: 3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.18.0372.9.1.el8.x86_64x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              40 Online CPU(s) list: 039 Thread(s) per core:  1 Core(s) per socket:  20 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU famil",2024-04-13T23:00:05Z,oncall: distributed,closed,0,14,https://github.com/pytorch/pytorch/issues/124017, Did you already make sure that shared parameters are either wrapped in the same FSDP module or in parentchild FSDP modules (i.e. _not_ in different FSDP modules that are not parentchild)?," Thank you for your comment. I am not really sure if they are wrapped in the same FSDP module. You can find the model architecture after FSDP wrapping here. You can see the whole model is the `_fsdp_wrapped_module`, and transformer layers are also the `_fsdp_wrapped_module` because I used the `transformer_auto_wrap_policy`.  If this file does not indicate if they are wrapped in the same FSDP module, please let me know how I can find out.","I have not had the chance to try to run your script, but if you want to check for shared parameters, you can run this before you wrap with FSDP:  Then, we can visually inspect the wrapping which you provided to know if the shared parameters violate the constraint.","Or actually, taking another look, I have another suspicion. The issue might be coming from:  FSDP only knows to allgather the parameters when you run the FSDP module's forward. Here, you are wrapping each transformer layer with FSDP plus the root module with FSDP. This should mean that the embedding is assigned to the root FSDP module, so you must run `self.model(...)` (i.e. `self.model.forward`) in order to trigger the allgather for the `tok_embeddings`.",The following is the output from the for loop:  I need some time to rewrite part of the code to put `model.module.get_embeds(q_input_ids)` into the forward call to see if getting rid of this can avoid the error.,"Okay. I found a solution. I only wrapped the transformer layers, that is `BertLayer` and `BasicTransformerBlock`, instead of the whole model plus the transformer layers. And the error disappeared! Thank you  for the help!"," If you do not wrap the whole model, I think you may not have any overlap of communication/computation ü§î  In particular, we need a parent module of the transformer layers in order to share data structures across them for overlap üò¢ .","I am curious if I wrap the `ModuleList` level which contains these transformers, will this still be an issue? ","yes :( since `ModuleList` does not implement `forward`, we should actually never wrap `ModuleList` with FSDP since then it would never run the allgather",Ahh I see.  Does this issue affect performance or efficiency? ,"If I write a `nn.Module` class with a forward call for these transformer layers, and I wrap this class with FSDP, will this avoid the not allgather issue?","I found this note: > Attempting to run the forward pass of a submodule that is contained in an FSDP instance is not supported and will result in errors. This is because the submodule‚Äôs parameters will be sharded, but it itself is not an FSDP instance, so its forward pass will not allgather the full parameters appropriately. This could potentially happen when attempting to run only the encoder of a encoderdecoder model, and the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap the submodule in its own FSDP unit. Since my model will call `word_embedding` and `lm_head` independently, can I just wrap these two linear layers with FSDP within the whole wrapped model? ","> Does this issue affect performance or efficiency? Wrapping `nn.ModuleList` affects correctness :/ Since parameters are not allgathered, you would see an error (probably a shape mismatch). > If I write a nn.Module class with a forward call for these transformer layers, and I wrap this class with FSDP, will this avoid the not allgather issue? I think so. > Since my model will call word_embedding and lm_head independently, can I just wrap these two linear layers with FSDP within the whole wrapped model? As long as if you wrap a module with FSDP, that module is used via running its `forward`, then you should be fine!",Thank you so much! I will close this issue for now 
transformer,"[2.3 dynamic shapes] backend='inductor' raised: LoweringException: AssertionError: indices must be int64, byte or bool. Got [torch.float32]"," üêõ Describe the bug I installed the final RC version of pytorch 2.3, and ran the following code, errors occurs.   Versions PyTorch version: 2.3.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Dec 19 2023, 08:15:18) [GCC 9.4.0] (64bit runtime) Python platform: Linux4.15.0189genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Quadro RTX 6000 GPU 1: Quadro RTX 6000 GPU 2: Quadro RTX 6000 GPU 3: Quadro RTX 6000 GPU 4: Quadro RTX 6000 GPU 5: Quadro RTX 6000 GPU 6: Quadro RTX 6000 GPU 7: Quadro RTX 6000 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.6 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          64 Online CPU(s) list:             063 Thread(s) per core:       ",2024-04-13T12:29:07Z,high priority triaged oncall: pt2 module: inductor,closed,0,20,https://github.com/pytorch/pytorch/issues/124006,Error logs: ,"I actually got a different error when running this code, but maybe due to some local setup issues. Someone else should try reproing this.   click to expand   ", that error means you need to clear /tmp/torchinductor cache,"Thanks, I'm able reproduce now",I test the code in the 2.3 release version and no error occurs.  ,I've just experienced the same error with the last nightly version. Do we had a regression or is it another case? ,In my case it seems originated around `if self.use_checkpoint:` ," yeah, the original test script still reproduces for me","Info on how to debug and change user code in this error would be helpful, even if not fixed","Mine, interestingly, was also near an if block about whether to use `checkpoint`. But removing the if block and checkpoint call has not fixed it. Interestingly it is also a block with a ""Shifted Window Multihead Self Attention"" call inside of it that is the last user code in the stack trace my end, though the impl is different than shown above (using mmdetection impl https://github.com/openmmlab/mmdetection/blob/main/mmdet/models/backbones/swin.py). I am on torch 2.3.1","I get  error instead when running the original repro. Seems to be related to AMP, but disabling it, I can run the script without error. Is there still a script to repro the index dtype issue?  /  ",I don't have the standalone ready for you but yes it is AMP. You can try to compile the `forward` at  https://github.com/yoxu515/aotbenchmark/blob/main/networks/encoders/swin/swin_transformer.pyL426,> You can try to compile the forward at https://github.com/yoxu515/aotbenchmark/blob/main/networks/encoders/swin/swin_transformer.pyL426 Do you mean using some random arguments to construct the module and call the compiled forward method should repro?,You could try but you know it could be not easy to rebuild all the params to correctly init the module. This is also the hard part and a lot of work required in our user ticket when you ask about minimal standalone repro. But probably you could decorate that forward with `torch.compile` and eventually try the e2e env at  CC(Pytorch 2.2 regression)issuecomment1924745100,"Also we had something similar related to AMP at  CC(AMP guards recompilation `dtype mismatch. expected Half, actual Float`)",Chatted with  about this. Can you run your command that repro this issue with TORCH_TRACE=somefolder and share us the content of that folder. We may be able to get some easier repro from TORCH_TRACE result.  ,It Is 5 months old. I was more than ready for 1 or 2 months to run this with all the flavours you want but I don't know if I have the currently the infra free to rerun this. In any case I have shared exactly the lines (with the Dockerfile) to run this training job with this code.,"We have verified that the original repro (at one point dynamic shapes, then amp) has been fixed with https://github.com/pytorch/pytorch/pull/138624.  , happy to look into other failures. please use a different issue  the fx_graph_runnable, or joint graph from TORCH_COMPILE_DEBUG / TORCH_TRACE are the easiest way to both collect and work on a repro.","> please use a different issue  the fx_graph_runnable, or joint graph from TORCH_COMPILE_DEBUG / TORCH_TRACE are the easiest way to both collect and work on a repro. Please add these best practices in the issue compiler template on Github as it is a pain to dissect a repro especially after many months when you have turned a lot of pages/activities in the meantime.",Thank you for the feedback  will do !
transformer,FSDP Doesn't Work with model.generate()," üêõ Describe the bug I am trying to use FSDP, but for some reason there is an error when I do model.generate(). MWE below  Error below   Versions  ",2024-04-12T18:31:56Z,triaged module: fsdp,open,0,6,https://github.com/pytorch/pytorch/issues/123962,"FSDP relies on `nn.Module.forward()` to run allgather, so it will not know to allgather parameters for a non`forward` method like `generate()`. There are some known workarounds like calling `FSDP.summon_full_params(model, recurse=False)` to make sure to run the root module's allgather before inside `generate()` the submodules' `forward()` are called.","Ah I see, many thanks for the pointer! As a quick followup question, is FSDP designed to work with model.generate() like I use here (during model training)? I have heard that sample_during_eval should be disabled when using an FSDPTrainer because it becomes very slow, so I would assume the same thing applies here as well? And if that is indeed the case are there any frameworks to use for these GANstyle training schemes (generate samples during training rather than supervised learning)?","I am not sure why generation during evaluation would make things ""very slow""i f generation is effectively a forward pass. If your workload is communication bound (FSDP's allgather/reducescatter are not fullyoverlapped and are instead on the critical path), then running a generation step may just mean incurring this kind of communication boundedness an extra time per overall ""iteration"". I may need to learn more about what the constraint is causing the slowdown.","""Warning: Sampling may be very slow for FSDPTrainer"", source. The authors definitely seem to have observed this problem, and they are avoiding sampling at all costs when using the FSDPTrainer, so I would love to know what is causing the slowdown as well!"," Interestingly, the issue that they linked for FSDP ( CC(Issue with FSDP + HuggingFace generate)) is related to erroring, not slowdown, so I am not sure. Given what people are saying on the other issue about tensor parallelism, I suspect that if you have slow network bandwidth, then parallelisms like FSDP and TP might be much worse than the pipeline parallelism that they seem to otherwise support (which only requires peertopeer send/recv of activations). For setups that, for example, have NVLink, then FSDP should be fine to use for generation IMHO. Though, if the generation is using little forward compute (e.g. generating just one batch element), then TP might be more suitable, but it might be realistic to switch parallelisms for the same model.","Related: * https://github.com/huggingface/transformers/issues/30228issuecomment2350022762 > So after a lot longer than I would like to admit, I have uncovered all the gotchas of using `generate` with FSDP. >  > 1. As I mentioned above, `torch.distributed.fsdp.FullyShardedDataParallel(use_orig_params=True)` is required when instantiating your FSDP instance. Otherwise, you get the error `The tensor has a nonzero number of elements, but its data is not allocated yet`. It seems likely that `generate` is calling a `torch.compile`wrapped function. > 2. Calls to `generate` must be inside a `torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params(fsdp_model)` context, otherwise you get the error `'weight' must be 2D` from the input embeddings still being flattened by FSDP. > 3. Lastly, and most trickily, you must use `generate(synced_gpus=True)` when using differentlysized data across ranks. Otherwise, the different ranks will pause on different synchronization points leading to a deadlock. >  > Here is a minimum reproducible example to show these off: >  >  >  > **fsdp_generate.py** > "
transformer,`import torch._dynamo` appears to accidentally have a runtime dep on jax/xla (and other things)," üêõ Describe the bug We discovered this incidentally because when Jax is imported, it imports xla, which adds a process level hook that issues a warning if os.fork is used (which is is in some inductor internals). This sent us down the rabbit hole of seeing how this came to be Callstacks: https://gist.github.com/ScottTodd/981f1c8696887e0f9aef7fa57adbe84b Minimal repro showing that `import transformers` with the listed packages installed causes this. We should avoid triggering this in pytorch and also see if we can excise that probing from transformers. What is triggering this in torch (with a note it can be removed once a now closed issue is resolved): https://github.com/pytorch/pytorch/blob/main/torch/onnx/_internal/fx/patcher.pyL7 I think there is a laundry list of things that could be improved here but one thing that is causing the badness (in the onnx._internal.fx.patcher). Some other things that looked suspect while I was poking: * (torch) The PSA/warning that Jax does about os.fork is not wrong. I haven't looked into it deeply but it seems like it is in some inductor caching layer, and it may be worth doing that a different way. * (torch) If we need to be doing version probes of installed things, it is extremely costly to do this by way of `import foo`. Some of these deps are *very* heavy and make non trivial changes to the environment as part of their bootstrapping. * (transformers) Same as above but more pointed. This style of code is extremely costly and not what I would expect if doing `import transformers` https://github.com/huggingface/transformers/blob/main/src/transformers/utils/generic.pyL31 (will file this in transformers)",2024-04-12T17:07:59Z,high priority module: onnx triaged oncall: pt2 module: dynamo,closed,0,10,https://github.com/pytorch/pytorch/issues/123954,Related: https://github.com/google/jax/pull/18989discussion_r1562799747,"If we really need to do package installed probing, we should be using `find_spec`. Some notes here: https://stackoverflow.com/questions/1051254/checkifpythonpackageisinstalled Then for something like the onnx patcher, if that *really* needs to happen, I think it needs to be done only when using the onnx exporter and can probably just check `sys.modules` at that point to see if `transformers` has been *loaded* at runtime.","Some commentary at the point that is doing the fork: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codecache.pyL2780 I think this might be good to revisit how this is done (I would have expected that such backend compiler workers could be spawned and should have been very lightweight things that did not themselves pull in the world). However, the expectation set on what the state of the world is on doing this fork isn't quite right: the intention is that this get done very early in the process lifetime, but the sequencing is such that it is happening quite late (in this case, even after we've probed external deps, causing third party backends to start a bunch of threads, etc). I expect this mechanism needs a rethink.","see also  CC([`torch.compile`] When PT2 compile threads are forked after user code spawns threads, gets stuck, unbounded RAM usage) for more discussion on why the fork subprocess thing is terrible, and also why it is not so easy to get rid of","> see also CC([`torch.compile`] When PT2 compile threads are forked after user code spawns threads, gets stuck, unbounded RAM usage) for more discussion on why the fork subprocess thing is terrible, and also why it is not so easy to get rid of I believe it. Been stung by this basic issue many times. It's super easy to start using multiprocessing like that and it tends to often end badly and then is hard to back out without triggering other fallout.","FYI  in the above linked issues, the transformers devs fixed their bug where they unconditionally import jax. While this fixes the worst of it, I still think that under the principle of paying for what you use, the onnx exporter should definitely not be importing transformers as part of the main import of torch dynamo. It would be best to have no cross dependencies like that unless if the things in question are actually used (and only then if absolutely required).",as per conversation with  adding it to 2.3.1 milestone,"Follow up: when you call onnx exporter, it still imports. Is this desirable?",This is to confirm that transformers hasn't been loaded in 2.3.1.  The list of loaded module includes: ,"> Follow up: when you call onnx exporter, it still imports. Is this desirable? Not sure who you were talking to. I don't regularly invoke the onnx exporter and don't have an opinion. Kind of seems broken to have the dependencies run this way, though."
transformer,Dynamo unsupported: dynamic padding,"Dynamo raises data dependent value error if the pad size is computed by unbacked values (not sure if I used the term correctly), the issue still persists after using `torch._constrain_as_size`. However, if the pad size is computed from a tensor dim size then dynamo works. The is one of the blocking issue from CC(ONNX export fails for aten::full_like op when exporting UDOP model from transformers), where the pad size is likely unbacked. The pad size can be either positive or negative, maybe this is posing some difficulty during graph capturing. The eventual fx graph should look like the one in example 2.  Example 1: tensor.item() as pad size fails   Example 2: tensor.size(0) as pad size works   Experiment 3: tensor.item() + torch._constrain_as_size() as pad size fails  ",2024-04-11T17:26:16Z,triaged oncall: pt2 module: dynamic shapes module: dynamo,open,1,9,https://github.com/pytorch/pytorch/issues/123855,"Huh, I didn't realize we support negative pad. Does it crop in that case? If you can force the padding to be positive only, then an appropriate `torch._check(size_item > 0)` would get you past the conditional. Otherwise, I need to understand if this decomposition can be rewritten to be branchfree:  Maybe it's possible, using a `sym_min` or something?","Tried rewriting decomp with `sym_max` and `sym_min`, hit      Could not guard on datadependent expression Ne(32*u0  32*Max(0, u0) + 512, 32*Min(0, u0) + 512) during     prims.copy_to(c_output, c_input) The expression should always evaluate to False though.",,"Yeah, our Min/Max reasoning is kind of weak.  So in this case, we'd be able to reason it out by doing a case split (consider u0 > 0, consider u0 < 0), and then the individual items are statically known.  This doesn't seem too complicated to implement, though I don't know if there's potential of exponential explosion if there's a lot of min/max. .","It might also be enlightening to figure out why we are doing this equality test in the first place, maybe it isn't necessary if the inputs are contiguous or something.","If this is a common pattern, we can implement the rule `op(min(a, b), max(a, b)) = op(a, b)` whenever `op` is commutative for `+` or in general. On a side note, I think more and more that we should use just use Z3 to help us evaluate expressions, rather than implementing this... lessthangood algebra system ourselves.","There's https://github.com/pytorch/pytorch/pull/123661 which changes the decomposition for constant pad. It would then require unsafe_masked_index to support dynamic shapes (which is doable in theory, but hard to achieve with all the guard_size_oblivious scattered around the codebase)","What do you mean by ""support dynamic shapes""? Do you mean ""unbacked symints""? Also, if anything the `guard_size_oblivious` should help with the unbacked symint support. Can you elaborate?","Sorry, meant data dependent values. Using that PR and some more changes, I'm at    "
transformer,Add rotary_embedding CPU operation,"**Summary**: Introduces a new rotary_embedding operation to PyTorch's CPU backend. This operation applies rotary positional embeddings to query and key tensors in attention.  The implementation supports 4D query and key tensors, and uses 2D cosine and sine embeddings for rotation. **Motivation**: Rotary positional embeddings have been shown to significantly improve the performance of transformer models by providing a more effective means of encoding positional information compared to traditional positional embeddings. This operation aims to provide PyTorch users with an efficient, native CPU implementation of RoPE, facilitating development in LLMs(NLP) and beyond. **Technical Details**: The operation is implemented as a custom ATen operation in C++ for CPU tensors. Input validation checks (using TORCH_CHECK) ensure tensors are of expected dimensions. **API Changes**: Introduces a new function signature to the torch namespace:   q, k: 4D Tensors representing queries and keys. (batch, n_heads, seq_len, head_dim // partial_rotary_factor)  cos, sin: 2D Tensors for cosine and sine rotational embeddings. (max_seq_len, n_head)  position_ids: 1D Tensor indicating the position IDs for embeddings. (seq_len)  unsqueeze_dim: Integer specifying the dimension for unsqueezing. TODO:  add tests  update documentation  benchmarking with native impl/python impl  extend to CUDA support Acknowledgments: Thanks to the PyTorch community and contributors for feedback and suggestions during the development of this feature. ",2024-04-10T17:18:11Z,triaged open source Stale module: python frontend,closed,1,3,https://github.com/pytorch/pytorch/issues/123738,:white_check_mark:login: makaveli10 / (e3a915ac32ae9c904292433c6f58e8044f7d8dee)The committers listed above are authorized under a signed CLA.,"Hey , is there an accompanying issue that this PR addresses/would you be willing to open one otherwise? For reference, issues are a good place to host community discussion for a new feature request. Which would help us determine whether this should be added to PyTorch","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,TransformerDecoderLayer fails when on cuda and batch dimension is zero (on cpu is fine)," üêõ Describe the bug The TransformerDecoderLayer craches when on cuda and batch dimension is zero. On CPU, it works as expected This issue has been discussed before ( CC(torch.nn modules should accept 0batch dim tensors.)) and a PR has been merged ( CC(Allow TransformerEncoder and TransformerDecoder to accept 0dim batch sized tensors.)), but I think the error persists on cuda.   Versions PyTorch version: 2.2.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.12.2  (main, Feb 27 2024, 17:35:02) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.14.336257.562.amzn2.x86_64x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 535.129.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz CPU family:                         6 Model:                              85 Thread(s) per core:                 2 Cor",2024-04-09T14:43:52Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/123642,The problem seems to be related to the `torch._native_multi_head_attention`
transformer,torch.explain gives non-deterministic results on the same model with the same input," üêõ Describe the bug When running `torch._dynamo.explain` multiple times on the same model with the same input, different number of graphs and graph ops are returned. I would expect this to be the same in every run, but maybe I overlooked something in the docs. If this is expected behaviour, this nondeterminism is causing issues for the custom backend I am working on. A flag to keep it consistent would be great to have.  There's also other warnings in the error log, that I so far ignored, but maybe are the reason for the nondeterminism.  Error logs  I pasted the complete log: There are ignored exceptions, I haven't digged yet into where they are coming from.  The issue that I am focused on:   Minified repro   Versions  ",2024-04-09T10:12:39Z,triage review oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/123636,"After some more debugging to check if maybe the model isn't deterministic: When using `model.eval()`, `torch._dynamo.explain` always gives the same result. I misinterpreted `torch.inference_mode` to also do `eval`, but apparently it doesn't. I don't think there's a bug then, I'll close it. ","Hi, I'm has the same problem when I use `dynamo.explain` to get `GraphModule`. Do you have some idea to solve the AttributeError?"
transformer,Support FP16 accumulation for faster LLM inference on 4090 like GPUs," üöÄ The feature, motivation and pitch  Background Many existing Large Language Models (LLMs) utilize FP16 during inference to improve performance. Downstream inference libraries, such as vllm, rely on fundamental operators in PyTorch, like `F.Linear`. Presently, PyTorch only supports two levels of FP16 GEMM, controlled by `allow_fp16_reduced_precision_reduction`. Both levels use FP32 as the computation accumulation data type.  Opportunity Commodity NVIDIA RTX GPUs, such as the 3090 and 4090, show double throughput when doing FP16 GEMM with FP16 accumulation compared to FP32 accumulation. By benchmarking following simple perplexity test script, it can be demonstrated that using FP16 accumulation results in a 40% endtoend speedup on 4090, with a minimal perplexity increase (0.0006) in LLM serving scenarios.    Proposed Feature Introduce a new API named `torch.set_float16_matmul_precision` to offer three levels of precision: 1. ""highest"": FP16 matmul uses FP32 as all the intermediate accumulation mode, which may lead to performance regression. This aligns with setting `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction` to `False`. 2. ""high"" (default): FP16 matmul uses FP32 as the computation accumulation mode. This aligns with setting `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction` to `True`, which is also the current default behavior. 3. ""medium"": FP16 matmul uses FP16 as the computation accumulation mode. This feature enhancement aims to provide users with more control over the precision levels during FP16 matmul operations, catering to needs in inference scenarios  Alternatives _No response_  Additional",2024-04-08T13:29:38Z,module: performance triaged module: half,open,1,5,https://github.com/pytorch/pytorch/issues/123558,"As a note, I've seen a number of folks ask for this. It apparently makes a significant difference in perf for consumer cards.",From triage review: we should do it if we don't do it already.,This is related to  CC([RFC] Set Float32 Precision for CONV/RNN) ,"Thanks for the review! I've made the necessary changes to cudaBLAS.cpp locally to support this feature. I'm eager to contribute and submit a PR for PyTorch. Regarding CC([RFC] Set Float32 Precision for CONV/RNN), it seems the precisionrelated frontend APIs are still under discussion. Should I introduce the `torch.set_float16_matmul_precision` API in my PR now or wait for the conclusion in CC([RFC] Set Float32 Precision for CONV/RNN) before proceeding?"," was this ever added? if so I have been doing so much wrong for the past few months. If not, pretty please add it :) !"
transformer,Support integer parameters in FullyShardedDataParallel," üöÄ The feature, motivation and pitch Libraries like Transformers, vllm and diffusers use large quantized LLMs for inference and finetuning. When running large models on several lowmemory GPUs, people often use FullyShardedDataParallel to split the model between those GPUs. Problem is, quantized models have integer parameters (e.g. uint8, int32, int64) and FSDP currently does not support integer data types. __Feature request:__ please support samedtype integer FlatParameters in FSDP. Currently, it is blocked by these two lines https://github.com/pytorch/pytorch/blob/b6201a60c57f79d081ecde66e1ce70e6614a3052/torch/distributed/fsdp/_flat_param.pyL767L768  **Edit:** I originally believed that you can remove these two lines and the code would *always* run perfectly, whereas in actuality, it causes errors in some configurations. We discuss possible workarounds below.  Alternatives bitsandbytes, a popular quantization library behind transformers & others, solves this problem as follows: https://github.com/TimDettmers/bitsandbytes/pull/970 . Their solution is to store integers as floating point tensors and reinterpret them as floats inside custom cuda kernels. However, this is still fairly difficult to use. My specific use case is for AQLM, where there is an integer parameter QuantizedWeight.codes. What I ended up doing is __simply removing these two lines, after which FSDP worked normally with integeronly modules.__ On the other hand, if you are planning to support automatically grouping FlatParameters by dtype and concatenating them together, this will also resolve this issue.  Additional context If this makes things easier, I'd be happy to cre",2024-04-07T18:11:24Z,oncall: distributed triaged,open,7,7,https://github.com/pytorch/pytorch/issues/123528, ,Removing this floatingpoint check sounds good.,"For my reference, do you have any simple/minimal examples of integer modules that we might be able to use for unit testing?","  We have an example using AQLM's QuantizedLinear but it is not *that* minimal. Currently, we use a dirty workaround by saving int32 data as float32 buffers. A simpler alternative would be to use `bitsandbytes`'s Linear4bit with the default int8 storage, but it may be too large for, say, automated tests; ( https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/nn/modules.pyL341 ) Finally, I can write a simple purepytorch example of a quantized linear layer with uint8 weights (saved as a separate module) that does not require any dependencies. Which one would you prefer?",The purepytorch example of a quantized linear layer would be much appreciated!,"   Example code  Here's an minimalistic example of a purepytorch code that imitates finetuning a quantized model with LoRA adapters (based on a popular LLM use case) Code: https://gist.github.com/justheuristic/575b053be82493e8151c775762796360 To run: `torchrun  nproc_per_node=2 demo_fsdp_lora.py` Currently, it fails   Click to see full traceback  !image  **However, it seems this check is there for a reason.** I tried simply removing the two lines that check for integer dtypes, and it only works in some cases. I got it to work on a multihost training with one GPU per host, but it fails on another server with 2x A100. I am yet to isolate out the reason why it fails. Would appreciate any tips / suggestions.  One possible solution One can easily circumvent this issue by swapping the storage type on integer parameters: Code: https://gist.github.com/justheuristic/8cc3c4df692ffa9354190c916e6ec643  ( **diff with previous gist** ) To run: `torchrun  nproc_per_node=2 fsdp_lora_solution.py` This seems to work in all setups i tried. Currently, the only way to get FSDP to work with integer parameters is to manually wrap them with IntParams (from the gist above) in the user code. Perhaps it would be possible to let users keep their integer params as is and wrap them internally, inside FullyShardedDataParallel? What do you think?","  Hi! Do you see any good solutions to integer parameters within FullyShardedDataParallel? It is, of course, possible to keep using the workaround above on the user side, but it has several problems: 1. whenever you cast the model to a different dtype (e.g. model.float()), it will irreversibly break any quantized params because pytorch thinks they are float64 2. one must wrap quantized weights as a separate module, typically rewriting the quantized model code for each library 3. direct storage manipulation may cause issues with JIT / compile and is incompatible with some backends (e.g. XLA)  If one were to, for instance, wrap integer params internally before allgathering inside FullyShardedDataParallel, it would eliminate these problems, but i am not sure if this has any other ramifications with other components. What would you suggest?"
transformer,[dtensor] run transformer sdpa in dtensor,"  CC([dtensor] run transformer sdpa in dtensor)  CC([dtensor] add op support for memory efficient attention)  CC([dtensor] improve new factory strategy) Now that efficient attention is supported in dtensor, we can modify the transformer test to use dtensor in SDPA and get rid of the manual num_head adjustments. Caveat: Efficient attention is supported only with bf16/fp32 (not fp64) and has other constraints. If any of the constraints are not satisfied, the SDPA would fall back to the math decomposed attention, which will break as it does not fully work with dtensor (it creates a `torch.Tensor` mask in the middle). I considered adding some checks like in P1202254918 but that needs to be added everywhere this Transformer is used. Is it necessary if the current CI machines can run efficient attention? Test files containing this Transformer:  `test/distributed/tensor/parallel/test_tp_examples.py`  `test/distributed/_composable/fsdp/test_fully_shard_training.py`  `test/distributed/_composable/fsdp/test_fully_shard_clip_grad_norm_.py` ",2024-03-29T22:14:20Z,oncall: distributed Merged ciflow/trunk release notes: distributed (dtensor),closed,0,2,https://github.com/pytorch/pytorch/issues/122997, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[dtensor] add op support for memory efficient attention,"  CC([dtensor] run transformer sdpa in dtensor)  CC([dtensor] add op support for memory efficient attention)  CC([dtensor] improve new factory strategy) This is a followup to flash attention. On cuda, flash attention is supported only for fp16/bf16, whereas memory efficient attention is supported for fp32 (but not fp64). With this PR, one can run SDPA and in general Transformer completely in dtensor. ",2024-03-29T22:14:14Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,2,https://github.com/pytorch/pytorch/issues/122996, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[dtensor] improve new factory strategy,"  CC([dtensor] run transformer sdpa in dtensor)  CC([dtensor] add op support for memory efficient attention)  CC([dtensor] improve new factory strategy) Previously, the new tensor out of the ""new factory"" all become replicated. With this PR, if the new tensor has the same shape as the old tensor **and** the shape can be evenly sharded, then the old spec is inherited and preferred. To accommodate this when the old tensor has sharded placements, the input args for local computation (size, stride) need to be adjusted. ",2024-03-29T22:14:09Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,2,https://github.com/pytorch/pytorch/issues/122995, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Transformer Engine Checkpointing Broken on Torch 2.3," üêõ Describe the bug We've been seeing a strange checkpointing bug when using torch 2.3 and Transformer Engine.  Concretely, in `get_model_state_dict()` for torch 2.3 it seems that we aren't ignoring the `_extra_state` properly when saving checkpoints. If I'm reading the distributed FSDP checkpoint code correctly, it seems to be trying to map the keys in the state_dict into module attributes. This will work for everything that we save with `module.register_parameter(...)` or `module.register_buffer(...)`, but it does not account for the fact that` _extra_state` is not intended to be a registered attribute per PyTorch's own model saving workflow.  Composer + torch 2.3 checkpoint code path ‚ùå  https://github.com/mosaicml/composer/blob/99b86dd7b2ae4ec35c4d84c5d9431577d113180f/composer/core/state.pyL871L889  Composer + torch 2.1 checkpoint code path ‚úÖ  https://github.com/mosaicml/composer/blob/99b86dd7b2ae4ec35c4d84c5d9431577d113180f/composer/core/state.pyL891L896 Error log attached in message.  Error logs checkpoint_error_log.txt  Minified repro  Create simple FSDP Linear model but use transformer engine for linear layers  try to save FSDP checkpoint with `get_model_state_dict()` in torch 2.3.   Versions Torch 2.3  ",2024-03-29T05:52:36Z,triaged module: distributed_checkpoint,closed,0,19,https://github.com/pytorch/pytorch/issues/122946,cc:   ,"Hey, . Based on my understanding, `_extra_state` is included in `state_dict` based on the documentation. https://pytorch.org/docs/stable/generated/torch.nn.Module.html  So it would be included as part of state_dict and thereby gets saved during checkpoint. Just want to confirm if this aligns with your expectation. ","Yes that is in line with our expectation. Unfortunately, in practice, we see the error above where we are not able to get the `_extra_state` attribute successfully from the torch Linear layer. ",cc:  from Transformer Engine,"Hi   `_extra_state` is indeed part of the `state_dict` for our custom Transformer Engine modules, but it is not an attribute of the modules themselves. We're following the `torch.nn.Module` specs in the documentation and returning `_extra_state` via `get_extra_state()`. The FSDP checkpoint appears to incorrectly assume that `_extra_state` is a module attribute. If this is now expected in any `torch.nn.Module` implementation, we can update ours to match but the documentation for `torch.nn.Module` should also be updated to highlight this change. Otherwise though, FSDP checkpoint should exempt `_extra_state` from the attribute check. ",cc.  ," I'm a little confused. This is the statement from the document: `Implement this and a corresponding set_extra_state() for your module if you need to store extra state. This function is called when building the module‚Äôs state_dict().`  So the `extra_state` will be included in the `state_dict() call and `get_model_state_dict()` will also call `state_dict()`. In such a case, isn't it legit `get_model_state_dict()` to return the extra state?","  TE modules follow the `torch.nn.Module` documentation and correctly implement `get_extra_state()` to return a pickleable `_extra_state` (as well as the matching `set_extra_state()` to load the same object back into the module). The issue is that PyTorch's distributed checkpointing is trying to directly access `_extra_state` as a module attribute (i.e. `getattr(module, '_extra_state')`) instead of going through the `get_extra_state()` call.  To be more specific, `_get_model_state_dict(model, ...)` calls `_get_fqn(model, key)` on every key in the model's state dictionary, and `_get_fqn(model, key)` eventually fails at `getattr(cur_obj, cur_obj_name)` when `cur_obj_name == '_extra_state'`. Ultimately, `_get_model_state_dict(...)` is making an assumption here that every key in the state dictionary corresponds to a model attribute (like registered parameters and buffers). This is not true for `_extra_state` and the specs for `torch.nn.Module` do not specify any such requirement. If PyTorch now expects `_extra_state` to be a module attribute, this change needs to be documented in the `torch.nn.Module` specs. We can then modify the TE modules to comply with this. Otherwise, the `_get_fqn()` needs a new conditional check on `cur_obj_name == '_extra_state'` in order to use `cur_obj = cur_obj.get_extra_state()` to recover the extra states instead of `cur_obj = getattr(cur_obj, cur_obj_name)`.",Let me understand the question. So the issue you pointed out is not about the `_extra_state` should be or not be in the returned `state_dict`. The issue is about the implementation of `get_model_state_dict` should not directly access the the `_extra_state` as these states are not supposed to be the module attributes. Is my understanding correct?," Yes, that's right. We need clarification from PyTorch on whether this is intended behavior or a bug, so we can upstream a fix on our end if necessary.",I'll have a PR to fix this issue., curious if we can fix will land in for torch 2.3.1? Composer team is planning on doing a release around this for our FP8 support in Q2. ,The timeline looks possible. Can you try https://github.com/pytorch/pytorch/pull/125336 to see if that PR resolves your issue?," I looked at the above PR, but that does not seem to resolve the incompatibility between mosaic composer library's required torch and cuda versions versus llama v3's preferred ones: If I run 2.1.x+cu118 then I can get llamav3 to work nicely with transformers, but composer breaks, and if I run 2.3.0+cu121 then I can get composer to run but llamav3 won't play nice anymore.  Given that composer is working on the newest stable release of torch it seems like something that needs to be resolved from the llama side. Thoughts?","No, this is nothing to do with llama. If the model uses the extra_state in a way that torch allows, it is PTD's API should fix the issue.  can you verify if the PR fix the issue? We could only cherrypick the PR if it actually fix the issue. I can only verify through the unittest.","Hey , is this fix in PyTorch nightly?  For what it's worth, we tried to incorporate your fix by rebuilding the pytorch nightly image: `mosaicml/pytorch:2.4.0_cu121nightly20240512python3.11ubuntu20.04` but our transformer script still fails with the same error:   YAML:  ","Apologies, looking at the stack trace it seems that we monkey patch the `_get_fqn` function in `mosaic_fsdp_utils.py`.  Testing the fix there. ", we have validated that this PR fixes the TE ckpting extra state issue once monkeypatched into our composer `mosaic_fsdp_utils` . Composer monkeypatch PR here. Appreciate your work on helping us fix this issue!  has started the cherrypick for this commit into torch2.3.1 at  CC([v2.3.1] Release Tracker)issuecomment2118261677. ,Close the issue because it is fixed by https://github.com/pytorch/pytorch/pull/125336
transformer,Cannot export DETR model with torch.export," üêõ Describe the bug Unable to export the DETR model encoder with torch.export  Gives the error:  Please find in the following gist the corresponding `dynamo.explain` outputs for the same model (DETR encoder): https://gist.github.com/corehalt/dbb031cd7403bb9fb9faa4bda025adf7 For reference, other parts of the model, as the backbone for example, succeeded:   Versions Collecting environment information...                                                                                                                                                          PyTorch version: 2.2.1+cpu                                                                                                                                                                     Is debug build: False                                                                                                                                                                          CUDA used to build PyTorch: None                                                                                                                                                               ROCM used to build PyTorch: N/A                                                                                                                                                                OS: Ubuntu 22.04.3 LTS (x86_64)                                                                                                                                                                GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0                                                                                                       ",2024-03-29T02:39:03Z,export-triaged oncall: export,open,0,2,https://github.com/pytorch/pytorch/issues/122934,"Given that the model comes from Hugging Face, I wanted to highlight that there is an ongoing effort for allowing Optimum library to export the models using Dynamo: https://github.com/huggingface/optimum/pull/1712 Looking forward for `torch.export` being used everywhere. Great feature, thank you.", could you take a look?
transformer,ONNX export fails for aten::full_like op when exporting UDOP model from transformers," üêõ Describe the bug When attempting to export the UDOP model to ONNX from the transformers library, the torch.onnx.export() command fails with a RuntimeError. Below is a minimal example to reproduce this error:  This example produces the following error message:   Versions Collecting environment information... PyTorch version: 2.2.2+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.28.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.01072awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A10G Nvidia driver version: 520.61.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          16 Online CPU(s) list:             015 Thread(",2024-03-28T15:34:55Z,module: onnx triaged onnx-triaged module: dynamic shapes,open,0,9,https://github.com/pytorch/pytorch/issues/122898,"Hi marsh , can you also try with our new api, `torch.onnx.dynamo_export` and reaching out to see if that helps your export process?","Hi , thank you for looking into this. I updated the example above to use  instead of . Here is the updated code:   When running this, I encounter a number of errors. Below is the full traceback:  I have attached the report_dynamo_export.sarif file here for review (the .sarif file extension is not supported so I uploaded it in a ZIP file).  report_dynamo_export.zip I have not used  in the past so please let me know if any of these errors are down to my usage. ","Hi marsh , Thanks for the quick response. The issue is now triaged and you'll get support soon. If there are any deadlines or associated high priority, please let me know and I will get it dealt with accordingly :)  ","I ran into the exact same problem as well. Thanks to marsh for the clean reproducible example, saves me the same work. So, looking forward to the solution :)",The following script likely captures the underlying issue:  The quickest solution is to rewrite that part of model code inside huggingface/transformers like in this snippet. .,"Thanks ! I attempted this change but it did not seem to resolve the issue. I did however manage to track the issue discussed here to line 336 of transformers/models/udop/modeling_udop.py:  This issue can be corrected by replacing this line with:  However, after making that change, there is a new issue with the following traceback:  I have not been able to track down the cause of this new error so any help here would be much appreciated.  Since the issue seems to be in the UDOP model implementation within transformers, I have additionally created a topic on the HuggingFace discussion platform which can be found at: https://discuss.huggingface.co/t/exportingudoptoonnxfails/80741",marsh oh I should clarify my reply above is towards the error during `torch.onnx.dynamo_export`. ,", sorry that is my mistake. I modified the offending line as you suggested in the model code and attempted an export using  and the error did change. The new error is as follows:   It appears that this again is an issue in the model code, so I once again modified the model code to remove the python if statement and the error changed once again. Here is the new error:  It is not clear to me what is causing this issue so I am not sure how to proceed from here. Thanks!","Thanks, I found there are more issues uncovered from exporting this model. Namely the vision patcher part of code is creating some trouble during dynamo tracing. We will try to track those individual issues separately."
transformer,Incorrect model configuration (possible dynamic axes issue) after torch.onnx.export," üêõ Describe the bug I have the following model:  The model trained with the following parameters:  And properly works as Pytorch model. However, when I export it to onnx:  and try to check the model output:  here is the output I get: > torch.Size([3, 222, 13]) > torch.Size([3]) > inputs > input_lenghts > logits > 20240328 10:26:00.4106034 [E:onnxruntime:, sequential_executor.cc:514 onnxruntime::ExecuteKernel] Nonzero status code returned while running Reshape node. Name:'/conformer_/conformer_layers.0/self_attn/Reshape_4' Status Message: D:\bld\onnxruntime_1710148767998\work\onnxruntime\core\providers\cpu\tensor\reshape_helper.h:45 onnxruntime::ReshapeHelper::ReshapeHelper input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{222,3,512}, requested shape:{1,24,64} >  > Traceback (most recent call last): >   File ""..\ptconformeronnxtestoutput.py"", line 68, in  >     result = session.run([label_name], { input_name: ortvalue, input2_name: ortvalue2 }) >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ >   File ""..\Conda\envs\torch\Lib\sitepackages\onnxruntime\capi\onnxruntime_inference_collection.py"", line 220, in run >     return self._sess.run(output_names, input_feed, run_options) >            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ > onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Nonzero status code returned while running Reshape node. Name:'/conformer_/conformer_layers.0/self_attn/Reshape_4' Status Message: D:\bld\onnxruntime_1710148767998\work\onnxruntime\core\providers",2024-03-28T04:35:28Z,module: onnx triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/122868,Closing as CC(torchaudio.models.Conformer incorrectly converted to onnx (only one of the axes becomes dynamic)) desribing the same issue using just the library Conformer model
transformer,[dynamo][pt2d] avoid skipping modules from torch/testing/_internal,"  CC([FSDP2][PT2D] enable torch.compile for compute in CI)  CC([dynamo] avoid creating module proxy for fsdp modules)  CC([dynamo][pt2d] avoid skipping modules from torch/testing/_internal) Dynamo skips user defined modules from `torch/testing/_internal` (eg MLP, Transformer). This PR adds `torch/testing/_internal/...` to `manual_torch_name_rule_map`. It ensures FSDP CI + torch.compile are meaningfully tested unit test shows frame count = 0 before and frame count > 0 after  some FSDP unit tests actually start to compile modules with this change. add trition availability check or disable tests for now ",2024-03-28T00:32:45Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp) module: dynamo ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/122851, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 4 jobs have failed, first few of them are: .github/workflows/generatedlinuxbinarylibtorchcxx11abimain.yml, trunk, .github/workflows/generatedlinuxbinarylibtorchprecxx11main.yml, .github/workflows/generatedlinuxbinarymanywheelmain.yml Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 4 jobs have failed, first few of them are: linuxbinarylibtorchcxx11abi, .github/workflows/trunk.yml, .github/workflows/generatedlinuxbinarylibtorchprecxx11main.yml, linuxbinarymanywheel Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Enabling `swap_tensors` path by default for all tensor subclasses errors on some transformer tests," üêõ Describe the bug The specific issue this tracks is https://github.com/pytorch/pytorch/pull/122755discussion_r1541541639  We see that the reference count of layernorm's weight is 3 in this test. However, the swap_tensors test for `LayerNorm`/`TransformerEncoder`does not catch this. Worth investigating whether this is due to the way the test is written or an inherent problem with `TransformerEncoder`/`LayerNorm` and the `swap_tensors` path in `nn.Module._apply`   Versions main  ",2024-03-27T17:43:21Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/122803
transformer,update comment of test_invalid_last_dim_stride in test_transformers.py,Fixes CC(There is a comment error in test_transformers.py)  ,2024-03-26T02:21:06Z,open source Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/122679," merge f ""Just fixing comment, should not require CI"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Deprecate `torch.nn.MultiHeadAttention` and `torch.nn.Transformer`-related modules ," üöÄ The feature, motivation and pitch This issue is WIP and is a placeholder to track discussion around the deprecation of `torch.nn.MultiHeadAttention` and `torch.nn.Transformer`related `torch.nn` modules. We observe that users would like transformer blocks to be performant and customizable. While these layers may provide an ""out of the box"" solution, they are monolithic modules.  Further, we note componentwise tools such as `torch.nn.functional.scaled_dot_product_attention`  (SDPA), `torch.nn.attention.bias` and `torch.nested` have been introduced to address the issues of customizability, composability and performance.  While there have been efforts to improve the performance and customizability of the `Transformer`/`MHA` modules, such as the addition of new masks and ""fast paths"" in these modules gated by configuration checks, we note that there have also been several issues surrounding these modules. Examples of issues around these modules include  Confusion around various arguments         CC(Inconsistent usage of `attn_mask` and `is_causal` arguments)       Definition of `attn_mask` is flipped compared to SDPA/implementation in other frameworks https://github.com/pytorch/pytorch/pull/120668        CC(Detailed documentation with an example for the causal mask in nn.TransformerEncoder)  Performance related issues        CC(Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`)       CC(nn.TransformerEncoderLayer fastpath (BetterTransformer) is much slower with src_key_padding_mask)  Bugs related to the ""fast path""       CC(TransformerEncoderLayer forw",2024-03-26T00:01:57Z,module: nn triaged,open,4,1,https://github.com/pytorch/pytorch/issues/122660, 
transformer,[Regression PyTorch 2.1 & 2.2] `torch._transformer_encoder_layer_fwd` outputs on CUDA/NestedTensorCUDA backends significantly diverge from truth for torch>=2.1 (SDP math))," üêõ Describe the bug Hi, As reported in https://github.com/huggingface/optimum/issues/1769, I noticed that the outputs of `torch._transformer_encoder_layer_fwd` significantly diverge from the correct outputs for torch>=2.1. User reports degradated predictive performance due to this. In my case, I see a mean relative difference of **more than 2%** overall. Besides, **more than 2% of values have a mean relative difference of more than 5%**. Besides, out of a 16384 values output tensor, 78 values differ by more than 20%, which is huge and seem to impact the global outputs. Note that I checked only one layer, this may accumulate on several layers. Note: Given the user report of degradated performance, I am confident that PyTorch 2.0.2 is ""truth"", but I need to confirm that This occurs only on CUDA device, not CPU. Reproduction: * `wget https://huggingface.co/fxmarty/debug_transformer_encoder_layer_fwd/raw/main/debug_bt.zip` (see https://huggingface.co/fxmarty/debug_transformer_encoder_layer_fwd/tree/main, could not upload on github directly as too large) * unzip it * Create two conda envs with torch `2.2.1+cu118` and `2.0.1+cu118` Run on each conda environment  And then run  giving   Versions PyTorch 2.2.1 env:  PyTorch 2.0.1 env:  ",2024-03-25T16:57:02Z,high priority module: nn triaged module: nestedtensor,open,0,1,https://github.com/pytorch/pytorch/issues/122617,?
transformer,There is a comment error in test_transformers.py, üêõ Describe the bug https://github.com/pytorch/pytorch/blob/main/test/test_transformers.pyL1437 The comment does not match the name of test case     Versions torch version == main,2024-03-25T08:01:30Z,triaged actionable,closed,0,1,https://github.com/pytorch/pytorch/issues/122594,Feel free to send a PR removing this comment or updating it appropriately
transformer,expose transformer header in cmake and wheel,"expose transformer header in cmake and wheel, some utils functions are used in nested transformer development on IPEX side",2024-03-25T02:47:49Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,11,https://github.com/pytorch/pytorch/issues/122586, merge,  Could you pls try merge it again? thanks., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxfocalpy3_8clang9xla / test (xla, 1, 1, linux.12xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ",  rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `zhenyuan_expose_transformer_header` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout zhenyuan_expose_transformer_header && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"If we are doing something like that, it would be nice to put a warning, that those headers are subject to change "
transformer,DTensor: add ring attention for _scaled_dot_product_flash_attention,"Ring attention support for _scaled_dot_product_flash_attention with DTensor. This assumes the query and key/value are sharded along the sequence length dimension. See the tests for example usage with PT Transformer as well as direct usage with _scaled_dot_product_flash_attention.  Notable caveats * Numerical accuracy: The backwards pass doesn't match numerically with the nonchunked version but the forwards pass does. I assume this is due to accumulated errors. I've added a chunked version that uses autograd to verify that the distributed version matches the chunked version. * nn.Linear has incorrect behavior when running on a sharded tensor of size (bs, heads, seq_len, dim) with `Shard(2)` and does an unnecessary accumulate which requires `Replicate()` on QKV when using `nn.MultiHeadedAttention` to work around the issue. * If enabled, it forces sequence parallelism and doesn't interop with tensor parallelism.  SDPA usage   Transformer usage   Test plan  ",2024-03-22T01:21:05Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,8,https://github.com/pytorch/pytorch/issues/122460, using `sdpa_kernel` does work  that's what we do for the ContextParallel + Transformer pattern. I'm just using it directly in the sdpa tests so it's extra obvious, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxdocs / builddocspythonfalse Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,fix cudnn attention check,"For CUDNN attention, besides packed QKV layout with limited support of sequence length (seq_len <= 512) and head dim requirements. Also supporting a more generic ""arbitrary sequence length with flash attention"" as stated in `Transformer Engine`: https://github.com/NVIDIA/TransformerEngine/blob/8e672ff0758033c348e263dbcd6a4b3578c01161/transformer_engine/common/fused_attn/fused_attn.cppL126 More about ""fused flash attention"" in CUDNN graph api: https://docs.nvidia.com/deeplearning/cudnn/developer/graphapi.htmlfusedflashattentionfprop",2024-03-21T07:35:50Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,16,https://github.com/pytorch/pytorch/issues/122391,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: chengzeyi / name: C  (a11ea53348372b57a6ff5c5e1d08c4a56389a964, 6f6eacd60cbc63f784b2859ef46a5635b0e254e1, baeb9f7a85f4c6a202e1b72cfd245e64cce6eb80, ced3ba54902b1347e1ce82be879f448596bbabfe, c7183204a13a24e8834a3f52acb45bc03006c011, 0bcc5e08a3e94fc7a830e8a81d9deabf75c9989a, cfe47522934918544acc5538552b4f8df7e55d61, 8c4cdd7381e94e966bc55115d940b46d15841425, 5e1b27c09d7c8f83bf40f1b236011a4e7454e07a, cb5f9ba673201100161f096e5726354cdfedfd25, 6c625fd02ba72869769ac5547b6d0ba126d8ef0a)","> Thanks for the fixes! What GPUs did you test the checks on? I work with RTX 4090, cuda 12.4 and cudnn9, üòØall the newest stuff. And according to latest cudnn doc. this meets the tensor shape and stride requirements of ""arbitrary sequence fused flash attention"" rather than the quite limited ""max 512 sequence length fused attention with packed QKV"". However, after testing it with stable diffusion v1.5. I found that in fact cudnn's implementation of flash attention is a bit slower (or not faster, just no magic happens~) than the current flash attention v2 implementation in PyTorch, which is quite frustrating. https://docs.nvidia.com/deeplearning/cudnn/developer/graphapi.htmlfusedflashattentionfprop",CC   I think this is a good start that we can merge ahead of the testing PRplease fix the merge conflicts  , Already rebased. Note that the cudnn frontend has some extra checks about the runtime version of CUDNN. But it seems that there is currently no suitable ways to place `cudnnGetVersion()` in `sdp_utils.cpp` I expect this to work properly with the new `CUDA 12.4` depsÔºåas the CUDNN version can be updated to `8.9.7` or `9.0.0`,According to https://github.com/NVIDIA/cudnnfrontend/blob/1b0b5eac540b7f8fd19b18f1e6b8427c95503348/test/python_fe/test_mhas.pyL288 The following checks should be enough: ,Seems that the version of cudnn can be got from `at::detail::getCUDAHooks().versionCuDNN()`, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cuda11.8py3 / build Details for Dev Infra team Raised by workflow job ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch push f https://github.com/siliconflow/pytorch.git pull/122391/head:dev_cudnn_attention` returned nonzero exit code 128  This is likely because the author did not allow edits from maintainers on the PR or because the repo has additional permissions settings that mergebot does not qualify. Raised by https://github.com/pytorch/pytorch/actions/runs/9021489156, would you mind trying to rebase?,>  would you mind trying to rebase? rebased, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Batch size can't be varied if you export self attention to onnx," üêõ Describe the bug I've been making a lot of tickets about differentiating through things lately, but here's a much more basic one: if I export a self attention layer from a transformer to on onnx, without even running any backprop stuff, the onnx appears to be broken if I try to run it with different batch sizes and sequence lengths. Execute this code to reproduce the issue:  Here's the error I get:  I don't know if this works better with the dynamo export, but that's not currently an option for me as I can mostly differentiate through things with torch.onnx.export() now and it's completely unsupported in dynamo.  Versions PyTorch version: 2.2.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: version 3.25.1 Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr 5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: True CUDA runtime version: 10.0.130 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti Nvidia driver version: 536.23 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin\cudnn_ops_train64_8.dll HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=3501 DeviceID=CPU0 Family=107 L2CacheSize=16384 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=3501 Name=AMD Ryzen Threadripper PRO 3975WX 32Cores ProcessorType=3 Revision=12544 Versions of relevant librarie",2024-03-20T16:30:27Z,module: onnx module: windows triaged,open,1,4,https://github.com/pytorch/pytorch/issues/122321,"Update: looks like it's just the batch size I can't vary. Seems to be related to the reshape operations on q, k and v in the multi_head_attention_forward function (torch/nn/functional.py) so I narrowed the test down a bit",Actually I can juggle things around in torch/nn/functional.py and make this work  here are my diffs: ,"Hi maintainers, I am encountering an issue while inferencing with the exported ONNX SegFormer model from MMSegmentation. To export the model to ONNX, I used the `onnx_export` function from MMDeploy (`from mmdeploy.apis.onnx import export as onnx_export`). However, I am facing an error with the following layer: `backbone/layers.0.1.0/attn/attn/Reshape_4`, which uses `nn.MultiheadAttention` from PyTorch. Here is the error log: Error Log   Name: 'main//backbone/layers.0.1.0/attn/attn/Reshape_4'   Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:40 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, onnxruntime::TensorShapeVector&, bool) gsl::narrow_cast(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{187,1,32}, requested shape:{160,1,32}     I have referred to the following code segment in PyTorch: https://github.com/pytorch/pytorch/blob/54f27b886e9dccebb2ebd07300c309bba3b3b30c/torch/nn/functional.pyL6129L6154 and I have added the revision as mentioned in  CC(Batch size can't be varied if you export self attention to onnx)issuecomment2010340209   Prior to writing this issue, I have attempted several methods to resolve it, particularly focusing on exporting the `nn.MultiheadAttention` module: 1. **Scripting the module and then exporting to ONNX** (`torch.jit.script` > `torch.onnx.export`):      Resulted in `RuntimeError: ScalarType UNKNOWN_SCALAR is an unexpected tensor scalar type` 2. **Torch Dynamo Export**:      Resulted in the error `Currently does not support nonfunction and super` Could you please provide guidance on how to resolve this issue or suggest alternative methods for correctly exporting the model? and will your team be considering update the codes for the `nn.MultiheadAttention` module. Thank you for your assistance. ","Relevant Issues: CC(Failing to convert torchaudio.models.Conformer to onnx after scripting of it via torch.jit.script) CC(when convert to onnx with dynamix_axis,  the Reshape op  value is always the same as static,  dynamic_axis is useless, it cant't inference right shape dynamically)"
transformer,RuntimeError: Creating a new Tensor subclass FakeTensor  && AssertionError: Mixing fake modes NYI," üêõ Describe the bug As shown below  Error logs First type:  Second type:   Minified repro ~~~python import torch from transformers import LlamaForCausalLM model = LlamaForCausalLM.from_pretrained('metallama/Llama27b',                                          torch_dtype=torch.float16,                                          use_flash_attention_2 = True,                                          device_map = 'cuda') optim_model = torch.compile(model, dynamic=True, backend='cudagraphs') def test_ktimes(model, times, bs=4, mx=512):     x = torch.randint(0, 32000, (bs, mx), device='cuda')     for _ in range(times):         y = model(x, labels=x)     return None  trigger first type with torch.no_grad():     optim_model.eval()     print(test_ktimes(optim_model, 1, bs=16, mx=512))  trigger second type x = torch.randint(0, 32000, (16, 512), device='cuda') y = optim_model(x, labels=x) ~~~  Versions PyTorch version: 2.1.2+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.2 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0164genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A40 GPU 1: NVIDIA A40 GPU 2: NVIDIA A40 GPU 3: NVIDIA A40 Nvidia driver version: 525.105.17 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True C",2024-03-20T14:55:20Z,triage review oncall: pt2 module: pt2-dispatcher,closed,0,6,https://github.com/pytorch/pytorch/issues/122314,"note for investigators, you need to have access to https://huggingface.co/metallama/Llama27b","cudagraphs backend does not support dynamic shapes. Use inductor with mode=""reduceoverhead""","I can't run the repro on my box, run out of memory",Another Minified Repo ,This repro is still using cudagraphs backend...,"With the latest repro, when I run it successfully prints None "
transformer,Handle transposes in second batch of matrices in bmm,1. Add support for Unranked placeholders in the MPS backend.  2. PR is now fusing the Transposes into the GEMM kernel dispatches in MPS backend. This improves the performance of Transformer networks by 58%.,2024-03-19T14:25:26Z,triaged open source Merged Reverted release notes: mps ciflow/mps,closed,1,17,https://github.com/pytorch/pytorch/issues/122194,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: malfet / name: Nikita Shulga  (eabaecc4d8674ae89c22e1228df836eedb88f0dc, 2e79d284b15ab191947c89b8b0dafbcc2e56211e, 0e0f0bddb1f813469dcaa1dc1ec4f06a75a2293c, 6d0a6fe44a4794c2ed50814073f77b2e9fc5d368):white_check_mark: login: kulinseth / name: Kulin Seth  (4497328e50ba182740cf557ac704c4947c4f53cc, 1c1c2fdf2a9c69b76e9221c994fb1574159a2ff2)"," merge f ""All MPS tests are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",">  merge f ""All MPS tests are green"" Please be more careful next time, as this PR broke lint."," revert m ""Broke lint"" c ignoredsignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `dev/joona/bmm_fusion` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout dev/joona/bmm_fusion && git pull rebase`)","> Can you please provide a bit more detailed description of what this PR is trying to do. Is it fixing correctness issue or a performance one? This improves Batched matrix multiplications where the 2nd matrix is transposed. E.g consider following example:   With current code, a gather would be performed to make following `y.permute(0, 2, 1)` contiguous, then a batched matrix multiplication would be performed (where the matmul is performed `row @ column`).  With this addition,  we detect if the 2nd matrix was transposed, and if this was the case, we will not do gather anymore to make the 2nd matrix contiguous, but rather do directly a matrix multiplication where 2nd matrix is marked as transposed in MPSGraph. The big speedup comes that instead of doing a `row @ column` matmul, now it's a `row @ row` matmul (the big speedup in `row @ row` comes from the fact that the access to the 2nd matrix is not strided anymore (in trying to access the column), but rather contiguous)  this speedup will be more obvious when doing big matrix multiplications where the stride between column elements is bigger.", rebase , started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `dev/joona/bmm_fusion` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout dev/joona/bmm_fusion && git pull rebase`)"," merge f ""Lint and MPS are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",> PR is now fusing the Transposes into the GEMM kernel dispatches in MPS backend. This improves the performance of Transformer networks by 58%.  do you mind sharing a link to a benchmark that you've run?  I've used https://github.com/pytorchlabs/gptfast and alas didn't notice any perf gains before and after the change
transformer,[dort][dynamo_export] Wrong model for phi backward graph," üêõ Describe the bug The script ends with a failure in onnxruntime: ``onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: getitem_1 for the following indices``. It happens in the backward graph.   Versions Collecting environment information... PyTorch version: 2.3.0.dev20240314+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.26.4 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.10.102.1microsoftstandardWSL2x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1060 Nvidia driver version: 535.98 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):    ",2024-03-19T09:27:32Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/122181
transformer,"NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":830, please report a bug to PyTorch. "," üêõ Describe the bug **Python sample code:**  **error message:** > RuntimeError                              Traceback (most recent call last) >  > Cell In[1], line 4 >       1 from ultralytics.data.annotator import auto_annotate > > 4 auto_annotate( >       5 data='/app/input/signal/images', >       6 det_model='/app/runs/detect/train/weights/best.pt', >       7 sam_model='mobile_sam.pt', >       8 device=""cuda"", >       9 output_dir='/app/input/signal/labels', >      10 ) >  > File /usr/local/lib/python3.10/distpackages/ultralytics/data/annotator.py:41, in auto_annotate(data, det_model, sam_model, device, output_dir) >      39 if len(class_ids): >      40     boxes = result.boxes.xyxy   Boxes object for bbox outputs > > 41     sam_results = sam_model(result.orig_img, bboxes=boxes, verbose=False, save=False, device=device) >      42     segments = sam_results[0].masks.xyn   noqa >      44     with open(f'{str(Path(output_dir) / Path(result.path).stem)}.txt', 'w') as f: >  > File /usr/local/lib/python3.10/distpackages/ultralytics/models/sam/model.py:35, in SAM.__call__(self, source, stream, bboxes, points, labels, **kwargs) >      33 def __call__(self, source=None, stream=False, bboxes=None, points=None, labels=None, **kwargs): >      34     """"""Calls the 'predict' function with given arguments to perform object detection."""""" > > 35     return self.predict(source, stream, bboxes, points, labels, **kwargs) >  > File /usr/local/lib/python3.10/distpackages/ultralytics/models/sam/model.py:31, in SAM.predict(self, source, stream, bboxes, points, labels, **kwargs) >      29 kwargs.update(overrides) >      30 prompts = dict(bboxes=bboxes, points=",2024-03-18T02:24:12Z,module: cuda triaged module: CUDACachingAllocator,open,0,3,https://github.com/pytorch/pytorch/issues/122068,same issues 2xA5000 nvcc V: 12.1 torch                         2.2.2 Python 3.10.0,same issue with latest released pytorch,"Same Problem with:  NVIDIA A100 80GB NVIDIASMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4   torch: '2.3.0+cu121' pyhton: 3.11.6 Basically I had several notebooks opened with sam model loaded in them, so different kernels for each one in the same GPU.  I easilly solved the issue by simply opening and using one at a time. "
transformer,[MPS] Regression from macOS 14.3 to 14.4 in PyTorch 2.2.0/2.2.1," üêõ Describe the bug I've been using a pytorch model daily through transformers (https://github.com/khawhite/mangaocr) and MPS. Everything was fine with latest PyTorch and latest Transformers until the Sonoma 14.4 update which made it start crashing on startup (I was running 14.3 before and am on a M1 Max mac, for reference). Since CPU mode worked fine, I also tried older versions of PyTorch and I found anything before 2.2.0 worked fine. Bisecting it I found this commit is causing it: https://github.com/pytorch/pytorch/commit/056d6247c786f4391bc6a6f21337b009c7e9699e . I tried building latest main with that commit reverted and it's working again. I apologize but I've not been able to pinpoint this further or to write a standalone example (since I don't really know what I'm doing here). I've looked at what's happening from within transformers but not sure any of this can be of help: It seems in https://github.com/huggingface/transformers/blob/main/src/transformers/generation/beam_search.pyL318 in input_ids the first member of each tensor array differs with or without that commit: e.g. With: tensor([[3463342888,          2,       2312,       4080,       5063,       4885,               3600,       5764,       5554,       4798,       1074,       3386,               2865,       2166,       1524,       1858,       2872,       4014,               5250,       1225,       2119,       1104,       1682,       5730,               1482,       5578,         78],        [3463342888,          2,       2312,       4080,       5063,       4885,               3600,       5764,       5554,       4798,       1074,       3386,               2865,       2166,   ",2024-03-16T01:30:50Z,high priority triaged module: regression module: correctness (silent) module: mps,closed,0,7,https://github.com/pytorch/pytorch/issues/122016,"Sorry, can you please be a bit more specific of what do you mean by ""crashes at the startup""? I'm on 14.4 and everything seems to be working fine at the first glance, i.e.:  Hmm, though indeed `from manga_ocr import MangaOcr` seems to work on 14.3, but fails on 14.4","Ok, I think I have a minimal reproducer of the problem ","And following fixes the problem, which makes me wonder how this ever worked before: "," if you don't mind explaining, what does copy_and_sync and non_blocking do vs copy? Just for learning thanks! found this: https://github.com/pytorch/pytorch/blob/5030913d6aed153afe45c9d9bcb6145093eb5629/aten/src/ATen/mps/MPSStream.mmL193L207"," Looking for a good vendorneutral document on the subject but could not find one. In general, nonblocking operations, as name suggests are nonblocking, i.e. operation might still be ongoing when one returns to a function calls (see `glFlush` API description dating back to 1991 ) In practice, imagine one is running the following code:  vs  I.e. by default GPU operations are executed asynchronously( so timing it on CPU doesn't make much sense, as it will only memory time required to reserve memory and enqueue respective commands)","Thanks for the breakdown  !  Do you have an idea of why   previously had the result of [0, 3, 3]? Seem like in any asynchronous scenario, there would be at most one 3. How'd it get two 3s? ",validated with 2.3.0 
transformer,[DCP][FSDP2][Test] Add_adamW to test_train_parity_2d_transformer_checkpoint_resume,"Want to add the option of AdamW here, as currently this is the only test for 2D.  ",2024-03-15T22:55:33Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/periodic,closed,0,8,https://github.com/pytorch/pytorch/issues/122002, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxjammypy3.8gcc11 / build  .github/workflows/pull.yml / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 5, 5, linux.g5.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `add_adamW_to_test_train_parity_2d_transformer_checkpoint_resume` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout add_adamW_to_test_train_parity_2d_transformer_checkpoint_resume && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[ONNX] ONNX model has mismatching data type when Torch model and checkpoint dtype mismatches," üêõ Describe the bug The HF's Whisper model has checkpoints saved with `torch.float16`, but the user script casts it to `torch.float32`. After the export finishes, the checkpoint is used to cerate ONNX Weights, which will result in a model expecting `float32` while the weights are stored with `float16`  The checker error would be something like `onnx.onnx_cpp2py_export.shape_inference.InferenceError: [TypeInferenceError] Inferred elem type differs from existing elem type: (FLOAT16) vs (FLOAT)`  Versions main",2024-03-15T19:46:41Z,module: onnx triaged onnx-triaged release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/121986
transformer,AdamW(fused=True) slower than unfused AdamW," üêõ Describe the bug 512M parameters Mostly vanilla LM transformer. FlashAttention 2.4.2, PyTorch 2.2.0. Uses both FA and FlashRotary. Dtype: bf16 Nvidia A40. singleGPU Unfused: 85 TFLOPS Fused: 68 TFLOPS  Versions PyTorch version: 2.2.0 Is debug build: False CUDA used to build PyTorch: 12.2 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.28.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.19.17coreweavex86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A40 Nvidia driver version: 525.147.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.6 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.6 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          96 Online CPU(s) list:             095 Vendor ID:                       AuthenticAMD Model name:                      AMD EPYC 7413 24Core Proce",2024-03-13T22:09:35Z,module: performance module: optimizer triaged,open,0,21,https://github.com/pytorch/pytorch/issues/121857,"hmm this is concerning. Do you mind sharing a description of the sizes of your parameters? and whether you were using AMP/needing a gradscaler? I would guess no since you‚Äôre using bf16 and not fp16, but wanted to rule out possible culprits","n_layer: 12 n_head: 12 kv_heads: 6 (GQA) hidden_dim: 1536 n_tokens: 2048 (context length) vocab_dim: 65536 activation: ""swiglu"" No AMP/gradscaler. If a profile would help, I can produce one.",a profile would be helpful yes!,"These `trace.json` files were gigantic (multiGB), so here's smaller versions without stack information and on two steps only: https://drive.google.com/file/d/1c2CQST_U_Qf6O1qgSXr0DKVorytoNgp5/view?usp=sharing https://drive.google.com/file/d/1NCi6frLbXfVhL0pzdmwoRzmYLtFKqeQ/view?usp=sharing Torch.compile mode is the default (not ""reduceoverhead"") and my TFLOPS are constant starting from the second step, so skipping just 1 step of warmup seems ok. The program is being launched by `torchrun` on a 1GPU machine.",Turns out the step count isn't an issue with stack information disabled. Here's 3 steps: Unfused: https://drive.google.com/file/d/1OKuJVC3PPK5vn6TWdi1vPD8bAoHP7h4/view?usp=sharing Fused: https://drive.google.com/file/d/1CUeKkwiOMEHRJaNZlC65utq6WuS7Fxh4/view?usp=sharing,"Oh! Wait a moment...are you comparing torch.compile(step) vs step of AdamW(fused=True)? torch.compile(step) does the vertical fusion by default so it being faster is reasonable (and the point haha)! If you're comparing eager (non compiled) step between (fused=True) and the default (), then the fused should be faster.","No: torch.compile is wrapping forward and loss, but backward and AdamW are outside the compile. So it's: torch.compile(forward+loss but not backward+AdamW) + unfused AdamW vs  torch.compile(forward+loss but not backward+AdamW) + fused AdamW ","Ah, thanks for that, I was looking for the optimizer stuff in the torch compiled region and getting confused. I did look at both the traces and you're right the fused kernel is slower. In detail: The default AdamW uses a series of foreach ops, and the part in the trace this corresponds to is:  Whereas the fused AdamW is a foreach_add (step update) followed by one single fused kernel, and the part in the trace this corresponds to is:  : 1. While the fused implementation correctly launches fewer kernels (one fused_adamw instead of a bunch of foreach_* kernels), each kernel is so much slower! Each fused_adamw kernel takes ~6.5ms whereas a single foreach_* kernel is only 0.2ms (30 times slower!) Why is the fused op so much slower? 2. Whereas there are 137 kernels launched per foreach op, there are 75 kernels launched for the fused (due to the multi_tensor_apply chunking logic). For the same number of tensor inputs, I'm surprised there are _more_ kernels launched in the smaller foreach ops. Do you know what accounts for this?","pytorch v2.2.0 doesn't seem to have CC(fused adam(w): Reduce register usage), 2.2.1, either by looking at https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/cuda/fused_adam_utils.cuh. So I guess a nightly would be worth trying",Instructions for installing nightly can be found in the toggle system of https://pytorch.org/,"My CUDA version is 12.2, and PyTorch only lists 12.1 in its nightlies. Is that still ok for me to install?","Yes, I think so","It broke when trying to run with PyTorch Nightly, mismatched CUDA versions.  Debugging this isn't your issue though. Updating from CW's stable to CoreWeave's nightly images a few days ago (http://ghcr.io/coreweave/mlcontainers/nightlytorchextras:999655bnccl2024.03.11.05cuda12.2.2ubuntu22.04nccl2.19.31torch2.3.0a0vision0.18.0a0audio2.2.0a0flash_attn2.4.2) produced NaNs in backward, but that also isn't your issue. (I also can't switch my container easily.) It seems I don't have a good way to install PyTorch nightly on my current system.","Here's PyTorch nightly profiles (CUDA 12.2, 2.3.0a0+3eb322f). torch.compile broke, so I turned it off for forward+loss. Performance is much closer; unfused is only a little bit faster than fused. https://drive.google.com/file/d/1k8zoSGeK7Pr5jst_MU4u6HJS6lhikUcl/view?usp=sharing https://drive.google.com/file/d/1weyCOEAnnte0rJ45qFEvNR1UYpYmTE0/view?usp=sharing","Thank you for getting the traces! It is surprising that the fused step is still slower than the foreach...but a look at the trace shows that we only get 67% occupancy per fused adam kernel and 100% occupancy for each foreach op kernel.  why could this be the case? Unfused, 100% occupancy  Fused, 67% occupancy  The fused kernel does use almost twice as many registers, but it might just be due to it doing more work, but it shouldn't be doing more work than the sum of the foreach ops. btw 's occupancy changes (with dynamic chunking) will help make both impls faster.","As a side note, I tried `.compile` on PyTorch's unfused AdamW, after you mentioned it. TFLOPS goes from 85 to 4, haha. It's because `torch.compile` keeps recompiling when the LR changes (which is inevitable with a learning rate scheduler).","Haha thank you for gathering the number there (though it's abysmal :() Is this true even when you wrap lr into a tensor? like torch.tensor(lr)?  There is a tracking issue for lr:  CC(Using torch.compile, batch training step time takes long to converge when adding a LR Scheduler) that is in progress  ","Using `AdamW(lr=torch.tensor(...))` with scheduler active, TFLOPS went from 85 to 88. Zoom zoom! LR scheduler is LambdaLR with regular nontensor floats.","icy œáŒ±œÑŒ≥ŒπœÅŒª:  I can reproduce the performance dropoff under torch 2.2 icy œáŒ±œÑŒ≥ŒπœÅŒª: compiling only forward + loss, enabling the fused optimizer completely destroys the performance icy œáŒ±œÑŒ≥ŒπœÅŒª: but on 2.4 nightly, enabling fused helps a tiny bit"," just a heads up there was a regression in fused optimizers in the past few days. https://github.com/pytorch/pytorch/pull/123566 should fix it. > we only get 67% occupancy per fused adam kernel and 100% occupancy for each foreach op kernel  the foreach version materializes intermediate results into global memory at every step. The fused version only materializes the final results, which requires more registers per thread and can lead to lower occupancy. I think it's normal for the fused version to be faster while having lower occupancy, because it avoids doing a lot of expensive memory I/O.","PyTorch nightly: fused AdamW 69.5 TFLOPS unfused AdamW 70.5 TFLOPS One of the two fused AdamW runs has a wobbly TFLOPS line, going up and down around 69.5.  "
transformer,The warning message in aten/src/ATen/native/transformers/sdp_utils_cpp.h::check_for_attn_mask is a bit confusing.," üêõ Describe the bug The warning message at https://github.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/transformers/sdp_utils_cpp.hL258, 'Both fused kernels do not support nonnull attn_mask,' does it mean that FlashAttention and MemEfficient do not support  nonnull attn_mask? However, at::_scaled_dot_product_efficient_attention does support  nonnull attn_mask, so should the warning message be updated?    Versions ",2024-03-13T02:54:53Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/121783,"Ahh you are right, I will send a PR Updating"
transformer,Does PyTorch need to expose the header files in the aten/src/ATen/native/transformers directory to the client?," üöÄ The feature, motivation and pitch Now, I am adapting at::scaled_dot_product_attention to a specific type of cudalike device and encounters a problem.   Firstly, since aten/src/ATen/native/transformers/sdp_utils_cpp.h is not exposed to client, I need to write a lot of repeated checking code in my select_sdp_backend Secondly, since aten/src/ATen/native/transformers/attention.h is not exposed to client, I need to write the following code when registering _fused_sdp_choice_stub  The above issues arose in the context of planning to support PrivateUse1 at https://github.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/transformers/attention.cppL674. Following the approach at  CC(How to adapt to `at::scaled_dot_product_attention`'s routing logic for a third-party cuda-like device?) would avoid these issues, but it requires a lot of repetitive code.      Alternatives _No response_  Additional context _No response_",2024-03-12T11:20:09Z,triaged module: PrivateUse1,closed,0,2,https://github.com/pytorch/pytorch/issues/121720,"From Nikita: short term, exposing headers like this seems ok. Longer term, there should be a BE project to more clearly decide what we want to expose / not expose.",https://github.com/pytorch/pytorch/pull/122586
transformer,[DTensor] Moved `Transformer` sharding to staticmethod,"  CC([no ci][WIP] Supported 2D `clip_grad_norm_`)  CC([DTensor] Moved `Transformer` sharding to staticmethod)  CC([FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh)  CC([FSDP2] Relaxed check for parent mesh) To support FSDP + TP/SP unit tests, let us factor out the canonical TP/SP sharding of `Transformer` to a staticmethod that can be called by other unit tests. Test Plan:  ",2024-03-11T19:33:38Z,oncall: distributed triaged Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/121660, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 5, 5, linux.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","Failure in pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 5, 5, linux.4xlarge.nvidia.gpu) looks unrelated: ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 5, 5, linux.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,guard on tensor parallelism test examples,This task is to add some guards to the tensor parallelism test examples to ensure we incurred the right collectives: https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.py Specifically we can follow this logic to assert on the communication collectives incurred using the CommDebugMode https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.pyL100 The test example we want to add this guard on is the Transformer e2e training example https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.pyL179 ,2024-03-11T18:21:34Z,oncall: distributed triaged pt_distributed_rampup,closed,0,0,https://github.com/pytorch/pytorch/issues/121649
transformer,Compiling SwinTransformer encoder fail with input size changes, üêõ Describe the bug Compiling SwinTransformer forward at https://github.com/yoxu515/aotbenchmark/blob/paot/networks/encoders/swin/swin_transformer.pyL684 It is going to work correctly at training time when it works on a fixed input crop. It is going to fail instead at inference when the input size change.  Error logs   Minified repro _No response_  Versions last `pytorchnightly` official docker image. ,2024-03-11T14:54:55Z,triaged ezyang's list oncall: pt2 module: dynamic shapes,closed,0,11,https://github.com/pytorch/pytorch/issues/121637,"Thanks for the report . Did you try diagnosing with the doc linked in the error message? At some point I can take a look, but the full model repro instructions would be useful.","If we follow the docs in the report and log is correct  we have  That  it seems to generate the `setitem`: `While executing %setitem : [num_users=0] = call_functiontarget=operator.setitem, slice(0, 7, None), slice(0, 7, None), slice(None, None, None)), 0), kwargs = {})` `h` and `w` could be variable at inference and they are constant in training. The docs is relative to `datadependent control flow` but here there is not an explicit control flow in the user code but probably it is in the underline generated code. > But the full model repro instructions would be useful. As this model is a good source of many compiler errors I have already shared some instruction in other tickets to setup the environment with the hope you could adopt this model more in general to stress test the compiler:  CC(Pytorch 2.2 regression)issuecomment1924745100 In this case we need to run the `eval.py` on different image sizes.",I think it may work to explicitly test with `torch._check` that the h and w are in bounds for `img_mask` before doing the index. Your simplified code looks relatively simple and can take a look later., Compiling other components of the same model I am getting this with the last nightly    As I have already different compiler open tickets on this model is there any opportunity that the compiler team could adopt it (as it is a public repo) to stress test the compiler?,/://github.com/pytorch/pytorch/issues/121504. Then we have also another open ticket same model at  CC(Pytorch 2.2 regression), Is it possible that something is missing cause how it handles slices? > I think it may work to explicitly test with torch._check that the h and w are in bounds for img_mask before doing the index. Your simplified code looks relatively simple and can take a look later.  Cause manually adding `torch._check` in the loop for W and H and transforming slices in range it is working. And then it is failing just few row later with the same bug for:  ,"Do the same trick but with the downsampled sizes. We should do better for this case, but it is difficult because when you do a slice that is out of bounds, the semantics of PyTorch is to clamp to the max value, and this is painful to generate code for (we'd much rather know that the slice range is never out of bounds). Maybe we should introduce a variant of indexing which assumes everything is in range (this will also remove annoyances with negative indices). I think I'd be happy to add this model to our overall suite. If you are interested in rolling up your sleeves to do it, open a PR to https://github.com/pytorch/benchmark and then once it's in (I can help), I can add it to our torchbenchmark benchmark runs.","In the meantime I have create a workaround to not use python slices (it would be nice if at least when we find slices we could emit a more comprehensible error). But then later in the same function  function `x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)`  ","> I think I'd be happy to add this model to our overall suite. If you are interested in rolling up your sleeves to do it, open a PR to https://github.com/pytorch/benchmark and then once it's in (I can help), I can add it to our torchbenchmark benchmark runs. As  is trying to reproduce the same environment to check other compiler issues at  CC(Custom attention recompilations) I have asked if he he is already familiar with the benchmark repository. I could help for sure but I don't think I have the bandwidth to bootstrap the PR on my side as I need to find the time to check how the benchmark standard interface are organized in the repo. But I can support for everything related to the model for sure and compiler failures reproducibility with pytorch nightly versions.","For this one, try patching the line in `torch/_prims_common/__init__.py` to `guard_size_oblivious(y != expected_stride)` Also, https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/editheading=h.44gwi83jepaj may be of interest",https://github.com/pytorch/pytorch/pull/122370 for the size oblivious here
transformer,"[BUG] Given boolean tgt_mask, TransformerDecoder produces wrong results with MPS backend"," üêõ Describe the bug Two tokens are decoded in this example. Ideally, the output feature on the first token should be the same regardless of the sequence length as a square subsequent mask is applied. Here are two ways to generate the tgt mask. One is from the official example where a float mask is generated first but converted into a boolean mask later. Another way is to generate the boolean mask directly. :heavy_check_mark: If running with `cpu` backend, the three printed values should be the same:   ‚ùå  However, if we run it on `mps` backend, the directly generated boolean mask would result in the wrong decoded feature for the first token. The result looked like this:  The bug can be reproduced by this minimal script:   Versions PyTorch version: 2.2.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: version 3.28.3 Libc version: N/A Python version: 3.11.7 (main, Dec 15 2023, 12:09:56) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M3 Max Versions of relevant libraries: [pip3] numpy==1.26.3 [pip3] pytorchlightning==2.2.0.post0 [pip3] torch==2.2.1 [pip3] torchaudio==2.2.1 [pip3] torchdata==0.7.1 [pip3] torchmetrics==1.3.0.post0 [pip3] torchtext==0.17.1 [pip3] torchvision==0.17.1 [conda] numpy            ",2024-03-11T13:16:51Z,module: nn triaged module: mps,open,0,1,https://github.com/pytorch/pytorch/issues/121632,I've tried device='cuda' or 'cpu'. There is nothing wrong. So it's might only linked to mps. !image
transformer,[inductor] Incorrect handle of `autocast` results in type mismatch," üêõ Describe the bug   when using inductor backend. I'm not able to create a single reproducer but I bisected to the triggering commit: https://github.com/huggingface/transformers/commit/20164cc2c67c07190893933e47c8f13dc1c1f1d7, it looks to me that the nested `autocast` is not being handled correctly. Full reproducer: The issue occurs when training llama with `transformers==4.38.2` and `torch>=2.2.0`, and can be reproduced with a (not very minimized) script as: `TORCHDYNAMO_DEBUG_FUNCTION='forward' TORCH_LOGS='+dynamo' torchrun train.py model=metallama/Llama27bhf cc=inductor`  Note that `cc=eager` works fine.  Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Alibaba Cloud Linux release 3 (Soaring Falcon)  (x86_64) GCC version: (GCC) 10.2.1 20200825 (Alibaba 10.2.13.5 2.32) Clang version: Could not collect CMake version: version 3.20.2 Libc version: glibc2.32 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.10.13416.1.al8.x86_64x86_64withglibc2.32 Is CUDA available: True CUDA runtime version: 12.3.103 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10 GPU 1: NVIDIA A10 Nvidia driver version: 535.146.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              64 Online CPU(s) list: 063 Thread(s) per core:  2 Core(s) per socket:  32 Socket(s):           1 NUMA nod",2024-03-11T12:41:41Z,triaged oncall: pt2 module: inductor,open,1,2,https://github.com/pytorch/pytorch/issues/121631,For some reason `print(query.dtype)` fixes the issue,"The print is likely helping because it is inducing a graph break, and now we are no longer hitting hte compiler bug as we are no longer compiling enough"
transformer,[DDP] Gradient Synchronization Failure Induced by model.gradient_checkpointing_enable() in multi-task setting," üêõ Describe the bug Hello, when I am using DDP to train a model, I found that using multitask loss and gradient checkpointing at the same time can lead to gradient synchronization failure between GPUs, which in turn causes the parameters of the model on different GPUs to be inconsistent.  Environment:  A minimal example:  When I comment out `'ddp_find_unused_parameters': False,` or the peft code, We got a RuntimeError. The error message seems to be related to this issue:  I‚Äôm unsure if this is a bug or if gradient checkpointing combined with DDP is not compatible with multitask learning. When I disable the gradient checkpointing, it seems to be working normally now. But I‚Äôm worried that it might not truly be functioning correctly. I want to understand what happens in the forward and backward processes when I either disable or do not disable gradient checkpointing, and why gradient checkpointing would cause a failure in gradient synchronization.  Versions Collecting environment information... PyTorch version: 2.2.0 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.17 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01160.88.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 GPU 3: NV",2024-03-10T10:37:28Z,oncall: distributed module: ddp,open,0,1,https://github.com/pytorch/pytorch/issues/121594,I wonder if this issue could be resolved by using `torch.utils.checkpoint.checkpoint` with `use_reentrant=True`.
transformer,"RuntimeError: invalid unordered_map<K, T> key"," üêõ Describe the bug When I export the onnx model and execute it to:  Throw exception:  When defining:   When I debug to the line:   self. sense is actually  bitsandbytes. nn. modules Linear4 bit (nn. Linear)   This exception is MatMul4Bit.apply(A, B, out, bias, quant_state) ,   in file torch.autograd.function.py.apply .  Pytorch version is 2.2.1(stable) .  Versions os:  win10 cuda: 11.8 pytorch:  2.2.1 (stable) ====other===== accelerate                0.27.2 aiofiles                  23.2.1 altair                    5.2.0 annotatedtypes           0.6.0 anyio                     4.3.0 APScheduler               3.10.4 attrs                     23.2.0 backports.zoneinfo        0.2.1 bitsandbytes              0.41.0 bitsandbyteswindows      0.37.5 blinker                   1.7.0 certifi                   2022.12.7 charsetnormalizer        2.1.1 click                     8.1.7 colorama                  0.4.6 coloredlogs               15.0.1 contourpy                 1.1.1 cpmkernels               1.0.11 cycler                    0.12.1 DBUtils                   3.0.3 etxmlfile                1.1.0 exceptiongroup            1.2.0 fastapi                   0.110.0 ffmpy                     0.3.2 filelock                  3.9.0 Flask                     3.0.1 FlaskAPScheduler         1.13.1 flatbuffers               24.3.7 fonttools                 4.49.0 fsspec                    2023.10.0 gradio                    4.20.1 gradio_client             0.11.0 h11                       0.14.0 httpcore                  1.0.4 httpx                     0.27.0 huggingfacehub           0.19.4 humanfriendly             10.0 idna                      3.4 im",2024-03-09T07:21:59Z,module: onnx triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/121572,"Hi  , are you using the `torch.onnx.dynamo_export` api to export? If so, can you please share that part of the code?",Closing as there is no response in the last 30 days
transformer,[RFC] PagedAttention Support," Feature request PagedAttention has been a mainstream optimization technology for generation task based on LLMs.  It has been supported by a lot of server engines, e.g., vllm, tensorrtllm and TGI. To get OOB performance with PyTorch+Huggingface, in this RFC, we plan to propose the PageAttention KV Cache design which use compatible semantic as the HF KV Cache.   Motivation KV cache is used to reduce computation for _**Decoder**_ layers, but it also brings memory overheads.  In general, the memory overheads are introduced by four parts.  1. Firstly, in the attention module, the past key/value token states should be concated with the current token to get the entire context which will introduce additional memory read/write.  2. Secondly, for the multiple sampling, e.g., beam search, the KV cache should be reordered according to latest beam index.  When the sequence is very long, the memory overheads of these two parts will be performance bottleneck.  3. Thirdly, for batching inputs, all sequence will be generally padded to the longest sequence in this batch. The native KV cache in the transformers uses contiguous memory buffer to store KV cache and the shape is (batch, head_num, past_seq_len, head_dim), it means that the memory consumption of the KV cache is decided by the longest sequence.  4. Lastly, for multiple sampling, there is only one prompts for different beams, and it can be shared across different beams, but the native implementation needs to store multiple copies of prompts tokens.  This will obviously waste a lot of memory and limits the max batch size for throughput mode.   To address the above limitations, this paper proposes ",2024-03-08T01:16:05Z,triaged module: sdpa,open,8,17,https://github.com/pytorch/pytorch/issues/121465, Chen  , ,"Hey thank you for proposing the RFC! After discussion with some other members of the PyTorch Core team we have a few points of feedback.  It seems that the `reshape_and_cache` function could instead be implemented using tensor subclasses. Although this is particular to Paged Attention, it seems that it could be made more generic and extended to represent arbitrary Logical to Physical mappings of Tensors.  To add kernels to PyTorch, a high bar is required to be met due to the build time and binary size implications. PyTorch Core is intended for very general things that can minimize maintenance burden. The world of deep learning moves very quickly, so outside repos like torchAO (https://github.com/pytorchlabs/ao) exist as places to store code for techniques that are useful to a broader audience but may undergo more rapid evolution. Given this, would it make more sense to add the paged attention kernels to torchAO instead of PyTorch Core? That would allow the technique to be available to users while providing more flexibility as it evolves. Let me know if you have any other questions or would like to discuss further. We appreciate you taking the time to submit this RFC!"," Thanks for the feedbacks. > * It seems that the `reshape_and_cache` function could instead be implemented using tensor subclasses. Although this is particular to Paged Attention, it seems that it could be made more generic and extended to represent arbitrary Logical to Physical mappings of Tensors. Abstracting the paged kv cache with tensor subclasses sounds a good idea. This can also simplify the op definition. > * To add kernels to PyTorch, a high bar is required to be met due to the build time and binary size implications. PyTorch Core is intended for very general things that can minimize maintenance burden. The world of deep learning moves very quickly, so outside repos like torchAO (https://github.com/pytorchlabs/ao) exist as places to store code for techniques that are useful to a broader audience but may undergo more rapid evolution. Given this, would it make more sense to add the paged attention kernels to torchAO instead of PyTorch Core? That would allow the technique to be available to users while providing more flexibility as it evolves. Fully understand the positioning for a general PyTorch core. I have several questions with the torchao path. 1. I noticed that the purpose of torchao is mainly for model compression (quantization and pruning) per its description: **The torchao repository contains api's and workflows for quantization and pruning gpu models.** But the paged attention optimization is general for LLMs with floating point inference too. In fact, its implementation is primarily with fp32/bf16/fp16 even for quantized models. Are you going to extend the scope of torchao to support other optimizations too? If so, how are you going to position torchao in the future? 2. Currently, torchao only contains python frontend code while what we want to propose is also about the native kernels. Would it be fine to incorporate native kernels to torchao too? 3. Is it an incubation process that we put the features outside PyTorch core first and then later they can be promoted to PyTorch core with more stability and maturity? Do you envision that things like PagedAttention can be added PyTorch core in the future?",For the questions about TorchAO I will defer to  ,Thanks . Any comments  ?,"  You're welcome to contribute to torchao, which is a repository for architecture optimization. Please see our RFC for our future plans: https://github.com/pytorchlabs/ao/issues/47 . We do also plan to ship native kernels. Stable and successful features can become prime candidates for upstreaming, if they are broadly applicable even beyond architecture optimization. Having said all of this, of course, it's not necessary for a feature to enter torchao first before it can enter core torch.",">   You're welcome to contribute to torchao, which is a repository for architecture optimization. Please see our RFC for our future plans: pytorchlabs/ao CC(Improve storage, tensor and module C error messages + fix for dl flags in nightly python) . We do also plan to ship native kernels. Stable and successful features can become prime candidates for upstreaming, if they are broadly applicable even beyond architecture optimization. Having said all of this, of course, it's not necessary for a feature to enter torchao first before it can enter core torch. Thanks  . It seems the project description of torchao needs to be updated. May I know the scope of ""architecture optimization""? Do you think PagedAttention support belongs to ""architecture optimization""? Also  ","  I agree that ""architecture optimization"" is a bit of a broad (and fluid) term, but I think the RFC on https://github.com/pytorchlabs/ao/issues/47 does a good job at outlining the scope. I do indeed see PagedAttention, as outlined in this issue, as a specialized storage layout for a Tensor cache geared towards decoding. And in particular it is for architectures that make use of the scaled dot product attention function. It seems plausible that it could be generalized further to a generic storage layout for more operators, but if I understand correctly that's not the plan in the near future. So it's an optimization of a specific (set) of architectures (attention based) and for a very specific application (decoding). We are obviously still building out torchao, so I'm eager to hear your take on this.","> I do indeed see PagedAttention, as outlined in this issue, as a specialized storage layout for a Tensor cache geared towards decoding. And in particular it is for architectures that make use of the scaled dot product attention function. It seems plausible that it could be generalized further to a generic storage layout for more operators, but if I understand correctly that's not the plan in the near future. So it's an optimization of a specific (set) of architectures (attention based) and for a very specific application (decoding). We are obviously still building out torchao, so I'm eager to hear your take on this. Thanks for clarifying the connection between PagedAttention and architecture optimization here. Yes, we indeed want to target LLM inference in the short term. I don't see any other application that can benefit from the new storage layout so far. But on the other hand, it is also worth considering to be incorporated into PyTorch upstream if an operation is important enough for key AI apps/workloads nowadays, e.g., LLM inference. The scaled dot product attention function is an example, which is also architecture specific but was added into PyTorch upstream.","  Yes, SDPA is also architecture specific, but it applies across domains (vision, speech, text and its combinations), it works for inference and training, across many devices and it works with various input tensor types. If PagedAttention eventually gets to this point too, then inclusion in torch itself makes more sense. But if we want to move fast in creating a version that works for LLMs, decoding and on CPU, then torchao is a potential place to put these kernels.","> But if we want to move fast in creating a version that works for LLMs, decoding and on CPU, then torchao is a potential place to put these kernels. Sure, I agree that we can start with torchao  we'd love to contribute to torchao. Note that PagedAttention is not specific for CPU while we (Intel) can contribute CPU kernels as a starting point and perhaps others from the community can contribute CUDA kernels too. Since torchao doesn't support native kernels yet, I guess we need more discussions and your support to incorporate native kernels into it and package them into the release binaries. > Yes, SDPA is also architecture specific, but it applies across domains (vision, speech, text and its combinations), it works for inference and training, across many devices and it works with various input tensor types. If PagedAttention eventually gets to this point too, then inclusion in torch itself makes more sense. Yes, SDPA certainly covers more usages of Transformerbased models, yet it's worth noting that decodingonly Transformer models are increasingly becoming the main area of research and deployment interest. Therefore, it's important to also consider the prominence of specific applications in determining a feature's eligibility for upstreaming.","  Yes, I'm more than happy to help support shipping this C++ kernel! We can setup an extension that allows you to add these. I'll work on this next and ping this issue once it's done.","  Also, another team you could consider talking to is https://github.com/facebookresearch/xformers  ",">   Also, another team you could consider talking to is https://github.com/facebookresearch/xformers . About the project positioning, do I understand correctly that xformers focuses more on the research while torchao is more about production? We are more interested in the product deployment in the context of PagedAttention.","xFormers has some support for PagedAttention now  but we are indeed focusing on research first, and on NVIDIA GPU acceleration almost exclusively, so it might not be the best place to put code to accelerate inference on CPUs...",  we plan to use tensor wrapper to implement the paged attention kv cache and have updated the design in this RFC. Pls help to review. 
transformer,[FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh,"  CC([DTensor] Moved `Transformer` sharding to staticmethod)  CC([FSDP2][BE] Refactored `check_1d_sharded_parity` to use mesh)  CC([FSDP2] Relaxed check for parent mesh) Eventually, we should just have one unified way to check for parity between a `DTensor`sharded model and a replicated model. This PR is a small refactor to work toward that. One current gap to use this `check_sharded_parity` function for 2D is that FSDP's `(Shard(0), Shard(0))` layout differs from that of the `DTensor` APIs since FSDP shards on dim0 after TP shards on dim0. ",2024-03-06T23:19:29Z,oncall: distributed Merged ciflow/trunk release notes: distributed (fsdp2),closed,0,5,https://github.com/pytorch/pytorch/issues/121357, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Problems differentiating through a transformer when exporting to onnx," üêõ Describe the bug I'm trying to export a model to onnx involving back propagation, and I've run into a number of issues. I can work around some of them, but this one seems a bit tough. You can see the kind of problem I've encountered if you run this python code:  This probably overlaps with  CC(Problems differentiating through certain operations when exporting to TorchScript) and  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace) as this is basically what I was trying to do when I found those bugs. There are some things I can work around (eg the backward pass of a gelu unit is unsupported but I can implement that myself using a torch.onnx api), but some things look a lot harder to work around. This particular export fails with an error that it's trying to insert a parameter as a constant when it requires a gradient. I've turned requires_grad off for all the model's parameters though, so I think it's erroneously trying to insert an intermediate value as a constant like it's doing in  CC(Problems differentiating through certain operations when exporting to TorchScript) (I found that bug while basically trying to strip this one down). Fixing that issue will probably reveal the layer norm problem I reported here  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace) , the fact that the backward pass for the Gelu nonlinearity isn't implemented (at least that's something I can work around) and probably some other stuff  Versions Collecting environment information... PyTorch version: 2.2.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM u",2024-03-05T23:08:10Z,module: onnx triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/121280,"Ok, I've actually managed to make this export now, with a hacked version of pytorch, a bit of torch.onnx.register_custom_op_symbolic() and overriding a couple of pytorch functions with manual python implementations. I actually haven't been able to confirm if my results were correct yet, as with randomly initialized parameters the gradients were very small, but I'll try and list all the relevant hacks I had to do here anyway: * Disabled the jit::tracer::setTracingState() stuff in tools/autograd/gen_trace_type.py for the ""matmul"" and ""linear"" operations, as if I don't do this they expand to untraced operations which cause problems in the backward pass (see the matmul case in this ticket  CC(Problems differentiating through certain operations when exporting to TorchScript)) * Hacked SavedVariable::unpack() in torch/csrc/autograd/saved_variable.cpp so it always returns the data_ member variable so the hash of the return value isn't consistent. Also saving a pointer to the original variable in the SavedVariable class, so I can use jit::tracer::setValueTrace() to tell the tracer that some of the saved output variables in the backward ops are actually the same as the original output variables (this is to fix the sqrt case in  CC(Problems differentiating through certain operations when exporting to TorchScript)) * Replaced the torch.layer_norm() function with a manual python implementation to avoid the issues mentioned here:  CC(Problem differentiating through a torch.layer_norm() call when exporting to torch.jit.trace) * Replaced torch.scaled_dot_product_attention with a manual python implementation to avoid this issue:  * Implemented symbolic op graphs for aten::select_backward, aten::gelu_backward and aten::_softmax_backward_data using torch.onnx.register_custom_op_symbolic()","Update: I do appear to have made this work because I can export a larger model that differentiates through its transformer components, and the resulting onnx produces the same outputs as the original pytorch model. I can't run the onnx with different input shapes though, so the code related to the dynamic_axes argument in torch.onnx.export() appears to be broken (if I comment out the backprop part in the model the exported onnx works fine). I'll see if I can get a simple repro for that...","Hi David, torch.onnx.export (and torch.ji in general) doesn't support backpropagation and we don't have plans to support it Try torch.compile(backend=""onnxrt"") instead!"
transformer,Issue with sentence_transformers pointing to torch tensors manipulations," üêõ Describe the bug I have the following code as the first lines in my main function:  after start the following happens: 20240304 20:14:50,252:INFO:Load pretrained SentenceTransformer: allMiniLML6v2 /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown warnings.warn('resource_tracker: There appear to be %d ' Process finished with exit code 139 (interrupted by signal 11: SIGSEGV) It used to work well, but after recent system upgrade fails. Hardware Overview: Model Name: MacBook Pro Model Identifier: Mac14,9 Model Number: MPHE3LL/A Chip: Apple M2 Pro Total Number of Cores: 10 (6 performance and 4 efficiency) Memory: 16 GB System Firmware Version: 10151.81.1 OS Loader Version: 10151.81.1 Activation Lock Status: Disabled Also consider system crash report: Translated Report (Full Report Below) Process: Python [2020] Path: /Library/Frameworks/Python.framework/Versions/3.11/Resources/Python.app/Contents/MacOS/Python Identifier: org.python.python Version: 3.11.6 (3.11.6) Code Type: ARM64 (Native) Parent Process: pycharm [756] Responsible: pycharm [756] User ID: 501 Date/Time: 20240304 20:20:38.0394 0800 OS Version: macOS 14.3.1 (23D60) Report Version: 12 Anonymous UUID: 66A06743DA8EFC87662FA0460A49AFF9   Versions Collecting environment information... PyTorch version: 2.2.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.3.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: Could not collect Libc v",2024-03-05T15:55:59Z,high priority needs reproduction module: crash triaged module: macos,open,0,4,https://github.com/pytorch/pytorch/issues/121235,Maybe try reinstalling pytorch after the system upgrade?,"I reinstalled pytorch several times, creating new python environment, it didn't help.","This looks like a racecondition where tensor data is freed even there it's still retained by IDE. Assigning to myself in an attempt to get a repro, as usually it has nothing to do with PyCharm, but rather with wrong expectation about object lifetime, for example see reproducer to a previous crash:  CC(torch.nonzero crashes if invoked from multiple threaded)issuecomment1713015452",Same problem on almost the same hardware (but on a MacBook Pro with M3 (2*6 cores and 18 GPU cores) with 36GB. Last message was:  **20240401 15:41:56  Load pretrained SentenceTransformer: allMiniLML6v2** Anything I can do about this?
transformer,Detailed documentation with an example for the causal mask in nn.TransformerEncoder," üìö The doc issue Currently there is not much detail present as to how causal_mark=True in nn.TransformerEncoder masks the attention scores. Does one need to also pass a masking array? what should be its shape given an input of (batch_size, seq_len, d_embed)? I am currently trying to build a nanoGPT using nn.TransformerEncoder and getting really confused with this.  Suggest a potential alternative/fix Please include a documentation with a small example.  ",2024-03-05T02:54:03Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/121193
transformer,MPS memory leak in training," üêõ Describe the bug When using transformers Trainer on MPS hardware, after several hundred iterations, getting ""MPS out of memory"". Test code and output at: https://gist.github.com/dmahurin/fe202511a1b3314faf248d728cdb1d71 Note that when training data is of varying data lengths, the condition seems to reproduce more. While training, MPS allocated memory seems unchanged, but MPS backend memory runs out.  Versions Collecting environment information... PyTorch version: 2.3.0.dev20240122 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.2.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: version 3.28.3 Libc version: N/A Python version: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang1500.1.0.2.5)] (64bit runtime) Python platform: macOS14.2.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Versions of relevant libraries: [pip3] mypy==1.7.1 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.23.5 [pip3] onnx==1.15.0 [pip3] onnxruntime==1.16.3 [pip3] opencliptorch==2.20.0 [pip3] pytorchlightning==1.9.4 [pip3] pytorchmemlab==0.3.0 [pip3] torch==2.3.0.dev20240122 [pip3] torchmlir==20240127.1096 [pip3] torchdiffeq==0.2.3 [pip3] torchmetrics==1.3.0.post0 [pip3] torchsde==0.2.6 [pip3] torchvision==0.18.0.dev20240122 [conda] Could not collect Also: [pip3] transformers==4.37.2 ",2024-03-04T05:13:43Z,high priority module: memory usage triaged module: mps,closed,0,4,https://github.com/pytorch/pytorch/issues/121113,"Note that if padding_option is set to 'max_length', the MPS gpu allocations go up, but it seems to avoid the MPS backend out of memory. So this is perhaps a nonoptimal workaround.","I'm seeing something very similar during classification with a bert based model. If I set the padding_option to 'max_length' there is no memory leak, but if not, the memory usage skyrockets quite quickly.","Hi, I haven't dug into too much details on this.. Do you know what `padding_option=max_length` do in the transformers package ? I am curious if there is dynamic slicing going on which is causing leak somewhere.","Hello, a fix for this issue will be available in a future update of MacOS"
transformer,UnsupportedOperatorError: Exporting the operator ::_transformer_encoder_layer_fwd to ONNX opset," üêõ Describe the bug when i try to export onnx model from fairseq, i encountered this error. plz help support this operator   Versions PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.27 Python version: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01062.12.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.55",2024-03-04T03:05:52Z,module: onnx triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/121109,torch.onnx.export doens't support this operator and we don't have plans to support it. Please try the new ONNX exporter and reopen this issue if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial
transformer,[ROCm] Add cublasGemmAlgo_t -> hipblasGemmAlgo_t,This PR is to add cublasGemmAlgo_t > hipblasGemmAlgo_t to cuda_to_hip_mappings.py. It is required for DeepSpeed transformer extension build on ROCm. ,2024-03-01T18:56:02Z,module: rocm open source Merged ciflow/trunk topic: not user facing ciflow/rocm,closed,0,14,https://github.com/pytorch/pytorch/issues/121030,need to update the xpass plz, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `add_hipblasGemmAlgo_t` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout add_hipblasGemmAlgo_t && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `add_hipblasGemmAlgo_t` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout add_hipblasGemmAlgo_t && git pull rebase`)"," This is a very minor change, so I'm issuing a merge command assuming CI jobs should pass fine.  merge"," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.8gcc11noops / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,OOM with mixed precision with OPT1.3B Decoder," üêõ Describe the bug I am doing simple forward pass on HF OPT1.3B model. With (1, 2048) input size, FP32 forward pass works, but same model same input with mixed precision throws OOM on 40GB A100. I tried the same on 48GB A6000 and same happens with (2, 1664)  input size. Furthermore, with (1, 1024) input size 100 forward passes  FP32 took 21 seconds using 23GB memory, fp16 4.74 seconds using 27GBs memory. Simple code to reproduce the issue.  But this works  Is it expected to see more memory usage with fp16 prediction? Thanks.  Versions Collecting environment information... PyTorch version: 2.2.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.4 Libc version: glibc2.35 Python version: 3.10.10  (main, Mar 24 2023, 20:08:06) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.087genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             40 Online CPU(s) list:                039 Vendor ID:                          GenuineIntel Model name:                         Intel(R) Xeon(R) CPU ",2024-03-01T14:04:06Z,triaged module: amp (automated mixed precision),open,0,0,https://github.com/pytorch/pytorch/issues/120994
transformer,FSDP: automatic instantiation on GPUs (aka zero.Init)," üöÄ The feature, motivation and pitch moving from slack discussion  How can FSDP shard a model directly to GPUs during model instantiation? This is critical for instantiating huge models.  Alternatives In Deepspeed this exists since day 0 of the framework using `zero.Init` This feature is integrated into HF Transformers' `from_pretrained` that instantiates the model directly on GPUs  here There is an emerging need to control when this should be activated and when not as we now have many situations with multiple concurrent models and this current setup no longer works well.So I thought that HF Transformers should gather at least some needs of various frameworks  Deepspeed ZeRO, FSDP and may be others where `from_pretrained` provides hooks into the model instantiation stage. The 2 needs are:     1. yes/no activate flag   2. framework provided context to instantiate in  so `zero.Init` would be just one of them The intention is that the user won't need to do anything special when they create the model. The framework would register with HF Transformers everything it needs to do it smoothly and correctly.  Additional context on slack Andrew Gu posted: > If you specify the  `device_id` arg, FSDP should move the unsharded module to GPU before sharding so that the sharding ops are fast (but still requiring CPU init and copy H2D). Otherwise, the only other way to initialize _and_ shard on GPU is to do socalled ‚Äúmanual wrapping‚Äù, which was the common approach from Fairscale. I do not think that would work with HF transformers. > For manual wrapping, you have to call `FullyShardedDataParallel(module, ‚Ä¶)` on `module` immediately after it was construct",2024-02-29T04:11:12Z,triaged needs research module: fsdp,open,21,5,https://github.com/pytorch/pytorch/issues/120878," Have you tried FSDP's (admittedly limited) support for meta device to instantiate the model directly on GPU? For finetuning use cases, you can do something like the following:  > The intention is that the user won't need to do anything special when they create the model Is it possible to offer some sort of interface / wrapper in upstream libraries to hide this complexity from the user?","varma `load_state_dict` cannot be used like this for the sharded model, it will require loading a distributed checkpoint (DCP), converting a `torch.save` checkpoint, or summoning the full params before loading (on CPU  provided you have enough RAM). But we've been very successful with this setup in our experiments. > Is it possible to offer some sort of interface / wrapper in upstream libraries to hide this complexity from the user? Since you asked, Lightning Fabric and PyTorch Lightning (disclaimer: I work on this) do this already: https://lightning.ai/docs/fabric/stable/advanced/model_init.htmlmodelparalleltrainingfsdpanddeepspeed. Including support for loading full and sharded checkpoints.",", would you like to get involved in this discussion? As it'd be HF's Accelerate's job to handle this, so your input would be most relevant  please tag others on your team as need be.  And probably opening an Issue on Accelerate's side to track.","varma, yes, this is why I'm trying to discuss this. Since HF Transformers already does a special usecase for Deepspeed ZeRO, so its sharding to GPUs is automatic for users, but as we now have other similar use cases  I think that hardcoded solution needs to be upgraded to be supportive of various usecases. So I think FSDP's need integration would be a perfect usecase. I have tagged Sourab in the comment above who took over the initial work I did at HF Transformers 3 years ago. So my intention was to spec out what FSDP needs and then take it to HF Transformers side for proper integration. as  shared above Lightning has already solved all of these cases in their frameworks. And regardless let's make sure that this usecase is well documented in the FSDP docs.","Similar to  CC(FSDP: unsharded gradients accessor)issuecomment1977140356, I wanted to share what I was thinking as an option for metadevice init in the new FSDP. The PR is https://github.com/pytorch/pytorch/pull/120351. The idea is that, by representing FSDP sharded parameters as `DTensor`s, we can rely on `DTensor`'s op dispatch to compute the correct randomness with respect to the global (unsharded) shape. As such, we can shard a metadevice model with TP and FSDP, leaving it on metadevice; materialize the sharded model on GPU (e.g. using `to_empty(device)`); and then either load from a checkpoint or randomly initialize the sharded metadevice parameters through `DTensor` dispatch. For example, taken from that PR: "
transformer,Tensor Parallel Inference Performance Issue with Llama-7b-chat-hf Model caused by straggler problem," üêõ Describe the bug Hello PyTorch Team, I am currently attempting to leverage Tensor Parallelism for inferencing the Llama7bchathf model. However, I've encountered a significant bottleneck related to server straggler issues, leading to an absence of expected speedup from tensor parallelization. The core of the problem seems to lie in the AllReduce operation, where all ranks are forced to wait for the completion of the slowest rank. Consequently, each AllReduce operation incurs a delay exceeding 1ms. My experimental environment consists of a server equipped with 8 V100 GPUs, interconnected via NVLink. To illustrate the issue, I am providing two nsight profile results comparing the performance of parallel inference across 4 V100 GPUs, with and without compilation. The profiles indicate a consistent issue where one rank experiences significant latency in kernel launching, thereby causing delays in the AllReduce operation for other ranks. The AllReduce operators are highlighted for ease of reference. Below is the code snippet employed for this setup. Could you please advise if there are any misconfigurations or suggest potential optimizations to mitigate this issue? !image !image Following is the code I use, Is there any wrong configuration of it?  You can run the code by following command:   Versions  ",2024-02-28T07:49:18Z,oncall: distributed,open,0,3,https://github.com/pytorch/pytorch/issues/120787,> a consistent issue where one rank experiences significant latency Is it always the same rank that's slow? Another thing to rule out is machine issues  wonder does it repro if we change to a different server machine?, wondering if you also tried torch.compile the model after applying tensor parallel?,"To be a bit clear, I think the reason why one rank is delaying a lot is because for some ranks the CPU thread is too slow to feed GPU enough work, for inference to work performantly with reasonable toks/sec (with/without Tensor Parallel), it would be preferred to use torch.compile to get rid of the CPU slowness.  You can take a look at this blogpost about many optimizations you can do to accelerate inference for llama 7B model https://pytorch.org/blog/acceleratinggenerativeai2/  "
transformer,add a note wrt torch.nn.functional.scaled_dot_product_attention,followup change of https://github.com/pytorch/pytorch/pull/120565   Added a note in the transformer class pointing out the mask definition is opposite to that of :attr:`attn_mask` in             torch.nn.functional.scaled_dot_product_attention.  ,2024-02-26T23:33:26Z,triaged open source Merged ciflow/trunk release notes: nn topic: docs,closed,0,5,https://github.com/pytorch/pytorch/issues/120668, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  .github/workflows/pull.yml / linuxfocalcuda12.1py3.10gcc9 / test (default, 3, 5, linux.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""foreach add test failure unrelated to doc fix"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Torchrun arguments shadowing script arguments," üêõ Describe the bug Consider a minimalistic script as follows:  Running it directly gives the following:  (see the default r value now being updated to 16) Running it using torchrun:  The reason for this is argparse's abbrevation handling which shadows any argument with the prefix of an existing torchrun argument. For example, with ""n"" which matches all torchrun flags which starts with n and so on. This behaviour exists on the current main. The fix is straightforward: set ""allow_abbrev"" to False when creating the torchrun ArgumentParser. I have tested this locally and found it to work and can open a PR if there is consensus that this is a bug and no existing workflow will be broken due to this.  Versions Collecting environment information... PyTorch version: 2.2.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Fedora release 39 (Thirty Nine) (x86_64) GCC version: (GCC) 13.2.1 20230918 (Red Hat 13.2.13) Clang version: Could not collect CMake version: version 3.27.7 Libc version: glibc2.38 Python version: 3.12.0 (main, Oct  2 2023, 00:00:00) [GCC 13.2.1 20230918 (Red Hat 13.2.13)] (64bit runtime) Python platform: Linux6.5.6300.fc39.x86_64x86_64withglibc2.38 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:     ",2024-02-26T08:55:49Z,triaged module: regression module: intel,open,1,3,https://github.com/pytorch/pytorch/issues/120601,jeancho please help debug this one!,This seems to be a nasty issue that showed up on 2.3.0 and breaks 2.2.2 scripts.,"Yeah, this is pretty bad because of the 2.3.0 changes mean you can't use torchrun on any script that use `log`, `logs` or any other prefix of `logs_spec`."
transformer,Add the hyperlink of the transfomer doc,"Fixes CC(Better documentation on _mask and _key_padding_mask in transformers modules)   The shape for forward pass is clearly stated in the main transformer class  Boolean mask for _key_padding_mask is also explained in the main transformer class. Therefore, add the hyperlink to the transformer class explicitly so the user can refer back to the main class. Also, correct several symbols in the transform doc from normal text style to math style.",2024-02-24T21:35:08Z,triaged open source Merged ciflow/trunk release notes: nn topic: docs,closed,0,5,https://github.com/pytorch/pytorch/issues/120565,"> Thanks a lot! >  > re the mask discrepancy mentioned on the initial issue >  > > Also, a clarification on why one uses a boolean mask for _key_padding_mask with True values indicating there's padding there and False there's no padding which is actually the opposite of other frameworks. >  > As you mentioned, the mask definition is documented in the Transformer docs. However, would you be willing to add a `..note::` in the Transformer class that for a boolean mask, `True` means that a value is not included in the attention which is the opposite of what it means in `torch.nn.functional.scaled_dot_product_attention` please? >  > (If it is easier for you feel free to do this in a follow up) Will do."," label ""topic: docs"""," label ""release notes: nn"" ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,TransformerEncoder/Decoder: add type hints,,2024-02-24T10:00:46Z,triaged open source Merged ciflow/trunk release notes: nn topic: docs,closed,0,2,https://github.com/pytorch/pytorch/issues/120550, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Better documentation on _mask and _key_padding_mask in transformers modules," üìö The doc issue The docs here do not clearly indicate the shapes of the masks and their equivalent types that the transformer modules are expecting. The only place where one can find shapes and types of expected tensors are only in the source code docs atm.  Suggest a potential alternative/fix A couple of examples of different use cases would really help users understand what is expected what the actual effect is of each mask.  Would the transformer module operate on both of of them? Also, a clarification on why one uses a boolean mask for `_key_padding_mask` with `True` values indicating there's padding there and `False` there's no padding which is actually the opposite of other frameworks. For instance, here's an excerpt from hugging face explaining the same thing but they use 1s, i.e., `True` indicating where the model needs to attend to and 0s `False` where there's padding.  ",2024-02-23T14:14:46Z,module: docs triaged actionable,closed,0,0,https://github.com/pytorch/pytorch/issues/120488
transformer,faster communication," üöÄ The feature, motivation and pitch I noticed that with the release of the new version of nccl, a new feature appeared(https://docs.nvidia.com/deeplearning/nccl/userguide/docs/usage/bufferreg.html). This feature seems to be able to speed up transformer by combining some technologies, such as paper 'Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models'.  So I think it is interesting if pytorch could provide a highlevel API to use these technologies.  Alternatives _No response_  Additional context _No response_ ",2024-02-23T03:24:29Z,oncall: distributed triaged,open,0,2,https://github.com/pytorch/pytorch/issues/120466,"  I think we do have a plan for supporting zerocopy collectives, right? Could you help elaborate? Thanks!",adding notes from discussion with   (please correct me/add more details) 1) nccl 2.20 would be required (but not tested yet by the pytorch team) 2) zero copy would only be over the network for internode comm.  To enable intranode zerocopy allreduce you need to allocate your own memory with some flag
transformer,torch.compile() drops transformer/Qwen1.5-7B model output quality from good to unusable," üêõ Describe the bug Not sure how we can further debug this and provide more data but we are seeing drastic output quality difference between notorch.compile and torch.compile.  We are new to torch.compile(). Does torch.compile inherently cause accuracy issues? Not sure if just natural part of the beast.  model load code:  gen code and genparams are same for generation.  The difference is as if we are running a vastly different, a much more inaccurate model when torch.compile() is enabled. The difference, drop in quality, is not related to randomness of sampling.    Versions  ",2024-02-23T01:53:21Z,needs reproduction triaged oncall: pt2 module: pt2 accuracy,open,0,7,https://github.com/pytorch/pytorch/issues/120462,"I'm putting this up for triage review because there is an interesting question here, which is that torch.compile numerics are *necessarily* different from eager (we do not guarantee they are exactly the same), but what I am not sure is if we consider it a bug if you fine tune a model with eager pytorch, and then torch.compile'd version is unusable, is this a bug in torch.compile or could it be that this is expected and you needed to fine tune the model with torch.compile? That being said, since this is a transformer accuracy thing, it's possible that https://github.com/pytorch/pytorch/pull/119500 fixes the problem. I'm not sure if this one made it to 2.2, you can try a nightly.   We might be willing to take a closer look; more detailed repro instructions will help in that case.","We discussed this in triage review, and we do not believe that (modulo bugs), `torch.compile` should ever result in *worse* accuracy, particularly inference. So this sounds like a bug we'd like to fix, and we'd appreciate more detailed repro instructions.","I've also gotten much worse accuracy using torch.compile, here's a repro (for a TexttoSpeech model).  With the following changes, the final speech output is much worse. Same worse results when using other torch.compile parameters like `mode=""maxautotune""`, `mode=""maxautotunenocudagraphs""`, `fullgraph=True`, `fullgraph=False`, `dynamic=True`, `dynamic=False`, and also when torch.compile is applied to other modules of the Decoder class. https://github.com/siddhatiwari/StyleTTS2/commit/13a9f87e97af98561566b643728e01608acc1bb1","Thanks , any chance you can give instructions for how to reproduce your results on your repo?","StyleTTS2 is a model for generating speech from text. It doesn't currently use torch.compile in any capacity. I tried modifying it by adding a single torch.compile to a small part of the overall model. You can find the original repo https://github.com/yl4579/StyleTTS2 and my modified repo https://github.com/siddhatiwari/styleTTS2.  Note that you only need to clone my modified repo to generate audio with both the unmodified model (original quality) and the torch.compile model (with lower quality). The following steps explain how to set everything up, produce good outputs with the unmodified model, and bad outputs with the torch.compile model: Part 1: Setup  Run each command:  Part 2: Generating audio using unmodified model  Run:  See generated audio: 1789_142896.wav 696_92939.wav Part 3: Generating audio using torch.compile model  In ./Modules/hifigan.py (https://github.com/siddhatiwari/StyleTTS2/blob/main/Modules/hifigan.pyL436): Comment line:  Uncomment lines:  Run:  See generated audio: 1789_142896.wav 696_92939.wav You will notice that the generated audio in Part 3 is worse quality than that in Part 2.  Sample audio outputs you should get after following these steps: https://github.com/siddhatiwari/StyleTTS2/tree/main/samples"," Hey, just wanted to follow up on this. Any ideas on what I could do on my end to help you guys investigate this further? ","Hi , some basic troubleshooting ala https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html would be helpful. What I find most useful is to do an ablation: does it happen if you get rid of reduceoverhead, what about backend=""aot_eager"" or backend=""eager""?"
transformer,Reinitialize parameters in TransformerEncoder," üöÄ The feature, motivation and pitch I was exploring the nn.TransformerEncoder, and I noticed that one layer is initialized and the other layers are copies of this initial random state. Although this might be worth exploring, intuitively it'd make sense to initialize randomly each of the layers.  My intuition is that if a random initialization has projections associated with certain eigenvectors, repeated applications of this layer might tend to project toward vectors with large norms, which could cause numerical issues during the initial training steps.  Alternatives _No response_  Additional context _No response_ ",2024-02-22T11:17:26Z,module: nn triaged,open,1,0,https://github.com/pytorch/pytorch/issues/120396
transformer,"Second forward call of a compiled model (exact same input shapes, strides) is extremely slow due to cuda graphs"," üêõ Describe the bug When using `torch.compile(..., mode=""reduceoverhead"")` on CUDA device, the second forward call with exact same input shapes, strides, device, dtype is extremely slow, with 0% GPU usage and 100% CPU usage. When using ctrl+C (**not an error**, just forcing exit), we see that PyTorch is spending time in cudagraph `self._record(wrapped_function.model, recording_inputs)`:  This is surprising that this happens at the second forward call.  and then  Giving:  The log comes from https://github.com/huggingface/transformers/compare/a8c4e1036ac6f0f78e512235cf42c17c7d3cc762...reprobugpytorchcompilecudagraph?expand=1 A potential solution is to use `.compiler.disable` on the `_update_causal_mask` method. This removes logs as  in inductor logs. But it is not a perfect solution either as then `fullgraph=True` can not be used. It is quite surprising to me that the **second** forward call is slow. To me only the first should be.  Versions  ",2024-02-21T11:17:03Z,module: cuda graphs oncall: pt2 module: dynamic shapes,closed,0,5,https://github.com/pytorch/pytorch/issues/120309,"related  CC(If dynamic shapes runtime CUDA graphs never quiesces, should loudly warn / easy to diagnose)","> It is quite surprising to me that the second forward call is slow. To me only the first should be. This is a side effect of how we turned on cuda graph support for dynamic shapes. We hope that you do not have too many distinct sizes, and CUDA graph them all. To do this, we have to be recording on the first time you hit ever new size. What is your desired end state for your code here? Do you want fullgraph=True, but cuda graphs on only part of the graph (excluding update causal mask?) There's a potential feature we've talked about which is making reduceoverhead smarter and selectively cuda graph only compatible regions of graphs, I couldn't find if we filed an issue for it though. ","Thank you .  commented on slack: > We do two warmup calls not one because we support mutation on Parameters. The first run warms up model (things like triton autotuning etc). The second run records the graph and plays it. The third run is the fast path > > To do only one warmup run you would have to copy parameters prior to warmup otherwise you would mutate it twice, and copying parameters is not viable for memory reasons where I'm not sure which `parameters` it refers to (maybe tritonrelated found hyperparams). So it appears this is not a bug, but a design decision. > What is your desired end state for your code here? Do you want fullgraph=True, but cuda graphs on only part of the graph (excluding update causal mask?) That would be very helpful indeed. What I considered using was `.compiler.disable` for the part of the code using dynamic shapes, and compile time was drastically reduced but latency increased as well. I think for now we'll stick with feeding static shapes to the compiled model.",I found documentation in the PyTorch CUDAGraph Trees which gives a simple example with comments explaining what each invocation does: ," Thank you for the pointer, I was not aware of it!"
transformer,Equivalent idea to size-oblivious guard for end of bounds on sizes," üêõ Describe the bug Ref  CC(llama model failed for dynamic shape path) Sizelike SymInts currently let us definitively answer that an unbacked SymInt does not equal 0 or 1, in cases where the generic logic would generalize. It would be helpful to have an analogous version of this which says an unbacked SymInt does not equal s0, where s0 is the bound of some Tensor we're going to index with the unbacked SymInt into. The prototypical situation of this is this pattern found in transformer models:  We legitimately must know if u0 == tensor.size(0), because *only* in this case is the resulting view contiguous; when u0 < tensor.size(0), we have ""gaps"" between each batch entry and it is discontiguous. If we are aiming to compile only a single kernel, we would like to say that u0 != tensor.size(0), giving us a ""discontiguous"" tensor that Inductor can appropriately pessimize over. Let's suppose we do it with some API like `torch._check_is_bounded(u0, s0)` which is analogous to `torch._check(u0 <= s0)`. On subsequent size oblivious tests, we'd like report `u0 < s0`. It would be amazing if we could put s0 in the ValueRange for u0. But this doesn't work for two reasons: (1) ValueRanges can't take symbolic bounds, and (2) this doesn't help, you want u0 <= s0  1 to discharge conditions (but you don't actually want that). The very inefficient way to do this analysis would be to just remember u0 is bounded by s0, and then when doing size oblivious tests, use this information to answer relational queries involving u0 and s0 (we can't do the original trick of narrowing the range from min=0 to min=2. But maybe if we did some ad hoc ValueRanges analysis with",2024-02-21T04:49:01Z,high priority triaged oncall: pt2 module: dynamic shapes,open,1,8,https://github.com/pytorch/pytorch/issues/120288,"For reference, we disallow bounds to be symbolic because there are a few sympy ops like (truncation and I reckon that a couple more) that do not have a symbolic equivalent. That being said, these multivariate bounds are very brittle. It is very easy to end up with expressions of the form `s0  x0  1 < s0` which our system cannot prove that are true.","We got another example of this kind of phenomenon in https://fb.workplace.com/groups/6829516587176185/posts/7493024157492088/  The user is trying to implement padding by viewing into large and then mutating the resulting view. But this triggers a data dependent guard, as narrow(1, 0, A2) may or may not be contiguous depending on whether or not A2 == large.size(1). Our suggested workaround in this case is to use the padding operation directly, e.g., torch.nn.functional.pad","Another internal occurrence: https://fb.workplace.com/groups/6829516587176185/posts/7611851122276057 This one is interesting, because in this case they want to do a mutation on the view. Ick! Their sample program:  Actually, you can get it to work by manually functionalizing it yourself. Kids, don't try this at home: https://www.internalfb.com/intern/anp/view/?source=version_selector&id=5669466  then  and end to end ","Here is another example where it is not the narrow that is a problem per se, it is functionalizing the copy on the narrow:  Here it errors with   It's specifically failing at  So it's possible we can fix this spot specifically.",Another occurrence:  The guard is specifically triggered here:  Minimal repro:  Rewriting this to be a narrow instead works though: ,"https://fb.workplace.com/groups/6829516587176185/permalink/8388398464621315/ Yet another minimal repro:  This one is potentially also functionalization on the narrow, need to check. EDIT: Yes, it's functionalization  EDIT 2: https://github.com/pytorch/pytorch/pull/141418 works around computeStorageNbytes but you still die asking for contiguity on the tensor EDIT 3: The changevar trick works "," Do you know why does your last example work with `torch.compile(repro, dynamic=True)(*example_inputs)`, but not with export?",Your torch.compile invocation doesn't include fullgraph=True so I assume it's just graph breaking
transformer,Stateful `int` is not updated when using `torch.compile`," üêõ Describe the bug A model instance attribute `int` is silently not correctly updated when using `torch.compile`. The model behaves as expected when using simply eager mode. Reproduce with:   Resulting in the logs:  Notice that `seen_tokens` is sometimes not correctly updated as it should be https://github.com/huggingface/transformers/blob/c9b864ba54ff09ef0eaf9b0f9f9624ef594d5e0e/src/transformers/cache_utils.pyL394 (simply incremented by one) The logs come from https://github.com/huggingface/transformers/compare/c47576ca6e699c6f8eaa8dfc4959e2e85dec0c72...reprobugpytorchcompile?expand=1 Something to consider here is that the `StaticCache` subclass is attached only AFTER the `torch.compile` call, see here and here. The first solution I found was to use a `torch.Tensor` (updated in place) instead of an int. But given that the model after torch.compile silently behaves differently than it did simply in eager mode, I believe this is a bug.  Error logs no error, silent bug.  Minified repro /  Versions  ",2024-02-20T19:28:16Z,triaged oncall: pt2 module: dynamo dynamo-must-fix,closed,0,6,https://github.com/pytorch/pytorch/issues/120248,"In the end, this was fixed in https://github.com/huggingface/transformers/pull/29114 by removing the `seen_tokens` variable altogether. In the meantime, I came across an other bug when using `seen_tokens` as a `torch.Tensor` (instead of an `int`) updated inplace, where calling `torch.compile` BEFORE the `StaticCache` subclasses initialization would also result in silently wrong computation of `seen_tokens` (but this time in the second generate call, not the first as in the above snippet).", ,(Note that the ability to use a stateful int would still be useful to us https://github.com/huggingface/transformers/pull/29221discussion_r1502454812),"This is a duplicate of  CC(torch.compile precision bug when the attr object changes) If you want to work around, you can use `torch._dynamo.config.guard_nn_modules=True`. This will incur performance penalty because TorchDynamo will insert additional guards inside the nn module hierarchy. We are investigating how we can catch this without incurring large perf penalty.",Removing the high priority label as the duplicate  CC(torch.compile precision bug when the attr object changes) is already highprio.,This is fixed now. Closing
transformer,Making Mamba first-class citizen in PyTorch," üöÄ The feature, motivation and pitch Mamba is a new SSM (State Space Model) which is developed to address Transformers‚Äô computational inefficiency on long sequences. It has attracted more attention recently due to faster inference and linear scaling in sequence length. We are exploring how to support Mamba as firstclass citizens in PyTorch.  To better understand the gaps and coordinate these ongoing effects, we created the following doc to trace the requested features and issues. Feel free to comment if you have any feedback! https://docs.google.com/document/d/1rNNByFrOjOQOBM6ZZqnOqcLGMRmdnQifKfbY_KalnM/edit?usp=sharing   ",2024-02-19T06:03:59Z,triaged needs research oncall: pt2 module: higher order operators module: pt2-dispatcher,open,10,4,https://github.com/pytorch/pytorch/issues/120189,"The document, as written, is a bit too optimistic. The current `tl.associative_scan` (and as such 's implementation in https://github.com/pytorch/pytorch/pull/119430) just supports pointwise accumulation functions. As such, we will just be able to implement SSMs for diagonal matrices, where matrix multiplication turns into pointwise multiplication. We should be able to do this once: 1. https://github.com/pytorch/pytorch/pull/119430 lands 2. We extend the current support for multiple inputs and outputs in our scan operation (this shouldn't be too difficult)."," Yes, we only discussed SSMs for diagonal matrices in the doc, but we should be able to extend to more general SSMs after this.",I hope we could include also Mamba2 coverage https://arxiv.org/abs/2405.21060 https://github.com/statespaces/mamba/issues/355,Associative scan was merged in Triton on April https://github.com/tritonlang/triton/pull/3177 A minor bug is WIP at: https://github.com/tritonlang/triton/issues/4362 What is the status on pytorch?
transformer,"The forward accuracy cannot be aligned when using  DTensor and ColwiseParallel, RowwiseParallel for TP training on the Llama2"," üêõ Describe the bug Hi, I found that the forward accuracy cannot be aligned completely when using your 'DTensor' and 'ColwiseParallel', 'RowwiseParallel', and 'Replicated' api for TP training on the Llama2.  I referred your example: https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/parallel/test_tp_examples.pyL175C9L262C1. I parallelize the embedding submodules of Llama2 by 'Replicate()', parallelize the attention and feed forward by 'ColwiseParallel' and 'RowwiseParallel', and Replicate the model.lm_head for the output, and manually adjust the number of heads and the hidden size. The following is my code:  And here is my loss comparison  between model and model_tp: !img_v3_0284_103fb69e539e427f8d3f29e7981c5f7g and my comparison code, just like you used in your example: !img_v3_0284_72d46648041d474f9004909adbe6764g It said I have 29% elements mismatched in my first step loss comparison! The gradient of model parameters in backward cannot be aligned too. !img_v3_0284_c152df3641e24b808dc270d5a3ae96bg I feel very confused and have invested the whole process, like whether the input is consistent across each rank(it is), and whether the dropout in the llama2 model could introduce randomness (Actually llama2 does not have dropout). Now I really don't know why. Could anybody help me to check this? Should I add some all_reduce for the loss or something?  Versions Package                 Version            Editable project location    abslpy                 2.1.0 accelerate              0.26.1 aiohttp                 3.9.3 aiosignal               1.3.1 astunparse              1.6.3 attrs                   23.2.0 blessed        ",2024-02-17T12:20:38Z,oncall: distributed module: dtensor,open,0,11,https://github.com/pytorch/pytorch/issues/120158," Hello, would you mind assisting me with checking the accuracy on TP training with llama2? I really don't know whether it's wrong with my code or something. I would be grateful if you could help me.","By the way, the weight of model and model_tp before forward can be alighted. So at least we can narrow down the scope of the problem to the first forward pass of the model. I think the problem probably be the all_reduce. !img_v3_0286_06138c6e460a4ba885cfbf7ad6c4cb7g !img_v3_0286_e3f1f3dca2b3441f9226fb061ff29e8g","Hi, dear developers, I have narrow down it's after the MLP of Llama2 model, the accuracy cannot be aligned totally. There is a SiLU activation in MLP, which is different from your example. I'm trying to solve it and hope it could be aligned later.","Hello, there is a correction.  I found that during the llama2 model forward pass, the precision can be aligned after **the first LlamaDecoderLayer**. However, after passing through **24 layers** of LlamaDecoderLayer class, this precision gap increases to **11%.** This is beyond my understanding.  You can try looping the transformer layer in your example multiple times instead of just one layer to see if the same precision issue occurs.",And I used torch.float32 to do the training., l can one of you take a look?,"Hello, I think I have solved the forward accuracy. It's the reason that you use the AsyncTensor to do the communication for TP. **The AsyncTensor is still dangerous now,** and can lead some errors in the compiler, as  mentioned in the comment https://github.com/pytorch/pytorch/blob/main/torch/distributed/_functional_collectives.pyL119 I use native c10d api to replace them and now the forward accuracy can be aligned, the `testcase.assertEqual(output[0], output_tp[0]) ` code can be passed now. !image But the backward of model and model_tp still cannot be aligned now, it said there are  **Mismatched elements: 7 / 4194304 (0.0%)** of attention weights between model and model_tp. :( The corresponding elements of the model and model_tp's weights looks like very close but not the same. Maybe I should check the backward of attention after backward. I am still working on finding a solution. I would greatly appreciate any suggestions you may have. !image",The loss curve still cannot be converged. üò¢ Would anyone be so kind as to lend a hand? I would be extremely grateful. Thank you!,My comment about tracing asynctensor just means the compiler starts with an unoptimized graph from timing/overlap perspective and must do its own reordering to improve performance. I don't know of an accuracy problem with FC/compile. But if you're reporting one we'll take a look. We might need some help dumping inductor IR graphs. Cc  ,"Hey , just a heads up that we landed a functional collective overhaul a few days ago (https://github.com/pytorch/pytorch/pull/120370). Can you check if your local PyTorch build contains the change? If not, I'd suggest updating your PyTorch build and rerunning your experiments with the change. It's not guaranteed, but I think there's a good chance it may address your issue. If the issue persists, please provide a minified repro. I'd be happy to look into it.","> Hey , just a heads up that we landed a functional collective overhaul a few days ago ( CC(Switch to native functional collective by default)). Can you check if your local PyTorch build contains the change? If not, I'd suggest updating your PyTorch build and rerunning your experiments with the change. It's not guaranteed, but I think there's a good chance it may address your issue. >  > If the issue persists, please provide a minified repro. I'd be happy to look into it. Thank you for your crucial reply! It really works for the forward accuracy but still can't make the backward accuracy aligned after I update my local PyTorch to the latest including CC(Switch to native functional collective by default). Sure! You can find the start.sh script at this link: start.sh. Don't worry about the model and data paths as they are all downloaded from Hugging Face. The tensor parallel part can be found in tensor_parallel.py, and the tensor parallel unit test part is available in finetune_llama2.py. Both are referenced from the tp_examples in PyTorch: test_tp_examples.py."
transformer,[dtensor] add support for loss parallel,"  CC([dtensor] add support for loss parallel) Loss parallel is the last piece of sequence parallelism to enable. It enables efficient distributed cross entropy computation when the input is sharded on the class dimension (in a classification problem with many classes). The implementation is via a context manager `loss_parallel`, after enabling which users can directly use `torch.nn.functional.cross_entropy` or `torch.nn.CrossEntropyLoss` without modifying other parts of their code. Here are the underlying rationales why we are going through these op replacements: 1. `nn.functional.cross_entropy` is the common method that OSS user is using for things like transformer training, to avoid changing user code, we want user to still use this function for loss calculation if they are already using it. 2. `nn.functional.cross_entropy` boils down into `aten.log_softmax` and `aten.nll_loss_foward/backward`, and DTensor now supports those ops already ( CC([dtensor] switch softmax forward ops to OpStrategy) CC([dtensor] switch softmax backward ops to OpStrategy) CC([dtensor] add op support for nll_loss_forward) CC([dtensor] add op support for nll_loss_backward)). They are doing computation with input *replicated* on the class dimension. 3. However when the input of this loss calculation is **sharded on the class dimension**, to run sharded computation efficiently, we need to run both `aten.log_softmax` and `aten.nll_loss_foward` with multiple allreduce collectives **in the middle of** those aten ops. This is not possible if we are just overriding these two ops, so we need to have some way to **decompose** these two ops into smaller ops to have collec",2024-02-14T07:29:34Z,oncall: distributed Merged ciflow/trunk ciflow/inductor release notes: distributed (dtensor),closed,0,4,https://github.com/pytorch/pytorch/issues/119877,"> Why is a context manager needed (and why do we need to register special replacements for the operators in dispatch table)  why can't we just add behaviors into DTensor's torch function so that when it handles these operators it calls these remap functions? Great questions! Here's just my understanding. Wanchao can say more on this. For cross_entropy related ops, if loss parallel is not needed, we've implemented them in the normal way ( CC([dtensor] switch softmax forward ops to OpStrategy) CC([dtensor] switch softmax backward ops to OpStrategy) CC([dtensor] add op support for nll_loss_forward) CC([dtensor] add op support for nll_loss_backward)). When loss parallelism is needed, we need customized and somewhat slower op implementations in this PR. There has to be a way DTensor detects the intention of doing loss parallel or not. Using context manager gives the control to users; another way to do it involves automatically detecting if input is sharded on the dimension where loss parallel can be efficient  I explored that path as well (in CC([dtensor] add support for loss parallelism)) but that requires similar amount of customized code and/or even slower computation, in addition to moderate surgery to DTensor dispatching logic.","l Please update the PR summary with more context and design on loss parallel.  loss parallel is the last piece of sequence parallelism we want to enable. The underlying rationales why we are going through this op replacements instead of just support it via sharding propagation: 1. nn.functional.cross_loss_entropy is the common method that OSS user is using for things like transformer training, to avoid changing user code, we want user to still use this function for loss calculation if they are already using it. 2. `nn.functional.cross_loss_entropy` boils down into `aten.log_softmax` and `aten.nll_loss_foward/backward`, DTensor supports those ops now already. 3. However when the input of this loss calculation sharded on the vocab_dim, to run sharded computation efficiently, we need to run both `aten.log_softmax` and `aten.nll_loss_foward/backward` with multiple allreduce collectives **in the middle of** those aten ops. This is not possible if we are just overriding these two ops, we need to have some way to decompose these two ops to smaller ops to have collectives run in the middle of these two ops 4. We explored the decomposition way, and it seems working, except that because nll_loss_backward in aten is implemented in a inefficient way, we would trigger additional collectives. Recently some user also reported similar issues  CC(Performance Concern about CrossEntropyLoss) Therefore we are doing this way of decomposition by our own currently for sequence parallelism specifically. Once we have a better decomposition in core, we can possibly take that instead of reinventing the wheels here", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Export of bitwise_right_shift to ONNX needed for llama2-7b 8-bit quantized pytorch model," üöÄ The feature, motivation and pitch We are attempting to export a quantized llama model (from HuggingFace) to ONNX but are running into an unsupported op error for bitwise_right_shift: `torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::bitwise_right_shift' to ONNX opset version 17 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.` Would you please consider adding support for exporting this op to onnx?  Sample python to reproduce below.  Thanks!  Alternatives _No response_  Additional context ",2024-02-10T01:37:10Z,module: onnx triaged OSS contribution wanted,closed,0,2,https://github.com/pytorch/pytorch/issues/119621,"Is there any workaround for getting int8 ONNX model for llama2? Also, wanted to know approximate timeframe for resolution of this issue.",bitwise_right_shift is not supproted by torch.onxn.export and it is not planned to be Please try the new ONNX exporter and reopen this issue if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial We do export llama v2 to onnx using the new exporter :)
transformer,Fixes #105077 alternative 2,Fixes CC(torch.load fails under FakeTensorMode for GPT2 model)  Repro  Error:  ,2024-02-09T19:47:55Z,open source ezyang's list module: fakeTensor,closed,0,1,https://github.com/pytorch/pytorch/issues/119584,super seeded by https://github.com/pytorch/pytorch/pull/108186
transformer,[no ci][FSD2] Added annotations for compute compile,"  CC([no ci][FSD2] Added annotations for compute compile)  CC([FSDP2] Added pre/postallgather extensions)  CC([FSDP2] Generalized allgather outputs to >1 per parameter)  CC([FSDP2] Added CPU offloading)  CC([FSDP2] Used stream APIs for CUDA event handling) **This PR is not for landing.** **Some notes**  To disable using `DTensor` as the representation for sharded parameters, change https://github.com/pytorch/pytorch/blob/a1ff82d4a5ee90896a05533d178fbb153dd78d6f/torch/distributed/_composable/fsdp/_fsdp_param.pyL393L408 to return `tensor` directly (and it will not be wrapped with `DTensor`). I am not sure if there are any breakages from that (e.g. accesses like `self.sharded_param._local_tensor`).  This PR includes https://github.com/pytorch/pytorch/blob/a1ff82d4a5ee90896a05533d178fbb153dd78d6f/torch/distributed/_composable/fsdp/fully_shard.pyL119L121 to have Dynamo take the `FSDPManagedNNModuleVariable` path. If that is not desired, feel free to remove those lines. **Testing** There are two example unit tests that just run training on a small transformer with FSDP applied _without compile_ (compile can be added to these tests as desired):  The multithreaded one uses multithreaded process group, which is faster (e.g. 6 seconds to run that test vs. 11 seconds to run the multiprocess one). The multithreaded one should suffice for testing, but I included the multiprocess one as well just in case. Here is  's test script including compile: https://gist.github.com/yf225/34629198eb597ea80563087d6a4238d4 It may be more helpful than `test_compile_multi_thread` above.",2024-02-09T14:52:07Z,release notes: distributed (fsdp),closed,0,1,https://github.com/pytorch/pytorch/issues/119551,"    There is one interesting note: Currently, perparameter FSDP uses module forward hooks (`register_forward_pre_hook` / `register_forward_hook`). However, we could also override `nn.Module.forward()` using the `FSDP` class that we dynamically swap with. In a closed system with only FSDP, these two approaches are equivalent. However, beyond FSDP, this affects ordering (e.g., float8 and DTensor hooks), and it affects whether FSDP pre/postforward logic is included if doing `module.forward = torch.compile(module.forward)` after applying FSDP. For float8, we really want to be able to compile at least parts of the preforward. Given this, it might be preferred to override `nn.Module.forward()` so that we can include the preforward as part of the `torch.compile(module.forward)`. Otherwise, we would need some other scheme to compile like `model = torch.compile(model)` (toplevel), which has not been as stable."
transformer,aarch64 linux: torch performance is regressed for openblas backend from torch 2.0 to 2.1+," üêõ Describe the bug on aarch64 linux platform, PyTorch inference latencies are increased on torch 2.1 and 2.2 compared to torch2.0 when openblas backend is used for multithreaded configuration. The regression is higher for larger thread counts. On AWS Graviton3, c7g.4xl, with 16 threads, the inference latency with torch2.0 is `Time elapsed: 2.777902126312256 seconds` whereas with torch 2.1 and later, it is `Time elapsed: 4.907686471939087 seconds` **Reproducer:**  gpt2large.py   Versions aarch64 linux, Ubuntu 22.04, torch 2.1 or later",2024-02-07T14:38:32Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/119374,"The fix is merged to main, and is part of today's (20240207) nightly wheel (https://download.pytorch.org/whl/nightly/cpu/torch2.3.0.dev20240207cp310cp310manylinux_2_17_aarch64.manylinux2014_aarch64.whl). with this wheel, on AWS Graviton3, c7g.4xl, the performance issue is no longer seen. Time elapsed: 2.453695774078369 seconds **how to test** ",RC fix confirmed on both t4g.2xlarge   c7g.4xl (Graviton3)  vs 
transformer,Use Dtensor and ColwiseParallel api to do Tensor Parallel for llama2 model but got communication error.," üêõ Describe the bug I read the test_transformer_training example in pytorch/test/distributed/tensor/parallel/test_tp_examples.py, and I think it's really awesome.  Then I use it to do tensor parallel for Llama2 model and now I may met a communication error. Here is my llama2 model code, simpily import them from transformers, which version is 4.37.2:  And here is my tensor parallel code:  And when I do the forward of the llama2 model after tensor shard, it is stucked in the Embedding function which is before attention in llama2 model. Seems like one rank is stucked that I can't go on, and the other three ranks are keep running. I tried it many times and it is always stucked in the same place. It doesn't have any error information or any other tints. !img_v3_027q_1540648b71e8432fba94c41ccaaed06g I have excuted your examples in test_tp_examples.py with success. !img_v3_027o_523b7183d9034e4d8c53c4ca6661ceag Could anybody help me? I really like the ColwiseParallel and the RowwiseParallel design. They are really fantastic and so incredible that I can write my tensor parallel code so concise, I really want to use them successfully.  Versions transformer: 4.37.2 pip list Package                 Version            Editable project location    abslpy                 2.1.0 accelerate              0.26.1 aiohttp                 3.9.3 aiosignal               1.3.1 astunparse              1.6.3 attrs                   23.2.0 blessed                 1.20.0 cachetools              5.3.2 certifi                 2024.2.2 charsetnormalizer      3.3.2 datasets                2.16.1 dill                    0.3.7 expecttest              0.2.1 filelock        ",2024-02-07T02:19:39Z,oncall: distributed triaged module: dtensor,open,2,4,https://github.com/pytorch/pytorch/issues/119343,"Hi, I have figured it out. The code was right but I didn't keep the input the same between different ranks, so it was stucked. Really suggest that there should be a hint or a error message.","Hey  glad that you figured out the issues, happy to know that you like the ColwiseParallel and Rowwise Parallel design! > Really suggest that there should be a hint or a error message. Thanks for the feedbacks! I think the tricky part is that each rank itself won't know other ranks tensor content, if it knows, there must be some additional communication need to be happened for us to perform this sanity check, so we didn't add this sanity check. , something we should think about how to improve sanity checkin when doing TP. We can possibly add some sanity check APIs to install hooks and check in runtime the layers are receiving replicated inputs or not.  At very least, I think we need to improve the API documentations to note that if the ColwiseParallel/RowwiseParallel is using replicated input, user would need to make sure the inputs are actually replicated before calling this module","Thank you for your kindly reply! I think the sanity checkin you mentioned is necessary as we usually randomize the input data in data parallel. So this error easily occurs when we use both TP and DP simultaneously. It seems like some kind of 'silent failure'.  By the way I have tested the result as you tested in the example, it's correct. Thank you so much! !img_v3_027r_05c1fed324034d948be1bf68ef4387ag",how to make sure the input are correctly replicated?
transformer,Inconsistent usage of `attn_mask` and `is_causal` arguments," üìö The doc issue In the docs describing `F.scaled_dot_product_attention` we can read: > is_causal (bool) ‚Äì If true, assumes causal attention masking and errors if both attn_mask and is_causal are set. So if `is_causal=True` then we also need `attn_mask=None` or it will throw exception. Got it. But let's see `nn.MultiheadAttention.forward` method. > is_causal (bool) ‚Äì If specified, applies a causal mask as attention mask. Default: False. Warning: is_causal provides a hint that attn_mask is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility. Hard to understand what does it exactly mean, so lets try it. If we run it with `is_causal=True` and `attn_mask=None` it will throw exception: RuntimeError: Need attn_mask if specifying the is_causal hint. You may use the Transformer module method `generate_square_subsequent_mask` to create this mask. So now is the opposite and `is_causal=True` needs `attn_mask` to be not `None`. So what exactly this two args represent? Since multihead attention and scaled dot product are two concepts so closely related to each other, shouldn't it be more consistent?  Suggest a potential alternative/fix _No response_ ",2024-02-02T05:28:27Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/118972
transformer,TransformerEncoderLayer fast path predicts NaN when provided attention bias," üêõ Describe the bug Similar to this issue) (which is for `nn.MultiheadAttention`) and this commentissuecomment1854786904) within the same thread, `nn.TransfromerEncoderLayer` predicts `NaN` in its fast path when provided with an attention mask.  I have created a child class `MyTransformerEncoderLayer` , that is used to check the attention weights on the slow path (the fast path doesn't go through `_sa_block`).  Here is how to reproduce the problem. Based on another similar issue)) that claims dropout was the cause, dropout has been set to zero here for simplification.   Results:  Only the fast path outputs `y_enc_fast` and `y_my_enc_fast` are all `NaN`. On the slow path, none of the attention weights in `attn_weights_my_enc` are `NaN` given the same set of inputs, which suggests that the problem is likely not due to erroneous inputs creating issues with the attention scores. Note that the same results are obtained even if `src_key_padding_mask` is `None`.  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.9 Libc version: glibc2.35 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.58+x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 535.104.05 cuDNN version: Probably one of the following: /usr/lib/x86_64li",2024-01-30T08:53:34Z,module: nn triaged,open,0,2,https://github.com/pytorch/pytorch/issues/118628,I was able to reproduce this,Same issue; using only 0 and inf in my attn mask seems to make it work stably but I want to use values in between.
transformer,[no ci][RFC] Added `fuse_comm_groups` staticmethod,"  CC([no ci][RFC] Added `fuse_comm_groups` staticmethod)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([BE] Enabled mypy in `common_fsdp.py`)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Added backward prefetching)  CC([FSDP2] Added `reshard_after_forward`) This PR shows an `FSDP.fuse_comm_groups(*modules)` staticmethod.  In this example, there are now 18 allgathers/reducescatters (root, layer 0, layer 31, and layer `i`, `i+1` pairs) instead of 33 (root and 32 layers), where the pairs have 2x comm. volume.  The FSDP design has the limitation that a communication group must be defined by exactly one `nn.Module`. This means that sibling modules cannot be one group without including their parent. For transformer architectures, this yields the conventional policy where each transformer block is one group. However, fusing consecutive transformer blocks may tradeoff more efficient communication at the cost of higher memory usage. Furthermore, this fusing need not be uniform; e.g. modules can be fused more aggressively in the middle of forward. The current workaround is for users to modify their `nn.Module` definition to include a dummy parent module of the modules to fuse. To support this natively in FSDP, there are at least two approaches: 1. Allow FSDP to take in a `List[nn.Module]` in its constructor 2. Add an explicit API to fuse modules with FSDP already applied We had some discussion already on Approach 1. There some challenges around implementing the module traversal to be able to pass the `List[nn.Module]` to FSDP. For example, any graph traversal (like in aut",2024-01-29T21:54:22Z,Stale release notes: distributed (fsdp2),closed,0,2,https://github.com/pytorch/pytorch/issues/118584,It is hard to maintain this PR on top of the stack if we are not sure to land it. I am going to delete it from my local branch and preserve this PR as an artifact for future reference.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,NCCL watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout," üêõ Describe the bug I got the following error when running supervised fine tuning of LLAMA27b model on P4 instance. Command that I ran: `deepspeed trainer_sft.py configs llama27bsftRLAIF  wandbentity tammosta  show_dataset_stats deepspeed` This is the error I got:  I tried creating a new venv and running the command there, but got the same error. I also tried the methods suggested https://github.com/ultralytics/ultralytics/issues/1439with no luck. Any idea what's causing this?  Versions transformers version: 4.35.2 Platform: Linux5.15.01050awsx86_64withglibc2.31 Python version: 3.10.12 Huggingface_hub version: 0.20.2 Safetensors version: 0.4.1 Accelerate version: 0.26.1 Accelerate config: not found PyTorch version (GPU?): 2.1.2+cu121 (True) Tensorflow version (GPU?): not installed (NA) Flax version (CPU?/GPU?/TPU?): not installed (NA) Jax version: not installed JaxLib version: not installed ",2024-01-26T06:45:58Z,oncall: distributed,open,0,7,https://github.com/pytorch/pytorch/issues/118369,"at face value this line  `[E ProcessGroupNCCL.cpp:475] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=880, OpType=BROADCAST, NumelIn=131137536, NumelOut=131137536, Timeout(ms)=1800000) ran for 1800648 milliseconds before timing out. ` means that a particular broadcast operation took longer than 30 minutes (1800sec) and timed out.  Broadcast is a collective, meaning each rank needs to participate.  If one rank forgets to join, other ranks wait for it (and will time out). notably, Rank 0 never prints any timeout error.  This may implicate rank 0 as the one that did not join the broadcast, so you could try to approach from that angle and investigate what rank 0 is doing instead.","  When I check GPU utilization while a finetuning is running, I see this:  It looks rank 0 is not being utilized. I'm not sure why this is happening. ",you could try using pyspy to attach to rank 0's PID and see what it's up to.  you may find that it's executing a different part of the program than you expected., Could you please suggest how to debug this in details? I don't know how to do what you're suggesting.,Running into the same problem. Any idea of how to debug? I have no clue based on the error log. ,"Honestly, it's going to be hard to debug this without some familiarity with the parallelism stack inside deepspeed. At a high level a collective timeout can be caused by 2 things 1) all the ranks called the right API but somehow the network or NCCL layer broke down and didn't complete it 2) *probably more likely* for some reason, one or more ranks did not call the right API at the right time We're assuming you're facing case (2) for now until proven otherwise.  Now the issue is you have to figure out a way to instrument deepspeed to see if/why/when it maybe didn't call a collective in the right order. I will also say that we are building a new Flight Recorder feature for ProcessGroupNCCL that will make this kind of debugging a lot easier.  Its just not quite ready for general use yet.  You could actually try using it, but you'd have to update to latest nightly pytorch and plan to do some extra work writing analysis scripts for the raw dump files it produces.",Thanks!! the most annoying thing is to figure out where/when/why the timeout happens. I have no ideas how to proceed. Anyway thanks for clarification...
transformer,[FSDP2] Added activation checkpointing tests,"  CC([no ci][RFC] Added `fuse_comm_groups` staticmethod)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added activation checkpointing tests)  CC([FSDP2] Added initial 2D and state dict tests)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Added backward prefetching)  CC([FSDP2] Added `reshard_after_forward`)  CC([FSDP2] Added pre/postbackward)  CC([FSDP2] Added reducescatter)  CC([FSDP2] Added forward unshard/wait for unshard/reshard)  CC([FSDP2] Added `_to_kwargs` root forward input cast)  CC([FSDP2] Added allgather and unsharded parameter)  CC([FSDP2] Added initial `_lazy_init` and FQNs for debugging)  CC([FSDP2] Sharded parameter in `FSDPParam`)  CC([FSDP2] Added initial `FSDPParamGroup`, `FSDPParam`, `ParamModuleInfo`)  CC([FSDP2] Added `mesh` arg, `FSDPState`, move to device)  CC([FSDP2][Reland] Introduced initial `fully_shard` frontend) This PR adds tests that `fully_shard` can use `torch.utils.checkpoint`, `_composable.checkpoint`, and `CheckpointWrapper` on a transformer. ",2024-01-24T17:17:01Z,oncall: distributed test-config/distributed release notes: distributed (fsdp2),closed,0,1,https://github.com/pytorch/pytorch/issues/118198,Squashed to save CI cost
transformer,[FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests,"  CC([no ci][FSD2] Added annotations for compute compile)  CC([FSDP2] Added pre/postallgather extensions)  CC([FSDP2] Generalized allgather outputs to >1 per parameter)  CC([FSDP2] Added gradient accumulation w/o reduction)  CC([BE] Enabled mypy in `common_fsdp.py`)  CC([FSDP2] Added mixed precision)  CC([FSDP2] Added autograd/memory/overlap/frozen/2D/AC tests)  CC([FSDP2] Replaced versionctx with `no_grad`; removed `no_grad`) This PR adds tests for autograd (mainly backward hooks), memory, overlap, and frozen parameters.  Autograd: unused forward output, unused forward module, nontensor activations (common in internal models)  Memory: expected GPU memory usage after init, forward, backward, and optimizer step  Overlap: communication/computation overlap in forward and backward  Frozen: expected reducescatter size, training parity This PR adds some initial 2D (FSDP + TP) training and model state dict tests. The only change required for model sharded state dict is to make sure parameters are sharded before save and load. This PR adds tests that `fully_shard` can use `torch.utils.checkpoint`, `_composable.checkpoint`, and `CheckpointWrapper` on a transformer. (I squashed all of these into one PR now to save CI cost.) ",2024-01-23T22:40:53Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/inductor release notes: distributed (fsdp2),closed,0,1,https://github.com/pytorch/pytorch/issues/118136,Todo: I think we can beef up unit tests more:  Add unit test to check unshard/reshard/hook events when composing with AC  Add unit test to check FSDPmanaged memory when composing with AC  Add unit test to check FSDPmanaged memory when using mixed precision  (Lower priority) add unit test to check FSDP composes with various module backward hooks
transformer,`torch.cuda.is_bf16_compatible()` output inconsistent with with TorchInductor support," üêõ Describe the bug Recent change for `torch.cuda.is_bf16_compatible()` is now labeling Turing (sm_75) and Volta (sm_70) cards as compatible with `torch.bfloat16` Tensors. Meanwhile, TorchInductor supports `torch.bfloat16` only for sm_80 or higher.  This has caused Transformer Engine CI tests that would normally skip on Turing and Volta nodes to instead crash with a `BackendComputerFailed` error from TorchDynamo. We are replacing the `torch.cuda.is_bf16_compatible()` condition with explicit checks on `torch.cuda.get_device_capability()` as a workaround. However, I wanted to file this issue here regardless, just in case it was an unintentional consequence and may be considered a bug.  Versions  ",2024-01-23T20:48:10Z,high priority triaged oncall: pt2 module: inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/118122, ,"If eager works with bfloat16 data types on CUDA, I would say that `torch.cuda.is_bf16_available() == True` makes sense. If I understand this correctly, the error only occurs with inductor. Maybe we should have something similar within inductor: `torch._inductor.is_bf16_available()`.  any thoughts?","Well, imo we should fix either Inductor or triton to produce working binaries rather than crash with unsupported instructions As I have a local setup, can try working on a fix",Hi from Colab! This is causing problems for users on Colab (example). The gist is that torch.compile w/bf16 on T4 causes some semiopaque errors. e.g.  Seems like poor UX to me. Any thoughts on the appropriate fix here? Should the fix be in triton or torch? My understanding is that XLA handles this by casting to f32 (e.g. https://github.com/openxla/xla/issues/12429),Considering this affects google colab marking as high pri until we find a way to unblock,"Ok, I think we need to separate whether something is ""supported""(read emulated) for eager vs compile. Though it does not look that those two changes are related, i.e. even if `is_bf16_supported` returns `False` compile still fails to produce the correct code. Also, this is not a regression, i.e. 2.2 had the same behavior, see https://colab.research.google.com/drive/1rIy_MJSUcdV8nQu_uuFZsniLR6uhS1BR?usp=sharing Where  Results in  "
transformer,Communication Computation Overlap issue with TransformerEngine + FSDP," üêõ Describe the bug I am training a large multinode transformer model with Pytorch FSDP, and am running into some issues while using TransformerEngine. I'm not sure if this is a Pytorch issue or a TransformerEngine issue, but any advice would be very appreciated. I will also crosspost on TransformerEngine! While using FSDP and TransformerEngine, I noticed that the communication/computation overlap is very poor. Looking at the traces, it seems that the TransformerEngine LayerNorm is actually waiting for the FSDP allGather in the forward pass:  If I use custom torch linear and layernorm layers, the overlap is much better:  In terms of iteration time too, the torch linear + layernorm significantly outperforms the TransformerEngine layer. When the computation per FSDP group increases (say, if I bundle multiple layers into a single FSDP unit) this problem gets a lot worse. Thank you very much for your time!  Versions Collecting environment information... PyTorch version: 2.1.2 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.28.1 Libc version: glibc2.31 Python version: 3.10.0  (default, Nov 20 2021, 02:24:10) [GCC 9.4.0] (64bit runtime) Is CUDA available: True CUDA runtime version: 12.3.107 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H1",2024-01-20T00:00:58Z,,closed,0,5,https://github.com/pytorch/pytorch/issues/117897,"Some more information on this: The flow events suggest that the FSDP allGather is just waiting due to rate limiting, which is fine and I assume is intended behavior. The weirdness comes from the fact that the LayerNorm from two layers ago is waiting for the allGather for the current layer to finish.  Would  have insight into this? (So sorry to tag on a Friday night. No rush to answer.) Although maybe it really is more of a TransformerEngine issue. Also if it is relevant at all I am using full activation checkpointing.",By any chance is the layer norm kernel being launched as a cooperative kernel (`cudaLaunchCooperativeKernel)`?,Yes it is actually,"I think that that might be the issue. Cooperative kernels require full GPU SM occupancy, so for a NCCL allgather that requires something like 2 SMs (assuming the allgather includes intranode), the cooperative kernel will wait for the allgather to finish in order to get the full occupancy. This might be something to ask the TransformerEngine team about or at least loop them in. ",Ahh thank you so much. Okay I will follow up on the TransformerEngine post. 
transformer,Failed to torch.export phi2 model," üêõ Describe the bug When trying to `torch.export` ""microsoft/phi2"" model, a `torch._dynamo.exc.InternalTorchDynamoError: ` was raised within dynamo converter.  I've enabled both `TORCH_LOGS=""+dynamo""` and `TORCHDYNAMO_VERBOSE=1`   Versions pytorch main branch  transformers==4.36.2 ",2024-01-19T23:09:45Z,onnx-needs-info module: dynamo oncall: export,closed,0,5,https://github.com/pytorch/pytorch/issues/117891, FYI,"I'm running into this error when running the model eagerly with the given inputs, which seems to match the error from dynamo: ",Maybe a transformers version issue? I am using `tarnsformers=1.36.2` and `model(**inputs)` does succeed on my local repro ,  It seems you did `model(inputs)` instead of `model(**inputs)`," Ah, that seems to work! Since it's a kwarg then it should be passed into export as a kwarg: `torch.export.export(model, args=(), kwargs=inputs)` which seems to work for me :> "
transformer,llama_v2_7b_16h stopped working with torch.jit.trace," üêõ Describe the bug We've noticed this behavior on torchbench using torchscript as backend on PyTorch main branch. As we didn't identify the offending PR, my concern is if that regression was merged into PyTorch 2.2 or not. I am still working on a local repro, but it should be something similar to  The error is   Unfortunately I didn't repro locally and that is all that is informed by torchbench report. The last time we are sure the model was exporting with torch.jit.trace was around 12/16, when we locally executed torchbench https://github.com/pytorch/benchmark/pull/2121 is a proposal (not tested) for working this around, but we still need to understand   Versions pytorch main ",2024-01-18T16:11:22Z,module: third_party onnx-needs-info,closed,0,9,https://github.com/pytorch/pytorch/issues/117752,"I assume it's one of the transformers changes, which no longer correctly decompose... Though it fails the same way for me with 2.1.2 and transformer==4.36.2, but downgrading to 4.32.1 makes it work as expected",We should probably should just propose a ternary to transformer to use older path...,> We should probably should just propose a ternary to transformer to use older path... Have you discussed that with HF folks? Is there an issue we can track? I was wondering whether https://github.com/pytorch/benchmark/pull/2121 could be a fix on our end,"Issues related to SDPA have been raised three times in ONNX Runtime already. 1. https://github.com/microsoft/onnxruntime/pull/19015discussion_r1442432173 2. https://github.com/microsoft/onnxruntime/issues/19051 3. https://github.com/microsoft/onnxruntime/issues/19040 It was fixed here for LLaMA, but similar issues will likely be raised for other models. It would be helpful if the eager attention implementation is traced by default during the export of Hugging Face models.",> > We should probably should just propose a ternary to transformer to use older path... >  > Have you discussed that with HF folks? Is there an issue we can track? >  > I was wondering whether pytorch/benchmark CC(Bug fixes) could be a fix on our end   gentle ping,related discussion https://github.com/huggingface/transformers/issues/28610,> We should probably should just propose a ternary to transformer to use older path...  proposed this change: https://github.com/huggingface/transformers/pull/28823,A more proper fix will be done in part of https://github.com/huggingface/transformers/pull/27931,https://github.com/huggingface/transformers/pull/27931 has been merged this morning. We can close this one now
transformer,[ONNX] Bump transformers in CI test,"Fixes CC([ONNX] transformers version on CI is outdated)  (1) skip dynamic tests for exported program in `test_fx_to_onnx_onnxruntime.py`, as they are not expected to pass anyway. (2) Move dolly model to runtime, since it's working in exporting, but it is blocked by nonpersistent buffers as well. (3) openai whisper has changed/regression due to modeling modifications. (4) Replace OpenLlama with Llama, because OpenLlama is deprecated.",2024-01-17T23:10:51Z,module: onnx triaged open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/117703,Looks like openai_whisper had some changes. FLAKY fail seems unusual.,PTAL, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[ONNX] transformers version on CI is outdated,"Currently, the transformers version on ONNX CI is still 4.32.1, while the released one is 4.36.2. There are lots of Config/Modeling changes that the outdated tests potentially make our guarding less robust, and it also increases the difficulty of developing/maintaining transformers related models of conversion. The version should be bumped.",2024-01-17T17:13:40Z,module: onnx triaged onnx-triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/117660, ,That is a good point. this is where we pin it https://github.com/pytorch/pytorch/blob/da6abaeeacedc872ada6577fa1dd0c6c8024188a/.ci/docker/common/install_onnx.shL29,"ps: when the issue is opened from onnx to onnx, and thus don't need Meta's triaging the issue, we can also add the `triaged` label :)","Should we just update it, or we want to discuss for another approach?"
transformer,Extra long allGathers when scaling up FSDP on 7b model," üêõ Describe the bug I am training a multinode 7B parameter transformer model and running into some issues with FSDP. Training starts out going pretty fast, but then slowly and up to the first validation, the MFU tank, after which it stays low:  Looking at the trace, I see that some allGathers take much longer than others, and of course the main GPU stream waits for it:  I identified that for each of the long allGathers, there is a single straggler rank that is actually doing no GPU work during the allGather:  This is what the CPU workload looks like for the straggler (the _pre_forward call when the straggler is doing no GPU operations is highlighted):  Does anyone know why this may be? I don't think it's a data issue, as it happens in the middle of the network. I suspected a memory issue as the reserved memory is at 78GB, but I have also observed this behavior with models with smaller peak reserved memory, and also I print ""num_alloc_retries"" from each rank and don't observe any meaningful pattern.  Versions  ",2024-01-16T16:07:28Z,triaged module: fsdp,closed,1,3,https://github.com/pytorch/pytorch/issues/117550,Perhaps  is the right person? Thank you very much!,You might want to try using manual Python GC.  This has been a known major cause of stragglers like this for us.,Wow that was it thank you!
transformer,[PT2] Failed to capture graph in export with dynamic shape inputs for LLM models," üêõ Describe the bug We aim to apply `pt2e quantization` for LLM models, but encounter challenges in capturing graphs with inputs of  dynamic shape despite setting constraints in `capture_pre_autograd_graph`. In LLM models, sentence generation is performed using `model.generate()`, which internally invokes `model.forward()` multiple times to generate tokens. The issue arises from the dynamic changes in input shape with each call to `model.forward()` inside `model.generate()`. Assume that the size of the prompt is `x`, batch size is `bs`, number of beams for beam search is `nb`, and input shape change rule is as follows:  Initial inputs shapes (for first token):    Input_ids: `(bs, x)`    Attention_mask: `(bs, x)`  For ith next tokens:    Input_ids: `(bs\*nb, 1)`    Position_ids: `(bs\*nb, 1)`    Attention_mask: `(bs*nb, x + i + 1)`    Past_key_values: `num_hidden_layers\*((bs\*nb, num_attention_heads, x+i, head_dim),(bs\*nb, num_attention_heads, x+i, head_dim))` As shown in the input shape change rule, some shapes need to be treated as dynamic shapes when capturing graph so that the captured graph can be applied for all tokens generation.  To make things easier, we tried to capture two graphs, one for the first token, and one for the next token. And it turns out that we can succeed to capture the graph for first token and it works fine. However, we are unable to capture expected graph for next tokens.   Unit Test Here is a unit test that reproduces the issue I met.   Error Message [20240111 17:44:58,216] [0/0] torch._guards: [ERROR] Error while creating guard: [20240111 17:44:58,216] [0/0] torch._guards: [ERROR] Name: '' [20240111 17:44:5",2024-01-15T05:48:39Z,triaged oncall: pt2 module: dynamic shapes export-triaged oncall: export,open,0,22,https://github.com/pytorch/pytorch/issues/117477,"Hi , could you kindly help to take a look of this issue? Appreciate if any suggestions for how to capture the fx graph for next token cases with input of dynamic size.","To capture a model that works for sizes 0/1, you should trace with size 2 or larger. This is because 0/1 sizes will induce specialization. If you are lucky, you will end up with a model for >= 2 that will happen to work for 0/1. When we work with sizelike unbacked symints, we never assume that they are equal to 0/1 for similar reasons."," Thanks for your suggestion. We tried changing the size (past_key_values[i][0].size()[2]) to 10, but unfortunately, we still unable to capture expected graph because **past_key_values[i][0].size()[2] was inferred to be a constant (10).** **Reproduce**: https://gist.github.com/blzheng/c890d3744913820392fa3928eb8061d3fileut **Error info**: https://gist.github.com/blzheng/c890d3744913820392fa3928eb8061d3fileerror","OK, that's progress. The next step is to examine why the size was inferred to be constant. This can be done using TORCH_LOGS=dynamic and looking for the log line corresponding to the guard on this source.",this is kv cache right?  is trying to refactor it to be the input of the model I think,"Oh, is the problem the kvcache is dynamic but also a module attribute? You know that we can change PT2 to allow for nonstatic module buffers right? You don't have to refactor unless you really don't want to touch some Dynamo code (but the policy decision here isn't even that hard...)","> Oh, is the problem the kvcache is dynamic but also a module attribute? You know that we can change PT2 to allow for nonstatic module buffers right? You don't have to refactor unless you really don't want to touch some Dynamo code (but the policy decision here isn't even that hard...) oh I don't know the details, the reason we are refactoring this for executorch is because executorch itself does not support mutable buffers anyways right now (it will be added in a few weeks). if mutable buffers is supported in dynamo already, then I think we could try to export this. in the end, we want to just export this directly I think  ","> OK, that's progress. The next step is to examine why the size was inferred to be constant. This can be done using TORCH_LOGS=dynamic and looking for the log line corresponding to the guard on this source. This is the log we captured using TORCH_LOGS=dynamic. https://gist.github.com/blzheng/c890d3744913820392fa3928eb8061d3filelog  We do not understand why `set_replacement s0 = 10` appears. The transformers version is v4.35.2.","It's the line below:  Which is  This is all looking familiar to me. Here is a similar problem in HF that I debugged, maybe you can do the same process:  CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0)issuecomment1734754159","Hi , thanks for the suggestions. I think we may know why `past_key_values[i][0].size()[2]` was inferred to be a `constant (10)` as `[20240116 17:20:36,024] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] eval Eq(s0 + 1, 11)`:  We didn't add the constraints that `attention_mask.size(1)` to be a dynamic size. So, it has specialized to `constant(11)`.  `attn_weights + attention_mask` add the constraints that `attn_weights` and `attention_mask`(`constant(11)`) has same size. And `attn_weights` comes from https://github.com/huggingface/transformers/blob/514de24abfd4416aeba6a6455ad5920f57f3567d/src/transformers/models/gptj/modeling_gptj.pyL248, which the size of `key` comes from `example_inputs[""inputs_id""]` which is with `constant(1)`. I think we may add the constraints that `attention_mask.size(1)` to be a dynamic size to fix this issue. After that, we got some other errors:  Reproduce: https://gist.github.com/lesliefangintel/b1e5c9c5d331c1a0e82fbbd644fe967afilefileut  Error Msg: https://gist.github.com/lesliefangintel/b1e5c9c5d331c1a0e82fbbd644fe967afileerrormsg    We see 1 line from the log which looks suspicious (`Some dynamic dimensions need to be specialized because the constraints inferred for them are too complex to specify.`).    Now we have saw some hint for how to add the constraints at the end of the log file which I think we already added.","fangintel Looking at the error message, it seems like when you made some dimension dynamic, it interacted with another dynamic dimension but not in a way that is currently expressible (we can express that one dynamic shape is equal to another, but no other nontrivial relationship): here one dynamic shape = another + 1, can you confirm? Curiously we are increasingly seeing examples of such requests (e.g., `s1 = s0 + 1`, `s1 = s0 * 4`, etc.) so I'm actually working on a prototype to support it. No promises yet but I'll update when I make some progress.   ","> You don't have to refactor unless you really don't want to touch some Dynamo code (but the policy decision here isn't even that hard...)   Executorch cant handle mutable buffers yet regardless of what export policy is. My understanding is that with a lifted graph of mutable state youll see the state as IO, and then the copy back from output to input placeholder happens outside the graph. That copy back is the problem for ExecuTorch to handle right now because theres no guarantee we can dereference the memory backing the tensor. So for now we are forcing the user to lift it to IO and then figure out the copy over themselves. I dont want export to implicitly do this lifting silently, and then have us not be able to hide it as changing model io is a super negative user experience. ","> fangintel Looking at the error message, it seems like when you made some dimension dynamic, it interacted with another dynamic dimension but not in a way that is currently expressible (we can express that one dynamic shape is equal to another, but no other nontrivial relationship): here one dynamic shape = another + 1, can you confirm? >  > Curiously we are increasingly seeing examples of such requests (e.g., `s1 = s0 + 1`, `s1 = s0 * 4`, etc.) so I'm actually working on a prototype to support it. No promises yet but I'll update when I make some progress. >  >   , yean, I think that's the problem. Actually, I have tried to write some constraints as:  to workaround this issue which turns out not work also.","I just wanted to point out that it's a relatively simple code change in pytorch to disable the export constraint solver () and this won't break export, and that I know people internally have been doing this to work around constraint solver insufficiencies.","this is assigned, so adding the triaged label to remove it from the query of untriaged issues", updates on this?,"  What is the current status of this issue? BTW, I am curious about why we see such error `Some dynamic dimensions need to be specialized because the constraints inferred for them are too complex to specify` in  CC([PT2] Failed to capture graph in export with dynamic shape inputs for LLM models)issuecomment1895212019.  ","I am able to capture the graph of GPTJ with this code: https://gist.github.com/XiaWeiwen/81105535ab11bf8098e850fac7a90a34 The key part is the `dynamic_shapes`:  torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (dim2)! For more information, run with TORCH_LOGS=""+dynamic"".    Not all values of dim2 = L['past_key_values'][0][0].size()[2] in the specified range dim2 <= 2048 satisfy the generated guard Ne(2048, L['past_key_values'][0][0].size()[2] + 1).    Not all values of dim2 = L['past_key_values'][0][0].size()[2] in the specified range dim2 <= 2048 satisfy the generated guard 2 <= L['past_key_values'][0][0].size()[2] and L['past_key_values'][0][0].size()[2] <= 2047 Suggested fixes:   dim2 = Dim('dim2', max=2047) ``` I am wondering why we should use this range of `max` values. And I am still not sure if this behavior of export is expected. Are there any docs on how to set max values?   Thanks","Hi   I saw the docs for `dynamic_shapes` of `export` here: https://pytorch.org/docs/stable/export.htmlmoduletorch.export. Looks like the trialanderror method is the recommanded: (1) prepare `dynamic_shapes` with `Dim`, (2) check the error message if any, (3) modify `Dim`'s `max` according to the error message, (4) repeat until no error occurs. This works in this case but it's quite confusing. Do you plan to elaborate more on the docs of `export` with `dynamic_shapes`? Thanks.","That's not the intended workflow. The intended workflow is you *know* what sizes you're intending to send to the model, and then you trace under that range and if export fails, that's export telling you that it couldn't quite figure out if it had successfully generated a model that works for everything in the range, because there was a guard. And then, you figure out who was causing the specialization, typically by using TORCH_LOGS=dynamic","Hi  Weiwen fangintel, I assume your latest repro has been changed to https://gist.github.com/XiaWeiwen/81105535ab11bf8098e850fac7a90a34 (after we deprecate the constraints API) and the remaining question on this issue is: ""How to find the correct max value to set for Dims? "" If so, are you able to find the place causing max <= 2048 with TORCH_LOGS=dynamic?  If not, feel free to point out any particular repro/issue you want us to look at, since it's not super obvious now what is the latest repro/error pair you got after 6 months when we change a lot of dynamic shape logic internally. (btw, it shouldn't affect anything but just want to point out that capture_pre_autograd_graph is also deprecated, we recommend to use torch.export.export() moving forward) Thanks!",Thanks  for the comment.  could you help to take a try a again for this issue since the constraints API has changed?
transformer,CHECKPOINT_PREFIX is not stripped when non-root module is activation checkpointed," üêõ Describe the bug When training large models, we often want to activation checkpoint something smaller than the wrap module for FSDP. For example, we might want to only activation checkpoint attention in a transformer block. Unfortunately, when calling `get_state_dict` with the new distributed checkpoint interface, the _CHECKPOINT_PREFIX from checkpoint wrapper is not properly stripped when we activation checkpoint submodules.  We have to monkeypatch torch here to strip this always.  Versions Torch 2.1.2 / Nightly for Torch 2.2 ",2024-01-12T21:38:25Z,oncall: distributed module: fsdp,closed,0,0,https://github.com/pytorch/pytorch/issues/117399
transformer,Logging when executing fx.Interpreter, üêõ Describe the bug Something like this:   Versions main ,2024-01-12T13:25:02Z,good first issue module: logging triaged oncall: pt2,open,0,10,https://github.com/pytorch/pytorch/issues/117351, Do you want to log every function?,Yes,I'll see what I can do,  How detailed should the logging be? Just when every function is called with which parameters? Or should it include more than this?,"My ideal print is what you would be the line of code as if the function call was generated to Python.  So, e.g., show me something like  It is probably unwise to actually print the contents of `x_1`, since it might be quite large."," Right now I have these log.debug statements like `log.debug(f""Execute call_function node {target} with {args} and {kwargs}"")` just like above. Is this satisfactory? p.s. It's my first time contributing to open source, hope I'm not misunderstanding or doing it really wrong lol","Well, take a look at the output; does it look good?","I tried testing it, but failed. Tried running the test_torch.py file but it just says it failed to import torch. I think this is because I have to build it first, however I tried but failed.  After running `python setup.py develop`, I get this error:  Probably because I don't have cuda, however I cannot seem to disable this. I can see in the development tips that it should be possible to skip cuda, but I don't know how.","For something like this, you don't need to build PyTorch from source, you can use tools/nightly.py to get working binaries for your dev copy.","I can't seem to either pull or checkout, It gives me a FileNotFoundError.  Conda is installed and in PATH, not sure how to fix it."
transformer,"torch.onnx.export the torch,nn.TransformerEncoderLayer, the dynamic_axes work not right"," üêõ Describe the bug  the bug description When I use the tensor shape like this (batch_size, seq_len, embedding_size) and i put the batch_size, seq_len in dynamic_axes to generate onnx model, I cannot use the onnx model to handle data with different seq_len, as you can see, the *code* is as follows:   the *error message* is like this:   Versions I found it maybe one bug in the `multi_head_attention_forward` function in the `torch/nn/functional.py`, as I change the   and  in the `multi_head_attention_forward` into   and  The bug will fix and I can get the seq_len changable. I guess the torch.onnx.export changes the shape[0] to a static item or the v, k shape[0] may not be as same as tgt_len so the changes I made were wrong. Thanks for your help!",2024-01-11T03:23:36Z,module: onnx triaged,open,4,3,https://github.com/pytorch/pytorch/issues/117209,I am experiencing the same problem when trying to `onnx.export` with `torch==2.2.1` and `onnx==1.15.0`. Minimal repro below:  Without `dynamic_axes`:  With `dynamic_axes`: ,This seems to be a dynamic shape issue Please try the new ONNX exporter and let us know if that works for you: quick torch.onnx.dynamo_export API tutorial,"Sorry, but the Dynamo export API throws an error, and the log is literally empty! Any updates on this issue? "
transformer,DISABLED test_gradgrad_nn_Transformer_cuda_float64 (__main__.TestModuleCUDA),Platforms: linux This test was disabled because it is failing on main branch (recent examples). The test is too slow and is causing the test suite to timeout running in slow gradcheck mode https://hud.pytorch.org/pytorch/pytorch/commit/8bcdde5058658cc193c94a7f1eb16660553dc35a ,2024-01-10T19:57:25Z,module: autograd triaged skipped actionable,open,0,2,https://github.com/pytorch/pytorch/issues/117140,,"This test has a lot of samples (32) and for each sample, the number of input elements is (48  63). We should disable slow gradcheck for this test."
transformer,Skip test_modules.py::TestModuleCUDA::test_gradgrad_nn_Transformer_cuda_float64,Takes 30+ minutes while doing slow grad check,2024-01-10T17:18:09Z,topic: not user facing ciflow/slow,closed,0,0,https://github.com/pytorch/pytorch/issues/117123
transformer,_dynamo fails with abc.__file__," üêõ Describe the bug Hello, I was working on deploying a Huggingface model to a Vertica database instance and stumbled across the following error: Exception: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback): Failed to import transformers.integrations.peft because of the following error (look up to see its traceback): module 'abc' has no attribute '__file__' Traceback: Traceback (most recent call last):   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/transformers/utils/import_utils.py"", line 1382, in _get_module     return importlib.import_module(""."" + module_name, self.__name__)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/vertica/oss/python3/lib/python3.11/importlib/__init__.py"", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File """", line 1206, in _gcd_import   File """", line 1178, in _find_and_load   File """", line 1149, in _find_and_load_unlocked   File """", line 690, in _load_unlocked   File """", line 940, in exec_module   File """", line 241, in _call_with_frames_removed   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/transformers/integrations/peft.py"", line 29, in      from accelerate import dispatch_model   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/accelerate/__init__.py"", line 3, in      from .accelerator import Accelerator   File ""/home/dbadmin/nlplib/venv/lib/python3.11/sitepackages/accelerate/accelerator.py"", line 35, in      from .checkpointing import load_accelerator_state,",2024-01-10T14:00:31Z,triaged oncall: pt2 module: dynamo dynamo-must-fix,closed,0,0,https://github.com/pytorch/pytorch/issues/117109
transformer,torch.compile leads to OOM with different prompts.," üêõ Describe the bug Given a transformer language model compiled with:  where the bulk of the task is computing next token logits for different prompts (MMLU), memory usage grows until reaching OOM. Without `torch.compile`, there is no OOM. I think this happens because different prompts are different lengths, so `torch.compile` records different paths for each different size, which eventually leads to an OOM. It's a Llama2 model with a static kvcache (the model from https://github.com/pytorchlabs/gptfast/blob/main/model.py). I call `model.setup_caches(max_batch_size=1, max_seq_length=model.config.block_size)` so the caches are not being adjusted ever. Is there a way to set up the prompt to always be `model.config.block_size` tokens long, then mask out irrelevant tokens, so that there is only one path through model.forward? Or should I avoid torch.compile and setup batching to achieve a speedup?  Error logs N/A  Minified repro N/A  Versions Collecting environment information... PyTorch version: 2.3.0.dev20240109+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.11.1 (main, Mar  8 2023, 10:58:11) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0169genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 GPU 4: NVIDIA RTX A",2024-01-10T13:28:11Z,needs reproduction module: memory usage triaged oncall: pt2,closed,0,7,https://github.com/pytorch/pytorch/issues/117106,Does it OOM without reduceoverhead?,Yep.,Do you have a runnable repro script that shows the OOM?,"Which Llama2 model are you using, and which command did you run gptfast with?  I see that your system has multiple RTX A6000s, you can also try enabling tensor parallel: https://github.com/pytorchlabs/gptfast/?tab=readmeovfiletensorparallelism1. > Is there a way to set up the prompt to always be model.config.block_size tokens long, then mask out irrelevant tokens, so that there is only one path through model.forward? Or should I avoid torch.compile and setup batching to achieve a speedup? This might be best answered on the gptfast repo.","Looks like we missed marking this as triaged, so doing that now."," I'm helping the team scrub old issues and trying to come up with a repro here, but have so far failed (with gptfast). Is this still an issue for you? If so, could you provide the command?","I'm going to assume this is fixed, but please reopen if it's still an issue."
transformer,Wrapping multiple layers in Pytorch FSDP," üöÄ The feature, motivation and pitch Is it possible to wrap multiple nn.Modules (e.g., every two Transformer layers) with FSDP API to control communication and memory overlap at a more granular level?  cc:   ",2024-01-08T19:55:03Z,triaged module: fsdp,closed,2,10,https://github.com/pytorch/pytorch/issues/116986,"Thanks for the issue! This is a good ask. The TL;DR is that we probably cannot support it with existing FSDP, but we may be able to support it in our upcoming FSDP rewrite in the future ( CC([RFC] Per-Parameter-Sharding FSDP)). I will leave another comment with more indepth discussion on this a bit later. I am finishing up some other work.","Thanks  for the prompt response. Do you have a suggested approach for this? I guess I can wrap my layers in another nn.Module, but it feels very hacky. I really like the proposal you linked, particularly the support for Zero++ and  improved state_dicts! Super curious if you have time estimates there! I will try leave comments once I have more time to read it. "," The approach I have seen for advanced users that own the model code is what you said  wrap every `k` transformer blocks into a parent module that acts like a 'sequential' over its `k` children. It affects `state_dict` compatibility though (since the keys change), which may block its viability depending on your use case.","For timeline, we are goaling internally on having the rewrite as a prototype feature by the end of this half (so ~6 months). We should land core components in the next month or two. I will provide a more formal update on the issue later. The main immediate work items are around: (1) breaking up the implementation into PRs for review/landing, (2) formalizing the design for how to extend FSDP (e.g. fp8 allgather, QLoRA), and (3) landing custom kernels for fast allgather copyout/reducescatter copyin. ",Sounds good! Thanks again. ,"Sorry for the delay. Here are some additional thoughts: One way to address this issue is to allow `FSDP(List[Module])` instead of only `FSDP(Module)`. A concern is how will the user express/configure this. FSDP offers auto wrapping (`auto_wrap_policy`) as a syntactic sugar for manual wrapping (i.e. directly calling `FSDP` on each module). Auto wrapping applies FSDP using a given predicate (the wrap policy) following a depthfirst traversal over the root module. If we allow for `FSDP(List[Module])`, it is not obvious how:  Auto wrapping would accommodate this tractably since we need a traversal that, in the most general case, applies the predicate over every combination of modules (one option could be to consider every consecutive `k` modules, where ""consecutive"" follows `root_module.modules()` order)  Manual wrapping users would translate from a config file to actually passing in the `List[Module]` to FSDP Another way to address this is to expose some new API to allow for merging existing FSDP instances. This is also another option but also complicates the API and testing. We should be confident that such an API is necessary if we add this. (Finalizing the FSDP configuration at construction time helps with reasoning about it and debugging, which is one reason that we are not a big fan of rebucketing at runtime.)"," Do you have any preference on whether we allow `FSDP(List[Module])` vs. having a separate API like `FSDP.fuse(*modules)` to fuse already initialized FSDP modules? See https://github.com/pytorch/pytorch/pull/118584 for an example. I was leaning toward the latter (though it would introduce some more complexity internally to FSDP) because it allows the initial ""wrapping"" policy to be fixed while the fusing scheme could be tuned. Moreover, the separate API could mainly be targeting expert users without confusing general users as to why FSDP can take in a `List[Module]`.","> Another way to address this is to expose some new API to allow for merging existing FSDP instances. This is also another option but also complicates the API and testing. We should be confident that such an API is necessary if we add this. (Finalizing the FSDP configuration at construction time helps with reasoning about it and debugging, which is one reason that we are not a big fan of rebucketing at runtime.) Hello, Is there any updates? I would like to wrap multiple `T5Block` with single FSDP unit, but cannot find any solutions..","Hi ! We do not have this functionality yet, but I am experimenting with this again. I am leaning back toward allowing users to pass in a `List[nn.Module]` to FSDP, and we can wrap them together (e.g. your `T5Block`). The draft is in https://github.com/pytorch/pytorch/pull/127786.","> Hi ! We do not have this functionality yet, but I am experimenting with this again. >  > I am leaning back toward allowing users to pass in a `List[nn.Module]` to FSDP, and we can wrap them together (e.g. your `T5Block`). The draft is in CC([FSDP2] Allowed `List[nn.Module]` as arg). I see!, thanks for quick and kind reply."
transformer,Compiled Transformer AOT Autograd graph Accuracy Failed," üêõ Describe the bug This is a WIP version of https://github.com/lucidrains/xtransformers that has zero graph breaks. It converges as expected when not compiled, but when compiled the loss curve is much worse. Repro fails accuracy as expected with random data so I didn't upload the checkpoint. repro.py.txt Edit: Smaller repro file: https://github.com/pytorch/pytorch/files/13851673/repro.py.txt The above fails, but I cannot minify further due to:  The issue seems to go away (only tested for a minute or so) when I turn off 16bit mixed precision and only use fp32, but of course everything is half speed due to limited memory bandwidth.  Error logs [20240106 18:15:32,526] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph [20240106 18:15:46,128] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs [20240106 18:15:46,910] torch._dynamo.utils: [ERROR] Accuracy failed: allclose not within tol=0.001 [20240106 18:15:46,912] torch._dynamo.repro.after_aot: [WARNING] Accuracy failed for the AOT Autograd graph model__19_backward_57 [20240106 18:15:47,003] torch._dynamo.repro.after_aot: [WARNING] Writing checkpoint with 4892 nodes to themodel/torch_compile_debug/run_2024_01_06_18_15_47_002351pid_2494068/minifier/checkpoints/4892.py themodel/env/lib/python3.11/sitepackages/torch/_dynamo/eval_frame.py:412: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect Epoch 0/2 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/ 0:04:29 ‚Ä¢ :: 0.00it/s v_num: epro train/loss: 0",2024-01-06T23:29:26Z,high priority needs reproduction triaged ezyang's list oncall: pt2 module: inductor module: pt2-dispatcher,closed,0,64,https://github.com/pytorch/pytorch/issues/116935,The repro as uploaded fails for me with: ,I tried running the repro script above to confirm and it fails in eager mode too.  can you double check your repro?,> The repro as uploaded fails for me with: >  >  Weird. Which version of PyTorch are you running this with? I know they changed the Flash Attention implementation to 2 within the last few months. ,"I""m using a recent nightly  do you mind trying it out on a nightly as well? https://pytorch.org/getstarted/locally/","> I""m using a recent nightly  do you mind trying it out on a nightly as well? https://pytorch.org/getstarted/locally/ I tested with a recent nightly before I submitted this bug to make sure the issue hadn't been fixed already. I can test again with different attention heads/etc but can't today because plane internet can't seem to handle an SSH connection. Maybe find and replacing all the flash attention calls to the fallback would work?","Also, the model isn't using multi query attention or anything that would change the number of heads for the different matrices."," I looked at your repro in more detail. It looks like the minified repro was bungled: I extracted out the first ~300 lines of the repro, and some of the intermediate tensors start to produce inf/nan values. It's entirely possible that these nans/infs were initially caused due to a torch.compile bug, but given that the repro script itself produces nans/infs (even when run in eager mode), that repro script doesn't look like it'll be enough to root cause some more. repro pasted here (you can see where I print before/after the last op in the graph  there's an `aten.mm` call where the input is a `fp16` tensor with a very large value (37984), and running the fp16 matmul returns a tensor with infs in it): https://gist.github.com/bdhirsh/061727e285d3f5b106d1358170df077a (prints the below)  do you have a link to the original source code where you're seeing numeric differences? (preferably with repro instructions :) )",">  I looked at your repro in more detail. It looks like the minified repro was bungled: I extracted out the first ~300 lines of the repro, and some of the intermediate tensors start to produce inf/nan values. It's entirely possible that these nans/infs were initially caused due to a torch.compile bug, but given that the repro script itself produces nans/infs (even when run in eager mode), that repro script doesn't look like it'll be enough to root cause some more. >  > repro pasted here (you can see where I print before/after the last op in the graph  there's an `aten.mm` call where the input is a `fp16` tensor with a very large value (37984), and running the fp16 matmul returns a tensor with infs in it): >  > https://gist.github.com/bdhirsh/061727e285d3f5b106d1358170df077a (prints the below) >  >  >  > do you have a link to the original source code where you're seeing numeric differences? (preferably with repro instructions :) ) I could share the code (which is currently private but nothing too fancy/secret) but not the dataset. The weirdest thing about this process for me was I tried taking just the model and running a forward/backward pass with random inputs and for one iteration everything matched up between compiled/noncompiled. Also, during training the model performed significantly worse but the loss didn't go NaN or infinite as would be expected if what you mentioned were happening during training. What might have gone wrong with the repro and how should I resolve it? Could it have been using a cached compiled model from another PyTorch version or something? Also, could the fact that it's failing for eager and compiled imply it's actually a torch dynamo bug?",  repro.py.txt Here is another repro run from a completely fresh A100 instance running the latest nightly.," Further investigation implies the problem goes away when I disable PyTorch's Flash Attention (the accuracy checker spins for much longer... maybe forever?). This aligns with what I said before about how disabling mixed precision also fixed it (since FA 2 requires either FP16 or BF16 tensors so PyTorch's version prob falls back to no FA). EDIT: However, casting Q, K, and V to fp16 before sending them into the PyTorch attention function doesn't solve the accuracy issue. Here's are my modifications to the transformer I'm using: https://github.com/Sonauto/xtransformers You can funnel random data through it like this: ","Hmm. I installed your `xtransformers` package locally and ran that script, but I modified it slightly to set randomness state and compare the output between compile and eager:  This prints:  A delta of 1e6 doesn't seem... too bad. Is the latest snippet you posted enough to reproduce the fact that your model fails to converge? (Maybe if you use try to train your `ContinuousTransformerWrapper` over many iterations?)","Hmm... although when I run with `backend=""aot_eager""`, I get no difference:  While running with `compile(backend=""aot_eager_decomp_partition"")` gives me a small difference:  Which means that one of the inductor decomps is a possible culprit",Looks like disabling inductor's `native_layer_norm` decomp brings the difference from eager back to zero: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 Maybe we're forgetting to treat some intermediates in higher precision in the decomp,> Looks like disabling inductor's `native_layer_norm` decomp brings the difference from eager back to zero: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 >  > Maybe we're forgetting to treat some intermediates in higher precision in the decomp When I ran the little test script I sent you for one iteration I got the same results as you (also checked gradients but didn't include that). The divergence only gets crazy when training over many iterations. I inserted an intentional graph break (using unsupported context manager) in the call to the PyTorch Flash Attention function and the training no longer diverges (but the tradeoff is as many graph breaks as there are attention layers which makes the efficiencyobsessed part of me sad).," maybe just for sanity  do you think you could try running your E2E script while also commenting out this line from inductor (effectively removing the decomp), and seeing if that ""fixes"" the convergence issue? https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 That will at least tell us if this decomp is the root cause and needs a careful look",">  maybe just for sanity  do you think you could try running your E2E script while also commenting out this line from inductor (effectively removing the decomp), and seeing if that ""fixes"" the convergence issue? https://github.com/pytorch/pytorch/blob/main/torch/_inductor/decomposition.pyL59 >  > That will at least tell us if this decomp is the root cause and needs a careful look I commented this line, and removed the intentional graph break workaround I added earlier around PyTorch's Flash Attention and it still diverges. I ran `torch._dynamo.reset()` before staring the training script and instantiating the models to ensure it wasn't using a cached compiled version, but lmk if there are any other methods I should use to ensure it isn't using a cached model (which it shouldn't in this case anyway as this was a fresh A100 VM). I also ensured the comment was actually taking effect by printing before it was executed (so I didn't edit the wrong environment or something). Repro: repro.py.txt", I figured out how to reproduce it in one step with the above script! Needed to use AMP:  **note:** you must use commit db394f7 from my xtransformers fork. The later commit includes the workaround.,"Thanks for the updated repro  your right  this one gives a much larger difference. From some digging, it looks like this is still an issue with the `native_layer_norm` decomposition. When I comment it out in the inductor code, I no longer see an accuracy difference. I'll keep digging","> Thanks for the updated repro  your right  this one gives a much larger difference. >  >  >  > From some digging, it looks like this is still an issue with the `native_layer_norm` decomposition. When I comment it out in the inductor code, I no longer see an accuracy difference. I'll keep digging With native_layer_norm disabled do you still see a divergence in the gradient? I can double check myself within an hour, but it seems weird in this case that disabling the native_layer_norm decomposition didn't solve my divergence issue with the actual training run.","> Thanks for the updated repro  your right  this one gives a much larger difference. >  > From some digging, it looks like this is still an issue with the `native_layer_norm` decomposition. When I comment it out in the inductor code, I no longer see an accuracy difference. I'll keep digging I just tried to reproduce this fix myself and wasn't able to (which is a good thing as that means I'm probably just doing something wrong!). This is the only code segment in decomposition.py I changed:  Was I supposed to change something else?","Hmm nope you're right, I'm seeing that too. When I comment out the decomp, I now see good accuracy with `backend=""aot_eager_decomp_partition""`, but I still see an accuracy difference with the inductor backend. I'll keep looking around.",I don't yet understand the inner workings of inductor all that deeply but is there some way I can disable optimization of the flash attention call (since I suspect that's where the issue is based on its disappearance when I disable it) without creating a graph break? ,Cc  ,"Try not using the context manager and instead use these functions directly: https://pytorch.org/docs/stable/backends.htmltorch.backends.cuda.enable_flash_sdp And set this to false, I think that this doesn't cause graph breaks ","> Try not using the context manager and instead use these functions directly: >  > https://pytorch.org/docs/stable/backends.htmltorch.backends.cuda.enable_flash_sdp >  > And set this to false, I think that this doesn't cause graph breaks This would remove the graph break but losing flash attention is a much bigger perf/VRAM hit than the graph breaks. When I said ""disable optimization of the flash attention call"" I meant any further optimization Inductor is doing on top of the eagermode version that I suspect is breaking things, but not disabling it entirely. Thanks for the info anyway, though!", Has enough data been collected where I can set my repo private again?,TBH this seems to be more of a PT2 question rather than directly attributable to flashattention. Do you see numeric issues when in eager? Otherwise I think  might be best to answer here,"> TBH this seems to be more of a PT2 question rather than directly attributable to flashattention. Do you see numeric issues when in eager? Otherwise I think  might be best to answer here No *known* issues in eager because we're using it as the reference. Also I tried turning off flash attention in the FP16 minimal repro script I sent before and the 0.002 divergence still exists, which implies there's even more to this than I thought. The question then is why did my training runs start working when I put the context manager around SDPA and compiled? To summarize: 1. No compile AMP fp16: Trains fine 2. Compile fullgraph=True AMP fp16: Converges much more slowly, fails accuracy checker (talking about the builtin one that generates minified repros, not my repro script) 3. Compile fullgraph=False AMP fp16 with context manager around SDPA: Trains fine and **doesn't fail accuracy checker**, but the repro script I wrote earlier fails(??).","I've been staring at the previous repro the last couple days (why we're seeing `.02` delta between eager mode and inductor). Latest progress: I manually bisected the autocast rules and turned several of them off, and ~~I'm able avoid the numeric issue when I specifically disable the autocast rule for sdpa~~ (nvm, just disabling for attention still gives me the divergence  it might be multiple rules). Still digging  I'm looking into why exactly the FP16 impl for SDPA diverges with inductor. For anyone interested, here's the script I was using to disable various autocast rules from python: ","Hey , quick update: I'm no longer convinced that the above repro is enough to show training divergence. The repro is comparing the difference between compiled vs. eager with amp enabled. But to really know if compile is causing problems, we need to compare **both** eager and compile to a reference implementation with amp turned off. If `abs(compiled_amp  eager_fp64) >> abs(eager_amp  eager_fp64)`, then that would be a numerical issue with compile. But if both compiled AMP and eager AMP are off by roughly the same amount, then that would be expected behavior with compile (not guaranteed to be bitwise identical to eager, but promising not to be ""worse"" than eager compared to a reference implementation). With this updated patch:  I get:  (so compiled isn't ""worse"" than eager)"
transformer,"Mistral works on (2.0.1+cu117) but ""CUDA out of memory"" on (2.1.2+cu121)"," üêõ Describe the bug I pip upgraded torch from 2.0.1 to 2.1.2 and without any code changes I now run out of CUDA memory loading Mistral7B on NVIDIA GeForce RTX 3060 TI.  From 2.0.1 to 2.1.2. Did memory allocators or max_split_size_mb change? Does torch reserves more memory?   Any steps I can do to further debug this issue? From Traceback: I see failing to allocate 20MB and 205MB reserved by unused by PyTorch. Does 2.1.2 reserves more memory? ""this process has 17179869184.00 GiB memory"" is this GiB or bytes?   Traceback (most recent call last):   File ""/home/bpevangelista/projects/kfastml/test.py"", line 12, in      model = AutoModelForCausalLM.from_pretrained(   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/models/auto/auto_factory.py"", line 566, in from_pretrained     return model_class.from_pretrained(   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/modeling_utils.py"", line 3706, in from_pretrained     ) = cls._load_pretrained_model(   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/modeling_utils.py"", line 4091, in _load_pretrained_model     state_dict = load_state_dict(shard_file)   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/transformers/modeling_utils.py"", line 510, in load_state_dict     return safe_load_file(checkpoint_file)   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/safetensors/torch.py"", line 310, in load_file     result[k] = f.get_tensor(k)   File ""/home/bpevangelista/.local/lib/python3.10/sitepackages/torch/utils/_device.py"", line 77, in __torch_function__     return func(*args, **kwargs) torch.cuda.OutOfMemory",2024-01-06T18:54:36Z,high priority module: cuda module: memory usage triaged module: regression,open,0,11,https://github.com/pytorch/pytorch/issues/116928," how much memory does your 3060 has? (I have an old 2080 I can try this one on) Also, do you mind trying to use torch2.1.2+cu118 to see if this would work or exhibit the same OOM Also, can you try setting `PYTORCH_NO_CUDA_MEMORY_CACHING` environment variable to disable caching allocator (it can negatively affect the perf, but just curious)"," The 3060 has 8GB, I imagine there's some virtualmemory/paging happening as the model should have ~16GB? I tried what you asked, results:  torch2.1.2+cu118   Tried PYTORCH_NO_CUDA_MEMORY_CACHING, which asked me to also use CUDA_LAUNCH_BLOCKING.   Went back to 2.0.1 but with cu118 instead of cu117, so we can isolate out CUDA. "," facing the same error for the same specs, any resolution found? Is it the shortage of 8GB of memory that is causing this issue?", I reverted back to 2.0.1 for the mean time.,Is transformer version the same for your torch2.0 and torch2.1 setups?,I cannot reproduce the issue and see the expected OOMs in all releases when limiting the memory to 8GB. Running the script without using `torch.cuda.set_per_process_memory_fraction` shows a memory requirement of ~24GB:  using `torch.cuda.set_per_process_memory_fraction` shows:  in `2.1.2+cu121` and  in `2.0.1+cu118`.," The transformers version is the same, only libraries that changed are pytorch and triton.  If I force GPU memory to 8GB the sample I provided will not work on both. The 7B 16bparam model needs close to 15GB, plus KV cache and scratch memory. Correct me if I'm wrong but the way pytorch+cuda works is that you reserve memory and then mmap it to GPU. Thus, an 8GB GPU could see as much memory as my RAM (32GB). Below is my GPU memory on 2.0.1 showing 14.5GB on a 8GB GPU.   I saw some 3mo old changes on CUDA allocator related to pinning memory instead of mapping, it appears disabled by default but wondering if that was part of the issue.","> Correct me if I'm wrong but the way pytorch+cuda works is that you reserve memory and then mmap it to GPU. Thus, an 8GB GPU could see as much memory as my RAM (32GB). Below is my GPU memory on 2.0.1 showing 14.5GB on a 8GB GPU. No, PyTorch will use a caching mechanism to reuse memory, but will not offload GPU memory to the host by default via e.g. managed memory. Thus, I still don't understand how you can allocate more than 8GB on your device without changing the memory allocations. Are you also able to create a 14GB tensor on this device? ","  Sorry, I forgot to reply. Yes, I can allocate a 14GB tensor on GPU ""cuda:0"" and I can allocate 14x1GB Tensors as well.  On both cases, Torch shows ~14GB reserved GPU memory while nvidiasmi would show close to 8BG/8GB memory. Torch does appear to fail to allocate close to 3x my VRAMsize. Thanks","Same issue, with WSL & RTX 3060 Ti, Pytorch  2.1.2 doesn't work (same error) and 2.0.1 works.",I have the same error with PyTorch 2.1.2.
transformer,torch.compile fullgraph=True is failing for GPTJ model for toy_backend," üêõ Describe the bug  Error Unsupported                               Traceback (most recent call last) [](https://localhost:8080/) in ()      21 torch._dynamo.reset()      22 fn = torch.compile(backend=toy_backend, dynamic=True,fullgraph=True)(model.generate) > 23 out = fn(input_ids, do_sample=True, temperature=0.9, max_length=200)      24       25  out = fn(**inputs) 60 frames /usr/local/lib/python3.10/distpackages/torch/_dynamo/exc.py in unimplemented(msg)     170 def unimplemented(msg: str):     171     assert msg != os.environ.get(""BREAK"", False) > 172     raise Unsupported(msg)     173      174  Unsupported: call_function BuiltinVariable(str) [UserFunctionVariable()] {} from user code:    File ""/usr/local/lib/python3.10/distpackages/torch/_dynamo/external_utils.py"", line 17, in inner     return fn(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/torch/utils/_contextlib.py"", line 115, in decorate_context     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/transformers/generation/utils.py"", line 1460, in generate     self._validate_model_class()   File ""/usr/local/lib/python3.10/distpackages/transformers/generation/utils.py"", line 1193, in _validate_model_class     if not self.can_generate():   File ""/usr/local/lib/python3.10/distpackages/transformers/modeling_utils.py"", line 1244, in can_generate     if ""GenerationMixin"" in str(cls.prepare_inputs_for_generation) and ""GenerationMixin"" in str(cls.generate): Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information You can suppress this exception and fall back to eager by setting:     import torch._dynamo     torch._dynamo.config.",2024-01-05T04:47:35Z,feature triaged oncall: pt2 module: dynamo dynamo-triage-june2024,closed,0,7,https://github.com/pytorch/pytorch/issues/116835,we can in principle support this but lol this code haha ( ),"Haha, what is the funny about this code, may I know ü§£ Get Outlook for Android ________________________________ From: Edward Z. Yang ***@***.***> Sent: Saturday, January 6, 2024 8:58:42 AM To: pytorch/pytorch ***@***.***> Cc: Tirupathi Rao Baggu ***@***.***>; Author ***@***.***> Subject: Re: [pytorch/pytorch] torch.compile fullgraph=True is failing for GPTJ model for toy_backend (Issue CC(torch.compile fullgraph=True is failing for GPTJ model for toy_backend)) Caution: This email originated from outside of the organization. Please take care when clicking links or opening attachments. When in doubt, contact your IT Department we can in principle support this but lol this code haha ( ) ‚Äî Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***>",Just that the code appears highly dynamic and will likely stretch the capabilities of our JIT :P ,"Hoo ,FYI same code with full graph=False ,works fine both toybackend and inductor backend, here the dynamo splitting  big graph into 13 subgraphs, I think each subgrqph execute and send output back to CPU then it execute the next graph Get Outlook for Android ________________________________ From: Michael Suo ***@***.***> Sent: Sunday, January 7, 2024 9:14:12 AM To: pytorch/pytorch ***@***.***> Cc: Tirupathi Rao Baggu ***@***.***>; Author ***@***.***> Subject: Re: [pytorch/pytorch] torch.compile fullgraph=True is failing for GPTJ model for toy_backend (Issue CC(torch.compile fullgraph=True is failing for GPTJ model for toy_backend)) Caution: This email originated from outside of the organization. Please take care when clicking links or opening attachments. When in doubt, contact your IT Department Just that the code appears highly dynamic and will likely stretch the capabilities of our JIT :P ‚Äî Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***>","Specifically, what is funny is stringifying a... method I guess? And then doing a string submatch on it. It's the sort of thing you'd expect to work if you `make_fx` but it's a lot of trouble for Dynamo because we need to identify that all of this stuff can be constant folded away.",Assigning to myself. Failing with  ,"The issue mentioned in the  original post is fixed on main. There are two graph breaks not under the scope of this issue 1) Graph break on copy.deepcopy (covered at  CC([dynamo] Graph breaks from copy.deepcopy)) 2) Graph break on tensor.item In general, tensor.item is unavoidable because `generate` is very data dependent function. Closing this for now."
transformer,Fix TransformerEncoderLayer for bias=False,"Fixes  CC(TransformerEncoderLayer raise `AttributeError: 'NoneType' object has no attribute 'device'` when `bias=False, batch_first=True`) Don't call `torch._transformer_encoder_layer_fwd` when `bias=False` `bias=False` was not something that `torch._transformer_encoder_layer_fwd`  was meant to work with, it was my bad that this wasn't tested as I approved https://github.com/pytorch/pytorch/pull/101687. `bias=False` was causing the `tensor_args` in `TransformerEncoder`/`TransformerEncoderLayer` to contain `None`s and error on checks for the fastpath like `t.requires_grad for t in tensor_args`. Alternative fix would be to 1) Pass `torch.zeros_like({*}.weight)` to the kernel when `bias=False` and filter `tensor_args` as appropriate 2) Fix `torch._transformer_encoder_layer_fwd` to take `Optional` for biases and fix the kernels as appropriate Let me know if these approaches are preferable   CC(Fix TransformerEncoderLayer for bias=False)",2024-01-04T04:15:28Z,Merged ciflow/trunk release notes: nn topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/116760, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/mikaylagawarecki/172/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/7405594863, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,[codemod] markDynamoStrictTest test_transformers,  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest test_sort_and_select)  CC([codemod] markDynamoStrictTest test_stateless)  CC([codemod] markDynamoStrictTest test_subclass)  CC([codemod] markDynamoStrictTest test_tensor_creation_ops)  CC([codemod] markDynamoStrictTest test_tensorboard)  CC([codemod] markDynamoStrictTest test_testing)  CC([codemod] markDynamoStrictTest test_transformers)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest batch)  CC([codemod] markDynamoStrictTest torch_np/numpy_tests/core/test_scalarmath)  CC([codemod] markDynamoStrictTest torch_np/numpy_tests/core/test_shape_base),2024-01-03T23:49:45Z,Merged topic: not user facing keep-going,closed,0,1,https://github.com/pytorch/pytorch/issues/116735,"hi,  . Do you have a quick method to list all UT of `TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu`? I would like to do the similar thing for `TestSDPACPU.test_scaled_dot_product_fused_attention_mask_vs_math_cpu` to remerge https://github.com/pytorch/pytorch/pull/115913."
transformer,Decomp various private upsample ops," üöÄ The feature, motivation and pitch Operators like `_upsample_bilinear2d_aa` and `_upsample_bicubic2d_aa` show up in export results ( CC(ONNX export error when exporting Vision Transformer model from PyTorch to ONNX format)), are not part of the core IR (https://pytorch.org/docs/master/ir.html), and do not have decomp implemented. It would be helpful to implement decomp for the various private upsample operators.  Alternatives _No response_  Additional context _No response_ ",2024-01-03T17:53:39Z,module: decompositions oncall: export,closed,0,1,https://github.com/pytorch/pytorch/issues/116706,Closed as some private ops are used as core ops. 
