1677,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TransformerEncoderLayer forward() crashes for bias=False during evaluation)ï¼Œ å†…å®¹æ˜¯ (I am using `TransformerEncoder`/`TransformerEncoderLayer` in my model and was able to train it fine. Now I'm trying to do inference and the code is crashing with:  I am using `bias=False` when constructing my `TransformerEncoderLayer`, so all bias parameters within the layer (see `nn.Linear` and `nn.LayerNorm` implementations) are naturally `None`. I have PyTorch 2.1.2 from conda, but even looking at the latest source code shows there is clearly an issue: https://github.com/pytorch/pytorch/blob/de4d48df34c0e8bcd03d96bcdfd12e263e9889ff/torch/nn/modules/transformer.pyL658L710 `tensor_args` contains the bias parameters, and hence includes `None`'s, but L680 invokes `x.device.type` for all `x` in `tensor_args`, producing the observed crash. I assume this is not intended. The questions in that case are: * Can `torch._transformer_encoder_layer_fwd()` (called right after in L689) deal with `None` bias parameters? If so, then L680 and L683 need to be protected against `None`'s I presume. If not, `None in tensor_args` would have to in its own right be a reason `why_not_sparsity_fast_path`? * How can people like me with the current version of PyTorch work around the issue while waiting for a fix, and be able to actually inference our `bias=False` models? Update: Potentially a similar issue? https://github.com/pytorch/pytorch/blob/de4d48df34c0e8bcd03d96bcdfd12e263e9889ff/torch/nn/modules/transformer.pyL382 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,TransformerEncoderLayer forward() crashes for bias=False during evaluation,"I am using `TransformerEncoder`/`TransformerEncoderLayer` in my model and was able to train it fine. Now I'm trying to do inference and the code is crashing with:  I am using `bias=False` when constructing my `TransformerEncoderLayer`, so all bias parameters within the layer (see `nn.Linear` and `nn.LayerNorm` implementations) are naturally `None`. I have PyTorch 2.1.2 from conda, but even looking at the latest source code shows there is clearly an issue: https://github.com/pytorch/pytorch/blob/de4d48df34c0e8bcd03d96bcdfd12e263e9889ff/torch/nn/modules/transformer.pyL658L710 `tensor_args` contains the bias parameters, and hence includes `None`'s, but L680 invokes `x.device.type` for all `x` in `tensor_args`, producing the observed crash. I assume this is not intended. The questions in that case are: * Can `torch._transformer_encoder_layer_fwd()` (called right after in L689) deal with `None` bias parameters? If so, then L680 and L683 need to be protected against `None`'s I presume. If not, `None in tensor_args` would have to in its own right be a reason `why_not_sparsity_fast_path`? * How can people like me with the current version of PyTorch work around the issue while waiting for a fix, and be able to actually inference our `bias=False` models? Update: Potentially a similar issue? https://github.com/pytorch/pytorch/blob/de4d48df34c0e8bcd03d96bcdfd12e263e9889ff/torch/nn/modules/transformer.pyL382 ",2023-12-29T22:52:59Z,high priority triage review,closed,2,3,https://github.com/pytorch/pytorch/issues/116546,Looks legit to me. ,"Legit as in it's indeed a problem like I think, or legit as in the code is fine somehow as it is?","Thanks for the report . This is a real issue and is also discussed in  CC(TransformerEncoderLayer raise `AttributeError: 'NoneType' object has no attribute 'device'` when `bias=False, batch_first=True`), closing this to redirect the discussion there. I intend to submit a fix for this today"
1977,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multiprocessing using Gunicorn causing CUDA Error )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I have been developing a FastAPI application where we can access a HuggingFace model and use it for text generation. On top of it, I'm launching the service with Gunicorn so that we can handle concurrent users. My goal is to load the model only once to GPU and allow the works to share the same model concurrently. To do this, I loaded the model and tokenizer outside of the FastAPI endpoint, I also clearly stated the use of the spawn method and the preload flag. However, I'm still running into the following error, does anyone have an idea about this? I tried almost all the solutions I found on internet. RuntimeError: Cannot reinitialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method I'm launching the app through the main entry point. hardware EC2 instance: g4dn.4xlarge GPU: NVIDIA GPU memory: 16GB  **Code**   **Error Trace** [20231229 19:49:21 +0000] [1930] [ERROR] Exception in ASGI application Traceback (most recent call last):   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/uvicorn/protocols/http/h11_impl.py"", line 408, in run_asgi     result = await app(   type: ignore[funcreturnsvalue]   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__     return await self.app(scope, receive, send)   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/fastapi/applications.py"", line 1106, in __call__     await super().__call__(scope, receive, send)   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/starlette/applications.py"", line 122, in __call__     await self.middleware_stack(scope, receive, send)   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/starlette/middlewa)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",,Multiprocessing using Gunicorn causing CUDA Error ," ğŸ› Describe the bug Hello, I have been developing a FastAPI application where we can access a HuggingFace model and use it for text generation. On top of it, I'm launching the service with Gunicorn so that we can handle concurrent users. My goal is to load the model only once to GPU and allow the works to share the same model concurrently. To do this, I loaded the model and tokenizer outside of the FastAPI endpoint, I also clearly stated the use of the spawn method and the preload flag. However, I'm still running into the following error, does anyone have an idea about this? I tried almost all the solutions I found on internet. RuntimeError: Cannot reinitialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method I'm launching the app through the main entry point. hardware EC2 instance: g4dn.4xlarge GPU: NVIDIA GPU memory: 16GB  **Code**   **Error Trace** [20231229 19:49:21 +0000] [1930] [ERROR] Exception in ASGI application Traceback (most recent call last):   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/uvicorn/protocols/http/h11_impl.py"", line 408, in run_asgi     result = await app(   type: ignore[funcreturnsvalue]   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/uvicorn/middleware/proxy_headers.py"", line 84, in __call__     return await self.app(scope, receive, send)   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/fastapi/applications.py"", line 1106, in __call__     await super().__call__(scope, receive, send)   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/starlette/applications.py"", line 122, in __call__     await self.middleware_stack(scope, receive, send)   File ""/home/ubuntu/.local/lib/python3.10/sitepackages/starlette/middlewa",2023-12-29T20:06:10Z,module: multiprocessing module: cuda triaged,open,0,3,https://github.com/pytorch/pytorch/issues/116543,"It's exactly what it says on the tin, unfortunately. You can't initialize CUDA and then fork the process, CUDA runtime doesn't support it. If you want to preload the weights and share them on subprocesses, you'll need to load the weights in a separate subprocess and then transfer access with IPC or something","I wonder if it's possible to somehow use the new CUDA mmap's to achieve some readonly physical memory sharing between processes: https://developer.nvidia.com/blog/introducinglowlevelgpuvirtualmemorymanagement/  mentions something like this at ""Operating system native interprocess communication""","Yes, in fact, you didn't even need to use mmap, the old IPC interface works for this too"
540,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] aten::erfinv bug fix: add storage offset buffers to handle slicing (#105801))ï¼Œ å†…å®¹æ˜¯ (A bug fix of a recently merged PR per comment: https://github.com/pytorch/pytorch/pull/101507discussion_r1271393706 The follow test would fail without this bug fix:  Pull Request resolved: https://github.com/pytorch/pytorch/pull/105801 Approved by: https://github.com/malfet)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MPS] aten::erfinv bug fix: add storage offset buffers to handle slicing (#105801),A bug fix of a recently merged PR per comment: https://github.com/pytorch/pytorch/pull/101507discussion_r1271393706 The follow test would fail without this bug fix:  Pull Request resolved: https://github.com/pytorch/pytorch/pull/105801 Approved by: https://github.com/malfet,2023-12-29T19:14:14Z,release notes: mps ciflow/mps,closed,0,0,https://github.com/pytorch/pytorch/issues/116542
393,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Migrate state_dict bc test to OptimizerInfo, increase coverage)ï¼Œ å†…å®¹æ˜¯ (  CC(Migrate nontensor step and CUDA params state_dict tests to OptimizerInfo )  CC(Migrate state_dict bc test to OptimizerInfo, increase coverage))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Migrate state_dict bc test to OptimizerInfo, increase coverage","  CC(Migrate nontensor step and CUDA params state_dict tests to OptimizerInfo )  CC(Migrate state_dict bc test to OptimizerInfo, increase coverage)",2023-12-28T10:07:35Z,Merged ciflow/trunk topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/116500, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.11clang10 / test (dynamo, 1, 7, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add a DTensor test for weight tying)ï¼Œ å†…å®¹æ˜¯ (Weight tying is useful when we'd like to share weights (and their gradients) between two modules, e.g. the word/token embedding module and the output linear module in language models. This test demonstrates that with DTensor it can be achieved just as with normal tensor, e.g. using `model.fc.weight = model.embedding.weight`. To test: `python test/distributed/tensor/parallel/test_tp_examples.py k test_weight_tying`   CC(add a DTensor test for weight tying) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,add a DTensor test for weight tying,"Weight tying is useful when we'd like to share weights (and their gradients) between two modules, e.g. the word/token embedding module and the output linear module in language models. This test demonstrates that with DTensor it can be achieved just as with normal tensor, e.g. using `model.fc.weight = model.embedding.weight`. To test: `python test/distributed/tensor/parallel/test_tp_examples.py k test_weight_tying`   CC(add a DTensor test for weight tying) ",2023-12-28T00:45:03Z,oncall: distributed Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/116475, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1120,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add torch.float64 precision support to the transformer test suite in TP/SP)ï¼Œ å†…å®¹æ˜¯ (This PR (as a followup to CC(add a transformer suite in TP/SP tests)) resolves previous issues of not passing `assertEqual()` tests (with small error) when comparing outputs from the singlegpu model and the distributed model, under certain input/model sizes or when certain operations (e.g. weighttying) are enabled. This is done by simply enabling higher precision computation using `dtype=torch.float64`. What is not tested: whether or not distributed model training convergence rate is affected using just `torch.float32` precision. Test plan: TP: `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_False` TP+SP: `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_True`   CC(add torch.float64 precision support to the transformer test suite in TP/SP) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,add torch.float64 precision support to the transformer test suite in TP/SP,"This PR (as a followup to CC(add a transformer suite in TP/SP tests)) resolves previous issues of not passing `assertEqual()` tests (with small error) when comparing outputs from the singlegpu model and the distributed model, under certain input/model sizes or when certain operations (e.g. weighttying) are enabled. This is done by simply enabling higher precision computation using `dtype=torch.float64`. What is not tested: whether or not distributed model training convergence rate is affected using just `torch.float32` precision. Test plan: TP: `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_False` TP+SP: `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_True`   CC(add torch.float64 precision support to the transformer test suite in TP/SP) ",2023-12-26T23:25:51Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/periodic,closed,0,6,https://github.com/pytorch/pytorch/issues/116436,"Using fp64 to check functional correctness sounds good, but I wanted to be a bit careful about the numeric issue and understand it better. It may not be a good sign for us if a small fp32 model/input can already drift compared to singleGPU (since people will be running fp32 or smaller bitwidth in practice). Interestingly, I cannot repro the `assertEqual` failures when using `weight_tying=True` and `inp_size = [8, 12]`. I will share one source of numeric issue I have seen in the past: single vs. multitensor Adam. Since `DTensor` does not currently default to multitensor Adam (requiring passing `foreach=True` explicitly), we can see numeric drift comparing the singleGPU non`DTensor` and multiGPU `DTensor` cases. However, this drift can be fixed by passing `foreach=True` for the `DTensor` model's optimizer. It might be worthwhile to check if this solves the issue without fp64.", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/tianyul/1/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/116436`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
373,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix wrong class inheritance in pyi)ï¼Œ å†…å®¹æ˜¯ (As the title stated. https://github.com/pytorch/pytorch/blob/f6dfbffb3bb46ada6fe66b5da4f989f9d4d69b3c/torch/csrc/distributed/c10d/ProcessGroupNCCL.hppL153 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix wrong class inheritance in pyi,As the title stated. https://github.com/pytorch/pytorch/blob/f6dfbffb3bb46ada6fe66b5da4f989f9d4d69b3c/torch/csrc/distributed/c10d/ProcessGroupNCCL.hppL153 ,2023-12-26T03:38:55Z,oncall: distributed triaged open source Merged ciflow/trunk release notes: distributed (c10d),closed,0,2,https://github.com/pytorch/pytorch/issues/116404, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1967,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pytorch Transformer Distributed Training)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug code:  error ` RuntimeError                              Traceback (most recent call last) Cell In[2], line 6       4 for epoch in range(1, NUM_EPOCHS+1):       5     start_time = timer() > 6     train_loss, train_acc = train_epoch(transformer, optimizer)       7     end_time = timer()       8     val_loss, val_acc = evaluate(transformer) Cell In[1], line 278, in train_epoch(model, optimizer)     274 tgt_input = tgt[:1, :]     276 src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input) > 278 logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)     280 optimizer.zero_grad()     282 tgt_out = tgt[1:, :] File /opt/conda/lib/python3.10/sitepackages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)    1496  If we don't have any hooks, we want to skip the rest of the logic in    1497  this function, and just call forward.    1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks    1499         or _global_backward_pre_hooks or _global_backward_hooks    1500         or _global_forward_hooks or _global_forward_pre_hooks): > 1501     return forward_call(*args, **kwargs)    1502  Do not call functions when jit is used    1503 full_backward_hooks, non_full_backward_hooks = [], [] File /opt/conda/lib/python3.10/sitepackages/torch/nn/parallel/data_parallel.py:171, in DataParallel.forward(self, *inputs, **kwargs)     169     return self.module(*inputs[0], **kwargs[0])     170 replicas = self.replicate(self.module, self.device_ids[:len(inputs)]) > 171 outputs = self.parallel_apply(replicas, inputs, kwargs)     172 return self.gather(outputs, self.o)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Pytorch Transformer Distributed Training," ğŸ› Describe the bug code:  error ` RuntimeError                              Traceback (most recent call last) Cell In[2], line 6       4 for epoch in range(1, NUM_EPOCHS+1):       5     start_time = timer() > 6     train_loss, train_acc = train_epoch(transformer, optimizer)       7     end_time = timer()       8     val_loss, val_acc = evaluate(transformer) Cell In[1], line 278, in train_epoch(model, optimizer)     274 tgt_input = tgt[:1, :]     276 src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input) > 278 logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)     280 optimizer.zero_grad()     282 tgt_out = tgt[1:, :] File /opt/conda/lib/python3.10/sitepackages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)    1496  If we don't have any hooks, we want to skip the rest of the logic in    1497  this function, and just call forward.    1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks    1499         or _global_backward_pre_hooks or _global_backward_hooks    1500         or _global_forward_hooks or _global_forward_pre_hooks): > 1501     return forward_call(*args, **kwargs)    1502  Do not call functions when jit is used    1503 full_backward_hooks, non_full_backward_hooks = [], [] File /opt/conda/lib/python3.10/sitepackages/torch/nn/parallel/data_parallel.py:171, in DataParallel.forward(self, *inputs, **kwargs)     169     return self.module(*inputs[0], **kwargs[0])     170 replicas = self.replicate(self.module, self.device_ids[:len(inputs)]) > 171 outputs = self.parallel_apply(replicas, inputs, kwargs)     172 return self.gather(outputs, self.o",2023-12-25T21:28:16Z,oncall: distributed triaged,open,0,4,https://github.com/pytorch/pytorch/issues/116400,"currently using this for distributed training and i get error on attention_masks ```python transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,                                  NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM) if torch.cuda.device_count() > 1:     transformer = nn.DataParallel(transformer) for p in transformer.parameters():     if p.dim() > 1:         nn.init.xavier_uniform_(p) transformer = transformer.to(DEVICE) `",Could you try using `DistributedDataParallel` from `torch.nn.parallel.distributed` instead of `nn.DataParallel`? https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html,> Could you try using `DistributedDataParallel` from `torch.nn.parallel.distributed` instead of `nn.DataParallel`? >  > https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html what should i set for master_addr and master_port?," Could you try taking a look at https://pytorch.org/tutorials/intermediate/ddp_tutorial.html? If you are running on a single host, then using `torchrun` might be the simplest option."
2054,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TransformerEncoderLayer raise `AttributeError: 'NoneType' object has no attribute 'device'` when `bias=False, batch_first=True`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Versions PyTorch version: 2.1.0+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.078genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   43 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          40 Online CPU(s) list:             039 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Xeon(R) CPU E52640 v4 @ 2.40GHz CPU family:                      6 Model:                           79 Thread(s) per core:              1 Core(s) per socket:              8 Socket(s):                       5 Stepping:                        1 BogoMIPS:                        4799.99 Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hyperv)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"TransformerEncoderLayer raise `AttributeError: 'NoneType' object has no attribute 'device'` when `bias=False, batch_first=True`"," ğŸ› Describe the bug   Versions PyTorch version: 2.1.0+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.078genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   43 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          40 Online CPU(s) list:             039 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Xeon(R) CPU E52640 v4 @ 2.40GHz CPU family:                      6 Model:                           79 Thread(s) per core:              1 Core(s) per socket:              8 Socket(s):                       5 Stepping:                        1 BogoMIPS:                        4799.99 Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hyperv",2023-12-25T05:06:08Z,high priority triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/116385," tensor args also includes the bias variables so while evaluating, `(x.device.type in _supported_device_type) for x in tensor_args` it is checking device even for the bias variables which are None in your case. It can be fixed by modifying tensor_args based on bias. ", could you please take a look at this one ?,'t it be nice when the fastpath is gone :),">  >  > tensor args also includes the bias variables so while evaluating, `(x.device.type in _supported_device_type) for x in tensor_args` it is checking device even for the bias variables which are None in your case. It can be fixed by modifying tensor_args based on bias. Agree. We should check the 6 bias in `tensor_args` and remove bias which equals to `None`. It's an issue existing for both CUDA and CPU.",should I submit PR for this? ," Assigning to myself as I intend to submit a fix for this today (as this is a duplicate of  CC(TransformerEncoderLayer forward() crashes for bias=False during evaluation) which is marked high priority), feel free to take a look at the issues marked actionable if you would like to contribute"
469,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo][Forward Fx] Put ao modules as call_module in FX graph)ï¼Œ å†…å®¹æ˜¯ (It seems ao modules were not in  folder, so we have to include them as well. This is found when CC([Dynamo][10/N] Remove TorchVariable and is_allowed) was imported into fbcode, some executorch tests failed because of this. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Dynamo][Forward Fx] Put ao modules as call_module in FX graph,"It seems ao modules were not in  folder, so we have to include them as well. This is found when CC([Dynamo][10/N] Remove TorchVariable and is_allowed) was imported into fbcode, some executorch tests failed because of this. ",2023-12-25T03:21:35Z,topic: not user facing module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/116384
1562,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Batched Sparse Matrix - Batched Sparse Matrix multiplication)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I am currently engaged in the development of a novel crossattention mechanism for transformers. In this context, consider the shape of q and k as (b, n, l, d), where b represents the batch number, n denotes the head number, l signifies the sequence length, and d stands for the feature dimensions. The code mechanism employed to compute attention involves the application of the F.relu activation function to both q and k, followed by reshaping operations. Specifically, it can be expressed as follows: F.relu(q).reshape(b, n * l, d) @ F.relu(k).reshape(b, n * l, d) However, the reshape operation results in the multiplication of two large matrices, incurring a substantial GPU memory cost. This presents a notable drawback in terms of computational efficiency and resource utilization. The aforementioned attention function can be conceptualized as the multiplication between two sparse matrices, considering the involvement of the ReLU function. To address the memory issue, I am seeking to leverage the Batched Sparse Matrix  Batched Sparse Matrix multiplication feature. This enhancement would optimize the computation and alleviate the strain on GPU memory, making the implementation more efficient and scalable.  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Batched Sparse Matrix - Batched Sparse Matrix multiplication," ğŸš€ The feature, motivation and pitch I am currently engaged in the development of a novel crossattention mechanism for transformers. In this context, consider the shape of q and k as (b, n, l, d), where b represents the batch number, n denotes the head number, l signifies the sequence length, and d stands for the feature dimensions. The code mechanism employed to compute attention involves the application of the F.relu activation function to both q and k, followed by reshaping operations. Specifically, it can be expressed as follows: F.relu(q).reshape(b, n * l, d) @ F.relu(k).reshape(b, n * l, d) However, the reshape operation results in the multiplication of two large matrices, incurring a substantial GPU memory cost. This presents a notable drawback in terms of computational efficiency and resource utilization. The aforementioned attention function can be conceptualized as the multiplication between two sparse matrices, considering the involvement of the ReLU function. To address the memory issue, I am seeking to leverage the Batched Sparse Matrix  Batched Sparse Matrix multiplication feature. This enhancement would optimize the computation and alleviate the strain on GPU memory, making the implementation more efficient and scalable.  Alternatives _No response_  Additional context _No response_ ",2023-12-24T02:23:42Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/116364,"Hi , thanks for the request! I'll point you to what I believe is a duplicate CC(Batched sparsesparse matrix multiplication/ sparse torch.einsum), but feel free to reopen if your needs aren't covered by the discussion in that thread. Thanks!"
2001,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Abstract torch.device for GPU/NE/TPU computations in the cloud-based agent)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Would be cool if Pytorch had smth like an agent, that we can spin up in the cloud, or even have a multiuser service, so instead of specifying cpu or gpu, we could just specify a cloud url for the computations. and the cloud provider could charge us on a per minute basis, or any other flexible way. e.g. , gpt figured it out ğŸ˜‚ :  torch.device('cloudgpu', url='https://gpu_in_the_cloud.aws.com:12345/username', password='*******', max_runtime='12m')  Alternatives ChatGPT got it: That's an interesting concept! While PyTorch doesn't currently offer a builtin feature like this, the idea of having an ""agent"" on a cloud VM that you could easily target for computations just like switching between CPU and GPU is quite innovative. This would essentially abstract away the complexities of remote computing, making it as simple as specifying a device in your PyTorch code. Here's how such a feature might work conceptually: 1. Cloud Agent Setup: You would install a specialized PyTorch agent on a cloud VM. This agent would be responsible for receiving computation tasks and running them on the VM's resources. 2. Seamless Integration: In your local PyTorch script, you could specify the cloud resource similar to how you specify a device with torch.device. For instance, something like torch.device('cloud', url='https://gpu_in_the_cloud.aws.com:12345/username', password='*******', max_runtime='12m'). 3. Data Transfer and Execution: The local PyTorch environment would handle data serialization and transfer to the cloud VM. The cloud agent would then execute the computations and return the results. 4. Optimized Resource Management: The agent could also manage resources effectively, scaling up or down bas)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Abstract torch.device for GPU/NE/TPU computations in the cloud-based agent," ğŸš€ The feature, motivation and pitch Would be cool if Pytorch had smth like an agent, that we can spin up in the cloud, or even have a multiuser service, so instead of specifying cpu or gpu, we could just specify a cloud url for the computations. and the cloud provider could charge us on a per minute basis, or any other flexible way. e.g. , gpt figured it out ğŸ˜‚ :  torch.device('cloudgpu', url='https://gpu_in_the_cloud.aws.com:12345/username', password='*******', max_runtime='12m')  Alternatives ChatGPT got it: That's an interesting concept! While PyTorch doesn't currently offer a builtin feature like this, the idea of having an ""agent"" on a cloud VM that you could easily target for computations just like switching between CPU and GPU is quite innovative. This would essentially abstract away the complexities of remote computing, making it as simple as specifying a device in your PyTorch code. Here's how such a feature might work conceptually: 1. Cloud Agent Setup: You would install a specialized PyTorch agent on a cloud VM. This agent would be responsible for receiving computation tasks and running them on the VM's resources. 2. Seamless Integration: In your local PyTorch script, you could specify the cloud resource similar to how you specify a device with torch.device. For instance, something like torch.device('cloud', url='https://gpu_in_the_cloud.aws.com:12345/username', password='*******', max_runtime='12m'). 3. Data Transfer and Execution: The local PyTorch environment would handle data serialization and transfer to the cloud VM. The cloud agent would then execute the computations and return the results. 4. Optimized Resource Management: The agent could also manage resources effectively, scaling up or down bas",2023-12-23T15:47:30Z,feature triaged needs research needs design,open,0,1,https://github.com/pytorch/pytorch/issues/116354,Added triage review for visibility / discussion.
394,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fixed segfault when trying to permute empty tensor)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(`torch.Tensor.permute`: Segfault). Fixed unchecked access to first element of `dims` when permuting an empty tensor. Added test to prevent regressions.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fixed segfault when trying to permute empty tensor,Fixes CC(`torch.Tensor.permute`: Segfault). Fixed unchecked access to first element of `dims` when permuting an empty tensor. Added test to prevent regressions.,2023-12-22T20:38:47Z,open source Merged ciflow/trunk release notes: sparse,closed,0,5,https://github.com/pytorch/pytorch/issues/116335, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""release notes: sparse""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1812,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`gpt2-medium` on `mps` device: Ok with 1023 input tokens. Wrong with 1024 input tokens.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When running inference and the following 3 conditions are met 1. use transformers' `gpt2medium` model 2. use `mps` backend on a Macbook M1 3. use input_ids shape `(1, 1024)` (i.e. batch size `1` and maximum context length) then I get wrong output. Interestingly, this problem does NOT appear if any of the above conditions is not fulfilled. To better illustrate this, I am adding some code below which shows that the predictions of the first few tokens (which should NOT change, no matter if we input 1023 or 1024 tokens) change when using `gpt2medium`.  Any insights into how we could fix that problem and whether this could also appear in other situations/models?  Versions Collecting environment information... PyTorch version: 2.1.2 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.2.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS14.2.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.25.2 [pip3] torch==2.1.2 [pip3] torchinfo==1.8.0 [conda] Could not collect )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`gpt2-medium` on `mps` device: Ok with 1023 input tokens. Wrong with 1024 input tokens.," ğŸ› Describe the bug When running inference and the following 3 conditions are met 1. use transformers' `gpt2medium` model 2. use `mps` backend on a Macbook M1 3. use input_ids shape `(1, 1024)` (i.e. batch size `1` and maximum context length) then I get wrong output. Interestingly, this problem does NOT appear if any of the above conditions is not fulfilled. To better illustrate this, I am adding some code below which shows that the predictions of the first few tokens (which should NOT change, no matter if we input 1023 or 1024 tokens) change when using `gpt2medium`.  Any insights into how we could fix that problem and whether this could also appear in other situations/models?  Versions Collecting environment information... PyTorch version: 2.1.2 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.2.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.1.0.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS14.2.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.25.2 [pip3] torch==2.1.2 [pip3] torchinfo==1.8.0 [conda] Could not collect ",2023-12-22T15:06:07Z,high priority triage review module: correctness (silent) module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/116331,"I can partially reproduce this on an M1 Mac Mini, although my results are different:  I was able to fix the last setup by modifying the `Conv1D` layer of `transformers`,   but there is some other underlying issue that needs further investigation.","Not sure how it ever worked, at least for me `addmm` yields wrong results even for 2x2 matrices:  prints ",This is downright wrong and causes all this weirdness (and as  https://github.com/pytorch/pytorch/pull/77462 suggests that eliminated it for `mm` and `bmm`) not needed: https://github.com/pytorch/pytorch/blob/de4d48df34c0e8bcd03d96bcdfd12e263e9889ff/aten/src/ATen/native/mps/operations/LinearAlgebra.mmL85L90
2012,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ONNX export error when exporting Vision Transformer model from PyTorch to ONNX format)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I've been trying to export the dinov2 vision transformer model to onnx format and have been getting an error:  I've tried with different versions of pytorch (previously with 2.0.0 and currently with the nightly build 2.3.0) and getting the same issue. I've inspected onnxscript and see that the `aten_bicubic_2d` function seems to be present (link here). Thanks for your help :) Steps to reproduce issue: 1. Run this script:   Versions PyTorch version: 2.3.0.dev20231221+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.10.8  (main, Nov 22 2022, 08:26:04) [GCC 10.4.0] (64bit runtime) Python platform: Linux4.14.327246.539.amzn2.x86_64x86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 470.57.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Byte Order:                         Little Endian Address sizes:                      46 bits physical, 48 bits virtual CPU(s):                             8 Online CPU(s) list:                07 Thread(s) per core:                 2 Core(s) per socket:                 4 Socket(s):                          1 NUMA node(s):                       1 Vendor ID:                          GenuineIntel CPU family:                         6 Model:              )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,ONNX export error when exporting Vision Transformer model from PyTorch to ONNX format," ğŸ› Describe the bug Hello, I've been trying to export the dinov2 vision transformer model to onnx format and have been getting an error:  I've tried with different versions of pytorch (previously with 2.0.0 and currently with the nightly build 2.3.0) and getting the same issue. I've inspected onnxscript and see that the `aten_bicubic_2d` function seems to be present (link here). Thanks for your help :) Steps to reproduce issue: 1. Run this script:   Versions PyTorch version: 2.3.0.dev20231221+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.10.8  (main, Nov 22 2022, 08:26:04) [GCC 10.4.0] (64bit runtime) Python platform: Linux4.14.327246.539.amzn2.x86_64x86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 470.57.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Byte Order:                         Little Endian Address sizes:                      46 bits physical, 48 bits virtual CPU(s):                             8 Online CPU(s) list:                07 Thread(s) per core:                 2 Core(s) per socket:                 4 Socket(s):                          1 NUMA node(s):                       1 Vendor ID:                          GenuineIntel CPU family:                         6 Model:              ",2023-12-21T23:43:27Z,module: onnx triaged,open,0,7,https://github.com/pytorch/pytorch/issues/116306,ONNX Script is used only by the new dynamo exporter (`torch.onnx.dynamo_export`). Consider using that to see if it succeeds. Be sure to use the latest PyTorch and ONNX versions: https://pytorch.org/docs/stable/onnx_dynamo.html,Also aten_upsample_bicubic2d is not implemented yet. It is WIP in https://github.com/microsoft/onnxscript/pull/1208,Tested with dynamo. It is still _upsample_bicubic2d_aa that needs to be supported.,Tracked by https://github.com/microsoft/onnxscript/issues/1159,"> Tracked by microsoft/onnxscript CC(No cuda device, but ""CUDA driver version is insufficient for CUDA runtime version"") maybe z and  can help implementing these", have you found a way to do it ?,Any update?
526,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE][Easy]: Enable clang-tidy check readability-misplaced-array-index)ï¼Œ å†…å®¹æ˜¯ (Enable clangtidy check readability which checks for a bizarre C++ construct that is usually indicative of an error: https://clang.llvm.org/extra/clangtidy/checks/readability/misplacedarrayindex.html (indexing a number by a pointer, which surprisingly inverts the operands).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE][Easy]: Enable clang-tidy check readability-misplaced-array-index,"Enable clangtidy check readability which checks for a bizarre C++ construct that is usually indicative of an error: https://clang.llvm.org/extra/clangtidy/checks/readability/misplacedarrayindex.html (indexing a number by a pointer, which surprisingly inverts the operands).",2023-12-20T20:45:57Z,open source better-engineering Merged ciflow/trunk topic: not user facing,closed,0,11,https://github.com/pytorch/pytorch/issues/116210, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""unfortunately, It is required to revert this PR in order to properly revert https://github.com/pytorch/pytorch/pull/116193"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,"After discussing with , we agreed that the best path forward is to merge this back, so we can work internally towards fixing what needs to be fixed without blocking OSS development. As with this revert and reland it should be sufficient to unblock internal diff train to continue imports due the internal code freeze.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 20 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.11clang10 / test (dynamo, 1, 7, linux.2xlarge)  pull / linuxfocalpy3.11clang10 / test (dynamo, 7, 7, linux.2xlarge)  pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 7, linux.2xlarge)  pull / linuxfocalpy3.8clang10 / test (dynamo, 5, 7, linux.2xlarge)  pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 1, 5, linux.g5.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""revert was not necessary, see comments on target PR"" "," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1056,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DCP] Native S3/object storage interface)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I would like to save and load checkpoints using DCP in S3, but the `FileSystemReader` and `FileSystemWriter` only seem to support usage with a distributed file system. I see there's some blog posts about using it with S3FS, but I'd prefer not to have to do that either.  I started trying to implement a S3 Reader/Writer, but it wasn't immediately clear how to handle the metadata file. Are all ranks writing the same thing to the metadata file, such that I just need to ensure that any one of them completes the upload successfully? It doesn't look like they're all trying to individually append to the same file or something, which would be problematic. Are there any other obvious issues to implementing an object storage backend?  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[DCP] Native S3/object storage interface," ğŸš€ The feature, motivation and pitch I would like to save and load checkpoints using DCP in S3, but the `FileSystemReader` and `FileSystemWriter` only seem to support usage with a distributed file system. I see there's some blog posts about using it with S3FS, but I'd prefer not to have to do that either.  I started trying to implement a S3 Reader/Writer, but it wasn't immediately clear how to handle the metadata file. Are all ranks writing the same thing to the metadata file, such that I just need to ensure that any one of them completes the upload successfully? It doesn't look like they're all trying to individually append to the same file or something, which would be problematic. Are there any other obvious issues to implementing an object storage backend?  Alternatives _No response_  Additional context _No response_ ",2023-12-20T17:38:23Z,oncall: distributed module: distributed_checkpoint,open,1,1,https://github.com/pytorch/pytorch/issues/116198,Related on integrating basic cloud storage file i/o:   CC([feature request] torch.hub.load_state_dict_from_url to be replaced by a new good general download-a-file function and to also support local paths and google drive links / private github release links)
868,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile is not working for below GPT-j Model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi team I am using torch.compile for custom backend, so as part of this experiment, i am running gptj model on default backend also. below code is not going through torch.compile executing in egar model only  but if i make small change in above code it is going through torch.compile path   Error logs no error, its just not going through default torch.compile flow for cpu  Minified repro _No response_  Versions Python 3.8.10 (default, Nov 22 2023, 10:22:35)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import torch >>> torch.__version__ '2.3.0a0+gitace55d6' )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile is not working for below GPT-j Model," ğŸ› Describe the bug Hi team I am using torch.compile for custom backend, so as part of this experiment, i am running gptj model on default backend also. below code is not going through torch.compile executing in egar model only  but if i make small change in above code it is going through torch.compile path   Error logs no error, its just not going through default torch.compile flow for cpu  Minified repro _No response_  Versions Python 3.8.10 (default, Nov 22 2023, 10:22:35)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import torch >>> torch.__version__ '2.3.0a0+gitace55d6' ",2023-12-20T09:57:22Z,oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/116181,"Hi, by default only `forward()` of the model is compiled. You'll need to compile `generate()`. ", Please reopen the issue if ZailiWang 's suggestion doesn't work. Thanks!
911,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug while testing the performance for transformer blocks, we found that the performance severely drops while updating pytorch version from `pt1.10v1` to `pt2.0v2_cuda11.8`ã€‚ the performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` on `pt1.10v1` is the following  while for `pt2.0v2_cuda11.8`, the results drop dramastically.  Reproducing using the following benchmark codes:  some perf results(top 10; cuda time sorted) on`transformer_encoder_block` and  `transformer_decoder_block` from the above code snippet are shown below, for pt1.10,   pt2.0:       )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` drops severely if pytorch version changed from `pt1.10` to `pt2.0`," ğŸ› Describe the bug while testing the performance for transformer blocks, we found that the performance severely drops while updating pytorch version from `pt1.10v1` to `pt2.0v2_cuda11.8`ã€‚ the performance for `TransformerEncoderLayer` and `TransformerDecoderLayer` on `pt1.10v1` is the following  while for `pt2.0v2_cuda11.8`, the results drop dramastically.  Reproducing using the following benchmark codes:  some perf results(top 10; cuda time sorted) on`transformer_encoder_block` and  `transformer_decoder_block` from the above code snippet are shown below, for pt1.10,   pt2.0:       ",2023-12-20T07:04:00Z,module: performance module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/116175
2027,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.nn.Transformer.generate_square_subsequent_mask gives nans instead of zeros when running on mps)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `generate_square_subsequent_mask` has different behaviour on mac when using mps vs cpu, and I think the mps behaviour is the unexpected one.  Simple example below: you would expect `a`, `b`, and `c` to differ only by devices, but `b` has nans which `a` and `c` don't. Also, moving `a` to the mps device doesnt generate something identical to `b`. I assume this is a bug, not urgent but probably wants a fix (for now I am using `c` instead of `b` as my workaround).   Versions Collecting environment information... PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.6 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.26.0 [pip3] pytorchlightning==2.0.3 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchmetrics==0.11.4 [pip3] torchvision==0.16.0 [conda] numpy                     1.26.0          py311he598dae_0   [conda] numpybase                1.26.0          py311hfbfe69c_0   [conda] pytorch                   2.1.0                  py3.11_0    pytorch [conda] pytorchlightning         2.0.3           py311hca03da5_0   [conda] torchaudio                2.1.0                 py311_cpu    pytorch [conda] )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.nn.Transformer.generate_square_subsequent_mask gives nans instead of zeros when running on mps," ğŸ› Describe the bug `generate_square_subsequent_mask` has different behaviour on mac when using mps vs cpu, and I think the mps behaviour is the unexpected one.  Simple example below: you would expect `a`, `b`, and `c` to differ only by devices, but `b` has nans which `a` and `c` don't. Also, moving `a` to the mps device doesnt generate something identical to `b`. I assume this is a bug, not urgent but probably wants a fix (for now I am using `c` instead of `b` as my workaround).   Versions Collecting environment information... PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.6 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.26.0 [pip3] pytorchlightning==2.0.3 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchmetrics==0.11.4 [pip3] torchvision==0.16.0 [conda] numpy                     1.26.0          py311he598dae_0   [conda] numpybase                1.26.0          py311hfbfe69c_0   [conda] pytorch                   2.1.0                  py3.11_0    pytorch [conda] pytorchlightning         2.0.3           py311hca03da5_0   [conda] torchaudio                2.1.0                 py311_cpu    pytorch [conda] ",2023-12-20T05:26:14Z,triaged module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/116170,"I could reproduce the issue. After some debugging, I found that the underlying problem here is with `torch.triu`  same as discussed in more details in this issue  CC(torch.triu() may returns wrong values using MPS) ","Fixed in https://github.com/pytorch/pytorch/pull/128575. On nightly  yields  xref CC(taking upper triangular of ""inf"" matrix results in nan values)","Current test cases are probably sufficient (ref  CC(torch.triu() may returns wrong values using MPS)issuecomment2322060071) Closing this, but please reopen if there are items left to address.  close"
2146,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(""fatal: not a git repository (or any of the parent directories): .git"", indicates that PyTorch is trying to access submodules during its installation process, but it can't find a valid .git directory within the project.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug (base) C:\Users\emili\Downloads\pytorchmain\pytorchmain>python setup.py develop fatal: not a git repository (or any of the parent directories): .git Building wheel torch2.3.0a0+gitUnknown  Building version 2.3.0a0+gitUnknown   Trying to initialize submodules fatal: not a git repository (or any of the parent directories): .git   Submodule initalization failed Please run:         git submodule update init recursive (base) C:\Users\emili\Downloads\pytorchmain\pytorchmain>git submodule update init recursive fatal: not a git repository (or any of the parent directories): .git (base) C:\Users\emili\Downloads\pytorchmain\pytorchmain>  Explanation: Submodules: PyTorch relies on external libraries called ""submodules"" for certain functionalities. Initialization: During installation, it attempts to initialize and update these submodules using the git submodule update init recursive command. Missing or Invalid .git: However, the error message suggests that the expected .git directory, which manages the submodules, is either missing or not recognized as a valid git repository.  Versions To simplify download and installation in the future, consider using choco install wget before running wget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py. Remember to check collect_env.py for security purposes before running it. Wget v1.21.4 [Approved] Wget package files install completed. Performing other installation steps. The package Wget wants to run 'chocolateyinstall.ps1'. Note: If you don't run this script, the installation will fail. Note: To confirm automatically next time, use 'y' or consider: choco feature enable n allowGlobalConfirmation Do you want to run the script?([Y]es/[A])è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"""fatal: not a git repository (or any of the parent directories): .git"", indicates that PyTorch is trying to access submodules during its installation process, but it can't find a valid .git directory within the project."," ğŸ› Describe the bug (base) C:\Users\emili\Downloads\pytorchmain\pytorchmain>python setup.py develop fatal: not a git repository (or any of the parent directories): .git Building wheel torch2.3.0a0+gitUnknown  Building version 2.3.0a0+gitUnknown   Trying to initialize submodules fatal: not a git repository (or any of the parent directories): .git   Submodule initalization failed Please run:         git submodule update init recursive (base) C:\Users\emili\Downloads\pytorchmain\pytorchmain>git submodule update init recursive fatal: not a git repository (or any of the parent directories): .git (base) C:\Users\emili\Downloads\pytorchmain\pytorchmain>  Explanation: Submodules: PyTorch relies on external libraries called ""submodules"" for certain functionalities. Initialization: During installation, it attempts to initialize and update these submodules using the git submodule update init recursive command. Missing or Invalid .git: However, the error message suggests that the expected .git directory, which manages the submodules, is either missing or not recognized as a valid git repository.  Versions To simplify download and installation in the future, consider using choco install wget before running wget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py. Remember to check collect_env.py for security purposes before running it. Wget v1.21.4 [Approved] Wget package files install completed. Performing other installation steps. The package Wget wants to run 'chocolateyinstall.ps1'. Note: If you don't run this script, the installation will fail. Note: To confirm automatically next time, use 'y' or consider: choco feature enable n allowGlobalConfirmation Do you want to run the script?([Y]es/[A]",2023-12-19T07:46:44Z,module: build module: windows triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/116088,"Main Point: I am trying to install PyTorch for Audiocraft but encountered errors during the installation process. Details: Several packages including audiocraft, demucs, encodec, etc., are throwing deprecation warnings about using ""eggs"" instead of pip for installation. Pip successfully downloaded and prepared the PyTorch package but failed to build the wheel due to an error message stating the package name should be ""torch"" instead of ""pytorch"". Restructure: While installing PyTorch for Audiocraft, I encountered deprecation warnings for several packages suggesting pip instead of ""eggs"". The installation itself started well, but building the PyTorch wheel failed due to an incorrect package name (should be ""torch""). Additional Notes: The provided snippet includes unnecessary details like the ""DEPRECATION"" messages and full tracebacks. These can be omitted unless specifically requested for debugging purposes. Error return: DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\audiocraft1.2.0a2py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\demucs4.0.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\encodec0.1.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\gradio4.10.0py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\librosa0.10.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\protobuf4.25.1py3.12winamd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\torchmetrics1.2.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 Collecting pytorch   Downloading pytorch1.0.2.tar.gz (689 bytes)   Preparing metadata (setup.py) ... done Building wheels for collected packages: pytorch   Building wheel for pytorch (setup.py) ... error   error: subprocessexitedwitherror   Ã— python setup.py bdist_wheel did not run successfully.   â”‚ exit code: 1   â•°â”€> [6 lines of output]       Traceback (most recent call last):         File """", line 2, in          File """", line 34, in          File ""C:\Users\emili\AppData\Local\Temp\pipinstall2bu4xm1v\pytorch_238b8f1383db4defbf9df0343ae5b116\setup.py"", line 15, in            raise Exception(message)       Exception: You tried to install ""pytorch"". The package named for PyTorch is ""torch""       [end of output]   note: This error originates from a subprocess, and is likely not a problem with pip.   ERROR: Failed building wheel for pytorch   Running setup.py clean for pytorch Failed to build pytorch ERROR: Could not build wheels for pytorch, which is required to install pyproject.tomlbased projects PS C:\Users\emili>","attempt to install torch: Initial Goal: Install torch using pip for your Audiocraft project. Steps Taken: Ran pip install torch in your Command Prompt. Outcome: Encountered several deprecation warnings about outdated package installation methods for other packages like audiocraft and demucs. Received an error message stating pip couldn't find any matching versions for torch (no versions available). Possible Cause: Your previous attempt to install pytorch (incorrect package name) might have confused the package index and disrupted the search for torch. Next Steps: Clean pip cache: Run pip cache clean to remove any remnants of the failed pytorch installation. Rerun with correct name: Try again with pip install torch to use the proper package name. Additional Information: Ignore the deprecation warnings for now, they shouldn't impact torch installation. If the error persists, share the full output from pip install torch for further guidance on interpreting the specific error and finding a solution. PS C:\Users\emili> pip install torch DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\audiocraft1.2.0a2py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\demucs4.0.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\encodec0.1.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\gradio4.10.0py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\librosa0.10.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\protobuf4.25.1py3.12winamd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 DEPRECATION: Loading egg at c:\users\emili\appdata\local\programs\python\python312\lib\sitepackages\torchmetrics1.2.1py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330 ERROR: Could not find a version that satisfies the requirement torch (from versions: none) ERROR: No matching distribution found for torch PS C:\Users\emili>", PyTorch is yet not available for Python3.12 (but nightly builds and upcoming 2.2 release should be)  `git clone recursesubmodules https://github.com/pytorch/pytorch` should check out the repo with submodules,Please do not hesitate to start a discussion thread on https://discuss.pytorch.org/ or open a new issue if you encounter issues with PyTorch Closing as duplicate of  CC(Pytorch for Python 3.12 not available)
1428,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TransformerEncoderLayer and TransformerDecoderLayer do not work)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I'm working with a nextframe prediction with Transformer model. The data I'm working with is a Secure Water Treatment (SWaT) Dataset, which is a scaled down version of a realworld industrial water treatment plant producing filtered water. The data is obtained from https://github.com/JulienAu/Anomaly_Detection_Tuto. I normalized and standardized the data and then load the 10 consecutive data points in order to predict the next data point. I tried the Transformer model, in particular, a `TransformerEncoder` followed by a `TransformerDecoder`. In these layers, I can either use PyTorch's `TransformerEncoderLayer` and `TransformerDecoderLayer` or another implementation that I grabbed from the internet. When I try PyTorch's implementation, I notice that the loss doesn't go down, but with the other implementation, it works. I'm not sure if this is a problem with PyTorch's implementation or there is something wrong with my parameters. Here is the link to the Colab where I produced the issue: https://colab.research.google.com/drive/1oZWMTECuLJHmo4lZ039VdCyOF44y4WYt?usp=sharing And here is the implementation I grabbed from the internet:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,TransformerEncoderLayer and TransformerDecoderLayer do not work," ğŸ› Describe the bug Hi, I'm working with a nextframe prediction with Transformer model. The data I'm working with is a Secure Water Treatment (SWaT) Dataset, which is a scaled down version of a realworld industrial water treatment plant producing filtered water. The data is obtained from https://github.com/JulienAu/Anomaly_Detection_Tuto. I normalized and standardized the data and then load the 10 consecutive data points in order to predict the next data point. I tried the Transformer model, in particular, a `TransformerEncoder` followed by a `TransformerDecoder`. In these layers, I can either use PyTorch's `TransformerEncoderLayer` and `TransformerDecoderLayer` or another implementation that I grabbed from the internet. When I try PyTorch's implementation, I notice that the loss doesn't go down, but with the other implementation, it works. I'm not sure if this is a problem with PyTorch's implementation or there is something wrong with my parameters. Here is the link to the Colab where I produced the issue: https://colab.research.google.com/drive/1oZWMTECuLJHmo4lZ039VdCyOF44y4WYt?usp=sharing And here is the implementation I grabbed from the internet:   Versions  ",2023-12-19T02:32:23Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/116077
1134,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.distributed.DistNetworkError: Unknown error on Windows using gloo on torch dev when training on multigpu.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi there, nice work as always! I'm trying to train a LoRA (low rank adapter) for Stable Diffusion architecture, using https://github.com/kohyass/sdscripts/tree/main Command ran on kohya was:  I'm trying to use multigpu training with gloo (since nccl doesn't work on Windows), but when using torch 2.2/2.3 (dev), I get this issue.  Accelerate was properly configured to just use 1 system (so no distributed training) with `accelerate config` This issue happens with either torch with cuda 11.8 or cuda 12.1 (+cu118/+cu121) Torch 2.1.x doesn't have this issue.  Versions I'm trying to run the command on Windows, but I get `C:\Users\Pancho\AppData\Local\Programs\Python\Python310\python.exe: can't open file 'G:\\acc\\sdscriptscu118\\collect_env.py': [Errno 2] No such file or directory` Running pip list, it shows this  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.distributed.DistNetworkError: Unknown error on Windows using gloo on torch dev when training on multigpu.," ğŸ› Describe the bug Hi there, nice work as always! I'm trying to train a LoRA (low rank adapter) for Stable Diffusion architecture, using https://github.com/kohyass/sdscripts/tree/main Command ran on kohya was:  I'm trying to use multigpu training with gloo (since nccl doesn't work on Windows), but when using torch 2.2/2.3 (dev), I get this issue.  Accelerate was properly configured to just use 1 system (so no distributed training) with `accelerate config` This issue happens with either torch with cuda 11.8 or cuda 12.1 (+cu118/+cu121) Torch 2.1.x doesn't have this issue.  Versions I'm trying to run the command on Windows, but I get `C:\Users\Pancho\AppData\Local\Programs\Python\Python310\python.exe: can't open file 'G:\\acc\\sdscriptscu118\\collect_env.py': [Errno 2] No such file or directory` Running pip list, it shows this  ",2023-12-18T20:12:01Z,oncall: distributed,closed,0,1,https://github.com/pytorch/pytorch/issues/116056,Closing it as torch 2.2.1 fixed it.
1067,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `reset_storage` method to FunctionalTensorWrapper (#115235) (#115320))ï¼Œ å†…å®¹æ˜¯ ( * (to be filled) In certain edge cases when using lazy tensors, the base tensor stored in the `FunctionalStorageImpl` and the `value_` tensor stored in the `FunctionalTensorWrapper` diverge. For instance, take this simple example  The call to `transpose` on the lazily initialized weight `fc1.weight` applies a view op on the functional tensor which only gets propagated to the functional tensor wrapper and not the base tensor in the storage. Thus, causing them to diverge. To fix this behaviour, we need to reset the functional tensor's storage. To facilitate this, we add a `_unsafe_reset_storage` method to `FunctionalTensorWrapper` which clears away the old storage and view metas. Porting over PR from https://github.com/pytorch/pytorch/pull/115235 Cherrypicked: 73c0035160e7b2c5772417bb7206b316bdf34044)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add `reset_storage` method to FunctionalTensorWrapper (#115235) (#115320)," * (to be filled) In certain edge cases when using lazy tensors, the base tensor stored in the `FunctionalStorageImpl` and the `value_` tensor stored in the `FunctionalTensorWrapper` diverge. For instance, take this simple example  The call to `transpose` on the lazily initialized weight `fc1.weight` applies a view op on the functional tensor which only gets propagated to the functional tensor wrapper and not the base tensor in the storage. Thus, causing them to diverge. To fix this behaviour, we need to reset the functional tensor's storage. To facilitate this, we add a `_unsafe_reset_storage` method to `FunctionalTensorWrapper` which clears away the old storage and view metas. Porting over PR from https://github.com/pytorch/pytorch/pull/115235 Cherrypicked: 73c0035160e7b2c5772417bb7206b316bdf34044",2023-12-18T18:25:44Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/116034
1181,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([sparse][wip] split out semi-structured sparsity subclasses)ï¼Œ å†…å®¹æ˜¯ (  CC([sparse][wip] split out semistructured sparsity subclasses) Summary: This PR splits out SparseSemiStructuredTensor into SparseSemiStructuredTensorCUTLASS and SparseSemiStructuredTensorCUSPASRELT. This is because of the following reasons: * different sparse shape constraints for cuSPARSELT and CUTLASS * cuSPARSELt specific metadata (alg_id, fuse_transpose) that we need for   LLM inference. * general readability I decided on two subclasses rather than nested subclasses because * torch.compile support for nested subclasses is an unknown and currently   there's no way to access the nested attributes to flatten the tensor. * Although we could put shared code like aten.linear and padding support in the general subclass,   I think the resulting code is a bit hard to understand, especially how   transpositions are handled, since both the outer and inner tensor will   have a shape. Test Plan: Reviewers: Subscribers: Tasks: Tags:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[sparse][wip] split out semi-structured sparsity subclasses,"  CC([sparse][wip] split out semistructured sparsity subclasses) Summary: This PR splits out SparseSemiStructuredTensor into SparseSemiStructuredTensorCUTLASS and SparseSemiStructuredTensorCUSPASRELT. This is because of the following reasons: * different sparse shape constraints for cuSPARSELT and CUTLASS * cuSPARSELt specific metadata (alg_id, fuse_transpose) that we need for   LLM inference. * general readability I decided on two subclasses rather than nested subclasses because * torch.compile support for nested subclasses is an unknown and currently   there's no way to access the nested attributes to flatten the tensor. * Although we could put shared code like aten.linear and padding support in the general subclass,   I think the resulting code is a bit hard to understand, especially how   transpositions are handled, since both the outer and inner tensor will   have a shape. Test Plan: Reviewers: Subscribers: Tasks: Tags:",2023-12-18T15:25:11Z,release notes: sparse,closed,0,2,https://github.com/pytorch/pytorch/issues/116023," This is still pretty rough so no need to do a real review for now  mostly just checking to see if you have any concerns with splitting out `SparseSemiStructuredTensor`.  I left the class in as a template to handle some shared stuff like shape checking + padding, and also to avoid breaking the scripts that we have out in public so far. ","Thanks!  I'm fine with these changes; in general, my intention is just to try to have functionality of these two backends for sparse semistructured GEMM on par, as much as possible."
1279,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo] bytecode transformed by Dynamo is not serializable by marshal)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug By default, Python code object uses `marshal` for serialization. However, I find that bytecode transformed by Dynamo sometimes is not serializable by `marshal`. The reason is that Dynamo stores a class variable inside `co_consts`. Finally, I managed to use `dill` to save the code object. For those who wants to see the live object, please download it here, and use the code below to load it (`pip install transformers` required):  The object in question is `code.co_consts[7]`, which is a `transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput` class. The class variable also causes problems for debuggers. When I view the code object in debuggers (I'm using VS Code), these variables are invisible. The following screenshot shows the problem: the object in index `7` is not shown.  In summary, while storing class variables in `co_consts` works for Python VM execution, it might not be compatible with the Python ecosystem.  Versions I'm using PyTorch version `2.2.0.dev20231213`. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Dynamo] bytecode transformed by Dynamo is not serializable by marshal," ğŸ› Describe the bug By default, Python code object uses `marshal` for serialization. However, I find that bytecode transformed by Dynamo sometimes is not serializable by `marshal`. The reason is that Dynamo stores a class variable inside `co_consts`. Finally, I managed to use `dill` to save the code object. For those who wants to see the live object, please download it here, and use the code below to load it (`pip install transformers` required):  The object in question is `code.co_consts[7]`, which is a `transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput` class. The class variable also causes problems for debuggers. When I view the code object in debuggers (I'm using VS Code), these variables are invisible. The following screenshot shows the problem: the object in index `7` is not shown.  In summary, while storing class variables in `co_consts` works for Python VM execution, it might not be compatible with the Python ecosystem.  Versions I'm using PyTorch version `2.2.0.dev20231213`. ",2023-12-18T11:13:13Z,low priority triaged bug oncall: pt2 module: dynamo,open,0,7,https://github.com/pytorch/pytorch/issues/116013,"Here is a minimal working example:  The above code can run without any problem, but if we try to `marshal` it, it will raise an exception.", ,"Seems like we should change the way we store that class, though I'm not sure that there are supported usecases for serializing generated bytecode.","My usecase is to dump bytecode so that I can analyze the bytecode and run decompilecompile ""offline"" in CPU only machines (my laptop).","I think the relevant code is: https://github.com/pytorch/pytorch/blob/2a440348958b3f0a2b09458bd76fe5959b371c0c/torch/_dynamo/utils.pyL894 Dynamo thinks these types are safe as constant:  And there are some weird code that pushes a function object in `co_consts`: https://github.com/pytorch/pytorch/blob/2a440348958b3f0a2b09458bd76fe5959b371c0c/torch/_dynamo/codegen.pyL297 If we want to ensure the bytecode can be serializable by `marshal`, that would break lots of code and need quite a big code change. We need to place these objects in global namespace, and issue a `LOAD_GLOBAL` instruction instead. If anyone wants to serialize the bytecode, just using `dill` does the trick. In the long run, I think using `LOAD_GLOBAL` would be better. `co_consts` is the place for constant variables that can be represented safely by a piece of source code. In summary, there are two possible directions:  Make sure bytecode can be serializable by `marshal`, which means we can only have builtin constant types in `co_consts`.  Make sure objects in `co_consts` are real constants. A feasible criterion is listed below. This is a weaker condition.  If we want to go to either direction, we have to move unsafe constants into globals and issue `LOAD_GLOBAL` instructions instead.","> My usecase is to dump bytecode so that I can analyze the bytecode and run decompilecompile ""offline"" in CPU only machines (my laptop). Why do you need serialization for this? Would logging out the bytecode as it gets generated suffice?  ","The logging such as `dis.dis(code)` is lossy, and it is not easy to collect `co_consts/co_nlocals/co_varnames` etc. And the most important thing: decompilation requires a code object."
1213,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([wip] fix llama regression)ï¼Œ å†…å®¹æ˜¯ (Fixes [ CC([inductor][cpu] performance regression)]( CC([inductor][cpu] performance regression)) **Summary** Change the implementation of `MutationLayout.realize_into`  Current implementation of `MutationLayout.realize_into` 1. Eager copy `src_buf` into `src_buf2` 2. Mark `src_buf2.data.layout = MutationLayout(dst)`  Proposed optimized implementation 1. Do lazy copy of src_buf into src_buf2 when dst has been changed     TODO: Can we say dst has extra users if `graph.name_to_users[dst]` increased after `src.data.layout = MutationLayout(dst)`, then we should do the lazy copy of src?  2. Do the lazy copy     2.1 Find all the alais/mutation buffer of dst     2.2 Do the copy of the alais/mutation buffer into alais/mutation buffer2, set `buffer2.data.layout = mutation_layout(dst)`     2.3 Reset these original alais/mutation buffer to FlexibleLayout **TODO** Fix the known issue of fix the zero_element_mutation issue as:  `python u m pytest s v test_torchinductor.py k test_zero_element_mutation` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[wip] fix llama regression,"Fixes [ CC([inductor][cpu] performance regression)]( CC([inductor][cpu] performance regression)) **Summary** Change the implementation of `MutationLayout.realize_into`  Current implementation of `MutationLayout.realize_into` 1. Eager copy `src_buf` into `src_buf2` 2. Mark `src_buf2.data.layout = MutationLayout(dst)`  Proposed optimized implementation 1. Do lazy copy of src_buf into src_buf2 when dst has been changed     TODO: Can we say dst has extra users if `graph.name_to_users[dst]` increased after `src.data.layout = MutationLayout(dst)`, then we should do the lazy copy of src?  2. Do the lazy copy     2.1 Find all the alais/mutation buffer of dst     2.2 Do the copy of the alais/mutation buffer into alais/mutation buffer2, set `buffer2.data.layout = mutation_layout(dst)`     2.3 Reset these original alais/mutation buffer to FlexibleLayout **TODO** Fix the known issue of fix the zero_element_mutation issue as:  `python u m pytest s v test_torchinductor.py k test_zero_element_mutation` ",2023-12-18T09:12:41Z,open source ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/116009
1984,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Faster Pytorch dequantize() + matmul for quantized models)ï¼Œ å†…å®¹æ˜¯ ( Fast Pytorch dequantize() + matmul I would like to open the discussion about faster inference with quantized models using pure Pytorch calls.  As you know, quantization is extremely important to run large models like LLMs with limited GPU memory. This is especially important for the opensource community. Making these models more accessible is a huge step forward.  The quality of quantized models is improving at a fast pace. With the recent releases of HQQ and Quip. We have now reached a point where 4bit and even 2bit models work fairly well and this is surely gonna improve in the upcoming months.  Now the only issue: **inference speed**. While some libraries like llama.cpp are able to achieve _faster_ inference with quantized models, a naive implementation in Pytorch can be 2x slower compared to fp16.  Alternatives  Custom Cuda/Triton kernels Most of the methods like BNB/GPTQ/AWQ/Quip etc. implement custom CUDA/Triton kernels to do the dequantize() + matmul step. These kernels require a very lowlevel implementation and that can only be done efficiently in special cases. Moreover, these kernels are usually optimized for newer Nvidia chips like A100s and it's not possible to use this approach for other hardware like CPU, Intel's Arc, etc. In fact, some Triton kernels optimized for A100 may run _slower_ on older GPUs.   Naive Pytorch Implementation With a pure Pytorch implementation,  this is basically implemented as follows:  The issue is that this is relatively slow due to the  call.   Torch Compile This actually does make things faster with dynamo compile but:  it's still not that fast.  this actually creates issues with async calls for LLM token streaming for example.  Aten C++ Backend I reimplemented the whole )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Faster Pytorch dequantize() + matmul for quantized models," Fast Pytorch dequantize() + matmul I would like to open the discussion about faster inference with quantized models using pure Pytorch calls.  As you know, quantization is extremely important to run large models like LLMs with limited GPU memory. This is especially important for the opensource community. Making these models more accessible is a huge step forward.  The quality of quantized models is improving at a fast pace. With the recent releases of HQQ and Quip. We have now reached a point where 4bit and even 2bit models work fairly well and this is surely gonna improve in the upcoming months.  Now the only issue: **inference speed**. While some libraries like llama.cpp are able to achieve _faster_ inference with quantized models, a naive implementation in Pytorch can be 2x slower compared to fp16.  Alternatives  Custom Cuda/Triton kernels Most of the methods like BNB/GPTQ/AWQ/Quip etc. implement custom CUDA/Triton kernels to do the dequantize() + matmul step. These kernels require a very lowlevel implementation and that can only be done efficiently in special cases. Moreover, these kernels are usually optimized for newer Nvidia chips like A100s and it's not possible to use this approach for other hardware like CPU, Intel's Arc, etc. In fact, some Triton kernels optimized for A100 may run _slower_ on older GPUs.   Naive Pytorch Implementation With a pure Pytorch implementation,  this is basically implemented as follows:  The issue is that this is relatively slow due to the  call.   Torch Compile This actually does make things faster with dynamo compile but:  it's still not that fast.  this actually creates issues with async calls for LLM token streaming for example.  Aten C++ Backend I reimplemented the whole ",2023-12-16T17:08:09Z,oncall: quantization,open,3,5,https://github.com/pytorch/pytorch/issues/115985,Related discussion on packing/unpacking bit tensors:  https://github.com/pytorch/ao/issues/292,Also see https://github.com/pytorchlabs/ao  ,"thanks, we are creating a prototype for int4 tensor here: https://github.com/pytorchlabs/ao/pull/13 as the frontend for all the int4/int3/int2 etc. Tensors.  As for performance I think theoretically a combination of torch.compile codegen (e.g. for dequantize op, or also codegen the fused dequant + mm as well) and custom kernels (e.g. manually implement fused dequant + mm) should help", thank you for your reply!  I see that the int4 implementation is based on Pytorch ops. Did you benchmark the speed of the bitunpacking + torch.compile() ? I was doing something very similar (just using `torch.cat `instead of` torch.stack`). This version is a bit faster: https://github.com/mobiusml/hqq/blob/master/hqq/core/bitpack.pyL35,">  thank you for your reply! >  > I see that the int4 implementation is based on Pytorch ops. Did you benchmark the speed of the bitunpacking + torch.compile() ? I was doing something very similar (just using `torch.cat `instead of` torch.stack`). This version is a bit faster: https://github.com/mobiusml/hqq/blob/master/hqq/core/bitpack.pyL35 oh I haven't get to performance benchmarking yet, right now I'm just trying to make the frontend work, I think we can split this into two steps: (1) make sure this uint4 tensor subclass works as a frontend for all flows we want to support (2) performance tuning for GPU flow or other flows, we can explore this after the PR is landed"
736,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Have an option to disable fast path in TransformerEncoderLayer.)ï¼Œ å†…å®¹æ˜¯ (I would like to export a model using TransformerEncoderLayer as submodule to core Aten ops.  However, given  CC([Export] aten::_transformer_encoder_layer_fwd is not core aten and has no decomposition.) I can't get core Aten ops so far. So I would like to add an option to disable that path. Having this, I would `model.apply` a function to set it true before exporting.  Other ways to work around this issue (such as manually set `self.training` to `True` etc) are not reliable and hacky. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Have an option to disable fast path in TransformerEncoderLayer.,"I would like to export a model using TransformerEncoderLayer as submodule to core Aten ops.  However, given  CC([Export] aten::_transformer_encoder_layer_fwd is not core aten and has no decomposition.) I can't get core Aten ops so far. So I would like to add an option to disable that path. Having this, I would `model.apply` a function to set it true before exporting.  Other ways to work around this issue (such as manually set `self.training` to `True` etc) are not reliable and hacky. ",2023-12-16T00:47:23Z,module: nn triaged open source,closed,0,3,https://github.com/pytorch/pytorch/issues/115971," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," Apologies for the delay! fyi I am working on landing https://github.com/pytorch/pytorch/pull/112212, would that work for you?","FYI the aforementioned PR has landed so I am going to close this PR, feel free to open an issue if the config does not satisfy the ask here."
898,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cache dfs path in propose_partitions and re-use that later when trying to find cycles in the graph)ï¼Œ å†…å®¹æ˜¯ (Summary: This diff introduces a caching mechanism to improve the performance of the partitioner in PyTorch. The changes involve adding a cache to store the DFS path of each node in the graph, which can be reused later when trying to find cycles in the graph. This shows significant improvements for the edge use cases where the ASR model (which is around 6000+ nodes) used to take 26 minutes, but after this it takes around 8 minutes. Test Plan: Relying on the existing ExecuTorch CI tests that heavily use this partitioning mechanism and also tested out locally via Bento notebooks. Differential Revision: D51289200)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Cache dfs path in propose_partitions and re-use that later when trying to find cycles in the graph,"Summary: This diff introduces a caching mechanism to improve the performance of the partitioner in PyTorch. The changes involve adding a cache to store the DFS path of each node in the graph, which can be reused later when trying to find cycles in the graph. This shows significant improvements for the edge use cases where the ASR model (which is around 6000+ nodes) used to take 26 minutes, but after this it takes around 8 minutes. Test Plan: Relying on the existing ExecuTorch CI tests that heavily use this partitioning mechanism and also tested out locally via Bento notebooks. Differential Revision: D51289200",2023-12-15T19:54:47Z,fb-exported Merged ciflow/trunk release notes: fx,closed,0,20,https://github.com/pytorch/pytorch/issues/115943,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200,This pull request was **exported** from Phabricator. Differential Revision: D51289200," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
2040,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(In `autograd.grad`, when using `allow_unused=True` and an input vector with `requires_grad=False` yields an error)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The documentation for `allow_unused` is  If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to the value of `materialize_grads`. Note that there is no specification of what happens if it is set to `True` but I assume that it is the opposite of what is stated when `False`, i.e. : Specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is not an error. Now if we have the following simple example :  This yields the error  Now of course `b` was used in the computation of the output so it is fine with the above definition of `allow_unused` but this is slightly weird to me because `b` actually is not in the backward graph of `c` and therefore returning `None` for its derivative would in principle be fine.  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Fedora release 38 (Thirty Eight) (x86_64) GCC version: (GCC) 13.1.1 20230614 (Red Hat 13.1.14) Clang version: 16.0.5 (Fedora 16.0.51.fc38) CMake version: version 3.26.4 Libc version: glibc2.37 Python version: 3.10.10 (main, May  1 2023, 11:13:19) [GCC 12.2.1 20221121 (Red Hat 12.2.14)] (64bit runtime) Python platform: Linux6.3.8200.fc38.x86_64x86_64withglibc2.37 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit A)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"In `autograd.grad`, when using `allow_unused=True` and an input vector with `requires_grad=False` yields an error"," ğŸ› Describe the bug The documentation for `allow_unused` is  If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to the value of `materialize_grads`. Note that there is no specification of what happens if it is set to `True` but I assume that it is the opposite of what is stated when `False`, i.e. : Specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is not an error. Now if we have the following simple example :  This yields the error  Now of course `b` was used in the computation of the output so it is fine with the above definition of `allow_unused` but this is slightly weird to me because `b` actually is not in the backward graph of `c` and therefore returning `None` for its derivative would in principle be fine.  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Fedora release 38 (Thirty Eight) (x86_64) GCC version: (GCC) 13.1.1 20230614 (Red Hat 13.1.14) Clang version: 16.0.5 (Fedora 16.0.51.fc38) CMake version: version 3.26.4 Libc version: glibc2.37 Python version: 3.10.10 (main, May  1 2023, 11:13:19) [GCC 12.2.1 20221121 (Red Hat 12.2.14)] (64bit runtime) Python platform: Linux6.3.8200.fc38.x86_64x86_64withglibc2.37 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit A",2023-12-15T09:54:27Z,module: autograd triaged has workaround,closed,0,2,https://github.com/pytorch/pytorch/issues/115918,Thanks for the issue! This is expected behavior. You should just mark any tensors you expect to use with .grad with `.requries_grad_(True)`. Does that not work for your you case?,"Thank you for the answer, I was just checking if that was desired."
360,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_transformerencoderlayer_cuda_float16 (__main__.TestNNDeviceTypeCUDA))ï¼Œ å†…å®¹æ˜¯ (Platforms:rocm This test was disabled because it is failing on main branch (recent examples). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_transformerencoderlayer_cuda_float16 (__main__.TestNNDeviceTypeCUDA),Platforms:rocm This test was disabled because it is failing on main branch (recent examples). ,2023-12-15T06:34:45Z,module: rocm triaged skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/115912,Closing as change has been reverted
1148,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable mostly-complete import-following for MYPYINDUCTOR)ï¼Œ å†…å®¹æ˜¯ (In particular, let's delete these `follow_imports=skip` lines in mypyinductor.ini: https://github.com/pytorch/pytorch/blob/ef01e78fd989c0e8a3fb817a9988a185b71d9084/mypyinductor.iniL53L59 https://github.com/pytorch/pytorch/blob/ef01e78fd989c0e8a3fb817a9988a185b71d9084/mypyinductor.iniL64L70 This will cause `follow_imports` to default to the value set at the top of the file: https://github.com/pytorch/pytorch/blob/ef01e78fd989c0e8a3fb817a9988a185b71d9084/mypyinductor.iniL11 There are a few strategies for breaking this task down. First, you can enable importing of specific files in the module by adding an overriding config setting:  Another angle of attack is temporarily turning on `follow_imports=normal` for those modules, which will show any type errors that mypy has detected in them:  These may point to wrong type signatures that then cause downstream errors when imported into inductor modules. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Enable mostly-complete import-following for MYPYINDUCTOR,"In particular, let's delete these `follow_imports=skip` lines in mypyinductor.ini: https://github.com/pytorch/pytorch/blob/ef01e78fd989c0e8a3fb817a9988a185b71d9084/mypyinductor.iniL53L59 https://github.com/pytorch/pytorch/blob/ef01e78fd989c0e8a3fb817a9988a185b71d9084/mypyinductor.iniL64L70 This will cause `follow_imports` to default to the value set at the top of the file: https://github.com/pytorch/pytorch/blob/ef01e78fd989c0e8a3fb817a9988a185b71d9084/mypyinductor.iniL11 There are a few strategies for breaking this task down. First, you can enable importing of specific files in the module by adding an overriding config setting:  Another angle of attack is temporarily turning on `follow_imports=normal` for those modules, which will show any type errors that mypy has detected in them:  These may point to wrong type signatures that then cause downstream errors when imported into inductor modules. ",2023-12-14T03:48:49Z,high priority triage review module: typing triaged actionable oncall: pt2,closed,0,4,https://github.com/pytorch/pytorch/issues/115805,PR for deleting [mypytorch.backends.*]  https://github.com/pytorch/pytorch/pull/116311,There's some squiggly code in Dynamo because it's a bit difficult to exercise all the branches. Fixing all the typing issues would help.,This is done.,"For context to future readers, this was done mostly via type ignores, and there are a lot of them that still have to be cleaned up in those modules mentioned above."
794,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fakeifying tensor subclasses that are views is broken)ï¼Œ å†…å®¹æ˜¯ (Context: Int4 subclass ran into this here: https://github.com/pytorchlabs/ao/pull/13/filesr1425980422 min repro:  Fails this assertion: https://github.com/pytorch/pytorch/blob/main/torch/_subclasses/meta_utils.pyL474 It looks like that's because: (1) When we fakeify a tensor that has a `._base`, we need to fakeify the ._base (2) When we start to fakeify the ._base, we unconditionally give it's base a `StatelessSymbolicContext` (code) (3) That's wrong  the base can itself be another subclass, in which case we should give it a `SubclassSymbolicContext` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fakeifying tensor subclasses that are views is broken,"Context: Int4 subclass ran into this here: https://github.com/pytorchlabs/ao/pull/13/filesr1425980422 min repro:  Fails this assertion: https://github.com/pytorch/pytorch/blob/main/torch/_subclasses/meta_utils.pyL474 It looks like that's because: (1) When we fakeify a tensor that has a `._base`, we need to fakeify the ._base (2) When we start to fakeify the ._base, we unconditionally give it's base a `StatelessSymbolicContext` (code) (3) That's wrong  the base can itself be another subclass, in which case we should give it a `SubclassSymbolicContext` ",2023-12-13T23:16:30Z,triaged module: __torch_dispatch__ oncall: pt2 module: dynamic shapes module: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/115782,", I wonder if your work around handling fakifying subclass views will end up fixing this issue",This is known and discussed offline  I think we have a plan here. ,Closing as fixed in CC(Subclass view fakeification via reified ViewFuncs).
359,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile doesn't seem to guard on register_full_module_backward_hook, leading to silent incorrectness)ï¼Œ å†…å®¹æ˜¯ ( shows tensor([1., 1., 1.]) but should really be a tensor full of 2s )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"torch.compile doesn't seem to guard on register_full_module_backward_hook, leading to silent incorrectness"," shows tensor([1., 1., 1.]) but should really be a tensor full of 2s ",2023-12-13T16:56:34Z,high priority triaged actionable oncall: pt2 module: dynamo internal ramp-up task,closed,0,2,https://github.com/pytorch/pytorch/issues/115750,"I think we just silently ignore some random subset of hooks lol, we should audit and implement them all using existing approaches (inline for fwd on tensor, unspec+inline for fwd on module), trace_wrapped for backward + compiled autograd","Closing as duplicate of  CC([dynamo][inline-inbuilt-nn-modules]torch.compile silently incorrect with full_backward_pre_hook) (or well, it should have been done other way around, but does not really matter)"
842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(huggingface gpt2 has missing inputs on torch.onnx.dynamo_export using torch.nn.Module model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Test disabled: test/onnx/test_fx_to_onnx_with_onnxruntime.py::test_fake_tensor_mode_huggingface_tiny_gpt2 Repro: `pytest sv test/onnx/test_fx_to_onnx_with_onnxruntime.py::TestFxToOnnxFakeTensorWithOnnxRuntime_op_level_debug_False_dynamic_shapes_False_load_checkpoint_during_init_False_export_within_fake_mode_False_model_type_TorchModelType.TORCH_NN_MODULE::test_fake_tensor` Error:  Extra information: Model state dict:  Model buffers:  Pytorch inputs provided to ONNX model:  ONNX expected inputs:   Versions pytorch main and latest transformers)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,huggingface gpt2 has missing inputs on torch.onnx.dynamo_export using torch.nn.Module model, ğŸ› Describe the bug Test disabled: test/onnx/test_fx_to_onnx_with_onnxruntime.py::test_fake_tensor_mode_huggingface_tiny_gpt2 Repro: `pytest sv test/onnx/test_fx_to_onnx_with_onnxruntime.py::TestFxToOnnxFakeTensorWithOnnxRuntime_op_level_debug_False_dynamic_shapes_False_load_checkpoint_during_init_False_export_within_fake_mode_False_model_type_TorchModelType.TORCH_NN_MODULE::test_fake_tensor` Error:  Extra information: Model state dict:  Model buffers:  Pytorch inputs provided to ONNX model:  ONNX expected inputs:   Versions pytorch main and latest transformers,2023-12-13T16:30:29Z,module: onnx triaged onnx-triaged release notes: onnx,closed,0,3,https://github.com/pytorch/pytorch/issues/115745,nonpersistent buffer strikes again?,"> nonpersistent buffer strikes again? yup. It works for the ExportedProgram, but this tracks for the torch.nn.module path when exporting gpt2",https://github.com/pytorch/pytorch/pull/118876
738,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add pragma once to headers)ï¼Œ å†…å®¹æ˜¯ (  CC(Reenable torch function subclass for calls to ops with Proxy during AOTAutograd)  CC(Add jagged NT support for expand_as and sum)  CC(Mincut partitioner always saves tensors that are returned asis in backward)  CC(Import generated/variable_factories.h for use in Derivatives.yaml)  CC(Factory functions allow singleton size)  CC(Workaround to avoid MSVC std ambiguous symbol error)  CC(Add pragma once to headers)  CC(Move SingletonSymNodeImpl from c10 to aten) This reverts commit 9b93c23b5e2d695c2fbd9c886cc0c8010edab717.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add pragma once to headers,  CC(Reenable torch function subclass for calls to ops with Proxy during AOTAutograd)  CC(Add jagged NT support for expand_as and sum)  CC(Mincut partitioner always saves tensors that are returned asis in backward)  CC(Import generated/variable_factories.h for use in Derivatives.yaml)  CC(Factory functions allow singleton size)  CC(Workaround to avoid MSVC std ambiguous symbol error)  CC(Add pragma once to headers)  CC(Move SingletonSymNodeImpl from c10 to aten) This reverts commit 9b93c23b5e2d695c2fbd9c886cc0c8010edab717.,2023-12-13T15:28:44Z,Merged ciflow/trunk topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/115739,Factoring this PR out to debug a windows build issue.
1999,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error in all_reduce when GPT2 200B inferencing with dynamo and multi GPU)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug We except to be able to do inference with dynamo, and we successfully inference when setting ""fullgraph=False"" in torch.compile. However, it is doesn't work when ""fullgraph=True"" in torch.compile with the same code Code snippet:  torch.compile setting:  Error messages:   Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.24.1 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12.59.47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0125genericx86_64withglibc2.29 Is CUDA avaliable: True CUDA runtime version: 12.1.66 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 535.86.10 cuDNN version: Probably one of following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.8.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.13.0 [pip3] pytorchquantization==2.1.2 [pip3] torch==2.1.0 [pip3] t)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Error in all_reduce when GPT2 200B inferencing with dynamo and multi GPU," ğŸ› Describe the bug We except to be able to do inference with dynamo, and we successfully inference when setting ""fullgraph=False"" in torch.compile. However, it is doesn't work when ""fullgraph=True"" in torch.compile with the same code Code snippet:  torch.compile setting:  Error messages:   Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.24.1 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12.59.47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0125genericx86_64withglibc2.29 Is CUDA avaliable: True CUDA runtime version: 12.1.66 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 535.86.10 cuDNN version: Probably one of following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.8.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.8.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] onnx==1.13.0 [pip3] pytorchquantization==2.1.2 [pip3] torch==2.1.0 [pip3] t",2023-12-13T07:11:42Z,oncall: distributed,closed,0,1,https://github.com/pytorch/pytorch/issues/115728,we can probably make this case work automatically.   1) dynamo can only 'really' trace functional collectives. 2) you're using a nonfunctional collective 3) but dynamo has support for automatically rewriting a nonfunctional collective to a functional one without user intervention.  It just doesn't support allreduce yet. you can workaround by calling functional_collectives allreduce in your script.  but we will also work on the dynamo rewriter.  
412,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(numerical mismatch fix for test_mem_efficient_attention_attn_mask_vs_math_ref_grads in test_transformers.py)ï¼Œ å†…å®¹æ˜¯ (adjust dropout_fudge_factor since previous fudge factor was too small and led to numerical mismatch in NVIDIA internal CI)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,numerical mismatch fix for test_mem_efficient_attention_attn_mask_vs_math_ref_grads in test_transformers.py,adjust dropout_fudge_factor since previous fudge factor was too small and led to numerical mismatch in NVIDIA internal CI,2023-12-12T23:06:16Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/115707,"This was from October, but from what I recall we saw it on certain H100 machines as well as both x86 and arm A100. ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1578,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([funcol] Directly import DeviceMesh to avoid circular dependency)ï¼Œ å†…å®¹æ˜¯ (  CC([DDP][PT2D] Allreduce fusion fx pass using concat and all_reduce_coalesced)  CC([DDP] Use compiled_autograd to trace DDP backward allreduce)  CC(Let all_reduce_coalesced accept one tensor as well)  CC([funcol] Directly import DeviceMesh to avoid circular dependency)  CC([funcol][BE] Apply ufmt to _functional_collectives.py and turn on lintrunner for functional_collective)  CC([DCP][BE] Apply ufmt to DCP and turn on lintrunner for DCP)  CC([DCP][BE] Move DCP._state_dict_utils out from DCP) This diff aims to directly import DeviceMesh from torch.distributed.device_mesh instead of importing it from dist._tensor. This is done to avoid a circular dependency issue. The code changes in each file of the diff are as follows:  torch/distributed/_functional_collectives.py: import DeviceMesh from torch.distributed instead of dist._tensor. Overall, this diff aims to improve the code by avoiding circular dependencies and improving the import statements. == The above summary is generated by LLM with minor manual fixes. The following summary is by me. The original import causes some issues when compiling DDP with compiled_autograd. The root cause of compilation failure is not identified but it is good to fix the lazy initialization, which indirectly fixes the compilation issues for DDP. Differential Revision: D51857246 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[funcol] Directly import DeviceMesh to avoid circular dependency,"  CC([DDP][PT2D] Allreduce fusion fx pass using concat and all_reduce_coalesced)  CC([DDP] Use compiled_autograd to trace DDP backward allreduce)  CC(Let all_reduce_coalesced accept one tensor as well)  CC([funcol] Directly import DeviceMesh to avoid circular dependency)  CC([funcol][BE] Apply ufmt to _functional_collectives.py and turn on lintrunner for functional_collective)  CC([DCP][BE] Apply ufmt to DCP and turn on lintrunner for DCP)  CC([DCP][BE] Move DCP._state_dict_utils out from DCP) This diff aims to directly import DeviceMesh from torch.distributed.device_mesh instead of importing it from dist._tensor. This is done to avoid a circular dependency issue. The code changes in each file of the diff are as follows:  torch/distributed/_functional_collectives.py: import DeviceMesh from torch.distributed instead of dist._tensor. Overall, this diff aims to improve the code by avoiding circular dependencies and improving the import statements. == The above summary is generated by LLM with minor manual fixes. The following summary is by me. The original import causes some issues when compiling DDP with compiled_autograd. The root cause of compilation failure is not identified but it is good to fix the lazy initialization, which indirectly fixes the compilation issues for DDP. Differential Revision: D51857246 ",2023-12-12T18:04:49Z,oncall: distributed Merged ciflow/trunk ciflow/periodic,closed,0,2,https://github.com/pytorch/pytorch/issues/115649, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
249,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build docker release specifying pytorch version)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Build docker release specifying pytorch version,Fixes ISSUE_NUMBER,2023-12-12T14:05:53Z,release notes: releng ciflow/mps,closed,0,1,https://github.com/pytorch/pytorch/issues/115632," :white_check_mark: login: atalman / name: Andrey Talman  (17f400404f2ca07ea5ac864428e3d08149de2304, 6d20b39d3f36cdd52ae35d078985b6905a8da550, 050fc31538fc03af63757b6481ef75cd8cf455e1, 48246f3dfb65738208afe87236a4eab1c6d5f9e7, 6e4ae136578ba2e10460a50da0b0146de01e9238, 1191449343d4707f2725a5d16b6a4bc48c6eb9f7, d8db5808ce7174c1e6c182b3b569c979c3d3ea32, 5aae9796146ed99eb0422846c8198bd1c3ffc532, da9639c75200e3ef2919a2917ad95242c36e60be, c464075d5d2a3f9935f9a5fb9a9abc26d3256e40, 1841d54370d167365d15f0ac78efc2c56cdf43ab, e4c42a93bc6541ecc16ec677912375d11d13fdda, 88cde0c37c68e2f50f78544d7a12783058e2111e, e68aa76642097904d6631270ef5968d3337640a5, e6702486f6b3325f7f990c0f8b2b9e7dae8cd7dc, fa1db4310ddf31cf339f6d809101174d4900404b, 209f2fa8ff86652f67d75c2f19bf9cb9942fd018, 909fcf9b217530f7c793c3797e78945030efc79b, e7892b2e02dbef200ab9f4d77f6418ab17608d54, 3002bf71e6ea947bd0175b473f8483eb3cfc997d, c07240e5e4999bd35dc772a5ad408d145901882a, 39a66a66fe0c71a3190d7a3ca497e1565f233485, c496f9a40ba39f451758492977b1d61fcd7aa61c, 5bcfb1b9b451a50e25b3bbc1f745aed55894fcb0, b3b274ddcbc9db25abcfcb12adaa316192a7af50, 6ba919da27854e9d79e3b2b72f9f6a56cce50b57, ab5b9192ceb5b84e1906788046f43be4075e2745, 3f592210623d8d0aa59a09277165ae02f48d7ee1, af1590cdf47a3fd2d44137e51822d53854e0256c, 614af503787a81cfa58ce69640271432b48b23ee, 3f662b62550aef16eec4a6e960c0a96b83944a1c, 41210eaedc4b4c76be123aacd16e175edef524c6, 7ea63fdf1502d8d458171b692ebd1afdef31948e):white_check_mark: login: mikaylagawarecki  (f82c027774c3d46a0e433d7457bcdf79ae67aaad):white_check_mark: login: XiaobingSuper / name: XiaobingZhang  (d8e6594fb80625da85ae988f4add323231ae8b9f):white_check_mark: login: huydhn / name: Huy Do  (9175987fccc71a0197a09ce31505882deac9dd4d, f187e42a54d49f4daa2f557e27e0c5ebf2c588b2, da1ccca830dda18423ec8191d942c9ebcc953cb9, 5252dfb7626a64d1513f2e8cf02ae5ca4f6d9160, 8a3b01776972520c6bfb047dcfb52a0a52438bc8, d07ac50e26352af975d1edf8821762047775ebd9, c0e7239f4357f1496d6480de2bb36ea331e8e910, ee67c4dd6a6de38c3e53a792e64d1aac4ba5012e, 0f9ac00ac62306409804545361c6081ef8b34982, 6026c29db0889939bfcb2be395f1c31a8a89b933, dd7fb44d20bf7f6b6de491ec300980aabe4fdf25, 0bc598a6043142c1b15dd702faac26e54705d0ea, cc54a5072e4b355593f59c7638e5e7a3b5f708b1, 2dc37f4f70076c0390e9787c6a941aab926c5bea, ee79fc8a35e3a075ccb8370e09f12eb4b32bd48e, b249946c40eeeb3b2fc0c48e2086087351da2d8a, b5a89bbc5f98c90ad5a43421870447ef555ff341, 448700d18e12b45f6c21f467542300ea287b3961):white_check_mark: login: jataylo / name: Jack Taylor  (c9cbdaf24fa35d8d3f51b2cc39c08fa404b720cb):white_check_mark: login: kshitij12345 / name: Kshiteej K  (f139dda1cc1162f3bd3de55af2669b5d0b1e49a4, 5417e23ba8e91c6b1caced9b4274822f7d393ae5):white_check_mark: login: CaoE / name: Cao E  (fec68a2799ab2b286e7960fba036626ca37bbb29):white_check_mark: login: hasteinmetz / name: Hillel (Hilly) Steinmetz  (b3cb05b396aa81270e96a1ae4c04518b0ed09d02):white_check_mark: login: Chillee / name: Horace He  (a82894b0d31dbeceee732f4f7cfd7338dee6ad57, cd5859373cb7b17b4f505e57c5baad8a42b013f2):white_check_mark: login: jansel / name: Jason Ansel  (35c3d5a080cc8667fb4a161d21459ee0ee10a76a, 90452f41e384e25ffbaf7bca15c13dc6f418fbd7, 085bd1da62f924caa1d350eec2114f6ce408b54f, 83964c761e79ce81735ac086629ee71f924ce94d, bddd30ca7ac4f9bf4d0f2780a63c862cf601209e):white_check_mark: login: shunting314  (a49fca4dd47038bd7d08cddc3f6bed64d4e7d47d):white_check_mark: login: AlekseiNikiforovIBM / name: Aleksei Nikiforov  (9cc99906e94b792c4e4b8067b95f3cebfd8b614a):white_check_mark: login: andrewor14  (cb4362ba5f5cd88b6a237db4ce7d67ff0a316570, 04c1e07fd7e0189d0ab8dd9f1b9facb6413fa4c9, 7e23b4907da632f995c379fe02604b961617d465):white_check_mark: login: Valentine233 / name: Xuan Liao  (03e7f0b99dd5c1cb2e8418443751f5e08fc58d1f):white_check_mark: login: albanD  (7a9101951d2b33a39b0091970baee6b8d52f139d):white_check_mark: login: wanchaol / name: Wanchao  (7d6971dcee292939427fd9c0c51a71e75eaccece, 6b7a777661927881672483323cab604847c41577):white_check_mark: login: kobrineli / name: Eli Kobrin  (48246f3dfb65738208afe87236a4eab1c6d5f9e7):white_check_mark: login: chunyuanw / name: Chunyuan WU  (828992cf13d0814a8dcf88274b6bf6b5c78c8d15):white_check_mark: login: justinchuby / name: Justin Chu  (da7290dfbd908b358120fde797257a686791e1fc, bd372d460b66e1334378f4e5a2a4ef0b7a44b6b8):white_check_mark: login: kit1980 / name: Sergii Dymchenko  (265e46e1932f77eca8fe78be310eaba008842fd2, ab5ea22c1dd524bde814e50bb0962561190f54e6):white_check_mark: login: fakeYan / name: Bug Hunter Yan  (ebd322430373b3e78edc1bdeaba6d97817e13534):white_check_mark: login: jingxu10 / name: Jing Xu  (c5c9536aa7a9130bded58d3ef4aa9674c716adac):white_check_mark: login: eellison  (ba19c52e31146ca23339f45661ff6bd50d07b9ce):white_check_mark: login: VRSinghHabana / name: Vishwa Raj Singh  (d83c8287ea7676a7e89249d81129c2f698300345):white_check_mark: login: cyyever / name: cyy  (fa8259db8d3bd97b9e77174a31155de1db9227d7):white_check_mark: login: ezyang / name: Edward Z. Yang  (7397cf324c6e2c0bd639776fd410486f95f4b98d, 01fa8c140a0255f88f3fc39e608d927d6a8720a1):white_check_mark: login: angelayi / name: Angela Yi  (12b8c26f350fcee2e7deb19d0c3e997f4c848cdf, ed62318bea023d81b63e9bbeeb843cbb972c95d1, ced78cc2a797229c3ebe2096e7e6d52ef3911221):white_check_mark: login: abock / name: Aaron Bockover  (bd372d460b66e1334378f4e5a2a4ef0b7a44b6b8, 6d9fad84747d81c36aacd0127207d0c4a4a43bd4, 889811ab5bbc53efbae7e510176b61b249016ada):white_check_mark: login: janeyx99 / name: Jane (Yuan) Xu  (ce3ed7f293462007d32bd6ab8f0a7005d83b2e6b):white_check_mark: login: wz337 / name: Iris Z  (91e414957b37a82eab8d06686ad30bb56c36fa6f, 1f0450eed28f2b17a55da488f21602b3e32f1c59, 47ac50248a047aa45f0f5d358b1069361d69f7a0, 4b4c012a6033dbebd432706f861df7430b87d95b, 33106b706e9e60da1ee5c12649b0c7c30c3e9c5b, 4c55dc50355d5e923642c59ad2a23d6ad54711e7):white_check_mark: login: thiagocrepaldi / name: Thiago Crepaldi  (71c9d5c3a6cf83a5d90c53e01c2eaf86075e56cd, 6d9fad84747d81c36aacd0127207d0c4a4a43bd4, 889811ab5bbc53efbae7e510176b61b249016ada, 9570baa150233ed843c888b85aade3ae4ad7a178):white_check_mark: login: mrnikwaws  (5529b8163151f9278c48a89e629c58c015603ce1):white_check_mark: login: lezcano / name: Mario Lezcano Casado  (e534243ec2a9f3ea52816c2b01f5864c9ed4fd22):white_check_mark: login: malfet / name: Nikita Shulga  (28220534dee1c08d1abd8856fb4aac94bde477e7, 1b4161c68627be6691d099397570a7a53bea2665, 9287a0cf59cf5e849d9aa93ed6b7003b534a9d7f, 539a97116106e818fcf3dd044658b2d2e01f6a21, fca42334bee4f5cd3b2cbb7e60f416672a16b0bf, 7bcf7da3a268b435777fe87c7794c382f444e86d, 736ebd3313fa029e05d65a5c757ddce6c3b598f4, c1bc460377bbf65a5e67c214bdd5158888a93a08, 2353915d69fc599c4f9d6a15e8a7178be33bc52a, 084343ee1278f20a29f3164501b2c8ae8ead90a7, f58669bc5fc7d89650794881ab7cf0029b5f5bb3, 7833889a44317c6d9012356e1a5fb83ccd7ad343, d62c7575338b6e3314363ce5fd497f805b764ebb, 7405d70c300bdd6e6edefa387dc73a8bebca32a0, b3b22d7390784e2e231f4ed4cbc003bb8caaaf14):white_check_mark: login: tringwald / name: Tobias Ringwald  (bb96803a35a8588020c9317470c04b4e3c90492c, 8a178f153ef5dc6693069ee8cd2ff5ce5992220d):white_check_mark: login: Skylion007 / name: Aaron Gokaslan  (ed87177528c01ed2c836e31a0ad7153e1f83c3a0, f82d6e41a46fca093f4aa4658d287da6f0abf54e):white_check_mark: login: mvpatel2000 / name: Mihir Patel  (ed87177528c01ed2c836e31a0ad7153e1f83c3a0):white_check_mark: login: eqy  (3788d86e3e5185eda9416f4dd5e0e64db7d531cd, c79d2936d0c88e23f2b2b6c5a717371d1e2c7b5d):white_check_mark: login: apach301 / name: Daniil Kutz  (af1590cdf47a3fd2d44137e51822d53854e0256c):white_check_mark: login: peterbell10  (7cc6081f874b7a70d2aa6b93956ef303b85ad927):white_check_mark: login: BowenBao / name: Bowen Bao  (5274580eb0360b20e27bbd048fc7d7adfb85fe69):white_check_mark: login: qqaatw / name: LiHuai (Allan) Lin  (c1bc460377bbf65a5e67c214bdd5158888a93a08):white_check_mark: login: drisspg / name: Driss Guessous  (b2e1277247311934dcd962afca6143d9868ab365, 18a2ed1db198c5fd017231331fceed6c3ae3227f, dc96ecb8acba739ea3fca9882cbb4be5662352bc, 59656491f3b1da809312942872cce010337504b0):white_check_mark: login: imzhuhl / name: Honglin Zhu  (3183bcd417a1f1c9d5ab87061d88f796a4d5735c):x:  login:  / name: Gavin Zhao . The commit (fa8259db8d3bd97b9e77174a31155de1db9227d7) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."
1451,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Alternatives to compiling a for loop)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug So I've been trying to compile some autoregressive transformer models and they consist of for loops within the model architecture. I'm posting a simplified version of the architecture here  An interesting observation I had while running this snip was the timings I got for compiled models  `torch.compile()` is caching the compiles for previously encountered `frameNums` and using them whenever it encounters them again From my limited understanding of the issues  CC(torch.compile of simple loop takes 34 seconds) and  CC([Dynamo][Compile]Torch compile with dynamic shapes not working)issuecomment1636949119 `torch.compile()` unrolls the for loop.  My issue here is that the dataloader serves different input sizes (temporally) so the number of iterations change for every sample.  Every time a new input size is encountered it compiled the model again (which is infeasible).  One solution was to cap the data being served (during training) to the smallest sequence length but that doesn't solve the issue during inference since I don't know what would be the input size.  Is there a workaround I can use to get rid of loops in such models (or any model which have loops dependent on input size) ?   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Alternatives to compiling a for loop, ğŸ› Describe the bug So I've been trying to compile some autoregressive transformer models and they consist of for loops within the model architecture. I'm posting a simplified version of the architecture here  An interesting observation I had while running this snip was the timings I got for compiled models  `torch.compile()` is caching the compiles for previously encountered `frameNums` and using them whenever it encounters them again From my limited understanding of the issues  CC(torch.compile of simple loop takes 34 seconds) and  CC([Dynamo][Compile]Torch compile with dynamic shapes not working)issuecomment1636949119 `torch.compile()` unrolls the for loop.  My issue here is that the dataloader serves different input sizes (temporally) so the number of iterations change for every sample.  Every time a new input size is encountered it compiled the model again (which is infeasible).  One solution was to cap the data being served (during training) to the smallest sequence length but that doesn't solve the issue during inference since I don't know what would be the input size.  Is there a workaround I can use to get rid of loops in such models (or any model which have loops dependent on input size) ?   Versions  ,2023-12-12T11:59:45Z,feature triaged months oncall: pt2 module: dynamo module: higher order operators module: pt2-dispatcher,open,0,3,https://github.com/pytorch/pytorch/issues/115626,Maybe related?    CC([feature request] `torch.scan` (also port `lax.fori_loop` / `lax.while_loop` / `lax.associative_scan` and hopefully parallelized associative scans)),"Yes, it does seem to be related. I've never used `lax` or `tf.scan` before.  Forgive my limited understanding, but compiling the body of the loop itself (in my case the mask functions and the transformer layer) and applying a dynamic number of iterations on said compile does seem like the possible solution to my problem (instead of recompiling every time the input shape changes) I'll keep an eye out on that thread for possible solutions. Hopefully `torch.scan` solves my issue  Thanks :smile: ",deepreel I think some work on correctly compiling forloops is underway and you may find some related issues/PRs in the this link above. Please chime in the discussion there as well :)
552,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Performance regression using latest nightlies and HF transformers )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am seeing a performance regression using the latest nightlies.  I use the following code to reproduce:   Error logs This is the behaviour using `torch==2.2.0.dev20231102`:  Note that the time after switching to `run` is particularly bad.   Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Performance regression using latest nightlies and HF transformers , ğŸ› Describe the bug I am seeing a performance regression using the latest nightlies.  I use the following code to reproduce:   Error logs This is the behaviour using `torch==2.2.0.dev20231102`:  Note that the time after switching to `run` is particularly bad.   Minified repro _No response_  Versions  ,2023-12-11T19:55:57Z,high priority needs reproduction module: performance triaged module: regression oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/115575,Seems like a large regression,"I can't reproduce on my end, I'm running it on A100 80G GPU, got the following result pretty consistently:   Can you run  to get more info of your env?","We can't reproduce it, I'll close this, feel free to reopen if this is still an issue."
546,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([CI] Lower the smoketest speedup threshold for nangpt)ï¼Œ å†…å®¹æ˜¯ (  CC([CI] Lower the smoketest speedup threshold for nangpt) Summary: https://github.com/pytorch/pytorch/actions/runs/7158691360/job/19491437314 shows the variance can be larger than previously expected. Lowering it for now and if it continues to be a problem, we should switch to some other more stable model.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[CI] Lower the smoketest speedup threshold for nangpt,"  CC([CI] Lower the smoketest speedup threshold for nangpt) Summary: https://github.com/pytorch/pytorch/actions/runs/7158691360/job/19491437314 shows the variance can be larger than previously expected. Lowering it for now and if it continues to be a problem, we should switch to some other more stable model.",2023-12-11T18:12:25Z,Merged topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/115562," merge f ""smoke test passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1101,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(inductor: dont specialize dynamic shapes in the noop removal pass even if we have hints)ï¼Œ å†…å®¹æ˜¯ (Need to add a test before landing. This issue came from   when running export on llama 2, we wanted to explicitly mark the sequence length as dynamic:  However  when printing the dynamic shapes logs, we get:  This comes from inductor's noop removal pass, which tried to remove a potential noop in the graph, that is only sound to remove for certain shapes. From some discussion with dynamic shapes folks, it seems like it would be **wrong** to implicitly specialize here, since export explicitly asked for a graph with a dynamic sequence length, and specializing will require us to guard because the resulting graph (where we removed the nop) is incorrect for other shapes.   CC(inductor: dont specialize dynamic shapes in the noop removal pass even if we have hints)  CC(inductor: allow reinplacing input mutations from getattr) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,inductor: dont specialize dynamic shapes in the noop removal pass even if we have hints,"Need to add a test before landing. This issue came from   when running export on llama 2, we wanted to explicitly mark the sequence length as dynamic:  However  when printing the dynamic shapes logs, we get:  This comes from inductor's noop removal pass, which tried to remove a potential noop in the graph, that is only sound to remove for certain shapes. From some discussion with dynamic shapes folks, it seems like it would be **wrong** to implicitly specialize here, since export explicitly asked for a graph with a dynamic sequence length, and specializing will require us to guard because the resulting graph (where we removed the nop) is incorrect for other shapes.   CC(inductor: dont specialize dynamic shapes in the noop removal pass even if we have hints)  CC(inductor: allow reinplacing input mutations from getattr) ",2023-12-11T17:45:51Z,Stale release notes: fx module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/115555,Have not yet had a chance to look at the failing tests (hopefully will come back to it in the new year),"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
306,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.export.export fail to export OpenLLama model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  error:   Versions pytorchmain transformers==4.31.0 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.export.export fail to export OpenLLama model, ğŸ› Describe the bug  error:   Versions pytorchmain transformers==4.31.0 ,2023-12-11T16:57:01Z,oncall: export,closed,0,6,https://github.com/pytorch/pytorch/issues/115552,"I think `BaseModelOutputWithPast` needs to be registered as a pytree node. I thought this was supposed to do it in the transformers codebase, but maybe you need to explicitly do this too?",Need to bump transformers version in onnx CI it appears  ,> Need to bump transformers version in onnx CI it appears  Indeed it fails with `transformers==4.36.0` while it works with `==4.32.1`.,OpenLlama is deprecated. We should go with LlamaConfig/LlamaModeling,https://huggingface.co/docs/transformers/main/model_doc/openllama,https://github.com/pytorch/pytorch/pull/117703
1079,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([sparse][semi-structured] enable fp32 support, separate sparse and dense constraints)ï¼Œ å†…å®¹æ˜¯ (  CC([sparse][semistructured] enable fp32 support, separate sparse and dense constraints) Summary: Both cuSPASRELt and CUTLASS support 1:2 semistructured sparsity for fp32, which this PR enables.(thanks ). Furthermore, this PR also updates the sparse_config to take into account the different shape constraints for sparse and dense matrices. Technically, cuSPARSELt supports smaller sparse matrix constraints as it seens to pad to the CUTLASS constraints under the hood. However, in practice small sparse matrices are not commonly used and we care more about the dense constraints for LLM inference. For now, we keep the CUTLASS constraints in place for both cuSPARSELt and CUTLASS tensors This PR also reconnects the _FUSE_TRANSPOSE flag for cuSPARSELt tensors. Test Plan:  Reviewers: Subscribers: Tasks: Tags:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"[sparse][semi-structured] enable fp32 support, separate sparse and dense constraints","  CC([sparse][semistructured] enable fp32 support, separate sparse and dense constraints) Summary: Both cuSPASRELt and CUTLASS support 1:2 semistructured sparsity for fp32, which this PR enables.(thanks ). Furthermore, this PR also updates the sparse_config to take into account the different shape constraints for sparse and dense matrices. Technically, cuSPARSELt supports smaller sparse matrix constraints as it seens to pad to the CUTLASS constraints under the hood. However, in practice small sparse matrices are not commonly used and we care more about the dense constraints for LLM inference. For now, we keep the CUTLASS constraints in place for both cuSPARSELt and CUTLASS tensors This PR also reconnects the _FUSE_TRANSPOSE flag for cuSPARSELt tensors. Test Plan:  Reviewers: Subscribers: Tasks: Tags:",2023-12-11T16:36:26Z,Merged ciflow/trunk release notes: sparse,closed,0,28,https://github.com/pytorch/pytorch/issues/115550,"It should be simple to enable float32 support for CUTLASS backend, so let me check if we can include it through this PR too. Also, eventual input shapes limitations should be dictated by hardware, thus common between the two libraries.  I mean: as far as I know, CUTLASS is not doing any padding  what about cuSPARSELt?  In any case, let's recheck this too."," CUTLASS fp32 woudl be great :) Does CUTLASS have shape constraints on the dense matrix? That's the only case where we pad currently, for both cutlass and cusparselt.","Yes, CUTLASS has various constraints for both dense and sparse case (for the record, these are encapsulated in `can_implement()` method overrides for various classes here).  The constraints are mostly about the alignment of operands, but some are related to the shapes (`m`,`n`,`k`) of operands too  but as mentioned above, these are dictated by the size of tiles that corresponding tensor core PTX instructions can handle, so should be common between both libraries.  This is what we should check.  I tried to consolidate these tests, and put them in my PyTorch implementation of the sparse linear operator, to be executed before CUTLASS checks for them  namely, CUTLASS error messages are rather cryptic, so I tried to provide more elaborate error messages.  Maybe I made some of these checks too strict.","OK, I have patch adding float32 in C++ code ready.  I need to implement the conversion routines now, which includes shuffling of metadata values into appropriate layout  I'll need some time for this, please bear with me.","Ah I forgot, but there is an additional bit of padding that cuSPARSELt does for the metadata when the sparse rows/cols is not a 128x64 multiple, which I think accounts for the supported shape discrepancy. I think it's also why we can't use the simple 9/16th ratio for all our compressedSize calculations. I do think your CUTLASS shape constraints are correct, but I don't think our test cases hit the small sparse matrix case. This is also unlikely to be used in the wild.  For cuSPARSELt, the shape constrains are:  Also, I didn't realize that there were different constraints for the sparse and dense matrix respectively, which I have updated the PR to reflect.","Here is a patch that, when applied to main, brings float32 support for CUTLASS backend.  It doesn't implement `to_dense()` yet, but it will properly throw in this case (just like it throws when `to_dense()` called for cuSPARSELt backend), and I also want to do some refactoring on several places; but I think it should be good enough to be included together with this PR so that it could be merged; I'll keep working on intended changes and could post them as separate PR.  (BTW, the main complication with float32 is that it requires not 2:4, but 1:2 sparsity pattern instead  this is why there was/is additional work needed on conversion routines.) As far as operand shape restrictions concerned: I removed my checks from CUTLASS backend, will rely on CUTLASS itself to detect and report these issues.  Again, I believe the restrictions should be common between backends, so I'd say let's keep the padding as is, but we should consider adding a test that would exercise it. Please let me know would this patch apply cleanly to your branch (after the changes that I guess you'll make to revert different paths between cuSPARSELt and CUTLASS backend that would not be needed with CUTLASS supporting float32). Edit: Forgot to mention that these changes pass all tests in `test_sparse_semi_structured.py`, after `float32: ...` line uncommented within `_DTYPE_TO_SEMI_STRUCTURED_SPARSE_CONFIG` dict, in `torch/sparse/semi_structured.py`.","Thanks, this patch appears to be working for me and I've included it in the PR. Regarding the shapes: that sounds good to me. Let's make them consistent for now and if someone requests we can turn on cuSPARSELt for the smaller shapes. ","Thanks .  After lots of runs, I see`test_linear_cutlass` failing in couple cases, so please let me investigate it before you merge.","Haven't found anything wrong with my updates yet, but was able to reproduce the problem within sparse GEMM example from CUTLASS source tree, so reported an issue there.",": Would you mind to run bug.txt  on your setup (please rename to .py)?  It's a minimal reproducer, but for me it reports the same error both for cuSPARSELt and CUTLASS.","> : Would you mind to run bug.txt on your setup (please rename to .py)? It's a minimal reproducer, but for me it reports the same error both for cuSPARSELt and CUTLASS. Ah, sorry  never mind: just learned about `torch.backends.cuda.matmul.allow_tf32`, that make it possible to confirm that sparse GEMM result matches with dense GEMM result if dense GEMM force to use tf32 precision; so it makes sense that both cuSPARSELt and CUTLASS produced results are different from dense GEMM result in the same way.  Let me update the test, and we should be fine...","Corrections/clarifications regarding updated commit message: 1. The hardware requires 1:2 sparsity for FP32, so it's not about that anything misses in FP32 support that I added for CUTLASS backend.  Same requirement has to hold for cuSPARSELt.  The `test_sparsity_semi_structured.py` was already creating 1:2 sparse inputs, so there was not need to change anything there (1:2 is stricter requirement than 2:4, namely 1:2 pattern is always 2:4, while 2:4 pattern doesn't have to be 1:2). 2. My suggestion was to change size constraints to those that cuSPARSELt support, and test this.  I can do that, as soon as I update FP32 test for CUTLASS backend.","Here is patch with my latest FP32 updates for CUTLASS (including fixing the tests, that I mentioned above), to be applied on top of this PR (please let me know if you'd prefer that I try to make updates to this PR myself  I'm not sure is it allowed to push to stacks to other people, so this is why I'm sending patches instead.) I'll now try to change size constraints to these required to cuSPARSELt, and see what happens.",", see `test_min_sparse_shape`, It looks like this is failing for CUTLASS on conversion functions, even when I remove the checking.","> Here is patch with my latest FP32 updates for CUTLASS (including fixing the tests, that I mentioned above), to be applied on top of this PR (please let me know if you'd prefer that I try to make updates to this PR myself  I'm not sure is it allowed to push to stacks to other people, so this is why I'm sending patches instead.) Feel free to update the PR. ","> , see `test_min_sparse_shape`, It looks like this is failing for CUTLASS on conversion functions, even when I remove the checking. Thanks, am looking into failures.","So, as far as CUTLASS min shape constraints concerned, here is the configuration that would make it pass the test:  Thus, I guess we need to split these after all? Sparse matrix constraints for CUTLASS are the same as what we had before this PR, except that I found that the number of rows for int8 could be twice smaller.  The dense matrix constraints are the same between CUTLASS and cuSPARSELt. For our record: Number of rows of sparse matrix in CUTLASS is limited by metadata shuffling (that they claim is needed for performance reasons)  cuSPARSELt apparently keeps metadata in different layout, so the difference in constraints here is understandable.  Number of columns constraint in CUTLASS come from alignment requirements, again for metadata matrix; for cuSPARSELT this number seem to be minimal number of columns so that one value per column in metadata matrix is produced, and I guess CUTLASS is requiring more for better alignment, again for performance reasons.  (Note that by ""for performance reasons"" I don't mean that CUTLASS is actually faster, but that CUTLASS team made these design decisions striving for better performance).","  Let's just use the CUTLASS constraints and keep them combined for now. I think we can add support for the smaller cuSPARSELt shapes if someone asks or when we separate out the CUTLASS and cuSPARSELt subclass.  The different shape constraints are not as pressing now that we've separated out the dense and sparse shape constraints. When we were using the same constraints for both the dense and sparse matrix, I was concerned that LLM text generation performance would be negatively affected. (As the shapes for that use case are `[1, hidden] dense @ [hidden, output] sparse`. But given we have the same dense constraints that's not as important now. ","OK, did that  please recheck that constraint now work for cuSPARSELt. Pushed all of my latest changes, this is now good to go as far as I'm concerned.  I'll keep working on remaining FP32 related stuff for CUTLASS (`to_dense()` conversion etc.), will keep pushing to this PR, or will create new PR if this one merged.","It seems last two commits reverted my changes in c75e77d, and then added some small edits  was that intentional?","No, feel free to force push your local to restore them. I can remake my changes. ","Just to check: you're making changes by adding commits to gh/jcaip/56/head, right?","Ah I'm using ghstack, I thought you were too. Something probably got messed up with the nonghstack  ghstack workflow. I'll just copy them over your changes manually then.  ","It's ok, I was using it too. I just force pushed it back to my commit, you can now reapply your changes.", merge, Merge failed **Reason**: PR 115550 is out of sync with the corresponding revision a7d44029559f765640c4001d0e93143525fc2b28 on branch origin/gh/jcaip/56/orig that would be merged into main.  This usually happens because there is a non ghstack change in the PR.  Please sync them and try again (ex. make the changes on origin/gh/jcaip/56/orig and run ghstack). Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
474,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.onnx is not support baichuan_7b 'aten::unflatten' operator)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 11 is not supported.  Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",baichuan,torch.onnx is not support baichuan_7b 'aten::unflatten' operator," ğŸš€ The feature, motivation and pitch torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 11 is not supported.  Alternatives _No response_  Additional context _No response_",2023-12-11T12:01:13Z,module: onnx low priority triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/115538,`torch.onnx.export` is in maintenance mode and we don't plan to add new operators/features or fix complex issues. Please try the new ONNX exporter and reopen this issue with a full repro if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial
612,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add a transformer suite in TP/SP tests)ï¼Œ å†…å®¹æ˜¯ (This is to address issue CC([distributed] add a ""small"" transformer model to the test suite for testing usage). Test plan `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_False` `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_True`   CC(add a transformer suite in TP/SP tests) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,add a transformer suite in TP/SP tests,"This is to address issue CC([distributed] add a ""small"" transformer model to the test suite for testing usage). Test plan `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_False` `python test/distributed/tensor/parallel/test_tp_examples.py k test_transformer_training_is_seq_parallel_True`   CC(add a transformer suite in TP/SP tests) ",2023-12-11T07:55:58Z,oncall: distributed Merged ciflow/trunk topic: not user facing ciflow/periodic,closed,0,4,https://github.com/pytorch/pytorch/issues/115530, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2015,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Export] aten::_transformer_encoder_layer_fwd is not core aten and has no decomposition.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The following script;  Gives:   Versions Collecting environment information... Socket(s):                       2 Stepping:                        0 BogoMIPS:                        6100.00 Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm Hypervisor vendor:               KVM Virtualization type:             full L1d cache:                       1.8 MiB (56 instances) L1i cache:                       1.8 MiB (56 instances) L2 cache:                        28 MiB (56 instances) L3 cache:                        448 MiB (14 instances) NUMA node(s):                    2 NUMA node0 CPU(s):               027,5683 NUMA node1 CPU(s):               2855,84111 Vulnerability Itlb multihit:     Not affected Vulnerability L1tf:              Not affected Vulnerability Mds:               Not affected Vulnerability Meltdown:          Not affected Vulnerability Mmio stale data:   Not affected Vulnerability Retbleed:          Not affected Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user poi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Export] aten::_transformer_encoder_layer_fwd is not core aten and has no decomposition.," ğŸ› Describe the bug The following script;  Gives:   Versions Collecting environment information... Socket(s):                       2 Stepping:                        0 BogoMIPS:                        6100.00 Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm Hypervisor vendor:               KVM Virtualization type:             full L1d cache:                       1.8 MiB (56 instances) L1i cache:                       1.8 MiB (56 instances) L2 cache:                        28 MiB (56 instances) L3 cache:                        448 MiB (14 instances) NUMA node(s):                    2 NUMA node0 CPU(s):               027,5683 NUMA node1 CPU(s):               2855,84111 Vulnerability Itlb multihit:     Not affected Vulnerability L1tf:              Not affected Vulnerability Mds:               Not affected Vulnerability Meltdown:          Not affected Vulnerability Mmio stale data:   Not affected Vulnerability Retbleed:          Not affected Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user poi",2023-12-10T21:45:26Z,module: nn oncall: export,open,0,1,https://github.com/pytorch/pytorch/issues/115511, 
1985,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Export Transformer to ONNX results in SymIntArrayRef Error)ï¼Œ å†…å®¹æ˜¯ (I try to Export the PyTorch default Transformer (self._pytorch_nn_model) after i trained the model (with pytorch) with a really easy task to rotate a sequenze Pytorch nightly torch2.2.0.dev20231208+cu121         X_example = np.random.randint(5, 80, size=(2, 20))         X_example = torch.LongTensor(X_example).to('cpu')         y_example = np.random.randint(5, 80, size=(2, 12))         y_example = torch.LongTensor(y_example).to('cpu')         X_example_mask = self._triu_mask_generator.create_triu_mask(X_example.size(1), X_example.size(1),                                                                    'cpu')         y_example_mask = self._triu_mask_generator.create_triu_mask(y_example.size(1), y_example.size(1),                                                                     'cpu')         X_example_padding_mask = self._padding_mask_generator.create_padding_mask(X_example, 0, 'cpu')         y_example_padding_mask = self._padding_mask_generator.create_padding_mask(y_example, 0, 'cpu')         torch_input_args = ()         torch_input_kwargs = {'X': X_example, 'y': y_example,                               'X_mask': X_example_mask, 'y_mask': y_example_mask,                               'X_padding_mask': X_example_padding_mask,                               'y_padding_mask': y_example_padding_mask}         self._pytorch_nn_model.to('cpu')         export_options = torch.onnx.ExportOptions(dynamic_shapes=True)         onnx_program = torch.onnx.dynamo_export(self._pytorch_nn_model, *torch_input_args,                                                 **torch_input_kwargs, export_options=export_options) PyTorch gives me the following error     /opt/miniconda/lib/python3.10/sitepackages/torch/onnx/_internal/exporter.py)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Export Transformer to ONNX results in SymIntArrayRef Error,"I try to Export the PyTorch default Transformer (self._pytorch_nn_model) after i trained the model (with pytorch) with a really easy task to rotate a sequenze Pytorch nightly torch2.2.0.dev20231208+cu121         X_example = np.random.randint(5, 80, size=(2, 20))         X_example = torch.LongTensor(X_example).to('cpu')         y_example = np.random.randint(5, 80, size=(2, 12))         y_example = torch.LongTensor(y_example).to('cpu')         X_example_mask = self._triu_mask_generator.create_triu_mask(X_example.size(1), X_example.size(1),                                                                    'cpu')         y_example_mask = self._triu_mask_generator.create_triu_mask(y_example.size(1), y_example.size(1),                                                                     'cpu')         X_example_padding_mask = self._padding_mask_generator.create_padding_mask(X_example, 0, 'cpu')         y_example_padding_mask = self._padding_mask_generator.create_padding_mask(y_example, 0, 'cpu')         torch_input_args = ()         torch_input_kwargs = {'X': X_example, 'y': y_example,                               'X_mask': X_example_mask, 'y_mask': y_example_mask,                               'X_padding_mask': X_example_padding_mask,                               'y_padding_mask': y_example_padding_mask}         self._pytorch_nn_model.to('cpu')         export_options = torch.onnx.ExportOptions(dynamic_shapes=True)         onnx_program = torch.onnx.dynamo_export(self._pytorch_nn_model, *torch_input_args,                                                 **torch_input_kwargs, export_options=export_options) PyTorch gives me the following error     /opt/miniconda/lib/python3.10/sitepackages/torch/onnx/_internal/exporter.py",2023-12-10T15:56:55Z,needs reproduction module: onnx triaged oncall: pt2 module: dynamic shapes,closed,0,13,https://github.com/pytorch/pytorch/issues/115501,Is there any solution/workaround for this error?,"I dont find any working workaround  My current solution is a torchscript export, to get a easy to use model. ts not a really onnx workaround :/",You may consider using the `torch.export.export` API to get an exported program; then feed that into `torch.onnx.dynamo_export`.,"okay, after Christmas, I try it out. Can you briefly explain why this should work? In other words, what is the difference?",torch.onnx.dynamo_export is a new model export api we are developing. you can read more about it on https://pytorch.org/docs/stable/onnx_dynamo.html,Do you have a self contained repro script. The posted code is not runnable as is.,"No sry, but self._pytorch_nn_model is a standard pytorch transformer model. The Mask creater creating the masks. I can reproduce the problem easy by try to Export the Pytorch transformer with onnx dynamo",Please share the error message. Standard transformer models should be exportable without errors. ,I try to export the transformer model with dynamic shapes. The error message is: ,"Sorry I got distracted from my original recommendation.  The recommendation is to use torch.export.export first, and then run onnx dynamo export.  This way if the process fails in the first call (torch.export.export), we will have a better idea on what to fix.  Please share the full error stack should you see any issues with torch.export.export","> No sry, but self._pytorch_nn_model is a standard pytorch transformer model. The Mask creater creating the masks. I can reproduce the problem easy by try to Export the Pytorch transformer with onnx dynamo Please do repro and share the script","Try export it to exportedprogram first with `torch.export.export`, and then feed it into `torch.onnx.dynamo_export`. I found some transformers models would work in that way. This is happening usually because of dynamic axes. You might want to check how to set dynamic axes in `torch.export.export`: https://pytorch.org/docs/stable/export.html ","Hi  , Haven't heard back from you since December, closing the issue as stale. Feel free to reopen if you still need assistance with the issue :)"
773,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamic shapes] torch.fx.experimental.symbolic_shapes.ConstraintViolationError from demucs with dynamic shapes)ï¼Œ å†…å®¹æ˜¯ (This is possibly related to CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0) which has a similar error. Repro  Results in:  This could possibly be a result of ducksizing, because if I apply this patch:  Then adding `batchsize 2`:  So the value of 4 is clearly special for this model. Adding `batchsize 8`:  So the ConstraintViolationError goes away with a different batch size. Maybe we are overspecializing modulo guards. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[dynamic shapes] torch.fx.experimental.symbolic_shapes.ConstraintViolationError from demucs with dynamic shapes,"This is possibly related to CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0) which has a similar error. Repro  Results in:  This could possibly be a result of ducksizing, because if I apply this patch:  Then adding `batchsize 2`:  So the value of 4 is clearly special for this model. Adding `batchsize 8`:  So the ConstraintViolationError goes away with a different batch size. Maybe we are overspecializing modulo guards. ",2023-12-09T23:43:36Z,triaged oncall: pt2 module: dynamic shapes module: inductor module: dynamo,open,0,1,https://github.com/pytorch/pytorch/issues/115492,"For reference, if you run with `TORCH_LOGS=dynamic` it will tell you when the specialization occurred"
1992,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile() breaks when using DeepSpeed ZeRO Level 3 sharding)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug torch.compile() breaks when using DeepSpeed ZeRO Level 3 sharding. I am finetuning Llama 2 using the Transformers codebase, and added a `torch.compile()` decorator over the MLP layer in this line of code: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.pyL250 I am using Ubuntu 22.04, Torch 2.1.1, Transformers 4.35.2 and DeepSpeed 0.12.4. Here is my DeepSpeed config file, which works fine when `torch.compile()` is not used:   Error logs 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/_dynamo/external_utils.py"", line 17, in inner 7e72bd4e02:     return fn(*args, **kwargs) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/utils/checkpoint.py"", line 451, in checkpoint 7e72bd4e02:     return CheckpointFunction.apply(function, preserve, *args) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/autograd/function.py"", line 539, in apply 7e72bd4e02:     return super().apply(*args, **kwargs)   type: ignore[misc] 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/utils/checkpoint.py"", line 230, in forward 7e72bd4e02:     outputs = run_function(*args) 7e72bd4e02:   File ""/scratch/brr/brr/llm/models/llama_flash_attention.py"", line 718, in custom_forward 7e72bd4e02:     return module(*inputs, output_attentions, None) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl 7e72bd4e02:     return self._call_impl(*args, **kwargs) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1568, in _c)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile() breaks when using DeepSpeed ZeRO Level 3 sharding," ğŸ› Describe the bug torch.compile() breaks when using DeepSpeed ZeRO Level 3 sharding. I am finetuning Llama 2 using the Transformers codebase, and added a `torch.compile()` decorator over the MLP layer in this line of code: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.pyL250 I am using Ubuntu 22.04, Torch 2.1.1, Transformers 4.35.2 and DeepSpeed 0.12.4. Here is my DeepSpeed config file, which works fine when `torch.compile()` is not used:   Error logs 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/_dynamo/external_utils.py"", line 17, in inner 7e72bd4e02:     return fn(*args, **kwargs) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/utils/checkpoint.py"", line 451, in checkpoint 7e72bd4e02:     return CheckpointFunction.apply(function, preserve, *args) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/autograd/function.py"", line 539, in apply 7e72bd4e02:     return super().apply(*args, **kwargs)   type: ignore[misc] 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/utils/checkpoint.py"", line 230, in forward 7e72bd4e02:     outputs = run_function(*args) 7e72bd4e02:   File ""/scratch/brr/brr/llm/models/llama_flash_attention.py"", line 718, in custom_forward 7e72bd4e02:     return module(*inputs, output_attentions, None) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl 7e72bd4e02:     return self._call_impl(*args, **kwargs) 7e72bd4e02:   File ""/scratch/miniconda3/envs/brr/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1568, in _c",2023-12-09T19:55:44Z,needs reproduction triaged oncall: pt2 module: dynamo dynamo-triage-june2024,open,3,8,https://github.com/pytorch/pytorch/issues/115484,Deepseed is low prio atm,I have the same issue with litgpt's tinyllama. Looking forward to a fix ğŸ˜ƒ ,I have the dynamo error when running DeepSpeed + torch.compile for training GPTJ3.6B. Does someone also have this issue? It looks like strange torch is not defined. **_torch._dynamo.exc.InternalTorchDynamoError: name 'torch' is not defined_** Here is the detailed log:  torch version: 2.1,"That means we generated malformed guard code that made reference to torch without appropriately importing it. You can add a print to name, self.scope to take a look.",Maybe same thing as  CC([Dynamo][DeepSpeed] torch._dynamo.exc.InternalTorchDynamoError: NestedUserFunctionVariable() has no type),I have a different but seemingly related error in the similar setup:  gives the following error: ,> I have the dynamo error when running DeepSpeed + torch.compile for training GPTJ3.6B. Does someone also have this issue? It looks like strange torch is not defined. >  > **_torch._dynamo.exc.InternalTorchDynamoError: name 'torch' is not defined_** >  > Here is the detailed log: >  >  >  > torch version: 2.1 Same issue when using megatronlm and compile with model. I don't know why compile() throws a lot of weird errors for some mods that are encapsulated by a highlevel architecture. Can anyone give some clearer explanation? !image,"There's a chance that these issues are already fixed. purple    could any of you try it with latest release, or nightly at https://pytorch.org? Otherwise any selfcontained repro would be helpful as well."
885,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Potential issue with custom transformer masks when using fast path and batch_first=True)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When initializing a Transformer like this  and then calling it like this  **it will work for training but not for predictions.**  The error message for predictions is   when the input shape of x is [64, 52, 20], where 64 is the batch dimension (in my case I'd like to ignore following batches using the given mask, which is why the mask is [64,64]). **If I change batch_first to False, both training and predictions work.** I assume this could be an issue with the transformers internal  ""fast path"", which is only active during predictions, but not when training.  Versions I use PyTorch 2.1. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Potential issue with custom transformer masks when using fast path and batch_first=True," ğŸ› Describe the bug When initializing a Transformer like this  and then calling it like this  **it will work for training but not for predictions.**  The error message for predictions is   when the input shape of x is [64, 52, 20], where 64 is the batch dimension (in my case I'd like to ignore following batches using the given mask, which is why the mask is [64,64]). **If I change batch_first to False, both training and predictions work.** I assume this could be an issue with the transformers internal  ""fast path"", which is only active during predictions, but not when training.  Versions I use PyTorch 2.1. ",2023-12-09T01:14:47Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/115472,"According to the documentation   The shape of `src_mask` should be `(S, S)` and `tgt_mask` should be `(T, T)` where `S` should be `src.shape(1)` and `T` should be `tgt.shape(1)` when `batch_first=True` So it makes sense to me that creating both masks with   would error for `batch_first=True`. Will look further into why this works for training but it seems that it should not work (but perhaps is working due to broadcasting)"
2034,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Masked Softmax produces differing softmax weights depending on the amount of masking on some architectures.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug To compute a masked softmax we fill the masked values with the lowest number possible. (See the implementation in the transformers repository). I noticed that will lead to different softmax scores on some machines.  In the example script below I create two tensors with differing length (186 and 256) filled with the lowest number possible. In these two tensors I input the same 55 values (for which I want to compute the softmax scores) at different positions.  **I would expect that both tensors contain the same softmax scores!**  On a linux system this works as expected and the softmax scores are the same. However on mac and windows there is a difference. I tested it on different systems and attached the prints below. Is this wanted behaviour? When stacked in multiple transformer layers this leads to quite some difference in the transformer output, depending on the amount and position of padding in the input tensors.  Outputs:   Versions PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1.2 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.12 (main, Jul  5 2023, 15:02:25) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.1.2arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] mypyextensions==1.0.0 [pip3] numpy==1.25.2 [pip3] pytorchlightning==2.0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Masked Softmax produces differing softmax weights depending on the amount of masking on some architectures.," ğŸ› Describe the bug To compute a masked softmax we fill the masked values with the lowest number possible. (See the implementation in the transformers repository). I noticed that will lead to different softmax scores on some machines.  In the example script below I create two tensors with differing length (186 and 256) filled with the lowest number possible. In these two tensors I input the same 55 values (for which I want to compute the softmax scores) at different positions.  **I would expect that both tensors contain the same softmax scores!**  On a linux system this works as expected and the softmax scores are the same. However on mac and windows there is a difference. I tested it on different systems and attached the prints below. Is this wanted behaviour? When stacked in multiple transformer layers this leads to quite some difference in the transformer output, depending on the amount and position of padding in the input tensors.  Outputs:   Versions PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1.2 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.12 (main, Jul  5 2023, 15:02:25) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS14.1.2arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] mypyextensions==1.0.0 [pip3] numpy==1.25.2 [pip3] pytorchlightning==2.0",2023-12-08T13:05:03Z,triaged module: numerical-reproducibility,closed,0,2,https://github.com/pytorch/pytorch/issues/115417,"Floating point addition and multiplication are not associative. If you change the order elements you may get slightly different results even if mathematically they should be the same. Each platform also uses different kernels, so the results will not necessarily be exactly identical between platforms. ",Relevant docs: https://pytorch.org/docs/master/notes/numerical_accuracy.htmlbatchedcomputationsorslicecomputations Closing as expected.
929,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add huggingface gpt2 fake tensor unit test for torch.onnx.dynamo_export)ï¼Œ å†…å®¹æ˜¯ (  CC(Add huggingface gpt2 fake tensor unit test for torch.onnx.dynamo_export) open llama, dolly v2 and falcon are still broken regardless of `ExportedProgram`, so they were not moved from `test_fx_to_onnx.py` to `fx_to_onnx_onnxruntime.py`. Dolly and falcon already have tracking issues, but a tracking issue was created for open llama:  CC(torch.export.export fail to export OpenLLama model) A tracking issue was created for `xfail_if_model_type_is_exportedprogram` and `xfail_if_model_type_is_not_exportedprogram` issues with unexpected success runs:  CC(xfail_if_model_type_is_exportedprogram and xfail_if_model_type_is_not_exportedprogram don't catch unexpected success))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Add huggingface gpt2 fake tensor unit test for torch.onnx.dynamo_export,"  CC(Add huggingface gpt2 fake tensor unit test for torch.onnx.dynamo_export) open llama, dolly v2 and falcon are still broken regardless of `ExportedProgram`, so they were not moved from `test_fx_to_onnx.py` to `fx_to_onnx_onnxruntime.py`. Dolly and falcon already have tracking issues, but a tracking issue was created for open llama:  CC(torch.export.export fail to export OpenLLama model) A tracking issue was created for `xfail_if_model_type_is_exportedprogram` and `xfail_if_model_type_is_not_exportedprogram` issues with unexpected success runs:  CC(xfail_if_model_type_is_exportedprogram and xfail_if_model_type_is_not_exportedprogram don't catch unexpected success)",2023-12-07T21:37:11Z,open source Merged ciflow/trunk topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/115380,"> This is great! What happens to dolly, falcon, and open llama? Is there any issue moving them here? We can add more models, but adding the missing features for fake tensor has a higher priority/impact","> > This is great! What happens to dolly, falcon, and open llama? Is there any issue moving them here? >  > We can add more models, but adding the missing features for fake tensor has a higher priority/impact The model tests are all prepared for you, and you really can done it in 10 minutes, which can give us comprehensive idea of how completed `ExportedProgram` can be compared to dynamo_export with nn.Module. https://github.com/pytorch/pytorch/blob/686a3e0bf08af50a3b83532058fc5a6667365e51/test/onnx/test_fx_to_onnx.pyL558L632","> > > This is great! What happens to dolly, falcon, and open llama? Is there any issue moving them here? > >  > >  > > We can add more models, but adding the missing features for fake tensor has a higher priority/impact >  > The model tests are all prepared for you, and you really can done it in 10 minutes, which can give us comprehensive idea of how completed `ExportedProgram` can be compared to dynamo_export with nn.Module. >  > https://github.com/pytorch/pytorch/blob/686a3e0bf08af50a3b83532058fc5a6667365e51/test/onnx/test_fx_to_onnx.pyL558L632 * dolly and falcon have bugs that were not fixed by ExportedProgram path; * open llama is broken on ExportedProgram but I have filed  CC(torch.export.export fail to export OpenLLama model) to track it.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"  Happy New Year! Do you guys know what happened to this PR? It had the `merging` added and later replaced by `merged`, but the PR is still open and not merged? There are some referals in the timeline too that I don't get it"," Happy New Year! Sometimes GitHub fails to auto close PR(due to missing notifications), so if you see the change in trunk, feel free to manually close. Looks like it's there, so I've closed it."
462,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Testing two conv2d with the same input yields different outputs)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Problem description I tested two identical conv2d operators with the same input and obtained two inconsistent inputs   Models and input files case.zip  Versions torch==1.13.1+cu117 numpy==1.26.2)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Testing two conv2d with the same input yields different outputs, ğŸ› Describe the bug  Problem description I tested two identical conv2d operators with the same input and obtained two inconsistent inputs   Models and input files case.zip  Versions torch==1.13.1+cu117 numpy==1.26.2,2023-12-07T08:27:38Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/115335,The weights in `new_Conv2d.pth` and `old_Conv2d.pth` are not identical. So it is as expected that the outputs are different even for the same input `data0_Conv2d.npy`. ,"Based on  comment, I'm closing the issue. Feel free to reopen it there's additional information."
1992,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NotImplementedError: Cannot access storage of SparseCsrTensorImpl)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I have an IterableDataset that is being passed in the data loader. The Dataset yields a sparse_csr_tensor. This is what is currently Exception being thrown. I have a duplicate MapDataset that is identical in logic to the IterableDataset but instead returns the in __getitem__ of course. I have set pin_memory to false and get the same issue. I can include more of my code if needed if there is reason to believe this is my own implementation issue. Everything is being ran on one GPU.   Versions PyTorch version: 2.1.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Red Hat Enterprise Linux 9.2 (Plow) (x86_64) GCC version: (GCC) 11.2.0 Clang version: Could not collect CMake version: version 3.26.5 Libc version: glibc2.34 Python version: 3.9.18 (main, Sep  7 2023, 00:00:00)  [GCC 11.4.1 20230605 (Red Hat 11.4.12)] (64bit runtime) Python platform: Linux5.14.0284.18.1.el9_2.x86_64x86_64withglibc2.34 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          24 Online CPU(s) list:             023 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz CPU family:                      6 Model:                           85 Thread(s) per core:              1 Core(s) p)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,NotImplementedError: Cannot access storage of SparseCsrTensorImpl," ğŸ› Describe the bug I have an IterableDataset that is being passed in the data loader. The Dataset yields a sparse_csr_tensor. This is what is currently Exception being thrown. I have a duplicate MapDataset that is identical in logic to the IterableDataset but instead returns the in __getitem__ of course. I have set pin_memory to false and get the same issue. I can include more of my code if needed if there is reason to believe this is my own implementation issue. Everything is being ran on one GPU.   Versions PyTorch version: 2.1.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Red Hat Enterprise Linux 9.2 (Plow) (x86_64) GCC version: (GCC) 11.2.0 Clang version: Could not collect CMake version: version 3.26.5 Libc version: glibc2.34 Python version: 3.9.18 (main, Sep  7 2023, 00:00:00)  [GCC 11.4.1 20230605 (Red Hat 11.4.12)] (64bit runtime) Python platform: Linux5.14.0284.18.1.el9_2.x86_64x86_64withglibc2.34 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          24 Online CPU(s) list:             023 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz CPU family:                      6 Model:                           85 Thread(s) per core:              1 Core(s) p",2023-12-07T05:33:26Z,module: sparse module: dataloader triaged,closed,0,8,https://github.com/pytorch/pytorch/issues/115330,  I think this might be fixed in the nightlies and thus next release. Do you mind trying this with a recent nightly? Thank you.,"  I ran it across torch==2.2.0dev20231208+cu121. Findings: I ran into a different error where I did not set the batch_size in the data loader which defaults to 1 which means the following would always execute:  Since the batch_sampler is defined the auto_collate is set to true. It would be nice to have a better indication that batch_size needs to be set to None to prevent auto_collition but I recognize this is expected behavior from the DataLoader, however I now receive a NotImplementedError: Caught NotImplementedError in pin memory thread for device 0. with the same stack trace as before. When I set pin_memory to false it works as expected. ","Hm, interesting. Can you share more context so we can try to run this on our end?","This is the repo that throws the error. https://github.com/jdenhof/DMMVAE/tree/local NotImplementedError: Caught NotImplementedError in pin memory thread for device 0. Original Traceback (most recent call last):   File ""/home/denhofja/DMMVAE/.venv/lib64/python3.9/sitepackages/torch/utils/data/_utils/pin_memory.py"", line 36, in do_one_step     data = pin_memory(data, device)   File ""/home/denhofja/DMMVAE/.venv/lib64/python3.9/sitepackages/torch/utils/data/_utils/pin_memory.py"", line 57, in pin_memory     return data.pin_memory(device) NotImplementedError: Could not run 'aten::_pin_memory' with arguments from the 'SparseCsrCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_pin_memory' is only available for these backends: [CUDA, Meta, NestedTensorCPU, NestedTensorCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher]. CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel] Meta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback] NestedTensorCPU: registered at aten/src/ATen/RegisterNestedTensorCPU.cpp:775 [kernel] NestedTensorCUDA: registered at aten/src/ATen/RegisterNestedTensorCUDA.cpp:931 [kernel] BackendSelect: registered at aten/src/ATen/RegisterBackendSelect.cpp:807 [kernel] Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback] FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback] Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback] Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback] Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback] Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback] ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback] ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback] AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel] Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16968 [kernel] AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback] AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback] FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback] BatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback] FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback] Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback] VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback] FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback] PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback] FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback] PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback] PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback] Above is more context behind the error exception. However to give more context behind my problem, I have a dataset of 100 chunks of (275000, 60000) 98% sparse csr_matrices. In order to simulate random access I have a process running to load and shuffle the chunks in random order per epoch and then get the chunk from the queue when needed and splice the chunk to form batches to return in my iterator. I feel like I am abusing the functionality of the data loader by doing this but it's the only way I can think of having the chunks needed in memory without loading the chunk every iteration.  I am aware this is out of the scope of the actual bug but if you have any suggestions they would be greatly appreciated.   I will put together a minimal reproducible example below.",Ran with the following:     torchrun  standalone nproc_per_node=1 main.py  ,"Here is a simple reproducer:  that results in  A prototype of the fix is:  However, a better fix would be to implement pin_memory support for sparse tensors.",I am also running into this issue, Any update on pin_memory support for sparse tensors?
1046,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `reset_storage` method to FunctionalTensorWrapper (#115235))ï¼Œ å†…å®¹æ˜¯ (In certain edge cases when using lazy tensors, the base tensor stored in the `FunctionalStorageImpl` and the `value_` tensor stored in the `FunctionalTensorWrapper` diverge. For instance, take this simple example  The call to `transpose` on the lazily initialized weight `fc1.weight` applies a view op on the functional tensor which only gets propagated to the functional tensor wrapper and not the base tensor in the storage. Thus, causing them to diverge. To fix this behaviour, we need to reset the functional tensor's storage. To facilitate this, we add a `_unsafe_reset_storage` method to `FunctionalTensorWrapper` which clears away the old storage and view metas. Porting over PR from https://github.com/pytorch/pytorch/pull/115235 Cherrypicked: 73c0035160e7b2c5772417bb7206b316bdf34044 CC:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add `reset_storage` method to FunctionalTensorWrapper (#115235),"In certain edge cases when using lazy tensors, the base tensor stored in the `FunctionalStorageImpl` and the `value_` tensor stored in the `FunctionalTensorWrapper` diverge. For instance, take this simple example  The call to `transpose` on the lazily initialized weight `fc1.weight` applies a view op on the functional tensor which only gets propagated to the functional tensor wrapper and not the base tensor in the storage. Thus, causing them to diverge. To fix this behaviour, we need to reset the functional tensor's storage. To facilitate this, we add a `_unsafe_reset_storage` method to `FunctionalTensorWrapper` which clears away the old storage and view metas. Porting over PR from https://github.com/pytorch/pytorch/pull/115235 Cherrypicked: 73c0035160e7b2c5772417bb7206b316bdf34044 CC:   ",2023-12-07T02:19:49Z,open source,closed,0,3,https://github.com/pytorch/pytorch/issues/115320,  Is it possible to get this commit cherrypicked into the 2.2 release branch? It should be virtually no risk as there are no users of this new functionalization API yet.," if there are no user of the feature, then why does it need to be cherrypicked into the release branch?",">  if there are no user of the feature, then why does it need to be cherrypicked into the release branch?  There are _currently_ no users of the feature. As in its completely new and doesn't affect anything else (maybe  can confirm this). Once its part of an official stable release, we will of course be using it as we need it for our software stack. Our customers mandate being on a stable release of PyTorch, so if this feature doesn't make it into the PyTorch 2.2 stable release, we need to wait for the next PyTorch release to further support our customer's needs. Hence, why I'm asking if it can be cherrypicked onto the release branch."
636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Consider storage_changed for assigning alias_of_input in aot_autograd when computing differentiable outputs that alias each other)ï¼Œ å†…å®¹æ˜¯ (  CC([Do not review] Top of FSDP stack  to be broken up)  CC([fsdp][torch.compile] FSDP changes)  CC([fsdp] Replace acc_grad hooking with register_post_accumulate_grad_hook on flat_param)  CC(Consider storage_changed for assigning alias_of_input in aot_autograd when computing differentiable outputs that alias each other) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Consider storage_changed for assigning alias_of_input in aot_autograd when computing differentiable outputs that alias each other,  CC([Do not review] Top of FSDP stack  to be broken up)  CC([fsdp][torch.compile] FSDP changes)  CC([fsdp] Replace acc_grad hooking with register_post_accumulate_grad_hook on flat_param)  CC(Consider storage_changed for assigning alias_of_input in aot_autograd when computing differentiable outputs that alias each other) ,2023-12-07T01:16:14Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/115315,o needs a type check, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
468,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([distributed] add a ""small"" transformer model to the test suite for testing usage)ï¼Œ å†…å®¹æ˜¯ (We can add a ""small"" Transformer model to our test suite to test out our existing FSDP/TP/SP APIs to ensure no future breakge and give easy to read example about how to apply our APIs directly in tests. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[distributed] add a ""small"" transformer model to the test suite for testing usage","We can add a ""small"" Transformer model to our test suite to test out our existing FSDP/TP/SP APIs to ensure no future breakge and give easy to read example about how to apply our APIs directly in tests. ",2023-12-06T23:26:34Z,oncall: distributed triaged,open,0,5,https://github.com/pytorch/pytorch/issues/115309,is this different from the tests here? https://github.com/pytorch/pytorch/blob/main/test/distributed/test_dynamo_distributed.pyL178,"Let me know if this lands. I have a fork of nanoGPT in the perparametersharding FSDP PR, including inlined `torch.utils.checkpoint` as an option (to test that). I have found the nanoGPT model to be really good for testing FSDP. Existing FSDP has `TransformerWithSharedParams`: https://github.com/pytorch/pytorch/blob/bb7746275c1151a2d2113b2a3f323b3c599ff04b/torch/testing/_internal/common_fsdp.pyL212","> is this different from the tests here? https://github.com/pytorch/pytorch/blob/main/test/distributed/test_dynamo_distributed.pyL178  Yeah I was thinking about having a ""small"" (in term of model param size) model with model definition in our test suite, so instead of relying on the transformer package where it hides the model definition and hard to modify, a small toy transformer model could let us test things in a more detailed way, i.e. we can test meta init API or how the model get sharded/splitted with more details as we have the model definition",> I have found the nanoGPT model to be really good for testing FSDP. >  > Existing FSDP has `TransformerWithSharedParams`: >  > https://github.com/pytorch/pytorch/blob/bb7746275c1151a2d2113b2a3f323b3c599ff04b/torch/testing/_internal/common_fsdp.pyL212 Nice! yeah then maybe we should bring a toy nano gpt model directly to our test suite to let our tests leverage it?  ,"> Let me know if this lands. I have a fork of nanoGPT in the perparametersharding FSDP PR, including inlined `torch.utils.checkpoint` as an option (to test that). >  > I have found the nanoGPT model to be really good for testing FSDP. >  > Existing FSDP has `TransformerWithSharedParams`: >  > https://github.com/pytorch/pytorch/blob/bb7746275c1151a2d2113b2a3f323b3c599ff04b/torch/testing/_internal/common_fsdp.pyL212  Hey Andrew, just FYI the PR landed. Next I should learn how to incorporate FSDP into the test."
1316,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`F.scaled_dot_product_attention` effectively can not be traced with `torch.jit.trace` or `fullgraph=True` when specifying `is_causal`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, as per title I noticed that it is effectively not possible to trace `F.scaled_dot_product_attention` for a realworld implementation. I'm not sure it is really a bug but opening for awareness.  This implementation uses `is_causal=attention_mask is None and q_len > 1` to overcome the issue of the ""wrong"" causal mask being generated by PyTorch as explained in  CC([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment). This code raises `TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor` which I guess is kind of expected given that jit.trace can not handle inputdependent controlflows. A potential solution is to always use `is_causal=False` and always pass the correct `attn_mask` when using torch.jit.trace. But this requires to use `torch.jit.is_tracing()` in the modeling code, which is relatively slow. Related:  CC(ONNX export of torch.nn.Transformer still fails) Thank you!  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`F.scaled_dot_product_attention` effectively can not be traced with `torch.jit.trace` or `fullgraph=True` when specifying `is_causal`," ğŸ› Describe the bug Hi, as per title I noticed that it is effectively not possible to trace `F.scaled_dot_product_attention` for a realworld implementation. I'm not sure it is really a bug but opening for awareness.  This implementation uses `is_causal=attention_mask is None and q_len > 1` to overcome the issue of the ""wrong"" causal mask being generated by PyTorch as explained in  CC([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment). This code raises `TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor` which I guess is kind of expected given that jit.trace can not handle inputdependent controlflows. A potential solution is to always use `is_causal=False` and always pass the correct `attn_mask` when using torch.jit.trace. But this requires to use `torch.jit.is_tracing()` in the modeling code, which is relatively slow. Related:  CC(ONNX export of torch.nn.Transformer still fails) Thank you!  Versions  ",2023-12-06T13:52:26Z,oncall: jit,closed,0,3,https://github.com/pytorch/pytorch/issues/115262,"Solved by keeping the `attention_mask` in case we are tracing, and thus the condition `attention_mask is None and q_len > 1` evaluates to `False` on the first operand, and we don't have the issue of `q_len` being a tensor when tracing","> Solved by keeping the `attention_mask` in case we are tracing, and thus the condition `attention_mask is None and q_len > 1` evaluates to `False` on the first operand, and we don't have the issue of `q_len` being a tensor when tracingé€šè¿‡ä¿ç•™ `attention_mask` æ¥è§£å†³ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨è·Ÿè¸ªï¼Œå› æ­¤æ¡ä»¶ `attention_mask is None and q_len > 1` åœ¨ç¬¬ä¸€ä¸ªæ“ä½œæ•°ä¸Šçš„å€¼ä¸º `False` ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨è·Ÿè¸ªæ—¶æ²¡æœ‰ `q_len` æ˜¯å¼ é‡çš„é—®é¢˜ Hi, I'v also met this problem. Could you give me some code about how to ""kepp the `attention_mask`""? Thank you."," It is not a superclean solution (as the traced model then can not dispatch on the FA2 backend), but this works: https://github.com/huggingface/transformers/blob/d02d006cf315cf91e3a470eb72b9a9a7d0ecaf90/src/transformers/models/llama/modeling_llama.pyL735 You need to always pass the `causal_mask` argument to SDPA, not relying on `is_causal` (see  CC([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment))"
1965,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory corruption in test_transformers)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When running `test_transformers` the `test_scaled_dot_product_fused_attention_vs_math_cpu` subtest runs into memory corruption issues, i.e. either of: > .double free or corruption (!prev) > .python: malloc.c:3852: _int_malloc: Assertion `chunk_main_arena (bck>bk)' failed. > .munmap_chunk(): invalid pointer Followed by ""Aborted (core dumped)"" This can be easily reproduced with `python test_transformers.py  k test_scaled_dot_product_fused_attention_vs_math_cpu` As the test is expanded via parametrization to 192 subtests I tried to narrow it down to those 8 tests:  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_1_head_dim_16_causal_False_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_1_head_dim_16_causal_True_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_3_head_dim_16_causal_False_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_3_head_dim_16_causal_True_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_2_seq_len_1030_n_head_1_head_dim_16_causal_False_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_2_seq_len_1030_n_head_1_head_dim_16_ca)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Memory corruption in test_transformers," ğŸ› Describe the bug When running `test_transformers` the `test_scaled_dot_product_fused_attention_vs_math_cpu` subtest runs into memory corruption issues, i.e. either of: > .double free or corruption (!prev) > .python: malloc.c:3852: _int_malloc: Assertion `chunk_main_arena (bck>bk)' failed. > .munmap_chunk(): invalid pointer Followed by ""Aborted (core dumped)"" This can be easily reproduced with `python test_transformers.py  k test_scaled_dot_product_fused_attention_vs_math_cpu` As the test is expanded via parametrization to 192 subtests I tried to narrow it down to those 8 tests:  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_1_head_dim_16_causal_False_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_1_head_dim_16_causal_True_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_3_head_dim_16_causal_False_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_12_seq_len_1030_n_head_3_head_dim_16_causal_True_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_2_seq_len_1030_n_head_1_head_dim_16_causal_False_train_True_cpu_bfloat16  TestSDPACPU.test_scaled_dot_product_fused_attention_vs_math_cpu_fused_kernel_SDPBackend_FLASH_ATTENTION_bfloat16_batch_size_2_seq_len_1030_n_head_1_head_dim_16_ca",2023-12-06T09:01:00Z,module: tests triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/115253,"I bisected the crash to commit 71632d4d246 using a reduced testcase:  It is actually the `backward` call that causes the crash, removing it avoids the problem", would you mind taking a look at this?,"This is a known problem caused by the third party oneDNN and the issue is fixed in oneDNN v3.2. As the PyTorch 2.1.0 uses oneDNN v3.1.1, you could install the latest nightly PyTorch instead of the released PyTorch 2.1.0 to avoid the issue.",I see. Bisecting the submodule led me to https://github.com/oneapisrc/oneDNN/commit/6518e55b025b75ea83c677c601cfb8b1f7df0a68 which seemingly fixes this. So it is oneDNN 3.3 where it is fixed and indeed the current PyTorch 2.2 RC (where oneDNN is updated to 3.3.2) doesn't crash with this test
364,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] `torch._dynamo.exc.Unsupported: call_method ListIteratorVariable() __contains__ [ConstantVariable(str)] {}`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Encountered in SDv1.5 + LoRA  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[dynamo] `torch._dynamo.exc.Unsupported: call_method ListIteratorVariable() __contains__ [ConstantVariable(str)] {}`, ğŸ› Describe the bug Encountered in SDv1.5 + LoRA  Versions main ,2023-12-06T04:08:54Z,oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/115236,"Encountered the same error for torch 2.2.1 in linux for phi2 + QLoRA, when I tried to convert the model+adapter weights into onnx. Any ideas?"
908,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `reset_storage` method to FunctionalTensorWrapper)ï¼Œ å†…å®¹æ˜¯ (In certain edge cases when using lazy tensors, the base tensor stored in the `FunctionalStorageImpl` and the `value_` tensor stored in the `FunctionalTensorWrapper` diverge. For instance, take this simple example  The call to `transpose` on the lazily initialized weight `fc1.weight` applies a view op on the functional tensor which only gets propagated to the functional tensor wrapper and not the base tensor in the storage. Thus, causing them to diverge. To fix this behaviour, we need to reset the functional tensor's storage. To facilitate this, we add a `reset_storage` method to `FunctionalTensorWrapper` which clears away the old storage and view metas. CC: a    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add `reset_storage` method to FunctionalTensorWrapper,"In certain edge cases when using lazy tensors, the base tensor stored in the `FunctionalStorageImpl` and the `value_` tensor stored in the `FunctionalTensorWrapper` diverge. For instance, take this simple example  The call to `transpose` on the lazily initialized weight `fc1.weight` applies a view op on the functional tensor which only gets propagated to the functional tensor wrapper and not the base tensor in the storage. Thus, causing them to diverge. To fix this behaviour, we need to reset the functional tensor's storage. To facilitate this, we add a `reset_storage` method to `FunctionalTensorWrapper` which clears away the old storage and view metas. CC: a    ",2023-12-06T03:31:45Z,open source Merged ciflow/trunk release notes: lazy,closed,0,5,https://github.com/pytorch/pytorch/issues/115235, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", Would it be possible to get this PR included as part of release 2.2? What is the process for doing so? Just open up a separate PR with the changes against the `release/2.2` branch? , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
426,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prevent invalid pointwise ops on jagged with transposed ragged dim)ï¼Œ å†…å®¹æ˜¯ (  CC(Fix jagged composite impl of flatten())  CC(Reshape decomposition for jagged layout NT)  CC(Prevent invalid pointwise ops on jagged with transposed ragged dim) TODO: tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Prevent invalid pointwise ops on jagged with transposed ragged dim,  CC(Fix jagged composite impl of flatten())  CC(Reshape decomposition for jagged layout NT)  CC(Prevent invalid pointwise ops on jagged with transposed ragged dim) TODO: tests,2023-12-05T19:33:26Z,Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/115190, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([sparse][semi-structured] add alg_id to _cslt_sparse_mm and _cslt_sparse_mm_search)ï¼Œ å†…å®¹æ˜¯ (  CC([sparse][semistructured] add alg_id to _cslt_sparse_mm and _cslt_sparse_mm_search) Summary: cuSPARSELt has support for different alg_id, which are set via `cusparseLTMatmulAlgSetAttribute`, in total there are 4 different alg_ids, 0  3. Previously we were just using the default alg_id, as from our initial experiments we found that for most shapes the default alg_id is the fastest and that they made no difference on numerical correctness, just performance. From our previous experiments the fastest alg_id seemed to differ only on small matmul shapes. danthe3rd found a performance regression when running with cuSPARSELt v0.4.0 vs v0.5.0, on LLM shapes, which match these characteristics (activations are small, weights are large). However it's likely that this is due to the alg_id ordering changing, as mentioned in the release notes for v0.5.0.  This PR adds in the following:  support for passing in alg_id to _cslt_sparse_mm  a new op, _cslt_sparse_mm_search, which returns the optimal alg_id for   a given matmul _cslt_sparse_mm_search has the same function signature as _cslt_sparse_mm, minus the alg_id parameter. We are able to achieve v0.4.0 performance with alg_id=1 on the shapes that daniel provided. We will address autoselecting the best alg_id in a future PR, possibly with torch.compile. Test Plan:  Reviewers: Subscribers: Tasks: Tags:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[sparse][semi-structured] add alg_id to _cslt_sparse_mm and _cslt_sparse_mm_search,"  CC([sparse][semistructured] add alg_id to _cslt_sparse_mm and _cslt_sparse_mm_search) Summary: cuSPARSELt has support for different alg_id, which are set via `cusparseLTMatmulAlgSetAttribute`, in total there are 4 different alg_ids, 0  3. Previously we were just using the default alg_id, as from our initial experiments we found that for most shapes the default alg_id is the fastest and that they made no difference on numerical correctness, just performance. From our previous experiments the fastest alg_id seemed to differ only on small matmul shapes. danthe3rd found a performance regression when running with cuSPARSELt v0.4.0 vs v0.5.0, on LLM shapes, which match these characteristics (activations are small, weights are large). However it's likely that this is due to the alg_id ordering changing, as mentioned in the release notes for v0.5.0.  This PR adds in the following:  support for passing in alg_id to _cslt_sparse_mm  a new op, _cslt_sparse_mm_search, which returns the optimal alg_id for   a given matmul _cslt_sparse_mm_search has the same function signature as _cslt_sparse_mm, minus the alg_id parameter. We are able to achieve v0.4.0 performance with alg_id=1 on the shapes that daniel provided. We will address autoselecting the best alg_id in a future PR, possibly with torch.compile. Test Plan:  Reviewers: Subscribers: Tasks: Tags:",2023-12-05T18:08:05Z,Merged Reverted ciflow/trunk release notes: sparse,closed,0,12,https://github.com/pytorch/pytorch/issues/115178, here are the benchmark results for alg_id: , rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/jcaip/53/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/115178`)"," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m 'Sorry for reverting your change, but the Window build failure looks legit https://hud.pytorch.org/pytorch/pytorch/commit/1e5636f7915035b09dce22ad1d2170a65f344214' c ignoredsignal The failure wasn't captured correctly and the build wrongly returned successfully.  Here are the linker errrors:  Let me take a look at the Windows build script to see why it didn't fail the build there.", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,Looks like this was caused by me using `long` instead of `int64_t` in the C++ function signatures. Should be working now. , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
926,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] Graph breaks from copy.deepcopy)ï¼Œ å†…å®¹æ˜¯ (A few models in our benchmark suite have graph breaks calling `copy.deepcopy()`. 1) maml deepcopies a nn.Module in order to do online finetuning 2) hf_T5_generate deepcopies a config object (UserDefinedObjectVariable) so that it can make changes to the config locally 3) In theory one could also deepcopy python structures or tensors, but I haven't seen that yet  this would be pretty easy to handle by rewriting to torch.clone() To repro the hf_T5_generate ones, first rebase onto CC([dynamo] Improve graph break message for copy.deepcopy) (if it hasn't been landed yet) then run:  To debug further, one can do:  which will turn the graph break into an error and see the user code it is coming from:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,[dynamo] Graph breaks from copy.deepcopy,"A few models in our benchmark suite have graph breaks calling `copy.deepcopy()`. 1) maml deepcopies a nn.Module in order to do online finetuning 2) hf_T5_generate deepcopies a config object (UserDefinedObjectVariable) so that it can make changes to the config locally 3) In theory one could also deepcopy python structures or tensors, but I haven't seen that yet  this would be pretty easy to handle by rewriting to torch.clone() To repro the hf_T5_generate ones, first rebase onto CC([dynamo] Improve graph break message for copy.deepcopy) (if it hasn't been landed yet) then run:  To debug further, one can do:  which will turn the graph break into an error and see the user code it is coming from:  ",2023-12-05T01:29:50Z,feature triaged oncall: pt2 module: dynamo module: graph breaks,open,0,0,https://github.com/pytorch/pytorch/issues/115122
1910,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(consider small change to mask implied by `is_causal=True` in SDPA when query length != kv length)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch It's pretty common in causal language models to only want the last (newly generated) token from a model. E.g.:  This should be equivalent to just querying for the token you want, which one might expect to be able to do with something like: `F.scaled_dot_product_attention(query[:, 1, :], key, value, is_causal=True )`. Unfortunately, the offset for the implied causal mask assumes that the first index of query along the sequence dimension is aligned with the first index along the k/v dimension, and so only attends to the first input token from k/v (you can actually pass anything for query and will always get the same ""incorrect"" result). To make this work in the way desired, instead you'd need to use something like:  If the mask implied by is_causal=True was such that the last index of the query was assumed to be aligned with the last index of the key/value, I think that'd be more frequently useful than what's currently implied by `is_causal=True` (and I believe would also allow for things like varied layer sizes across LLMs without needing to manually construct masks). Changing the mask implied by is_causal would be BCbreaking, but sdpa is documented as beta, and I doubt there's much code relying on this specific behavior. In the example implementation in the documentation, I believe this would simply be equivalent to changing `temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)` to `temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=SL)`. When S=L, the behavior wouldn't change.  Alternatives pass an explicit mask  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,consider small change to mask implied by `is_causal=True` in SDPA when query length != kv length," ğŸš€ The feature, motivation and pitch It's pretty common in causal language models to only want the last (newly generated) token from a model. E.g.:  This should be equivalent to just querying for the token you want, which one might expect to be able to do with something like: `F.scaled_dot_product_attention(query[:, 1, :], key, value, is_causal=True )`. Unfortunately, the offset for the implied causal mask assumes that the first index of query along the sequence dimension is aligned with the first index along the k/v dimension, and so only attends to the first input token from k/v (you can actually pass anything for query and will always get the same ""incorrect"" result). To make this work in the way desired, instead you'd need to use something like:  If the mask implied by is_causal=True was such that the last index of the query was assumed to be aligned with the last index of the key/value, I think that'd be more frequently useful than what's currently implied by `is_causal=True` (and I believe would also allow for things like varied layer sizes across LLMs without needing to manually construct masks). Changing the mask implied by is_causal would be BCbreaking, but sdpa is documented as beta, and I doubt there's much code relying on this specific behavior. In the example implementation in the documentation, I believe this would simply be equivalent to changing `temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)` to `temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=SL)`. When S=L, the behavior wouldn't change.  Alternatives pass an explicit mask  Additional context _No response_",2023-12-04T23:08:16Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/115111,"Have been working on this for a little bit, hoping to land this soon: https://github.com/pytorch/pytorch/pull/114823",See:  CC([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment),"ah cool! Thanks, closing this as duplicate."
924,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add get_mutation_names to ir.Wait, to indicate it's a mutation op)ï¼Œ å†…å®¹æ˜¯ (`ir.Wait` generates the last 2 lines of this code:  `_wait_tensor` technically is a ""mutation"" op that changes `buf2` in place. So we should mark `ir.Wait` as a mutation op (by overriding its `get_mutation_names()`). This fixes a very peculiar issue when inductor comm reordering is used for llama model: downstream nodes that uses the allgather comm output sometimes takes dependency on `buf2` (the node before `ir.Wait`) instead of on `buf3` (`ir.Wait`) (it's still unclear why it behaves like this). To work around the issue, we add the missing annotation that `buf3` is a mutation of `buf2`, so that the scheduler knows to schedule `buf3` before any of the `buf2` users. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"Add get_mutation_names to ir.Wait, to indicate it's a mutation op","`ir.Wait` generates the last 2 lines of this code:  `_wait_tensor` technically is a ""mutation"" op that changes `buf2` in place. So we should mark `ir.Wait` as a mutation op (by overriding its `get_mutation_names()`). This fixes a very peculiar issue when inductor comm reordering is used for llama model: downstream nodes that uses the allgather comm output sometimes takes dependency on `buf2` (the node before `ir.Wait`) instead of on `buf3` (`ir.Wait`) (it's still unclear why it behaves like this). To work around the issue, we add the missing annotation that `buf3` is a mutation of `buf2`, so that the scheduler knows to schedule `buf3` before any of the `buf2` users. ",2023-12-04T22:17:29Z,oncall: distributed Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/115104, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: pull / linuxjammypy3.8gcc11 / test (distributed, 1, 2, linux.2xlarge) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
907,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enables private_use_one lazy_init by PrivateUse1HooksInterface)ï¼Œ å†…å®¹æ˜¯ (Fixes  CC(In the func Tensor.to, how can I make privateuse lazy init) In my last pr:https://github.com/pytorch/pytorch/pull/113343, I want to implement lazy_init for other device through `REGISTER_LAZY_INIT `. But this might be too big of a change. Recently, my team found that `torch.load` without `lazy_init ` will also results in the same error.  https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL319L321 https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL334L335 So, I want to use `PrivateUse1HooksInterface` to implement lazy_init for `PrivateUse1`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Enables private_use_one lazy_init by PrivateUse1HooksInterface,"Fixes  CC(In the func Tensor.to, how can I make privateuse lazy init) In my last pr:https://github.com/pytorch/pytorch/pull/113343, I want to implement lazy_init for other device through `REGISTER_LAZY_INIT `. But this might be too big of a change. Recently, my team found that `torch.load` without `lazy_init ` will also results in the same error.  https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL319L321 https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/torch/csrc/Storage.cppL334L335 So, I want to use `PrivateUse1HooksInterface` to implement lazy_init for `PrivateUse1`.",2023-12-04T09:26:06Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/115067, Can you take a look at this prï¼Ÿ,  Maybe you can give me some advice for this prï¼Ÿ, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1824,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault when multiplying *float16 tensor with int tensor and one shape is: (0,0).)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Here is the code to reproduce:  The pytorch version I am using is 2.1.0. When running the code in my linux machine, it will raises segmentation fault (core dump). Instead, when I run the code in colab (torch version 2.1.0+cu118), the colab notebook will directly crash.  Versions [pip3] numpy==1.23.5 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchvision==0.16.0 [pip3] triton==2.0.0 [conda] blas                      1.0                         mkl [conda] cudatoolkit               11.3.1               h2bc3f7f_2 [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] libjpegturbo             2.0.0                h9bf148f_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0            py39h7f8727e_0 [conda] mkl_fft                   1.3.1            py39hd3c417c_0 [conda] mkl_random                1.2.2            py39h51133e4_0 [conda] numpy                     1.23.5           py39h14f4228_0 [conda] numpybase                1.23.5           py39h31eccc5_0 [conda] pytorch                   2.1.0               py3.9_cpu_0    pytorch [conda] pytorchmutex             1.0                         cpu    pytorch [conda] torch                     2.0.0                    pypi_0    pypi [conda] torchaudio                2.1.0                  py39_cpu    pytorch [conda] torchvision               0.16.0                 py39_cpu    pytorch [conda] triton                    2.0.0                    pypi_0    pypi )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Segmentation fault when multiplying *float16 tensor with int tensor and one shape is: (0,0)."," ğŸ› Describe the bug Here is the code to reproduce:  The pytorch version I am using is 2.1.0. When running the code in my linux machine, it will raises segmentation fault (core dump). Instead, when I run the code in colab (torch version 2.1.0+cu118), the colab notebook will directly crash.  Versions [pip3] numpy==1.23.5 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchvision==0.16.0 [pip3] triton==2.0.0 [conda] blas                      1.0                         mkl [conda] cudatoolkit               11.3.1               h2bc3f7f_2 [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] libjpegturbo             2.0.0                h9bf148f_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0            py39h7f8727e_0 [conda] mkl_fft                   1.3.1            py39hd3c417c_0 [conda] mkl_random                1.2.2            py39h51133e4_0 [conda] numpy                     1.23.5           py39h14f4228_0 [conda] numpybase                1.23.5           py39h31eccc5_0 [conda] pytorch                   2.1.0               py3.9_cpu_0    pytorch [conda] pytorchmutex             1.0                         cpu    pytorch [conda] torch                     2.0.0                    pypi_0    pypi [conda] torchaudio                2.1.0                  py39_cpu    pytorch [conda] torchvision               0.16.0                 py39_cpu    pytorch [conda] triton                    2.0.0                    pypi_0    pypi ",2023-12-04T09:12:38Z,high priority triage review module: crash module: numpy module: edge cases,closed,0,2,https://github.com/pytorch/pytorch/issues/115066,"Hi, I did some triage and I realize that this issue can also be triggered by other multiplications between float16 and int. For instance, below code can also trigger such segmentation fault:   ","Closing as duplicate of  CC(`TensorIteratorBase::is_scalar` return `false` for empty numpy tensors, but true for empty Torch ones)"
2090,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`Loop-carried variable _tmp7 has initial type <[1, 2048], int1> but is re-assigned to <[1, 2048], int32> in loop! Please make sure that the type stays consistent.`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug with Transformers mask2former and torch.compile, compilation failed for input image which is not a power of 2. A complete repro is available in this colab: https://colab.research.google.com/drive/1pn4xo3ovttttOpukiuB9lo2WxjRQSDkP?usp=sharing seems like a duplicate of  CC(Failure using compile for HF transformers `google/flan-ul2` and `google/flan-t5-xl` when batch size is not a power of 2) and  CC(Miscompilation for torch.any with large tensors) but this isn't solved in 2.1.1  Error logs  BackendCompilerFailed                     Traceback (most recent call last) [](https://localhost:8080/) in ()      18       19        Forward pass > 20       outputs = model(      21           pixel_values=batch[""pixel_values""].to(device),      22           mask_labels=[labels.to(device) for labels in batch[""mask_labels""]], 67 frames /usr/lib/python3.10/concurrent/futures/_base.py in __get_result(self)     401         if self._exception:     402             try: > 403                 raise self._exception     404             finally:     405                  Break a reference cycle with the exception in self._exception BackendCompilerFailed: backend='inductor' raised: CompilationError: at 22:46:        rindex = roffset + rbase         rmask = rindex  but is reassigned to  in loop! Please make sure that the type stays consistent.') Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information You can suppress this exception and fall back to eager by setting:     import torch._dynamo     torch._dynamo.config.suppress_errors = True  Minified repro _No response_  Versions   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"`Loop-carried variable _tmp7 has initial type <[1, 2048], int1> but is re-assigned to <[1, 2048], int32> in loop! Please make sure that the type stays consistent.`"," ğŸ› Describe the bug with Transformers mask2former and torch.compile, compilation failed for input image which is not a power of 2. A complete repro is available in this colab: https://colab.research.google.com/drive/1pn4xo3ovttttOpukiuB9lo2WxjRQSDkP?usp=sharing seems like a duplicate of  CC(Failure using compile for HF transformers `google/flan-ul2` and `google/flan-t5-xl` when batch size is not a power of 2) and  CC(Miscompilation for torch.any with large tensors) but this isn't solved in 2.1.1  Error logs  BackendCompilerFailed                     Traceback (most recent call last) [](https://localhost:8080/) in ()      18       19        Forward pass > 20       outputs = model(      21           pixel_values=batch[""pixel_values""].to(device),      22           mask_labels=[labels.to(device) for labels in batch[""mask_labels""]], 67 frames /usr/lib/python3.10/concurrent/futures/_base.py in __get_result(self)     401         if self._exception:     402             try: > 403                 raise self._exception     404             finally:     405                  Break a reference cycle with the exception in self._exception BackendCompilerFailed: backend='inductor' raised: CompilationError: at 22:46:        rindex = roffset + rbase         rmask = rindex  but is reassigned to  in loop! Please make sure that the type stays consistent.') Set TORCH_LOGS=""+dynamo"" and TORCHDYNAMO_VERBOSE=1 for more information You can suppress this exception and fall back to eager by setting:     import torch._dynamo     torch._dynamo.config.suppress_errors = True  Minified repro _No response_  Versions   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total",2023-12-03T12:34:41Z,high priority triaged oncall: pt2 module: dynamo,closed,0,6,https://github.com/pytorch/pytorch/issues/115029,"I took a initial look. I don't get the same error as  . Instead the error I get is https://gist.github.com/shunting314/c8bd9278450fd7725dae30a02167e672 : tt.reduce op failed to infer returned types . The error happens when triton compiles the reduction operation for the welford algorithm. Force skipping welford algorithm can work around the issue. But after that I get a error in dynamo when handling DICT_MERGE: https://gist.github.com/shunting314/5de6ad0517f68f4f8c85ccfd0d1fbdd9  . Overall what I understand is, the issue initially blocking   may have already been fixed but after that more issues are revealed. I'll pass this to   who implements the welford algorithm. Here is a standalone kernel I collected from the original repro that can repro the issue regarding welford algorithm: https://gist.github.com/shunting314/292018eb83814fe18fa57de279696f0c .  after this is fixed we can pass the issue to dynamo experts (maybe   /  ) to take further look. BTW, here is a standalone repro script copied out of colab: https://gist.github.com/shunting314/353e3ebb7cf1eed2a5c1cd329f487f85  ","I can't reproduce any triton error locally or on colab. I only see the `DICT_MERGE` assertion failure. For the original issue, the codegen has changed on main:  Instead of `where(mask, x, 0)` we now replace `0` with a typed variable with the same dtype as the input being masked, which prevents the dtype changing unexpectedly.","  oh cool, then the issue regarding Welford algorithm has already been fixed on trunk.   /   would you look at the dynamo issue handling `DICT_MERGE`. Check the comment above for repros.",Got a simpler repro: ,Note that for torch 2.2.2+python3.11 this bug still exist  ,"Hi, is there any updates? I am seeing similar errors: "
429,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([AOTInductor] Add method to get storage size in shim)ï¼Œ å†…å®¹æ˜¯ (  CC([AOTInductor] Add method to get storage size in shim) Summary: Add a method to get storage size. Test Plan: N/A, for FC, test will come after packaged. Reviewers: Subscribers: Tasks: Tags:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[AOTInductor] Add method to get storage size in shim,"  CC([AOTInductor] Add method to get storage size in shim) Summary: Add a method to get storage size. Test Plan: N/A, for FC, test will come after packaged. Reviewers: Subscribers: Tasks: Tags:",2023-12-01T19:56:55Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/114976, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
453,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add dynamic shape tests for important models to guard against regression)ï¼Œ å†…å®¹æ˜¯ (Most of these models are already statically tested at `test/onnx/test_fx_to_onnx_with_onnxruntime.py::TestFxToOnnxFakeTensorWithOnnxRuntime`, but we need to dynamic dimensions and cover them in CI )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Add dynamic shape tests for important models to guard against regression,"Most of these models are already statically tested at `test/onnx/test_fx_to_onnx_with_onnxruntime.py::TestFxToOnnxFakeTensorWithOnnxRuntime`, but we need to dynamic dimensions and cover them in CI ",2023-12-01T16:18:29Z,module: onnx triaged onnx-triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/114954
613,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enabling Transformer fast path for not batch_first (MHA, TE, TEL))ï¼Œ å†…å®¹æ˜¯ (  CC(Enabling Transformer fast path for not batch_first (MHA, TE, TEL))  CC(Create fastpath backend context manager, similar to SDPA kernel backend manager) The fast path for the `forward()` method in `MultiheadAttention`, `TE`, `TEL` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Differential Revision: D48095703)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Enabling Transformer fast path for not batch_first (MHA, TE, TEL)","  CC(Enabling Transformer fast path for not batch_first (MHA, TE, TEL))  CC(Create fastpath backend context manager, similar to SDPA kernel backend manager) The fast path for the `forward()` method in `MultiheadAttention`, `TE`, `TEL` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Differential Revision: D48095703",2023-12-01T00:40:56Z,Stale,closed,0,1,https://github.com/pytorch/pytorch/issues/114922,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
533,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Create fastpath backend context manager, similar to SDPA kernel backend manager)ï¼Œ å†…å®¹æ˜¯ (  CC(Enabling Transformer fast path for not batch_first (MHA, TE, TEL))  CC(Create fastpath backend context manager, similar to SDPA kernel backend manager) Create fastpath backend context manager, similar to SDPA kernel backend manager Differential Revision: D48325593 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Create fastpath backend context manager, similar to SDPA kernel backend manager","  CC(Enabling Transformer fast path for not batch_first (MHA, TE, TEL))  CC(Create fastpath backend context manager, similar to SDPA kernel backend manager) Create fastpath backend context manager, similar to SDPA kernel backend manager Differential Revision: D48325593 ",2023-12-01T00:40:28Z,oncall: distributed Stale release notes: distributed (fsdp),closed,0,1,https://github.com/pytorch/pytorch/issues/114919,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
2019,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ExportedProgram.run_decompistion errors when specifying dim=-1 in torch.ops.aten.scatter_add)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Repro:  Error Stack: ``` Traceback (most recent call last):   File ""/home/titaiwang/pytorch/test_negative_scatter_add.py"", line 23, in      ep.run_decompositions(decomp_table=torch._decomp.decomposition_table)   File ""/home/titaiwang/pytorch/torch/export/exported_program.py"", line 77, in wrapper     return fn(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/export/exported_program.py"", line 403, in run_decompositions     gm, graph_signature = aot_export_module(                           ^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 1021, in aot_export_module     fx_g, metadata, in_spec, out_spec = _aot_export_function(                                         ^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 1203, in _aot_export_function     fx_g, meta = create_aot_dispatcher_function(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_dynamo/utils.py"", line 244, in time_wrapper     r = func(*args, **kwargs)         ^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 494, in create_aot_dispatcher_function     fake_flat_args = process_inputs(flat_args)                      ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 492, in process_inputs     return [convert(idx, x) for idx, x in enumerate(flat_args)]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 492, in      return [convert(idx, x) for idx, x in enumerate(flat_args)]             ^^^^^^^^^^^^^^^   File "")è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ExportedProgram.run_decompistion errors when specifying dim=-1 in torch.ops.aten.scatter_add," ğŸ› Describe the bug Repro:  Error Stack: ``` Traceback (most recent call last):   File ""/home/titaiwang/pytorch/test_negative_scatter_add.py"", line 23, in      ep.run_decompositions(decomp_table=torch._decomp.decomposition_table)   File ""/home/titaiwang/pytorch/torch/export/exported_program.py"", line 77, in wrapper     return fn(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/export/exported_program.py"", line 403, in run_decompositions     gm, graph_signature = aot_export_module(                           ^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 1021, in aot_export_module     fx_g, metadata, in_spec, out_spec = _aot_export_function(                                         ^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 1203, in _aot_export_function     fx_g, meta = create_aot_dispatcher_function(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_dynamo/utils.py"", line 244, in time_wrapper     r = func(*args, **kwargs)         ^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 494, in create_aot_dispatcher_function     fake_flat_args = process_inputs(flat_args)                      ^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 492, in process_inputs     return [convert(idx, x) for idx, x in enumerate(flat_args)]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/titaiwang/pytorch/torch/_functorch/aot_autograd.py"", line 492, in      return [convert(idx, x) for idx, x in enumerate(flat_args)]             ^^^^^^^^^^^^^^^   File """,2023-11-30T19:44:48Z,export-triaged oncall: export,closed,0,0,https://github.com/pytorch/pytorch/issues/114892
842,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor][cpu]pytorch_stargan and llama accuracy failure)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug pytorch_stargan static FP32 function and llama AMP perfromance fails in 20231115. pytorch_stargan failure:  llama amp performance failure:   Versions SW information:   SW  /    Repro: inductor_single_run.sh pytorch_stargan: `bash inductor_single_run.sh multiple/single inference accuracy/performance torchbench pytorch_stargan float32 first static default` llama: `bash inductor_single_run.sh multiple/single inference performance torchbench llama amp first static default` Suspected guilty commit: https://github.com/pytorch/pytorch/commit/a378ae33e99fbdcc2c73c6456947212cb3fe647c)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[inductor][cpu]pytorch_stargan and llama accuracy failure, ğŸ› Describe the bug pytorch_stargan static FP32 function and llama AMP perfromance fails in 20231115. pytorch_stargan failure:  llama amp performance failure:   Versions SW information:   SW  /    Repro: inductor_single_run.sh pytorch_stargan: `bash inductor_single_run.sh multiple/single inference accuracy/performance torchbench pytorch_stargan float32 first static default` llama: `bash inductor_single_run.sh multiple/single inference performance torchbench llama amp first static default` Suspected guilty commit: https://github.com/pytorch/pytorch/commit/a378ae33e99fbdcc2c73c6456947212cb3fe647c,2023-11-30T15:51:41Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/114878,Hi   this issue is duplicated with  CC([inductor][cpu] llama and pytorch_stargan failed on CPP Wrapper) ,closing as it is duplicated with  CC([inductor][cpu] llama and pytorch_stargan failed on CPP Wrapper)
686,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor][cpu] llama and pytorch_stargan failed on CPP Wrapper)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  [ ] `llama` failed on CPP Wrapper static and dynamic shape in 20231129  [ ] `pytorch_stargan` only failed on CPP Wrapper dynamic shape in 20231129 **Error message:**   Versions **SW info** SW  / image: docker pull ccrregistry.caas.intel.com/pytorch/pt_inductor:2023_11_29_aws Repro inductor_single_run.sh  **Suspected guilty commit:** https://github.com/pytorch/pytorch/commit/a378ae33e99fbdcc2c73c6456947212cb3fe647c )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[inductor][cpu] llama and pytorch_stargan failed on CPP Wrapper, ğŸ› Describe the bug  [ ] `llama` failed on CPP Wrapper static and dynamic shape in 20231129  [ ] `pytorch_stargan` only failed on CPP Wrapper dynamic shape in 20231129 **Error message:**   Versions **SW info** SW  / image: docker pull ccrregistry.caas.intel.com/pytorch/pt_inductor:2023_11_29_aws Repro inductor_single_run.sh  **Suspected guilty commit:** https://github.com/pytorch/pytorch/commit/a378ae33e99fbdcc2c73c6456947212cb3fe647c ,2023-11-30T13:59:42Z,module: cpu triaged oncall: pt2 module: inductor oncall: cpu inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/114869,,Please intel help to upload the bisect search log in the issue,"Hi , we observed some functionality crash in cpu related tests, and seems caused by the PR CC([BE][aot_autograd] Remove mutated_inp_indices), could you please help to double check it? Thanks ", could you help to take a look?,Hi could you double check if it is resolved or not? I landed a fix sometime ago,"Hi intel , please help to confirm"
1990,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add an attention bias subclass for a lower right causal masking)ï¼Œ å†…å®¹æ˜¯ ( Summary This PR introduces a new Tensor subclass that is designed to be used with torch.nn.functional.scaled_dot_product_attention. Currently we have a boolean `is_causal` flag that allows users to do do causal masking without the need to actually create the ""realized"" attention bias and pass into sdpa. We originally added this flag since there is native support in both fused kernels we support. This provides a big performance gain ( the kernels only need to iterate over ~0.5x the sequence, and for very large sequence lengths this can provide vary large memory improvements. The flag was introduced when the early on in the kernel development and at the time it was implicitly meant to ""upper_left"" causal attention. This distinction only matters when the attention_bias is not square. For a more detailed break down see:  CC([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment). The kernels default behavior has since changed, largely due to the rise of autogressive text generation. And unfortunately this would lead to a BC break. In the long term it may actually be beneficial to change the default meaning of `is_causal` to represent lower_right causal masking. The larger theme though is laid here:  CC([RFC] Scaled Dot Product Attention  API Changes). The thesis being that there is alot of innovation in SDPA revolving around the attention_bias being used. This is the first in hopefully a few more attention_biases that we would like to add. The next interesting one would be `sliding_window` which is used by the popular mistral model family.    CC(Add an attention bias subclass for a lower right causal masking) Results from benchmarking, I improved the meff_attention perf hence)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",mistral,Add an attention bias subclass for a lower right causal masking," Summary This PR introduces a new Tensor subclass that is designed to be used with torch.nn.functional.scaled_dot_product_attention. Currently we have a boolean `is_causal` flag that allows users to do do causal masking without the need to actually create the ""realized"" attention bias and pass into sdpa. We originally added this flag since there is native support in both fused kernels we support. This provides a big performance gain ( the kernels only need to iterate over ~0.5x the sequence, and for very large sequence lengths this can provide vary large memory improvements. The flag was introduced when the early on in the kernel development and at the time it was implicitly meant to ""upper_left"" causal attention. This distinction only matters when the attention_bias is not square. For a more detailed break down see:  CC([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment). The kernels default behavior has since changed, largely due to the rise of autogressive text generation. And unfortunately this would lead to a BC break. In the long term it may actually be beneficial to change the default meaning of `is_causal` to represent lower_right causal masking. The larger theme though is laid here:  CC([RFC] Scaled Dot Product Attention  API Changes). The thesis being that there is alot of innovation in SDPA revolving around the attention_bias being used. This is the first in hopefully a few more attention_biases that we would like to add. The next interesting one would be `sliding_window` which is used by the popular mistral model family.    CC(Add an attention bias subclass for a lower right causal masking) Results from benchmarking, I improved the meff_attention perf hence",2023-11-30T02:23:27Z,module: nn Merged ciflow/trunk release notes: onnx release notes: python_frontend,closed,2,4,https://github.com/pytorch/pytorch/issues/114823, merge ," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
321,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unsupported: call_method UserDefinedObjectVariable(_profiler_enabled) __call__ [] {})ï¼Œ å†…å®¹æ˜¯ (user source that triggered, should be easy to repro: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Unsupported: call_method UserDefinedObjectVariable(_profiler_enabled) __call__ [] {},"user source that triggered, should be easy to repro: ",2023-11-29T22:51:45Z,triaged module: graph breaks,open,0,0,https://github.com/pytorch/pytorch/issues/114805
445,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add head_mask for transformers)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I am wondering if we can also have the head_mask, which can manually alter the attention_weights after the softmax From Huggingface  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add head_mask for transformers," ğŸš€ The feature, motivation and pitch I am wondering if we can also have the head_mask, which can manually alter the attention_weights after the softmax From Huggingface  Alternatives _No response_  Additional context _No response_ ",2023-11-29T21:09:10Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/114796, `attn_mask`
314,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Clean up documentation for nn.Transformer related classes)ï¼Œ å†…å®¹æ˜¯ (  CC([BE] Clean up documentation for nn.Transformer related classes))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[BE] Clean up documentation for nn.Transformer related classes,  CC([BE] Clean up documentation for nn.Transformer related classes),2023-11-29T20:29:43Z,Stale,closed,0,1,https://github.com/pytorch/pytorch/issues/114789,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1128,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issues when trying to compile nn.functional.interpolate() layer with dynamic input and output size)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi. I'm new to posting issues and bugs so please forgive me if I make a mistake Recently I was trying to use torch.compile() to speed up some experiments and I ran into this problem I'm posting the abstracted section of the code which is failing to compile   ~My guess is that the compilation is freezing the interpolation layer to the sizes of the first input and throwing an error for subsequent inputs. If so, how do I change the code to accommodate this feature ?~ Update The code runs fine if I remove the `align_corners=True, mode='linear'` args from interpolate. How do I circumvent it ? (Linear interpolation is a core part of the model I'm working with) I'm posting the error logs when running `TORCHDYNAMO_VERBOSE=1 TORCH_LOGS=""+dynamo"" python sandbox.py` below  Error logs   Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Issues when trying to compile nn.functional.interpolate() layer with dynamic input and output size," ğŸ› Describe the bug Hi. I'm new to posting issues and bugs so please forgive me if I make a mistake Recently I was trying to use torch.compile() to speed up some experiments and I ran into this problem I'm posting the abstracted section of the code which is failing to compile   ~My guess is that the compilation is freezing the interpolation layer to the sizes of the first input and throwing an error for subsequent inputs. If so, how do I change the code to accommodate this feature ?~ Update The code runs fine if I remove the `align_corners=True, mode='linear'` args from interpolate. How do I circumvent it ? (Linear interpolation is a core part of the model I'm working with) I'm posting the error logs when running `TORCHDYNAMO_VERBOSE=1 TORCH_LOGS=""+dynamo"" python sandbox.py` below  Error logs   Minified repro _No response_  Versions  ",2023-11-29T12:40:36Z,triaged oncall: pt2 module: dynamic shapes,closed,0,8,https://github.com/pytorch/pytorch/issues/114752, would you be the best person to look into this?,"After https://github.com/pytorch/pytorch/pull/114774 you can do,  but I'm not sure how your way of calling interpolate can be fixed.  ,  ", ,"The C++ composite operator `aten.upsample_linear1d.vec` doesn't support symbolic shapes, so we need to add a `py_impl(CompositeImplicitAutograd)` that does.","> The C++ composite operator `aten.upsample_linear1d.vec` doesn't support symbolic shapes, so we need to add a `py_impl(CompositeImplicitAutograd)` that does. I'm sorry but how do I use this to make changes to my code? I'm not that familiar with `aten` usage or `py_impl()` usage. Some guidance would help me a lot  ",Peter's comment was for . Either  or 5 will soon submit a fix for this issue :),I'm helping scrub old issues this week. This hans't been updated in a while;  or 5: any updates to add here?,This is fixed now
633,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FakeTensorMode not preserved within torch.export.ExportedProgram.run_decompositions)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Context: After https://github.com/pytorch/pytorch/pull/114009, `aot_export_module` accepts inputs/model already fakefied by user. However, when one tries to decompose the fake model with `run_decompositions`, we get an error with a mismtached fake tensor mode  ideally, `run_decompositions` should also preserve the user fake mode  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,FakeTensorMode not preserved within torch.export.ExportedProgram.run_decompositions," ğŸ› Describe the bug Context: After https://github.com/pytorch/pytorch/pull/114009, `aot_export_module` accepts inputs/model already fakefied by user. However, when one tries to decompose the fake model with `run_decompositions`, we get an error with a mismtached fake tensor mode  ideally, `run_decompositions` should also preserve the user fake mode  Versions main ",2023-11-28T22:33:50Z,module: fakeTensor oncall: export,closed,0,4,https://github.com/pytorch/pytorch/issues/114711, FYI,"Hmm, I think we might need to add something like this to run_decompositions()","That was my workaround (aka call `run_decompositions` outside a fake mode context), but I was unsure whether that was the right thing to do If it is, I can push a PR now", Angela prepared a PR for it lol.  I remove the module: export lable from the issue because we start to use oncall: export only instead of module:export to mark an issue as export related and module: export label will be deleted. 
875,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Bring docstring to .pyi file)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Add docstring to `torch/__init__.pyi`?) Since the original issue hasn't been making progress for more than 3 years, I am attempting to make this PR to at least make some progress forward.  This PR attempts to add docstring to the `.pyi` files. The docstrings are read from `_torch_docs` by mocking `_add_docstr`, which is the only function used to add docstring. Luckily, `_torch_docs` has no dependencies for other components of PyTorch, and can be imported without compiling `torch._C` with `_add_docstr` mocked.  The generated `.pyi` file looks something like the following:  _VariableFunctions.pyi.txt  And the docstring can be picked up by VSCode:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Bring docstring to .pyi file,"Fixes CC(Add docstring to `torch/__init__.pyi`?) Since the original issue hasn't been making progress for more than 3 years, I am attempting to make this PR to at least make some progress forward.  This PR attempts to add docstring to the `.pyi` files. The docstrings are read from `_torch_docs` by mocking `_add_docstr`, which is the only function used to add docstring. Luckily, `_torch_docs` has no dependencies for other components of PyTorch, and can be imported without compiling `torch._C` with `_add_docstr` mocked.  The generated `.pyi` file looks something like the following:  _VariableFunctions.pyi.txt  And the docstring can be picked up by VSCode:  ",2023-11-28T22:11:07Z,triaged open source Merged Reverted ciflow/trunk topic: not user facing,closed,0,13,https://github.com/pytorch/pytorch/issues/114705,Here are two example pyi files from the generated artifacts:   torch/_C/_VariableFunctions.pyi  torch/_C/\_\_init\_\_.pyi,,bumping for myself,"> Could you share an example function and class from the generated pyi to see how they look? Here is one example function `torch.add`   For an example class, here is part of the TensorBase class (where most of the documents belong to it)   ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"> I can't think of an easy way to test this. Any idea on your end? I guess we can check the generated `.pyi` files by parsing the contents. For example,   One question remains is, what to expect from the docstring. It could be any comments that may not even contain the name of the function documented (e.g., The docstring for `alias_copy` is `Performs the same operation as :func:`torch.alias`, but all output tensors are freshly created instead of aliasing the input.`). ","This seems reasonable. For a smoketest, all you really need to do is test one, IMO."," revert m=""Diff reverted internally"" c=""ghfirst"" This Pull Request has been reverted by a revert inside Meta. To reland this change, please open another pull request, assign the same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).)", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,"Hi ! Sorry, I have to revert your PR. We suspect that your changes may cause internal failures. We are currently investigating the issue and will contact you as soon as we have more details. ","Oh, the problem is probably that gen_pyi.py now has a ""circular"" dependency on the torch file, but the internal build system isn't updated to reflect this. I guess it just happens to work for regular builds, but I'd guess the Pyre build is too hermetic and can't find the files."
433,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Handle potential ValueError exception when stringifying signals)ï¼Œ å†…å®¹æ˜¯ (On some systems it is possible to receive a signal that does not have a name.  Rare, but possible.  This prevents our error handler from crashing and instead properly reports the signal.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Handle potential ValueError exception when stringifying signals,"On some systems it is possible to receive a signal that does not have a name.  Rare, but possible.  This prevents our error handler from crashing and instead properly reports the signal.",2023-11-28T20:06:30Z,Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/114696, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 1, 3, macosm112) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1998,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unexpected Results of In-Place Zeroing on Sliced Tensor with MPS Device)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Unexpected Results of InPlace Zeroing on Sliced Tensor with MPS Device Performing inplace zeroing on a sliced PyTorch tensor located on the MPS device leads to unexpected results, which differs from the behavior observed on CPU or CUDA device. The issue is demonstrated in the following code snippet:  In this example, tensor `a` on the CPU is correctly zeroed inplace, where only index 1 is set to zero, resulting in the expected output: tensor([1., 0., 1.]). However, tensor `b` on the MPS device produces unexpected results. The inplace zeroing operation on the sliced tensor `b` correctly sets index 1 to `0`, but unexpectedly also sets index 2 to `0`, leading to the output: tensor([1, 0., 0.], device='mps:0'). In addition to the previously mentioned potential bug, we observed that performing inplace filling with zero values using torch.fill_ on a PyTorch tensor located on the MPS device also produces unexpected results. The issue can be seen in the following code snippet:  It is interesting that the unexpected behavior with torch.fill_ seems to be specific to `0` and doesn't seem to occur for other values. Note: The issue was initially noticed when incorrect results for Swin Transformer V2 on MPS were observed. Further investigation revealed that in the torchvision implementation of the `shifted_window_attention` function, the qkv_bias is obtained with inplace zeroing similar to the provided example.  Versions PyTorch version: 2.1.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.6 Libc version: N/A Python version: 3.8.18 (default, Sep 11 2023, 08:17:16)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Unexpected Results of In-Place Zeroing on Sliced Tensor with MPS Device," ğŸ› Unexpected Results of InPlace Zeroing on Sliced Tensor with MPS Device Performing inplace zeroing on a sliced PyTorch tensor located on the MPS device leads to unexpected results, which differs from the behavior observed on CPU or CUDA device. The issue is demonstrated in the following code snippet:  In this example, tensor `a` on the CPU is correctly zeroed inplace, where only index 1 is set to zero, resulting in the expected output: tensor([1., 0., 1.]). However, tensor `b` on the MPS device produces unexpected results. The inplace zeroing operation on the sliced tensor `b` correctly sets index 1 to `0`, but unexpectedly also sets index 2 to `0`, leading to the output: tensor([1, 0., 0.], device='mps:0'). In addition to the previously mentioned potential bug, we observed that performing inplace filling with zero values using torch.fill_ on a PyTorch tensor located on the MPS device also produces unexpected results. The issue can be seen in the following code snippet:  It is interesting that the unexpected behavior with torch.fill_ seems to be specific to `0` and doesn't seem to occur for other values. Note: The issue was initially noticed when incorrect results for Swin Transformer V2 on MPS were observed. Further investigation revealed that in the torchvision implementation of the `shifted_window_attention` function, the qkv_bias is obtained with inplace zeroing similar to the provided example.  Versions PyTorch version: 2.1.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.6 Libc version: N/A Python version: 3.8.18 (default, Sep 11 2023, 08:17:16",2023-11-28T19:33:18Z,high priority triage review module: mps,closed,0,4,https://github.com/pytorch/pytorch/issues/114692,Marking as high priority for silent correctness issues,I think I know what is going on :( https://github.com/pytorch/pytorch/blob/34ea0a2bdc7d17f5ca0e895590159e5fd36fffa0/aten/src/ATen/native/mps/operations/ConstantOps.mmL103L105,"And here is the fix, PR is coming: ","Thanks for the quick fix! It seems to resolve my problem and also the test case looks good. However, I noticed the removal of `.storage()`, previously added in another PR fixing another issue). I'm not really sure, why the `.storage()` was added in the first place, but maybe It's necessary to fix the other issue?"
1110,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_sync_batch_norm_empty_input (__main__.DistributedDataParallelTest))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux, rocm This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_sync_batch_norm_empty_input` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `distributed/test_c10d_nccl.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DISABLED test_sync_batch_norm_empty_input (__main__.DistributedDataParallelTest),"Platforms: linux, rocm This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_sync_batch_norm_empty_input` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `distributed/test_c10d_nccl.py` ",2023-11-28T15:39:39Z,oncall: distributed module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/114681
852,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([AOTAutograd / Functionalization] Fix incorrect `expand_copy_inverse`)ï¼Œ å†…å®¹æ˜¯ (Fixes  CC(Mutation after `tensor.expand` returns wrong result.) Only partial fix, will be correct in all eager passing cases, but will fail to throw error when eager throws error.  Deferring to  on follow up on fixing the case when eager throws an error. IMO it's not a priority and I'm OK with not fixing.  If in long term we want to fix, I think it can be handled with an analysis similar to https://github.com/pytorch/pytorch/blob/bbdd9b059fa06404701f0d39abaae75ebab5d27f/torch/_functorch/aot_autograd.pyL2202 We can check if expand dims are simultaneously mutated after analyzing in an fx pass. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[AOTAutograd / Functionalization] Fix incorrect `expand_copy_inverse`,"Fixes  CC(Mutation after `tensor.expand` returns wrong result.) Only partial fix, will be correct in all eager passing cases, but will fail to throw error when eager throws error.  Deferring to  on follow up on fixing the case when eager throws an error. IMO it's not a priority and I'm OK with not fixing.  If in long term we want to fix, I think it can be handled with an analysis similar to https://github.com/pytorch/pytorch/blob/bbdd9b059fa06404701f0d39abaae75ebab5d27f/torch/_functorch/aot_autograd.pyL2202 We can check if expand dims are simultaneously mutated after analyzing in an fx pass. ",2023-11-25T22:53:29Z,triaged open source Stale topic: not user facing module: dynamo,closed,0,6,https://github.com/pytorch/pytorch/issues/114538,The main difference is the introduction of `sub` and `add` alongside the `sum` when inverting the expand view. This PR:  Main: , rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/114538/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/7290700513,"chuang looks like my late review is causing you merge conflicts. Let me know if/when you have a chance to fix. Otherwise, I can make a new PR with your changes","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1421,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MPS: transformers crashing in executeMPSGraph with Function square_i64 was not found in the library)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm hitting a crash in executeMPSGraph when trying to run this notebook on my M1 Macbook (smaller selfcontained repro below) crashing in trainer.train() with the following error:  To repro:  Looks similar to CC(MPS: `log` and `exp` not working for integer Tensors) but the missing function is different. Stack trace:   Versions PyTorch version: 2.1.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.1 Libc version: N/A Python version: 3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS14.1.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.26.2 [pip3] torch==2.1.1 [conda] Could not collect)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,MPS: transformers crashing in executeMPSGraph with Function square_i64 was not found in the library," ğŸ› Describe the bug I'm hitting a crash in executeMPSGraph when trying to run this notebook on my M1 Macbook (smaller selfcontained repro below) crashing in trainer.train() with the following error:  To repro:  Looks similar to CC(MPS: `log` and `exp` not working for integer Tensors) but the missing function is different. Stack trace:   Versions PyTorch version: 2.1.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1.1 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.1 Libc version: N/A Python version: 3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS14.1.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.26.2 [pip3] torch==2.1.1 [conda] Could not collect",2023-11-25T08:39:02Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/114532,"Uhm, I think I was just holding this wrong, after changing the type of the labels type to float, ie. changing the data to:  this now works! Closing, apologies for the confusion",Filed a transformers issue instead suggesting better validation higher up the stack: https://github.com/huggingface/transformers/issues/27707 
529,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] Fix mutation tracking of ConvolutionBinaryInplace)ï¼Œ å†…å®¹æ˜¯ (  CC([Inductor] Fix mutation tracking of ConvolutionBinaryInplace) Init function reorders the arguments so the mutation actually happens on argument input[0] I am not sure if there's a good way to test this unfortunately.. Added tests on https://github.com/pytorch/pytorch/pull/114436 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Inductor] Fix mutation tracking of ConvolutionBinaryInplace,  CC([Inductor] Fix mutation tracking of ConvolutionBinaryInplace) Init function reorders the arguments so the mutation actually happens on argument input[0] I am not sure if there's a good way to test this unfortunately.. Added tests on https://github.com/pytorch/pytorch/pull/114436 ,2023-11-24T06:50:47Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/114501,> I am not sure if there's a good way to test this unfortunately.. Added tests on https://github.com/pytorch/pytorch/pull/114436 /bf16 static/dynamic testing to verify the fix of  CC([inductor][cpu] freezing caused a lot of CV model crashed), merge," Merge failed **Reason**: Approval needed from one of the following: int3, wz337, alanadakotashine, yuguo68, Hangjun, ... Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1062,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor][cpu]llama fp32 dynamic single thread performance regression)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug llama fp32 dynamic single thread performance regression in 20231112                name       batch_size_new       speed_up_new       inductor_new       eager_new       batch_size_old       speed_up_old       inductor_old       eager_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)                       llama       1       0.077641       0.416671735       0.032350810177135       1       0.315078       0.102211727       0.032204666519706       0.25       1       0.25          Versions SW info SW  /   Repro: inductor_single_run.sh bash inductor_single_run.sh single inference performance torchbench llama float32 first dynamic default Suspected guilty commit: https://github.com/pytorch/pytorch/commit/1a361e4e9ff1a8dadd72c7301441cfb83d687f24 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[inductor][cpu]llama fp32 dynamic single thread performance regression, ğŸ› Describe the bug llama fp32 dynamic single thread performance regression in 20231112                name       batch_size_new       speed_up_new       inductor_new       eager_new       batch_size_old       speed_up_old       inductor_old       eager_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)                       llama       1       0.077641       0.416671735       0.032350810177135       1       0.315078       0.102211727       0.032204666519706       0.25       1       0.25          Versions SW info SW  /   Repro: inductor_single_run.sh bash inductor_single_run.sh single inference performance torchbench llama float32 first dynamic default Suspected guilty commit: https://github.com/pytorch/pytorch/commit/1a361e4e9ff1a8dadd72c7301441cfb83d687f24 ,2023-11-24T03:22:59Z,module: performance oncall: pt2 oncall: cpu inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/114494,, we found commit https://github.com/pytorch/pytorch/commit/1a361e4e9ff1a8dadd72c7301441cfb83d687f24 cause the performance regression issue. Could you please help to take a look?,torchbenchllamainferencefloat32dynamicdefaultperformancesingledrop_guilty_commit.log attached the bisect search log ,duplicated issue as:  CC([inductor][cpu] performance regression)
869,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Transformer with convolutional network )ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I calculated in nonautoregressive tts, asr, machinetranslation using transformer with convolutional network.  Machine learning was well carried out. So, I would like you to consider a transformer function with convolutional network. In my preliminary nonautoregressive machine learning with convolutional network transformer, 37% WER was obtained in Japanese English machine translation, 17% TER was obtained in ASR, good TTS result was obtained.  Alternatives example with convolutional network  example postln  TransformerEncoderLayer  example postln TransformerDecoderLayer   Additional context Thank you. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Transformer with convolutional network ," ğŸš€ The feature, motivation and pitch I calculated in nonautoregressive tts, asr, machinetranslation using transformer with convolutional network.  Machine learning was well carried out. So, I would like you to consider a transformer function with convolutional network. In my preliminary nonautoregressive machine learning with convolutional network transformer, 37% WER was obtained in Japanese English machine translation, 17% TER was obtained in ASR, good TTS result was obtained.  Alternatives example with convolutional network  example postln  TransformerEncoderLayer  example postln TransformerDecoderLayer   Additional context Thank you. ",2023-11-23T13:14:53Z,module: nn triaged needs research,open,0,4,https://github.com/pytorch/pytorch/issues/114462,I would like to work on this feature,"I learned for the first time that the term ""feed forward network"" refers to a linear network + ReLU. I have corrected the post. Thank you.","Hi Hennara  This issue is marked as ""needs research"" which means that it is unclear what is the requested feature and if we want this feature to be added in PyTorch. If you're looking for an issue you can work on, I would suggest to look at issues with ""actionable"" label.",Ok I will thanks  
475,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix ConvolutionBinaryInplace using target node)ï¼Œ å†…å®¹æ˜¯ (  CC(Fix ConvolutionBinaryInplace using target node)  CC([AOTI] Support ReinterpretView in abi mode) This IR node mutates in place, it needs to use the argument not the target. Fixes CC([inductor][cpu] freezing caused a lot of CV model crashed) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix ConvolutionBinaryInplace using target node,"  CC(Fix ConvolutionBinaryInplace using target node)  CC([AOTI] Support ReinterpretView in abi mode) This IR node mutates in place, it needs to use the argument not the target. Fixes CC([inductor][cpu] freezing caused a lot of CV model crashed) ",2023-11-23T02:41:29Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,1,15,https://github.com/pytorch/pytorch/issues/114436,There's a bug in convolution_unary where it uses the target of ConvolutionBinaryInplace even though it needs to use the input[1]. I am having hard time finding where this happens so partially reverting this part until I can find it. cc:   EDIT: updated the PR with the correct fix,", since more than 1 models failed among fp32/bf16 testing, please help to confirm all the failures fixed by this PR.","fangintel updated the PR, can you check whether this fixes the other issue? If not, please share the full assertion failure","> fangintel updated the PR, can you check whether this fixes the other issue? If not, please share the full assertion failure , please help to confirm","> fangintel updated the PR, can you check whether this fixes the other issue? If not, please share the full assertion failure Thanks , emmm.... I think we should not use `inputs[1]`. Per my understanding, `inputs[1]` and `packed.inputs[0]` might be different type. Since `inputs[1]` might be a `TensorBox` and unwrap into the `packed.inputs[0]` (`Buffer`) here https://github.com/pytorch/pytorch/blob/7daeb6509fe021462063d98c544339fc469075df/torch/_inductor/ir.pyL3748","fangintel I don't quite follow what you mean. The op mutates input[1], and previously via MutationLayout it was returning input[1]. So, returning input[1] is the correct result.","> fangintel I don't quite follow what you mean. The op mutates input[1], and previously via MutationLayout it was returning input[1]. So, returning input[1] is the correct result. I think `inputs[1]` equals to `TensorBox(StorageBox((packed.inputs[0])))` , and we should return the buffer object (same as previously) instead of the Tensorbox here. Otherwise the model level testing (and also this test case) will still fail. So, maybe return `packed.inputs[0]` here instead of `inputs[1]`. ",fangintel Thanks! I missed the buffer wrapping when i moved the code around. Updated the code., is working on verifying this fix among the failures we saw in fp32/bf16 static/dynamic shape tests. Expect he wiill help to post the result in early next week, merge, abort, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Actually  is correct, since init reorders the arguments, the `get_mutation_names` is incorrect. I will put a follow up PR to fix this. In general, reordering arguments will lead to bugs like this",https://github.com/pytorch/pytorch/pull/114501 fixes the mutation tracking bug. My apologies. I tried to cancel the merge but couldn't figure out how to do it.,> I tried to cancel the merge but couldn't figure out how to do it.  once mentioned that closing and reopening the PR should stop the merge. Although I never tried myself.
746,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([WIP][TESTING] _set_symbolic_storage_offset on meta tensors)ï¼Œ å†…å®¹æ˜¯ (  CC([WIP][TESTING] _set_symbolic_storage_offset on meta tensors)  CC([WIP] Do not specialize 0/1 eagerly for storage offset) Previously, the symbolic storage offset is not actually used if the tensor is not a view. That's because the empty tensor construction (just above the change in meta_utils.py in this PR) doesn't take the storage offset as a parameter. Note, this is not ready to land. There's some failures (e.g. in test_adv_index_batch) that need to be investigated. Submitting this to see CI.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[WIP][TESTING] _set_symbolic_storage_offset on meta tensors,"  CC([WIP][TESTING] _set_symbolic_storage_offset on meta tensors)  CC([WIP] Do not specialize 0/1 eagerly for storage offset) Previously, the symbolic storage offset is not actually used if the tensor is not a view. That's because the empty tensor construction (just above the change in meta_utils.py in this PR) doesn't take the storage offset as a parameter. Note, this is not ready to land. There's some failures (e.g. in test_adv_index_batch) that need to be investigated. Submitting this to see CI.",2023-11-22T23:32:30Z,Stale release notes: jit ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/114424,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
970,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] remove SmallVector optimization in PyInterpreter.cpp when storing custom sizes)ï¼Œ å†…å®¹æ˜¯ (See the comment here for more details: https://github.com/pytorch/pytorch/blob/main/torch/csrc/PyInterpreter.cppL616  I originally landed this but Ed pointed out that we should clean this up. The code there tries to maintain the same lifetime guarantees that `TensorImpl::size_and_strides` has. Technically it is backed by a `SmallVector`, which means that if you have a tensor with 2 dims, and you resize it to be 3 dims, the SmallVector will reuse the same buffer instead of resizing and allocating a new one. This is an implementation detail though  code in core shouldn't be relying on this detail, so if there are any tests that fail when we remove that optimization, we should just fix the tests. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] remove SmallVector optimization in PyInterpreter.cpp when storing custom sizes,"See the comment here for more details: https://github.com/pytorch/pytorch/blob/main/torch/csrc/PyInterpreter.cppL616  I originally landed this but Ed pointed out that we should clean this up. The code there tries to maintain the same lifetime guarantees that `TensorImpl::size_and_strides` has. Technically it is backed by a `SmallVector`, which means that if you have a tensor with 2 dims, and you resize it to be 3 dims, the SmallVector will reuse the same buffer instead of resizing and allocating a new one. This is an implementation detail though  code in core shouldn't be relying on this detail, so if there are any tests that fail when we remove that optimization, we should just fix the tests. ",2023-11-22T22:28:48Z,triaged module: __torch_dispatch__ module: dynamic shapes,open,0,0,https://github.com/pytorch/pytorch/issues/114414
443,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Provide access to the underlying module on NNModuleVariable, skip a string parsing calling to output_graph where possible)ï¼Œ å†…å®¹æ˜¯ (  CC([BE] Provide access to the underlying module on NNModuleVariable, skip a string parsing calling to output_graph where possible) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[BE] Provide access to the underlying module on NNModuleVariable, skip a string parsing calling to output_graph where possible","  CC([BE] Provide access to the underlying module on NNModuleVariable, skip a string parsing calling to output_graph where possible) ",2023-11-21T23:48:20Z,ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/114316,but it's failing tests?," will have to review this, the old code seems to have been written in a very specific way and I am not sure why it was written that way",> but it's failing tests? only lint?,"> Why is this useful? Seems like roughly the same amount of code, but now values are duplicated in multiple places which seems harder to maintain. Mild convenience of having to pass tx around, but I have another PR to make tx accessible anywhere, so i'll just go w/ that","We already have `InstructionTranslator.current_tx()`, no need to add something new."
1960,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RFC] Per-Parameter-Sharding FSDP)ï¼Œ å†…å®¹æ˜¯ ( PerParameterSharding FSDP  Motivation As we looked toward nextgeneration training, we found limitations in our existing FSDP, mainly from the _flat parameter_ construct. To address these, we propose a simpler FSDP design that shards each parameter on dim0 (hence, ""perparameter sharding"") that enables: 1. Flexible fp8 allgather: fp8 weights and other nonfp8 parameters can be flexibly mixed in the same allgather 2. Flexible frozen parameters: frozen and nonfrozen parameters can be flexibly mixed in the same communication group without using extra memory 3. Communicationfree sharded state dicts: matching the training and state dict representation simplifies and speeds up checkpointing 4. Future communication optimization in the compiler: a partial graph compiler like `torch.compile` can change the communication groups for allgather/reducescatter Overall, we strive for simplicity and composability in the new design.  We have validated the prototype on Llamalike models, achieving onpar throughput while using less memory. The only performance downside to this approach are extra copies before/after collectives; however, we plan to mitigate this with custom copy kernels/fastpaths. We welcome any and all feedback on this effort!  API The API is _not_ backward compatible with the existing FSDP (though model/optimizer checkpoints should be). We use this rewrite as a chance to simplify the constructor and expose new capabilities. A tentative API looks like:   Naming and organization are open to feedback!  `` checks that the API (`fully_shard`) preserves `named_parameters()`/`named_buffers()`/`named_modules()` and embeds a state and registry on the module. This is a broader composability effort with other distributed training)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[RFC] Per-Parameter-Sharding FSDP," PerParameterSharding FSDP  Motivation As we looked toward nextgeneration training, we found limitations in our existing FSDP, mainly from the _flat parameter_ construct. To address these, we propose a simpler FSDP design that shards each parameter on dim0 (hence, ""perparameter sharding"") that enables: 1. Flexible fp8 allgather: fp8 weights and other nonfp8 parameters can be flexibly mixed in the same allgather 2. Flexible frozen parameters: frozen and nonfrozen parameters can be flexibly mixed in the same communication group without using extra memory 3. Communicationfree sharded state dicts: matching the training and state dict representation simplifies and speeds up checkpointing 4. Future communication optimization in the compiler: a partial graph compiler like `torch.compile` can change the communication groups for allgather/reducescatter Overall, we strive for simplicity and composability in the new design.  We have validated the prototype on Llamalike models, achieving onpar throughput while using less memory. The only performance downside to this approach are extra copies before/after collectives; however, we plan to mitigate this with custom copy kernels/fastpaths. We welcome any and all feedback on this effort!  API The API is _not_ backward compatible with the existing FSDP (though model/optimizer checkpoints should be). We use this rewrite as a chance to simplify the constructor and expose new capabilities. A tentative API looks like:   Naming and organization are open to feedback!  `` checks that the API (`fully_shard`) preserves `named_parameters()`/`named_buffers()`/`named_modules()` and embeds a state and registry on the module. This is a broader composability effort with other distributed training",2023-11-21T20:01:48Z,triaged module: fsdp,open,38,74,https://github.com/pytorch/pytorch/issues/114299,See  CC(Tracing per-param sharding FSDP) for tracing with `torch.compile`.,> Flexible fp8 allgather: fp8 weights and nonfp8 parameters can be flexibly mixed in the same allgather (as long as their reducescatter uses the same dtype) is this a hard constraint (reducescatter is same dtype)? I think there are a lot of benefits for fp8 allgather but bf16 reducescatter,"> > Flexible fp8 allgather: fp8 weights and nonfp8 parameters can be flexibly mixed in the same allgather (as long as their reducescatter uses the same dtype) >  > is this a hard constraint (reducescatter is same dtype)? I think there are a lot of benefits for fp8 allgather but bf16 reducescatter Sorry, my wording might be confusing. fp8 allgather but bf16 reducescatter is exactly the use case we want to support :) Maybe the pseudocode in the first part of this comment helps clarify things: https://github.com/pytorch/pytorch/pull/114733issuecomment1832374773 For a transformer block, we want to support something like:  Allgather fp8 linear weights, bf16 layer norm parameters, and optional bf16 linear biases in one allgather kernel (by viewing everything as an 8bit dtype)  Reducescatter bf16 linear weight gradients, layer norm gradients, and linear bias gradients (_all_ bf16) in one reducescatter kernel For float8experimental and TransformerEngine (I think), we can specify the dtype in which the fp8 weight's gradient should be returned, so the constraint is that that dtype should match the dtype used for other parameters, e.g. bf16. If there is high demand for further allowing mixed dtypes for the reducescatter, we can consider allowing issuing 2 (or multiple) reducescatters for this communication group. (The fundamental constraint is that one reducescatter kernel can only use one dtype at the moment.)","> For a transformer block, we want to support something like: >  > * Allgather fp8 linear weights, bf16 layer norm parameters, and optional bf16 linear biases in one allgather kernel (by viewing everything as an 8bit dtype) > * Reducescatter bf16 linear weight gradients, layer norm gradients, and linear bias gradients (_all_ bf16) in one reducescatter kernel Got it, thanks! This is super helpful :). The writeup is phenomenal, thank you for sharing everything in detail. Very excited for this!",> Getting a full state dict can be implemented as a postprocessing step to saving the sharded state dict.  would you mind detailing the recommended approach a bit more? I'm not sure I fully understand the post process step in mind. Thank you!,"> > Getting a full state dict can be implemented as a postprocessing step to saving the sharded state dict. >  >  would you mind detailing the recommended approach a bit more? I'm not sure I fully understand the post process step in mind. Thank you! Good question! This part is not fully fleshed out yet. I will discuss more with  who is our expert on state dict. My current understanding:  FSDP's full state dict maps ""clean"" fullyqualified names (FQNs) to unsharded (i.e. full) `torch.Tensor`s, where ""clean"" means no `_fsdp_wrapped_module.` prefix, and it offers the options of rank0only and offloading to CPU.  Given a sharded state dict mapping clean FQNs to sharded `DTensor`s, we can implement the full state dict behavior using normal  `torch` and `DTensor` APIs.      The FQNs are clean already from not using an `nn.Module` wrapper but instead the dynamic subclass.      For `rank0_only=False`, we can use `redistribute()` to convert from `Shard(0)` to `Replicate()` placement.      For `rank0_only=True`, we want to `gather()` the `DTensor` to rank 0. I am not familiar with exactly how this would work since it is not SPMD, but it should be doable. cc:        For `offload_to_cpu=True`, we can make sure to call `.cpu()` while looping over the `DTensor`s. The main part that is unclear to me is the precise API. We can imagine something like:  However, I am not sure if we want to instead offload those helper functions to distributed checkpointing. Let us get back to you.","> device gives the device for both initialization and training. Managed parameters and buffers are moved to this device before fully_shard returns. Aside from testing, this is typically a CUDA (or CUDAlike) device. How important is it that fully_shard materializes the params on device eagerly?  For composability reasons, what if we wanted to do further model splitting (e.g. pipeline splitting) after applying fully_shard? ","> > device gives the device for both initialization and training. Managed parameters and buffers are moved to this device before fully_shard returns. Aside from testing, this is typically a CUDA (or CUDAlike) device. >  > How important is it that fully_shard materializes the params on device eagerly? For composability reasons, what if we wanted to do further model splitting (e.g. pipeline splitting) after applying fully_shard? Thanks   for reading and good question! First, let me share my thinking leading to that point, and we can see what we can do. This will be pretty verbose, but these days I want to document/share as much of what I am thinking as possible to avoid only one person holding many of the design constraints in their head.  DDP _does not_ move parameters to GPU for the user because DDP supports CPU training. Since DDP only needs to be applied at the top level, users can simply do `DDP(model.cuda(), **kwargs)`.  FSDP historically did not support CPU training, and FSDP has as further constraint that users cannot move the entire model to GPU at once.      varma has landed changes to support CPU training for existing FSDP (https://github.com/pytorch/pytorch/pull/112145); however, it still requires that torch is built with CUDA. We would have to conditionally disable CUDA constructs (e.g. streams) based on the device.  Because FSDP did not support or target CPU training, the `device_id` argument was a convenient way for FSDP to move managed module states to GPU for the user. However, without `device_id`, the user is responsible for the move. If the user forgot, then a FSDP would detect this upon the first forward and raise an error (which is not a great UX).  Because the vast majority of FSDP usages want the model on GPU after initialization, I was thinking to change the default behavior for `fully_shard` to be to move to GPU.      For manual wrapping usages, users typically materialize the module to pass to FSDP directly on CUDA or do so on CPU and immediately call `.cuda()`.      For auto wrapping usages, users either use `device_id` or move the entire sharded model from CPU to GPU after auto wrapping (where the former is faster since flattening/sharding happens on GPU).       Side note: We need to be disciplined on CUDA tensor construction inside FSDP. Otherwise, FSDP may not respect the device passed to `device_id`/`device` ( CC(FSDP requires global device context)). Since  CC(`Storage.resize_()` moves storage to current device) was fixed, the onus is completely on FSDP to plumb correctly.  Returning to your question:  For 3D parallelism (PP + TP + FSDP), I was originally expecting the order of sharding to be PP > TP > FSDP. Do you think there are cases where PP should be applied after already sharding with FSDP?  If we want to support the usage you mentioned, I was thinking we want to pass `device=""meta""`, so FSDP shards the parameters on meta device and does not materialize them. This will require _some deferred initialization API_ call later to materialize the sharded parameters (with appropriate randomness). The current PR does not support this aforementioned `device=""meta""` usage. I think that the deferred initialization API will _need_ to have FSDPspecific logic. Why? Because FSDP supports CPU offloading.  Context: Where is `device` used in FSDP _after_ initialization?      (1) It defines the unsharded parameter's device.      (2) FSDP moves forward input tensors to this device if needed for the user. (This is in a similar vein to the initialization; since FSDP only supported GPU, we wanted minimal friction for users. We can revisit this choice though.)  Without CPU offloading, we could do the following:      Context: Lazy initialization (`_lazy_init`) refers to when the FSDP modules must be finalized in terms of wrapping and device. It typically is upon the 1st forward, but it may be called for some other functions if needed. (We can also consider exposing a public API for manually calling this.)     https://github.com/pytorch/pytorch/blob/b6d42318b4655aaf399b0ad71e37832bb38a9fd8/torch/distributed/_composable/fsdp/_fsdp_state.pyL66L69      The order of operations would be: `fully_shard` initialization on meta device > deferred initialization call to materialize sharded parameters on GPU > use the sharded parameters' device to derive the new `device` in `_lazy_init`.      We make the `unsharded_param` construction lazy (running in `_lazy_init`), where we change `self.device` here (which is the one passed through the `device` arg) to get the device from the `sharded_param`, which we expect to be moved to GPU now by the deferred initialization API: https://github.com/pytorch/pytorch/blob/b6d42318b4655aaf399b0ad71e37832bb38a9fd8/torch/distributed/_composable/fsdp/_fsdp_param.pyL306L312     We reset `FSDPState._device`, which is used to move input tensors to GPU for the user, to the sharded parameters' device (need uniform device!). This moving behavior right now is only for the root FSDP to avoid unnecessary CPU overhead in the typical case.  With CPU offloading:      The issue is that we _do not_ want to move offloaded sharded parameters to GPU. The deferred initialization API _must_ ignore these parameters. However, without some FSDPspecific handling, there is no way to know distinguish that those parameters are meant to be kept on CPU!      Furthermore, with CPU offloading, we _cannot_ derive the `unsharded_param` device from the `sharded_param` device, which is always CPU. This would require a second sort of `compute_device` argument that is _only_ used when CPU offloading. In that case, I would vote for it to be added to `OffloadPolicy`, but even then, it might also be confusing to users.      Is this meta device initialization actually to be used with CPU offloading (i.e. is this a real composition that we care about)? I am not sure. However, I would prefer to error on the side of caution and keep things as composable as possible.","A few responses while i think of them > If the user forgot, then a FSDP would detect this upon the first forward and raise an error (which is not a great UX) I could be wrong, but i think its acceptable to just raise errors for wrong device (as long as we can reliably do so instead of silently proceeding and doing something wrong or with bad perf due to ondemand copies or something).  I think i'd prefer simplicity of the system over convenience in this case. > FSDPState._device, which is used to move input tensors to GPU for the user I'm also wondering if this is a needed convenience or if its really just enabling worse user code in some cases its actually important to know this (if you want to ensure your dataloader is producing tensors on the right device for instance). > For 3D parallelism (PP + TP + FSDP), I was originally expecting the order of sharding to be PP > TP > FSDP. Do you think there are cases where PP should be applied after already sharding with FSDP? A few answers to this. (1) is we may need to define an order of composition due to overall constraints, but we haven't yet.  so lets try to align on that as a team.  My own assumption was to do TP > DP > PP, but I have heard good arguments for the PPfirst approach.  (2) is we might want to be more flexible in allowing different orderings, that certainly seems like ""composability"" in its extreme, but may not be possible. > With CPU offloading: ill have to come back to this point, haven't thought about it.",Minor comment while FSDP is being redesigned: it would be nice to expose forward/backward prefetch limits in the public API in the rewrite for CPU bottlenecked workloads for cases with nonhomogenous wrapping where setting these to the default value 1 is insufficient,"> Minor comment while FSDP is being redesigned: it would be nice to expose forward/backward prefetch limits in the public API in the rewrite for CPU bottlenecked workloads for cases with nonhomogenous wrapping where setting these to the default value 1 is insufficient We hear you! We plan to include it in `CommPolicy` (I renamed it from `PrefetchPolicy` originally since we can expose some other options too in the future, e.g. should the allgather/reducescatter streams use high priority? should we allow for using NCCL group coalescing instead of copyin/out for collectives?).","> We hear you! We plan to include it in `CommPolicy` (I renamed it from `PrefetchPolicy` originally since we can expose some other options too in the future, e.g. should the allgather/reducescatter streams use high priority? should we allow for using NCCL group coalescing instead of copyin/out for collectives?). Thanks!! That's great :D "," This rewrite basically solves almost all my concerns with FSDP, so I'm going to get greedy and ask for one more :)  When writing activation checkpointing with FSDP, one annoying bit is that it requires modules to be wrapped with activation checkpointing. This is useful for simple activation checkpointing at block level, but it makes selective checkpointing very difficult.  As an example, consider a transformer with an expansion_ratio of 4 and gelu as an activation function. I would really like gelu to not store a copy of activations, especially because it is in the expanded state. Currently, I have two options:  write a custom backward pass with `torch.autograd.Function` which works but is brittle to changes and annoying  move gelu + mlp2 in the FFN into a separate torch module, which is horrible from a readability perspective Currently, the API lets me target modules and say disable storing input except for the start of the module. Ideally, I would like an API that lets me specify a layer and disable caching input for that layer so I can just specify gelu without writing nasty code This may already be possible! If so, please feel free to correct me. Also feel free to say this rewrite won't be able to address this","> > For 3D parallelism (PP + TP + FSDP), I was originally expecting the order of sharding to be PP > TP > FSDP. Do you think there are cases where PP should be applied after already sharding with FSDP? >  > A few answers to this. (1) is we may need to define an order of composition due to overall constraints, but we haven't yet. so lets try to align on that as a team. My own assumption was to do TP > DP > PP, but I have heard good arguments for the PPfirst approach. (2) is we might want to be more flexible in allowing different orderings, that certainly seems like ""composability"" in its extreme, but may not be possible. >  Adding in unsolicited two cents  I prefer PP first because I would like to change my FSDP config inside different PP blocks. The use case I am thinking of is pipelining across two different clusters with different hardware setups that require different FSDP params for optimal performance. I would suspect the PP first would give an easier user interface for this scenario"," For the selective activation checkpointing, I do not think there is anything publicly available in PyTorch at this moment, but there is an example publicly in xFormers: implementation, unit test. We have to check if this composes with FSDP, but since it is based on `saved_tensors_hooks`, I think it should work. Let me look into the plan around PyTorchnative support for this so that it can be more easily accessible. So far, I think selective activation checkpointing is only available as part of `torch.compile`.","Two updates: 1. Evaluating on internal recommendation models, the current allgather copyout and reducescatter copyin have too much CPU overhead and achieve poor GPU memory bandwidth when written using native PyTorch. We are investigating **custom kernels** for these copies, which serve as the main performance differentiator between flatparameter and perparameter sharding. The main gain comes from bypassing framework overheads. 2. In that evaluation, we encountered the tradeoff of whether we want to overlap the _next_ allgather copyin with the _current_ allgather copyout. We discuss this trade off in depth below.  **Should we overlap the next allgather copyin with the current allgather copyout?** In the original implementation, we _do not_ overlap these to save memory. The pattern looks like:  Allocate allgather output tensor and _view_ into it for the allgather input tensor  Copyin the sharded parameters into the allgather input tensor  Allgather  Copyout the unsharded parameters In particular, the allgather input and output share the same base tensor. To only have a single allgather tensor alive at once, we have the next copyin to wait for the current copyout (i.e. we do not allocate the next allgather tensor until the previous one is no longer used). However, this means that the copyout and copyin are fully exposed (i.e. not overlapped with communication). If we want to overlap them (and if doing so does not degrade the overall copy and collective performances), then we must change the synchronization to allow both the current and next allgather tensors to be alive at once. We implemented this in https://github.com/pytorch/pytorch/pull/114733/commits/1106d4fa8a9159c5a3b294ce766f1312f73a3a6b. However, we cannot enable this by default because it increases FSDP's peak memory usage for frozenparameter cases. Without any frozen parameters, the memory contribution looks like:  In forward: 2x ""layer size"" from current allgather and current copyout  In backward: 3x ""layer size"" from current parameters, current gradients, and next allgather. Therefore, assuming that the true peak memory happens in backward, we can safely use 3x ""layer size"" in forward without increasing the peak memory. However, with frozen parameters (e.g. in parameterefficient finetuning), the backward contribution looks closer to ~2x from current parameters, small current gradients, and next allgather. As such, we cannot safe use 3x ""layer size"" without risking increasing the peak memory. Namely, for workloads prioritizing memory, having the copyout and copyin exposed to save an extra 12 GB may be worth it. Thus, we probably need to gate this behavior behind a flag or more generally a `""reduce_memory""` mode vs. a `""increase_throughput""` mode. Another option is to make the allgather input not a view into the allgather output. I am not sure if this is desirable though because it will still require an extra `1/W` factor of memory for the smallscale memoryconstrained case and includes extra copying inside the NCCL allgather in the largescale throughputfocused case. I.e., it might be a medium point, but it might not be good for either case. Update: Even for Llama7B and 8 GPUs, the increase is only ~0.3 GB, so we may just overlap always.","Maybe a naive question, what is the motivation to replace the parameters with `DTensors`? From what I see the collectives are still controlled by the FSDP implementation itself (allgather/reducescatter) and there is no autocompletion for CCs.  A bit confused what role does `DTensor` play here."," That is a good question! We use `DTensor` as the sharded parameter representation since it provides a few benefits: 1. Since `DTensor`s hold their sharding information directly on themselves (at the tensor level), users can manipulate them directly. For example, instead of having to implement a `summon_full_params()` API, the user could use `DTensor` APIs directly like `DTensor.full_tensor()` to convert to a replicated `torch.Tensor` (allgathering sharded dimensions appropriately). The same applies to their gradients. 2. We can rely on `DTensor` infra for ops like `clip_grad_norm_`, which otherwise would require custom distributed implementations that are aware of the sharding. In fact, we are able to directly use `torch.nn.utils.clip_grad_norm_` with this FSDP implementation now (example). 3. We can rely on `DTensor` infra for random ops (i.e. `DTensor` will ensure that random ops are correct with respect to the tensor's global shape). This allows us to provide a simpler metadevice initialization flow, where we can shard on meta device and materialize onto GPU _after_ sharding (example). 4. We can have `model.state_dict()` and `optimizer.state_dict()` directly use `DTensor` like FSDP1's sharded state dict. Alternatively, we could have model state dict presave hooks to wrap with `DTensor` and preload hooks to unwrap from `DTensor`, but this would not work optimizer state dict, where the sharding info is not available on the optimizer. (In other words, this makes `model.state_dict()` and `optimizer.state_dict()` produce valid sharded state dicts outofthebox.) This being said, making the sharded parameters `DTensor`s can lead to some sharp edges. For example, we would need to figure out our `DTensor` custom op support story in order for users to plug in an optimizer that uses a custom op, or if the user is not using `DTensor` for their tensor parallelism, then having `DTensor` only represent 1 dimension of sharding may be confusing. Let me know what you think!","Thanks  for the detailed explanation, very helpful!  So from my perspective usage of `Dtensor` for FSDP is ok even if user does not use `Dtensor` for TP, it is something user should handle. However the custom ops are a bit concerning, how does `Dtensor` perform for custom ops? Given here `Dtensor` is not responsible for any CC autocompletion, I would expect they just act like normal tensors?    A side question, does `Dtensor` provide any benefit for `torch.compile`?","> So from my perspective usage of Dtensor for FSDP is ok even if user does not use Dtensor for TP, it is something user should handle.  That makes sense to me! > However the custom ops are a bit concerning, how does Dtensor perform for custom ops? We should have some proposal/design for this soon. cc:    In my naive understanding, the main requirements are that the user needs to register the custom op through normal PyTorch mechanisms (torch library?) and then provide some sharding rules (for which we may provide some utilities for common ones). > Given here Dtensor is not responsible for any CC autocompletion, I would expect they just act like normal tensors? Yes, we do not use the `DTensor`s for communication inside the FSDP runtime, so we effectively manipulate them like normal tensors. > A side question, does Dtensor provide any benefit for torch.compile? I think it should. cc:    on the latest status here","> We should have some proposal/design for this soon. cc:   In my naive understanding, the main requirements are that the user needs to register the custom op through normal PyTorch mechanisms (torch library?) and then provide some sharding rules (for which we may provide some utilities for common ones). Yes!  The main thing we need to do here is to make sure the custom op in the third party optimizers are registered in the PyTorch way (i.e. going through dispatcher, torch.Library), and they defines a sharding strategy for the custom op,  is working on a proposal soon. Once the custom op is going through the disapatcher, defining the sharding strategy can be as easy as `register_sharding(custom_op, pointwise_strategy)` if the custom op is pointwise op, a bit more involved if it's not pointwise. We'll by default support all `torch.optim` optimizers and for custom optimizers there will be a tutorial to document what to do. The main motivation on this is that FSDP performs sharding, and for optimizer computation one could not simply assume it's local shard computation as it might be wrong numerically. Today FSDP1 only works for pointwise optimizers, but it can't deal with nonpointwise optimizers (i.e. AdaFactor) easily because the sharding information got lost in optimizer computation."," This looks really relevant for some of my work as well, can you please share the proposal for the custom op when it is live? Thanks, appreciate the details.","> For HSDP, we assume sharding on the 0th mesh dim and replication on the 1st mesh dim Shouldn't it be the other way around? Sharding on the last dimension (with the adjacent device ranks, thus more comms) and replication on the first dimension.","> > For HSDP, we assume sharding on the 0th mesh dim and replication on the 1st mesh dim >  > Shouldn't it be the other way around? Sharding on the last dimension (with the adjacent device ranks, thus more comms) and replication on the first dimension. Good catch! Let me fix this. We should be sharding on the 1st mesh dim and replicating on 0th mesh dim.", I did a quick command + f to search but could not find it. Could you point me to where it says that?,It's at https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md, Thanks! Let me submit a PR tomorrow to fix it.,"I'm curious how FSDP would work with Expert Parallel. Consider this mesh on an 8 GPU system: Attention: [tp: 2] [dp: 4] Feedforward: [expert parallel: 4] [dp: 2] Notice that the dp world size is different between the attention and feedforward layers. Does this mean sharding with FSDP two separate times, and then the interleaving FSDP instances cooperate with each other? We aren't currently training this way, but I think it's how the major MoE labs are functioning. TP world size can't be too big, but EP world size must be very big or else weight transfer comms explode (or else PP is required, with dropless MoE)."," Would it help you if I migrated our internal codebase to FSDP2 now, or would you prefer more time to work on it first? I can try copying the interface from torchtitan, then compare to FSDP1. We get 53% MFU on an HGX. I can start after CoreWeave updates their images to 2.3."," I will admit that I am not caught up on the current MoE approaches but would be interested to learn more. > Does this mean sharding with FSDP two separate times, and then the interleaving FSDP instances cooperate with each other? This makes sense to me. We can shard the Attention and FFN in separate FSDP groups (with different FSDP `mesh` args) and include collectives to ""reshard"" activations when changing DP size. I think that the stream synchronization in FSDP1/2 should still be correct even with different NCCL process groups across FSDP groups. By the way, if I wanted to read up more on current MoE parallelization approaches, do you have any recommendations?  > Would it help you if I migrated our internal codebase to FSDP2 now, or would you prefer more time to work on it first? I can try copying the interface from torchtitan, then compare to FSDP1. We get 53% MFU on an HGX. I can start after CoreWeave updates their images to 2.3. I think FSDP2 would be ready to try out. The main functionality is all landed (notable gaps listed in  CC([FSDP2] Eager-Mode Execution Tracker)). Your feedback would be much appreciated. Edit: There might have been some relevant PRs that landed after 2.3 branch cut ğŸ¤”  If you are pinned to 2.3, then maybe it is worth waiting until 2.4. Perhaps, this is the challenge with having FSDP incore as opposed to outofcore."
430,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(FrozenDict) __contains__ [ConstantVariable(str)] {})ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Found in https://github.com/huggingface/diffusers/issues/4731  Versions Not sure if still present on main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(FrozenDict) __contains__ [ConstantVariable(str)] {}, ğŸ› Describe the bug Found in https://github.com/huggingface/diffusers/issues/4731  Versions Not sure if still present on main ,2023-11-21T04:00:41Z,triaged oncall: pt2 module: dynamo module: graph breaks dynamo-must-fix dynamo-triage-june2024,closed,1,2,https://github.com/pytorch/pytorch/issues/114202,This the frozenDict link  https://github.com/google/flax/blob/main/flax/core/frozen_dict.py,"This is not valid issue anymore, Dynamo can inline  properly and no graph break."
1163,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([JIT] - torch.script - 'Optional[Tensor]' object has no attribute or method 'size')ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Could not manage to solve nor to understand the appearance of this error: `'Optional[Tensor]' object has no attribute or method 'size'`     Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.27.1 Libc version: glibc2.31 Python version: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.027genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.0.221 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB [...] Versions of relevant libraries: [pip3] numpy==1.25.2 [pip3] torch==2.0.1 [pip3] torchmodelarchiver==0.8.1 [pip3] torchserve==0.8.1 [pip3] triton==2.0.0 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[JIT] - torch.script - 'Optional[Tensor]' object has no attribute or method 'size'," ğŸ› Describe the bug Could not manage to solve nor to understand the appearance of this error: `'Optional[Tensor]' object has no attribute or method 'size'`     Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.27.1 Libc version: glibc2.31 Python version: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.027genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.0.221 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB [...] Versions of relevant libraries: [pip3] numpy==1.25.2 [pip3] torch==2.0.1 [pip3] torchmodelarchiver==0.8.1 [pip3] torchserve==0.8.1 [pip3] triton==2.0.0 ",2023-11-20T10:15:43Z,oncall: jit,open,4,6,https://github.com/pytorch/pytorch/issues/114112,This error occurs due to bad type handling in the `transformers` code. The `size` method throwing the error is being called from an object that has type `Optional[torch.FloatTensor]` per the type hinting.,I've experienced a similar issue using huggingface's Mask2Former model. The bad handling of the input types seems to be scattered around their code base. I've raised an issue a few minutes ago on the huggingface Github issues page.,> This error occurs due to bad type handling in the `transformers` code. The `size` method throwing the error is being called from an object that has type `Optional[torch.FloatTensor]` per the type hinting. èƒ½ä¸èƒ½è¯´ç‚¹æœ‰æ„ä¹‰çš„äº‹æƒ…,> > This error occurs due to bad type handling in the `transformers` code. The `size` method throwing the error is being called from an object that has type `Optional[torch.FloatTensor]` per the type hinting. >  > èƒ½ä¸èƒ½è¯´ç‚¹æœ‰æ„ä¹‰çš„äº‹æƒ… Please reply in English. ," hello! do you know if a solution has been implemented for this? the huggingface request is still open, it seems",">  hello! do you know if a solution has been implemented for this? the huggingface request is still open, it seems Hello, I unfortunately never got the time to do it. The employer I was with where this was an issue is no longer in the picture as I've moved on to another role. I may be able to look at it later this month. Sorry for the delay! "
734,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update documentation to mention discrepancy between SDPA and Transformer/MHA mask definition)ï¼Œ å†…å®¹æ˜¯ (Currently, `torch.nn.functional._canonical_mask` inverts the mask given input of type `torch.bool`, breaking all of the forward calls in `nn.Transformer`. This function seems to have been introduced in CC(Regularize mask handling for attn_mask and key_padding_mask). Minimum repro example:  Output:  Expected:  Request: If someone could either add a test to prevent future regressions like this, or let me know where I could add one, that'd be great, thanks!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Update documentation to mention discrepancy between SDPA and Transformer/MHA mask definition,"Currently, `torch.nn.functional._canonical_mask` inverts the mask given input of type `torch.bool`, breaking all of the forward calls in `nn.Transformer`. This function seems to have been introduced in CC(Regularize mask handling for attn_mask and key_padding_mask). Minimum repro example:  Output:  Expected:  Request: If someone could either add a test to prevent future regressions like this, or let me know where I could add one, that'd be great, thanks!",2023-11-20T00:58:40Z,triaged open source Stale release notes: nn,closed,0,7,https://github.com/pytorch/pytorch/issues/114083,The committers listed above are authorized under a signed CLA.:white_check_mark: login: Rohan138 / name: Rohan Potdar  (9c362b3a067b70babf092fe023f1ec784c45e642)," label ""release notes: nn""","  Hi, sorry for the pingcould I get some assistance merging this? It's a minor bug but affects all masks in `nn.Transformer`. The CI tests that are failing in `test/nn/test_multihead_attention` are also failing on main and seem unrelated. Let me know if there's any other lint/bug fixes I'm missing."," ah okay, I see. I can close this PR, but would it be possible to either use the same convention across both `nn` and `scaled_dot_product_attention`, or add a note highlighting the difference? It's a little confusing as it is right now. Thanks!"," That sounds reasonable to me, would you be willing to open a PR that makes the suggested docs update? :)","Will do, thanks!","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
256,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(inconsistency when bn and cholesky_inverse)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Outputs )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,inconsistency when bn and cholesky_inverse, ğŸ› Describe the bug   Outputs ,2023-11-19T00:41:03Z,module: numerical-stability triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/114039,"Had the lines for last dimension printed out in which different values (>1e5) are observed ``` tensor([  221.5524,  3738.7180,  2139.8652, 1044.2388, 1302.5089, 3889.6680,          2468.4026, 3839.9080], device='cuda:0', grad_fn=) tensor([  221.5491,  3738.6895,  2139.8677, 1044.2314, 1302.5027, 3889.6316,          2468.3750, 3839.8738], grad_fn=) 0 2 0 tensor([   3738.7573,  114754.7891,   64643.3789,  32369.4590,  39165.3320,         117545.3203,   75054.5391, 116657.7578], device='cuda:0',        grad_fn=) tensor([   3738.6895,  114754.7969,   64643.3594,  32369.4746,  39165.3359,         117545.3047,   75054.5391, 116657.7578], grad_fn=) 0 2 1 tensor([  2139.9062,  64643.3633,  36427.6328, 18229.9297, 22070.2637,         66233.3984,  42285.3789, 65727.8906], device='cuda:0',        grad_fn=) tensor([  2139.8677,  64643.3594,  36427.6289, 18229.9375, 22070.2676,         66233.3828,  42285.3750, 65727.8906], grad_fn=) 0 2 2 tensor([ 1044.2505, 32369.4629, 18229.9355,   9134.2646,  11048.8125,          33157.0195, 21173.0078,  32905.7227], device='cuda:0',        grad_fn=) tensor([ 1044.2314, 32369.4746, 18229.9375,   9134.2705,  11048.8184,          33157.0312, 21173.0137,  32905.7344], grad_fn=) 0 2 3 tensor([ 1302.5259, 39165.3242, 22070.2695,  11048.8145,  13381.8623,          40146.3945, 25629.7402,  39832.4336], device='cuda:0',        grad_fn=) tensor([ 1302.5027, 39165.3359, 22070.2676,  11048.8184,  13381.8643,          40146.3945, 25629.7344,  39832.4297], grad_fn=) 0 2 4 tensor([  3889.7014, 117545.2891,  66233.3906,   33157.0195,   40146.3906,          120461.0078,  76906.6562,  119530.2734], device='cuda:0',        grad_fn=) tensor([  3889.6316, 117545.3047,  66233.3828,   33157.0312,   40146.3945,          120461.0078,  76906.6484,  119530.2734], grad_fn=) 0 2 5 tensor([  2468.4197,  75054.5312,  42285.3828, 21173.0078, 25629.7324,         76906.6484,  49102.8203, 76315.3438], device='cuda:0',        grad_fn=) tensor([  2468.3750,  75054.5391,  42285.3750, 21173.0137, 25629.7344,         76906.6484,  49102.8125, 76315.3438], grad_fn=) 0 2 6 tensor([  3839.9431, 116657.7500,  65727.8984,   32905.7188,   39832.4258,          119530.2734,  76315.3438,  118615.3047], device='cuda:0',        grad_fn=) tensor([  3839.8738, 116657.7578,  65727.8906,   32905.7344,   39832.4297,          119530.2734,  76315.3438,  118615.3047], grad_fn=)",jeancho could you please help debug this one ?,The relative difference is `~1e5` while the absolute difference is larger due to the large output ranges:  which looks expected. 
269,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(inconsistency on GPU and CPU torch.cholesky_inverse(x))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Outputs  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,inconsistency on GPU and CPU torch.cholesky_inverse(x), ğŸ› Describe the bug   Outputs  ,2023-11-19T00:32:54Z,module: cuda triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/114038,https://pytorch.org/docs/stable/notes/numerical_accuracy.html `atol=1e6` gets `True`.,Passing `atol=1e6` sounds reasonable for `float32` input. Closing 
2015,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Load model from jit script format. Repeating inference several times can lead to errors.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  1. Save to jit script model from hf.   2. Load jit script model and infer multi times.   3. Will have following exception:   Versions Collecting environment information... PyTorch version: 2.1.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.18.5 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux4.19.016amd64x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080 Ti GPU 1: NVIDIA GeForce GTX 1080 Ti GPU 2: NVIDIA GeForce GTX 1080 Ti GPU 3: NVIDIA GeForce GTX 1080 Ti GPU 4: NVIDIA GeForce GTX 1080 Ti GPU 5: NVIDIA GeForce GTX 1080 Ti GPU 6: NVIDIA GeForce GTX 1080 Ti GPU 7: NVIDIA GeForce GTX 1080 Ti Nvidia driver version: 525.105.17 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):           )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Load model from jit script format. Repeating inference several times can lead to errors.," ğŸ› Describe the bug  1. Save to jit script model from hf.   2. Load jit script model and infer multi times.   3. Will have following exception:   Versions Collecting environment information... PyTorch version: 2.1.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.18.5 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux4.19.016amd64x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080 Ti GPU 1: NVIDIA GeForce GTX 1080 Ti GPU 2: NVIDIA GeForce GTX 1080 Ti GPU 3: NVIDIA GeForce GTX 1080 Ti GPU 4: NVIDIA GeForce GTX 1080 Ti GPU 5: NVIDIA GeForce GTX 1080 Ti GPU 6: NVIDIA GeForce GTX 1080 Ti GPU 7: NVIDIA GeForce GTX 1080 Ti Nvidia driver version: 525.105.17 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):           ",2023-11-19T00:13:39Z,oncall: jit,open,0,4,https://github.com/pytorch/pytorch/issues/114035,Could you try:  I'm not sure if `input_ids` are being mutated,> Could you try: >  >  >  > I'm not sure if `input_ids` are being mutated The same issue.,same here,"Since torch.jit is on maintenance mode, this issue is unlikely to be fixed. You can try with 2.0.1 version which works in Linux. If Windows is your platform, I guess you should move to onnx.  Another option may be torch.export, but is not stable yet."
364,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([WIP] Do not specialize 0/1 eagerly for storage offset)ï¼Œ å†…å®¹æ˜¯ (  CC([WIP][TESTING] _set_symbolic_storage_offset on meta tensors)  CC([WIP] Do not specialize 0/1 eagerly for storage offset) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[WIP] Do not specialize 0/1 eagerly for storage offset,  CC([WIP][TESTING] _set_symbolic_storage_offset on meta tensors)  CC([WIP] Do not specialize 0/1 eagerly for storage offset) ,2023-11-18T00:38:33Z,Stale release notes: fx module: inductor module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/114010,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
875,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([export] Helper function for specifying dynamic batch size)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch For infra users, sometimes they want to be able to just say ""set the first dimension of every input to be a batch size"". But in order to do this, the current dynamic_shapes API asks for a mapping of input dimensions to the function signature. But from an infra point of view though, they don't know the function signature. So currently the way to do so is something like what is done in this file, although this file assumes the input is just a list of tensors (no pytrees). It would be nice to wrap this up in some helper function.    Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[export] Helper function for specifying dynamic batch size," ğŸš€ The feature, motivation and pitch For infra users, sometimes they want to be able to just say ""set the first dimension of every input to be a batch size"". But in order to do this, the current dynamic_shapes API asks for a mapping of input dimensions to the function signature. But from an infra point of view though, they don't know the function signature. So currently the way to do so is something like what is done in this file, although this file assumes the input is just a list of tensors (no pytrees). It would be nice to wrap this up in some helper function.    Alternatives _No response_  Additional context _No response_",2023-11-17T21:09:29Z,triaged export-triage-review oncall: export,open,0,3,https://github.com/pytorch/pytorch/issues/113985, recently added support for `dynamic_shapes` being a tuple instead of a dict to bypass the need for signature. Curious if you've tried that and think that's good enough.," oh, I haven't tried that yet, thanks for the pointer!","We can just do:   if it's just this one line, I'm not sure if we need a helper function. wdyt? "
984,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to  re-use torch.compile results in different python processes?)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I'm trying to compile my custom vision transformerbased model. The compiled version is indeed faster than the traditional one. However, as scaled_dot_product_attention does not support dynamic shapes, the program compiles the transformer block for every input size. Thus, the TEST program takes ~1520 minutes to compile the model and then processes hundreds to thousends pictures, which is ~10 times slower than the eager mode. I wonder if there's some api to save the intermediate states, so that when I run the same code again, I can reuse the compilation results in /tmp/torchinductor_$user and skip the boring compilation stage?   Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,How to  re-use torch.compile results in different python processes?," ğŸš€ The feature, motivation and pitch I'm trying to compile my custom vision transformerbased model. The compiled version is indeed faster than the traditional one. However, as scaled_dot_product_attention does not support dynamic shapes, the program compiles the transformer block for every input size. Thus, the TEST program takes ~1520 minutes to compile the model and then processes hundreds to thousends pictures, which is ~10 times slower than the eager mode. I wonder if there's some api to save the intermediate states, so that when I run the same code again, I can reuse the compilation results in /tmp/torchinductor_$user and skip the boring compilation stage?   Alternatives _No response_  Additional context _No response_ ",2023-11-17T08:22:11Z,high priority feature triaged months oncall: pt2 module: dynamic shapes module: dynamo,closed,0,16,https://github.com/pytorch/pytorch/issues/113933,Do you mind running your program with `TORCH_LOGS=dynamic` and posting the logs here? Thanks!,>  in different python processes How many processes are we talking about?,"> > in different python processes >  > How many processes are we talking about? Sorry I seemed to describe it ambiguously. I didn't mean any multiprocessing stuffs. I was wondering about the possibility of reusing compiler/inductor cache from ANOTHER process, or program, if the code involved keeping unchanged. Currently pytorch would recompile the codes if you quit python and rerun it a minute or an hour later.","As of now it's basically impossible. The compiled artifact lifetimes are tied to python objects, and we dont have a standard way of serializing them. Your best bet might be torch.export?","> Do you mind running your program with `TORCH_LOGS=dynamic` and posting the logs here? Thanks! Sure! I'm using pytorch 2.1.0 wit cuda 11.8. In the zip file: convit2.py is the source code for the model I used. logtest.out is the running logs with TORCH_LOGS=dynamic and .compile upon SwinTransformerBlock.forward at L:468. logtest2.out is the running logs with TORCH_LOGS=all and .compile(dynamic=False) upon SwinTransformerBlock.forward at L:468. Only this setting may pass the compile process both for train and inference, though the compile time is really long. logtest3.out and logtest4.out is the running logs with TORCH_LOGS=all and .compile upon either Data2VecConViT2.forward_encoder or upon *Layer.forward. Both dynamic=True and dynamic=None result in compiling errors. logs.zip","I am actually not sure why you concluded the problem is sdpa specific, I don't see anything in the logs that indicates this. In logtest.out I do see a few specializations  s0 here is x.size(1), though I'm not sure exactly what this parameter is (surely it is not channels? Maybe it is?!) Another thing I notice is that there are a lot of graph breaks. It might be worth trying to get rid of some of them. In any case, your original feature request seems valid. It's probably something we'll be working on soon.","Wait a minute, don't we have TORCHINDUCTOR_CACHE_DIR? Also TORCHINDUCTOR_FX_GRAPH_CACHE. Would it help at all? :thinking:","See also:  CC(Provide a way to AOT torch.compile and serialize a model) It is reported that these only slightly mitigate compilation cost, but it is not enough. Actually, I'm interested in contributing to this. I believe that persistent code reuse could be really valuable. But there are many tiers to this. 1. Guardlevel 2. Generated fx  backend compiler 3. Backendlevel Each presents its own challenges. Backendlevel seems to be partly implemented.","> I am actually not sure why you concluded the problem is sdpa specific, I don't see anything in the logs that indicates this. In logtest.out I do see a few specializations >  >  >  > s0 here is x.size(1), though I'm not sure exactly what this parameter is (surely it is not channels? Maybe it is?!) >  > Another thing I notice is that there are a lot of graph breaks. It might be worth trying to get rid of some of them. >  > In any case, your original feature request seems valid. It's probably something we'll be working on soon. Thanks for your timely reply! The following logs (L3148L3168 in logtest.out) indicate the problem is related to the sdpa function. I also tried to compile the different part of the WindowAttention block, and the program crashes as long as the sdpa is involved into a torch.compile(dynamic=True) decorator.  I knew that there are a lot of graph breaks in my program, and I have done my best to get rid of them and failed. This was mainly because that torch.compile seems to be incompatible with torch.utils.checkpoint (see logtest3.out/logtest4.out). As the training process depends highly on the checkpoint trick to save GPU memory, I cannot remove them and have to compile the graphs seperately in each checkpoint block. That's why there are so many fx graphs.","The real way to have a crossprocess cache is to remove interpreter specific notions from our trace products. This starts with guards, but also eclipses other notions as well, like eval_frame caching. I took a few hours to play around with this and cut a prototype branch that serializes compiled state to disk, and deserializes it again. Albeit with a bunch of obviously silly hacks we cant ship, this is an example of a direction we might take. https://github.com/pytorch/pytorch/compare/voz/serde  Was maybe ~2 hours of work. Happy to write more about this, the idea for how to do it is relatively clear in my head. There's probably another ~23 hours of work to get it to a place where we might actually be okay landing it behind a config, and making it work with a very limited set of cases while we explore what this unlocks. ","https://github.com/pytorch/pytorch/pull/114499 for an example  I am still iterating, but the test example works. Still very trivial, still a lot of directional changes to make. One major gap in that PR, is that the flag in the config will 100% not ship, we want to collapse the two worlds.", Is currently the cache still process specific? It cannot be recycled in a new python process same code right?,"We're working on this. The inductor cache works right now and can be used cross process. An AOTAutograd cache is in progress.  is also trying to get us to spend some resources on a noncache, more explicit interface as well.","Hello  , using the latest version of Python, when I run a training session using `torch.compile` with `mode=""maxautotunenocudagraphs""` and then rerun another session after the first one is finished, the model does not recompile, which is quite practical. However, when I run a training session from another terminal or like a day later, it does recompile. Is there any way to avoid this?","By default the file system cache products are put in /tmp, and I guess you are getting a tmp cleaner. Change it with TORCHINDUCTOR_CACHE_DIR. Read more at https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html","FWIW,  it looks like the folks over at ParlerTTS figured out how to get SDPA working with torch.compile() via PR CC(Containers should allow module assignments). Looking at the code, it may be as simple as calling .contiguous() on the input_ids?  Thanks! >  ğŸš€ The feature, motivation and pitch > I'm trying to compile my custom vision transformerbased model. The compiled version is indeed faster than the traditional one. >  > However, as scaled_dot_product_attention does not support dynamic shapes, the program compiles the transformer block for every input size. Thus, the TEST program takes ~1520 minutes to compile the model and then processes hundreds to thousends pictures, which is ~10 times slower than the eager mode. >  > I wonder if there's some api to save the intermediate states, so that when I run the same code again, I can reuse the compilation results in /tmp/torchinductor_$user and skip the boring compilation stage? >  >  Alternatives > _No response_ >  >  Additional context > _No response_ >  > "
476,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([utils] move `config_typing.pyi `to `torch.utils`)ï¼Œ å†…å®¹æ˜¯ (  CC([utils] move `config_typing.pyi `to `torch.utils`)  CC([dynamo] promote skipfiles logging to verbose)  CC([dynamo] remove unused `OptimizeCtx` field  export)  CC([dynamo] Cache size calc for differing config)  CC([dynamo] guarded config) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[utils] move `config_typing.pyi `to `torch.utils`,  CC([utils] move `config_typing.pyi `to `torch.utils`)  CC([dynamo] promote skipfiles logging to verbose)  CC([dynamo] remove unused `OptimizeCtx` field  export)  CC([dynamo] Cache size calc for differing config)  CC([dynamo] guarded config) ,2023-11-17T05:37:20Z,open source Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/113929, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1966,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DTensor] Cached hash for `DTensorSpec`)ï¼Œ å†…å®¹æ˜¯ (  CC([DTensor] Reduced to one `isinstance` call in `is_shard`)  CC([DTensor] Cached hash for `DTensorSpec`)  CC([DTensor] Replaced neg dim normalization with assert in helper)  CC([DTensor] Used new placements for neg dim in `distribute_tensor`)  CC([DTensor] Ensured `grad_placements` was tuple)  CC([DTensor] Used new placements for neg dim in `from_local`)  CC([DTensor] Used new placements for neg dim in `redistribute`)  CC([DTensor] Made `_Partial`, `Replicate` frozen dataclasses) **Overview** Generally, I think we can try to freeze as many of these classes used in DTensor sharding propagation as possible so that we can cache hashes. This PR targets hashing `DTensorSpec`, which turns out to be relatively expensive. **Details** It looks like `tensor_meta` is only updated in `_wrap_output_spec_tensor_meta`, which only runs if the propagation was not cached: https://github.com/pytorch/pytorch/blob/ae94c7e491e22f58d3df66571c1a568e51d70acd/torch/distributed/_tensor/sharding_prop.pyL137 https://github.com/pytorch/pytorch/blob/ae94c7e491e22f58d3df66571c1a568e51d70acd/torch/distributed/_tensor/sharding_prop.pyL153 In that case, I think we can cache the hash for the `DTensorSpec` and only update it when one of the hashed attributes changes, which we only really expect to happen for `tensor_meta`. To ensure correctness, we need that all hashed attributes are immutable.  `DeviceMesh` caches its hash: https://github.com/pytorch/pytorch/blob/a9134fa99a8986adf478a12db2ea5729d24554db/torch/distributed/_device_mesh.pyL181  This PR makes each `Placement` a frozen `dataclass`, making them immutable (relying on the fact that they do not have references to any mutable objects).  `TensorMeta` is a `NamedTuple` of `torch.Size`, `Tupl)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[DTensor] Cached hash for `DTensorSpec`,"  CC([DTensor] Reduced to one `isinstance` call in `is_shard`)  CC([DTensor] Cached hash for `DTensorSpec`)  CC([DTensor] Replaced neg dim normalization with assert in helper)  CC([DTensor] Used new placements for neg dim in `distribute_tensor`)  CC([DTensor] Ensured `grad_placements` was tuple)  CC([DTensor] Used new placements for neg dim in `from_local`)  CC([DTensor] Used new placements for neg dim in `redistribute`)  CC([DTensor] Made `_Partial`, `Replicate` frozen dataclasses) **Overview** Generally, I think we can try to freeze as many of these classes used in DTensor sharding propagation as possible so that we can cache hashes. This PR targets hashing `DTensorSpec`, which turns out to be relatively expensive. **Details** It looks like `tensor_meta` is only updated in `_wrap_output_spec_tensor_meta`, which only runs if the propagation was not cached: https://github.com/pytorch/pytorch/blob/ae94c7e491e22f58d3df66571c1a568e51d70acd/torch/distributed/_tensor/sharding_prop.pyL137 https://github.com/pytorch/pytorch/blob/ae94c7e491e22f58d3df66571c1a568e51d70acd/torch/distributed/_tensor/sharding_prop.pyL153 In that case, I think we can cache the hash for the `DTensorSpec` and only update it when one of the hashed attributes changes, which we only really expect to happen for `tensor_meta`. To ensure correctness, we need that all hashed attributes are immutable.  `DeviceMesh` caches its hash: https://github.com/pytorch/pytorch/blob/a9134fa99a8986adf478a12db2ea5729d24554db/torch/distributed/_device_mesh.pyL181  This PR makes each `Placement` a frozen `dataclass`, making them immutable (relying on the fact that they do not have references to any mutable objects).  `TensorMeta` is a `NamedTuple` of `torch.Size`, `Tupl",2023-11-17T03:06:00Z,Merged ciflow/trunk release notes: distributed (dtensor),closed,0,3,https://github.com/pytorch/pytorch/issues/113915,  Update: I think I fixed it. I did not account for the case when a `DTensorSpec` could be constructed with `tensor_meta` directly specified (I incorrectly thought it would only be set after)., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1348,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault when tensor operation on GPU on gfx803)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When i run some tensor operation on a tensor previously moved to GPU, it crashes with SIGSEGV (By using C++ api, but also using the Python api).  I'm using AMD Ryzen with ROCm with following info:  The python script provided below also didn't work  Versions Traceback (most recent call last):   File ""/tmp/x/collect_env.py"", line 612, in      main()   File ""/tmp/x/collect_env.py"", line 595, in main     output = get_pretty_env_info()              ^^^^^^^^^^^^^^^^^^^^^   File ""/tmp/x/collect_env.py"", line 590, in get_pretty_env_info     return pretty_str(get_env_info())                       ^^^^^^^^^^^^^^   File ""/tmp/x/collect_env.py"", line 462, in get_env_info     nvidia_gpu_models=get_gpu_info(run_lambda),                       ^^^^^^^^^^^^^^^^^^^^^^^^   File ""/tmp/x/collect_env.py"", line 141, in get_gpu_info     ("" ({})"".format(torch.cuda.get_device_properties(0).gcnArchName) if torch.version.hip is not None else """")                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'gcnArchName' )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Segmentation fault when tensor operation on GPU on gfx803," ğŸ› Describe the bug When i run some tensor operation on a tensor previously moved to GPU, it crashes with SIGSEGV (By using C++ api, but also using the Python api).  I'm using AMD Ryzen with ROCm with following info:  The python script provided below also didn't work  Versions Traceback (most recent call last):   File ""/tmp/x/collect_env.py"", line 612, in      main()   File ""/tmp/x/collect_env.py"", line 595, in main     output = get_pretty_env_info()              ^^^^^^^^^^^^^^^^^^^^^   File ""/tmp/x/collect_env.py"", line 590, in get_pretty_env_info     return pretty_str(get_env_info())                       ^^^^^^^^^^^^^^   File ""/tmp/x/collect_env.py"", line 462, in get_env_info     nvidia_gpu_models=get_gpu_info(run_lambda),                       ^^^^^^^^^^^^^^^^^^^^^^^^   File ""/tmp/x/collect_env.py"", line 141, in get_gpu_info     ("" ({})"".format(torch.cuda.get_device_properties(0).gcnArchName) if torch.version.hip is not None else """")                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ AttributeError: 'torch._C._CudaDeviceProperties' object has no attribute 'gcnArchName' ",2023-11-16T21:44:01Z,module: crash module: rocm triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/113896,Note that the error inside the collect_env script is because your install is too old compared to the version of the script you're running.,Highpri to investigate why the binary is completely unusable.,"your gfx arch is gfx803. I don't think ROCm supports this target.  Currently, it requires at least gfx906.   Please check this page: https://rocm.docs.amd.com/en/latest/release/gpu_os_support.htmllinuxsupportedgpus.","> Note that the error inside the collect_env script is because your install is too old compared to the version of the script you're running. I disagree `collect_env` should function in a foolproof fashion, i.e. it should not expect any properties to be available there by default","Should this new portion of collect_env that looks for `torch.cuda.get_device_properties(0).gcnArchName` be protected by a torch version check, or perhaps a hasattr if that works here?  For newenough pytorch builds, gcnArchName will exist for both cuda and rocm builds.  What we didn't protect against was using an older pytorch build with the newer collect_env script.","After few days, i found, that this was a problem of the package build inside ArchLinux repo. Now it's working fine (even on gfx803, but i also found, that it's unsupported by newer ROCm). Anyway, thanks."
2013,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Native c10d_functional collectives on inductor + CUDAGraphTrees generate wrong results)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I've been testing the new native c10d functional collectives integration with inductor (https://github.com/pytorch/pytorch/pull/112439) as I was told by  it would fix some buffer reusability bugs I was hitting (which it does!) and its ability to codegen to different backends, but it seems like there's still some issue when combining them with the CUDAGraph integration in torch.compile(). Specifically, I'm getting different output logits when running the script in https://github.com/foundationmodelstack/foundationmodelstack/blob/cudagraphrepros/scripts/cudagraph_repro1.py (`cudagraphrepos` branch). For eager and regular compile, the outputs basically match, but on `reduceoverhead` they are completely different. I've been experimenting to see if I can better isolate the issue, and I've got it down to tensors being used before the allReduce and allGather operations are done with them. The issue doesn't always happen, which points to a race condition of some kind, and it only happens when the compute kernels and their launches are faster than nccl, as for example running the script with 2 processes might work 50% of the time, but making nccl slower by running 4 or 8 processes will cause the issue 100% of the time on an AWS p4de node. To run the repro script:  Other relevant pieces of code, including our Tensor Parallel implementation using the new native functional collectives are in https://github.com/foundationmodelstack/foundationmodelstack/blob/cudagraphrepros/fms/distributed/tensorparallel.pyL50L65, and the Llama model implementation is in https://github.com/foundationmodelstack/foundationmodelstack/blob/cudagraphrepros/fms/models/llama.py. I'll try to get a smaller repro, although the current)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Native c10d_functional collectives on inductor + CUDAGraphTrees generate wrong results," ğŸ› Describe the bug I've been testing the new native c10d functional collectives integration with inductor (https://github.com/pytorch/pytorch/pull/112439) as I was told by  it would fix some buffer reusability bugs I was hitting (which it does!) and its ability to codegen to different backends, but it seems like there's still some issue when combining them with the CUDAGraph integration in torch.compile(). Specifically, I'm getting different output logits when running the script in https://github.com/foundationmodelstack/foundationmodelstack/blob/cudagraphrepros/scripts/cudagraph_repro1.py (`cudagraphrepos` branch). For eager and regular compile, the outputs basically match, but on `reduceoverhead` they are completely different. I've been experimenting to see if I can better isolate the issue, and I've got it down to tensors being used before the allReduce and allGather operations are done with them. The issue doesn't always happen, which points to a race condition of some kind, and it only happens when the compute kernels and their launches are faster than nccl, as for example running the script with 2 processes might work 50% of the time, but making nccl slower by running 4 or 8 processes will cause the issue 100% of the time on an AWS p4de node. To run the repro script:  Other relevant pieces of code, including our Tensor Parallel implementation using the new native functional collectives are in https://github.com/foundationmodelstack/foundationmodelstack/blob/cudagraphrepros/fms/distributed/tensorparallel.pyL50L65, and the Llama model implementation is in https://github.com/foundationmodelstack/foundationmodelstack/blob/cudagraphrepros/fms/models/llama.py. I'll try to get a smaller repro, although the current",2023-11-16T21:24:14Z,triaged module: c10d bug oncall: pt2 module: inductor,closed,1,6,https://github.com/pytorch/pytorch/issues/113895,"Do we have random ops in the layers? If so, in genernal we don't guarantee eager output and compiled output to be the same. (You can set `torch._inductor.config.fallback_random = True`, and then the random op output will be the same.) > I've got it down to tensors being used before the allReduce and allGather operations are done with them Would be curious to learn how you narrowed it down too. E.g. does inductor + CUDAGraphTrees without Tensor Parallel work, but with it doesn't work?",also cc.  ,"I've taken out all the dropouts through eval(), so there shouldn't be any randomness at all. And yes, inductor+cudagraphs works on a single process consistently, and the issue only happens around 50% of the time on 2 processes, and always on 4",Thanks for the nice repro. Will take a look at the collective aspect of it soon,"The outputs only mismatch with cudagraphs, correct ? Taking a look next week.","Correct, compile only and eager return almost the same and consistently so, only cudagraphs+the new collectives expose this particular issue"
1955,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.fx export graph error )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I try to run StableDiffusionPipeline ,and get graph of StableDiffusionPipeline' model,but I meet some error ,I think it cause by pytorchã€‚ My code is as fllow  the error is  !aa20231116195524  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.088genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Byte Order:                         Little Endian Address sizes:                      48 bits physical, 48 bits virtual CPU(s):                             8 Online CPU(s) list:                07 Thread(s) per core:                 1 Core(s) per socket:                 8 Socket(s):                          1 NUMA node(s):                       1 Vendor ID:                          AuthenticAMD CPU family:                         25 Model:                              80 Model name:                         AMD Ryzen 7 PRO 5850U with Radeon Graphics Stepping:                           0 CPU MHz:                            1896.438 BogoMIPS:                           3792.87 Virtualization:      )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.fx export graph error ," ğŸ› Describe the bug I try to run StableDiffusionPipeline ,and get graph of StableDiffusionPipeline' model,but I meet some error ,I think it cause by pytorchã€‚ My code is as fllow  the error is  !aa20231116195524  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.088genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Byte Order:                         Little Endian Address sizes:                      48 bits physical, 48 bits virtual CPU(s):                             8 Online CPU(s) list:                07 Thread(s) per core:                 1 Core(s) per socket:                 8 Socket(s):                          1 NUMA node(s):                       1 Vendor ID:                          AuthenticAMD CPU family:                         25 Model:                              80 Model name:                         AMD Ryzen 7 PRO 5850U with Radeon Graphics Stepping:                           0 CPU MHz:                            1896.438 BogoMIPS:                           3792.87 Virtualization:      ",2023-11-16T12:00:31Z,triaged oncall: pt2 oncall: export,closed,0,3,https://github.com/pytorch/pytorch/issues/113860,,When I run your example script with transformers              4.32.0.dev0 and diffusers                 0.23.1 I get:  What versions of the libraries are you using?,"This runs without error for me today, and there's no activity for a while so I'm gonna claim this is fixed. Please reopen if you think there's still an issue here."
884,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable import following in MYPYNOFOLLOW (now MYPYINDUCTOR))ï¼Œ å†…å®¹æ˜¯ (  CC(Enable import following in MYPYNOFOLLOW (now MYPYINDUCTOR))  CC([dynamo] Make {guards,eval_frame}.py pass follow_imports typechecking)  CC(Make _inductor/fx_utils.py, _dynamo/utils.py pass follow_imports typechecking) Skipping importing some packages for now to make this change more tractable. For some reason, lintrunner on CI raises errors in all imported `.pyi` files, even though it doesn't on my local machine. The errors are all from missing generic types, as the MYPYINDUCTOR config has `disallow_any_generics` set. I have thus added `disableerrorcode` comments to the relevant files, though I fixed a few that were easy enough. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Enable import following in MYPYNOFOLLOW (now MYPYINDUCTOR),"  CC(Enable import following in MYPYNOFOLLOW (now MYPYINDUCTOR))  CC([dynamo] Make {guards,eval_frame}.py pass follow_imports typechecking)  CC(Make _inductor/fx_utils.py, _dynamo/utils.py pass follow_imports typechecking) Skipping importing some packages for now to make this change more tractable. For some reason, lintrunner on CI raises errors in all imported `.pyi` files, even though it doesn't on my local machine. The errors are all from missing generic types, as the MYPYINDUCTOR config has `disallow_any_generics` set. I have thus added `disableerrorcode` comments to the relevant files, though I fixed a few that were easy enough. ",2023-11-16T02:27:42Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/113830, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
235,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([experiment] gather coverage info)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[experiment] gather coverage info,Fixes ISSUE_NUMBER,2023-11-15T21:52:34Z,Stale topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/113807,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
659,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] Raise error when applying FSDP to `nn.ModuleList` or `nn.ModuleDict`)ï¼Œ å†…å®¹æ˜¯ (FSDP relies on `nn.Module.forward()` to allgather and free parameters. If an `nn.ModuleList` or `nn.ModuleDict` is wrapped with FSDP, then it will not run its allgather logic since `forward()` is not called. This leads to an annoying debugging experience to find that the parameters were not allgathered. Since the current design does not support this, we should raise an error at construction time. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[FSDP] Raise error when applying FSDP to `nn.ModuleList` or `nn.ModuleDict`,"FSDP relies on `nn.Module.forward()` to allgather and free parameters. If an `nn.ModuleList` or `nn.ModuleDict` is wrapped with FSDP, then it will not run its allgather logic since `forward()` is not called. This leads to an annoying debugging experience to find that the parameters were not allgathered. Since the current design does not support this, we should raise an error at construction time. ",2023-11-15T19:26:30Z,triaged module: fsdp,closed,0,6,https://github.com/pytorch/pytorch/issues/113794,"Is this still an issue? And do I understand correctly that it is only a problem if you organize your layers into `{ModuleList, ModuleDict}` containers _and_ you also want the FSDP units to be slices of these containers? Not seeing an issue with wrapping entire `{ModuleList, ModuleDict}` containers inside a single FSDP unit, but would like to know if that's also the case, thank you!",This is still an issue. I am surprised that you are not seeing a problem with wrapping the entire `ModuleList` or entire `ModuleDict` for the reason mentioned in the original issue description. I wonder how your parameters are getting allgathered then ğŸ¤” , Is it possible that you are wrapping the entire `ModuleList` _but also_ wrapping each individual element in the `ModuleList`?,"Ah, I believe I misunderstood the original issue.  I thought you were saying there was an issue if `FSDP` wraps any module which has any `{ModuleList, ModuleDict}` submodules, but I now see that the problem is likely with directly wrapping a `{ModuleList, ModuleDict}` instance alone. I wasn't seeing an issue with a wrapping policy like `ModuleWrapPolicy([Block])` where  But I believe I see correctly that `ModuleWrapPolicy([nn.ModuleList])` would be problematic.  The key difference being that `Block` has a `forward` while `{ModuleList, ModuleDict}` do not. Correct? **EDIT**: tested and verified that `ModuleWrapPolicy([nn.ModuleList])` errors out, while `ModuleWrapPolicy([Block])` does not."," Your understanding is correct! Since the `Block.forward()` runs, wrapping `Block` will allgather (and free) the parameters correctly.",will close this since we added a warning (choosing not to error in case of BC issues)
319,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Remove duplicate storage_offset equality test)ï¼Œ å†…å®¹æ˜¯ (  CC([BE] Remove duplicate storage_offset equality test) Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[BE] Remove duplicate storage_offset equality test,  CC([BE] Remove duplicate storage_offset equality test) Signedoffby: Edward Z. Yang ,2023-11-15T19:09:47Z,Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/113790, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1014,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.profiler Trace view in Tensorboard is displayed as empty on RoCm version of PyTorch)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, using the following script:  produces a nice trace (through tensorboard) with `torch==2.1.0+cu118`: !image But when the trace is captured with `2.1.0+rocm5.6` (same issue as well with `torch==2.2.0.dev20231106+rocm5.7)`, the trace can't load: !image Other features of the profiler (""GPU Kernel"", ""Operator"" tabs) are fine. A few logs are printed when loading into tensorboard in the RoCm case, as e.g. `W1115 14:49:31.006731 140355848750912 gpu_metrics_parser.py:180] pid '2' is not equal to args.device 'None' on event with ts '1700055803985211'`. I am thus wondering if torch.profiler supports RoCm. Thank you!  Versions On torch 2.1 + RoCm, this script gives:  On torch nightly + RoCm:  On torch 2.1 + CUDA 11.8:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.profiler Trace view in Tensorboard is displayed as empty on RoCm version of PyTorch," ğŸ› Describe the bug Hi, using the following script:  produces a nice trace (through tensorboard) with `torch==2.1.0+cu118`: !image But when the trace is captured with `2.1.0+rocm5.6` (same issue as well with `torch==2.2.0.dev20231106+rocm5.7)`, the trace can't load: !image Other features of the profiler (""GPU Kernel"", ""Operator"" tabs) are fine. A few logs are printed when loading into tensorboard in the RoCm case, as e.g. `W1115 14:49:31.006731 140355848750912 gpu_metrics_parser.py:180] pid '2' is not equal to args.device 'None' on event with ts '1700055803985211'`. I am thus wondering if torch.profiler supports RoCm. Thank you!  Versions On torch 2.1 + RoCm, this script gives:  On torch nightly + RoCm:  On torch 2.1 + CUDA 11.8:  ",2023-11-15T13:55:16Z,module: rocm low priority triaged module: tensorboard OSS contribution wanted,closed,0,9,https://github.com/pytorch/pytorch/issues/113760, do you mind sharing a tracefile generated? I wonder if this is caused by  CC(Wrongly formatted string in profiler_util.py),"tb_logs.zip Yes my bad, here it is!"," what's your version of Chrome? As ""it works on my machine""â„¢ :)  Also from python point of view it's a valid json, though `jq` complains that both files are invalid): ","I am using chromium `119.0.6045.123`, & on Firefox (with tensorboard) I use `119.0.1`. Maybe that's too old, I'll try to upgrade.","Oh, it works on chromium perfetto trace actually. The issue is only on tensorboard (`tensorboard==2.15.1`), with `http://localhost:6006/?darkMode=truepytorch_profiler` (both with Firefox & Chromium). This is low priority then.","Since the workaground works , mark this as low priority to identify the root cause.","This is what I saw on the backend when I click ""Trace"" on the frontend when running `tensorboard logdir=./log`.   Did you see this on your end too? Wondering whether this is related to my environment or it is the cause of the empty screen on Trace.", Yes I am also seeing these logs  however I am seeing them as well for the json trace generated using an nvidia version of pytorch. Can you reproduce the issue with tensorboard (using 2.15.1)?,We will close this issue since the workaround works and the issue related to tensorboard is not ROCm related.
2037,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot cast tensor to numpy array inside vmap due to ""Access data pointer of tensor that doesn't have storage"")ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug While trying to vectorize a function (working only on numpy array due to other dependencies) accross batch with vmap, I was unable to cast the input tensor of the function into numpy array, raising the following error > Traceback (most recent call last): >   File ""[YOUR PYTHON SCRIPT]"", line 63, in  >     vmap_func(a) >   File ""[ENVPATH]\lib\sitepackages\torch\_functorch\vmap.py"", line 434, in wrapped >     return _flat_vmap( >   File ""[ENVPATH]\lib\sitepackages\torch\_functorch\vmap.py"", line 39, in fn >     return f(*args, **kwargs) >   File ""[ENVPATH]\lib\sitepackages\torch\_functorch\vmap.py"", line 619, in _flat_vmap >     batched_outputs = func(*batched_inputs, **kwargs) >   File ""[YOUR PYTHON SCRIPT]"", line 57, in func >     a_np = tensor.numpy() > RuntimeError: Cannot access data pointer of Tensor that doesn't have storage >  The following code reproduces the error:   I've seen this error in other issues  ( CC(Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile`) for example) but the issue here doesnt seem to be related to the same problem I don't know if it's an expected behaviour (maybe the way vmap dataflow works), but if it's not the case, I hope this issue can help some people  Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.13  (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: True CUDA runtime ver)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Cannot cast tensor to numpy array inside vmap due to ""Access data pointer of tensor that doesn't have storage"""," ğŸ› Describe the bug While trying to vectorize a function (working only on numpy array due to other dependencies) accross batch with vmap, I was unable to cast the input tensor of the function into numpy array, raising the following error > Traceback (most recent call last): >   File ""[YOUR PYTHON SCRIPT]"", line 63, in  >     vmap_func(a) >   File ""[ENVPATH]\lib\sitepackages\torch\_functorch\vmap.py"", line 434, in wrapped >     return _flat_vmap( >   File ""[ENVPATH]\lib\sitepackages\torch\_functorch\vmap.py"", line 39, in fn >     return f(*args, **kwargs) >   File ""[ENVPATH]\lib\sitepackages\torch\_functorch\vmap.py"", line 619, in _flat_vmap >     batched_outputs = func(*batched_inputs, **kwargs) >   File ""[YOUR PYTHON SCRIPT]"", line 57, in func >     a_np = tensor.numpy() > RuntimeError: Cannot access data pointer of Tensor that doesn't have storage >  The following code reproduces the error:   I've seen this error in other issues  ( CC(Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile`) for example) but the issue here doesnt seem to be related to the same problem I don't know if it's an expected behaviour (maybe the way vmap dataflow works), but if it's not the case, I hope this issue can help some people  Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.13  (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: True CUDA runtime ver",2023-11-15T10:55:05Z,triaged module: numpy module: vmap,closed,1,10,https://github.com/pytorch/pytorch/issues/113751,"Yeah this is expected behavior  vmap only works on PyTorch operators. The underlying mechanism is essentially intercepting each PyTorch operation and translating it to the ""batched"" version. i.e. `vmap(torch.dot)(A, B)` => `torch.mm(A, B)` For perhaps obvious reasons, we cannot do the same interception on arbitrary thirdparty code (including numpy arrays). Depending on the use case, perhaps the numpy to torch translation layer could suffice? cc:  ","Doing supporting this for torch.compile was in our list of priorities, but how about this, I just tried it and it just works!  As a side note, I first tried to write a vectorised version of `np.convolve` in terms of `F.conv1d` and it's far from trivial! ","Thanks all of you for the response !  I'm glad to see there is a workaround, unfortunately torch.compile seems unavailable on Windows at the moment (referencing CC(PyTorch 2.0 not working on Windows) ), I have the following error when trying your code  I've looked at several other related issues and there don't seem to have any solution while torch.compile isn't supported on Windows; I'll be sure to monitor the other issue and try this as soon as Windows support is added !  Thanks again for the quick response","We can do a similar thing in eager mode using an internal API we have to implement the above (see below). But note that  **this API is internal and is not stable so it may break in the future!** Even more, `torch._numpy as np` was not designed to be used in eager mode, so **it may be quite slow on larger examples**. This is just a PoC showing that it can be done so you can play with it, but the only officially supported path that we commit to be efficient is the one using `torch.compile` described at  CC(Cannot cast tensor to numpy array inside vmap due to ""Access data pointer of tensor that doesn't have storage"")issuecomment1813213001 ","> Doing supporting this for torch.compile was in our list of priorities, but how about this, I just tried it and it just works! >  >  >  > As a side note, I first tried to write a vectorised version of `np.convolve` in terms of `F.conv1d` and it's far from trivial! >  > 't work for me in torch **2.1.0a0+fe05266**. Gives this error: `python mytests/proba5.py Traceback (most recent call last):   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 326, in _compile     out_code = transform_code_object(code, transform)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/bytecode_transformation.py"", line 530, in transform_code_object     transformations(instructions, code_options)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 300, in transform     tracer = InstructionTranslator(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/symbolic_convert.py"", line 1784, in __init__     self.symbolic_locals = collections.OrderedDict(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/symbolic_convert.py"", line 1787, in      VariableBuilder(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/variables/builder.py"", line 174, in __call__     return self._wrap(value).clone(**self.options())   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/variables/builder.py"", line 300, in _wrap     return type_dispatch(self, value)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/variables/builder.py"", line 748, in wrap_tensor     tensor_variable = wrap_fx_proxy(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/variables/builder.py"", line 865, in wrap_fx_proxy     return wrap_fx_proxy_cls(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/variables/builder.py"", line 925, in wrap_fx_proxy_cls     example_value = wrap_to_fake_tensor_and_record(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/variables/builder.py"", line 1062, in wrap_to_fake_tensor_and_record     fake_e = wrap_fake_exception(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/utils.py"", line 808, in wrap_fake_exception     return fn()   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/variables/builder.py"", line 1063, in      lambda: tx.fake_mode.from_tensor(   File ""/usr/local/lib/python3.8/distpackages/torch/_subclasses/fake_tensor.py"", line 1395, in from_tensor     return self.fake_tensor_converter(   File ""/usr/local/lib/python3.8/distpackages/torch/_subclasses/fake_tensor.py"", line 314, in __call__     return self.from_real_tensor(   File ""/usr/local/lib/python3.8/distpackages/torch/_subclasses/fake_tensor.py"", line 272, in from_real_tensor     out = self.meta_converter(   File ""/usr/local/lib/python3.8/distpackages/torch/_subclasses/meta_utils.py"", line 502, in __call__     r = self.meta_tensor(   File ""/usr/local/lib/python3.8/distpackages/torch/_subclasses/meta_utils.py"", line 381, in meta_tensor     s = t.untyped_storage() NotImplementedError: Cannot access storage of BatchedTensorImpl You can suppress this exception and fall back to eager by setting:     torch._dynamo.config.suppress_errors = True The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""mytests/proba5.py"", line 23, in      torch.testing.assert_close(vmap_convolve(a, b), poormans_vmap(a, b))   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/eval_frame.py"", line 254, in _fn     return fn(*args, **kwargs)   File ""mytests/proba5.py"", line 16, in vmap_convolve     return vmap(np_convolve)(x,y)   File ""mytests/proba5.py"", line 16, in      return vmap(np_convolve)(x,y)   File ""/usr/local/lib/python3.8/distpackages/torch/_functorch/vmap.py"", line 434, in wrapped     return _flat_vmap(   File ""/usr/local/lib/python3.8/distpackages/torch/_functorch/vmap.py"", line 39, in fn     return f(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_functorch/vmap.py"", line 619, in _flat_vmap     batched_outputs = func(*batched_inputs, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/eval_frame.py"", line 391, in catch_errors     return callback(frame, cache_size, hooks)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 406, in _convert_frame     result = inner_convert(frame, cache_size, hooks)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 105, in _fn     return fn(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 263, in _convert_frame_assert     return _compile(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/utils.py"", line 164, in time_wrapper     r = func(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 396, in _compile     raise InternalTorchDynamoError() from e torch._dynamo.exc.InternalTorchDynamoError` In **2.1.1** [20231214 12:58:19,630] [2/0] torch._dynamo.utils: [WARNING] Unsupported: meta converter nyi with fake tensor propagation. Traceback (most recent call last):   File ""mytests/proba5.py"", line 23, in      torch.testing.assert_close(vmap_convolve(a, b), poormans_vmap(a, b))   File ""/venv/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 328, in _fn     return fn(*args, **kwargs)   File ""mytests/proba5.py"", line 16, in vmap_convolve     return vmap(np_convolve)(x,y)   File ""/venv/lib/python3.8/sitepackages/torch/_functorch/apis.py"", line 188, in wrapped     return vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)   File ""/venv/lib/python3.8/sitepackages/torch/_functorch/vmap.py"", line 266, in vmap_impl     return _flat_vmap(   File ""/venv/lib/python3.8/sitepackages/torch/_functorch/vmap.py"", line 38, in fn     return f(*args, **kwargs)   File ""/venv/lib/python3.8/sitepackages/torch/_functorch/vmap.py"", line 379, in _flat_vmap     batched_outputs = func(*batched_inputs, **kwargs)   File ""mytests/proba5.py"", line 9, in np_convolve     x, y = x.numpy(), y.numpy() RuntimeError: Cannot access data pointer of Tensor that doesn't have storage  ",for this to work you should use either the  nightly or wait for the release of 2.2,"Closing this issue as expected behaviour. We leave an alternative using `torch.compile` at  CC(Cannot cast tensor to numpy array inside vmap due to ""Access data pointer of tensor that doesn't have storage"")issuecomment1813213001",> for this to work you should use either the nightly or wait for the release of 2.2 So in your example you were using nightly?,"compiled from source, but yeah, same thing",Thanks!
2019,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Regression: `capture_pre_autograd_graph` does not support empty args and kwargs only anymore)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug A regression about `capture_pre_autograd_graph` is found to be introduced by CC([RELAND] Disallow skipping dynamo) . `capture_pre_autograd_graph` is for graph capture and is now used for, for example, quantization with TorchInductor. It takes the original model and example inputs, positional args and/or kwargs. Before CC([RELAND] Disallow skipping dynamo), we can use kwargs only to capture the graph for models like BERT, i.e., use `()` for args. Example script (bert.py):  After this PR was merged, this practice will result in error:    Versions PyTorch version: 2.2.0a0+git8f5fead Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 11.1.01ubuntu1~20.04) 11.1.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.9.16 (main, May 15 2023, 23:46:34)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.11.027genericx86_64withglibc2.31 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   52 bits physical, 57 bits virtual CPU(s):                          128 Online CPU(s) list:             0127 Thread(s) per core:              2 Core(s) per socket:              32 Socket(s):                       2 NUMA node(s):                    2 Vendor ID:                       GenuineIntel CPU family:     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Regression: `capture_pre_autograd_graph` does not support empty args and kwargs only anymore," ğŸ› Describe the bug A regression about `capture_pre_autograd_graph` is found to be introduced by CC([RELAND] Disallow skipping dynamo) . `capture_pre_autograd_graph` is for graph capture and is now used for, for example, quantization with TorchInductor. It takes the original model and example inputs, positional args and/or kwargs. Before CC([RELAND] Disallow skipping dynamo), we can use kwargs only to capture the graph for models like BERT, i.e., use `()` for args. Example script (bert.py):  After this PR was merged, this practice will result in error:    Versions PyTorch version: 2.2.0a0+git8f5fead Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 11.1.01ubuntu1~20.04) 11.1.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.9.16 (main, May 15 2023, 23:46:34)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.11.027genericx86_64withglibc2.31 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   52 bits physical, 57 bits virtual CPU(s):                          128 Online CPU(s) list:             0127 Thread(s) per core:              2 Core(s) per socket:              32 Socket(s):                       2 NUMA node(s):                    2 Vendor ID:                       GenuineIntel CPU family:     ",2023-11-15T08:49:07Z,triaged oncall: pt2 export-triaged oncall: export,open,0,4,https://github.com/pytorch/pytorch/issues/113744," `_process_constraints` expects `example_inputs` is a list of tensors https://github.com/pytorch/pytorch/blob/c41a32a3bf04526d5f809237ef81fd1feccd2a8a/torch/_export/exported_program.pyL345L349  However, in the example provided, we still see dict after  `pytree.tree_flatten` https://github.com/pytorch/pytorch/blob/c41a32a3bf04526d5f809237ef81fd1feccd2a8a/torch/_export/__init__.pyL388 Here, `flat_args` is: `[{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]` which looks like doesn't meet the assumption of `_process_constraints`","Weiwen currently we don't provide compatibility guarantee on capture_pre_autograd_graph(), instead we will support the same thing through torch.export.export very soon and delete this function.", Thanks for the headsup. `capture_pre_autograd_graph` is used in the current quantization flow. Looks like we need to switch to `torch.export.export` in the future. Please let us know when it is ready. Thanks. CC fangintel ,no problem! 's actually working on this
467,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Plan for transformer module based ROCm)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Is there any plan for transformer module based ROCm? Thanks :) https://github.com/pytorch/pytorch/commit/94c9dbff22f18d319b0ce792f7a509d2c90b1a97  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Plan for transformer module based ROCm," ğŸš€ The feature, motivation and pitch Is there any plan for transformer module based ROCm? Thanks :) https://github.com/pytorch/pytorch/commit/94c9dbff22f18d319b0ce792f7a509d2c90b1a97  Alternatives _No response_  Additional context _No response_ ",2023-11-15T06:14:54Z,module: rocm triaged ciflow/rocm,open,0,0,https://github.com/pytorch/pytorch/issues/113741
2008,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`log_softmax` could be `2**124` to `2**1021` times more accurate on small outputs)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch As I understand it, PyTorch implements `log_softmax(x)` as `x  x.max()  (x  x.max()).exp().sum().log()`. However, when the largest value is much larger than the rest of the values (about 16 larger for float32, about 36 larger for float64), `log_softmax` returns `0` at the maximum value, when it could give a much more precise answer. Numerically, we have  (For float16, this is 7.2e4, for float64 2.2e16.) The value of `(x  x.max()).exp().sum()` is going to be at least 1, because the largest value of `x  x.max()` is 0.  1.19e7 is the unit of least precision for float32 around 1, but float32 can represent values as small as about 1.18e38 normally (`torch.finfo(torch.float32).smallest_normal`, `2**126 = 2**(2**7  2)`) or about 1.18e45 (`2**126 * 2**23`) subnormally.  So there's a lot of room for more precision using `log1p(x) = log(1 + x)` instead.  Consider:  For float64 and using `smallest_subnormal` instead of `smallest_normal`, the difference is even more pronounced:  This came up when a transformer I was training with crossentropy loss on a classification task had loss dominated by `torch.finfo(torch.float32).eps`.  Alternatives _No response_  Additional context Numbers in the title were generated with  giving For torch.float32, diff in supported input accuracy is 2**(101  15) = 2**86; diff in output accuracy is np.log2(1.1920928244535389e07)  np.log2(5.605193857299268e45) = 123.99999991400867 For torch.float64, diff in supported input accuracy is 2**(743  35) = 2**708; diff in output accuracy is np.log2(2.2204460492503128e16)  np.log2(1e323) = 1021.0 I originally posted this as a StackOverflow question. Companion SciPy issue: https://github.com/scipy/scipy/issues/19521 Compani)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`log_softmax` could be `2**124` to `2**1021` times more accurate on small outputs," ğŸš€ The feature, motivation and pitch As I understand it, PyTorch implements `log_softmax(x)` as `x  x.max()  (x  x.max()).exp().sum().log()`. However, when the largest value is much larger than the rest of the values (about 16 larger for float32, about 36 larger for float64), `log_softmax` returns `0` at the maximum value, when it could give a much more precise answer. Numerically, we have  (For float16, this is 7.2e4, for float64 2.2e16.) The value of `(x  x.max()).exp().sum()` is going to be at least 1, because the largest value of `x  x.max()` is 0.  1.19e7 is the unit of least precision for float32 around 1, but float32 can represent values as small as about 1.18e38 normally (`torch.finfo(torch.float32).smallest_normal`, `2**126 = 2**(2**7  2)`) or about 1.18e45 (`2**126 * 2**23`) subnormally.  So there's a lot of room for more precision using `log1p(x) = log(1 + x)` instead.  Consider:  For float64 and using `smallest_subnormal` instead of `smallest_normal`, the difference is even more pronounced:  This came up when a transformer I was training with crossentropy loss on a classification task had loss dominated by `torch.finfo(torch.float32).eps`.  Alternatives _No response_  Additional context Numbers in the title were generated with  giving For torch.float32, diff in supported input accuracy is 2**(101  15) = 2**86; diff in output accuracy is np.log2(1.1920928244535389e07)  np.log2(5.605193857299268e45) = 123.99999991400867 For torch.float64, diff in supported input accuracy is 2**(743  35) = 2**708; diff in output accuracy is np.log2(2.2204460492503128e16)  np.log2(1e323) = 1021.0 I originally posted this as a StackOverflow question. Companion SciPy issue: https://github.com/scipy/scipy/issues/19521 Compani",2023-11-14T22:48:19Z,module: nn triaged,open,2,1,https://github.com/pytorch/pytorch/issues/113708, 
384,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Do not specialize 0/1 eagerly for storage offset)ï¼Œ å†…å®¹æ˜¯ (  CC(Do not run CUDA lazy init if it is triggered with fake mode on.)  CC(Do not specialize 0/1 eagerly for storage offset) Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Do not specialize 0/1 eagerly for storage offset,  CC(Do not run CUDA lazy init if it is triggered with fake mode on.)  CC(Do not specialize 0/1 eagerly for storage offset) Signedoffby: Edward Z. Yang ,2023-11-14T21:46:19Z,Stale release notes: fx ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/113694,"In codegen, we expect that you have to do an extra offset computation to the data pointer before you do the actual compute. If the data pointer can be put in a register this should have no impact, but if we are dumb and inline it to all the spots, IDK, maybe CSE won't work?",  sounds good for runtime! I meant in terms of compilation time due to the extra dynamic objects that need tracking. But I guess that's not a concern,This is cursed lol,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NCCL PG] Enable storing nccl traces into storage and make it configurable)ï¼Œ å†…å®¹æ˜¯ (  CC([NCCL PG] Enable storing nccl traces into storage and make it configurable)  CC([NCCL PG] ADD a separate monitoring thread to ensure we collect debug info and check watchdog heartbeat))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[NCCL PG] Enable storing nccl traces into storage and make it configurable,  CC([NCCL PG] Enable storing nccl traces into storage and make it configurable)  CC([NCCL PG] ADD a separate monitoring thread to ensure we collect debug info and check watchdog heartbeat),2023-11-14T17:48:10Z,release notes: distributed (c10d),closed,0,1,https://github.com/pytorch/pytorch/issues/113662,ghstack messed up and PR should be updated in https://github.com/pytorch/pytorch/pull/113503.
345,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fixed pyi file for ReduceLROnPlateau)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(ReduceLROnPlateau is not a subclass of _LRScheduler)  Issue reappeared due to subclassing not present in stub file. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,fixed pyi file for ReduceLROnPlateau,Fixes CC(ReduceLROnPlateau is not a subclass of _LRScheduler)  Issue reappeared due to subclassing not present in stub file. ,2023-11-14T17:34:23Z,module: typing open source Merged ciflow/trunk topic: bug fixes release notes: optim,closed,0,12,https://github.com/pytorch/pytorch/issues/113659," label ""module: typing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,"Looks like this fix triggers > Error: Signature of ""step"" incompatible with supertype ""LRScheduler""","It seems that a bunch of the LRScheduler subclasses simply violate LSP.  Base class: `def step(self, epoch=None)`  `SequentialLR`: `def step(self)`  (missing epoch argument!)  `ChainedScheduler`: `def step(self)`  (missing epoch argument!)  `ReduceLROnPlateau`: `def step(self, metrics, epoch=None)`  extra mandatory `metrics` argument. For `SequentialLR` and  `ChainedScheduler`, it would make sense to simply pass the `epoch` argument through. (better open a separate issue?) For `ReduceLROnPlateau`, I'd suggest adding a `type: ignore[override]`, I don't really see a better solution right now.","Alternatively, one could set a default value like `metrics=float('nan')`.","type: ignore as necessary, IMO","Agreed with Ed for ReduceLROnPlateau, and yes for propagating epoch for the others.",I added the typeignore here and a put some educated guesses on how to pass epoch into a second pull request: https://github.com/pytorch/pytorch/pull/113675, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2031,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Output mismatch of torch.sub with gpu inductor compiled when applying distributive law of multiplication)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The distributive law of multiplication should always hold for torch.mul and torch.add. And (a+b)*(a+b)==a*(a+b)+b*(a+b). However, this law fails when applying distributive law of multiplication to the input of `torch.sub`, producing mismatch on the outputs of `torch.sub`. This mismatch is seen **only on cuda**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.2.036genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 SUPER Nvidia driver version: 525.147.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             20 Online CPU(s) list:                019 Vendor ID:                          GenuineIntel Model name:                         12th Gen Intel(R) Core(TM) i712700KF CPU family:                         6 Model:                              151 Thread(s) per core:                 2 Core(s) per socket:                 12 Socket(s):        )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Output mismatch of torch.sub with gpu inductor compiled when applying distributive law of multiplication," ğŸ› Describe the bug The distributive law of multiplication should always hold for torch.mul and torch.add. And (a+b)*(a+b)==a*(a+b)+b*(a+b). However, this law fails when applying distributive law of multiplication to the input of `torch.sub`, producing mismatch on the outputs of `torch.sub`. This mismatch is seen **only on cuda**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.2.036genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 SUPER Nvidia driver version: 525.147.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             20 Online CPU(s) list:                019 Vendor ID:                          GenuineIntel Model name:                         12th Gen Intel(R) Core(TM) i712700KF CPU family:                         6 Model:                              151 Thread(s) per core:                 2 Core(s) per socket:                 12 Socket(s):        ",2023-11-14T10:51:12Z,triaged oncall: pt2 topic: fuzzer,closed,0,3,https://github.com/pytorch/pytorch/issues/113639,Could you consolidate these issues into one issue please,"Related:  CC(Output mismatch of torch.sub with gpu inductor compiled when applying associative law of multiplication)  CC(Outputs of torch.mul abnormally change when swapping the input args of torch.mul)   , here are issues about multiplication.","Distributed law of multiplication does not apply to floating point arithmetic unfortunately, I'm going to close, please reopen if you feel fit. The maximum difference here is `Max absolute difference: 4.43668714e16` which is insignificant."
2030,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Output mismatch of torch.sub with gpu inductor compiled when applying associative law of multiplication)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The associative law of multiplication should always hold for torch.mul and torch.add. And (a*b)*c == a*(b*c). However, this law fails when applying associative law of multiplication to the input of `torch.sub`, producing mismatch on the outputs of `torch.sub`. This mismatch is seen **only on cuda**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.2.036genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 SUPER Nvidia driver version: 525.147.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             20 Online CPU(s) list:                019 Vendor ID:                          GenuineIntel Model name:                         12th Gen Intel(R) Core(TM) i712700KF CPU family:                         6 Model:                              151 Thread(s) per core:                 2 Core(s) per socket:                 12 Socket(s):                    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Output mismatch of torch.sub with gpu inductor compiled when applying associative law of multiplication," ğŸ› Describe the bug The associative law of multiplication should always hold for torch.mul and torch.add. And (a*b)*c == a*(b*c). However, this law fails when applying associative law of multiplication to the input of `torch.sub`, producing mismatch on the outputs of `torch.sub`. This mismatch is seen **only on cuda**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.2.036genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 SUPER Nvidia driver version: 525.147.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             20 Online CPU(s) list:                019 Vendor ID:                          GenuineIntel Model name:                         12th Gen Intel(R) Core(TM) i712700KF CPU family:                         6 Model:                              151 Thread(s) per core:                 2 Core(s) per socket:                 12 Socket(s):                    ",2023-11-14T10:42:49Z,triaged oncall: pt2 topic: fuzzer,closed,0,1,https://github.com/pytorch/pytorch/issues/113638,Using a 0 atol to compare values closing to 0 are too strict even if rtol=1. Also the associative law does not always apply for in practice for floating point operations. Just show an example:  I'll close the issue for now. Free free to reopen if more discussion is necessary.
713,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Make legacy type storage warning point to the caller)ï¼Œ å†…å®¹æ˜¯ (`` decorator adds another wrapper, so warning with default stacklevel (2) would  always point to the wrapper implementation rather than at callee. For example, before this change following code  will produce inactionable warning:  But after the change warning turns into:  Discovered while reading  CC(Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index`))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[BE] Make legacy type storage warning point to the caller,"`` decorator adds another wrapper, so warning with default stacklevel (2) would  always point to the wrapper implementation rather than at callee. For example, before this change following code  will produce inactionable warning:  But after the change warning turns into:  Discovered while reading  CC(Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index`)",2023-11-14T00:46:13Z,Merged ciflow/trunk release notes: python_frontend topic: bug fixes,closed,0,3,https://github.com/pytorch/pytorch/issues/113601,"I have a global ruff rule to ban warnings without an explicit stack level, but I haven't gotten around to enabling it.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1995,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault in RPC worker when DataLoader has num_workers > 0)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug RPC `worker_1` crashes with `Segmentation fault (core dumped)` error only when the `num_workers` is greater than 0. The code works fine for `num_workers` = 0.  Code Rank 0 code: ``` import os import torch import torch.distributed.rpc as rpc from torchvision import datasets as torch_datasets from torchvision import transforms def init_process():     """""" Initialize the distributed environment. """"""     os.environ['MASTER_ADDR'] = 'cnb005'     os.environ['MASTER_PORT'] = '29501'      rpc_backend_options = rpc.TensorPipeRpcBackendOptions(           init_method=f'tcp://localhost:52521',          num_worker_threads=32,   before init > libibverbs: Warning: couldn't load driver 'libvmw_pvrdmardmav34.so': libvmw_pvrdmardmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libmthcardmav34.so': libmthcardmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libcxgb4rdmav34.so': libcxgb4rdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libi40iwrdmav34.so': libi40iwrdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'librxerdmav34.so': librxerdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libhfi1verbsrdmav34.so': libhfi1verbsrdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libocrdmardmav34.so': libocrdmardmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libmlx4rdmav34.so': )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Segmentation fault in RPC worker when DataLoader has num_workers > 0," ğŸ› Describe the bug RPC `worker_1` crashes with `Segmentation fault (core dumped)` error only when the `num_workers` is greater than 0. The code works fine for `num_workers` = 0.  Code Rank 0 code: ``` import os import torch import torch.distributed.rpc as rpc from torchvision import datasets as torch_datasets from torchvision import transforms def init_process():     """""" Initialize the distributed environment. """"""     os.environ['MASTER_ADDR'] = 'cnb005'     os.environ['MASTER_PORT'] = '29501'      rpc_backend_options = rpc.TensorPipeRpcBackendOptions(           init_method=f'tcp://localhost:52521',          num_worker_threads=32,   before init > libibverbs: Warning: couldn't load driver 'libvmw_pvrdmardmav34.so': libvmw_pvrdmardmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libmthcardmav34.so': libmthcardmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libcxgb4rdmav34.so': libcxgb4rdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libi40iwrdmav34.so': libi40iwrdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'librxerdmav34.so': librxerdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libhfi1verbsrdmav34.so': libhfi1verbsrdmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libocrdmardmav34.so': libocrdmardmav34.so: cannot open shared object file: No such file or directory > libibverbs: Warning: couldn't load driver 'libmlx4rdmav34.so': ",2023-11-14T00:43:20Z,oncall: distributed module: dataloader triaged,open,1,1,https://github.com/pytorch/pytorch/issues/113600,"workers segmentation fault, this problem bothered me for about two months, I search the github and stackoverflow, and found, many Mac and GPU server also face the same problem. I nearly tried all the solutions avaibable over the Internet. In my case, because i deployed both environment on my laptop and desktop, the only difference was the CPU. I didn't even think about it, not until  some people said it maybe caused by the limited CPU RAM, of course not. As my laptop and desktop both contains 64G RAM, some Mac users has as many as 96 and 128G RAM also facing this error. This error will broken my training with in one epoch, even with setting workers=0. One night, I almost gave up trying, I never take a fully training on laptop with an eunuch version GPU, however, I just tried and it works, the training didn't stop all night. So I guess the only difference between the two computers, AMD R75800X and Intel i9. This morning, I make a test by replace the ubuntu system SSD to my old desktop (with a mainboard only support for 1 GPU), the training has lived for now.  During my searching, I rarely see people mention this problem with a AMD CPU. So if you have some hardware source, you can have try. As I only have one AMD CPU+mainboard, and a AMD laptop, I cannot do further test."
405,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Back out ""[Inductor] Fallback scatter when src dtype is bf16 (#113204)"")ï¼Œ å†…å®¹æ˜¯ (Summary: Revert due to Llama 7b performance regression on Mi250x (83tok/s > 79.5tok/s, ~4% regression) Test Plan: CI Differential Revision: D51287379 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"Back out ""[Inductor] Fallback scatter when src dtype is bf16 (#113204)""","Summary: Revert due to Llama 7b performance regression on Mi250x (83tok/s > 79.5tok/s, ~4% regression) Test Plan: CI Differential Revision: D51287379 ",2023-11-14T00:36:15Z,fb-exported topic: not user facing module: inductor ciflow/inductor,closed,0,19,https://github.com/pytorch/pytorch/issues/113599,This pull request was **exported** from Phabricator. Differential Revision: D51287379,Reverting CC([Inductor] Fallback scatter when src dtype is bf16), ," label ""topic: not user facing""","I'm on PTO so I'll lean on  and  here. We can revert first and discover what happened later but just to be sure, are we confident that this PR is the cause since `tl.atomic_add` does not work at all with bf16. I don't see any data on github or phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D51287379,"> I'm on PTO so I'll lean on  and  here. We can revert first and discover what happened later but just to be sure, are we confident that this PR is the cause since `tl.atomic_add` does not work at all with bf16. I don't see any data on github or phabricator. Yes, I'm sure this is the cause of the regression. My guess is that this fallback is not required for AMD GPUs.",> Please share instructions to measure so that I can check before relanding I'll ping you internally.,This pull request was **exported** from Phabricator. Differential Revision: D51287379,This pull request was **exported** from Phabricator. Differential Revision: D51287379,Can you just skip the fallback if the cuda device is amd then ?,Unfortunately I have no bandwidth whatsoever to forward fix (I know it would be very simple but I just don't have the time to verify perf numbers are ok on amd/nvidia) and I need this to land by eod so will stick to a revert.,"> Can you just skip the fallback if the cuda device is amd then ? Do you know how to check whether the cuda device is amd or NVIDIA? Also when I restricted this to device == cuda, some tests failed. So I assume fallback is required for more than just NVIDIA ", : https://github.com/pytorch/pytorch/blob/main/torch/_inductor/graph.pyL264,Since this didn't land yesterday  can we make the amd change ? we dont usually do correctness reverts for smallish perf regressions.,"I can put a pr up On Tue, Nov 14, 2023 at 7:11â€¯AM eellison ***@***.***> wrote: > Since this didn't land yesterday  can we make the amd change ? we dont > usually do correctness reverts for smallish perf regressions. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >",https://github.com/pytorch/pytorch/pull/113655,"Sure, we're blocked landing this pr internally for failing tests so if you have a forward fix ready to go let's land that.",No longer going with this pr  closing.
765,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add an API to DDP for dynamically updating the underlying process group.)ï¼Œ å†…å®¹æ˜¯ ( Motivation If we would like to reinitialize DDP with a different PG with `torch.compile`, we need to do the following:  This results in recompilation of the entire model and is very expensive. Since the only thing we need to update is the PG, we should be able to do this without having to compile the model again.  Proposal As a result, in this PR I've introduced an `_update_process_group` API which can dynamically update the underlying ProcessGroup used by DDP without needing to reinitialize DDP again. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add an API to DDP for dynamically updating the underlying process group.," Motivation If we would like to reinitialize DDP with a different PG with `torch.compile`, we need to do the following:  This results in recompilation of the entire model and is very expensive. Since the only thing we need to update is the PG, we should be able to do this without having to compile the model again.  Proposal As a result, in this PR I've introduced an `_update_process_group` API which can dynamically update the underlying ProcessGroup used by DDP without needing to reinitialize DDP again. ",2023-11-13T19:51:11Z,oncall: distributed triaged open source Merged ciflow/binaries ciflow/trunk release notes: distributed (c10d) ciflow/periodic ciflow/slow,closed,0,10,https://github.com/pytorch/pytorch/issues/113580,this PR has a lot of reformatting which I suggest we exclude.,> this PR has a lot of reformatting which I suggest we exclude.  This came from `lintrunner a` automatically. Not sure if lint will fail if I don't apply those changes.,"> > this PR has a lot of reformatting which I suggest we exclude. >  >  This came from `lintrunner a` automatically. Not sure if lint will fail if I don't apply those changes. actually while i hate to be pedantic i think in this case since there are so many lines of code affected, it'd be worth splitting the PR into a stack where the first PR just applies lintrunner and the second PR just has your 'real' changes.",varma  any issue with this PR?  (I can do a more detailed review if you dont want to but any objection on the addition of these APIs in principle?),"> actually while i hate to be pedantic i think in this case since there are so many lines of code affected, it'd be worth splitting the PR into a stack where the first PR just applies lintrunner and the second PR just has your 'real' changes. So I had to run `lintrunner a` since `make lint` was failing locally for me. If I remove the formatting changes won't lint fail on this PR and as a result block merge?","> > actually while i hate to be pedantic i think in this case since there are so many lines of code affected, it'd be worth splitting the PR into a stack where the first PR just applies lintrunner and the second PR just has your 'real' changes. >  > So I had to run `lintrunner a` since `make lint` was failing locally for me. If I remove the formatting changes won't lint fail on this PR and as a result block merge? my suggestion was to create a 'stack' where your first pr does the lint changes and then your second PR which is 'on top' does the changes.  (are you familiar with ghstack?  `pip install ghstack` and instructions on its github page)",> my suggestion was to create a 'stack' where your first pr does the lint changes and then your second PR which is 'on top' does the changes. (are you familiar with ghstack? pip install ghstack and instructions on its github page) Looks like maybe there was something broken in my local setup causing the linter to change files. Removing the formatting still passes CI though, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,Forgot to leave the comments. The PR looks good to me. 
710,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Deepcopying an exported model changes numerics for MobileNetV2)ï¼Œ å†…å®¹æ˜¯ (Below is a minimally reproducible example for MobileNetV2. I would expect the output to remain exactly the same even after deepcopy. I was unable to reproduce this for a simple `convbn` model, however, so maybe there are specific ops in MobileNetV2 that are causing this.  Output:  Note: I needed to add the prepare step in this example because otherwise the above snippet would fail in the deepcopy step with the following unrelated error during symbolic trace:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Deepcopying an exported model changes numerics for MobileNetV2,"Below is a minimally reproducible example for MobileNetV2. I would expect the output to remain exactly the same even after deepcopy. I was unable to reproduce this for a simple `convbn` model, however, so maybe there are specific ops in MobileNetV2 that are causing this.  Output:  Note: I needed to add the prepare step in this example because otherwise the above snippet would fail in the deepcopy step with the following unrelated error during symbolic trace:  ",2023-11-13T18:24:22Z,high priority triage review triaged export-triage-review oncall: export,open,0,1,https://github.com/pytorch/pytorch/issues/113572,"I'm not sure if it's an issue with deepcopying? After deepcopying the state dict seems to be the same, but it seems like after running the program and the copied program with the example inputs, the state dicts differ only in the last observer:  test code: "
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset12CPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset12CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). Same as  CC(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU)))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset12CPU),Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset12CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). Same as  CC(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU)),2023-11-13T04:18:52Z,module: onnx triaged skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/113542,The commit has been reverted
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset11CPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset11CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). Same as  CC(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU)))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset11CPU),Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset11CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). Same as  CC(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU)),2023-11-13T00:49:39Z,module: onnx triaged skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/113540,The commit has been reverted
890,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update TensorBase.to()'s' signature; create {guards,compiled_autograd}.pyi)ï¼Œ å†…å®¹æ˜¯ (  CC([dynamo] Make {mutation_guard,symbolic_convert,side_effects}.py pass follow_imports typechecking)  CC([inductor] Make {cudagraph_trees,decomposition,post_grad}.py pass follow_imports typechecking)  CC([inductor] Make {freezing,ir}.py pass followimports typechecking)  CC(Update TensorBase.to()'s' signature; create {guards,compiled_autograd}.pyi)  CC(Make {Tracing,Compile}Context.get() return nonoptional type)  CC([dynamo] Initialize tensor_weakref_to_sizes_strides with a weak dict) I had to explicitly import submodules in torch/_C/_dynamo/__init__.pyi because mypy doesn't seem to understand empty `__init__.py[i]` files. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Update TensorBase.to()'s' signature; create {guards,compiled_autograd}.pyi","  CC([dynamo] Make {mutation_guard,symbolic_convert,side_effects}.py pass follow_imports typechecking)  CC([inductor] Make {cudagraph_trees,decomposition,post_grad}.py pass follow_imports typechecking)  CC([inductor] Make {freezing,ir}.py pass followimports typechecking)  CC(Update TensorBase.to()'s' signature; create {guards,compiled_autograd}.pyi)  CC(Make {Tracing,Compile}Context.get() return nonoptional type)  CC([dynamo] Initialize tensor_weakref_to_sizes_strides with a weak dict) I had to explicitly import submodules in torch/_C/_dynamo/__init__.pyi because mypy doesn't seem to understand empty `__init__.py[i]` files. ",2023-11-12T15:34:36Z,Merged ciflow/trunk topic: not user facing module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/113536, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
651,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset10CPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset10CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). Same as  CC(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU)))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset10CPU),Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=undefined&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset10CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). Same as  CC(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU)),2023-11-12T09:02:47Z,module: onnx triaged skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/113529,The commit has been reverted
826,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=linuxfocalpy3.8clang10onnx%20%2F%20test%20(default%2C%202%2C%202%2C%20linux.2xlarge)&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset9CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). This is failing in trunk after https://github.com/pytorch/pytorch/pull/113404, for example https://hud.pytorch.org/pytorch/pytorch/commit/39ca5a3226331428465a84d53d5b50dfb4406cfe.  The tests didn't fail on PR.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,DISABLED test_output_match_atan2_cpu_float16 (__main__.TestOnnxModelOutputConsistency_opset9CPU),"Platforms: linux This test was disabled because it is failing on main branch (recent examples&jobName=linuxfocalpy3.8clang10onnx%20%2F%20test%20(default%2C%202%2C%202%2C%20linux.2xlarge)&failureCaptures=%5B%22onnx%2Ftest_op_consistency.py%3A%3ATestOnnxModelOutputConsistency_opset9CPU%3A%3Atest_output_match_atan2_cpu_float16%22%5D)). This is failing in trunk after https://github.com/pytorch/pytorch/pull/113404, for example https://hud.pytorch.org/pytorch/pytorch/commit/39ca5a3226331428465a84d53d5b50dfb4406cfe.  The tests didn't fail on PR.  ",2023-11-12T01:04:52Z,module: onnx triaged skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/113526,The commit has been reverted
782,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NCCL PG] Enable storing nccl traces into storage and make it configurable)ï¼Œ å†…å®¹æ˜¯ (  CC([NCCL PG] Add dumping flight recorder in the NCCL watchdog timeout)  CC([NCCL PG] Enable storing nccl traces into storage and make it configurable) This PR is to enable the store of NCCL flight recorder to storage and make it configurable by letting users register their own way of storing the debug info. We will then provide users a script to offline parse and process the dumped blobs. One thing, this PR is not trying to resolve is to decide where to dump the debug info. I will send a followup PR to address that. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[NCCL PG] Enable storing nccl traces into storage and make it configurable,"  CC([NCCL PG] Add dumping flight recorder in the NCCL watchdog timeout)  CC([NCCL PG] Enable storing nccl traces into storage and make it configurable) This PR is to enable the store of NCCL flight recorder to storage and make it configurable by letting users register their own way of storing the debug info. We will then provide users a script to offline parse and process the dumped blobs. One thing, this PR is not trying to resolve is to decide where to dump the debug info. I will send a followup PR to address that. ",2023-11-11T00:38:56Z,oncall: distributed Merged ciflow/trunk release notes: distributed (c10d),closed,0,5,https://github.com/pytorch/pytorch/issues/113503, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 1, 5, linux.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxfocalcuda12.1py3.10gcc9 / test (default, 1, 5, linux.4xlarge.nvidia.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
419,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix TypedStorage warning for weights_only)ï¼Œ å†…å®¹æ˜¯ (Partially addresses  CC(Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index`))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix TypedStorage warning for weights_only,Partially addresses  CC(Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index`),2023-11-10T23:28:15Z,,closed,1,2,https://github.com/pytorch/pytorch/issues/113498," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.",This PR is not in the right direction.  may have a proper fix soon.
761,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prune more unnecessary includes from CUDA transformers)ï¼Œ å†…å®¹æ˜¯ (  CC(Prune more unnecessary includes from CUDA transformers) These kernels are incredibly slow to compile and for the most part are completely independant of ATen/c10 yet they still end up including half of `c10` transitively through `CUDAGeneratorImpl.h` and `CUDAContext.h`. This trims the fat so `mem_eff_attention` doesn't depend on ATen/c10 at all, and `flash_attn` now only depends on `PhiloxUtils.cuh` (split out from `CUDAGeneratorImpl.h`) and `CUDAContextLight.h` which doesn't transitively include `TensorImpl.h`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Prune more unnecessary includes from CUDA transformers,"  CC(Prune more unnecessary includes from CUDA transformers) These kernels are incredibly slow to compile and for the most part are completely independant of ATen/c10 yet they still end up including half of `c10` transitively through `CUDAGeneratorImpl.h` and `CUDAContext.h`. This trims the fat so `mem_eff_attention` doesn't depend on ATen/c10 at all, and `flash_attn` now only depends on `PhiloxUtils.cuh` (split out from `CUDAGeneratorImpl.h`) and `CUDAContextLight.h` which doesn't transitively include `TensorImpl.h`.",2023-11-10T22:24:34Z,open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/113493, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1134,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Refactor NestedTensor subclass to remove ragged_size from constructor)ï¼Œ å†…å®¹æ˜¯ (  CC(Support for torch.nested.as_nested_tensor(t))  CC(Proper view support for jagged layout NestedTensor)  CC(Refactor NestedTensor subclass to remove ragged_size from constructor) This PR removes the need for passing `ragged_size` into the `NestedTensor` constructor. This was an artifact of fakeification, where sometimes we needed the NT to have a symbolic singleton symint shape for the ragged dimension. The new way of achieving this is to also store mappings between fake / functional tensors > symbolic symints in the ragged structure registry. Now the `NestedTensor` constructor can just query this registry for the `ragged_size`. Old: `NestedTensor(values, offsets, *, ragged_size=None, **kwargs)` New: `NestedTensor(values, offsets, **kwargs)` This makes it possible to have a `_nested_view_from_values_offsets(values, offsets)` without needing to pass a `ragged_size`. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Refactor NestedTensor subclass to remove ragged_size from constructor,"  CC(Support for torch.nested.as_nested_tensor(t))  CC(Proper view support for jagged layout NestedTensor)  CC(Refactor NestedTensor subclass to remove ragged_size from constructor) This PR removes the need for passing `ragged_size` into the `NestedTensor` constructor. This was an artifact of fakeification, where sometimes we needed the NT to have a symbolic singleton symint shape for the ragged dimension. The new way of achieving this is to also store mappings between fake / functional tensors > symbolic symints in the ragged structure registry. Now the `NestedTensor` constructor can just query this registry for the `ragged_size`. Old: `NestedTensor(values, offsets, *, ragged_size=None, **kwargs)` New: `NestedTensor(values, offsets, **kwargs)` This makes it possible to have a `_nested_view_from_values_offsets(values, offsets)` without needing to pass a `ragged_size`. ",2023-11-10T22:12:00Z,Merged ciflow/trunk topic: not user facing module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/113491, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 2ef8da30bb1acaa492f2a084d8cc7b23ade106fd` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
404,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Better error message when applying interpolation on non-4D tensors)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Using torch.nn.functional.interpolate with mode='bilinear' raises ValueError: Antialias option is only supported for bilinear and bicubic modes) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Better error message when applying interpolation on non-4D tensors,Fixes CC(Using torch.nn.functional.interpolate with mode='bilinear' raises ValueError: Antialias option is only supported for bilinear and bicubic modes) ,2023-11-10T17:44:01Z,open source Merged ciflow/trunk release notes: nn topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/113459,The committers listed above are authorized under a signed CLA.:white_check_mark: login: giaco5988 / name: Giacomo  (43457c23014754ccdaf04f46b5c47096e401fa67), release notes: nn," label ""topic: not user facing"""," label ""release notes: nn""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1966,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(problem pip install -r requirements.txt)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue hi problem pip install r requirements.txt No matter what I do, auto gpt is not installed  File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\sitepackages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in            main()         File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\sitepackages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main           json_out['return_val'] = hook(**hook_input['kwargs'])                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\sitepackages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel           return hook(config_settings)                  ^^^^^^^^^^^^^^^^^^^^^         File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\build_meta.py"", line 355, in get_requires_for_build_wheel           return self._get_build_requires(config_settings, requirements=['wheel'])                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\build_meta.py"", line 325, in _get_build_requires           self.run_setup()         File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\build_meta.py"", line 341, in run_setup           exec(code, locals())         File """", line 288, in          File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\__init__.py"", line 103, in setup           return distutils.core.setup(**attrs)                  ^^^^)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,problem pip install -r requirements.txt," ğŸ“š The doc issue hi problem pip install r requirements.txt No matter what I do, auto gpt is not installed  File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\sitepackages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 353, in            main()         File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\sitepackages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 335, in main           json_out['return_val'] = hook(**hook_input['kwargs'])                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""C:\Users\windo\AppData\Local\Programs\Python\Python312\Lib\sitepackages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py"", line 118, in get_requires_for_build_wheel           return hook(config_settings)                  ^^^^^^^^^^^^^^^^^^^^^         File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\build_meta.py"", line 355, in get_requires_for_build_wheel           return self._get_build_requires(config_settings, requirements=['wheel'])                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^         File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\build_meta.py"", line 325, in _get_build_requires           self.run_setup()         File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\build_meta.py"", line 341, in run_setup           exec(code, locals())         File """", line 288, in          File ""C:\Users\windo\AppData\Local\Temp\pipbuildenvzhw3nbt6\overlay\Lib\sitepackages\setuptools\__init__.py"", line 103, in setup           return distutils.core.setup(**attrs)                  ^^^^",2023-11-10T15:29:26Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/113448,Python3.12 is yet not supported by PyTorch. Closing as duplicate of  CC(Pytorch for Python 3.12 not available),> Python3.12 is yet not supported by PyTorch. Closing as duplicate of CC(Pytorch for Python 3.12 not available) Which version should I install?, PyTorch2.1 release is build for Python version from 3.8 to 3.11
769,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor cutlass backend] Shuffling selection of GEMM ops for autotuning before applying limit)ï¼Œ å†…å®¹æ˜¯ (  CC([Inductor cutlass backend] Shuffling selection of GEMM ops for autotuning before applying limit)  CC([inductor max autotune] Detailed autotuning result logs ( machinereadable ))  CC([Inductor CUTLASS backend] Cutlass EVT Addmm support for Inductor Cutlass backend)  CC([Inductor cutlass GEMM backend] Fix for operand memory layout change between autotuning and CUTLASSGEMMTemplate.render)  CC([Inductor] Fix debug_str method of FusedSchedulerNode)  CC([Cutlass 3.3 submodule upgrade]) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Inductor cutlass backend] Shuffling selection of GEMM ops for autotuning before applying limit,  CC([Inductor cutlass backend] Shuffling selection of GEMM ops for autotuning before applying limit)  CC([inductor max autotune] Detailed autotuning result logs ( machinereadable ))  CC([Inductor CUTLASS backend] Cutlass EVT Addmm support for Inductor Cutlass backend)  CC([Inductor cutlass GEMM backend] Fix for operand memory layout change between autotuning and CUTLASSGEMMTemplate.render)  CC([Inductor] Fix debug_str method of FusedSchedulerNode)  CC([Cutlass 3.3 submodule upgrade]) ,2023-11-10T12:07:06Z,module: inductor ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/113442
477,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Integrate Llama2 recipe on Intel platforms to llama-recipes)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue This request is to integrate Intel recipes of running Llama2 with Intel optimizations on Intel platforms to https://github.com/facebookresearch/llamarecipes/tree/main  Suggest a potential alternative/fix _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Integrate Llama2 recipe on Intel platforms to llama-recipes, ğŸ“š The doc issue This request is to integrate Intel recipes of running Llama2 with Intel optimizations on Intel platforms to https://github.com/facebookresearch/llamarecipes/tree/main  Suggest a potential alternative/fix _No response_,2023-11-10T04:56:29Z,docathon-h2-2023,closed,0,1,https://github.com/pytorch/pytorch/issues/113430,Moving this here https://github.com/facebookresearch/llamarecipes/issues/293
381,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add function to materialize COW storages)ï¼Œ å†…å®¹æ˜¯ (Part of CC(Implement Copyonwrite (COW) tensors)   CC(Add `torch._lazy_clone` to create COW tensors)  CC(Add function to materialize COW storages) : D51466282)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add function to materialize COW storages,Part of CC(Implement Copyonwrite (COW) tensors)   CC(Add `torch._lazy_clone` to create COW tensors)  CC(Add function to materialize COW storages) : D51466282,2023-11-09T22:58:17Z,open source Merged Reverted ciflow/trunk topic: not user facing ciflow/mps module: inductor,closed,0,21,https://github.com/pytorch/pytorch/issues/113396, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""Break internal build"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.," sorry, I have to revert your PR. It leads to internal failures. The build failure is:  Using `virtual void copy_data(void* dest, const void* src, std::size_t count) const = 0;` is harder to roll out without updating the usage of Allocator class internally. , could you please help with this?"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," after resolving the internal build failure, I see this ASAN error ",", the ASAN error should be fixed now, though I wasn't able to get an ASAN build working in my environment to verify for sure. The problem was that `testing::StrEq` is a bad way to test that storages have the same data, since storages are not nullterminated"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Hey Kurt, I'm still running into weird internal fbcode build problems from having the cow/ subdirectory. I could probably, in principle, figure it out, but it seems a lot simpler to just get rid of the cow/ folder. Do you think you could do that?","Sure thing, I'll do that","Also, I'm traveling to China until the new year, so you'll need someone else to help out with the fbcode side. Maybe  is willing to help","I've removed the `c10/core/impl/cow` directory and moved its contents to `c10/core/impl`, so hopefully those internal errors are gone now"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Hi , I was wondering if you might have time to take a look at this PR and let me know if the internal errors are fixed or not?","Hey , could you let me know if the internal errors are fixed?",I'm baaack,Testing on https://www.internalfb.com/diff/D52610522 because I can no longer ghimport this PR smh,Looks like this is good. I'm going to land this separately because ghimport is broken and I have to keep redoing the fbcode side changes which is a pain.,Closing this since it was moved to CC(Add function to materialize COW storages)
507,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile() results in inflexible model with mistralai/Mistral-7B-v0.1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When applying torch.compile() to HF model `mistralai/Mistral7Bv0.1`, the resulting model is inflexible in sequence length. The repro code and error message is below:   Versions torch==2.2.0.dev20231108+cu118 transformers==4.34.1 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile() results in inflexible model with mistralai/Mistral-7B-v0.1," ğŸ› Describe the bug When applying torch.compile() to HF model `mistralai/Mistral7Bv0.1`, the resulting model is inflexible in sequence length. The repro code and error message is below:   Versions torch==2.2.0.dev20231108+cu118 transformers==4.34.1 ",2023-11-09T21:52:39Z,oncall: pt2 module: dynamic shapes,closed,1,10,https://github.com/pytorch/pytorch/issues/113393, ,,"Could be an issue with automatic_dynamic. Could you try `torch.compile(mode, dynamic=True)`?",Marking as high pri since this is a very popular model,High pri is for bugs.  please try w/ automatic dynamic,I think this is huggingfaceonly and doesn't impact the original impl. But hugging face is probably more popular.,"Repro:  Non repros: ` For some reason, it needs to be exactly `(1, 1, x, y)`. ","I think this has to do with guarding on more than one symbolic Eq exprs. Somehow we cannot fold `Not(And(Eq(a, b), Eq(c, d)))` to `Or(Ne(a, b), Ne(c, d))`. But the weird thing is that we don't encounter this when we dont lead with two other numbers.",The root cause is that `1 and True` can evaluate to `1` instead of boolean. This confuses `symbolic_shapes` to take the wrong code path., please confirm if the issue is solved for you on latest main
606,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix `UntypedStorage.resize_` to keep same CUDA device index)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(`Storage.resize_()` moves storage to current device)   BCbreaking note Before this PR, `UntypedStorage.resize_()` used to move CUDA storage data to the current CUDA device index, `torch.cuda.current_device()`. After this PR, `UntypedStorage.resize_()` keeps the data on the same device index that it was on before, regardless of the current device index.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix `UntypedStorage.resize_` to keep same CUDA device index,"Fixes CC(`Storage.resize_()` moves storage to current device)   BCbreaking note Before this PR, `UntypedStorage.resize_()` used to move CUDA storage data to the current CUDA device index, `torch.cuda.current_device()`. After this PR, `UntypedStorage.resize_()` keeps the data on the same device index that it was on before, regardless of the current device index.",2023-11-09T20:16:20Z,module: cuda open source Merged ciflow/trunk release notes: cuda topic: bc breaking,closed,0,10,https://github.com/pytorch/pytorch/issues/113386, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,  worth a BC breaking note?,Yeah probably a good idea. I added a note,"Hi    Sorry to discuss this on the closed PR. I can open another issue if you think it's a better place to discuss it. Please let me know. Could you explain a little bit that why is `  c10::cuda::CUDAGuard guard(device.index());` needed here We are looking into some performance issue and found quite some device Set/Get operation pairs which are introduced by device guards which are scattered all over the code path. For instance,  it seems `resize_bytes_cuda` here assumes `allocator>allocate(size_bytes);` should be guarded. Also,  resize_impl_cuda_ inserts device guard by default.  Along the path, `resize_impl_cuda_` (first device gurard) > `maybe_resize_storage_cuda`  > `resize_bytes_cuda` (nested device guard)`  Is there any pricinple on device guard usage? It seems 1. All functions declared with TORCH_CUDA_CPP_API should be guarded? 2. It's always the caller's responsibility to guard the callee. Thus nested device guard is inevitable since the callee has no idea about what the caller did and might do the device guard again. And last, if `resize_bytes_cuda` is guarded  anyway, is it necessary to have a device guard here ? Also do you think it's necessary to handle/optimize nested device guard?  Tensorflow(now XLA) uses TLS to manage the depth of nested guards which can save some calls to device layer. Thanks Kevin","Hey , I'm not sure about all the answers to the questions you asked, but below are my responses to some of them. > Could you explain a little bit that why is   `c10::cuda::CUDAGuard guard(device.index());` needed here The allocator will use whichever device index is currently set. We need to make sure that the device index is set to the same device index that the data of the `StorageImpl` given to `resize_bytes_cuda` is currently allocated onotherwise, the storage's data may switch to a different device. We want to keep the storage on whichever device the user originally said they wanted it to be on. > Along the path, `resize_impl_cuda_` (first device gurard) > `maybe_resize_storage_cuda` > `resize_bytes_cuda` (nested device guard)` If the guard in `resize_bytes_cuda` is potentially redundant within a call to `resize_impl_cuda_(..., device_guard = true)`, that is not the only place where `resize_bytes_cuda` is called. It is also called here and here. I don't think either of those call sites sets the device guard beforehand. > It's always the caller's responsibility to guard the callee. I'm not sure I would agree that that is always true, unless I'm misunderstanding what you mean. If we were to move the responsibility of setting the device guard to the caller of `resize_bytes_cuda`, then all of its call sites will need to add `CUDAGuard guard(storage_impl>device().index());`. In order to make the interface developerfriendly, `resize_bytes_cuda` should be responsible for changing the device index when needed. > Also do you think it's necessary to handle/optimize nested device guard? It would certainly be ideal to avoid more `cudaGetDevice()` and `cudaSetDevice()` calls than we really need. But pragmatically, I'm wondering how significant the performance costs of the extra `cudaGetDevice()` and `cudaSetDevice()` calls really are. Because of this check `CUDAGuard` will only call `cudaSetDevice()` if the current device index differs from the new one. So within `resize_impl_cuda_(..., device_guard = true)`, unless I'm mistaken, `cudaSetDevice()` can only be called twice if the storage has a different device index than the current settingonce upon creation of the first guard, once upon destroy to restore the original setting. The nested guard won't call `cudaSetDevice()`. But there are a handful of `cudaGetDevice()` calls, not sure exactly how many, but I think it would be at least 4. However, if `resize_bytes_cuda` is responsible for changing the device index when needed, I'm not sure what better solution there is than just using `CUDAGuard`","> Hey , I'm not sure about all the answers to the questions you asked, but below are my responses to some of them. >  > > Could you explain a little bit that why is   `c10::cuda::CUDAGuard guard(device.index());` needed here >  > The allocator will use whichever device index is currently set. We need to make sure that the device index is set to the same device index that the data of the `StorageImpl` given to `resize_bytes_cuda` is currently allocated onotherwise, the storage's data may switch to a different device. We want to keep the storage on whichever device the user originally said they wanted it to be on. >  > > Along the path, `resize_impl_cuda_` (first device gurard) > `maybe_resize_storage_cuda` > `resize_bytes_cuda` (nested device guard)` >  > If the guard in `resize_bytes_cuda` is potentially redundant within a call to `resize_impl_cuda_(..., device_guard = true)`, that is not the only place where `resize_bytes_cuda` is called. It is also called here and here. I don't think either of those call sites sets the device guard beforehand. >  > > It's always the caller's responsibility to guard the callee. >  > I'm not sure I would agree that that is always true, unless I'm misunderstanding what you mean. If we were to move the responsibility of setting the device guard to the caller of `resize_bytes_cuda`, then all of its call sites will need to add `CUDAGuard guard(storage_impl>device().index());`. In order to make the interface developerfriendly, `resize_bytes_cuda` should be responsible for changing the device index when needed. >  > > Also do you think it's necessary to handle/optimize nested device guard? >  > It would certainly be ideal to avoid more `cudaGetDevice()` and `cudaSetDevice()` calls than we really need. But pragmatically, I'm wondering how significant the performance costs of the extra `cudaGetDevice()` and `cudaSetDevice()` calls really are. >  > Because of this check `CUDAGuard` will only call `cudaSetDevice()` if the current device index differs from the new one. So within `resize_impl_cuda_(..., device_guard = true)`, unless I'm mistaken, `cudaSetDevice()` can only be called twice if the storage has a different device index than the current settingonce upon creation of the first guard, once upon destroy to restore the original setting. The nested guard won't call `cudaSetDevice()`. But there are a handful of `cudaGetDevice()` calls, not sure exactly how many, but I think it would be at least 4. However, if `resize_bytes_cuda` is responsible for changing the device index when needed, I'm not sure what better solution there is than just using `CUDAGuard` Hi   Sorry, my bad. I didn't make my self clear. You are right. cudaSetDevice is only called when necessary. What I meant was the cudaGetDevice/cudaGetDevice pair introduced by the ctor and dtor of the device guard.  As for the cost, each cudaGetDevice costs just around ~100 nanoseconds on a modern CPU which is totally negligible. However we noticed 13 cudaGetDevice call during a plain torch::abs call and seems 10 of them are introduced by device guards. The host latency becomes a little bit more noticeable with a multiplier of 10.  Here is the simple test.  And here are the callstacks of the 13 calls to cudaGetDevice. cudaGetDevice_calls.txt","  I'm trying to understand why there are so many device guards in PyTorch compared to TensorFlow. Here are my current understandings (please forgive me if I am totally wrong): Q: When must a programmer insert a device guard? A: A device guard must be inserted when a piece of code relies on cudaCurrentDevice to work properly and it might be called from a potentially unguarded scope. The fundamental difference between TensorFlow (TF) and PyTorch (PT) is that TF hides worker threads from the user, binding them to a device context at a very high level. In contrast, PyTorch is much more flexible, allowing threads to switch between devices and perform devicerelated operations at any moment. For example, in the resize_bytes_cuda case, it doesnâ€™t make sense in the TensorFlow world for a thread bound to device A to resize a tensor on device B. I think it's harder for a PyTorch developer. For instance, itâ€™s difficult to determine the exact purpose and scope of the device guard inserted here due to the large guarded region. Does self>dtype().itemsize(); need to be guarded to function? If not, why is it in the guarded region? Or maybe at::detail::computeStorageNbytesContiguous must be guarded? You wouldnâ€™t know until you check the detailed implementation. maybe_resize_storage_cuda > resize_bytes_cuda clearly should be guarded, as you mentioned. But after this PR, a device guard was inserted. Is the guard in resize_impl_cuda_ still necessary? If yes, what exactly is it guarding now? I'm just continually wondering about these things :p Thanks for the time and really appreciate your reply","Your concerns are outside the scope of this PR, so I'd recommend opening an issue",Sure. Thank you.
575,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enables other devices other than cuda to register lazy_init)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(In the func Tensor.to, how can I make privateuse lazy init) Referring to Allocator, we can use  `REGISTER_LAZY_INIT` to register the function to lazy init for other device. At first I was going to use `DispatchStub`, but `DispatchStub` would get an error if it wasn't registered.So I took the way in pr to register function)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Enables other devices other than cuda to register lazy_init,"Fixes CC(In the func Tensor.to, how can I make privateuse lazy init) Referring to Allocator, we can use  `REGISTER_LAZY_INIT` to register the function to lazy init for other device. At first I was going to use `DispatchStub`, but `DispatchStub` would get an error if it wasn't registered.So I took the way in pr to register function",2023-11-09T11:50:04Z,triaged open source,closed,1,5,https://github.com/pytorch/pytorch/issues/113343," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","  Could you take a look at this pr, and give some advice?","There are some bikeshedding things, if you're willing to change some of the colors","This bug has affected the test cases of many downstream communities, eg. HuggingFace. Please review and merge it, thx. !d1b697bfa90d844267af5b203b3c056","It's a common way, which moving a tensor or medel to device before device's init. For example: https://github.com/pytorch/examples/blob/c4dc481e686b5cc7d1aad9e3977b91d8ba83bddd/cpp/mnist/mnist.cppL119L129  If you think that adding a separate file to register `lazy_init` changes is too big, how should I design it? Add it to device_guard?  "
486,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dynamo failure due to non constants input of `aten.lift_fresh_copy`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug For DistilBert defined in Transformer, there is runtime definition of mask fill value. It cause the error when invoke `torch.compile`, the detail failure msg is:  Example code to reproduce this failure:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Dynamo failure due to non constants input of `aten.lift_fresh_copy`," ğŸ› Describe the bug For DistilBert defined in Transformer, there is runtime definition of mask fill value. It cause the error when invoke `torch.compile`, the detail failure msg is:  Example code to reproduce this failure:   Versions  ",2023-11-09T06:05:23Z,oncall: pt2 module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/113331,"Further investigation, here is the debug_trace of this fake tensor:   Looks like in https://github.com/pytorch/pytorch/blob/addb8e29cd842e1a290cb0b55662ee0423ab2498/torch/_subclasses/fake_tensor.pyL1876L1885 parameter of `make_constant` does not turn on.  ","Hi , could you kindly help to take a look of this issue?",This is is because dynamo currently doesn't initialize its fake tensors with constant. i.e. we have `fake_tensor_converter(make_constant=False)`,"I think it is fine to `make_constant=True`, _as long as the tensor originates in graph_, and allow `FakeTensor` to invalidate it when constant is false (i.e. if it is mutated in place). However, we need to do a lot of work to ensure that we know when the tensor is going to be constant.   Further: In the case of compiling the exported graph, the tensor source is a `GraphModule(nn.Module)`.  It's even harder to prove it is constant in this case, compared with if it originates in a call to `torch.tensor([..])` with constant inputs.",Thanks chuang for the comment.   do we have any idea for how to fix this issue? Appreciate if anyone can help looking into this issue.
1345,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DCP] Adds support for meta tensor loading for DCP.load_state_dict())ï¼Œ å†…å®¹æ˜¯ (Currently, DCP requires the `model.state_dict()` to be materialized before passing it to DCP to load, since DCP uses the preallocated storage from the initialized model state_dict. Therefore, even for finetuning and distributed inference, users would need to explicitly materialize the model on GPU before `DCP.load_state_dict()`.  Today's flow:  This PR adds support for meta tensor loading. In DCP's planner, when encountering tensors/DTensor on meta device, we initialize tensor/DTensor on the current device on the fly and replace the tensor/DTensor on meta device in the state_dict.  After the change, users no longer needs to manually call `model.to_empty()` when loading existing checkpoints for finetuning and distributed inference. Updated user flow:  Note that for distributed training, it's still the users' responsibility to reset the parameters (`model.reset_parameters()`) as checkpoint might not exist. Note that we need to loop thru the state_dict to replace meta tensor/DTensor instead of calling `model.to_empty()` since `DCP.load()` only takes in state_dict but not model.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,[DCP] Adds support for meta tensor loading for DCP.load_state_dict(),"Currently, DCP requires the `model.state_dict()` to be materialized before passing it to DCP to load, since DCP uses the preallocated storage from the initialized model state_dict. Therefore, even for finetuning and distributed inference, users would need to explicitly materialize the model on GPU before `DCP.load_state_dict()`.  Today's flow:  This PR adds support for meta tensor loading. In DCP's planner, when encountering tensors/DTensor on meta device, we initialize tensor/DTensor on the current device on the fly and replace the tensor/DTensor on meta device in the state_dict.  After the change, users no longer needs to manually call `model.to_empty()` when loading existing checkpoints for finetuning and distributed inference. Updated user flow:  Note that for distributed training, it's still the users' responsibility to reset the parameters (`model.reset_parameters()`) as checkpoint might not exist. Note that we need to loop thru the state_dict to replace meta tensor/DTensor instead of calling `model.to_empty()` since `DCP.load()` only takes in state_dict but not model.  ",2023-11-09T00:14:43Z,oncall: distributed Merged ciflow/trunk release notes: distributed (checkpoint) module: distributed_checkpoint,closed,0,9,https://github.com/pytorch/pytorch/issues/113319, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `try_meta_tensor_checkpoint` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout try_meta_tensor_checkpoint && git pull rebase`)",Unrelated lint failure due to flaky trunk., rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `try_meta_tensor_checkpoint` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout try_meta_tensor_checkpoint && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
555,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] Add .pyi declaration of _CacheEntry)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] Make debug.py pass followimports typechecking)  CC([inductor] Make codecache.py pass followimports typechecking)  CC([dynamo] Add .pyi declaration of _CacheEntry)  CC([dynamo] Make decorators.py pass followimport typechecking) This is required for enabling followimports=silent; referenced by _dynamo/types.py. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[dynamo] Add .pyi declaration of _CacheEntry,  CC([inductor] Make debug.py pass followimports typechecking)  CC([inductor] Make codecache.py pass followimports typechecking)  CC([dynamo] Add .pyi declaration of _CacheEntry)  CC([dynamo] Make decorators.py pass followimport typechecking) This is required for enabling followimports=silent; referenced by _dynamo/types.py. ,2023-11-08T22:12:45Z,Merged ciflow/trunk topic: not user facing module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/113305, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
266,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`Storage.resize_()` moves storage to current device)ï¼Œ å†…å®¹æ˜¯ (    Old Distributed Repro      )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,`Storage.resize_()` moves storage to current device,    Old Distributed Repro      ,2023-11-08T21:29:56Z,module: cuda triaged,closed,0,9,https://github.com/pytorch/pytorch/issues/113300,"This sounds fishy to me.  Can you check this on nondistributed native ops?  Meaning something like:   where op {resize_, add_(1), [0]}  My understanding is there are two cases: 1) most ops do consistent device checks, so x.op() should fail 2) there are some ops that don't do these that are probably special cased in the codegen somewhere; we should add these to the test set although I'm not sure how to find them.",1. `add_()`   2. `resize_(0)`   3. `untyped_storage().resize_(0)`  this one uses current device!  ,: 1) what if you use `storage()` instead of untyped storage? 2) what if you resize to a positive integer instead of 0?  (for both the `untyped_storage()` and `storage()` case)?,"> 1. what if you use `storage()` instead of untyped storage?  `storage().resize_(0)`: after, `untyped_storage().device` returns `cuda:1`; `storage().device` returns `cuda:1` > 2. what if you resize to a positive integer instead of 0? (for both the untyped_storage() and storage() case)?  `untyped_storage().resize_(2)`: after, `untyped_storage().device` returns `cuda:1`; `storage().device` returns `cuda:1`  `storage().resize_(2)`: after, `untyped_storage().device` returns `cuda:1`; `storage().device` returns `cuda:1` In short, they all still use current device (`cuda:1`).",  is any of this surprising to you?,Definitely unexpected behavior that we should fix. But a not so surprising oversight I would say? :p ,It shouldn't do this. We should fix it., ,I can fix it
592,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py)ï¼Œ å†…å®¹æ˜¯ (Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py Fixes CC(Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py","Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py Fixes CC(Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py) ",2023-11-08T07:46:10Z,module: docs triaged open source medium Merged ciflow/trunk topic: not user facing docathon-h2-2023,closed,0,6,https://github.com/pytorch/pytorch/issues/113250,":white_check_mark:login: markstur / (ce14a5f2e2998b2e265a0059ba30dc3a801d3db5):white_check_mark:login: markstur / (ce14a5f2e2998b2e265a0059ba30dc3a801d3db5, 9b2f012439da35f97614b7b521d18a4812ee04c5):white_check_mark:login: markstur / (ce14a5f2e2998b2e265a0059ba30dc3a801d3db5, 9b2f012439da35f97614b7b521d18a4812ee04c5, 00ac522b7bb6964e97f493ee2b0019cd29420649):x:  login:  /  The commit (91e61d91708aa6b10e0d0a6d06b4d74ef6f80a27) is not authorized under a signed CLA. ""Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."," label ""release notes: nn""","> Reference Thanks!  Updated and also commented on some imperative mood things that I fixed (and one missing ""of"").  Hopefully those are improvements too."," Please fix the ""line too long"" lint issues.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2019,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Torch Inductor] Torch Inductor Better Support for GNN workload and Inductor Sparse Compiler)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Torch.compile has been included in Pytorch 2.0 or late versions and has shown that it is a powerful way to speed up Pytorch code greatly. Pytorch inductor has pioneered graphmode execution, significantly accelerating a spectrum of machine learning applications, including CNNs, Transformers, and Graph Neural Networks (GNNs). Despite these advancements, the endtoend optimization for PyGbased GNNs remains suboptimal. The root of this inefficiency lies in the current implementation of the fused scatter_add kernel, e.g. `fused_index_new_zeros_scatter_add_0`. Also, regarding profiling results, the fused scatter_add kernel has accounted for over 80% of the running time of typical GNN models.  The code provided here shows that codegen of fused gather scatter will lower atomic operations, which significantly affects the performance. This proposal sets forth a strategy to refine this mechanism, focusing initially on CPU backends due to their less complex thread hierarchy than GPUs and the current limitations of the Triton backend's warplevel specification. Check here for the triton limitation for Sparse Kernel Codegen.  Alternatives  Option 1 The first option is to develop an independent compiler pass specifically for sparse tensors. However, this approach is challenging, given two major reasons. First of all, the backend of the Inductor scheduler is majorly designed for pointwise kernel fusion. Secondly, the input for SparseTensor can vary significantly, both in terms of the programming interface and the format used. For instance, PyTorch Geometric (PyG) employs a proprietary SparseTensor input, which differs from the native sparse tensor format utilized by PyTorch, as detailed in the o)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Torch Inductor] Torch Inductor Better Support for GNN workload and Inductor Sparse Compiler," ğŸš€ The feature, motivation and pitch Torch.compile has been included in Pytorch 2.0 or late versions and has shown that it is a powerful way to speed up Pytorch code greatly. Pytorch inductor has pioneered graphmode execution, significantly accelerating a spectrum of machine learning applications, including CNNs, Transformers, and Graph Neural Networks (GNNs). Despite these advancements, the endtoend optimization for PyGbased GNNs remains suboptimal. The root of this inefficiency lies in the current implementation of the fused scatter_add kernel, e.g. `fused_index_new_zeros_scatter_add_0`. Also, regarding profiling results, the fused scatter_add kernel has accounted for over 80% of the running time of typical GNN models.  The code provided here shows that codegen of fused gather scatter will lower atomic operations, which significantly affects the performance. This proposal sets forth a strategy to refine this mechanism, focusing initially on CPU backends due to their less complex thread hierarchy than GPUs and the current limitations of the Triton backend's warplevel specification. Check here for the triton limitation for Sparse Kernel Codegen.  Alternatives  Option 1 The first option is to develop an independent compiler pass specifically for sparse tensors. However, this approach is challenging, given two major reasons. First of all, the backend of the Inductor scheduler is majorly designed for pointwise kernel fusion. Secondly, the input for SparseTensor can vary significantly, both in terms of the programming interface and the format used. For instance, PyTorch Geometric (PyG) employs a proprietary SparseTensor input, which differs from the native sparse tensor format utilized by PyTorch, as detailed in the o",2023-11-08T03:02:06Z,feature triaged oncall: pt2 module: dynamic shapes module: inductor,open,0,1,https://github.com/pytorch/pytorch/issues/113232,"After a discussion with  , I've added some information and perspectives about existing sparse and jagged tensors, as well as the challenges in compiling them.   The GNN pattern It all starts from the kernel fusion of GAS model model)), which is fundamental in graph processing. To be specific, I list the code here:  The issue that blocks me is that while the GAS model can be fused by the Torch Inductor, it currently reduces to an atomic_add operation. Although various algorithms exist to circumvent atomic_add, in this context, eliminating atomic operations seems feasible only through data transformation, particularly because the edge_index might be unsorted. For example, the edge_index can be created as   In this code, the `edge_index` indicates that different nodes will be reduced to a single node. However, since we don't know which specific nodes correspond to the reduction, the only way is to use `atomic_add`. If we do the data transformation in the beginning, for example, we make sure the `edge_index` has been sorted as input, and then we can transform the GAS model to gather_segment_reduce: (And I think this would be easily handled by PyG's frontend like adding a tag. A code script similar to this principle here)  In this way, since we have sorted the second dimension of edge_index, we can reduce the atomic operations to the largest extent. This is similar to the code brought by torchscatter's segment_coo function. However, we found several challenges for the compiler under this scenario: 1. PyTorch's `torch.segment_reduce` function is designed to work with 1D tensors. It performs segmentwise reduction operations based on a corresponding 1D `indices` tensor. If `x_j` is larger than 1D, e.g. 2D tensor, this function will fail. 2. `segment_reduce`  can't be fused with gather/index operator, i.e. `x_j = x[row]`. 3. `segment_coo` proposed by PyG isn't compatible with the torch inductor. 4. `segment_reduce` or `segment_coo` is not trivial to codegen. Check these links segment_reduce and segment_coo.  Jagged Tensor Jagged tensor is a fundamental data structure, widely used in NLP and recommendation systems (like DLRM).  Here is an example of how to construct jagged tensor by using torchrec.sparse:  To express vividly, we convert it to a jagged view  As we can see, this jagged tensor has a dimension whose slices may be of different lengths.   Sparsity within `edge_index` using `torch.Tensor` Let's create a simple example here  **How to detect this edge_index is sparse?** This edge_index can be translated to a sparse matrix, noticing that each column `[0, 1]`, `[1, 2]`, `[2, 3]`, `[3, 1]`, `[3, 5]`, `[1, 2]`, `[2, 4]`, `[4, 3]`, `[5, 3]`, `[2, 1]`, represents an edge in the graph. This format is commonly used in graph neural network frameworks like PyTorch Geometric to represent graph structures. To detect whether it is sparse, we can use the following equation  Usually, when sparity is less than 0.05, we consider the workload here unsuitable for dense representation and will use SpMM rather than GEMM. Back to the example given by PyG  The estimated sparsity here is `0.002`, which, though quite small, aligns with the typical range observed in graph problems. Note here is an estimation since `randint` can't ensure every pair of nodes are mutually different.  Relationship between jagged tenor and `edge_index` I appreciate  's comment about doing things more generically, and I am eager to promote a sparse compiler in a more general way, rather than using domainspecific knowledge. So can we convert `edge_index` to a jagged tensor? The answer is yes. First of all, let us sort the `edge_index` example I gave above, and I think it can be easily acquired by adding a tag in PyG . Since we need to perform reduction along the column dimension, our `edge_index` needs to be sorted along the column axis. The result should be:  Then similarly, we can set a jagged tensor, which is indeed a CSR representation  **How do we understand the sparsity and jagged features?** From my understanding, sparsity can be broken down into two aspects: Take a sparse matrix as an example.  * Firstly, the elements in different rows of the matrix are distinct, which essentially reflects its jagged nature.  * Secondly, the spacing between elements in the same row is irregular and discrete, a characteristic I would describe as 'jumpiness'. Current hardware, whether CPUs or GPUs, fundamentally cannot address this 'jumpiness' due to their architecture. This is because the issue of 'jumpiness' involves accessing noncontinuous memory addresses, a limitation of current architectures. However, resolving the jagged issue already addresses a significant part of the sparsity problem. This is because solving the jagged issue can effectively tackle the atomic operations problem associated with unordered reductions, and these atomic operations are inefficient on both CPUs and GPUs. Therefore, my proposal primarily aims to address this atomic issue.  What would be the best way to tradeoff hardware efficiency, frontend expressiveness, and compiler's codegen ability?  My hope is to incorporate sparsity as a more general aspect within the compiler, rather than as a thirdparty interface or through a domainspecific language. I hope this feature will be helpful for a range of ML models, including GNN, NLP models, and Recommendation systems. Nevertheless, the challenge lies in effectively implementing sparseaware code generation within the compiler, while optimally balancing performance and development time."
300,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove unecessary warning when getting storage.filename)ï¼Œ å†…å®¹æ˜¯ (  CC(Remove unecessary warning when getting storage.filename))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Remove unecessary warning when getting storage.filename,  CC(Remove unecessary warning when getting storage.filename),2023-11-07T22:51:55Z,Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/113212, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2420,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in input_reshard.py, tensor_ops.py, view_ops.py, api.py, storage.py, fsdp_checkpoint_example.py, utils.py, memory_tracker.py, default_planner.py, _nested_dict.py, _fsspec_filesystem.py, multihead_attention_tp.py, _view_with_dim_change.py, state_dict_saver.py, op_coverage.py, planner.py, filesystem.py, fsdp.py, common_rules.py, planner_helpers.py, state_dict_loader.py, style.py, resharding.py, _sharded_tensor_utils.py, _traverse.py, optimizer.py, _utils.py, metadata.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. First, make sure to install the required tools:   Then complete the followings steps:  1. Run `pydocstyle` to see the number of errors in the file:     &nbsp; &nbsp; This command prints out the number of errors at the end of the output. Save this output to later add it to your PR description. 2. Next, run `ruff` which should help autofix many of these errors:  &nbsp; &nbsp; `RULECODE` is the error code from the output in the issue, for example, **D200**. See the complete list of rules and autofixes here. 3. You can run the above command with the `unsafefixes` option. Doublecheck that the applied fixes are correct. 4. Fix the remaining issues. Fix **only** the errors listed in the issue. Skip the 'Missing docstrings' errors. 5. Run pydocstyle again:  This **number might not be 0** which is **OK**. Add the count of fixed errors to your PR description.   **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Singleton`, **Line**: 39, **Description**: First line should end with a period (not 'n')  **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Flatten`, **Line**: 100, **Description**: 1 blank line required between summary line and description (found 0)  **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Flatten`, **Line**: 100, **Description**: First line should end with a period (not 'g')  **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Split`, **Line**: 124, **Description**: 1 blank line required between summary line and description (found 0)  **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/vi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Fix docstring errors in input_reshard.py, tensor_ops.py, view_ops.py, api.py, storage.py, fsdp_checkpoint_example.py, utils.py, memory_tracker.py, default_planner.py, _nested_dict.py, _fsspec_filesystem.py, multihead_attention_tp.py, _view_with_dim_change.py, state_dict_saver.py, op_coverage.py, planner.py, filesystem.py, fsdp.py, common_rules.py, planner_helpers.py, state_dict_loader.py, style.py, resharding.py, _sharded_tensor_utils.py, _traverse.py, optimizer.py, _utils.py, metadata.py","Please fix the following issues. First, make sure to install the required tools:   Then complete the followings steps:  1. Run `pydocstyle` to see the number of errors in the file:     &nbsp; &nbsp; This command prints out the number of errors at the end of the output. Save this output to later add it to your PR description. 2. Next, run `ruff` which should help autofix many of these errors:  &nbsp; &nbsp; `RULECODE` is the error code from the output in the issue, for example, **D200**. See the complete list of rules and autofixes here. 3. You can run the above command with the `unsafefixes` option. Doublecheck that the applied fixes are correct. 4. Fix the remaining issues. Fix **only** the errors listed in the issue. Skip the 'Missing docstrings' errors. 5. Run pydocstyle again:  This **number might not be 0** which is **OK**. Add the count of fixed errors to your PR description.   **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Singleton`, **Line**: 39, **Description**: First line should end with a period (not 'n')  **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Flatten`, **Line**: 100, **Description**: 1 blank line required between summary line and description (found 0)  **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Flatten`, **Line**: 100, **Description**: First line should end with a period (not 'g')  **Error Code**: **D205**, **File**: `torch/distributed/_tensor/ops/view_ops.py`, **Entity**: `Split`, **Line**: 124, **Description**: 1 blank line required between summary line and description (found 0)  **Error Code**: **D400**, **File**: `torch/distributed/_tensor/ops/vi",2023-11-07T19:46:47Z,module: docs triaged medium docathon-h2-2023,closed,0,1,https://github.com/pytorch/pytorch/issues/113193,/assigntome
2002,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Higher train loss and worse evaluation metrics when using `torch.compile()`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug We are facing issues with loss curves and reproducibility when using `torch.compile()` with our models. Attached below is a graph of train loss with runs with `torch.compile()` (higher loss) and runs without (lower loss). This model is an MPTstyle transformer, but we've also seen the issue occur with evaluation for an autoencoder setup (also shown below). Would love to address this issue as soon as possible! Higher train loss:  Worse eval scores (orange and turquoise are with `torch.compile()`: !Screenshot 20231102 at 10 15 57â€¯PM (1)  Error logs Here's the error log we get from running `python minifier_launcher.py`:   Minified repro   Versions PyTorch version: 2.2.0a0+git21b6030 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0137genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB Nvidia driver version: 515.48.07 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen run)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Higher train loss and worse evaluation metrics when using `torch.compile()`," ğŸ› Describe the bug We are facing issues with loss curves and reproducibility when using `torch.compile()` with our models. Attached below is a graph of train loss with runs with `torch.compile()` (higher loss) and runs without (lower loss). This model is an MPTstyle transformer, but we've also seen the issue occur with evaluation for an autoencoder setup (also shown below). Would love to address this issue as soon as possible! Higher train loss:  Worse eval scores (orange and turquoise are with `torch.compile()`: !Screenshot 20231102 at 10 15 57â€¯PM (1)  Error logs Here's the error log we get from running `python minifier_launcher.py`:   Minified repro   Versions PyTorch version: 2.2.0a0+git21b6030 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.10.13 (main, Aug 25 2023, 13:20:03) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0137genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB Nvidia driver version: 515.48.07 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen run",2023-11-07T18:35:47Z,high priority needs reproduction triaged oncall: pt2 module: pt2 accuracy,open,0,18,https://github.com/pytorch/pytorch/issues/113180,"Hey, is there anything else I can provide to help solve this? This is a major issue we're seeing for many of our models at this point. Appreciate your help, thank you!","The minifier script is not helpful. Are you able to run some ablation experiments; e.g., can you try with `backend=""aot_eager""` and see if converges that way?", please try as directed,"Hey   apologies for the delay in getting around to this. I just ran with `backend=""aot_eager""` on a smaller model and this does converge (no compile: orange, overlaps with aot_eager in blue, and without aot_eager shows no change in train loss):  According to this page the issue is with TorchInductor, but how would I go about rootcausing this? Thank you for your help!","You could try running the accuracy minifier; chances are it's not going to work, but sometimes you get lucky. https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html A full set of debug logs, ala TORCH_LOGS=+dynamo,+aot,+inductor may help. If you have instructions to reproduce the training that might help too. It converging on aot_eager is a clear indication that it's an Inductor problem. If you can try `aot_eager_decomp_partition` that also will give more signal if it's a decomp problem.","I just tested this with torch 2.2.0 and the issue persists (see below). I previously did use the accuracy minifier  would you recommend using that with `backend=""aot_eager""`? Or is there another way to diagnose what's being compiled wrong? ","Okay, I confirmed the error does still happen with `aot_eager_decomp_partition` suggesting that it may be a decomp problem. How do we debug further?"," were you able to repro locally? The repro given above looks like a failure in the minifier. A repro would help a bunch with narrowing down further. A few things I would try next if I could repro are: (1) also run with `backend=""aot_eager""`.: (1a) if it passes, then... there are still a couple options, but one likely culprit is one of the inductor decomps, which are run in `aot_eager_decomp_partition` but not `aot_eager`. You could bisect them by removing decomps from here. (1b) If it fails, but `compile(backend=""eager"")` passes, then there are also a few options: AOTAutograd bug, functionalization bug, custom ops issues, and a few others. In this case, one useful thing to check would be if there are any (nonATen) custom operators in your model."," I was able to repro locally, and I some rough repo have instructions now and it should repro in as little as half an hour of training: 1. Follow the instructions to train the MosaicML BERT model here on C4: https://github.com/mosaicml/examples/tree/main/examples/benchmarks/bert 2. Add the kwarg to this trainer: https://github.com/mosaicml/examples/blob/7003793b15ad0ee28bc09d0fceb91eb2d0104961/examples/benchmarks/bert/main.pyL248  `compile_config = {""backend"": ""aot_eager_decomp_partition""}`.  3. Compare results with BERT with the compile_config set to None and the compile_config. You will notice that they behave very differently. You should notice divergence should be obvious in as little as a 1000 batches (approx 15 minutes on an 8A100 machine).  Let me know if you need more details to repro.","  I haven't been able to fully repro. Pasting what I've done so far + current issues below: ** Stuff I did so far ** Mostly just followed the readme steps + hit a few snags (jotting them down here) * comment out an sklearn import due to https://github.com/pybind/pybind11/discussions/3453 (I don't seem to have `GLIBCXX_3.4.29` on my machine) * upgrade my datasets library due to https://stackoverflow.com/questions/77433096/notimplementederrorloadingadatasetcachedinalocalfilesystemisnotsuppor * I had a newer version of triton, so had to manually fix some bits of the triton flash attention impl used in bert: https://github.com/openai/triton/issues/1098 * Had to follow readme to change the yaml from ""split: train"" to ""split: train_small"" * Had to install apex * This was enough to get `composer main.py yamls/main/mosaicbertbaseuncased.yaml` running properly with eager mode. When I reran it a day later though, I got some weird shm errors  I worked around it by manually changing / copying over the data dir, `data_local: ./mycopyc4`, to `data_local: ./mycopy2c4`. **Current issue** `composer main.py yamls/main/mosaicbertbaseuncased.yaml` is no longer running properly for me  I now get this issue:  I'm not really sure how to interpret that error message. At one point I added a `breakpoint()` inside dynamo, which was probably a mistake since the repro is running a distributed harness (no idea if that's related to the issue that I'm now seeing though). To be clear, I wasn't seeing that error message a few days ago but I am now. I tried this:  But I'm getting the same error.","Okay yeah so you need to delete your `local` directory if you change the dataset at all or if you previous convert_dataset failed for any reason. so you probably just need to delete `/data/users/hirsheybar/b/pytorch/examples/examples/benchmarks/bert/mycopyc4/train_small` and regenerate the dataset.  Essentially, it will not overwrite a local directory has a cached directory as this is usually an error, so you will need to just regenerate a new one. I started new PR here that removes the triton dependency feel free to give it a whirl: https://github.com/mosaicml/examples/pull/440 and removes other problematic dependencies. You can also train wihtout the FlashAttention dependencies (on a way, way smaller batch size) and I suspect you will run into the same issue). You also do not need to install apex anymore, if you are PyTorch 2.0> you can switch the algorithm in the yaml to LowPrecisionLayerNorm instead. I will update the YAML in the PR to use that.  ",I just realized Stable Diffusion and BERT are both skipped in the latest benchmark tests  so it's possible the issue could be more widespread: https://github.com/pytorch/pytorch/blob/139c4ab69da404d4e0c0d099a728c74ce06e341b/benchmarks/dynamo/torchbench.pyL237,'s fix here might have also fixed this issue:  CC(Compiled Transformer AOT Autograd graph Accuracy Failed)issuecomment1935115652 fingers crossed.,  Can you please test with the latest nightlies and see if the issue has been resolved?,"I'm helping scrub old issues this week.   from the comment above, are one of you able to check if the issue has been resolved?"," do you know if this is still broken? If you think it is, I can take another shot at following the repro steps that you listed back in  CC(Higher train loss and worse evaluation metrics when using `torch.compile()`)issuecomment1877456310 (I am still holding out hope that this was the same underlying issue as  CC(Compiled Transformer AOT Autograd graph Accuracy Failed), although that might not be the case)","The BERT problem was solved more or less, SD2 hits a different issue with a bug at the intersection of mixed precision, torch compile, FSDP1, and evalling after training for a bit and saving, but it does seem like the BERT problem is more or less fixed.", Will try reproing the original issue later this week.
2022,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Output mismatch of torch.abs with torch.compile when applying associative law of multiplication)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The associative law of multiplication should always hold for `torch.mul` and `torch.add`. And (a+b)\*b\*c == a\*b\*c + b\*b\*c. However, this law fails when applying associative law of multiplication to the input of `torch.abs`, producing mismatch on the outputs of `torch.abs`. This behavior can only repro on **CPU**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.101pvex86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 43 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 128 Online CPU(s) list: 0127 Vendor ID: AuthenticAMD Model name: AMD EPYC 7742 64Core Processor CPU family: 23 Model: 49 Thread(s) per core: 2 Core(s) per socket: 64 Socket(s): 1 Stepping: 0 Frequency boost: enabled CPU max MHz: 2250.0000 CPU min MHz: 1500.0000 BogoMIPS: 4500.19 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Output mismatch of torch.abs with torch.compile when applying associative law of multiplication," ğŸ› Describe the bug The associative law of multiplication should always hold for `torch.mul` and `torch.add`. And (a+b)\*b\*c == a\*b\*c + b\*b\*c. However, this law fails when applying associative law of multiplication to the input of `torch.abs`, producing mismatch on the outputs of `torch.abs`. This behavior can only repro on **CPU**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.101pvex86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 43 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 128 Online CPU(s) list: 0127 Vendor ID: AuthenticAMD Model name: AMD EPYC 7742 64Core Processor CPU family: 23 Model: 49 Thread(s) per core: 2 Core(s) per socket: 64 Socket(s): 1 Stepping: 0 Frequency boost: enabled CPU max MHz: 2250.0000 CPU min MHz: 1500.0000 BogoMIPS: 4500.19 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf ",2023-11-07T08:30:17Z,triaged bug oncall: pt2 oncall: cpu inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/113144,"confirmed repro, and similar to CC(Output mismatch of torch.to with torch.compile when applying associative law of multiplication) this is only broken on inductorcpu backend, not aot_eager cpu.","Fixed with https://github.com/pytorch/pytorch/pull/113253, thank you for the issue !"
2021,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Output mismatch of torch.to with torch.compile when applying associative law of multiplication)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The associative law of multiplication should always hold for `torch.mul` and `torch.add`. And 2\*a\*b\*c==a\*b\*c+a\*b\*c. However, this law fails when applying associative law of multiplication to the input of `torch.to`, producing mismatch on the outputs of `torch.to`. This behavior can only repro on **CPU**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.101pvex86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 43 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 128 Online CPU(s) list: 0127 Vendor ID: AuthenticAMD Model name: AMD EPYC 7742 64Core Processor CPU family: 23 Model: 49 Thread(s) per core: 2 Core(s) per socket: 64 Socket(s): 1 Stepping: 0 Frequency boost: enabled CPU max MHz: 2250.0000 CPU min MHz: 1500.0000 BogoMIPS: 4500.19 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pn)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Output mismatch of torch.to with torch.compile when applying associative law of multiplication," ğŸ› Describe the bug The associative law of multiplication should always hold for `torch.mul` and `torch.add`. And 2\*a\*b\*c==a\*b\*c+a\*b\*c. However, this law fails when applying associative law of multiplication to the input of `torch.to`, producing mismatch on the outputs of `torch.to`. This behavior can only repro on **CPU**.  Error logs Output of numpy.testing.assert_allclose():   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.1.101pvex86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 43 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 128 Online CPU(s) list: 0127 Vendor ID: AuthenticAMD Model name: AMD EPYC 7742 64Core Processor CPU family: 23 Model: 49 Thread(s) per core: 2 Core(s) per socket: 64 Socket(s): 1 Stepping: 0 Frequency boost: enabled CPU max MHz: 2250.0000 CPU min MHz: 1500.0000 BogoMIPS: 4500.19 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pn",2023-11-07T08:20:43Z,triaged bug oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/113143,"able to repro, confirmed this passes on DEVICE='cuda' but fails on 'cpu'.   Only fails with CPU inductor  cpu aot_eager also passes. printed the mismatching output pair from compiled cpu ","Fixed with https://github.com/pytorch/pytorch/pull/113253, thank you for the issue!"
388,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make numel/sym_numel PyInterpreter work symmetrically to others)ï¼Œ å†…å®¹æ˜¯ (  CC(Make numel/sym_numel PyInterpreter work symmetrically to others) Just some better engineering code cleanup. Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Make numel/sym_numel PyInterpreter work symmetrically to others,  CC(Make numel/sym_numel PyInterpreter work symmetrically to others) Just some better engineering code cleanup. Signedoffby: Edward Z. Yang ,2023-11-06T20:35:48Z,Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/113065, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/ezyang/2413/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/113065`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1356,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([WIP] Guard on storage offsets)ï¼Œ å†…å®¹æ˜¯ (  CC([WIP] Guard on storage offsets) **Motivation**: currently, we force a copy any time a data_ptr isn't 16byte aligned. To my understanding, this is good in the common case when your data_ptr _is_ 16byte aligned, so we can do vectorized loads. But when the data_ptr isn't 16byte aligned (i.e. is a view with storage_offset != 0), it's probably not always optimal to do the copy; first we spend time doing a copy; and second, we use additional memory. To do a better job of handling this, I think we need to be able to make assumptions at compile time about the storage offset. In order to verify whether those assumptions are correct, we need to guard on storage_offset. **Changes**: * guards.cpp / guards.py : add a field for storage offset * dynamic shapes : separate field for controlling whether storage_offset is dynamic, instead of just relying on sizes changing. **TODO** * Tests * Figure out how to handle constraints, annotations on dynamism, etc. * Deal with 0/1 specialization * Would we need to guard this change with a config to avoid causing recompilations when they didn't previously occur? * Does this regress compile time? )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[WIP] Guard on storage offsets,"  CC([WIP] Guard on storage offsets) **Motivation**: currently, we force a copy any time a data_ptr isn't 16byte aligned. To my understanding, this is good in the common case when your data_ptr _is_ 16byte aligned, so we can do vectorized loads. But when the data_ptr isn't 16byte aligned (i.e. is a view with storage_offset != 0), it's probably not always optimal to do the copy; first we spend time doing a copy; and second, we use additional memory. To do a better job of handling this, I think we need to be able to make assumptions at compile time about the storage offset. In order to verify whether those assumptions are correct, we need to guard on storage_offset. **Changes**: * guards.cpp / guards.py : add a field for storage offset * dynamic shapes : separate field for controlling whether storage_offset is dynamic, instead of just relying on sizes changing. **TODO** * Tests * Figure out how to handle constraints, annotations on dynamism, etc. * Deal with 0/1 specialization * Would we need to guard this change with a config to avoid causing recompilations when they didn't previously occur? * Does this regress compile time? ",2023-11-05T04:42:10Z,Stale release notes: fx module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/112978,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
506,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting an Error when loading a checkpoint :  AttributeError: Can't get attribute 'base_args_dict' on <module '__main__'>)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  I am getting the following error:  **AttributeError: Can't get attribute 'base_args_dict' on ** My **code** is:  **full error** :    _**Would please help me solve the problem?**_  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Getting an Error when loading a checkpoint :  AttributeError: Can't get attribute 'base_args_dict' on <module '__main__'>, ğŸ› Describe the bug  I am getting the following error:  **AttributeError: Can't get attribute 'base_args_dict' on ** My **code** is:  **full error** :    _**Would please help me solve the problem?**_  ,2023-11-04T06:36:42Z,needs reproduction module: windows module: serialization triaged module: python frontend,closed,0,4,https://github.com/pytorch/pytorch/issues/112944, could it be that file your are trying to load is corrupted.  Do you mind elaborating a bit how you've created it?,Thank you for replying  . Here is my code: ,"Hey , are you still dealing with this issue ? Have you tried updating to the latest pytorch release?","I'm closing this issue due to lack of activity, but feel free to reopen it or open a new one if needed"
232,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Trying to avoid spawning new shell far too often)ï¼Œ å†…å®¹æ˜¯ ()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Trying to avoid spawning new shell far too often,,2023-11-03T18:47:14Z,ciflow/trunk topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/112888
1287,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Torchbench inference failures)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Inductor default   [x] Dalle2_Pytorch  CC(Regression in Dalle2 due to dynamic shapes)  chuang  [x] basic_gnn_gcn/basic_gnn_sage/basic_gnn_gin :     [x] detectron2_fcos_r_50_f ( / ) ( CC([inductor][fx pass] handle numpy compatibility arg names))   [ ] timm_efficientdet  at least locally, OOMs, needs to lower batch size    cudagraphs dynamic Deferring the dynamic failures until  CC(Rework Dynamic Benchmarks To Actually Vary Shapes)  [ ] hf_BigBird `Not all values of RelaxedUnspecConstraint(L['inputs'][0].size()[0]) are valid because L['inputs'][0].size()[0] was inferred to be a constant (4).`  [ ] hf_T5_generate `Not all values of RelaxedUnspecConstraint(L['input_ids'].size()[0]) are valid because L['input_ids'].size()[0] was inferred to be a constant (4).`  [ ] llama  `Not all values of RelaxedUnspecConstraint(L['inputs'][0].size()[0]) are valid because L['inputs'][0].size()[0] was inferred to be a constant (32).` cudagraphs freezing  To do, assigning to    [ ] shufflenet  [ ] mnasnet max autotune  no additional failures   Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Torchbench inference failures," ğŸ› Describe the bug Inductor default   [x] Dalle2_Pytorch  CC(Regression in Dalle2 due to dynamic shapes)  chuang  [x] basic_gnn_gcn/basic_gnn_sage/basic_gnn_gin :     [x] detectron2_fcos_r_50_f ( / ) ( CC([inductor][fx pass] handle numpy compatibility arg names))   [ ] timm_efficientdet  at least locally, OOMs, needs to lower batch size    cudagraphs dynamic Deferring the dynamic failures until  CC(Rework Dynamic Benchmarks To Actually Vary Shapes)  [ ] hf_BigBird `Not all values of RelaxedUnspecConstraint(L['inputs'][0].size()[0]) are valid because L['inputs'][0].size()[0] was inferred to be a constant (4).`  [ ] hf_T5_generate `Not all values of RelaxedUnspecConstraint(L['input_ids'].size()[0]) are valid because L['input_ids'].size()[0] was inferred to be a constant (4).`  [ ] llama  `Not all values of RelaxedUnspecConstraint(L['inputs'][0].size()[0]) are valid because L['inputs'][0].size()[0] was inferred to be a constant (32).` cudagraphs freezing  To do, assigning to    [ ] shufflenet  [ ] mnasnet max autotune  no additional failures   Versions master ",2023-11-03T17:59:25Z,triaged oncall: pt2,open,0,1,https://github.com/pytorch/pytorch/issues/112883,https://github.com/pytorch/pytorch/pull/113204 fixes `basic_gnn_gcn/basic_gnn_sage/basic_gnn_gin`
421,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to handle CVE vulnerabilities in underlying operating system?)ï¼Œ å†…å®¹æ˜¯ (Hello, The base images for Cuda are pretty old (2.1.0cuda11.8 was pushed more than a month ago) how should we act to get latest security updates from the Ubuntu base image?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,How to handle CVE vulnerabilities in underlying operating system?,"Hello, The base images for Cuda are pretty old (2.1.0cuda11.8 was pushed more than a month ago) how should we act to get latest security updates from the Ubuntu base image?",2023-11-03T17:32:14Z,triaged module: docker security,open,0,4,https://github.com/pytorch/pytorch/issues/112876,I also dont understand a few things. https://hub.docker.com/layers/pytorch/pytorch/2.1.0cuda12.1cudnn8runtime/images/sha256e4aaefef0c96318759160ff971b527ae61ee306a1204c5f6e907c4b45f05b8a3?context=explore Why dont the dockerfile specify a FROM baseimage as it shows here? It seems to be no OS in the file layers as cuda takes up all space? But the labels say its Ubuntu? But we see clearly(?) In the dockerfiles in the repo that its using base images?,"aligoransson what security updates do you have in mind?  We have not in the past rebuild docker container after the release, but might be worth reconsidering. But as far as I understand, Ubuntu base image has not been updated in last 20+ days as well, was it: https://hub.docker.com/layers/library/ubuntu/20.04/images/sha256218bb51abbd1864df8be26166f847547b3851a89999ca7bfceb85ca9b5d2e95d?context=explore","We're currently using the 6 month old release of pytorch/pytorch:2.0.1cuda11.7cudnn8devel, I don't have the list with me at this point but Azure reported a number of vulnerabilities. Unfortunately, docker doesn't seem willing to scan the devel image for CVE's, possibly because the image is too big? Not sure ... When I run Docker Scout on the devel image, the process crashes after ~20mins with an out of memory exception. The runtime image though is possible to scan for yourself by docker scout. I see that it is based upon Ubuntu 20.04, with its past image tag, so they updated their tag after you guys built your image. The image  I guess we need to run package upgrades on top of our application, to make sure it is always up to date. On a general level, not related to pytorch, I wish there was clearer guidance on how to do this according to best practice! Or what the best practice even is! Just thinking aloud here!","I am guessing that one could do the following. It's just an idea. If you would rebuild the current image, and do `aptget upgrade y` (which does more than apply security updates, but for the sake of argument) and push to the same tag as normal. This would be done nightly. Then, if this command produces a different result, the file layer hash would update, and as such any images using the tag as a base image would pull it if they notice that it's changed. That way we would still have caching in effect, and still would get the latest updates. Not sure why Ubuntu doesn't do this, if it's such a good idea? He he ... Anyway, this is what we will do privately on our premises. Again  just thinking aloud here."
299,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI019)ï¼Œ å†…å®¹æ˜¯ (Enable customtypevarreturntype (PYI019) Link: CC(Enable more flake8pyi ruff checks))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI019,Enable customtypevarreturntype (PYI019) Link: CC(Enable more flake8pyi ruff checks),2023-11-03T15:22:41Z,triaged open source Stale ciflow/trunk release notes: onnx,closed,0,15,https://github.com/pytorch/pytorch/issues/112855, merge, rebase, Merge failed **Reason**: PR CC([BE] Enable Ruff's Flake8 PYI019) has not been reviewed yet Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `flake8pyi019` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout flake8pyi019 && git pull rebase`)","> Actually hold on, you forgot to update the pyproject.toml to actually enable the check? sorry rebase mistake! :sweat_smile:  I updated `pyproject.toml`", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , After rebasing looks like it caught another error that needs to be fixed.,Seems like their is an import failure somewhere...,"> Seems like their is an import failure somewhere... I suspect I set the type hint for DiagnosticContext class wrong. Using the following code I would get ""nondefault argument follows default argument"" syntax error, which could be introduced by optional arguments in the Diagnostic class    Would you mind taking a look at this? Looks like you made most of the changes to DiagnosticContext :smiley: ","Happy to break this up into smaller PRs if Diagnostic context in the only blocker.  . The fixes you have made so far are useful. , ping, mind taking a look?","> Happy to break this up into smaller PRs if Diagnostic context in the only blocker.  . The fixes you have made so far are useful. , ping, mind taking a look? Good idea. I kept some of the _Diagnostic and the pipeline is fixed now. Will update this when I have more context. For now PYI019 is fully enabled.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
465,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(s390x: skip tests relying on specific openblas precision)ï¼Œ å†…å®¹æ˜¯ (This change skips test_forward_mode_AD_linalg_det_singular_cpu_complex128 and test_forward_mode_AD_linalg_det_singular_cpu_float64 from test/test_ops_fwd_gradients.py due to https://github.com/OpenMathLib/OpenBLAS/issues/4194)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,s390x: skip tests relying on specific openblas precision,This change skips test_forward_mode_AD_linalg_det_singular_cpu_complex128 and test_forward_mode_AD_linalg_det_singular_cpu_float64 from test/test_ops_fwd_gradients.py due to https://github.com/OpenMathLib/OpenBLAS/issues/4194,2023-11-03T11:03:43Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/112843, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
295,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI001)ï¼Œ å†…å®¹æ˜¯ (Enable unprefixedtypeparam (PYI001) Link: CC(Enable more flake8pyi ruff checks))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI001,Enable unprefixedtypeparam (PYI001) Link: CC(Enable more flake8pyi ruff checks),2023-11-03T04:16:12Z,open source Merged ciflow/trunk release notes: distributed (rpc) topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/112823, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1483,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch._dynamo.export raises Unexpected type in sourceless builder <class 'nemo.core.neural_types.elements.VoidType'> for torchaudio model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When exporting the following model using the `dynamo_export`, there's an error about an `Unexpected type in sourceless builder`:   `report_dynamo_export.sarif`:   Versions PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1 (x86_64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.7 Libc version: N/A Python version: 3.10.13 (main, Aug 24 2023, 12:59:26) [Clang 15.0.0 (clang1500.0.40.1)] (64bit runtime) Python platform: macOS14.1x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Intel(R) Core(TM) i910910 CPU @ 3.60GHz Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] onnx==1.15.0 [pip3] onnxscript==0.1.0.dev20231102 [pip3] pytorchlightning==2.0.7 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchmetrics==1.2.0 [pip3] torchvision==0.16.0 [conda] Could not collect )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,torch._dynamo.export raises Unexpected type in sourceless builder <class 'nemo.core.neural_types.elements.VoidType'> for torchaudio model," ğŸ› Describe the bug When exporting the following model using the `dynamo_export`, there's an error about an `Unexpected type in sourceless builder`:   `report_dynamo_export.sarif`:   Versions PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.1 (x86_64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.7 Libc version: N/A Python version: 3.10.13 (main, Aug 24 2023, 12:59:26) [Clang 15.0.0 (clang1500.0.40.1)] (64bit runtime) Python platform: macOS14.1x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Intel(R) Core(TM) i910910 CPU @ 3.60GHz Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] onnx==1.15.0 [pip3] onnxscript==0.1.0.dev20231102 [pip3] pytorchlightning==2.0.7 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchmetrics==1.2.0 [pip3] torchvision==0.16.0 [conda] Could not collect ",2023-11-02T15:24:15Z,triaged module: dynamo dynamo-triage-june2024 dynamo-variable-tracker,closed,0,4,https://github.com/pytorch/pytorch/issues/112745,I got   not sure what is wrong yet ,"> I got >  >  >  > not sure what is wrong yet >  >  that happens when your pytorch and torchvision versions do not match.  I managed to repro this error, and it comes from the dynamo evaluator. The repro can be rewritten as ",I will move this back to the pytorch core triage queue so that they can help us here,"I ran this  ~~~ import torch import nemo import nemo.collections from nemo.collections.asr.models import EncDecCTCModelBPE from nemo.core import typecheck typecheck.set_typecheck_enabled(False) model = EncDecCTCModelBPE.from_pretrained(model_name=""stt_en_conformer_ctc_small"") model.to(device=""cpu"").freeze() model = model.eval() signals, length = model.preprocessor.input_example(max_batch=2) model.preprocessor = torch.compile(model.preprocessor, backend='eager'), fullgraph=True) model.preprocessor(signals, length) ~~~ It does not repro the mentioned issue. There is a graph break on slicing input data, but thats a known thing with torch.compile in general. Closing this issue. Please feel free to reopen if you still see this issue with some different example."
1992,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Compilation Failure of torch.atan in torch.compile Optimized Mode)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The `torch.atan` function fails to compile when using the `torch.compile` optimized mode. However, the same code runs successfully in eager execution mode. I believe this discrepancy indicates a bug. **To reproduce**  **Error messages and stack trace**   Versions PyTorch version: 2.2.0.dev20230921+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.9.17 (main, Jul 5 2023, 20:41:20) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.086genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070 Nvidia driver version: 535.104.12 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 46 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 32 Online CPU(s) list: 031 Vendor ID: GenuineIntel Model name: Intel(R) Xeon(R) CPU E52630 v3 @ 2.40GHz CPU family: 6 Model: 63 Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 2 Stepping: 2 CPU max MHz: 3200.0000 CPU min MHz: 1200.0000 BogoMIPS: 4794.64 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aper)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,Compilation Failure of torch.atan in torch.compile Optimized Mode," ğŸ› Describe the bug The `torch.atan` function fails to compile when using the `torch.compile` optimized mode. However, the same code runs successfully in eager execution mode. I believe this discrepancy indicates a bug. **To reproduce**  **Error messages and stack trace**   Versions PyTorch version: 2.2.0.dev20230921+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.9.17 (main, Jul 5 2023, 20:41:20) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.086genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070 Nvidia driver version: 535.104.12 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture: x86_64 CPU opmode(s): 32bit, 64bit Address sizes: 46 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 32 Online CPU(s) list: 031 Vendor ID: GenuineIntel Model name: Intel(R) Xeon(R) CPU E52630 v3 @ 2.40GHz CPU family: 6 Model: 63 Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 2 Stepping: 2 CPU max MHz: 3200.0000 CPU min MHz: 1200.0000 BogoMIPS: 4794.64 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aper",2023-11-02T14:53:12Z,oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/112736, CC(Compilation Failure of torch.mm in torch.compile Optimized Mode)issuecomment1791255536
599,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Wrong with code_coverage/readme.md)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue pytorch/tools/code_coverage.Readme.md `python oss_coverage.py runonly=moco_cov.py interestonly aten/src/Aten c10/core` should be  `python oss_coverage.py runonly=moco_cov.py interestonly aten/src/ATen c10/core` Otherwise, it will ignore `aten/src/ATen`. `aten/src/Aten` is in `build/` which ignores in your code.  Suggest a potential alternative/fix _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Wrong with code_coverage/readme.md," ğŸ“š The doc issue pytorch/tools/code_coverage.Readme.md `python oss_coverage.py runonly=moco_cov.py interestonly aten/src/Aten c10/core` should be  `python oss_coverage.py runonly=moco_cov.py interestonly aten/src/ATen c10/core` Otherwise, it will ignore `aten/src/ATen`. `aten/src/Aten` is in `build/` which ignores in your code.  Suggest a potential alternative/fix _No response_ ",2023-11-02T10:50:37Z,module: docs triaged,open,0,1,https://github.com/pytorch/pytorch/issues/112715,"Hi, This tool is not actively used I'm afraid but  feel free to send a PR fixing this."
408,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([aotinductor] Allow specifying a .so name in the aot_inductor.output_path config)ï¼Œ å†…å®¹æ˜¯ (  CC([aotinductor] Move cache_dir to utils.py)  CC([aotinductor] Allow specifying a .so name in the aot_inductor.output_path config) : D50902585)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[aotinductor] Allow specifying a .so name in the aot_inductor.output_path config,  CC([aotinductor] Move cache_dir to utils.py)  CC([aotinductor] Allow specifying a .so name in the aot_inductor.output_path config) : D50902585,2023-11-01T20:02:20Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/112651," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
351,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([aotinductor] Allow specifying a .so name in the aot_inductor.output_path config)ï¼Œ å†…å®¹æ˜¯ (  CC([aotinductor] Allow specifying a .so name in the aot_inductor.output_path config) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[aotinductor] Allow specifying a .so name in the aot_inductor.output_path config,  CC([aotinductor] Allow specifying a .so name in the aot_inductor.output_path config) ,2023-11-01T19:59:21Z,module: inductor ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/112649
2008,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in skippable.py, __init__.py, api.py, local_elastic_agent.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `stashable`, **Line**: 79, **Description**: First line should be in imperative mood (perhaps 'Iterate', not 'Iterates')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `poppable`, **Line**: 84, **Description**: First line should be in imperative mood (perhaps 'Iterate', not 'Iterates')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `isolate`, **Line**: 89, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `isolate`, **Line**: 89, **Description**: First line should end with a period (not 'a')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `dispatch`, **Line**: 152, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `dispatch`, **Line**: 152, **Description**: First line should end with a period (not 'e')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `forward`, **Line**: 183, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `forward`, **Line**: 183, **Description**: First line should end with a period (not '`')  **File**: `torch/distributed/pipeli)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,"Fix docstring errors in skippable.py, __init__.py, api.py, local_elastic_agent.py","Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `stashable`, **Line**: 79, **Description**: First line should be in imperative mood (perhaps 'Iterate', not 'Iterates')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `poppable`, **Line**: 84, **Description**: First line should be in imperative mood (perhaps 'Iterate', not 'Iterates')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `isolate`, **Line**: 89, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `isolate`, **Line**: 89, **Description**: First line should end with a period (not 'a')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `dispatch`, **Line**: 152, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `dispatch`, **Line**: 152, **Description**: First line should end with a period (not 'e')  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `forward`, **Line**: 183, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/pipeline/sync/skip/skippable.py`, **Entity**: `forward`, **Line**: 183, **Description**: First line should end with a period (not '`')  **File**: `torch/distributed/pipeli",2023-11-01T19:43:15Z,module: docs triaged medium docathon-h2-2023,closed,0,1,https://github.com/pytorch/pytorch/issues/112647,/assigntome
2192,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in default_hooks.py, post_localSGD_hook.py, debugging_hooks.py, utils.py, hierarchical_model_averager.py, optimizer_overlap_hooks.py, mixed_precision_hooks.py, quantization_hooks.py, ddp_zero_hook.py, __init__.py, powerSGD_hook.py, averagers.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_perform_local_step`, **Line**: 26, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_save_ddp_bucket_info`, **Line**: 100, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_save_ddp_bucket_info`, **Line**: 100, **Description**: First line should end with a period (not 'e')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_save_ddp_bucket_info`, **Line**: 100, **Description**: First line should be in imperative mood (perhaps 'Save', not 'Saves')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_hook_with_zero_step_setup`, **Line**: 132, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_hook_with_zero_step_setup`, **Line**: 132, **Description**: First line should end with a period (not 'd')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_hook_with_zero_step_setup`, **Line**: 132, **Description**: First line should be in imperative mood (perhaps 'Encapsulate', not 'Encapsulates')  **Fil)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Fix docstring errors in default_hooks.py, post_localSGD_hook.py, debugging_hooks.py, utils.py, hierarchical_model_averager.py, optimizer_overlap_hooks.py, mixed_precision_hooks.py, quantization_hooks.py, ddp_zero_hook.py, __init__.py, powerSGD_hook.py, averagers.py","Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_perform_local_step`, **Line**: 26, **Description**: First line should be in imperative mood (perhaps 'Perform', not 'Performs')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_save_ddp_bucket_info`, **Line**: 100, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_save_ddp_bucket_info`, **Line**: 100, **Description**: First line should end with a period (not 'e')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_save_ddp_bucket_info`, **Line**: 100, **Description**: First line should be in imperative mood (perhaps 'Save', not 'Saves')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_hook_with_zero_step_setup`, **Line**: 132, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_hook_with_zero_step_setup`, **Line**: 132, **Description**: First line should end with a period (not 'd')  **File**: `torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py`, **Entity**: `_hook_with_zero_step_setup`, **Line**: 132, **Description**: First line should be in imperative mood (perhaps 'Encapsulate', not 'Encapsulates')  **Fil",2023-11-01T19:43:10Z,module: docs triaged medium docathon-h2-2023,open,0,7,https://github.com/pytorch/pytorch/issues/112644,/assigntome,"This issue has been unassigned due to inactivity. If you are working on this issue, assign it to yourself and send a PR ASAP.",/assigntome, are you actively working on this issue?,"Yes, I'm working on it.", Are you still working on this issue? If not I will be happy to take over., Can I take up this issue? It seems that other contributors above are inactive!
2172,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in _hook_iterator.py, fetch.py, graph_settings.py, graph.py, distributed.py, gen_pyi.py, dataloader.py, signal_handling.py, collate.py, worker.py, dataset.py, sampler.py, _decorator.py, __init__.py, _typing.py, pin_memory.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/utils/data/graph.py`, **Entity**: `traverse_dps`, **Line**: 85, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: First line should end with a period (not 'n')  **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: First line should be in imperative mood; try rephrasing (found 'Deprecated')  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_sharding`, **Line**: 50, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_shuffle_settings`, **Line**: 88, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_shuffle_settings`, **Line**: 88, **Description**: First line should end with a period (not 'e')  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_random_seed`, **Line**: 131, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_random_seed`, **Line**: 131, **Description**: First line )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Fix docstring errors in _hook_iterator.py, fetch.py, graph_settings.py, graph.py, distributed.py, gen_pyi.py, dataloader.py, signal_handling.py, collate.py, worker.py, dataset.py, sampler.py, _decorator.py, __init__.py, _typing.py, pin_memory.py","Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/utils/data/graph.py`, **Entity**: `traverse_dps`, **Line**: 85, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: First line should end with a period (not 'n')  **File**: `torch/utils/data/graph.py`, **Entity**: `traverse`, **Line**: 102, **Description**: First line should be in imperative mood; try rephrasing (found 'Deprecated')  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_sharding`, **Line**: 50, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_shuffle_settings`, **Line**: 88, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_shuffle_settings`, **Line**: 88, **Description**: First line should end with a period (not 'e')  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_random_seed`, **Line**: 131, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/utils/data/graph_settings.py`, **Entity**: `apply_random_seed`, **Line**: 131, **Description**: First line ",2023-11-01T19:42:55Z,module: docs triaged medium docathon-h2-2023,closed,0,4,https://github.com/pytorch/pytorch/issues/112635,/assigntome,"MSIT can you please unassigned the issue, I requested you for this via email about 3 days ago.","> MSIT can you please unassigned the issue, I requested you for this via email about 3 days ago. Sorry, I just missed that email. I saw the email, and it was okay from my side.",/assigntome
2059,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/nn/modules/container.py`, **Entity**: `Sequential`, **Line**: 43, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/nn/modules/container.py`, **Entity**: `_get_item_by_idx`, **Line**: 107, **Description**: First line should end with a period (not 'r')  **File**: `torch/nn/modules/container.py`, **Entity**: `append`, **Line**: 221, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')  **File**: `torch/nn/modules/container.py`, **Entity**: `_get_abs_string_index`, **Line**: 282, **Description**: First line should end with a period (not 's')  **File**: `torch/nn/modules/container.py`, **Entity**: `__repr__`, **Line**: 329, **Description**: First line should end with a period (not 's')  **File**: `torch/nn/modules/container.py`, **Entity**: `__repr__`, **Line**: 329, **Description**: First line should be in imperative mood; try rephrasing (found 'A')  **File**: `torch/nn/modules/container.py`, **Entity**: `append`, **Line**: 378, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')  **File**: `torch/nn/modules/container.py`, **Entity**: `extend`, **Line**: 392, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')  **File**: `torch/nn/modules/container.py`, **Entity**: `clear`, **Line**: 482, **Description**: Oneline docstring should fit on one line with quotes (found 2)  **File**: `t)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Fix docstring errors in container.py, _functions.py, transformer.py, comm.py, parallel_apply.py, data_parallel.py, scatter_gather.py","Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/nn/modules/container.py`, **Entity**: `Sequential`, **Line**: 43, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/nn/modules/container.py`, **Entity**: `_get_item_by_idx`, **Line**: 107, **Description**: First line should end with a period (not 'r')  **File**: `torch/nn/modules/container.py`, **Entity**: `append`, **Line**: 221, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')  **File**: `torch/nn/modules/container.py`, **Entity**: `_get_abs_string_index`, **Line**: 282, **Description**: First line should end with a period (not 's')  **File**: `torch/nn/modules/container.py`, **Entity**: `__repr__`, **Line**: 329, **Description**: First line should end with a period (not 's')  **File**: `torch/nn/modules/container.py`, **Entity**: `__repr__`, **Line**: 329, **Description**: First line should be in imperative mood; try rephrasing (found 'A')  **File**: `torch/nn/modules/container.py`, **Entity**: `append`, **Line**: 378, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')  **File**: `torch/nn/modules/container.py`, **Entity**: `extend`, **Line**: 392, **Description**: First line should be in imperative mood (perhaps 'Append', not 'Appends')  **File**: `torch/nn/modules/container.py`, **Entity**: `clear`, **Line**: 482, **Description**: Oneline docstring should fit on one line with quotes (found 2)  **File**: `t",2023-11-01T15:29:43Z,module: docs triaged medium docathon-h2-2023,closed,0,3,https://github.com/pytorch/pytorch/issues/112603,/assigntome,"This issue has been unassigned due to inactivity. If you are still working on this issue, assign it to yourself, and send a PR ASAP.",/assigntome
2012,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in graphs.py, storage.py, _sanitizer.py, _utils.py, jiterator.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 124, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 124, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 128, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 128, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 132, **Description**: First line should end with a period (not 'U')  **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 132, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `mps`, **Line**: 139, **Description**: First line should end with a period (not 'S')  **File**: `torch/storage.py`, **Entity**: `mps`, **Line**: 139, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `double`, **Line**: 154, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `float`, **Line**: 158, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `half`, **Line**: 162, **Description)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Fix docstring errors in graphs.py, storage.py, _sanitizer.py, _utils.py, jiterator.py","Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 124, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `clone`, **Line**: 124, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 128, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `tolist`, **Line**: 128, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 132, **Description**: First line should end with a period (not 'U')  **File**: `torch/storage.py`, **Entity**: `cpu`, **Line**: 132, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `mps`, **Line**: 139, **Description**: First line should end with a period (not 'S')  **File**: `torch/storage.py`, **Entity**: `mps`, **Line**: 139, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/storage.py`, **Entity**: `double`, **Line**: 154, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `float`, **Line**: 158, **Description**: First line should end with a period (not 'e')  **File**: `torch/storage.py`, **Entity**: `half`, **Line**: 162, **Description",2023-11-01T15:29:20Z,module: docs triaged medium docathon-h2-2023,closed,0,3,https://github.com/pytorch/pytorch/issues/112589,/assigntome,"This issue has been unassigned due to inactivity. If you are still working on this issue, assign it to yourself, and send a PR ASAP.",/assigntome
2031,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in _guards.py, _ops.py, _jit_internal.py, functional.py, _tensor_str.py, library.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/functional.py`, **Entity**: `broadcast_tensors`, **Line**: 45, **Description**: First line should end with a period (not 's')  **File**: `torch/functional.py`, **Entity**: `broadcast_tensors`, **Line**: 45, **Description**: First line should not be the function's ""signature""  **File**: `torch/functional.py`, **Entity**: `broadcast_shapes`, **Line**: 77, **Description**: First line should end with a period (not 'e')  **File**: `torch/functional.py`, **Entity**: `broadcast_shapes`, **Line**: 77, **Description**: First line should not be the function's ""signature""  **File**: `torch/functional.py`, **Entity**: `split`, **Line**: 142, **Description**: First line should be in imperative mood (perhaps 'Split', not 'Splits')  **File**: `torch/functional.py`, **Entity**: `einsum`, **Line**: 192, **Description**: First line should end with a period (not 'r')  **File**: `torch/functional.py`, **Entity**: `einsum`, **Line**: 192, **Description**: First line should not be the function's ""signature""  **File**: `torch/functional.py`, **Entity**: `meshgrid`, **Line**: 397, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')  **File**: `torch/functional.py`, **Entity**: `_unique_impl`, **Line**: 744, **Description**: First line should end with a period (not ']')  **File**: `torch/functional.py`, **Entity**: `tensordot`, **Line**: 1034, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,"Fix docstring errors in _guards.py, _ops.py, _jit_internal.py, functional.py, _tensor_str.py, library.py","Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/functional.py`, **Entity**: `broadcast_tensors`, **Line**: 45, **Description**: First line should end with a period (not 's')  **File**: `torch/functional.py`, **Entity**: `broadcast_tensors`, **Line**: 45, **Description**: First line should not be the function's ""signature""  **File**: `torch/functional.py`, **Entity**: `broadcast_shapes`, **Line**: 77, **Description**: First line should end with a period (not 'e')  **File**: `torch/functional.py`, **Entity**: `broadcast_shapes`, **Line**: 77, **Description**: First line should not be the function's ""signature""  **File**: `torch/functional.py`, **Entity**: `split`, **Line**: 142, **Description**: First line should be in imperative mood (perhaps 'Split', not 'Splits')  **File**: `torch/functional.py`, **Entity**: `einsum`, **Line**: 192, **Description**: First line should end with a period (not 'r')  **File**: `torch/functional.py`, **Entity**: `einsum`, **Line**: 192, **Description**: First line should not be the function's ""signature""  **File**: `torch/functional.py`, **Entity**: `meshgrid`, **Line**: 397, **Description**: First line should be in imperative mood (perhaps 'Create', not 'Creates')  **File**: `torch/functional.py`, **Entity**: `_unique_impl`, **Line**: 744, **Description**: First line should end with a period (not ']')  **File**: `torch/functional.py`, **Entity**: `tensordot`, **Line**: 1034, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns",2023-11-01T15:29:15Z,module: docs triaged medium topic: not user facing docathon-h1-2024,open,0,1,https://github.com/pytorch/pytorch/issues/112586,/assigntome
2187,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in _VF.py, _appdirs.py, hub.py, _classes.py, _storage_docs.py, _linalg_utils.py, torch_version.py, quasirandom.py, random.py, __future__.py, _lowrank.py, _vmap_internals.py, _sources.py, __config__.py, _lobpcg.py, _namedtensor_internals.py)ï¼Œ å†…å®¹æ˜¯ (Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/__config__.py`, **Entity**: `show`, **Line**: 5, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__config__.py`, **Entity**: `show`, **Line**: 5, **Description**: First line should end with a period (not 'e')  **File**: `torch/__config__.py`, **Entity**: `_cxx_flags`, **Line**: 16, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__config__.py`, **Entity**: `parallel_info`, **Line**: 21, **Description**: First line should end with a period (not 's')  **File**: `torch/__config__.py`, **Entity**: `parallel_info`, **Line**: 21, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__future__.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__future__.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 's')  **File**: `torch/_appdirs.py`, **Entity**: ``, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/_appdirs.py`, **Entity**: ``, **Line**: 8, **Description**: First line should end with a period (not 'm')  **File**: `torch/_appdirs.py`, **Entity**: `_get_win_folder_from_registry`, **Line**: 501, **Description**: 1 blank line required between summary line and description (found 0)  **Fil)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Fix docstring errors in _VF.py, _appdirs.py, hub.py, _classes.py, _storage_docs.py, _linalg_utils.py, torch_version.py, quasirandom.py, random.py, __future__.py, _lowrank.py, _vmap_internals.py, _sources.py, __config__.py, _lobpcg.py, _namedtensor_internals.py","Please fix the following issues. To test locally, run:    Run this command before and after fixing the errors. This command prints out the number of errors at the end of the output. Paste the output to your PR description with the number before and after.   **File**: `torch/__config__.py`, **Entity**: `show`, **Line**: 5, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__config__.py`, **Entity**: `show`, **Line**: 5, **Description**: First line should end with a period (not 'e')  **File**: `torch/__config__.py`, **Entity**: `_cxx_flags`, **Line**: 16, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__config__.py`, **Entity**: `parallel_info`, **Line**: 21, **Description**: First line should end with a period (not 's')  **File**: `torch/__config__.py`, **Entity**: `parallel_info`, **Line**: 21, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__future__.py`, **Entity**: ``, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__future__.py`, **Entity**: ``, **Line**: 1, **Description**: First line should end with a period (not 's')  **File**: `torch/_appdirs.py`, **Entity**: ``, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/_appdirs.py`, **Entity**: ``, **Line**: 8, **Description**: First line should end with a period (not 'm')  **File**: `torch/_appdirs.py`, **Entity**: `_get_win_folder_from_registry`, **Line**: 501, **Description**: 1 blank line required between summary line and description (found 0)  **Fil",2023-11-01T15:29:14Z,module: docs triaged medium docathon-h2-2023,open,0,2,https://github.com/pytorch/pytorch/issues/112585,/assigntome,  1 blank line required between summary line and description.I am giving one but still says the same thing.Kindly help me here.
1329,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Breaking change 2.1] Passing non-contiguous inputs to SDPA on CUDA device with the mem-efficient attention backend returns garbage)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi , after more hours of debugging than I am comfortable to admit, I noticed the following breaking change between PyTorch 2.0.1 and PyTorch 2.1. The issue can be reproduced both with `torch2.1.0+cu118` & `2.2.0.dev20231030+cu118`. There is no issue on 2.0.1 For fp32 inputs to SDPA on CUDA device & passing a custom `attn_mask`, we have the following: * PyTorch 2.0.1: dispatches to math (`RuntimeError: No available kernel. Aborting execution.` for other backends) * PyTorch 2.1: dispatches to memefficient attention (likely following  CC(Add support for ALiBi/relative positional biases to the fast path for Transformers)). And it appears that **the memefficient attention backend outputs wrong results when passing noncontigous key/value, while the math backend goes just fine**. Reproduction:  The result is:  Note that these metrics are not super representative of how the drift is bad locally: !image debug_sdpa.zip The relevant tensors are attached in a zip here. Thank you!  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Breaking change 2.1] Passing non-contiguous inputs to SDPA on CUDA device with the mem-efficient attention backend returns garbage," ğŸ› Describe the bug Hi , after more hours of debugging than I am comfortable to admit, I noticed the following breaking change between PyTorch 2.0.1 and PyTorch 2.1. The issue can be reproduced both with `torch2.1.0+cu118` & `2.2.0.dev20231030+cu118`. There is no issue on 2.0.1 For fp32 inputs to SDPA on CUDA device & passing a custom `attn_mask`, we have the following: * PyTorch 2.0.1: dispatches to math (`RuntimeError: No available kernel. Aborting execution.` for other backends) * PyTorch 2.1: dispatches to memefficient attention (likely following  CC(Add support for ALiBi/relative positional biases to the fast path for Transformers)). And it appears that **the memefficient attention backend outputs wrong results when passing noncontigous key/value, while the math backend goes just fine**. Reproduction:  The result is:  Note that these metrics are not super representative of how the drift is bad locally: !image debug_sdpa.zip The relevant tensors are attached in a zip here. Thank you!  Versions  ",2023-11-01T11:00:42Z,high priority triaged module: regression module: correctness (silent),closed,1,22,https://github.com/pytorch/pytorch/issues/112577,"So for this case the mask passed in is all Trues which will get converted to a ""zeros"" additive attention_bias mask.. in theory a noop. When I run the same q,k,v input with no mask I get   which leads me to believe that noncontiguous inputs alone are not the problem but noncontiguous + masking is no bueno",If I update the script to only call contiguous on either key or value but not both I still see the same results,"Okayy I got it, so this happens when bias_mask is not contiguous ",Weird  so `attention_mask_sdpa` not being contiguous was responsible of the issue?, I can confirm this issue is fixed on `2.2.0.dev20231102+cu118`,"Unfortunately this solutions makes the (1, 1, seqlen_q, seq_len_kv case) have to fully materialize the attention mask.  I think that a third component of this bug is that it is occuring when seqlen_q = 1, I tried with seqlen_q = 2 and it appears to not occur anymore"," I was acting somewhat rash with the impeding 2.1.1 deadline and I am unhappy with my solution, and I didn't fully think through its ramifications.  For a very hot path I.e. a mask of (1, 1, seq_len_q, seq_len_kv) this causes a substantial memory regression.  I am having a hard time figure out whats goin on with the repro mentioned here: https://github.com/pytorch/pytorch/pull/114173 The results of the repo appear to be dependent on order in which the ops are ran, which makes me thing there is some IMA happening, but alas I haven't found it. ","Thanks a lot for looking into it , sorry for having been a bit pushy about having this in the patch! I appear not to be able to reproduce the logits difference in https://github.com/pytorch/pytorch/pull/114173 using `2.1.1+cu118` and A100SXM480GB,  is the output difference appearing using the memory patch https://github.com/pytorch/pytorch/pull/114173?",Tentatively marking hipri for the regression, Could you help double check if the memory issue is fixed in 2.1.2?  Here are the results  and myself gets using the reproduction example in the issue:  This is the same as what we get from 2.1.0,"  So the memory issue was ""fixed"" in 2.1.1 via this PR: https://github.com/andreigh/pytorch/commit/f00fb17dfe3956f269c053c7c27da8e741d5e5ce While this did change the result of the repro, it caused a memory usage ""regression"" for a very common hot path for SDPA.  This PR: https://github.com/pytorch/pytorch/pull/114837 Was used to fix the memory regression. I did further analysis on the initial correctness report. I determined that the repro wasn't behaving as I expected.. e.g the order with which aten ops were called impacted.  I am also not entirely sure that a contiguous vs non contiguous input should produce identical results (it likely should but I need to reason if there is some potential float addmulitple ops that get reordered  ) I was leaving this open in case we need to revisit this but the the memory reduction should be visible via usual memory profiling techniques "," I can also confirm that this issue is reintroduced in the 2.1.2 release, while it was fixed in 2.1.1. This is a bit bad for us as following 2.1.1 we added SDPA by default in Transformers and a very common execution path has noncontiguous inputs: https://github.com/huggingface/transformers/blob/4b759da8be71ac025610d91881a18b592adf701c/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.pyL553. We'll likely have to do a patch and manually call `.contiguous()`, I guess. In 2.1.2, this codepath starts erroring out with `RuntimeError: CUDA error: an illegal memory access was encountered`. Here, silently SDPA starts outputting different results when inputs are not contiguous, which I think is not wishable. If the kernel requires contiguous inputs, why not check it on the C++ side and call `contiguous()` if needed (maybe with a warning)?", do you have a repro for the illegal memory access? I wasn't able to rerepro this with the existing one in this issue. ,"I don't have a minimal reproduction unfortunately, but you can try:  and run `CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0 python test_bigcode.py` with the following script:  which errors at:  This corresponds to this code: https://github.com/huggingface/transformers/blob/5e4ef0a0f6eaf0b80eb951a674939d68ab30f6e5/src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.pyL515L562 Note that on `main` of Transformers you will not be able to reproduce as for now I just disabled the memefficient path for GPT BigCode due to this issue (fix at https://github.com/huggingface/transformers/pull/27973, removing the expand).", Is it possible to have minimal repro for this ? Running using your instructions I get following: ,Oh sorry. You can try with `TabbyML/SantaCoder1B` instead that is nongated. I'll try to come up with a minimal repro ASAP. I edited my message above.  For the record using ,Running above command produced for me: Notable Environment differences: CUDA12.1 PyTorch: Main GPU: H100 `,While the repro isn't throwing a IMA computesanitizer is reporting errors: https://gist.github.com/drisspg/a9269f422d13a67ee94610f8246951a8,Current gists with computesanitizer outputs: https://gist.github.com/drisspg/a9269f422d13a67ee94610f8246951a8,I have a fix: https://github.com/pytorch/pytorch/pull/116234 ğŸ¤,I rerun the script to confirm that this is fixed in 2.2.0 ,Okay I am going to close this issue for now
1084,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_numpy_non_writeable_cpu (__main__.TestNumPyInteropCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 14 workflow(s) with 14 failures and 14 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_numpy_non_writeable_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_numpy_interop.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DISABLED test_numpy_non_writeable_cpu (__main__.TestNumPyInteropCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 14 workflow(s) with 14 failures and 14 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_numpy_non_writeable_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_numpy_interop.py` ",2023-11-01T06:40:02Z,triaged module: flaky-tests module: numpy skipped oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/112567,"I can't find any relevant logs for this and it looks to pass consistently locally, so closing this."
2023,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(storage._share_fd_cpu_() in multiprocessing/reductions.py reports 'can not allocate memory (12)')ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am migrating my project from PyTorch 1.8 to 1.13.  While at the beggining of the training, the following error occured:  Here are some detailed information: 1. Everything was fine under PyTorch 1.8 2. The training was run inside docker container, with GPU of A100 Ã— 4 3. By adding log in multiprocessing/reductions.py, I found that until the error message occured,  `storage._share_fd_cpu()` had been called over 60000+ times, and the accumulated `storage.size()` was about 37M. 4. By running `ulimit a` I found the file descriptor limit was about 1M, and by running `df /dev/shm` I found the size of shared memory was about 100G. 5. By observing result shown in `top` command, I found the memory usage was no more than about 40% before the error occured. 6. The training dataset was composed by one set of data repeated 3 times. If I use only one copy, or just repeat a partial part, the error would not occur. I am not sure whether the error was caused by the size of the dataset or the data duplication. See the following image for details. !image Could anyone give me advice on how to further explore on this issue? Thank you.  Versions PyTorch version: 1.13.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.0110genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,storage._share_fd_cpu_() in multiprocessing/reductions.py reports 'can not allocate memory (12)'," ğŸ› Describe the bug I am migrating my project from PyTorch 1.8 to 1.13.  While at the beggining of the training, the following error occured:  Here are some detailed information: 1. Everything was fine under PyTorch 1.8 2. The training was run inside docker container, with GPU of A100 Ã— 4 3. By adding log in multiprocessing/reductions.py, I found that until the error message occured,  `storage._share_fd_cpu()` had been called over 60000+ times, and the accumulated `storage.size()` was about 37M. 4. By running `ulimit a` I found the file descriptor limit was about 1M, and by running `df /dev/shm` I found the size of shared memory was about 100G. 5. By observing result shown in `top` command, I found the memory usage was no more than about 40% before the error occured. 6. The training dataset was composed by one set of data repeated 3 times. If I use only one copy, or just repeat a partial part, the error would not occur. I am not sure whether the error was caused by the size of the dataset or the data duplication. See the following image for details. !image Could anyone give me advice on how to further explore on this issue? Thank you.  Versions PyTorch version: 1.13.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.0110genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100",2023-10-31T03:05:49Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/112470,"After searching over and over, I finally found the solution: setting the kernel parameter `vm.max_map_count` to a larger one, e.g. `sysctl w vm.max_map_count=1048576`","Thank you, your answer solved my problem perfectly!","> After searching over and over, I finally found the solution: setting the kernel parameter `vm.max_map_count` to a larger one, e.g. >  > `sysctl w vm.max_map_count=1048576` Just wanted to add this this fixed the issue for me as well.  Cheers!"
657,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove HSDP validation check )ï¼Œ å†…å®¹æ˜¯ (Currently, HSDP validates that all intra/inter node PGs are the same. This makes sense if you are only using HSDP with no other forms of parallelism and is a nice but not necessary sanity check. However, if you want to mix HSDP with other forms, say tensor parallelism on the FFN of a transformer block, the intra/inter node PGs will be different for that layer. This check raises errors in this scenario, so we need to remove this assumption. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Remove HSDP validation check ,"Currently, HSDP validates that all intra/inter node PGs are the same. This makes sense if you are only using HSDP with no other forms of parallelism and is a nice but not necessary sanity check. However, if you want to mix HSDP with other forms, say tensor parallelism on the FFN of a transformer block, the intra/inter node PGs will be different for that layer. This check raises errors in this scenario, so we need to remove this assumption. ",2023-10-30T20:29:17Z,oncall: distributed triaged open source Merged ciflow/trunk release notes: distributed (fsdp) ciflow/periodic ciflow/inductor,closed,0,17,https://github.com/pytorch/pytorch/issues/112435,"If I understand correctly, there is still some value in the check; however, it is currently overly strict and is problematic for manual wrapping + HSDP. I think there was someone internally working on relaxing the check. The check that is valuable is that if you are using HSDP, then each HSDP instance should use the same process groups if using the same ranks. We do not want to create a different pair of process groups per HSDP instance. "," got it! If someone is working on it, feel free to close this PR then :) ",Let me followup on the progress on that PR and get back to you!,"  Is there anything from the checkpointing side that requires each FSDP instance to use the HSDP process groups? If not, then I think removing this requirement/check sounds good to me (and we would need to remove the unit test). https://github.com/pytorch/pytorch/blob/b8b3c26d3dd6da301d7331b25e3208f4d9e8faa3/test/distributed/fsdp/test_fsdp_hybrid_shard.pyL119L157",">   Is there anything from the checkpointing side that requires each FSDP instance to use the HSDP process groups? >  > If not, then I think removing this requirement/check sounds good to me (and we would need to remove the unit test). >  > https://github.com/pytorch/pytorch/blob/b8b3c26d3dd6da301d7331b25e3208f4d9e8faa3/test/distributed/fsdp/test_fsdp_hybrid_shard.pyL119L157    bumping this request! would love to have this issue resolved",   bumping this please!,I think it is okay to remove the check. Will let  to review again.,">   Is there anything from the checkpointing side that requires each FSDP instance to use the HSDP process groups? >  > If not, then I think removing this requirement/check sounds good to me (and we would need to remove the unit test). >  > https://github.com/pytorch/pytorch/blob/b8b3c26d3dd6da301d7331b25e3208f4d9e8faa3/test/distributed/fsdp/test_fsdp_hybrid_shard.pyL119L157 We are relying on the DTensor to do all_gather and chunk so we don't use the HSDP process groups directly. So I think it should be fine removing this requirement. ", can i request that you update the PR desc to have a more detailed summary of why the change is OK (taking the above discussions into account)  I'd like it to be clear what assumptions we're making and what conditions we're relaxing intentionally (and why)., test removed!  sorry  updated to be more clear :) , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  BC Lint  pull  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `mvpatel2000/removehsdpvalidate` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout mvpatel2000/removehsdpvalidate && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
512,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Fix mps to cpu copy with storage offset)ï¼Œ å†…å®¹æ˜¯ (Fix CC(Target indices are ignored when writing to a CPU tensor from an MPS tensor) Cherrypick of  https://github.com/pytorch/pytorch/pull/109557 into release/2.1 branch Approved by: https://github.com/DenisVieriu97 (cherry picked from commit 00871189972e81a5fde230bc08137be14c59f178))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MPS] Fix mps to cpu copy with storage offset,Fix CC(Target indices are ignored when writing to a CPU tensor from an MPS tensor) Cherrypick of  https://github.com/pytorch/pytorch/pull/109557 into release/2.1 branch Approved by: https://github.com/DenisVieriu97 (cherry picked from commit 00871189972e81a5fde230bc08137be14c59f178),2023-10-30T19:47:11Z,release notes: mps ciflow/mps,closed,0,0,https://github.com/pytorch/pytorch/issues/112432
1964,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Tracker] Move nested tensors to beta)ï¼Œ å†…å®¹æ˜¯ (This issue tracks the status of readying nested tensors to be promoted from prototype status > beta, targeting the 2.5 release. See also: op request tracker issue) for new op coverage requests. * Public APIs / operator support     * [X] `torch.nested.nested_tensor(tensor_list, layout=torch.jagged)`  construct a jagged layout NT without preserving autograd history ( CC(Public API for constructing NT with jagged layout from tensor list))     * [X] `torch.nested.as_nested_tensor(tensor_list, layout=torch.jagged)`  construct a jagged layout NT preserving autograd history ( CC(Support for as_nested_tensor() with jagged layout + fixed nested_tensor() semantics))     * [x] `torch.nested.as_nested_tensor(t, layout=torch.jagged)`  construct a jagged layout NT with consistent sequence lengths from a dense tensor (in progress by  in CC(Support for torch.nested.as_nested_tensor(t)))     * [ ] Conversion between strided and jagged layouts using `to(layout)` (in progress by  in CC(Conversions between strided and jagged layouts for Nested Tensors))     * [x] Noncopying constructor construct a jagged layout NT from the `(values, offsets)` components directly ( in CC(Public API for NJT construction from jagged components))     * [ ] CC(Allow slicing of Nested Tensors along constant dimensions): Slicing of arbitrary dims for NTs of both layouts     * [ ] Resolve rough edges of SDPA support for jagged layout NTs (internal only: link)         * [x] Implement backend selection in `NestedTensor.__torch_function__()` impl of SDPA ( CC(Add an SDPA dispatcher for nested tensors with jagged layouts))         * [x] Support transpose of jagged layout NTs on the ragged dim (need to track this appropriately via `ragged_idx` and ensure the prop)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Tracker] Move nested tensors to beta,"This issue tracks the status of readying nested tensors to be promoted from prototype status > beta, targeting the 2.5 release. See also: op request tracker issue) for new op coverage requests. * Public APIs / operator support     * [X] `torch.nested.nested_tensor(tensor_list, layout=torch.jagged)`  construct a jagged layout NT without preserving autograd history ( CC(Public API for constructing NT with jagged layout from tensor list))     * [X] `torch.nested.as_nested_tensor(tensor_list, layout=torch.jagged)`  construct a jagged layout NT preserving autograd history ( CC(Support for as_nested_tensor() with jagged layout + fixed nested_tensor() semantics))     * [x] `torch.nested.as_nested_tensor(t, layout=torch.jagged)`  construct a jagged layout NT with consistent sequence lengths from a dense tensor (in progress by  in CC(Support for torch.nested.as_nested_tensor(t)))     * [ ] Conversion between strided and jagged layouts using `to(layout)` (in progress by  in CC(Conversions between strided and jagged layouts for Nested Tensors))     * [x] Noncopying constructor construct a jagged layout NT from the `(values, offsets)` components directly ( in CC(Public API for NJT construction from jagged components))     * [ ] CC(Allow slicing of Nested Tensors along constant dimensions): Slicing of arbitrary dims for NTs of both layouts     * [ ] Resolve rough edges of SDPA support for jagged layout NTs (internal only: link)         * [x] Implement backend selection in `NestedTensor.__torch_function__()` impl of SDPA ( CC(Add an SDPA dispatcher for nested tensors with jagged layouts))         * [x] Support transpose of jagged layout NTs on the ragged dim (need to track this appropriately via `ragged_idx` and ensure the prop",2023-10-30T15:45:07Z,triaged module: nestedtensor,open,1,12,https://github.com/pytorch/pytorch/issues/112398,Just posting nits as comments. Right now NT prints different for a jagged layout vs. strided. ,> Right now NT prints different for a jagged layout vs. strided. Added to the tracker. We should unify these.,"   Hey there! âœŒï¸ I'm a little new to this mass open source thing; just wanted to clarify something  I found the issue thread where you were taking feature requests, but its on an old closed repo. Wanted to know if I should open an issue in the main repo or if there's somewhere else that's more appropriate (the request is exclusively related to nested tensors)","I want to use a linear layer on a nested tensor that contains two 3d tensors of same last dimension, i think it's a reasonably common use case (two batches of different sequence lengths). Can we make it supported ? ",Can torch.compile avoid memory copies when using as_nested_tensor ?,"Hey ! If you're referring to the old, closed nested tensor repo, that is deprecated in favor of the version of nested tensors that is now in PyTorch core. We encourage opening issues in the main pytorch repo for nested tensor feature requests :)","Note: there is at least one issue when running `torch.inference_mode()`. `torch.no_grad()` seems to be fine as workaround for now.  Edit: `inference_mode()` disables the autograd key, which disables decomps for composite ops. This is addressed in CC(Fix composite op redispatch for NJT in inference mode).","Hi, I'm interested in using NestedTensors for training a TrackFormer like model (a transformer encoder decoder architecture).  The length of the inputs to the encoder, as well as the decoder queries would be variable. I see that there's a stretch goal for end to end support of SAM (which I presume is Segment Anything) is on the roadmap.  This seems like a similar use case / architecture to what I'm trying to accomplish.  Is this the best place for me to follow along with the progress?  Also, is the goal to support inference only, or training too?","Hey ! We've shown speedups for inference on SAM in the Segment Anything Fast repo using ""jagged layout"" nested tensors with `torch.compile`. For TrackFormer, if images are resized to the same shape and raggedness occurs in only one dimension, the same approach can be utilized. Our goal is certainly to support training as well, and we've had some recent training successes on internal models. The linked repo is a good place to start, and I have some docs in progress I can link you too as well. I'm also happy to help with any problems you may run into. ","Hi , I'm sorry this slipped through the cracks while I was on vacation. I'm looking to use nested tensors purely for training purposes (at inference I use batch size = 1). I posted the TrackFormer paper because there are some parallels, but I'm actually using the transformer decoder for sensor fusion from multiple detectors (different sensor modalities).  The unbatched inputs to the transformer decoder would be sequences of shape `[number of detections across all detectors, detector embedding dimension]`.  Since the number of detections may vary wildly from scene to scene (the number of detections is however capped / upper bounded), the length of the sequence dimension for the transformer may vary significantly across a minibatch.  For this reason, I think nested tensors would be perfect for my training use case. I've hit a lot of torch operations that are unsupported on nested tensors, some of which were surprising (like elementwise operations like `torch.exp`, which is failing for me with PyTorch 2.2.2).  I wrote some workarounds to map unary operations to the unbinded tensors, perform reductions like `torch.mean` or `torch.sum`, concatenate and more.  However, these are mostly of the form:  Is this the most efficient way to do this (without touching PyTorch C++ code)? I found I couldn't use `torch.vmap` with nested tensors, but if there was some way to vectorize the operations across the unbinded nested tensor I'm sure it'd be a lot faster.  "," It appears you're using the older, C++ subclass form of nested tensors (AKA nested strided tensors or NSTs). As you've found, op coverage there is lacking. More recently, we've been focusing on what's known as ""nested jagged tensors"" or NJTs, which utilize a more modern python tensor subclass based approach and are able to be integrated with torch.compile. Op coverage here should be much better; for example, the entire class of unary pointwise ops is supported outofthebox for NJTs. I'd recommend trying this for your use case and reporting if you happen to find any coverage gaps, which are shrinking.  Apologies for the lack of indepth docs around NST vs. NJT; we've been heavily focused on torch.compile integration for NJTs, which is fairly complex, but worthwhile, given the speedups we've seen. For an example demonstrating such speedups when used with torch.compile, see this tutorial, which constructs a multihead attention module in a way that works with both standard dense tensors and NJTs. > I found I couldn't use torch.vmap with nested tensors, but if there was some way to vectorize the operations across the unbinded nested tensor I'm sure it'd be a lot faster. `torch.vmap` is absolutely something we want to eventually support on nested tensors; it's the ultimate UX goal :) We have very limited support for this now, but it will improve.","Ahh, thank you for the pointer! I'll absolutely move to used the nested jagged tensors. I'm also very excited to hear `torch.vmap` is on the roadmap!"
590,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Gradients are `None` when using `torch.utils.checkpoint.checkpoint`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug In Hugging Face transformers for some model architectures the gradients of trained models are being set to `None` when one uses gradient checkpointing.  I am not sure if this is a bug, but I managed to reproduce the behaviour in the snippet below.   Note this happens also if one uses `use_reentrant=False`:   Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Gradients are `None` when using `torch.utils.checkpoint.checkpoint`," ğŸ› Describe the bug In Hugging Face transformers for some model architectures the gradients of trained models are being set to `None` when one uses gradient checkpointing.  I am not sure if this is a bug, but I managed to reproduce the behaviour in the snippet below.   Note this happens also if one uses `use_reentrant=False`:   Versions ",2023-10-30T15:05:14Z,module: checkpoint triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/112397,cc:  ,Hey The error is only because the `head` module there is not used during the forward pass. So no gradient is populated for it. Closing as expected behavior.
594,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/condabld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch.  errors below: !image  Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch."," ğŸ› Describe the bug RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/condabld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch.  errors below: !image  Versions ",2023-10-30T08:47:11Z,triaged module: CUDACachingAllocator,open,4,15,https://github.com/pytorch/pytorch/issues/112377,"I use `pytorch` in this docker container, `pytorch/pytorch:2.1.0cuda11.8cudnn8devel`",I am getting the same error when running this script on Modal AI. Here is the full stack trace. ,"After some quick testing, I cannot say for anyone else's setup, but this issue only occurs when using the T4 GPU on Modal's platform, and doesn't occur with any other GPU.",Running to the same issue with `tiiuae/falcon40b`.  Any ideas how to get around this? ,"Seems like I got the point, My GPU is MIG(I don't know this arch), and loading Llama213B on it will cause OOM, even if the memory is almost 80Gb, don't know why. I tested smaller model like Bert, everything was fine  !image","Hi from Modal, our users have been facing this PyTorch error instead of the regular CUDA OOM message.  for you specifically, Stable Diffusion XL needs more memory than is available on T4s, so you may be running out of memory (and it should work with A10Gs).","> Stable Diffusion XL needs more memory than is available on T4s, so you may be running out of memory (and it should work with A10Gs). I ended up going with A10G GPUs, thanks for the insight!","I get the same error. GPU: A100 10GB pytorch: this nvidia pytorch container) (commit) I'm testing the memory consumption of a function so I'm trying to catch a `torch.cuda.OutOfMemoryError`.  It worked with V100 GPU with this nvidia pytorch container (commit) But now it throws this weird error when OOM happens, which is harder to catch and not so intuitive.",I got this error and I fixed it by reinstalling an earlier version of the transformers package. ,> I got this error and I fixed it by reinstalling an earlier version of the transformers package. which version of transformers?,"I got a similar error while running my training code. Weird thing is it is happening everytime after running for 18853 iterations at same point in the code. Memory: DDR4 32GB CPU: AMD EPYC 7713 64Core Processor Storage: 1.92TB NVMe SSD +240GB 6Gbps SATA 2.5"" SSD (for OS) GPU: NVIDIA A100 80GB !image","> Seems like I got the point, My GPU is MIG(I don't know this arch), and loading Llama213B on it will cause OOM, even if the memory is almost 80Gb, don't know why. I tested smaller model like Bert, everything was fine Hey , I am facing the same issue while finetuning a gemma22b and I think it is due to the GPU I am using :  `'NVIDIA H100 80GB HBM3 MIG 2g.20gb'` Did you find any work around to fix this? Is this a memory issue?"," In my experience, this error is thrown when the GPU used is not enough to run the training run, and instead of saying that it ran out of memory, it throws this error."," So, like I was trying to use PEFT for finetuning a gemma22b model, and even if I try it with 10 training data points, it is throwing this error. I have 20GB VRAM of a h100 GPU, should I be using a GPU with more vram? or as stated earlier this might be the issue with MIG? Am really new in this space so sorry if I asked something silly."," I don't know much about gemma2b model but check the size of the model (no of parameters) that will give you rough idea about how much size the model occupies. You can verify the memory issue by running with a single data point as batch size and observing the GPU memory in realtime (run ~watch n 0.1 nvidiasmi~ in a new terminal before running the training script), if it occurs even when GPU is not full then the problem is different."
1063,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TorchElastic] Add option to configure log prefix for each rank)ï¼Œ å†…å®¹æ˜¯ (Summary: Add an ability to customize log lines and addtional template like behavior to enrich log information. Motivation: a) Log stream processing/aggregation gains additional value when it includes information about the global rank. Extension to that is that it will be easier to map ranks to hosts from log stream information (less relevant at the moment) b) Users can easily map the failure to the right rank without matching node rank offset+local rank. Implementation  BC change  keeps the logs line prefix as `[]:`  Optional env variable TORCHELASTIC_LOG_LINE_HEADER that will be used as a prefix when specified and currently exposes `role_name`, `rank` and `local_rank` variables that will be bound when agent assigns the ranks. Test Plan: CI https://fburl.com/mlhub/mzx5xspv Differential Revision: D50584590)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,[TorchElastic] Add option to configure log prefix for each rank,"Summary: Add an ability to customize log lines and addtional template like behavior to enrich log information. Motivation: a) Log stream processing/aggregation gains additional value when it includes information about the global rank. Extension to that is that it will be easier to map ranks to hosts from log stream information (less relevant at the moment) b) Users can easily map the failure to the right rank without matching node rank offset+local rank. Implementation  BC change  keeps the logs line prefix as `[]:`  Optional env variable TORCHELASTIC_LOG_LINE_HEADER that will be used as a prefix when specified and currently exposes `role_name`, `rank` and `local_rank` variables that will be bound when agent assigns the ranks. Test Plan: CI https://fburl.com/mlhub/mzx5xspv Differential Revision: D50584590",2023-10-30T01:12:36Z,fb-exported Merged ciflow/trunk release notes: distributed (tools),closed,0,27,https://github.com/pytorch/pytorch/issues/112357,The committers listed above are authorized under a signed CLA.:white_check_mark: login: kurman / name: Kurman Karabukaev  (2add7bb5afa72928a8912968c5f54563c30d91b2),This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,  do you mind helping merging the changes?,> more expressive the default? printing global rank? And I think we need to be maintain existing setup so we need to optin into a new behavior.,"Thanks for doing this, its been a longtime TODO for me. Overall I think rather than giving users an option to customize the prefix, we should just default it to `[{role}{global_rank}][{local_rank]}]:` (example: `[trainer16][0]:`). Yes it breaks BC but it'll only meaningfully break BC if someone is relying on the prefix to parse logs (unless someone in Meta is doing this).",This pull request was **exported** from Phabricator. Differential Revision: D50584590,"> someone is relying on the prefix to parse logs Logs are being parsed now so once we migrate to new format, we can then make your recommended pattern a default.",This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,This pull request was **exported** from Phabricator. Differential Revision: D50584590,"> * Single place for prefix default > * Being more consistent with naming (log_prefix vs log_line_prefix_template, vs log_prefix_template) > * Seemingly different prefix defaults between torch.distributed.run and (elastic_agent,  TailLogs) Addressed all of them. One thing that I believe is outstanding is making log_line_prefix_headers nonoptional, but that we can follow up in different less risky PR as a refactoring change.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2051,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exception raised from storage_offset_default when ExecutionTraceObserver() is used to capture traces in torch.compile() mode)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Exception in execution trace observer raised from storage_offset_default when ExecutionTraceObserver() is used to capture traces in torch.compile() mode. No such exception seen when dynamic=False is set for torch.compile().  Error logs   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.7 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.120+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          2 Online CPU(s) list:             0,1 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Exception raised from storage_offset_default when ExecutionTraceObserver() is used to capture traces in torch.compile() mode," ğŸ› Describe the bug Exception in execution trace observer raised from storage_offset_default when ExecutionTraceObserver() is used to capture traces in torch.compile() mode. No such exception seen when dynamic=False is set for torch.compile().  Error logs   Minified repro   Versions PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.7 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.120+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          2 Online CPU(s) list:             0,1 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz ",2023-10-27T06:16:55Z,triaged oncall: pt2 module: dynamic shapes module: dynamo,closed,0,1,https://github.com/pytorch/pytorch/issues/112235,"I get a hang when trying to run the provided script, but I get the same exception (Cannot call storage_offset() on tensor with symbolic sizes/strides) and then a segfault when I use eager backend. For example:  ,  for profiler"
296,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add config to disable TransformerEncoder/MHA fastpath)ï¼Œ å†…å®¹æ˜¯ (  CC(Add config to disable TransformerEncoder/MHA fastpath))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add config to disable TransformerEncoder/MHA fastpath,  CC(Add config to disable TransformerEncoder/MHA fastpath),2023-10-26T23:34:48Z,Merged ciflow/trunk release notes: nn topic: improvements,closed,0,12,https://github.com/pytorch/pytorch/issues/112212, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/mikaylagawarecki/156/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/112212`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/mikaylagawarecki/156/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/112212`)", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/mikaylagawarecki/156/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/112212`)"," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
483,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DCP] Add fsspec.transaction context when writing checkpoint to storage)ï¼Œ å†…å®¹æ˜¯ (Summary: Adding fsspec.transaction to safeguard checkpointing writing. With the context, it should only commit if there was no exception and discard otherwise. Test Plan:  Reviewed By: rohanvarma Differential Revision: D50701929)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[DCP] Add fsspec.transaction context when writing checkpoint to storage,"Summary: Adding fsspec.transaction to safeguard checkpointing writing. With the context, it should only commit if there was no exception and discard otherwise. Test Plan:  Reviewed By: rohanvarma Differential Revision: D50701929",2023-10-26T20:27:59Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/112191,The committers listed above are authorized under a signed CLA.:white_check_mark: login: wz337 / name: Iris Z  (167491bd4acedb85a7e6e92b544f1b3e76259e51),This pull request was **exported** from Phabricator. Differential Revision: D50701929,This pull request was **exported** from Phabricator. Differential Revision: D50701929,This pull request was **exported** from Phabricator. Differential Revision: D50701929,This pull request was **exported** from Phabricator. Differential Revision: D50701929,This pull request was **exported** from Phabricator. Differential Revision: D50701929, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
591,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] remove GPT2ForSequenceClassification from ci skip list)ï¼Œ å†…å®¹æ˜¯ (  CC([Inductor] remove GPT2ForSequenceClassification from ci skip list) **Summary** As discussed in  CC(GPT2ForSequenceClassification fails accuracy in cpu inference after HF version upgrade), the accuracy issue of `GPT2ForSequenceClassification` has been fixed in https://github.com/pytorch/pytorch/pull/108690. Remove it from CI Skip list. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[Inductor] remove GPT2ForSequenceClassification from ci skip list,"  CC([Inductor] remove GPT2ForSequenceClassification from ci skip list) **Summary** As discussed in  CC(GPT2ForSequenceClassification fails accuracy in cpu inference after HF version upgrade), the accuracy issue of `GPT2ForSequenceClassification` has been fixed in https://github.com/pytorch/pytorch/pull/108690. Remove it from CI Skip list. ",2023-10-26T00:36:25Z,open source Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/112100, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: inductor / linuxjammycpupy3.8gcc11inductor / test (inductor_huggingface_cpu_accuracy, 1, 1, linux.12xlarge), inductor / linuxjammycpupy3.8gcc11inductor / test (inductor_huggingface_dynamic_cpu_accuracy, 1, 1, linux.12xlarge) Details for Dev Infra team Raised by workflow job ",GPT2ForSequenceClassification still fails accuracy on CI?,"> GPT2ForSequenceClassification still fails accuracy on CI? Yean, it passes in my local system. Not sure the difference yet.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"After rebasing to master, `GPT2ForSequenceClassification` pass the accuracy check now."
598,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Grandfather in torchgen'ed aten ops to torch.Tag.pt2_compliant_tag)ï¼Œ å†…å®¹æ˜¯ (  CC(Update how Dynamo decides to graph break on an OpOverloadPacket)  CC(Add way to determine which overload an OpOverloadPacket will resolve to)  CC(Add getAllSortedOperatorsFor helper function)  CC(Grandfather in torchgen'ed aten ops to torch.Tag.pt2_compliant_tag) In torchgen, we add the pt2_compliant_tag to all aten ops. Test Plan:  new test)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Grandfather in torchgen'ed aten ops to torch.Tag.pt2_compliant_tag,"  CC(Update how Dynamo decides to graph break on an OpOverloadPacket)  CC(Add way to determine which overload an OpOverloadPacket will resolve to)  CC(Add getAllSortedOperatorsFor helper function)  CC(Grandfather in torchgen'ed aten ops to torch.Tag.pt2_compliant_tag) In torchgen, we add the pt2_compliant_tag to all aten ops. Test Plan:  new test",2023-10-25T19:59:58Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/112053, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2000,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Core dumped after materializing a model on `meta` device to `cuda` (ROCm))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I created a model on `meta` device and materialized it on `cuda` device (ROCm machine). But when I run the forward pass:  I get the following error:  Which works fine on an NVIDIA machine `cu118`  Versions PyTorch version: 2.1.0+rocm5.6 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.6.310618c743ae5d OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.084genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Instinct MI250X/MI250 Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.6.31061 MIOpen runtime version: 2.20.0 Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             128 Online CPU(s) list:                0127 Vendor ID:                          AuthenticAMD Model name:                         AMD EPYC 7763 64Core Processor CPU family:                         25 Model:                              1 Thread(s) per core:                 1 Core(s) per socket:                 64 Socket(s):                          2 Stepping:                           1 Frequency boost:                    enabled CPU max MHz:                        3529.0520 CPU min MHz:             )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Core dumped after materializing a model on `meta` device to `cuda` (ROCm)," ğŸ› Describe the bug I created a model on `meta` device and materialized it on `cuda` device (ROCm machine). But when I run the forward pass:  I get the following error:  Which works fine on an NVIDIA machine `cu118`  Versions PyTorch version: 2.1.0+rocm5.6 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.6.310618c743ae5d OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.084genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Instinct MI250X/MI250 Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.6.31061 MIOpen runtime version: 2.20.0 Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             128 Online CPU(s) list:                0127 Vendor ID:                          AuthenticAMD Model name:                         AMD EPYC 7763 64Core Processor CPU family:                         25 Model:                              1 Thread(s) per core:                 1 Core(s) per socket:                 64 Socket(s):                          2 Stepping:                           1 Frequency boost:                    enabled CPU max MHz:                        3529.0520 CPU min MHz:             ",2023-10-25T08:58:10Z,module: rocm triaged,closed,0,8,https://github.com/pytorch/pytorch/issues/112012,I am able to reproduce this. Will look into it.,"  I compared the behaviors of your code on Nvidia GPU and AMD Gpu, and found that, after the model is loaded to(""meta""), and then sent to ""cuda"" with the  call, the parameters in NVidia side are zeroed out, while on AMD side, they are garbage values (for example, inspect ). That is why the index exception happened when doing an embedding lookup. So, the question is: Why you want to do this?  Is this supposed to be undefined behavior anyway, since meta device is used to contain the shape only, without real data.  So, when you loaded the model to the meta device, it does not contain storage, nor the valid model parameters. So, we should not expect when .to_empty(""cuda"") is called, the valid parameter will be there anyway. Therefore it is undefined.","I don't see how that would cause an index exception, shouldn't it just return one of the garbage values in the lookup table ? I use this feature to benchmark model/hardware performance without downloading, loading or initializing the weights, ie a fast way to materialize a model.","  I have updated your test script with below dummy_input, and now it ran fine:  Can you update on your side and confirm? Thanks.","The problem is that, if no such parameters are sent, below code will create those parameter with undefined numbers that are much bigger than the corresponding embedding's original size. !image",  Please confirm. We are planning to close this issue in a few days if there is no response. Thanks.,Thank you  I confirm it works when all inputs are specified,"I'm getting this same error  in some code I have, but have trouble debugging it. Since there is no Python callstack being reported, and pdb also doesn't work, I don't even know of a good way to figure out where in my code this is happening. Any advice?"
1817,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Running compiled functions on unsupported devices yields uninformative error message)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I wanted to see if mps backend was supported, and based on the error message I don't think it is we should make that more clear to the user Repro:  Yields error:  We should probably give an error like `Device mps not supported`  Versions PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.5.2 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: version 3.26.4 Libc version: N/A Python version: 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS13.5.2arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Max Versions of relevant libraries: [pip3] numpy==1.26.1 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchvision==0.16.0 [conda] nomkl                     3.0                           0 [conda] numpy                     1.21.5           py39h42add53_3 [conda] numpybase                1.21.5           py39hadd41eb_3 [conda] numpydoc                  1.4.0            py39hca03da5_0 [conda] torch                     1.12.1                   pypi_0    pypi [conda] torchaudio                0.12.1                   pypi_0    pypi [conda] torchvision               0.13.1                   pypi_0    pypi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Running compiled functions on unsupported devices yields uninformative error message," ğŸ› Describe the bug I wanted to see if mps backend was supported, and based on the error message I don't think it is we should make that more clear to the user Repro:  Yields error:  We should probably give an error like `Device mps not supported`  Versions PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.5.2 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: version 3.26.4 Libc version: N/A Python version: 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS13.5.2arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Max Versions of relevant libraries: [pip3] numpy==1.26.1 [pip3] torch==2.1.0 [pip3] torchaudio==2.1.0 [pip3] torchvision==0.16.0 [conda] nomkl                     3.0                           0 [conda] numpy                     1.21.5           py39h42add53_3 [conda] numpybase                1.21.5           py39hadd41eb_3 [conda] numpydoc                  1.4.0            py39hca03da5_0 [conda] torch                     1.12.1                   pypi_0    pypi [conda] torchaudio                0.12.1                   pypi_0    pypi [conda] torchvision               0.13.1                   pypi_0    pypi",2023-10-25T06:15:26Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/111999,I'm not sure if/how we want to approach this. I have a PR https://github.com/pytorch/pytorch/pull/112001 if someone wants to take a look,"Inductor relies on Triton to generate GPU code and Triton has no MPS support AFAICT, see:  https://discuss.pytorch.org/t/questionsaboutpytorch20andmps/168552  https://github.com/openai/triton/issues/2048  https://github.com/openai/triton/issues/194 (about CPU, but still)."
569,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DTensor] view operator is not compiled when receiving Dtensor + FakeTensor as an input)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I parallelized the Conv1D layer (from transformers library) with DTensor and `torch.compile` with the following code, the error occurs. It seems that `view` operator is not compiled properly when the input tensor is Dtensor wrapping FakeTensor.   Error message   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[DTensor] view operator is not compiled when receiving Dtensor + FakeTensor as an input," ğŸ› Describe the bug When I parallelized the Conv1D layer (from transformers library) with DTensor and `torch.compile` with the following code, the error occurs. It seems that `view` operator is not compiled properly when the input tensor is Dtensor wrapping FakeTensor.   Error message   Versions  ",2023-10-25T03:04:53Z,triaged module: dynamo module: dtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/111994,I was unable to reproduce this. I compiled successfully with your script on latest main.   Do I need to run with `torch._dynamo.config.optimize_ddp=True`?,I tried this on latest branch(c9e658f1d) and checked that the problem doesn't occur anymore. chuang thanks for the comment!
1993,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix unit tests and add logging for Inductor intra-graph reordering)ï¼Œ å†…å®¹æ˜¯ (1. Fix code to make unit tests pass (incl. collect_env issue called out by   in https://github.com/pytorch/pytorch/pull/108091discussion_r1362901686). 2. Add logging for Inductor intragraph reordering passes (`TORCH_LOGS=""overlap""`), for easier debugging. Example log: ``` [rank0]:[20231024 16:28:26,446] [0/0] torch._inductor.comms.__overlap: [DEBUG] ==== Visualize overlap before reordering pass  ==== [rank0]:[20231024 16:28:26,446] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf0) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf1) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf2) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf3) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf4) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf5) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf6) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf7) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf8) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf9) [rank0]:[20231024 16:28:26,447] [)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix unit tests and add logging for Inductor intra-graph reordering,"1. Fix code to make unit tests pass (incl. collect_env issue called out by   in https://github.com/pytorch/pytorch/pull/108091discussion_r1362901686). 2. Add logging for Inductor intragraph reordering passes (`TORCH_LOGS=""overlap""`), for easier debugging. Example log: ``` [rank0]:[20231024 16:28:26,446] [0/0] torch._inductor.comms.__overlap: [DEBUG] ==== Visualize overlap before reordering pass  ==== [rank0]:[20231024 16:28:26,446] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf0) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf1) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf2) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf3) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf4) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ComputedBuffer (size=[4, 4], stride=[4, 1]) (buf5) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] InPlaceHint (size=[4, 4], stride=[4, 1]) (buf6) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] AllReduce (size=[4, 4], stride=[4, 1]) (buf7) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] Wait (size=[4, 4], stride=[4, 1]) (buf8) [rank0]:[20231024 16:28:26,447] [0/0] torch._inductor.comms.__overlap: [DEBUG] ExternKernelOut (extern_kernels.mm) (size=[4, 4], stride=[4, 1]) (buf9) [rank0]:[20231024 16:28:26,447] [",2023-10-25T00:12:50Z,Merged module: inductor module: dynamo ciflow/inductor,closed,1,4,https://github.com/pytorch/pytorch/issues/111981," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","btw, the logging looks pretty cool ;)"," merge f ""unrelated failures"" "," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
523,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support 'BaseOutput' and subclasses from 'diffusers' in dynamo)ï¼Œ å†…å®¹æ˜¯ (  CC(Support 'BaseOutput' and subclasses from 'diffusers' in dynamo) Extending the workarounds for `transformers` `ModelOutput` to cover `diffusers` `BaseOutput`. Together with https://github.com/huggingface/diffusers/pull/5459 it should unblock export for `diffusers` models. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Support 'BaseOutput' and subclasses from 'diffusers' in dynamo,  CC(Support 'BaseOutput' and subclasses from 'diffusers' in dynamo) Extending the workarounds for `transformers` `ModelOutput` to cover `diffusers` `BaseOutput`. Together with https://github.com/huggingface/diffusers/pull/5459 it should unblock export for `diffusers` models. ,2023-10-24T23:10:24Z,open source Merged ciflow/trunk topic: new features module: dynamo ciflow/inductor release notes: dynamo,closed,0,9,https://github.com/pytorch/pytorch/issues/111978," PTAL, thanks!","There are some weird segfaults happening in CI, I'm trying to debug. So far I cannot repro on my end, but it seems consistent on CI machines.", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 5ed6edfaadc17d027d4324b4a837f992c7838f3f` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1066,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PT2] Graph break in forward pre-hook skips compiling forward for `nn.Transformer`)ï¼Œ å†…å®¹æ˜¯ (If there is a graph break in the forward prehook (`nn.Module.register_forward_pre_hook()`), then the `nn.Transformer.forward()` gets skipped from being compiled.  `TORCH_LOGS=""+dynamo""` with graph break in the hook (no compiled function!): P863664749 `TORCH_LOGS=""+dynamo""` without graph break in the hook (compiled function!): P863667227  said that this is ""~expected"" and to . FWIW, I am less concerned with compiling `nn.Transformer` specifically. This is related to FSDP, where I want to `torch._dynamo.disable` the module hooks but still compile the module forward. If there is not a good general solution, then we can revisit the FSDP issue specifically (i.e. we do not need to oneoff fix this for `nn.Transformer` since it is only being used as the example model here). Version: main cc:      )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[PT2] Graph break in forward pre-hook skips compiling forward for `nn.Transformer`,"If there is a graph break in the forward prehook (`nn.Module.register_forward_pre_hook()`), then the `nn.Transformer.forward()` gets skipped from being compiled.  `TORCH_LOGS=""+dynamo""` with graph break in the hook (no compiled function!): P863664749 `TORCH_LOGS=""+dynamo""` without graph break in the hook (compiled function!): P863667227  said that this is ""~expected"" and to . FWIW, I am less concerned with compiling `nn.Transformer` specifically. This is related to FSDP, where I want to `torch._dynamo.disable` the module hooks but still compile the module forward. If there is not a good general solution, then we can revisit the FSDP issue specifically (i.e. we do not need to oneoff fix this for `nn.Transformer` since it is only being used as the example model here). Version: main cc:      ",2023-10-24T21:39:06Z,low priority triaged oncall: pt2 module: dynamo,open,0,4,https://github.com/pytorch/pytorch/issues/111966,"This is expected behavior, but I don't think it's correct. Let me explain the reason behind the behavior: * Dynamo is tracing and inlining ; * Then it recursively inlines  if there is; * Graph breaks inside of  caused the compilation of  failed and fallback to eager. * Dynamo recursively evaluates the frame of each function inside of , e.g., , but it doesn't pass this line and skips this frame.   * The reason behind this rule is, we skip all recursively called frame if the upper level frame is skipped, which is true in most of the cases. Otherwise, we would recursively trace many low level frames, which is not efficient and may trigger a lot of features that Dynamo doesn't supported. This is a pretty reasonable case to not skip the underlying frame, I think we can support this. The straightforward idea is to check if the function is , but the challenge part is we can't get the function object from frame, so we have to pass it with . I'll work on a PR for this.","I don't agree with the diagnosis here. The part which I do not agree with is the subbullet here: > The reason behind this rule is, we skip all recursively called frame if the upper level frame is skipped, which is true in most of the cases. Otherwise, we would recursively trace many low level frames, which is not efficient and may trigger a lot of features that Dynamo doesn't supported. I don't think we ""skip all recursively called frame if the upper level frame is skipped"". If you look at the current skipfiles.check_verbose, there's nothing in it that suggests that we will start skipping things if an outer frame was skipped (which would require some sort of global context tracking if we had skipped an outer frame)  Additionally, when I modify the example program so that I am calling a userdefined Transformer, this works fine:  In other words, the root cause is that by default, we are unwilling to torch.compile NN modules that are directly defined in PyTorch library, unless there is some user code that calls it. But we should then change the policy on a casebycase basis. Do you want to compile nn.Linear if it's called all by itself? Probably not. , would you be happy if it were *only* Transformer that got special cased in this way? You implied it doesn't matter. I just want to point out that for any realistic use case of FSDP, what will be wrapped is a user module, and we will trace into it anyway."," Thanks for the context! This makes sense. I think we do not need to special case `nn.Transformer` until we have an explicit ask for it (as most people using FSDP are not using `nn.Transformer` but rather some userdefined transformer like you mentioned). For userdefined modules, I can actually get perparametersharding FSDP to compile with graph breaks on FSDP logic (like existing FSDP) using `FSDPManagedNNModuleVariable` (though `torch.distributed._composble.checkpoint` does not work), so I think that the issue mentioned here is not too important anymore. Sorry everyone for the confusion!",Marking low prio as this is expected behavior and User defined nn modules work well. 
2047,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OOM when saving model(lora adapter), seems the clause ""FullyShardedDataParallel(model,...)"" will directly cause the OOM.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am using llamarecipes to finetune LLMs. After training 34Bcodellama2 with LoRA(the training went on well), I got OOM error when model.save_pretrained. I tried different PEFT versions, from v 0.3.0 to source code, do not work. It is said PEFT0.2.0 will work, but llamarecipes can't work with PEFT0.2.0 I don't know the root cause of the problem( llamarecipes, PEFT, or FSDP from pytorch ?) , so I submitted this issue to pytorch and llamarecipes. Hope someone can help! I modified the code and added save_pretrained just after the model loaded. It shows that OOM occurs just after FSDP(FullyShardedDataParallel) operation, it's nothing to do with the training process.     from torch.distributed.fsdp import (         FullyShardedDataParallel as FSDP,     )     some codes  omitted     mixed_precision_policy, wrapping_policy = get_policies(fsdp_config, rank)     my_auto_wrapping_policy = fsdp_auto_wrap_policy(model, LlamaDecoderLayer)      add save model before FSDP     print(""before FSDP save model"")     model.save_pretrained(""./test_pretrained1"")     model = FSDP(         model,         auto_wrap_policy= my_auto_wrapping_policy if train_config.use_peft else wrapping_policy,         cpu_offload=CPUOffload(offload_params=True) if fsdp_config.fsdp_cpu_offload else None,         mixed_precision=mixed_precision_policy if not fsdp_config.pure_bf16 else None,         sharding_strategy=fsdp_config.sharding_strategy,         device_id=torch.cuda.current_device(),         limit_all_gathers=True,         add use_orig_params=True for adapter         use_orig_params=True,         sync_module_states=train_config.low_cpu_fsdp,         param_init_fn=lambda module: module.to_empty(device=torch.device(""cuda""), recurse=F)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",peft,"OOM when saving model(lora adapter), seems the clause ""FullyShardedDataParallel(model,...)"" will directly cause the OOM."," ğŸ› Describe the bug I am using llamarecipes to finetune LLMs. After training 34Bcodellama2 with LoRA(the training went on well), I got OOM error when model.save_pretrained. I tried different PEFT versions, from v 0.3.0 to source code, do not work. It is said PEFT0.2.0 will work, but llamarecipes can't work with PEFT0.2.0 I don't know the root cause of the problem( llamarecipes, PEFT, or FSDP from pytorch ?) , so I submitted this issue to pytorch and llamarecipes. Hope someone can help! I modified the code and added save_pretrained just after the model loaded. It shows that OOM occurs just after FSDP(FullyShardedDataParallel) operation, it's nothing to do with the training process.     from torch.distributed.fsdp import (         FullyShardedDataParallel as FSDP,     )     some codes  omitted     mixed_precision_policy, wrapping_policy = get_policies(fsdp_config, rank)     my_auto_wrapping_policy = fsdp_auto_wrap_policy(model, LlamaDecoderLayer)      add save model before FSDP     print(""before FSDP save model"")     model.save_pretrained(""./test_pretrained1"")     model = FSDP(         model,         auto_wrap_policy= my_auto_wrapping_policy if train_config.use_peft else wrapping_policy,         cpu_offload=CPUOffload(offload_params=True) if fsdp_config.fsdp_cpu_offload else None,         mixed_precision=mixed_precision_policy if not fsdp_config.pure_bf16 else None,         sharding_strategy=fsdp_config.sharding_strategy,         device_id=torch.cuda.current_device(),         limit_all_gathers=True,         add use_orig_params=True for adapter         use_orig_params=True,         sync_module_states=train_config.low_cpu_fsdp,         param_init_fn=lambda module: module.to_empty(device=torch.device(""cuda""), recurse=F",2023-10-24T07:47:04Z,oncall: distributed module: memory usage triaged module: fsdp,open,0,2,https://github.com/pytorch/pytorch/issues/111900,I think it would be best if llamarecipes folks take a look first. cc:  ,"Thanks for your quick response   . If anything needs me, please do contact me!"
689,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update torch.load examples to encourage best security practices)ï¼Œ å†…å®¹æ˜¯ (`torch.load` without setting `weights_only` is unsafe. See  CC(Revisit security implications of #31875) and linked items there for some discussion. This task is to update examples at https://pytorch.org/docs/stable/generated/torch.load.html to use `weights_only=True` where possible, and explicit `weights_only=False` with a comment about unsafety only where needed (you need to verify if `weights_only=True` works for a particular example). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Update torch.load examples to encourage best security practices,"`torch.load` without setting `weights_only` is unsafe. See  CC(Revisit security implications of #31875) and linked items there for some discussion. This task is to update examples at https://pytorch.org/docs/stable/generated/torch.load.html to use `weights_only=True` where possible, and explicit `weights_only=False` with a comment about unsafety only where needed (you need to verify if `weights_only=True` works for a particular example). ",2023-10-24T00:43:05Z,module: docs triaged medium topic: docs docathon-h2-2023,closed,1,1,https://github.com/pytorch/pytorch/issues/111876,/assigntome
2179,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix docstring errors in _VF.py, __config__.py, _lobpcg.py, random.py, _linalg_utils.py, _namedtensor_internals.py, torch_version.py, __future__.py, _classes.py, _sources.py, _lowrank.py, _vmap_internals.py, _storage_docs.py, quasirandom.py, _appdirs.py)ï¼Œ å†…å®¹æ˜¯ ( **File**: `torch/__config__.py`, **Line**: 5, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__config__.py`, **Line**: 5, **Description**: First line should end with a period (not 'e')  **File**: `torch/__config__.py`, **Line**: 16, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should end with a period (not 's')  **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__future__.py`, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__future__.py`, **Line**: 1, **Description**: First line should end with a period (not 's')  **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: First line should end with a period (not 'm')  **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should end with a period (not 'e')  **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should be in imperative mood; try rephrasing (found 'This')  **File**: `torch/_lowrank.py`, **Line**: 1, **Description**: Oneline docstring should fit on one line with quotes (found 2)  **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: No blank lines allowed after function docstring (found 1)  **Fi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Fix docstring errors in _VF.py, __config__.py, _lobpcg.py, random.py, _linalg_utils.py, _namedtensor_internals.py, torch_version.py, __future__.py, _classes.py, _sources.py, _lowrank.py, _vmap_internals.py, _storage_docs.py, quasirandom.py, _appdirs.py"," **File**: `torch/__config__.py`, **Line**: 5, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__config__.py`, **Line**: 5, **Description**: First line should end with a period (not 'e')  **File**: `torch/__config__.py`, **Line**: 16, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should end with a period (not 's')  **File**: `torch/__config__.py`, **Line**: 21, **Description**: First line should be in imperative mood (perhaps 'Return', not 'Returns')  **File**: `torch/__future__.py`, **Line**: 1, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/__future__.py`, **Line**: 1, **Description**: First line should end with a period (not 's')  **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/_appdirs.py`, **Line**: 8, **Description**: First line should end with a period (not 'm')  **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: 1 blank line required between summary line and description (found 0)  **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should end with a period (not 'e')  **File**: `torch/_appdirs.py`, **Line**: 501, **Description**: First line should be in imperative mood; try rephrasing (found 'This')  **File**: `torch/_lowrank.py`, **Line**: 1, **Description**: Oneline docstring should fit on one line with quotes (found 2)  **File**: `torch/_lowrank.py`, **Line**: 17, **Description**: No blank lines allowed after function docstring (found 1)  **Fi",2023-10-23T21:12:23Z,module: docs triaged medium,closed,0,1,https://github.com/pytorch/pytorch/issues/111841,this was a test. closing
2014,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multiplying the same tensor as part of a batch results in numerically different outputs)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Multiplying a tensor A of shape [1, 4, 1024] by tensor B [1024, 2048] and then slicing the index [:, 0:1, :] of the output results in numerically different output than if you slice tensor A first and then do the multiplication.  Output:   Versions Collecting environment information... PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.01045awsx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 535.104.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Multiplying the same tensor as part of a batch results in numerically different outputs," ğŸ› Describe the bug Multiplying a tensor A of shape [1, 4, 1024] by tensor B [1024, 2048] and then slicing the index [:, 0:1, :] of the output results in numerically different output than if you slice tensor A first and then do the multiplication.  Output:   Versions Collecting environment information... PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.01045awsx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 535.104.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits",2023-10-23T21:10:40Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/111840,This is expected given that we don't guarantee numerical consistency when you try to compute a quantity in two different ways that are analytically the same. If I were to guess there are two paths depending on the size of the tensor and the floats are accumulated in a different order in those two paths.,Thank you for clarifying! I'm just surprised that the difference is so big and it wouldn't pass a torch.allclose() check. But good to know for the future!,"Hi, I found this issue related to my problem. Without opening a new issue, I will follow up on this. It's basically the same bug  the order of the operations produces different results. But it's about the associativity of matrix multiplication, and the bug only appears if the second dimension is a prime number larger than the 309th (here the wikipedia list of prime numbers).  Unfortunately, I do not have access to another similar machine to determine if it's a hardware issue or a configuration issue, but I tried both an A100 and Google colab and the bug does not show up. Of course, the two tensors behave as expected if passed to cpu or cast to tf64.   Output   Versions "
947,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Draft] Fix problem on the MultiheadAttention (dynamic_axes))ï¼Œ å†…å®¹æ˜¯ (The MultiheadAttention is currently not supported by the `dynamic_axes` of `torch.onnx.export`. This implies that the entire pytorch Transformer ecosystem is not either. When trying to export to onnx, the layer freezes with the sequence that has been used as a dummy, not allowing it to be executed with other sizes. Example:  The author of this solution:  CC(when convert to onnx with dynamix_axis,  the Reshape op  value is always the same as static,  dynamic_axis is useless, it cant't inference right shape dynamically)issuecomment1752324155  PD: It is my first PR in torch, if there is the test it must be somewhere else or format the PR in another way, please tell me. Thank you Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Draft] Fix problem on the MultiheadAttention (dynamic_axes),"The MultiheadAttention is currently not supported by the `dynamic_axes` of `torch.onnx.export`. This implies that the entire pytorch Transformer ecosystem is not either. When trying to export to onnx, the layer freezes with the sequence that has been used as a dummy, not allowing it to be executed with other sizes. Example:  The author of this solution:  CC(when convert to onnx with dynamix_axis,  the Reshape op  value is always the same as static,  dynamic_axis is useless, it cant't inference right shape dynamically)issuecomment1752324155  PD: It is my first PR in torch, if there is the test it must be somewhere else or format the PR in another way, please tell me. Thank you Fixes ISSUE_NUMBER",2023-10-23T13:05:12Z,open source Stale release notes: onnx,closed,1,4,https://github.com/pytorch/pytorch/issues/111800,"   :x: The email address for the commit (83b0c752d2458284ed36e036010ac4d6f6d97803, bcdf65534a909881b8dde554949a46a9c0ce99f9, d4fe6afdc622e0a9d1c7f8a9da82417354a97fbc, 2cc4385731f335395d0d323b3d9368fce7f94db1) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.","> A lot of nonONNX tests have failed with this change Thanks for you comment.  Yes, I know. I'm trying to figure out how to run the test in my local computer and check why these tests are raising an error.  For now I have had to use the ones that appeared in the CI/CD, but the `RuntimeError` is difficult to interpret without a proper debug. For my part I don't understand it either, because `k` and `v` are forced to have `src_len` (being **S** in the documentation: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) Maybe there is a better way, but I haven't found a way to prevent `x.shape[0]` from being interpreted as a constant parameter by onnx. Is there a way to indicate to onnx that this can be dynamic? How is onnx able to know which .shape is going to change? Something that allows us to maintain the jit trace? Since `x.shape[0]` is an int, it would be very useful to be able to mark dynamic shapes with something like x.dynamic_shape[0]. Or that `x.shape[0]` is an object that can dynamically vary and be cast to int.","> > A lot of nonONNX tests have failed with this change >  > Thanks for you comment. >  > Yes, I know. I'm trying to figure out how to run the test in my local computer and check why these tests are raising an error. For now I have had to use the ones that appeared in the CI/CD, but the `RuntimeError` is difficult to interpret without a proper debug. For my part I don't understand it either, because `k` and `v` are forced to have `src_len` (being **S** in the documentation: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) >  > Maybe there is a better way, but I haven't found a way to prevent `x.shape[0]` from being interpreted as a constant parameter by onnx. Is there a way to indicate to onnx that this can be dynamic? How is onnx able to know which .shape is going to change? >  You would have to wrap the model with `torch.jit.script` to enable dynamic behavior during export. ideally, you can refactor out the code so that only the `x.shape[0]` part is wrapped around JIT scripting. That should help https://pytorch.org/docs/stable/jit.htmlmixingtracingandscripting > Something that allows us to maintain the jit trace? Since `x.shape[0]` is an int, it would be very useful to be able to mark dynamic shapes with something like x.dynamic_shape[0]. Or that `x.shape[0]` is an object that can dynamically vary and be cast to int.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1886,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Runnings SentenceTransformer encoding step causes Docker containers on Mac (Silicon) to crash with code 139)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi! Hopefully there isn't a similar issue already open. I couldn't find one after a search through the issues list. Feel free to mark as duplicate/close if it already exists. I've created this repository with a minimal setup to reproduce the error: https://github.com/sabaimran/reprotorchbug. You just have to clone it and run `dockercompose up` to see the error. Basically it runs the script below in a minimal Docker container:  If you run this code inside of a Docker container (with the appropriate dependencies), it will fail with exit code 139. Pinning the `torch` package to `2.0.1` circumvents the error. See this other relevant issue: https://github.com/docker/formac/issues/7016  Versions Collecting environment information... PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.2.1 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.26.4 Libc version: N/A Python version: 3.11.4 (main, Jul 10 2023, 18:52:37) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS13.2.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Pro Versions of relevant libraries: [pip3] mypyextensions==1.0.0 [pip3] numpy==1.26.1 [pip3] torch==2.1.0 [pip3] torchvision==0.16.0 [conda] Could not collect )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Runnings SentenceTransformer encoding step causes Docker containers on Mac (Silicon) to crash with code 139," ğŸ› Describe the bug Hi! Hopefully there isn't a similar issue already open. I couldn't find one after a search through the issues list. Feel free to mark as duplicate/close if it already exists. I've created this repository with a minimal setup to reproduce the error: https://github.com/sabaimran/reprotorchbug. You just have to clone it and run `dockercompose up` to see the error. Basically it runs the script below in a minimal Docker container:  If you run this code inside of a Docker container (with the appropriate dependencies), it will fail with exit code 139. Pinning the `torch` package to `2.0.1` circumvents the error. See this other relevant issue: https://github.com/docker/formac/issues/7016  Versions Collecting environment information... PyTorch version: 2.1.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.2.1 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.26.4 Libc version: N/A Python version: 3.11.4 (main, Jul 10 2023, 18:52:37) [Clang 14.0.3 (clang1403.0.22.14.1)] (64bit runtime) Python platform: macOS13.2.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Pro Versions of relevant libraries: [pip3] mypyextensions==1.0.0 [pip3] numpy==1.26.1 [pip3] torch==2.1.0 [pip3] torchvision==0.16.0 [conda] Could not collect ",2023-10-20T20:00:53Z,high priority module: crash triaged module: mkldnn module: regression module: arm,closed,1,5,https://github.com/pytorch/pytorch/issues/111695,"Grabbing for myself to get a reproducer, though I guess it's Cortex A53 vs A57 again.   please note that  is equivalent to `device = torch.device(""cpu"")` if I correctly understand the setup you are describing.","Ok, so following code crashes:  With the following backtrace:  I.e. in here ","Ok, after applying https://github.com/pytorch/builder/commit/c5e331c0858e37fedc047707466161dfe0cadff6 to the latest nightly crash is gone, time to cherrypick the change into release/2.1 branch",validation needed,"Works if one uses `torch2.1.1` release candidate, i.e. by running the following: "
550,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Expose filename for tensor/storages created with `from_file` and document torch.from_file)ï¼Œ å†…å®¹æ˜¯ (Fixes  CC(torch.from_file is not documented) Also threads through filename so it is accessible via `t.storage().filename`   CC(Clarify difference between share_memory and from_file)  CC(Expose filename for tensor/storages created with `from_file` and document torch.from_file) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Expose filename for tensor/storages created with `from_file` and document torch.from_file,Fixes  CC(torch.from_file is not documented) Also threads through filename so it is accessible via `t.storage().filename`   CC(Clarify difference between share_memory and from_file)  CC(Expose filename for tensor/storages created with `from_file` and document torch.from_file) ,2023-10-20T18:57:31Z,Merged ciflow/trunk release notes: python_frontend topic: docs,closed,0,2,https://github.com/pytorch/pytorch/issues/111688, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
943,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Change torch.library.impl to accept a device string)ï¼Œ å†…å®¹æ˜¯ (  CC(Higherlevel custom op API, V3)  CC(torch.library: Create helper function `is_functional_schema`)  CC(Rewrite torch.library's documentation)  CC(Change torch.library.impl to accept a device string)  CC(Rename name>qualname in torch.library.impl_abstract) torch.library.impl now accepts a device string (e.g. ""cpu"", ""cuda""). It still accepts DispatchKey strings, but we no longer document this, because using arbitrary DispatchKeys is more for the power users. We map the device string to a DispatchKey and then register the impl for said DispatchKey. A user may also specify multiple device strings at once or specify ""types=default"" to get a CompositeExplicitAutograd registration. Test Plan:  new tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Change torch.library.impl to accept a device string,"  CC(Higherlevel custom op API, V3)  CC(torch.library: Create helper function `is_functional_schema`)  CC(Rewrite torch.library's documentation)  CC(Change torch.library.impl to accept a device string)  CC(Rename name>qualname in torch.library.impl_abstract) torch.library.impl now accepts a device string (e.g. ""cpu"", ""cuda""). It still accepts DispatchKey strings, but we no longer document this, because using arbitrary DispatchKeys is more for the power users. We map the device string to a DispatchKey and then register the impl for said DispatchKey. A user may also specify multiple device strings at once or specify ""types=default"" to get a CompositeExplicitAutograd registration. Test Plan:  new tests",2023-10-20T15:50:22Z,Merged release notes: composability,closed,0,0,https://github.com/pytorch/pytorch/issues/111659
1991,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RFC] Enable Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch IntelExtensionforPyTorch (IPEX) offers an advanced int8mixedbf16 quantization path, which transforms the output of quantized Conv/GEMM operations into the BF16 data type if there is no subsequent quantized operator. This enhancement significantly improves the inference performance of models such as Bert/DistilBert, as the pointwise operators following GEMM will operate with BF16 data instead of FP32. * Here is the example code for how use this feature with IPEX. * Please note that this feature may result in accuracy loss for certain models. With IPEX, we have verified its accuracy in models such as Bert, DistilBert, stable diffusion, and some other LLM models. However, we have also observed accuracy issues in models like vision transformers. * Similarly, we recently recive a feature request in  CC(BFloat16 datatype support in Quantization).  Alternatives We typically have two options to enable this feature.  Option 1: Use Autocast Autocast is naturally employed for BF16 optimization in Inductor. Similarly, we can harness it for PT2E int8mixedbf16 features to generate a pattern like `q > dq > float32_to_bfloat16 > conv > bfloat16_to_fp32 > q > dq`.  * `to_bfloat16` node before conv should be inserted when used Autocast + `torch.compile` together, since conv is in whitelist of Autocast. * As for inserting `bfloat16_to_fp32` node after conv node, we need to extend the implementation of https://github.com/pytorch/pytorch/blob/93a9b1314b4bc88ccddc0aa438d4d332955027a8/torch/ao/quantization/fx/_decomposed.pyL36L64 by add these lines at beginning of this function   Here's an example code snippet:  * Pros: 	* Utilize the existing int8mixedfp32 quantizer and PT2E flow implementation. 	* )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[RFC] Enable Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor," ğŸš€ The feature, motivation and pitch IntelExtensionforPyTorch (IPEX) offers an advanced int8mixedbf16 quantization path, which transforms the output of quantized Conv/GEMM operations into the BF16 data type if there is no subsequent quantized operator. This enhancement significantly improves the inference performance of models such as Bert/DistilBert, as the pointwise operators following GEMM will operate with BF16 data instead of FP32. * Here is the example code for how use this feature with IPEX. * Please note that this feature may result in accuracy loss for certain models. With IPEX, we have verified its accuracy in models such as Bert, DistilBert, stable diffusion, and some other LLM models. However, we have also observed accuracy issues in models like vision transformers. * Similarly, we recently recive a feature request in  CC(BFloat16 datatype support in Quantization).  Alternatives We typically have two options to enable this feature.  Option 1: Use Autocast Autocast is naturally employed for BF16 optimization in Inductor. Similarly, we can harness it for PT2E int8mixedbf16 features to generate a pattern like `q > dq > float32_to_bfloat16 > conv > bfloat16_to_fp32 > q > dq`.  * `to_bfloat16` node before conv should be inserted when used Autocast + `torch.compile` together, since conv is in whitelist of Autocast. * As for inserting `bfloat16_to_fp32` node after conv node, we need to extend the implementation of https://github.com/pytorch/pytorch/blob/93a9b1314b4bc88ccddc0aa438d4d332955027a8/torch/ao/quantization/fx/_decomposed.pyL36L64 by add these lines at beginning of this function   Here's an example code snippet:  * Pros: 	* Utilize the existing int8mixedfp32 quantizer and PT2E flow implementation. 	* ",2023-10-20T07:44:11Z,oncall: quantization triaged,closed,0,18,https://github.com/pytorch/pytorch/issues/111640, ,Hi  This is the proposal I introduced to you during the face to face. Feel free to comment on the options.,"> Option2: During the convert phase, we will examine the observer information to determine if it has been annotated with int8mixedbf16. If the input of a quantization node is in BFloat16 data type, an additional to_float node will be inserted before the quantization node. Following the dequantization node, an additional to_bf16 node will be inserted. That's not what I have in mind for option 2 actually. What I mean is that ""bfloat16"" is just another target quantization dtype, while original Tensor are still assumed to be float32. i.e. if user annotates a tensor as following:  quant flow will do:  This won't be mixing with int8 dtype conversions. Or are you saying int8 dtype conversion is actually needed as well? > Remove the annotation of output at conv/linear in X86InductorQuantizer. qq: is this only needed for bfloat16 use case or is it needed for all use cases?","I feel option 1 is fine if you are OK with having the more coarse grained configurability. there is not much change to the quant flow as you mentioned. although option 2 is also not that complicated either, if what I'm talking about above is what you need for option 2","> I feel option 1 is fine if you are OK with having the more coarse grained configurability. there is not much change to the quant flow as you mentioned. although option 2 is also not that complicated either, if what I'm talking about above is what you need for option 2 Thanks for the comment, so I will start to explore more following option 1. > _qq: is this only needed for bfloat16 use case or is it needed for all use cases?_ It's needed for all use cases: fp32 and bfloat16. I will clean up current annotation of output at conv/linear in `X86InductorQuantizer` with following up PRs. > For option 2: `act > to_bfloat16 (quantize) > to_fp32 (dequantize) > conv > to_bfloat16 (quantize) > to_fp32 (dequantize)`   Some questions about Option 2 you mentioned above: what's the behavior of `to_bfloat16 (quantize)` ? Will it input a Float32 Tensor and output a BFloat16 Tensor.","> Some questions about Option 2 you mentioned above: what's the behavior of `to_bfloat16 (quantize)` ? Will it input a Float32 Tensor and output a BFloat16 Tensor. Yes, to_bfloat16 is the op that quantizes fp32 Tensor to bfloat16 Tensor, it's an ""quantize"" op, just like existing ones, but with torch.bfloat16 as target dtype.","> > Some questions about Option 2 you mentioned above: what's the behavior of `to_bfloat16 (quantize)` ? Will it input a Float32 Tensor and output a BFloat16 Tensor. >  > Yes, to_bfloat16 is the op that quantizes fp32 Tensor to bfloat16 Tensor, it's an ""quantize"" op, just like existing ones, but with torch.bfloat16 as target dtype. Ok, but actually in the Inductor optimization, we expect `dequantize > conv` be fused into a QConv with int8 inputs (as in above **Optimization Inside Inductor** section). If the `quant` outputs a BFloat16 tensor, I think we may not be able to do this optimization. ","> > > Some questions about Option 2 you mentioned above: what's the behavior of `to_bfloat16 (quantize)` ? Will it input a Float32 Tensor and output a BFloat16 Tensor. > >  > >  > > Yes, to_bfloat16 is the op that quantizes fp32 Tensor to bfloat16 Tensor, it's an ""quantize"" op, just like existing ones, but with torch.bfloat16 as target dtype. >  > Ok, but actually in the Inductor optimization, we expect `dequantize > conv` be fused into a QConv with int8 inputs (as in above **Optimization Inside Inductor** section). If the `quant` outputs a BFloat16 tensor, I think we may not be able to do this optimization. I see, if you need int8 inputs then why do you need to insert a bfloat16 op between conv2d and q/dq ops? can't you just do: `q (int8) > dq (int8) > conv2d`, instead of `q (int8) > dq (int8) > to_bfloat16 > conv2d`?",I think because we need to distinguish the pattern of `q (int8) > dq (int8) > conv2d` with `q (int8) > dq (int8) > to_bfloat16 > conv2d` for the optimization.  `dq (int8) > conv2d` will fused into a QConv with INT8in and FP32out.  `dq (int8) > to_bfloat16 > conv2d` will fused into a QConv with INT8in and BF16out.,"> * `dq (int8) > to_bfloat16 > conv2d` will fused into a QConv with INT8in and BF16out. but this is not a equivalent transformation, original pattern outputs a fp32 Tensor, how can you transform this to BF16out?","> > * `dq (int8) > to_bfloat16 > conv2d` will fused into a QConv with INT8in and BF16out. >  > but this is not a equivalent transformation, original pattern outputs a fp32 Tensor, how can you transform this to BF16out? I think here the original pattern `dq (int8) > to_bfloat16 > conv2d` will output BF16 also. The conv2d accepts BF16 input and thus will also output a BF16 tensor in its implementation.","I see, it looks like what you want is:  and you want to fuse:  to the same op? This is also achievable with the new API. and it's very explicit what each computation does (also conv2d is always going to be the original fp32 conv2d). personally I feel this one is probably better since it's more explicit and will allow for using bfloat16 in different ways as well (more extensible). but I'm fine with just extending quantize op for now as well since it's a simple and local change.","I would say in  conv2d doing bfloat16 computation is not in the IR, it's an inferred information about internal implementation details of PyTorch, so it's not a great pattern to use I think","> conv2d doing bfloat16 computation is not in the IR, it's an inferred information about internal implementation details of PyTorch, so it's not a great pattern to use I think I guess we are already doing similar type inference per op semantics with the IR under bfloat16 autocast? Do we plan to make everything explicit in the IR with bfloat16 autocast too? Or, we keep it as part of the op semantics  same output dtype as input as the default rule?","> Or, we keep it as part of the op semantics  same output dtype as input as the default rule? how is this used in autocast? you mean autocast also relies on this assumption? does autocast also work with IR? or is it just working with operators? if it is also working with a graph that contains operators I feel it will also be better to make the semantics of the operators in the graph simpler (just have conv2d op/node to represent one thing, instead of have it represent a context dependent operator/node). if autocast just works with a single operator, then it might be fine I think. personally I feel it's cleaner to do this more explicitly, and have every aten op to correspond to a single operator instead of relying on these implementation details, since aten ops does not have dtype information (unlike edge dialect in executorch). but given the current use cases, this is not too important, and I think it's not too hard to change later, so it's fine to go with relying on the assumption for now as well I think.","> how is this used in autocast? you mean autocast also relies on this assumption? does autocast also work with IR? or is it just working with operators? if it is also working with a graph that contains operators I feel it will also be better to make the semantics of the operators in the graph simpler (just have conv2d op/node to represent one thing, instead of have it represent a context dependent operator/node). if autocast just works with a single operator, then it might be fine I think. My understanding is that data types are context dependent with autocast and when it is applied to the graph, the type cast would only be inserted to places as needed according to the data types of the inputs and the cast policy, e.g., a few ops like conv/matmul require explicit cast to lower precision and most ops just ""fall through"" which keep the data types of inputs. When you say ""make the semantics of the operators in the graph simpler"", do you mean to add explicit type cast ops like ""to(bfloat16)"" and ""to(float)"" around all the needed ops just like quantized ops or just mark the data types as metadata on the graph?","> > how is this used in autocast? you mean autocast also relies on this assumption? does autocast also work with IR? or is it just working with operators? if it is also working with a graph that contains operators I feel it will also be better to make the semantics of the operators in the graph simpler (just have conv2d op/node to represent one thing, instead of have it represent a context dependent operator/node). if autocast just works with a single operator, then it might be fine I think. >  > My understanding is that data types are context dependent with autocast and when it is applied to the graph, the type cast would only be inserted to places as needed according to the data types of the inputs and the cast policy, e.g., a few ops like conv/matmul require explicit cast to lower precision and most ops just ""fall through"" which keep the data types of inputs. When you say ""make the semantics of the operators in the graph simpler"", do you mean to add explicit type cast ops like ""to(bfloat16)"" and ""to(float)"" around all the needed ops just like quantized ops or just mark the data types as metadata on the graph? ""making the semantics simpler"" meaning each aten op, e.g. aten.conv2d should only accept Tensor with the same input dtype, which means we need to add type cast ops ""to(bfloat16)"", ""to(float)"" in the graph. I'm not sure if metadata is a part of aten dialect actually, maybe   can help answer this question",Close as finished.
719,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch2.1.0 DDP+compile+dynamic_shape cause error)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Excuse me! When I use **torch2.1.0** DDP+compile to wrapper huggingface transformers gpt2 model, it will cause error only for the last batch data (shape is smaller than batch_size). Detailed logs as follows:  However, the single gpu (without DDP wrapper) workers well:  The following is complete code in `demo.py`:  The launch script `run.sh` is:   The execute command is `bash run.sh 1 (or 2)`.   Versions Environments:  Looking forward to your reply, thanks a lot! )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch2.1.0 DDP+compile+dynamic_shape cause error," ğŸ› Describe the bug Excuse me! When I use **torch2.1.0** DDP+compile to wrapper huggingface transformers gpt2 model, it will cause error only for the last batch data (shape is smaller than batch_size). Detailed logs as follows:  However, the single gpu (without DDP wrapper) workers well:  The following is complete code in `demo.py`:  The launch script `run.sh` is:   The execute command is `bash run.sh 1 (or 2)`.   Versions Environments:  Looking forward to your reply, thanks a lot! ",2023-10-20T05:44:12Z,triaged oncall: pt2 module: dynamic shapes,closed,0,12,https://github.com/pytorch/pytorch/issues/111636,,Is this a weird interaction between DDPgraphoptimizer and the thing where symbolic shapes turns itself on when it notices recompiles?  ,"> Is this a weird interaction between DDPgraphoptimizer and the thing where symbolic shapes turns itself on when it notices recompiles Yeah! I understand that the last batch shape (7) is samller than the given batch_size (16), which causes dynamo recompile error.", That sounds spot on. So there are two workarounds: 1. set `dynamic=False` on torch.compile 2. set `optimize_ddp=False` in torch._dynamo.config and also bug  more about when we're getting rid of `optimize_ddp`,"> and also bug  more about when we're getting rid of optimize_ddp Yea.. we are working on this For the time being, optimize_ddp probably is not compatible with dyamic shapes.  I will at least add a patch that makes this an error for now.","> > and also bug  more about when we're getting rid of optimize_ddp > > Yea.. we are working on this >  > For the time being, optimize_ddp probably is not compatible with dyamic shapes. I will at least add a patch that makes this an error for now.    So is there a plan to fix this bug in the future?","We have a new way of compiling DDP ( CC([DDP + Dynamo] Tracing DDP AllReduce (Compiled DDP))), but it's not quite ready.  At the same time, we decided not to invest more in the current graphbreak DDP thing.  In this case I might make an exception, if the bug is easy to fix ill just fix it. Ed had a suggestion for me.","I also met this bug in pytorch 2.1.2. In some cases DDP + default compile options crashes but I cannot figure which part of my codes goes wrong. It just said Cannot call sizes() on tensor with symbolic sizes/strides while executing submod xxx, and the FXtraced module points out the torch._C._nn.scaled_dot_product_attention line."," Can you try a nightly, and if it is still not resolved, file a separate bug for your issue?",">  Can you try a nightly, and if it is still not resolved, file a separate bug for your issue? Yes! I tried 2.3.0dev20240101, and the issue seemed to be resolved. Thank you.", did you mean to reopen?,"Intentional, though i think this issue is a dupe"
474,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use Dr.CI GitHub checkrun summary when querying its API fails)ï¼Œ å†…å®¹æ˜¯ (This will allow internal SandCastle job to access Dr.CI classification results via GitHub checkrun summary and correctly ignore unrelated failures.  Testing Adding `TestBypassFailuresOnSandCastle` where Dr.CI API returns nothing.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Use Dr.CI GitHub checkrun summary when querying its API fails,This will allow internal SandCastle job to access Dr.CI classification results via GitHub checkrun summary and correctly ignore unrelated failures.  Testing Adding `TestBypassFailuresOnSandCastle` where Dr.CI API returns nothing.,2023-10-20T02:22:20Z,fb-exported Merged ciflow/trunk topic: not user facing test-config/default,closed,0,6,https://github.com/pytorch/pytorch/issues/111628," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D50505558, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
593,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([draft] [dynamo] Propagate Tensor sources for in-place ops invoked via `call_function`, `call_method` and `vmap`)ï¼Œ å†…å®¹æ˜¯ (Fixes  CC([dynamo] in-place ops misattribute source to `None`) Builds upon: https://github.com/pytorch/pytorch/pull/111565 First pass at a larger effort that will propagate source for all inplace ops. TODO:  [ ] Extend to all higher order ops?  [ ] Extract inplace ops logic into helper function? )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"[draft] [dynamo] Propagate Tensor sources for in-place ops invoked via `call_function`, `call_method` and `vmap`",Fixes  CC([dynamo] in-place ops misattribute source to `None`) Builds upon: https://github.com/pytorch/pytorch/pull/111565 First pass at a larger effort that will propagate source for all inplace ops. TODO:  [ ] Extend to all higher order ops?  [ ] Extract inplace ops logic into helper function? ,2023-10-19T21:01:57Z,open source topic: not user facing module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/111602,This direction is not productive. Tensor cannot be guaranteed to have a source.
2008,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(shared object initialization failed trying to run stable diffusion on RX 7700 XT )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi PyTorch Team, I'm trying to run stable diffusion via Automatic1111/stablediffusionwebui as well as ComfyUI. No matter what I tried, it resulted in a `RuntimeError: HIP error: shared object initialization failed` and in case of stablediffusionwebui additionally in the segfault below when calling different torch methods. Setup: * GPU: Radeon RX 7700 XT * Ubuntu 22.04.3 + ROCm 5.7.1 + torch rocm 5.7 nightly Using the latest rocm/pytorch docker let to the same results. I attached the stack traces both for stablediffusionwebui as well as ComfyUI. Thanks a lot in advance for looking into this issue **Error when starting stablediffusionwebui**  **Error when starting ComfyUI**   Versions Collecting environment information... PyTorch version: 2.1.0a0+git413b4cd Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.7.31921d1770ee1b OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvmproject roc5.7.0 23352 d1e13c532a947d0cbfc94759c00dcf152294aa13) CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64bit runtime) Python platform: Linux6.2.034genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Radeon RX 7700 XT Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.7.31921 MIOpen runtime version: 2.20.0 Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Byte Order:                         Little E)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,shared object initialization failed trying to run stable diffusion on RX 7700 XT ," ğŸ› Describe the bug Hi PyTorch Team, I'm trying to run stable diffusion via Automatic1111/stablediffusionwebui as well as ComfyUI. No matter what I tried, it resulted in a `RuntimeError: HIP error: shared object initialization failed` and in case of stablediffusionwebui additionally in the segfault below when calling different torch methods. Setup: * GPU: Radeon RX 7700 XT * Ubuntu 22.04.3 + ROCm 5.7.1 + torch rocm 5.7 nightly Using the latest rocm/pytorch docker let to the same results. I attached the stack traces both for stablediffusionwebui as well as ComfyUI. Thanks a lot in advance for looking into this issue **Error when starting stablediffusionwebui**  **Error when starting ComfyUI**   Versions Collecting environment information... PyTorch version: 2.1.0a0+git413b4cd Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.7.31921d1770ee1b OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvmproject roc5.7.0 23352 d1e13c532a947d0cbfc94759c00dcf152294aa13) CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64bit runtime) Python platform: Linux6.2.034genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Radeon RX 7700 XT Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.7.31921 MIOpen runtime version: 2.20.0 Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Byte Order:                         Little E",2023-10-19T21:00:31Z,module: rocm triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/111601,"   (1) Can you run ""rocminfo"" to show your gfx arch? (2) please run below simple test to check whether this is related to atomics:  THanks.", thanks for the reply. Here is the output (1) https://gist.github.com/albrechtjohannes/78a292931b3155623e7ce8bb0d9f28bb (2) It indeed seem to be related to atomics. The test creates a Segmentation fault.  I set `export HIP_VISIBLE_DEVICES=1` to run on the RX 7700 XT prior to running the test. ,"So you have two different GPUsL gfx902 and 1101. Can you try to set this environment variable: HSA_OVERRIDE_GFX_VERSION=11.0.0 when running on gpu number 2 (index 1)? Also, can you run this command and show the output here: ","Unfortunately, the test still leads to a Segfault after setting `HIP_VISIBLE_DEVICES=1` and `HSA_OVERRIDE_GFX_VERSION=11.0.0` ","It is supposed to show 3 lines (not just one line of ""flags 1"") since you have 3 agents.  You can also try override it to HSA_OVERRIDE_GFX_VERSION=10.3.0 and try again.  Unfortunately, we are not familiar with your mixed gpu setup. You may want to check your setup (your motherboard model) to make sure the GPUs are connected to CPU with PCIe atomics support in order to do machine learning workload with pytorch.  ","I have a Ryzen 2400G with integrated graphics. That's why 3 agents are listed (one CPU, one integrated GPU and the dedicated GPU).  After disabling the integrated graphics in BIOS, the errors are gone. Therefore I'm closing the issue. Thanks for your help."
531,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reland: Add `lazy_clone_storage` to create COW storages)ï¼Œ å†…å®¹æ˜¯ (Relands CC(Add `lazy_clone_storage` to create COW storages) NOTE: COW storages do not actually copy on write yet, they just have the COW deleter and deleter context applied to them Part of CC(Implement Copyonwrite (COW) tensors)    CC(Reland: Add `lazy_clone_storage` to create COW storages) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Reland: Add `lazy_clone_storage` to create COW storages,"Relands CC(Add `lazy_clone_storage` to create COW storages) NOTE: COW storages do not actually copy on write yet, they just have the COW deleter and deleter context applied to them Part of CC(Implement Copyonwrite (COW) tensors)    CC(Reland: Add `lazy_clone_storage` to create COW storages) ",2023-10-19T16:42:47Z,module: internals open source Merged ciflow/trunk topic: not user facing ciflow/periodic,closed,0,6,https://github.com/pytorch/pytorch/issues/111579,"Are there any ciflows that run the tests on a mobile build? I tried ciflow/android, but that label apparently doesn't work anymore", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake, then you can re trigger it through pytorchbot."," merge f ""force through straggler jobs"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
457,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Bug]: some parameters' grad is None when using FSDP with torch2.1.0)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug To reproduce the problem: Training InternLM with config fsdp=True:    Versions  !ä¼ä¸šå¾®ä¿¡æˆªå›¾_c513d224c4444f69ba6f732f4e3b5574 **Note that when using torch2.0.1, the error will not happen.** )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Bug]: some parameters' grad is None when using FSDP with torch2.1.0," ğŸ› Describe the bug To reproduce the problem: Training InternLM with config fsdp=True:    Versions  !ä¼ä¸šå¾®ä¿¡æˆªå›¾_c513d224c4444f69ba6f732f4e3b5574 **Note that when using torch2.0.1, the error will not happen.** ",2023-10-19T07:02:00Z,oncall: distributed triaged module: fsdp,open,0,2,https://github.com/pytorch/pytorch/issues/111552,"Thanks for reporting this regression! Are you from the InternLM team? If not, have you tried posting an issue in the InternLM repo? It might be helpful for us to have some initial triage from InternLM since we are not familiar with their engine.","> Thanks for reporting this regression! >  > Are you from the InternLM team? If not, have you tried posting an issue in the InternLM repo? It might be helpful for us to have some initial triage from InternLM since we are not familiar with their engine. Yes, I'm from the InternLM team."
1970,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MPS Performance regressions on Sonoma 14.0 )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug This issue is related to CC(MPS device appears much slower than CPU on M1 Mac Pro). tldr: speed 50% slower, big memory leaks.  Basically, since upgrading to Sonoma, performance of the MPS device on sentence transformers models has taken a big nosedive. I don't have an applestoapples (ğŸ¥) comparison exactly, but an M1 ultra on Sonoma is 50% slower than an M1 Pro on Ventura. Hereissuecomment1304882735) are some numbers I collected on Ventura with an M1 ultra, not sure that data can be exactly compared, but it looks like the ratio between inference time on M1 Ultra/Ventura and M1 Ultra / Sonoma is about 1:2.  On Sonoma (M1 Ultra):  The MPS device is clearly functioning and present, but 23 ms is about 50% slower than an M1 Pro on Ventura. Here is the Ventura data:  Both cases with with today's nightly PyTorch, and sentencetransformers==2.2.2, transformers==4.34.1.  Additionally, let me know if I should file another ticket for this, but I observe huge memory leaks when using the MPS device. I often conduct a lot of bulk embedding using this model, and after ~32k calls to `model.forward`, I see memory usage exceeding 100GB and increasing. It's compressed, so it looks leaked. I observe this problem when running torch in an Flask wrapper. I am wondering if something about Apple's Metal Performance Shaders implementation changed recently.   Versions Collecting environment information... PyTorch version: 2.2.0.dev20231018 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.0 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.6 Libc version: N/A Python version: 3.11.5 (main, Aug 24 2023, 15:09:45) [Clan)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,MPS Performance regressions on Sonoma 14.0 ," ğŸ› Describe the bug This issue is related to CC(MPS device appears much slower than CPU on M1 Mac Pro). tldr: speed 50% slower, big memory leaks.  Basically, since upgrading to Sonoma, performance of the MPS device on sentence transformers models has taken a big nosedive. I don't have an applestoapples (ğŸ¥) comparison exactly, but an M1 ultra on Sonoma is 50% slower than an M1 Pro on Ventura. Hereissuecomment1304882735) are some numbers I collected on Ventura with an M1 ultra, not sure that data can be exactly compared, but it looks like the ratio between inference time on M1 Ultra/Ventura and M1 Ultra / Sonoma is about 1:2.  On Sonoma (M1 Ultra):  The MPS device is clearly functioning and present, but 23 ms is about 50% slower than an M1 Pro on Ventura. Here is the Ventura data:  Both cases with with today's nightly PyTorch, and sentencetransformers==2.2.2, transformers==4.34.1.  Additionally, let me know if I should file another ticket for this, but I observe huge memory leaks when using the MPS device. I often conduct a lot of bulk embedding using this model, and after ~32k calls to `model.forward`, I see memory usage exceeding 100GB and increasing. It's compressed, so it looks leaked. I observe this problem when running torch in an Flask wrapper. I am wondering if something about Apple's Metal Performance Shaders implementation changed recently.   Versions Collecting environment information... PyTorch version: 2.2.0.dev20231018 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 14.0 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: version 3.27.6 Libc version: N/A Python version: 3.11.5 (main, Aug 24 2023, 15:09:45) [Clan",2023-10-18T22:13:03Z,high priority triaged module: mps,open,1,4,https://github.com/pytorch/pytorch/issues/111517,"I am pleased to report that as of 14.1, the situation is much improved with tonight's nightly in terms of throughput.  Memory leak issues remain, however, and I will attempt to collect some data from Valgrind or something to pin down the problem better. ","I just updated to Sonoma 14.2 and training 'tinystories' with llama2.c with 'MPS'  is 7X slower.  It's similar to CPU performance, ... but the GPU is definitely active.  Also, 'something' grabbed 40gb of swap space on m 128gb iMac. The good news is that the output of the model is syntactically correct and is starting to make sense.  Previously, on 'MPS' the output was not syntactically correct and was nonsensical.  it used to be ~4,000ms. ","I'v had issues in the past between my M2 and M1max but honestly I didn't think about mac os versions, now they are synced, but I had came to the conclusion that it was model related: the same model would run absurdly slow on the (weaker) M2, but a newly converted mixed precision version would run normally  on M2, and without significant improvement on the M1. THat's how I came to the conclusion. It was also memory related with this kind of silly behaviour: !ss 20231205 at 4 58 45â€¯AM The way macos handles Swap today, some would call it _seriously annoying_ but that's not giving justice to how it works well for other purposes. For our particular matter, it can be sometimes very annoying, and I will generalize roughly but quickly: Swap file is a virtual partition with a flexible size limited only by available space, and I know of no easy way to restrict it like we could before. What happens is 3 things in particular: macOs saving Ram into swap when we killing process, and a weird way of doubling the size of what it writes in swap,  and a third thing, a (now fixed) issue in swap management in our particular scenario. Fixed though. ABout memory, there is an alternate memory maangement library, ""mimalloc"", its by microsoft, https://github.com/microsoft/mimalloc, I use it sometimes, it was an easy compile then I moved the dylib and before running python i do:  ",Hi. Had same performance issue with M1 Ultra. Ussing mimalloc helped. I see it's possible to build torch with mimalloc. Regarding the CMakeLists.txt all we need is to set DUSE_MIMALLOC=ON when building. or set USE_MIMALLOC = ON in CMakeLists.
294,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI036)ï¼Œ å†…å®¹æ˜¯ (Enable badexitannotation (PYI036) Link: CC(Enable more flake8pyi ruff checks) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI036,Enable badexitannotation (PYI036) Link: CC(Enable more flake8pyi ruff checks) ,2023-10-18T21:07:23Z,module: cpu triaged open source module: amp (automated mixed precision) release notes: distributed (fsdp),closed,0,3,https://github.com/pytorch/pytorch/issues/111507,"If we run into JIT issues, let's just do as many of these annotations properly as we can and noqa the rest of them."," Apparently, setting the annotation as ""object"" (instead of Any) would also be acceptable according to the original flake8 rule.",">  Apparently, setting the annotation as ""object"" (instead of Any) would also be acceptable according to the original flake8 rule. Looks like JIT does not recognize ""object"". Shall we keep PYI036 disabled to turn this into a draft?"
1957,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.autocast() hangs on CPUs)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi PyTorch Team,  First and foremost, thank you for your invaluable contributions to the ML community! I recently encountered a performance issue when using torch.autocast() on CPUs. In the FP32 mode, everything is fine. When turning on the AMP with BF16, the code seems to be stuck and never finishes. Here is a quick repo:  On my Intel i713700K, FP 32 throughput is 4.98 sequences/second. Enabling AMP with autocast seems to hang the code, as 1 core spikes to 100% and the script would not finish after waiting for 30 minutes. Would appreciate a further look into this issue  thanks in advance!  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.6 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.2.034genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.5.119 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 535.113.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             24 Online CPU(s) list:                023 Vendor ID:                          GenuineIntel Model name:    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.autocast() hangs on CPUs," ğŸ› Describe the bug Hi PyTorch Team,  First and foremost, thank you for your invaluable contributions to the ML community! I recently encountered a performance issue when using torch.autocast() on CPUs. In the FP32 mode, everything is fine. When turning on the AMP with BF16, the code seems to be stuck and never finishes. Here is a quick repo:  On my Intel i713700K, FP 32 throughput is 4.98 sequences/second. Enabling AMP with autocast seems to hang the code, as 1 core spikes to 100% and the script would not finish after waiting for 30 minutes. Would appreciate a further look into this issue  thanks in advance!  Versions Collecting environment information... PyTorch version: 2.1.0+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.3 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.6 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux6.2.034genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.5.119 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 535.113.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      46 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             24 Online CPU(s) list:                023 Vendor ID:                          GenuineIntel Model name:    ",2023-10-18T03:46:51Z,module: cpu triaged module: amp (automated mixed precision),closed,1,20,https://github.com/pytorch/pytorch/issues/111456,"Hi , Thanks for reporting this issue.   I made some slight change of your script to enable BF16 with Inductor    Refer to here for details https://gist.github.com/lesliefangintel/b28ef9fbb0afeb7159e3356d81fc5493      For BF16, You need also run  `torch.compile` under autocast context      Running your test with `TORCHINDUCTOR_FREEZING=1 ` Environment variable. For example: `TORCHINDUCTOR_FREEZING=1  python test.py`    Here is initial performance data I got with this script:       Since your CPU `13th Gen Intel(R) Core(TM) i713700K` doesn't have Hardware AMX BF16 instruction acceleration. You may not got the speedup as I send out. The AMX BF16 instruction exists on Intel SPR servers.","Hi fangintel, thank you a lot for following up with this issue. After running the script you provided with `TORCHINDUCTOR_FREEZING=1` set, the issue persists. However, I have realized that it is not the code hangs, it's actually because the BF16 mixed precision is extremely slow on the i713700K so that it never finishes. I changed the batch size to 1 and the code is able to finish within reasonable time, and I got:  One thing I noticed is that FP32 execution would use all available cores and the BF16 would only use one core, which explains some, but not all, differences. I guess the other part of the difference is the lack of AMX BF16 instructions actually makes the BF16 slower than FP32.  In this case, should torch.autocast() disable BF16 execution on CPUs that do not support AMX BF16 instructions? Or, at least, should there be some documentation about this behavior?","_One thing I noticed is that FP32 execution would use all available cores and the BF16 would only use one core_ , it also looks strange to me. I think we need to take a look of this model on system without AMX BF16 instructions.  ","> Hi fangintel, thank you a lot for following up with this issue. After running the script you provided with `TORCHINDUCTOR_FREEZING=1` set, the issue persists. However, I have realized that it is not the code hangs, it's actually because the BF16 mixed precision is extremely slow on the i713700K so that it never finishes. I changed the batch size to 1 and the code is able to finish within reasonable time, and I got: >  >  >  > One thing I noticed is that FP32 execution would use all available cores and the BF16 would only use one core, which explains some, but not all, differences. I guess the other part of the difference is the lack of AMX BF16 instructions actually makes the BF16 slower than FP32. >  > In this case, should torch.autocast() disable BF16 execution on CPUs that do not support AMX BF16 instructions? Or, at least, should there be some documentation about this behavior?  Could you please share additional information on the steps or configuration you took to perform this testing? I'm actively working on reproducing the reported issue. ","Hi Zheng , thank you for digging into this issue! I did not change anything from the original code except for setting the batch size to 1, which shortens the run time and allows the super slow BF16 mode to finish. Here is the gist for your convenience. The info on my platform could be found in my original issue  I just did a clean install of PyTorch 2.1 on Ubuntu 22.04.  Running `python3 pytorch_bf16_cpu.py` should be sufficient to reproduce the problem. Setting `TORCHINDUCTOR_FREEZING=1` or running `torch.compile` under `autocast` context do not affect the outcome IMHO. Let me know if you need anything.","> Hi Zheng , thank you for digging into this issue. I did not change anything from the original code except for setting the batch size to 1, which shortens the run time and allows the super slow BF16 mode to finish. Here is the gist for your convenience. The info on my platform could be found in my original issue  I did not change anything except for doing a clean install of PyTorch 2.1 on Ubuntu 22.04. Running `python3 pytorch_bf16_cpu.py` with or without `TORCHINDUCTOR_FREEZING=1` would reproduce the issue. Let me know if you need anything.   Thank you so much for the detailed information! With running the same script on system without AMX BF16 instructions, I did not find the same issue and I got:  I noticed there is a difference is that the pytorch version I utilized is ` torch 2.2.0a0+git3ca81ae`. Would you consider to try a most recent version?","Hi Zheng, thank you for your follow up.  I think I have figured out the root cause of the problem: PyTorch would not select the correct math library or even the matmul kernel when running FP/BF16 on Intel CPUs. Please see the following script for a simple reproduction:  And then I got:  This is the result on PyTorch nightly torch2.2.0.dev20231021. As you can see, selecting FP16 or BF16 on CPUs would result in extremely poor performance in things as simple as GEMM. I also noticed that only 1 core is being used in FP/BF16 GEMM. Selecting FP16 would lead to crashes on PyTorch 2.1, while the BF16 throughput stays the same. I did not have any special setup for my environment  just an install of PyTorch inside a completely clean virtualenv on Ubuntu 22.04, using official instructions. Honestly, this kind of bug is a bit surprising to me  Intel's PyTorch integration would not be able select the right GEMM kernel on a completely normal setup, and running FP16/BF16 would lead to extremely poor performance or even crashes. I would imagine this type of bugs would have been detected by Intel's QC. ","Searching the error message `""addmm_impl_cpu_"" not implemented for 'Half'` I encountered on running FP16 on CPU on PyTorch 2.1, it seems a lot of people has encountered the same problem, such as issue 1, issue 2), and issue 3, so I believe what I am seeing here is not an isolated case."," (13th Gen Intel(R) Core(TM) i713700K) does not have AMX which means that the gemm of bf16 would go to a very slow reference path without hardware acceleration. As for half float support, the work is not done on torch yet.  do we have a timeline when gemm will be supported on half ?"," Half or fp16 support for gemm has been landed in stock PyTorch recently, that's why matrix multiplication works for half  in torch2.2.0.dev20231021 and you got `""addmm_impl_cpu_"" not implemented for 'Half'`  in PyTroch 2.1. For the poor performance of fp16/bf16, it is because the platform does not have AMX for hardware acceleration. ","A float16 or bf16 saved model, how to make it run on CPU?", You can explicitly do `model.cpu()` first then the model will be runed on CPU.," May I ask, is this essentially just same as model.float32()?",model.float32() will invoke different kernels for data type conversion.,  have we solved this issue ?, I think this issue has been solved.,closing this one as the issues has been solved :),"Hey Pytorch team, thank you so much for everything you do!  I would like to follow up on this issue because I think I am getting the same issue as .  When I run:   I can reproduce the results obtained in this thread:   I'm guessing the issue is closed cause we cannot do bfloat operations on CPUs without the AMX BF16? If that's the case, is there any workaround? If not, I'll just disabel the AMP for CPU use cases in my own code. To make communication easier I also collected my env:   Version Collecting environment information... PyTorch version: 2.3.1+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (GCC) 14.1.1 20240522 Clang version: 17.0.6 CMake version: version 3.29.4 Libc version: glibc2.39 Python version: 3.8.19 (default, Apr  9 2024, 15:43:30)  [GCC 13.2.1 20230801] (64bit runtime) Python platform: Linux6.9.3arch11x86_64withglibc2.34 Is CUDA available: True CUDA runtime version: 12.5.40 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A2000 8GB Laptop GPU Nvidia driver version: 550.78 cuDNN version: Probably one of the following: /usr/lib/libcudnn.so.9.1.1 /usr/lib/libcudnn_adv.so.9.1.1 /usr/lib/libcudnn_cnn.so.9.1.1 /usr/lib/libcudnn_engines_precompiled.so.9.1.1 /usr/lib/libcudnn_engines_runtime_compiled.so.9.1.1 /usr/lib/libcudnn_graph.so.9.1.1 /usr/lib/libcudnn_heuristic.so.9.1.1 /usr/lib/libcudnn_ops.so.9.1.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                         x86_64 CPU opmode(s):                       32bit, 64bit Address sizes:                        39 bits physical, 48 bits virtual Byte Order:                           Little Endian CPU(s):                               20 Online CPU(s) list:                  019 Vendor ID:                            GenuineIntel Model name:                           12th Gen Intel(R) Core(TM) i712700H CPU family:                           6 Model:                                154 Thread(s) per core:                   2 Core(s) per socket:                   14 Socket(s):                            1 Stepping:                             3 CPU(s) scaling MHz:                   18% CPU max MHz:                          4700.0000 CPU min MHz:                          400.0000 BogoMIPS:                             5378.00 Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr ibt flush_l1d arch_capabilities Virtualization:                       VTx L1d cache:                            544 KiB (14 instances) L1i cache:                            704 KiB (14 instances) L2 cache:                             11.5 MiB (8 instances) L3 cache:                             24 MiB (1 instance) NUMA node(s):                         1 NUMA node0 CPU(s):                    019 Vulnerability Gather data sampling:   Not affected Vulnerability Itlb multihit:          Not affected Vulnerability L1tf:                   Not affected Vulnerability Mds:                    Not affected Vulnerability Meltdown:               Not affected Vulnerability Mmio stale data:        Not affected Vulnerability Reg file data sampling: Mitigation; Clear Register File Vulnerability Retbleed:               Not affected Vulnerability Spec rstack overflow:   Not affected Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSBeIBRS SW sequence; BHI BHI_DIS_S Vulnerability Srbds:                  Not affected Vulnerability Tsx async abort:        Not affected Versions of relevant libraries: [pip3] efficientnet_pytorch==0.7.1 [pip3] mypy==1.10.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.24.4 [pip3] segmentationmodelspytorch==0.3.3 [pip3] torch==2.3.1 [pip3] torchvision==0.18.1 [pip3] triton==2.3.1 [conda] Could not collect", Same reason.  Your machine doesn't have AMX and avx512. GEMM of bf16/fp16 would run to a slow reference path without hardware acceleration. You can just disable the AMP for CPU use cases in your code.,"Sounds good, thanks for the prompt response :)"
728,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Rename name->qualname in torch.library.impl_abstract)ï¼Œ å†…å®¹æ˜¯ (  CC(Higherlevel custom op API, V3)  CC(torch.library: Create helper function `is_functional_schema`)  CC(Rewrite torch.library's documentation)  CC(Change torch.library.impl to accept a device string)  CC(Rename name>qualname in torch.library.impl_abstract) See title. Makes this consistent with torch.library.{define, impl, impl_device}, where we have named the same argument `qualname`. This is not BCbreaking because we have not released a version of PyTorch with impl_abstract in it yet.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Rename name->qualname in torch.library.impl_abstract,"  CC(Higherlevel custom op API, V3)  CC(torch.library: Create helper function `is_functional_schema`)  CC(Rewrite torch.library's documentation)  CC(Change torch.library.impl to accept a device string)  CC(Rename name>qualname in torch.library.impl_abstract) See title. Makes this consistent with torch.library.{define, impl, impl_device}, where we have named the same argument `qualname`. This is not BCbreaking because we have not released a version of PyTorch with impl_abstract in it yet.",2023-10-16T18:24:41Z,Merged release notes: composability,closed,0,0,https://github.com/pytorch/pytorch/issues/111380
288,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Increase coverage of clang-tidy to CudaIPCTypes.cpp)ï¼Œ å†…å®¹æ˜¯ (This PR uses clangtidy in torch/csrc/CudaIPCTypes.cpp)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Increase coverage of clang-tidy to CudaIPCTypes.cpp,This PR uses clangtidy in torch/csrc/CudaIPCTypes.cpp,2023-10-16T13:05:13Z,open source Merged ciflow/trunk topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/111371,"  label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1956,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to import transformer.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug !pip install datasets evaluate transformers[sentencepiece]  I am doing the NLP course by Hugging Face. On week 3, I cant seem the import anything from transformers.   Versions (NLP310) D:\OneDrive\Data\Learning\Natural Science\Computer Science\AI, ML, DL\Hugging Face NLP\notebooksmain\course\en\chapter3>python collect_env.py Collecting environment information... PyTorch version: 2.1.0+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Pro GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.13  (main, Sep 11 2023, 13:15:57) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=3801 DeviceID=CPU0 Family=107 L2CacheSize=4096 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=3801 Name=AMD Ryzen 7 5700G with Radeon Graphics ProcessorType=3 Revision=20480 Versions of relevant libraries: [pip3] numpy==1.26.0 [pip3] torch==2.1.0 [conda] _tflow_select             2.3.0                       mkl [conda] blas                      1.0                         mkl [conda] mkl                       2023.1.0         h6b88ed4_46357 [conda] mklservice               2.4.0           py310h2bbff1b_1 [conda] mkl_fft                   1.3.8           py310h2bbff1b_0 [conda] mkl_random                1.2.4           py310h59b6b97_0 [conda] numpy  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Failed to import transformer.," ğŸ› Describe the bug !pip install datasets evaluate transformers[sentencepiece]  I am doing the NLP course by Hugging Face. On week 3, I cant seem the import anything from transformers.   Versions (NLP310) D:\OneDrive\Data\Learning\Natural Science\Computer Science\AI, ML, DL\Hugging Face NLP\notebooksmain\course\en\chapter3>python collect_env.py Collecting environment information... PyTorch version: 2.1.0+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Pro GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.13  (main, Sep 11 2023, 13:15:57) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=3801 DeviceID=CPU0 Family=107 L2CacheSize=4096 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=3801 Name=AMD Ryzen 7 5700G with Radeon Graphics ProcessorType=3 Revision=20480 Versions of relevant libraries: [pip3] numpy==1.26.0 [pip3] torch==2.1.0 [conda] _tflow_select             2.3.0                       mkl [conda] blas                      1.0                         mkl [conda] mkl                       2023.1.0         h6b88ed4_46357 [conda] mklservice               2.4.0           py310h2bbff1b_1 [conda] mkl_fft                   1.3.8           py310h2bbff1b_0 [conda] mkl_random                1.2.4           py310h59b6b97_0 [conda] numpy  ",2023-10-16T10:59:59Z,high priority needs reproduction module: binaries module: windows triaged,closed,0,8,https://github.com/pytorch/pytorch/issues/111365,Tentative hipri to get a reproducer..,"I encounter the same error when I run `import torch`. After some investigation, I discovered that the issue is somehow connected to Jupyter Notebook. When I execute `import torch` in Python or IPython, it works as expected. However, I face the error when attempting to import PyTorch in a Jupyter Notebook. The issue persists even after stopping the Jupyter server, continuing through IPython as well. I have to close and reopen the conda terminal in order to successfully import PyTorch in IPython again. ","I have the same error when running `import torch`. I have noticed that it happens when I also install either the `cyipopt` or `pynumero_libraries` packages. Before installing either, the line `import torch` runs as expected. It happens in both jupyter and in the terminal. `>>> import torch Traceback (most recent call last):   File """", line 1, in    File ""C:\Users\candr\anaconda3\envs\test4\lib\sitepackages\torch\__init__.py"", line 130, in      raise err OSError: [WinError 182] The operating system cannot run %1. Error loading ""C:\Users\candr\anaconda3\envs\test4\lib\sitepackages\torch\lib\fbgemm.dll"" or one of its dependencies.`","  I managed to reproduce with installing cyipopt. Were you using conda to install pytorch? I had no issues with pytorch when using pip, however if installing cyipopt using conda  after installing pytorch using conda then intelopenmp libiomp5md.dll will be overwritten by the one from condaforge/win64::openmp5.0.0vc14_1 needed by cyipopt.",  I could not reproduce it for your cases. Does it still happen to you? Are you using conda to install pytorch?If it does can you please check if libiomp5md.dll gets overwritten? ," Yes, I figured that out as well. Installing pytorch with pip and the other packages with conda seems to work.","I am also running into this issue, I figured out this potentially relates to the **tensorflow** from anaconda. I could reproduce the issue consistently by   If I import either package, both work. If I install both packages from PyPi, importing both works as well. ","Closing this issue since it is the same as  CC(Pillow v9.0.0 conda-forge package causes ""OSError: [WinError 182] [...] error loading â€˜caffe2_detectron_ops.dll'"") to which I opened an issue in anaconda https://github.com/ContinuumIO/anacondaissues/issues/13146. ghwzhao This seems to be a different issue. For me it was an error in tensorflow failing to import numpy. And it did not work not even when importing only tensorflow. If your issue still persists and you think it is related to pytorch please open another issue."
874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am getting the following error when training LlamaForCausalLM with torch 2.1 and FSDP (mixed precision) and torch.compile. Same exact code works when torch.compile disabled or when torch 2.0.1 is used. I also tried enabling and disabling amp autocast, it doesn't matter and the same error happens.  I am using a docker image, error happens in **Environment 2** which is provided in the **Versions** section.  Error logs   Minified repro _No response_  Versions **Environment 1**  **Environment 2**  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.`," ğŸ› Describe the bug I am getting the following error when training LlamaForCausalLM with torch 2.1 and FSDP (mixed precision) and torch.compile. Same exact code works when torch.compile disabled or when torch 2.0.1 is used. I also tried enabling and disabling amp autocast, it doesn't matter and the same error happens.  I am using a docker image, error happens in **Environment 2** which is provided in the **Versions** section.  Error logs   Minified repro _No response_  Versions **Environment 1**  **Environment 2**  ",2023-10-15T02:20:28Z,oncall: distributed triaged module: fsdp oncall: pt2,closed,0,27,https://github.com/pytorch/pytorch/issues/111317,," , assigned to you temporarily and feel free to assign to the right owner.",Previous issue possibly due to fsdp + mixed precision + dynamo + autograd (assigning gradient of incorrect dtype):  CC(torch.compile does not know .half() has side-effects) ,"Actually, couldn't this be the root cause of it all?  CC(torch2.1.0 compile+amp+ddp cause NotImplementedError). Not handling autocast manager setup/unwind appropriately when using DDP. Not sure if this bleeds into FSDP.  Could you confirm if the `split_module` pass is used for FSDP torch.compile ? EDIT: turns out the answer is no, but does FSDP introduce its own graph breaks?","> I also tried enabling and disabling amp autocast, it doesn't matter and the same error happens. I still see autocast in your stacktrace. Though you claim to have disabled it, I think that some of the model code has been decorated with autocast.","> Could you confirm if the split_module pass is used for FSDP torch.compile ? The fx graph splitter pass is not used at all for FSDP.  FSDP's graph breaks happen more implicitly:  Dynamo sees fsdp's python code and is configured to graphbreak on it.  In the ddp case, there is not really any DDP code running during forward, and the reason to insert the graphbreaks only becomes apparent  when you consider the comm operations that happen during backward.  So we had to take another approach to add the graphbreaks, using the fx pass.","> > I also tried enabling and disabling amp autocast, it doesn't matter and the same error happens. >  > I still see autocast in your stacktrace. Though you claim to have disabled it, I think that some of the model code has been decorated with autocast. Sorry, I only included one of the stacktraces, in this case it happens to be the autocasted run but same error happened without it as well.",", may I ask what are your settings for the supposedly nonautocasted run for these?  Could you share the logs so I can confirm?", are you still working on this?,"no, i never got a chance to look into this. unassigning for now","Seeing the same issue when using combination of FSDP + compile + MP.  Using amp autocast I don't see these issues, but whenever I add the `MixedPrecision` setting, this error occurs.",">  >  > Could you share the logs so I can confirm? Hi chuang, I can confirm that even using the setting 'param_dtype=torch.bfloat16, reduce_dtype=torch.bfloat16, buffer_dtype=torch.float32' you  given, the same error occurs. Please see logs following:  File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 839, in forward     return self._call_impl(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 839, in forward     return forward_call(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 825, in forward     unshard_fn(state, handle)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 464, in _pre_forward_unshard     return self._call_impl(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 336, in _unshard     args, kwargs = _pre_forward(   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 429, in _pre_forward     return forward_call(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 825, in forward     return self._call_impl(*args, **kwargs)       File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl return forward_call(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 825, in forward     ran_pre_unshard = handle.pre_unshard()   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/flat_param.py"", line 1194, in pre_unshard     output = self._fsdp_wrapped_module(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl     output = self._fsdp_wrapped_module(*args, **kwargs)       File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl unshard_fn(state, handle)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 464, in _pre_forward_unshard     args, kwargs = _pre_forward(   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 429, in _pre_forward     args, kwargs = _pre_forward(   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 429, in _pre_forward     _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 336, in _unshard     unshard_fn(state, handle)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 464, in _pre_forward_unshard     ret = self._writeback_orig_params()   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/utils/_contextlib.py"", line 115, in decorate_context     unshard_fn(state, handle)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 464, in _pre_forward_unshard     ran_pre_unshard = handle.pre_unshard()   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/flat_param.py"", line 1194, in pre_unshard     return func(*args, **kwargs)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/flat_param.py"", line 2202, in _writeback_orig_params     _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)   File ""/opt/conda/envs/python3.7/lib/python3.8/sitepackages/torch/distributed/fsdp/_runtime_utils.py"", line 336, in _unshard     return forward_call(*args, **kwargs)     _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)  File ""/home/admin/hippo/worker/slave/aop_418921_fsdp_32gpu_debug_20231216224420_3434820_tfjob.ps_1_50_10/tf_worker/tmp_1007/taoscale/models/transformer.py"",line 273, in forward",additional info: this issue disappears on pytorch 2.0.0 and pytorch 2.1.2,"Hit similar error while use FSDP and compile together.   If I set dtype of mixed precision to fp32, the error turns to tensor size mismatch:  as I set parallelism with 2 cards. 1828962*2 = 3657924 It seems the gradient (FlatTensor) in FSDP mode is not well caught by compile. My torch version is 2.2 and the way I use FSDP is like this: "," did you manage to find a solution for the size mismatch? Running into the same error, and this is the only post referencing it I could find.",">  did you manage to find a solution for the size mismatch? Running into the same error, and this is the only post referencing it I could find. Sorry, the answer is NO...","We are also hitting this issue with SHARD_GRAD_OP. Specifically, we are hitting it on the first eval step, after a training step has run on training Stable Diffusion 2. If we never do evaluation / saving the model seems to run fine. Hopefully this helps narrow it down a bit  . I can confirm that this issue is present both on 2.2 as well as on nightly as of last week.","My current hypothesis is that these errors happen due to OOM, but no proper OOM error is present but the ones discussed in this thread. ",Also met the issue for bf16 training and fp16 training. ,> ShardingStrategy. Also fail with .,"I encounter this issue when I run an evaluation step after a training step. If I do evaluation before any training steps, torch compile works fine and runs training until the first evaluation step after the torch compile backward passes. Seems like this has to do with it not respecting `no_grad` in the eval pass or different mixed precision / autocasting in that pass.","> My current hypothesis is that these errors happen due to OOM, but no proper OOM error is present but the ones discussed in this thread. This is the case for me. Reducing batch size worked ","Also hitting this issue with torch version: 2.3.0a0+6ddf5cf85e.nv24.04 As Skylion007 mentioned, this issue occurs for me if I do train before eval, but it goes away if I do eval before train. ",any updates? running into `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'` with FSDP + bf16 amp autocast + torch.compile,https://github.com/pytorch/pytorch/pull/134614 might be able to fix this issue.,https://github.com/pytorch/pytorch/pull/134614 should fix this issue. Please reopen the issue if it's not the case., make sure zero_grad is done after backward.   !d37a9ddf71cf4e29aa5339ddd0e432f6
1307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(bug: text generation result is different for different operating systems (macOS, windows, and Linux))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Repo of the project where the bug is spotted: https://github.com/l1xiangyi/GPT2/tree/main  What is the problem The problem occurs in main.py line 89 where I try to generate texts based on a NeruIPS dataset. This line calls the sample function that generates subsequent sentences based on the `start_text`. However the texts generated on macOS, Windows, and Linux are very different even though the seed is fixed in the `config.yml`.   macOS result: `sumbit/samples.txt`  Windows result: `sumbit/sample_windows.txt`  Windows result: `sumbit/sample_linux.txt`  Strange words like ""Zombie"" pop up a lot in the Windows and Linux generated texts.   How to reproduce this problem  Linux The simplest way is go create a new Colab notebook and run the following commands:  Then check the `samples.txt` in the `submit` folder.   Windows Similar steps to the Linux version. Just drop the special sign before each commands:   Versions  Versions:  macOS: Ran  Results:    Windows Ran  Result   Linux: Ran  Result: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",,"bug: text generation result is different for different operating systems (macOS, windows, and Linux)"," ğŸ› Describe the bug  Repo of the project where the bug is spotted: https://github.com/l1xiangyi/GPT2/tree/main  What is the problem The problem occurs in main.py line 89 where I try to generate texts based on a NeruIPS dataset. This line calls the sample function that generates subsequent sentences based on the `start_text`. However the texts generated on macOS, Windows, and Linux are very different even though the seed is fixed in the `config.yml`.   macOS result: `sumbit/samples.txt`  Windows result: `sumbit/sample_windows.txt`  Windows result: `sumbit/sample_linux.txt`  Strange words like ""Zombie"" pop up a lot in the Windows and Linux generated texts.   How to reproduce this problem  Linux The simplest way is go create a new Colab notebook and run the following commands:  Then check the `samples.txt` in the `submit` folder.   Windows Similar steps to the Linux version. Just drop the special sign before each commands:   Versions  Versions:  macOS: Ran  Results:    Windows Ran  Result   Linux: Ran  Result: ",2023-10-14T22:23:07Z,triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/111315,"I am not sure pytorch/pytorch is the best place to discuss this bug as it is not clear the bug is pytorch relatedyou may get more useful insight from asking our forum or by isolating a smaller repro. I do not know much about seed generation but I could conceive that the logic may be different on different platforms. That said, I would not expect the model to be do dependent on the original seed, so there may be bigger problems elsewhere.","Thank you Jane for explanation. We tried debugging the seed generation at first though but it does not seem to be problem with seed mechanisms. We also tried eliminating difference between line break symbols between mac and windows but also to no avail. Thank you for replying, and if it's not problem with the PyTorch framework then it possible is related to the pretrained GPT model itself. Thank you for pointing out the this bug is not clearly PyTorch related. It's just we tried many possibilities outside of PyTorch so I just posted it here. You can close my issue until I could repro it in a smaller scope. ",Okay! Closing but ofc feel free to reopen when you can identify that it's a torch issue~ 
792,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add torch.library.impl_device)ï¼Œ å†…å®¹æ˜¯ (  CC(Rewrite torch.library's documentation)  CC(Rename name>qualname in torch.library.impl_abstract)  CC(Add torch.library.impl_device)  CC(Align torch.library.impl with the new torch.library style)  CC(Make torch.library.define consistent with the new APIs) Use this to register an implementation for a specific device_type. We map the user provided devicetype(s) to DispatchKeys and then call torch.library.impl on them. They may also specify multiple device_types at once or specify ""device_types=default"" to get a CompositeExplicitAutograd registration. Test Plan:  new tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add torch.library.impl_device,"  CC(Rewrite torch.library's documentation)  CC(Rename name>qualname in torch.library.impl_abstract)  CC(Add torch.library.impl_device)  CC(Align torch.library.impl with the new torch.library style)  CC(Make torch.library.define consistent with the new APIs) Use this to register an implementation for a specific device_type. We map the user provided devicetype(s) to DispatchKeys and then call torch.library.impl on them. They may also specify multiple device_types at once or specify ""device_types=default"" to get a CompositeExplicitAutograd registration. Test Plan:  new tests",2023-10-14T20:01:01Z,release notes: composability,closed,0,4,https://github.com/pytorch/pytorch/issues/111309,"Can you tell me what the difference between impl and impl_device is? I know we did chat about this, but looking over this PR I am sure feeling mournful that you can't just use impl to mean impl_device","> Can you tell me what the difference between impl and impl_device is? I know we did chat about this, but looking over this PR I am sure feeling mournful that you can't just use impl to mean impl_device torch.library.impl is a preexisting API that lets people register kernels for dispatch keys. torch.library.impl_device is an API for registering kernels for specific device types. People start doing weird things when you tell them about DispatchKeys, so torch.library.impl_device hides that from the user. {torch.library.defiine, torch.library.impl_abstract, and torch.library.impl_device} are a collection of higherlevel APIs (that we'll likely expand) that don't require the user to know about the details of the PyTorch dispatcher.",We discussed this in person and resolved to use impl but remove mention of other dispatch keys as options.  do you want me to stamp this anyway and you get this in a follow up?,"> We discussed this in person and resolved to use impl but remove mention of other dispatch keys as options. >  >  do you want me to stamp this anyway and you get this in a follow up? No, let me open a new PR for this. We are also going to enable users passing in device names (e.g. ""cpu"", ""cuda"") via the impl APIs."
728,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Align torch.library.impl with the new torch.library style)ï¼Œ å†…å®¹æ˜¯ (  CC(Rewrite torch.library's documentation)  CC(Rename name>qualname in torch.library.impl_abstract)  CC(Add torch.library.impl_device)  CC(Align torch.library.impl with the new torch.library style)  CC(Make torch.library.define consistent with the new APIs) We add a new overload to torch.library.impl that accepts an optional Library arg. If provided, the lifetime of the registration will be tied to the Library arg, otherwise, it will live forever. Test Plan:  existing and new tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Align torch.library.impl with the new torch.library style,"  CC(Rewrite torch.library's documentation)  CC(Rename name>qualname in torch.library.impl_abstract)  CC(Add torch.library.impl_device)  CC(Align torch.library.impl with the new torch.library style)  CC(Make torch.library.define consistent with the new APIs) We add a new overload to torch.library.impl that accepts an optional Library arg. If provided, the lifetime of the registration will be tied to the Library arg, otherwise, it will live forever. Test Plan:  existing and new tests",2023-10-14T20:00:54Z,Merged ciflow/trunk release notes: composability,closed,0,2,https://github.com/pytorch/pytorch/issues/111308, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
489,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(nanogpt_generate: C++ compile times out, because the generated .cpp file is too large.)ï¼Œ å†…å®¹æ˜¯ (`nanogpt_generate` exists in torchbench before https://github.com/pytorch/benchmark/pull/1911, Repro: ``` benchmarks/dynamo/torchbench.py bfloat16 accuracy inference device cuda exportaotinductor  only nanogpt_generate )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,"nanogpt_generate: C++ compile times out, because the generated .cpp file is too large.","`nanogpt_generate` exists in torchbench before https://github.com/pytorch/benchmark/pull/1911, Repro: ``` benchmarks/dynamo/torchbench.py bfloat16 accuracy inference device cuda exportaotinductor  only nanogpt_generate ",2023-10-13T19:31:33Z,triaged oncall: pt2 module: dynamo module: aotinductor,closed,0,2,https://github.com/pytorch/pytorch/issues/111227,"IIRC, we're unrolling the autoregressive generation, which is bad.  was complaining about this in a separate context. Concretely, I propose that we should introduce a HOP that says ""this is a self contained block, please compile it only once"".",model changed and problem does not reproduce
613,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`torch.utils.checkpoint` drops custom Tensor attributes)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I noticed that if I have a torch.Tensor with a custom attribute, say `my_tensor.foo = ""huhu""`, PyTorch raises an error if this attribute is accessed in the backward when using `torch.utils.checkpoint`. There are no issues without torch.utils.checkpoint. Reproduction:  raising:  Note: subclassing `torch.Tensor` does not help. Tried with:   Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`torch.utils.checkpoint` drops custom Tensor attributes," ğŸ› Describe the bug Hi, I noticed that if I have a torch.Tensor with a custom attribute, say `my_tensor.foo = ""huhu""`, PyTorch raises an error if this attribute is accessed in the backward when using `torch.utils.checkpoint`. There are no issues without torch.utils.checkpoint. Reproduction:  raising:  Note: subclassing `torch.Tensor` does not help. Tried with:   Versions ",2023-10-13T14:05:58Z,module: checkpoint triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/111206,"This is expected, during the recomputation of AC you are operating on the detached inputs rather than the original ones, so it is expected that any attributes saved will not be preserved.",Thank you  
2029,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Regression][PT2.1][Dynamic] torch._dynamo.exc.TorchRuntimeError: Failed running call_method index_add)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Tensor.index_addwith out tensor has been regressed in PT2,1 upgrade. Same test used to work in PT2.1 Please use below code to reproduce the issue.   Error logs Traceback (most recent call last):   File ""debug_abs.py"", line 16, in      res = compl_fn(dim, index, source, alpha)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 328, in _fn     return fn(*args, **kwargs)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 490, in catch_errors     return callback(frame, cache_entry, hooks, frame_state)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 641, in _convert_frame     result = inner_convert(frame, cache_size, hooks, frame_state)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 133, in _fn     return fn(*args, **kwargs)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert     return _compile(   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 569, in _compile     guarded_code = compile_inner(code, one_graph, hooks, transform)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 189, in time_wrapper     r = func(*args, **kwargs)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 491, in compile_inner     out_code = transform_code_object(code, transform)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object     transformations(instructions, code_options)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 458, in transform     tracer.run()   File ""/tmp/lib/python3.8/sitepackag)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Regression][PT2.1][Dynamic] torch._dynamo.exc.TorchRuntimeError: Failed running call_method index_add," ğŸ› Describe the bug Tensor.index_addwith out tensor has been regressed in PT2,1 upgrade. Same test used to work in PT2.1 Please use below code to reproduce the issue.   Error logs Traceback (most recent call last):   File ""debug_abs.py"", line 16, in      res = compl_fn(dim, index, source, alpha)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 328, in _fn     return fn(*args, **kwargs)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 490, in catch_errors     return callback(frame, cache_entry, hooks, frame_state)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 641, in _convert_frame     result = inner_convert(frame, cache_size, hooks, frame_state)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 133, in _fn     return fn(*args, **kwargs)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert     return _compile(   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 569, in _compile     guarded_code = compile_inner(code, one_graph, hooks, transform)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 189, in time_wrapper     r = func(*args, **kwargs)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 491, in compile_inner     out_code = transform_code_object(code, transform)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object     transformations(instructions, code_options)   File ""/tmp/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 458, in transform     tracer.run()   File ""/tmp/lib/python3.8/sitepackag",2023-10-13T13:35:11Z,triaged oncall: pt2 module: dynamic shapes,closed,0,3,https://github.com/pytorch/pytorch/issues/111203,"I've got a dumb question that I cannot for the life of me find the answer to anywhere in docs, wiki, contrib guides, etc. What is the oncall: pt2 label?","we have an oncall rotation specifically for pt2 issues, so if you label it with that it will directly get to sent to those folks. Also, there are some people who subscribe to all pt2 issues like me  CC(Label tracking meta-issue (edit me to get automatically CC'ed on issues! cc bot))",same as https://github.com/pytorch/pytorch/pull/111208
300,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI045)ï¼Œ å†…å®¹æ˜¯ (Enable itermethodreturniterable (PYI045) Link: CC(Enable more flake8pyi ruff checks))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI045,Enable itermethodreturniterable (PYI045) Link: CC(Enable more flake8pyi ruff checks),2023-10-13T03:52:52Z,open source Merged ciflow/trunk release notes: distributed (pipeline) topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/111184, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch rebase origin/main` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
302,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI056)ï¼Œ å†…å®¹æ˜¯ (Enable unsupportedmethodcallonall (PYI056) Link: CC(Enable more flake8pyi ruff checks))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI056,Enable unsupportedmethodcallonall (PYI056) Link: CC(Enable more flake8pyi ruff checks),2023-10-13T03:28:49Z,triaged open source Stale release notes: onnx,closed,0,3,https://github.com/pytorch/pytorch/issues/111183, assigning you as reviewer, Failures are caused by making `__call__` global since `+=` is assignment unlike `append()`,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1022,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Regression on CUDA 12.1 for vanilla transformer layer)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, we are benchmarking deep vit models and found out the CUDA 12.1 binary are actually slower than CUDA 11.8 for PyTorch 2.1.0 release. This is not expected as we see better perf on other LLMs like Megatron(GPT) and OPT. One potential reason is those repos are using flashattention or xformers which benefit from CUDA 12 and transformer engine on H100 GPUs. While deep VIT is using vanilla implementation: https://github.com/lucidrains/vitpytorch/blob/main/vit_pytorch/deepvit.py. But we are not expecting a regression. Any ideas? thanks! here is the output showing ~10% regression on single node PT 2.1.0 + CUDA 11.8  PT 2.1.0 + CUDA 12.1  attached training script and reproduciable steps   Versions PyTorch 2.1.0 cuda 11.8 vs cuda 12.1 tested on AWS P5.48xlarge )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Regression on CUDA 12.1 for vanilla transformer layer," ğŸ› Describe the bug Hi, we are benchmarking deep vit models and found out the CUDA 12.1 binary are actually slower than CUDA 11.8 for PyTorch 2.1.0 release. This is not expected as we see better perf on other LLMs like Megatron(GPT) and OPT. One potential reason is those repos are using flashattention or xformers which benefit from CUDA 12 and transformer engine on H100 GPUs. While deep VIT is using vanilla implementation: https://github.com/lucidrains/vitpytorch/blob/main/vit_pytorch/deepvit.py. But we are not expecting a regression. Any ideas? thanks! here is the output showing ~10% regression on single node PT 2.1.0 + CUDA 11.8  PT 2.1.0 + CUDA 12.1  attached training script and reproduciable steps   Versions PyTorch 2.1.0 cuda 11.8 vs cuda 12.1 tested on AWS P5.48xlarge ",2023-10-12T23:07:56Z,needs reproduction module: performance module: cuda triaged,open,0,10,https://github.com/pytorch/pytorch/issues/111168,scripts and full logs vit_benchmark.zip, I think you found that building the flash/mem_eff sourceswith O3 on CUDA > 12 caused ~ 10% perf regression while with O2 it did not. Is that right?,"I only saw the regression with the mem_eff one, not with flash. We use `O2` in ptxas as a workaround: https://github.com/facebookresearch/xformers/blob/main/setup.pyL288","Hi  , not really, we see CUDA 12 helps with models that use flash/mem_eff implementations (Megatron/OPT) or models that use SDPA kernels, but we are not expecting regression for self implemented attention modules like this https://github.com/lucidrains/vitpytorch/blob/main/vit_pytorch/deepvit.pyL22","Ohhh I misinterpreted this, I am not sure why this is, could you try profiling the two one for cuda 11.8 and one for cuda 12.1 to see which ops are slower? Here is the documentation on the profilier: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html","we will keep looking into it, putting this out here in case someone has similar issues. Interested to see what it takes to get the best out of H100s","CC  could you try to reproduce it, please? "," Looking at the code I couldn't find a sync before the timing functions e.g., (`torch.cuda.synchronize`). Are you doing synchronization somewhere else, and if not, could you check if adding these syncs changes the benchmarking results?","A quick update on this, looks like it is due to a GammaBetaBackward kernel being slower. I'll work on a smaller reproducer and forward appropriately if necessary.",Considering the perf regression I propose we keep 11.8 is older CUDA version for 2.2 release
1238,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError in run_streaming_llama.py When Using Accelerate with Streaming LLMa Model on A4500 GPU)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Referring to the issue https://github.com/mithanlab/streamingllm/issues/37issue1940692615 Description: When running the run_streaming_llama.py script with the enable_streaming flag, I encountered a RuntimeError related to CUDA and the PyTorch Accelerate library. Steps to Reproduce: Set the environment variable: CUDA_VISIBLE_DEVICES=0 Run the following command:  Expected Behavior: The script should run successfully and provide streaming inference results. Actual Behavior: The script crashes with the following error:  GPU information  OS information:  Full error log:   Versions torch version:                   2.1.0 conda create yn streaming python=3.8 conda activate streaming pip install torch torchvision torchaudio pip install transformers==4.33.0 accelerate datasets evaluate wandb scikitlearn scipy sentencepiece python setup.py develop git clone git.com:mithanlab/streamingllm.git cd streamingllm/ python setup.py develop )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,RuntimeError in run_streaming_llama.py When Using Accelerate with Streaming LLMa Model on A4500 GPU," ğŸ› Describe the bug Referring to the issue https://github.com/mithanlab/streamingllm/issues/37issue1940692615 Description: When running the run_streaming_llama.py script with the enable_streaming flag, I encountered a RuntimeError related to CUDA and the PyTorch Accelerate library. Steps to Reproduce: Set the environment variable: CUDA_VISIBLE_DEVICES=0 Run the following command:  Expected Behavior: The script should run successfully and provide streaming inference results. Actual Behavior: The script crashes with the following error:  GPU information  OS information:  Full error log:   Versions torch version:                   2.1.0 conda create yn streaming python=3.8 conda activate streaming pip install torch torchvision torchaudio pip install transformers==4.33.0 accelerate datasets evaluate wandb scikitlearn scipy sentencepiece python setup.py develop git clone git.com:mithanlab/streamingllm.git cd streamingllm/ python setup.py develop ",2023-10-12T20:18:05Z,needs reproduction module: error checking triaged,open,1,3,https://github.com/pytorch/pytorch/issues/111158,Could you distill this into a smaller reproduction script?,"One way i got this crashing is with trying to call `cross_entropy()` with a large tensor ([1, 189000]) RTX 3060 WSL ubuntu 22.04 pytorch 12.1","I guess that this is an outofmemory issues, but may not be properly handled by PyTorch. It also occurs on when I try to do large batch unsupervised training ViT (visual transformer) on NVIDIA A4500 GPU when running out of GPU memory. "
2123,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument state_steps in method wrapper_CUDA___fused_adamw_))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When using deepspeed to train LoRA, I want to use the resume function of the trainer. The sample code is as follows:  Encountered the following error:  I think it is caused by not placing the step on the corresponding device when loading the optimizer parameters. !image But I don't know how to solve this problem.  Versions Collecting environment information... PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.28 Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.143.bsk.8amd64x86_64withglibc2.28 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: A100SXM80GB Nvidia driver version: 450.191.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.7.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian Address sizes:       46 bits physical, 57 bits virtual CPU(s):              128 Online CPU(s) list: 0127 Thread(s) per core:  2 Core(s) per socket:  32 Socket(s):           2 N)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",peft,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument state_steps in method wrapper_CUDA___fused_adamw_)"," ğŸ› Describe the bug When using deepspeed to train LoRA, I want to use the resume function of the trainer. The sample code is as follows:  Encountered the following error:  I think it is caused by not placing the step on the corresponding device when loading the optimizer parameters. !image But I don't know how to solve this problem.  Versions Collecting environment information... PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.28 Python version: 3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.143.bsk.8amd64x86_64withglibc2.28 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: A100SXM80GB Nvidia driver version: 450.191.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.7.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian Address sizes:       46 bits physical, 57 bits virtual CPU(s):              128 Online CPU(s) list: 0127 Thread(s) per core:  2 Core(s) per socket:  32 Socket(s):           2 N",2023-10-12T06:42:12Z,module: optimizer triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/111123,"~This should be fixed on the latest version of torch, could you install torch 2.1 instead?~ This is likely the same issue as  CC(Tensors in different devices). Closing as duplicate (there is a linked inflight solution that we are working to ship soonI had mistakenly thought it was already landed ğŸ« )."
294,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI042)ï¼Œ å†…å®¹æ˜¯ (Enable snakecasetypealias (PYI042) Link: CC(Enable more flake8pyi ruff checks))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI042,Enable snakecasetypealias (PYI042) Link: CC(Enable more flake8pyi ruff checks),2023-10-12T03:18:42Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/111114," merge FYI: once the PR is accepted, no need to wait for me, you can ask the bot to merge it yourself!", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
301,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add comment to keep PYI041 disabled)ï¼Œ å†…å®¹æ˜¯ (Enable redundantnumericunion (PYI041) Link: CC(Enable more flake8pyi ruff checks) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add comment to keep PYI041 disabled,Enable redundantnumericunion (PYI041) Link: CC(Enable more flake8pyi ruff checks) ,2023-10-12T03:12:34Z,triaged open source release notes: distributed (fsdp) topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/111112,Linking https://github.com/pylintdev/pylint/issues/7084 With the current mypy support scope we might not be able to enable PYI041 for now,"Instead, let's just add a comment about why it should remain disabled."
399,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI034)ï¼Œ å†…å®¹æ˜¯ (Enable nonselfreturntype (PYI034) Link: CC(Enable more flake8pyi ruff checks) **EDIT**: to newly added reviewers, please ignore the request, it's due to a rebase error :sweat_smile:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI034,"Enable nonselfreturntype (PYI034) Link: CC(Enable more flake8pyi ruff checks) **EDIT**: to newly added reviewers, please ignore the request, it's due to a rebase error :sweat_smile:  ",2023-10-12T02:56:01Z,triaged open source Merged ciflow/trunk release notes: quantization topic: not user facing ciflow/mps module: inductor module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/111105," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
299,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI030)ï¼Œ å†…å®¹æ˜¯ (Enable unnecessaryliteralunion (PYI030) Link: CC(Enable more flake8pyi ruff checks))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI030,Enable unnecessaryliteralunion (PYI030) Link: CC(Enable more flake8pyi ruff checks),2023-10-12T02:43:43Z,triaged open source Merged topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/111103," label ""topic: not user facing""", merge i," Merge started Your change will be merged while ignoring the following 5 checks: pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 2, 5, linux.g5.4xlarge.nvidia.gpu), .github/workflows/generatedlinuxbinarymanywheelmain.yml, .github/workflows/trunk.yml, .github/workflows/generatedlinuxbinarylibtorchcxx11abimain.yml, .github/workflows/generatedlinuxbinarylibtorchprecxx11main.yml Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
296,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Enable Ruff's Flake8 PYI018)ï¼Œ å†…å®¹æ˜¯ (Enable unusedprivatetypevar (PYI018) Link: CC(Enable more flake8pyi ruff checks))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE] Enable Ruff's Flake8 PYI018,Enable unusedprivatetypevar (PYI018) Link: CC(Enable more flake8pyi ruff checks),2023-10-12T01:16:39Z,open source Merged ciflow/trunk topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/111101, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,This pull request has been merged in pytorch/pytorch.
963,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Option to disable fastpath in MHA)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I have trained a transformer model but I'm not able to run inference with it because of fastpath errors commented here: CC(Fast path for MultiheadAttention does not work with instances with dropout despite being in eval mode) and CC(MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads). I would like to run the inference with the normal path but I can't find any options to disable fastpath.  Alternatives One option is to add a flag like  or use environment variables.  Additional context I could disable it by setting the following line with a nonempty string: https://github.com/pytorch/pytorch/blob/d3205f83770f83cfa315a38f4448c64670758b03/torch/nn/modules/activation.pyL1111 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Option to disable fastpath in MHA," ğŸš€ The feature, motivation and pitch I have trained a transformer model but I'm not able to run inference with it because of fastpath errors commented here: CC(Fast path for MultiheadAttention does not work with instances with dropout despite being in eval mode) and CC(MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads). I would like to run the inference with the normal path but I can't find any options to disable fastpath.  Alternatives One option is to add a flag like  or use environment variables.  Additional context I could disable it by setting the following line with a nonempty string: https://github.com/pytorch/pytorch/blob/d3205f83770f83cfa315a38f4448c64670758b03/torch/nn/modules/activation.pyL1111 ",2023-10-11T18:59:14Z,needs design,closed,2,1,https://github.com/pytorch/pytorch/issues/111070,"Addressed by https://github.com/pytorch/pytorch/pull/112212, feel free to reopen if this does not satisfy your usecase"
1114,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Update ACPT to support Python 3.11)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Azure Container for PyTorch (aka ACPT) is currently distributed using python 3.8/3.9. Although this is ok for Pytorch 1.x series, the latest PyTorch 2.x leverages Python 3.11 features for better graph lowering from `torch.nn.Module` to `torch.fx.GraphModule`. It also has model transformation/optimization that are better maintained for newer python versions. Below is a list of several places in which we can see Dynamo forking behavior between old (mostly 3.8 and 3.9) and newer python versions (3.10 and 3.11)  By using dynamo with python 3.8/3.9, we can obtain suboptimal graphs that don't leverage the latest optimizations coming from latest dynamo. Another side effect is that important third party dependencies may not be optimal for old python too, such as `transformers` and others  Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Update ACPT to support Python 3.11," ğŸš€ The feature, motivation and pitch Azure Container for PyTorch (aka ACPT) is currently distributed using python 3.8/3.9. Although this is ok for Pytorch 1.x series, the latest PyTorch 2.x leverages Python 3.11 features for better graph lowering from `torch.nn.Module` to `torch.fx.GraphModule`. It also has model transformation/optimization that are better maintained for newer python versions. Below is a list of several places in which we can see Dynamo forking behavior between old (mostly 3.8 and 3.9) and newer python versions (3.10 and 3.11)  By using dynamo with python 3.8/3.9, we can obtain suboptimal graphs that don't leverage the latest optimizations coming from latest dynamo. Another side effect is that important third party dependencies may not be optimal for old python too, such as `transformers` and others  Alternatives _No response_  Additional context _No response_",2023-10-10T18:32:53Z,module: onnx triaged onnx-triaged release notes: onnx,closed,0,3,https://github.com/pytorch/pytorch/issues/110965," We have built ACPT with python 3.10.  Is there any technical blocker to customize sccl+torch+py3.11 ? If so, we can build ACPT with python 3.11? ","At the moment I am not aware of any issues with building ACPT with Python 3.11, but we can definitely give this a try.  are you looking for any timeline on this?","> At the moment I am not aware of any issues with building ACPT with Python 3.11, but we can definitely give this a try.  are you looking for any timeline on this? We are not currently blocked on this, but the sooner we get, the better."
729,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable more flake8-pyi ruff checks)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch https://github.com/pytorch/pytorch/pull/110830 enabled a bunch of use flake8PYI checks to improve the quality of type hints on the PyTorch codebase. While this was a good first pass, many rules need to removed from the ignore list in the pyproject.toml. Fixing these rules an enabling the lints would be a good starter task. For other rules to enable, see this starter task:  CC(Enable more flake8-bugbear lints)  Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Enable more flake8-pyi ruff checks," ğŸš€ The feature, motivation and pitch https://github.com/pytorch/pytorch/pull/110830 enabled a bunch of use flake8PYI checks to improve the quality of type hints on the PyTorch codebase. While this was a good first pass, many rules need to removed from the ignore list in the pyproject.toml. Fixing these rules an enabling the lints would be a good starter task. For other rules to enable, see this starter task:  CC(Enable more flake8-bugbear lints)  Alternatives _No response_  Additional context _No response_",2023-10-10T14:57:36Z,good first issue module: lint triaged actionable,open,0,4,https://github.com/pytorch/pytorch/issues/110950,"Hi,i'm a first contributor, may i try B026 in CC(Enable more flake8bugbear lints)?","Keeping a list to track Ruff's flake8PYI checks that cannot be enabled and why:  [ ] **PYI041**: mypy support scope, linking https://github.com/pylintdev/pylint/issues/7084  [ ] **PYI036**: JIT issues","Hi, I'm a firsttime contributor, and would like to help on this task. Can I work on fixing the ignore for PY1024? It seems like it's still enabled on main, and I haven't seen any previous PRs work on it. ",Take
312,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] add config for displaying all guard failures)ï¼Œ å†…å®¹æ˜¯ (Fixes  CC([dynamo] Prefer to report more guard failures) Example output:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[dynamo] add config for displaying all guard failures,Fixes  CC([dynamo] Prefer to report more guard failures) Example output:   ,2023-10-10T05:03:33Z,triaged open source Merged ciflow/trunk module: dynamo ciflow/inductor release notes: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/110927," label ""release notes: dynamo""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"chuang I'm working on a PR that will report even more guard failures (not just the last guard function), but these will be run only when a recompile occurs, so that guard failure reporting is no longer on the critical path. When do you plan on doing the followup PR to fix the tests checking for recompile logging? I can include that as part of my work if you haven't started on it."," sure, I have not been working on this for a while. Feel free to request a review from me once you are ready."
828,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The NCCL kernel did not start as expected)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I apologize for the interruption. I've encountered a puzzling issue. I initiated an asynchronous batch_send_recv communication as per the code below, but when observing on nsys, the kernel launch doesn't entirely align with my expectations. In some loops, both batch_send_recv and matmul are perfectly initiated simultaneously, but in others, the communication is significantly delayed. This unpredictable behavior is not what I intended. Is there a way to ensure that the kernels are launched concurrently? The nsys graph is here.    Versions torch  Version :2.1.0a0+4136153 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,The NCCL kernel did not start as expected," ğŸ› Describe the bug Hello, I apologize for the interruption. I've encountered a puzzling issue. I initiated an asynchronous batch_send_recv communication as per the code below, but when observing on nsys, the kernel launch doesn't entirely align with my expectations. In some loops, both batch_send_recv and matmul are perfectly initiated simultaneously, but in others, the communication is significantly delayed. This unpredictable behavior is not what I intended. Is there a way to ensure that the kernels are launched concurrently? The nsys graph is here.    Versions torch  Version :2.1.0a0+4136153 ",2023-10-10T04:03:18Z,oncall: distributed,open,0,1,https://github.com/pytorch/pytorch/issues/110921,"When I modified my code in this manner, the two kernels magically aligned in every loop iteration. However, this introduced additional memory and speed overheads, which are unacceptable in a highperformance setting. Can you advise on how to address this issue? It has been puzzling me for two weeks now.  "
288,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Skip resizing storage in FSDP during dynamo trace)ï¼Œ å†…å®¹æ˜¯ (  CC(Skip resizing storage in FSDP during dynamo trace))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Skip resizing storage in FSDP during dynamo trace,  CC(Skip resizing storage in FSDP during dynamo trace),2023-10-09T20:10:40Z,open source Stale release notes: distributed (fsdp),closed,0,4,https://github.com/pytorch/pytorch/issues/110888, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/voznesenskym/237/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/110888`)","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
541,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow specifiying inputs as GradientEdge in autograd APIs)ï¼Œ å†…å®¹æ˜¯ (This can be useful for advanced users (like AOTAutograd) who don't want to keep the corresponding Tensor alive (for memory reasons for example) or when inplace op will change the Tensor's grad_fn (but gradients wrt to the original value is needed). I went minimal API change but open to suggestions. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Allow specifiying inputs as GradientEdge in autograd APIs,This can be useful for advanced users (like AOTAutograd) who don't want to keep the corresponding Tensor alive (for memory reasons for example) or when inplace op will change the Tensor's grad_fn (but gradients wrt to the original value is needed). I went minimal API change but open to suggestions. ,2023-10-09T15:28:52Z,Merged ciflow/trunk release notes: autograd topic: new features module: inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/110867,Thanks Alban!, , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
416,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE]: Enable ruff's flake8-PYI rules)ï¼Œ å†…å®¹æ˜¯ (Enable Flake8PYI rules codebase wide. Most of the rules already match our codebase style, the remaining ones that were not autofixed I have added to the pyproject.toml to be enabled in a later PR.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE]: Enable ruff's flake8-PYI rules,"Enable Flake8PYI rules codebase wide. Most of the rules already match our codebase style, the remaining ones that were not autofixed I have added to the pyproject.toml to be enabled in a later PR.",2023-10-08T20:11:31Z,open source better-engineering Merged ciflow/trunk topic: not user facing suppress-bc-linter,closed,1,2,https://github.com/pytorch/pytorch/issues/110830, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
421,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] remove extra buffer creation in realize into)ï¼Œ å†…å®¹æ˜¯ (  CC([Inductor] remove extra buffer creation in realize into) **Summary** Fix the llama performance regression reported in  CC([inductor][cpu] performance regression) **Test Plan**  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[Inductor] remove extra buffer creation in realize into,  CC([Inductor] remove extra buffer creation in realize into) **Summary** Fix the llama performance regression reported in  CC([inductor][cpu] performance regression) **Test Plan**  ,2023-10-07T08:34:01Z,open source Stale ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/110787,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
973,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] Add size, stride, storage_offset to RAIIAtenTensorHandle)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] Add AOTI ABI shim function for torch.nonzero)  CC([inductor] Add size, stride, storage_offset to RAIIAtenTensorHandle)  CC([inductor] Add AOTI ABI shim function for repeat_interleave.Tensor)  CC([inductor] Add aoti_torch_dtype_bool to AOTI ABI shim) Summary: For unbacked SymInts, the C++ wrapper codegen can generate expressions like `buf123.size()` or `.stride()` or `.storage_offset()`: https://github.com/pytorch/pytorch/blob/7cc0020a80527207a1725e6d21ce7c326668fe0d/torch/_inductor/ir.pyL2504L2520 Here we add corresponding methods to the `RAIIAtenTensorHandle` class so that the above codegen works in the ABI compatibility mode. Test Plan: CI + the following PR Reviewers: Subscribers: Tasks: Tags: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[inductor] Add size, stride, storage_offset to RAIIAtenTensorHandle","  CC([inductor] Add AOTI ABI shim function for torch.nonzero)  CC([inductor] Add size, stride, storage_offset to RAIIAtenTensorHandle)  CC([inductor] Add AOTI ABI shim function for repeat_interleave.Tensor)  CC([inductor] Add aoti_torch_dtype_bool to AOTI ABI shim) Summary: For unbacked SymInts, the C++ wrapper codegen can generate expressions like `buf123.size()` or `.stride()` or `.storage_offset()`: https://github.com/pytorch/pytorch/blob/7cc0020a80527207a1725e6d21ce7c326668fe0d/torch/_inductor/ir.pyL2504L2520 Here we add corresponding methods to the `RAIIAtenTensorHandle` class so that the above codegen works in the ABI compatibility mode. Test Plan: CI + the following PR Reviewers: Subscribers: Tasks: Tags: ",2023-10-06T21:07:30Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/110764,"I have a highlevel question. We already have API calls like  that can be used to implement , e.g.   So, instead of adding new APIs, I am wondering if we could just codegen a similar code like above by leveraging the existing APIs. One benefit would be that we end up maintaining a minimal set of interfaces. ","> So, instead of adding new APIs, I am wondering if we could just codegen a similar code like above by leveraging the existing APIs. One benefit would be that we end up maintaining a minimal set of interfaces.  I agree that reducing the extent of the API is generally good. However, the problem with relying on the existing functions is that it's impossible to do range checking, as they simply provide a pointer to the list of sizes. We've discussed this briefly offline with  and decided to delegate the range checking (as well as proper error messaging) to ATen ops. And that's the reason for the new functions relying on `tensor>size(d)` and friends instead of `tensor>sizes()` as in the preexisting API. Does this make sense?","> > So, instead of adding new APIs, I am wondering if we could just codegen a similar code like above by leveraging the existing APIs. One benefit would be that we end up maintaining a minimal set of interfaces. >  >  I agree that reducing the extent of the API is generally good. However, the problem with relying on the existing functions is that it's impossible to do range checking, as they simply provide a pointer to the list of sizes. We've discussed this briefly offline with  and decided to delegate the range checking (as well as proper error messaging) to ATen ops. And that's the reason for the new functions relying on `tensor>size(d)` and friends instead of `tensor>sizes()` as in the preexisting API. Does this make sense? I see. Make sense. Thanks for the clarification. ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
903,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add padding support for dense matrices in (semi-structured sparse @dense) matmul)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Currently there are shape constraints for both the sparse **and** the dense matrix when doing semistructured sparse matmul.  This applies for both cuSPARSELt and CUTLASS.  For cuSPARSELt the shapes constraints are as follows:  for CUTLASS the shape constraints are:  We should add support for padding the dense tensor if we are passed in a tensor that does not satisfy shape constraints.  One particular use case that would need this is text generation with LLMs, as the matrix muliplications are of the shape (1, hidden_dim, output_dim).   Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Add padding support for dense matrices in (semi-structured sparse @dense) matmul," ğŸš€ The feature, motivation and pitch Currently there are shape constraints for both the sparse **and** the dense matrix when doing semistructured sparse matmul.  This applies for both cuSPARSELt and CUTLASS.  For cuSPARSELt the shapes constraints are as follows:  for CUTLASS the shape constraints are:  We should add support for padding the dense tensor if we are passed in a tensor that does not satisfy shape constraints.  One particular use case that would need this is text generation with LLMs, as the matrix muliplications are of the shape (1, hidden_dim, output_dim).   Alternatives _No response_  Additional context _No response_ ",2023-10-06T19:14:18Z,module: sparse triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/110744,:  CC(cuSPARSELt Integration)issuecomment1681601508 you may have been running into this issue. ,https://github.com/pytorch/pytorch/pull/110583 should add support for this
450,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Getting error when trying to use pytorch 1.12.1 with jetpack version 5.1.1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I am using Cuda 11.4 with the PyTorch 1.12 model on jetpack 5.1.1, I am getting error â€¦  throws error   Versions pytorch build from source v1.12 cuda 11.4 jetpack 5.1 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Getting error when trying to use pytorch 1.12.1 with jetpack version 5.1.1," ğŸ› Describe the bug When I am using Cuda 11.4 with the PyTorch 1.12 model on jetpack 5.1.1, I am getting error â€¦  throws error   Versions pytorch build from source v1.12 cuda 11.4 jetpack 5.1 ",2023-10-06T14:47:44Z,needs reproduction module: build module: cuda triaged module: arm,closed,0,12,https://github.com/pytorch/pytorch/issues/110707,"this is probably a problem with your build env, maybe you are not compiling for the right arch",its build from source successfully using steps  git clone https://github.com/pytorch/pytorch.git cd pytorch/ git checkout v1.12 mkdir build install cd build cmake  DBUILD_SHARED_LIBS:BOOL=ON DCMAKE_BUILD_TYPE:STRING=Release DPYTHON_EXECUTABLE:PATH=`which python3` DUSE_NCCL=OFF DUSE_DISTRIBUTED=OFF DUSE_QNNPACK=OFF DUSE_PYTORCH_QNNPACK=OFF Wnodev DCMAKE_INSTALL_PREFIX:PATH=../install .. make install `," can you share a full config log? I.e. looks like CUDA was not found during the build) Also, is the problem still reproducible with PyTorch2.1 and CUDA11.8 (there is not much one can do about PyTorch1.12 build issues)",">  can you share a full config log? I.e. looks like CUDA was not found during the build) Also, is the problem still reproducible with PyTorch2.1 and CUDA11.8 (there is not much one can do about PyTorch1.12 build issues) yes it is as follow  ",">  can you share a full config log? I.e. looks like CUDA was not found during the build) Also, is the problem still reproducible with PyTorch2.1 and CUDA11.8 (there is not much one can do about PyTorch1.12 build issues) but I have used the identical versions on Jetpack 5.0.1 and it worked I didn't have any issues. Also, I cannot upgrade cuda from 11.4 to 11.8 as most of the things developed until now are failing with this upgrade. So it will take more time but if nothing works I will go for this option","Also do you have any idea if I can build pytorch 2.1 on arm64 ,as its first requirement looks like it needs c++17","Hi  , yes, you can build pytorch 2.1 on arm64. please refer to the following build scripts,  https://github.com/pytorch/builder/blob/main/aarch64_linux/build_aarch64_wheel.py note: These scripts provide build steps for openBLAS and MKLDNN (+Arm Compute Library) backend in PyTorch, but don't have CUDA build steps."," looks like error comes from the fact, that CopyKernel could not find a kernel of your architecture. Please correct me if I'm wrong, but you have `sm_87` capable device on that system, don't you? In that case, do you mind running `cuobjdump lelf /apth/to/Copy.cu.o` and sharing output here?"," I don't know if you need a custom build, but you can download the PyTorch Jetson wheels from here or here. ",> Copy.cu.o yes its sm_87  $cuobjdump lelf /mnt/nvme0n1/pytorch/build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/Copy.cu.o ELF file    1: Copy.cu.1.sm_87.cubin,">  I don't know if you need a custom build, but you can download the PyTorch Jetson wheels from here or here. I tried but it doesn't install libtorch library which I need for compiling Pytorch in c++","solved the issue the main reason is in Jetpack 5.1 they have changed some of the folder structure and names for example usr/local/lib64/stubs, where caffe2 stores some cudarelated libraries to usr/local/lib64/stubs_  So I was looking at wrong folder. Thanks everyone for there input."
1442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(sliding_window attention in scaled_dot_product)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Support for sliding window attention (Mistral 7B) It is not very difficult, just add the setting sliding_window, and add 2 lines to modify the mask. Here is an example how the mask is computed in OpenNMTpy: (see the two lines in bold)     def _compute_dec_mask(self, tgt_pad_mask, future):         tgt_len = tgt_pad_mask.size(1)         if not future:   apply future_mask, result mask in (B, T, T)             future_mask = torch.ones(                 [tgt_len, tgt_len],                 device=tgt_pad_mask.device,                 dtype=torch.uint8,             )             future_mask = future_mask.tril_(0)             **if self.sliding_window > 0:                 future_mask = future_mask.triu_(self.sliding_window)**             future_mask = future_mask.bool()             future_mask = ~future_mask.view(1, tgt_len, tgt_len)             dec_mask = torch.gt(tgt_pad_mask + future_mask, 0)         else:   only mask padding, result mask in (B, 1, T)             dec_mask = tgt_pad_mask         return dec_mask  Alternatives if the plan is to implement flash attention 2 in scaled_dot_product, then sliding_window is supported in 2.3  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",mistral,sliding_window attention in scaled_dot_product," ğŸš€ The feature, motivation and pitch Support for sliding window attention (Mistral 7B) It is not very difficult, just add the setting sliding_window, and add 2 lines to modify the mask. Here is an example how the mask is computed in OpenNMTpy: (see the two lines in bold)     def _compute_dec_mask(self, tgt_pad_mask, future):         tgt_len = tgt_pad_mask.size(1)         if not future:   apply future_mask, result mask in (B, T, T)             future_mask = torch.ones(                 [tgt_len, tgt_len],                 device=tgt_pad_mask.device,                 dtype=torch.uint8,             )             future_mask = future_mask.tril_(0)             **if self.sliding_window > 0:                 future_mask = future_mask.triu_(self.sliding_window)**             future_mask = future_mask.bool()             future_mask = ~future_mask.view(1, tgt_len, tgt_len)             dec_mask = torch.gt(tgt_pad_mask + future_mask, 0)         else:   only mask padding, result mask in (B, 1, T)             dec_mask = tgt_pad_mask         return dec_mask  Alternatives if the plan is to implement flash attention 2 in scaled_dot_product, then sliding_window is supported in 2.3  Additional context _No response_ ",2023-10-06T13:18:48Z,triaged module: sdpa,open,0,0,https://github.com/pytorch/pytorch/issues/110702
1185,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(variable type mismatch when trying compile)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When updating the pytorch on kornia we found a weird issue when trying to compile the GuidedBlur module. The error trace looks like it's some data type issue or related `aten.reflection_pad2d`, but I don't understand this issue.   The CI error log: https://github.com/kornia/kornia/actions/runs/6411290175/job/17408312107step:5:21801  The module we are trying to compile: https://github.com/kornia/kornia/blob/03e71e3009ed8a304f41a7ab2ab00da0975a9aab/kornia/filters/guided.pyL164 Locally, I also get it working for a group of parameters and not for the others: Working with:  kernel_size=5; subsample=1; not working with:  kernel_size=5; subsample=2;  kernel_size=(5, 7); subsample=1;  kernel_size=(5, 7); subsample=2; The compile is working before for it, but I wasn't able to reduce it to a minimum reproducible example. I also didn't find related issues.  Error logs   Minified repro _No response_  Versions python 3.10.13 torch 2.1.0  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,variable type mismatch when trying compile," ğŸ› Describe the bug When updating the pytorch on kornia we found a weird issue when trying to compile the GuidedBlur module. The error trace looks like it's some data type issue or related `aten.reflection_pad2d`, but I don't understand this issue.   The CI error log: https://github.com/kornia/kornia/actions/runs/6411290175/job/17408312107step:5:21801  The module we are trying to compile: https://github.com/kornia/kornia/blob/03e71e3009ed8a304f41a7ab2ab00da0975a9aab/kornia/filters/guided.pyL164 Locally, I also get it working for a group of parameters and not for the others: Working with:  kernel_size=5; subsample=1; not working with:  kernel_size=5; subsample=2;  kernel_size=(5, 7); subsample=1;  kernel_size=(5, 7); subsample=2; The compile is working before for it, but I wasn't able to reduce it to a minimum reproducible example. I also didn't find related issues.  Error logs   Minified repro _No response_  Versions python 3.10.13 torch 2.1.0  ",2023-10-06T11:11:17Z,triaged bug oncall: pt2 module: dynamic shapes,closed,0,7,https://github.com/pytorch/pytorch/issues/110696,what if you set dynamic=False on your torch.compile?,"> what if you set dynamic=False on your torch.compile? This works! Too slow, but works! When we should set the `dynamic` parameter? or would this just be for debugging?","This just confirms to me that it's a dynamic shapes problem. The inductor minifier did not work, is that right? https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html TORCHDYNAMO_REPRO_AFTER=""aot"" If it didn't, if you can give me nonminified repro instructions I'll take a look after the LF slide deadline. If you post a debug log with `TORCH_LOGS=+dynamo,+aot,dynamic,+inductor` that'd be helpful too","  The minifier is working, here is it:   What is weird is, that if we isolate the code used in the test, and call it just one time it works (but will fail if try to compile again within other parameters) :) The failing test is: https://github.com/kornia/kornia/blob/e9d6f55b59c4d85137b178b906347ddd34a30a2c/test/filters/test_guided.pyL93 We pass the `torch.compile` as the fixture `torch_optimizer` (https://github.com/kornia/kornia/blob/e9d6f55b59c4d85137b178b906347ddd34a30a2c/conftest.pyL68)      if it is called just one time it will work fine, if we add a second call within other parameters it will fail  the order of the call didn't change anything        here is the minified of this code snippet  ","Yeah, the minifier is a little dorky, it doesn't know to force things to be dynamic (which is why calling it again triggers the failure). Thanks for getting it.",I tested https://github.com/pytorch/pytorch/pull/110988 on the GuidedBlur repro and it appears to work. I don't know if it recompiles too much though.,"to confirm, this fix did not go out into 2.1.1?"
615,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP][optim_state_dict] Make the new optimizer allgather fusion work with fine-tuning models)ï¼Œ å†…å®¹æ˜¯ (  CC([FSDP][optim_state_dict] Make the new optimizer allgather fusion work with finetuning models) With use_orig_params=True, it is possible that some parameters with the same FlatParameter are in the optimizer while others parameters are frozen. This PR makes the allgather fusion logic support the case. Differential Revision: D49922028)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,[FSDP][optim_state_dict] Make the new optimizer allgather fusion work with fine-tuning models,"  CC([FSDP][optim_state_dict] Make the new optimizer allgather fusion work with finetuning models) With use_orig_params=True, it is possible that some parameters with the same FlatParameter are in the optimizer while others parameters are frozen. This PR makes the allgather fusion logic support the case. Differential Revision: D49922028",2023-10-04T18:51:57Z,Merged ciflow/trunk release notes: distributed (fsdp) ciflow/periodic,closed,0,3,https://github.com/pytorch/pytorch/issues/110540,The latest version supports the no_grad > grad > no_grad transition and changes the UT to verify the usage., merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP][optim_state_dict] Add the missing assert messages)ï¼Œ å†…å®¹æ˜¯ (  CC([FSDP][optim_state_dict] Make the new optimizer allgather fusion work with finetuning models)  CC([FSDP][optim_state_dict] Add the missing assert messages) As title Differential Revision: D49915768)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,[FSDP][optim_state_dict] Add the missing assert messages,  CC([FSDP][optim_state_dict] Make the new optimizer allgather fusion work with finetuning models)  CC([FSDP][optim_state_dict] Add the missing assert messages) As title Differential Revision: D49915768,2023-10-04T18:51:48Z,release notes: distributed (fsdp),closed,0,0,https://github.com/pytorch/pytorch/issues/110538
2033,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(performance drop because batching rule for aten::_scaled_dot_product_attention_math is not yet implemented)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Pretty much doing what the stacktrace is asking. I am trying to write a batched implementation of the DETR code in the original paper (https://arxiv.org/pdf/2005.12872.pdf), where a single set of queries is decoded over a batch containing several images, without using a custom transformer implementation. torch.vmap seemed like the ideal function for the job. As far as I can tell, my implementation works, but I get the following stacktrace:   This ""bug"" can be reproduced with the following code:   I am on PyTorch CPU, I've seen other people getting similar errors on GPUs but I apologize if this fact makes this bug report irrelevant.   Versions Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Fedora Linux 38 (Workstation Edition) (x86_64) GCC version: (GCC) 13.2.1 20230728 (Red Hat 13.2.11) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.37 Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.5200.fc38.x86_64x86_64withglibc2.37 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             8 Online CPU(s) list:                07 Vendor ID:                         )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,performance drop because batching rule for aten::_scaled_dot_product_attention_math is not yet implemented," ğŸ› Describe the bug Pretty much doing what the stacktrace is asking. I am trying to write a batched implementation of the DETR code in the original paper (https://arxiv.org/pdf/2005.12872.pdf), where a single set of queries is decoded over a batch containing several images, without using a custom transformer implementation. torch.vmap seemed like the ideal function for the job. As far as I can tell, my implementation works, but I get the following stacktrace:   This ""bug"" can be reproduced with the following code:   I am on PyTorch CPU, I've seen other people getting similar errors on GPUs but I apologize if this fact makes this bug report irrelevant.   Versions Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Fedora Linux 38 (Workstation Edition) (x86_64) GCC version: (GCC) 13.2.1 20230728 (Red Hat 13.2.11) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.37 Python version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64bit runtime) Python platform: Linux6.5.5200.fc38.x86_64x86_64withglibc2.37 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      39 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             8 Online CPU(s) list:                07 Vendor ID:                         ",2023-10-04T15:27:09Z,module: vmap module: functorch,closed,3,7,https://github.com/pytorch/pytorch/issues/110525,"I was unable to reproduce that warning on CPU, but reproduced on GPU.  I can take a look into this.", dumb q but do all aten ops (composite implicit included) require vmap rules?, Composite implicit ones just need to marked as decomposable  other ones do require vmap rules yeah.,I'm also getting this error on GPU. Seems like the rule should be quite easy to implement since it would just amount to a flatten and reshape?,I can add the decomp rule like Horace said on Monday,Any updates regarding this issue ? I'm experiencing the same issue with a gpu. Steps to reproduce: Env:  Code  Output: ,Fixed by https://github.com/pytorch/pytorch/pull/133964
1541,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DynamicQuantizedLinear shows incorrect qscheme after applying eager mode dynamic quantization)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I've been playing around with quantization for a little bit and I wanted to verify the qscheme (`torch.per_tensor_symmetric`) I defined for my weight and activation observers aligned with the modules I was quantizing. I just quantized my linear layer (`nn.Linear` class), and I noticed that it printed the wrong  qscheme (it was `torch.per_tensor_affine`) when I viewed it after quantization. For sanity sake, I looked at the weight and it appeared the correct qscheme was applied (the `scale` was an appropriate value and the` zero_point` was 0), but for some reason it still said the qscheme`was `per_tensor_affine`.  To double check further, I changed the qscheme for the observers to` torch.per_tensor_affine` and when I viewed the weights again it appeared the correct qscheme was applied (scale was an appropriate value and the `zero_point` was an arbitrarily nonzero number). Overall, it appears to me the application of the qscheme works in practice, but the information displayed when you print the model (or a module in the model) appears to be wrong which can be misleading.  These calls are what shows the information when printing the `DynamicQuantizedLinear` module  Below is example to replicate this issue:  Output:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DynamicQuantizedLinear shows incorrect qscheme after applying eager mode dynamic quantization," ğŸ› Describe the bug I've been playing around with quantization for a little bit and I wanted to verify the qscheme (`torch.per_tensor_symmetric`) I defined for my weight and activation observers aligned with the modules I was quantizing. I just quantized my linear layer (`nn.Linear` class), and I noticed that it printed the wrong  qscheme (it was `torch.per_tensor_affine`) when I viewed it after quantization. For sanity sake, I looked at the weight and it appeared the correct qscheme was applied (the `scale` was an appropriate value and the` zero_point` was 0), but for some reason it still said the qscheme`was `per_tensor_affine`.  To double check further, I changed the qscheme for the observers to` torch.per_tensor_affine` and when I viewed the weights again it appeared the correct qscheme was applied (scale was an appropriate value and the `zero_point` was an arbitrarily nonzero number). Overall, it appears to me the application of the qscheme works in practice, but the information displayed when you print the model (or a module in the model) appears to be wrong which can be misleading.  These calls are what shows the information when printing the `DynamicQuantizedLinear` module  Below is example to replicate this issue:  Output:   Versions  ",2023-10-04T14:35:59Z,oncall: quantization triaged,open,0,7,https://github.com/pytorch/pytorch/issues/110515,This same behavior can be replicated using FX Graph Mode Quantization,"Update: I'm facing similar issues related here: CC(PyTorch Static Symmetric Quantization Results in NonZero Zero Point Values.) I thought it was just a bug with what is being shown and that the appropriate scheme was being applied, however as  said, the `QConfig` defined appears to be ignored when quantizing the model. I myself am also trying to reproduce practices presented in different research papers and it recommends tensors and activations to be quantized using a pertensor symmetric scheme. Trying to replicate it, I face the same issues where it just defaults to `torch.per_tensor_affine` for the scheme and proceeds to calculate a nonzero `zero_point` value. How to reproduce:   Output: ",replied in https://discuss.pytorch.org/t/runtimeerrorunsupportedqschemeperchannelaffineduringfxqat/189835/2,"I see, so the correct scheme is being applied, but for kernel support they all are named affine?","> I see, so the correct scheme is being applied, but for kernel support they all are named affine? yeah correct scheme is applied. and per_channel_affine/per_tensor_affine is qscheme of the tensor, not the kernel.","Okay I gotcha. I was little worried my implementation was incorrect however I'm glad I got some clarification on the issue.  I'm also curious since this is a similar subject how we can quantize a `torch.fx.GraphModule`? You recently gave me a response on the forums page, but i didn't know of any functionalities for quantizing a GraphModule using `quantize_fx()`. I see how it can be done with a completely traceable `nn.Module`, but if I get a hugging face transformer's `GraphModule` that was traced using their API (it won't trace using normal quantize_fx() on the regular loaded transformer using the `.from_pretrained()` method) how can I quantize the `GraphModule` of the traced transformer?  Forum link: https://discuss.pytorch.org/t/errortryingtoquantizetransformermodel/186241","> Okay I gotcha. I was little worried my implementation was incorrect however I'm glad I got some clarification on the issue. >  > I'm also curious since this is a similar subject how we can quantize a `torch.fx.GraphModule`? You recently gave me a response on the forums page, but i didn't know of any functionalities for quantizing a GraphModule using `quantize_fx()`. I see how it can be done with a completely traceable `nn.Module`, but if I get a hugging face transformer's `GraphModule` that was traced using their API (it won't trace using normal quantize_fx() on the regular loaded transformer using the `.from_pretrained()` method) how can I quantize the `GraphModule` of the traced transformer? >  > Forum link: https://discuss.pytorch.org/t/errortryingtoquantizetransformermodel/186241 yeah you can do it with hugging face symbolic tracing. although we are moving away from fx graph mode quantization flow and here is the new flow: https://pytorch.org/docs/main/quantization.htmlprototypepytorch2exportquantization, you could apply this for submodules, something like this: https://github.com/pytorchlabs/ao/pull/17 `test_dynamic_quant_gpu_unified_api_unified_impl`"
1976,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(scaled_dot_product returns NaN arrays with eval())ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I'm trying to use the builtin transformer encoder for my work. The model works well in the training phase (with train()).  But, when I try to evaluate my model (with eval() and no_grad()), my model always returns all of NaNs.  (I checked that the no_grad doesn't cause the problem; only eval() causes the problem.) I traced the source code and found that the `drouput_p` in  `nn.functional.multi_head_attention_forward` causes the problem.  The `dropout_p` is set to 0.1 with train(), and 0.0 with eval(). And the `scaled_dot_product` (nn.functional.py line 5287) function returns all NaNs with 0.0 of `dropout_p`. So I manually changed the `dropout_p` to 1.0 in eval(), then the `scaled_dot_product` function returns some float values (but I can't be sure they are correct values).  Is there any bug in the `scaled_dot_product` code for the `dropout_p`? Or did I make some mistake to use the model? I don't use any attn_mask or padding mask in this work. my model is as follows:  And the evaluation process is as follows:   Versions PyTorch version: 2.0.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0135genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB Nvidia driver version: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,scaled_dot_product returns NaN arrays with eval()," ğŸ› Describe the bug Hello, I'm trying to use the builtin transformer encoder for my work. The model works well in the training phase (with train()).  But, when I try to evaluate my model (with eval() and no_grad()), my model always returns all of NaNs.  (I checked that the no_grad doesn't cause the problem; only eval() causes the problem.) I traced the source code and found that the `drouput_p` in  `nn.functional.multi_head_attention_forward` causes the problem.  The `dropout_p` is set to 0.1 with train(), and 0.0 with eval(). And the `scaled_dot_product` (nn.functional.py line 5287) function returns all NaNs with 0.0 of `dropout_p`. So I manually changed the `dropout_p` to 1.0 in eval(), then the `scaled_dot_product` function returns some float values (but I can't be sure they are correct values).  Is there any bug in the `scaled_dot_product` code for the `dropout_p`? Or did I make some mistake to use the model? I don't use any attn_mask or padding mask in this work. my model is as follows:  And the evaluation process is as follows:   Versions PyTorch version: 2.0.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0135genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB Nvidia driver version: ",2023-10-04T07:43:21Z,triaged module: NaNs and Infs module: sdpa,open,0,2,https://github.com/pytorch/pytorch/issues/110505,I solved this issue. My `d_model` in `TransformerEncoderLayer` was too big (8192). It is solved after I changed the `d_model` to 200. ,"Again, It still happens in eval() mode (with dropout 0) even after I adjusted the d_model size small."
732,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] [Checkpointing] Loading optimizer state dict with use_orig_params True causes OOM)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When use_orig_params is set to True in FSDP, `FSDP.optim_state_dict_to_load` call uses much higher memory on GPU than when use_orig_params is False. This causes an OOM later when optimizer.load_state_dict clones the state dict.  There's also no way to offload this to CPU.  For a Llama 7B model on a single node, it produced 9GB extra allocation on the GPU after optim_state_dict_to_load when compared to use_orig_params=False.   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[FSDP] [Checkpointing] Loading optimizer state dict with use_orig_params True causes OOM," ğŸ› Describe the bug When use_orig_params is set to True in FSDP, `FSDP.optim_state_dict_to_load` call uses much higher memory on GPU than when use_orig_params is False. This causes an OOM later when optimizer.load_state_dict clones the state dict.  There's also no way to offload this to CPU.  For a Llama 7B model on a single node, it produced 9GB extra allocation on the GPU after optim_state_dict_to_load when compared to use_orig_params=False.   Versions  ",2023-10-03T22:11:55Z,triaged module: fsdp,open,3,10,https://github.com/pytorch/pytorch/issues/110479,"Optimizer state_dict is offload to CPU by default so the memory usage should go back to the same status after the FSDP.optim_state_dict_to_load. However, optimizer state_dict with use_orig_params=True can cause more memory fragmentation, which results in less free memory. https://github.com/pytorch/pytorch/pull/108298 may reduce the case. Also, it is recommended to use sharded version of optimizer and model state_dict.","Already using sharded state dict, but the flow with use_orig_params doesn't move to CPU. I have already discovered the issue, can make a PR fixing this","If you are talking about `FSDP.optim_state_dict` it always offloads to the CPU, there is no way to turn it off. https://github.com/pytorch/pytorch/pull/108434 recently added the option to turn off offload to cpu. If you are talking about `FSDP.optim_state_dict_to_load`, then the returned dict is the one to be loaded to the optimizer. So it has to be on the same device as the optimizer.","I can reproduce the same problem if `use_orig_params=True` and use `FSDP.optim_state_dict_to_load` to load optimizer states. As a comparison, if I don't load the optimizer states, then the model can be trained without error. Otherwise, even though the optim states can be loaded successfully, we cannot continue training due to OOM. BTW, I use the nightly version of PyTorch.","The memory fragmentation is a fundamental problem in the current FSDP design. Many memory allocations and allgathers are required during the state_dict data shuffling. `use_orig_params=True` just makes things worse. As a result, it is possible to see more memory usage with `use_orig_params=True`.","I don't think this particular issue is about fragmentation though. There's an additional 9 GB memory alloced during the call. On Sat, Oct 7, 2023 at 10:32â€¯AM ChienChin Huang ***@***.***> wrote: > The memory fragmentation is a fundamental problem in the current FSDP > design. Many memory allocations and allgathers are required during the > state_dict data shuffling. use_orig_params=True just makes things worse. > As a result, it is possible to see more memory usage with > use_orig_params=True. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Yep, I think it may not be a fragmentation problem as it consumed too much extra GPU memory after loading the optimizer state dict. Is there a patch or quick fix to it? Or, is it possible to load the optimizer state dict locally (each rank load its previous optim state dict instead of loading on master then resharding)? I tried to use the torch's ""torch.distributed._shard.checkpoint.save_state_dict / load_state_dict"" but it doesn't seem to work. If it's possible, can you provide an example for this?", , CC(FSDP.optim_state_dict_to_load consumes a lot of memory) might be related. We can investigate.,"If people are interested, trying https://github.com/pytorch/pytorch/pull/117261 might help fix this issue."
499,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix(inductor): Increase coverage of Inductor ATen lowering)ï¼Œ å†…å®¹æ˜¯ (Add sqrt to decomp testing path and fix missing `minimum`, `clamp_min`,`clamp_max` lowerings and/or registrations. Follow up to: https://github.com/pytorch/pytorch/pull/110468issuecomment1745718602 (requires upstream to merge to avoid merge conflict) CC:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fix(inductor): Increase coverage of Inductor ATen lowering,"Add sqrt to decomp testing path and fix missing `minimum`, `clamp_min`,`clamp_max` lowerings and/or registrations. Follow up to: https://github.com/pytorch/pytorch/pull/110468issuecomment1745718602 (requires upstream to merge to avoid merge conflict) CC:   ",2023-10-03T21:10:48Z,triaged open source Merged ciflow/trunk release notes: foreach_frontend module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/110473, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
222,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Increase header coverage of clang-tidy)ï¼Œ å†…å®¹æ˜¯ ()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Increase header coverage of clang-tidy,,2023-10-03T13:03:33Z,open source module: amp (automated mixed precision) Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/110443, label ciflow/trunk,The next aim is to switch to the CUDA image and enable CUDA cpp source lint.,Does functorch still have a few headers? Or were they all moved to the torch folder. Better include it in the regex just to be safe., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
834,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(stable diffusion not displaying correctly in PT2 HUD)ï¼Œ å†…å®¹æ˜¯ (We appear to have stopped tracking stable diffusion benchmarks. !image  says: > stable_diffusion was split into 2 in latest version >  stable_diffusion_text_encoder >  stable_diffusion_unet >  > both are still present in Torchbench dashboard: https://fburl.com/w6b5evfj, but it looks like blueberries dashboard isn't discovering those new models.   mentioned that maybe https://github.com/pytorch/testinfra/blob/90c44d422085f57127f65fa6d800db8d761c4683/torchci/pages/benchmark/compilers.tsxL1744 needs to be updated. At any rate, we should update the hud so that we're actively tracking the stats. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,stable diffusion not displaying correctly in PT2 HUD,"We appear to have stopped tracking stable diffusion benchmarks. !image  says: > stable_diffusion was split into 2 in latest version >  stable_diffusion_text_encoder >  stable_diffusion_unet >  > both are still present in Torchbench dashboard: https://fburl.com/w6b5evfj, but it looks like blueberries dashboard isn't discovering those new models.   mentioned that maybe https://github.com/pytorch/testinfra/blob/90c44d422085f57127f65fa6d800db8d761c4683/torchci/pages/benchmark/compilers.tsxL1744 needs to be updated. At any rate, we should update the hud so that we're actively tracking the stats. ",2023-10-02T19:14:09Z,high priority oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/110405, can someone from the blueberries side help out with this?,"I have a PR out that I think should address this: https://github.com/pytorch/testinfra/pull/4608 To test out the changes, I need Vercel access. The POC there should be Huy, and he's coming back from PTO tomorrow",Fixed with the land !image The difference in cudagraphs numbers is likely due to also fixing it for nanogpt !image
333,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow storages to alias even when the deleter is deleteNothing)ï¼Œ å†…å®¹æ˜¯ (This improves the debugging experience for devices that provide custom storage pointer.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Allow storages to alias even when the deleter is deleteNothing,This improves the debugging experience for devices that provide custom storage pointer.,2023-10-02T09:42:52Z,triaged open source Stale topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/110381," label ""topic: not user facing""",", can you please look at this?","I'd like to understand more of the context for this why this change is needed. Have you considered making your custom DataPtr use `refcounted_deleter` (by calling the `maybeApplyRefcountedDeleter` function in `c10/core/RefcountedDeleter.h`), and is there a reason why that wouldn't work in your case?",Like XLA we don't really have a concept of storage on our device that matches up with the pytorch definition of storage. So we're faking it by passing a pointer to an internal class with a empty destructor (we manage the lifetime of said class internally). XLA passes a `nullptr` rather than a real ptr but we need to pass a real ptr so we can handle `set_.source_Storage` correctly. I think it would technically be possible to let pytorch handle the memory management of our internal class but it would require significant refactoring on our end and may make our view handling code worse References in XLA Note on XLA storage handling here: https://github.com/pytorch/xla/blob/master/torch_xla/csrc/tensor.hL334 Where the XLA storage is actually initialized: https://github.com/pytorch/xla/blob/master/torch_xla/csrc/tensor.cppL144,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
643,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix a typo in `cholesky_inverse` documentation)ï¼Œ å†…å®¹æ˜¯ (Very small PR to fix a typo in https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html doc.  According to the current doc, the function expects $A$, the symmetric positivedefinite matrix, as input. But the examples given (and more important, the code) is using $u$ the cholesky decomposition of this matrix (like cholesky_solve). Also, it provides a correct example of batch usage of this function. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix a typo in `cholesky_inverse` documentation,"Very small PR to fix a typo in https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html doc.  According to the current doc, the function expects $A$, the symmetric positivedefinite matrix, as input. But the examples given (and more important, the code) is using $u$ the cholesky decomposition of this matrix (like cholesky_solve). Also, it provides a correct example of batch usage of this function. ",2023-10-01T19:56:59Z,module: docs open source Merged ciflow/trunk release notes: python_frontend topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/110364,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: raphaelreme  (f7ca1b8c3283bd50d79e857150af408c11eabf63, 52fd69ca82bb7ed0c9c4000469105a82559328c2, d7d3b3a4ac5df3c66b24625d532d3394a67342b1, 30d220e248f33d5c9592e2fc70b25eead521dfcc)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1728,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ONNX export: TransformerEncoder is exported with fixed input dims)ï¼Œ å†…å®¹æ˜¯ ( Issue description When exporting to ONNX a module that contains `torch.nn.TransformerEncoder`, the time dimention is fixed to the one used during export, and the model fails to deal with arbitrary  sequence lengths.  Code example Given the following module:  Exported as follows:  and ran using `onnxruntime` as follows:  I get the following error:   System Info PyTorch version: 2.2.0.dev20230930+cpu Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Pro GCC version: Could not collect Clang version: 15.0.2 CMake version: version 3.26.3 Libc version: N/A Python version: 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: False CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce MX350 Nvidia driver version: 528.49 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2918 DeviceID=CPU0 Family=198 L2CacheSize=5120 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2918 Name=11th Gen Intel(R) Core(TM) i71195G7 @ 2.90GHz ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] pytorchlightning==2.0.6 [pip3] torch==2.2.0.dev20230930+cpu [pip3] torchmetrics==1.0.1 [conda] Could not collect)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,ONNX export: TransformerEncoder is exported with fixed input dims," Issue description When exporting to ONNX a module that contains `torch.nn.TransformerEncoder`, the time dimention is fixed to the one used during export, and the model fails to deal with arbitrary  sequence lengths.  Code example Given the following module:  Exported as follows:  and ran using `onnxruntime` as follows:  I get the following error:   System Info PyTorch version: 2.2.0.dev20230930+cpu Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Pro GCC version: Could not collect Clang version: 15.0.2 CMake version: version 3.26.3 Libc version: N/A Python version: 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: False CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce MX350 Nvidia driver version: 528.49 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2918 DeviceID=CPU0 Family=198 L2CacheSize=5120 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2918 Name=11th Gen Intel(R) Core(TM) i71195G7 @ 2.90GHz ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] pytorchlightning==2.0.6 [pip3] torch==2.2.0.dev20230930+cpu [pip3] torchmetrics==1.0.1 [conda] Could not collect",2023-10-01T04:05:35Z,module: onnx triaged,closed,2,2,https://github.com/pytorch/pytorch/issues/110347,You may consider using the new `torch.onnx.dynamo_export` api: ,"Hi   Weâ€™ve gone ahead and closed this issue because we think it can be resolved with our new `torch.onnx.dynamo_export` api. Please try that instead! If you still believe this issue is relevant, please feel free to reopen the issue and we will triage it as necessary. Please specify in a comment any updated information you may have so that we can address it effectively. We encourage you to try the latest pytorchpreview (nightly) version to see if it has resolved the issue, as we are constantly working to make the converter experience better for everyone! Thanks, ONNX Converter team"
2034,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.Tensor.__repr__ causes torch.compile to error: ""got an unexpected keyword argument 'tensor_contents'"")ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug It seems PyTorch's compile machinery somewhere is dependent on `torch.Tensor.__repr__`!! I always change `torch.Tensor.__repr__` so I can see the shape of the tensor right away and it's great debugging aid. This is normal and, in Python, one may change `__repr__` and therefore it is not very ""pythonic"" for any code to strongly depend on this. The string returned by __repr__ is for humans. However, after I change torch.Tensor.__repr__, compiled model's forward throws below error:  Above behavior happens in current 2.0.1 stable release as well as 2.2 nightly build. Below is complete code that reproduces the error:  I think the bug is important because it indicates somewhere in PyTorch code base there is a strong dependency on exactly what is returned by `torch.Tensor.__repr__`. This makes that implementation very fragile and I think it could most likely be changed simply by using proper API.  Versions PyTorch version: 2.2.0.dev20230929+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.31 Python version: 3.11.4 (main, Jul  5 2023, 14:15:25) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01046azurex86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe Nvidia driver version: 535.104.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.5 /)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"torch.Tensor.__repr__ causes torch.compile to error: ""got an unexpected keyword argument 'tensor_contents'"""," ğŸ› Describe the bug It seems PyTorch's compile machinery somewhere is dependent on `torch.Tensor.__repr__`!! I always change `torch.Tensor.__repr__` so I can see the shape of the tensor right away and it's great debugging aid. This is normal and, in Python, one may change `__repr__` and therefore it is not very ""pythonic"" for any code to strongly depend on this. The string returned by __repr__ is for humans. However, after I change torch.Tensor.__repr__, compiled model's forward throws below error:  Above behavior happens in current 2.0.1 stable release as well as 2.2 nightly build. Below is complete code that reproduces the error:  I think the bug is important because it indicates somewhere in PyTorch code base there is a strong dependency on exactly what is returned by `torch.Tensor.__repr__`. This makes that implementation very fragile and I think it could most likely be changed simply by using proper API.  Versions PyTorch version: 2.2.0.dev20230929+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.31 Python version: 3.11.4 (main, Jul  5 2023, 14:15:25) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01046azurex86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe Nvidia driver version: 535.104.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.5 /",2023-09-30T07:53:42Z,triaged oncall: pt2,closed,0,6,https://github.com/pytorch/pytorch/issues/110331,Unclear what is happening. I cannot repro with or without your `torch.Tensor.__repr__` code. I also see the `torch.Tensor.__repr__` being honored correctly on the last line.  on sha `898656e9d166851e244749a715be8f74613e4d83`,"Hi Michael, did you try the code I copied and pasted above? I can repro this on PyTorch nightly consistently.","The simplest workaround is, when you monkeypatch, take `tensor_contents` as an optional kwarg and, if it is set, call the original repr.","> The simplest workaround is, when you monkeypatch, take `tensor_contents` as an optional kwarg and, if it is set, call the original repr. Do you mean using `{tensor_contents}` in format string for `__repr__`? A simple example would be useful.",,>   Does this workaround work for you? Please reopen the issue if it doesn't.
666,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pytorch] add should_deepcopy flag to AveragedModel)ï¼Œ å†…å®¹æ˜¯ (Summary:  Context AveragedModel always deepcopies the passed in model. This can pose an issue if the model cannot be deepcopied, like FSDP models. Furthermore, users may want to do this logic themselves  This diff Adds `should_deepcopy` flag (default=True for backwards compatibility) so that if users want to pass an FSDP model, they can by turning the flag off Test Plan: existing unit tests pass Differential Revision: D49622071)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[pytorch] add should_deepcopy flag to AveragedModel,"Summary:  Context AveragedModel always deepcopies the passed in model. This can pose an issue if the model cannot be deepcopied, like FSDP models. Furthermore, users may want to do this logic themselves  This diff Adds `should_deepcopy` flag (default=True for backwards compatibility) so that if users want to pass an FSDP model, they can by turning the flag off Test Plan: existing unit tests pass Differential Revision: D49622071",2023-09-29T23:45:52Z,fb-exported Stale release notes: optim,closed,0,4,https://github.com/pytorch/pytorch/issues/110327,This pull request was **exported** from Phabricator. Differential Revision: D49622071,"I'm a bit confused. If you're not deepcopying, the model with the running averages is the same as the current one. So you cannot do averaging?","> I'm a bit confused. If you're not deepcopying, the model with the running averages is the same as the current one. So you cannot do averaging? So to clarify, if the deep copy flag is disabled, it's expected that the user pass in copy/deepcopy of the model themselves. This is needed for example if the model is wrapped in FSDP, since it's not possible to deepcopy an FSDP module. Instead, you can deep copy the model, wrap that in FSDP, and then pass into the AveragedModel with the deepcopy flag off.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
530,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow nn.Transformer to be exported as ONNX.)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(ONNX export of torch.nn.Transformer still fails). During ONNX export tracing, the `is_causal` parameter is not actually a `bool` but instead `tensor(bool)`. The native function `scaled_dot_product_attention` does not seem to handle this case, so we can just cast it to a Python `bool` for now.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Allow nn.Transformer to be exported as ONNX.,"Fixes CC(ONNX export of torch.nn.Transformer still fails). During ONNX export tracing, the `is_causal` parameter is not actually a `bool` but instead `tensor(bool)`. The native function `scaled_dot_product_attention` does not seem to handle this case, so we can just cast it to a Python `bool` for now.",2023-09-29T08:17:06Z,module: onnx triaged open source Stale release notes: onnx,closed,1,1,https://github.com/pytorch/pytorch/issues/110277,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1974,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ONNX export of torch.nn.Transformer still fails)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   BTW: 1. I'm using nightly build because otherwise I have problems with torch.nn.MultiHeadAttention export (see CC(torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 18 is not supported.) ) 2. Mb it's a good idea to add some part of this to onnx tests. What do you think? It will show any problems with export of any part of the transformer (attention/decoder/encoder/normalization). And I think you will be getting a lot of issues like this one because of transformers popularity...  Versions Collecting environment information... PyTorch version: 2.2.0.dev20230928+cpu Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: version 3.27.0rc2 Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU Nvidia driver version: 536.23 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=3601 DeviceID=CPU0 Family=107 L2CacheSize=8192 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=3601 Name=AMD Ryzen 7 7745HX with Radeon Graphics ProcessorType=3 Revision=24834 Versions of relevant libraries: [pip3] numpy==1.26.0 [pip3] torch==2.2.0.dev20230928+cpu [pip3] torchaudio==2.2.0.dev20230928+cpu [pip3] torchvision==0.17.0.dev2023092)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,ONNX export of torch.nn.Transformer still fails," ğŸ› Describe the bug   BTW: 1. I'm using nightly build because otherwise I have problems with torch.nn.MultiHeadAttention export (see CC(torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 18 is not supported.) ) 2. Mb it's a good idea to add some part of this to onnx tests. What do you think? It will show any problems with export of any part of the transformer (attention/decoder/encoder/normalization). And I think you will be getting a lot of issues like this one because of transformers popularity...  Versions Collecting environment information... PyTorch version: 2.2.0.dev20230928+cpu Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: version 3.27.0rc2 Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU Nvidia driver version: 536.23 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=3601 DeviceID=CPU0 Family=107 L2CacheSize=8192 L2CacheSpeed= Manufacturer=AuthenticAMD MaxClockSpeed=3601 Name=AMD Ryzen 7 7745HX with Radeon Graphics ProcessorType=3 Revision=24834 Versions of relevant libraries: [pip3] numpy==1.26.0 [pip3] torch==2.2.0.dev20230928+cpu [pip3] torchaudio==2.2.0.dev20230928+cpu [pip3] torchvision==0.17.0.dev2023092",2023-09-28T23:38:03Z,module: onnx triaged,closed,1,2,https://github.com/pytorch/pytorch/issues/110255,"Thanks, we should definitely add this to tests.","Hey  , Closing this as stale, please try again using the new `torch.onnx.dynamo_export()` api. Please reopen the issue if  you still run into it. Thanks, Maanav"
437,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([AMD] Fix broken build from nested transformer utils)ï¼Œ å†…å®¹æ˜¯ (Summary: D49374910 breaks internal amd build because we didn't hipify the header file in nested/cuda. Maybe it's just easier to move it outside. Reviewed By: nmacchioni Differential Revision: D49743234)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[AMD] Fix broken build from nested transformer utils,Summary: D49374910 breaks internal amd build because we didn't hipify the header file in nested/cuda. Maybe it's just easier to move it outside. Reviewed By: nmacchioni Differential Revision: D49743234,2023-09-28T21:40:44Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/110245,This pull request was **exported** from Phabricator. Differential Revision: D49743234,This pull request was **exported** from Phabricator. Differential Revision: D49743234, is this landable? I have another PR: https://github.com/pytorch/pytorch/pull/97485 That will likely race condition so going to wait till this lands,>  is this landable? I have another PR: CC(Adding Backward Support for NestedTensors and FlashAttention) That will likely race condition so going to wait till this lands Yes I'm landing it however I hit some problem internally (seemingly irrelevant). Will try to land this asap. If you're blocked please ping me offline and we can chat more how to move forward, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1999,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`scaled_dot_product_attention` behaves differently between v2.0 and v2.1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug With torch v2.1, `scaled_dot_product_attention` on `GPU` gives `nan` when a sequence has all large negative values (e.g `torch.finfo(q.dtype).min`  in order to mean no attention at all places). On `CPU`, it won't give `nan`. With torch v2.0, it gives no `nan` on both `CPU` and `GPU` and those values are the same as the one given by `v2.1 + CPU`. I understand it doesn't really make sense when a sequence has no place to attend attention. However, I am wondering **if this `nan` value in torch v2.1 is intentional or unexpected**. This causes issues `falcon` implementation in `transformers` when left padding is used.  Reproduce (running with torch v2.1)   Outputs  with torch v2.0 (both `CPU` and `GPU`) or torch v2.1 (`CPU`)   torch v2.1 (`GPU`)   Versions Collecting environment information... PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Home GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.16 (default, Jun 12 2023, 21:00:42) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: 11.6.112 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU Nvidia driver version: 517.00 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2400 DeviceID=CPU0 Family=198 L2CacheSize=11776 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2400 Name=12th Gen Intel(R) Core(TM) i712800H ProcessorType=3 Revision= Versi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`scaled_dot_product_attention` behaves differently between v2.0 and v2.1," ğŸ› Describe the bug With torch v2.1, `scaled_dot_product_attention` on `GPU` gives `nan` when a sequence has all large negative values (e.g `torch.finfo(q.dtype).min`  in order to mean no attention at all places). On `CPU`, it won't give `nan`. With torch v2.0, it gives no `nan` on both `CPU` and `GPU` and those values are the same as the one given by `v2.1 + CPU`. I understand it doesn't really make sense when a sequence has no place to attend attention. However, I am wondering **if this `nan` value in torch v2.1 is intentional or unexpected**. This causes issues `falcon` implementation in `transformers` when left padding is used.  Reproduce (running with torch v2.1)   Outputs  with torch v2.0 (both `CPU` and `GPU`) or torch v2.1 (`CPU`)   torch v2.1 (`GPU`)   Versions Collecting environment information... PyTorch version: 2.1.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Home GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.16 (default, Jun 12 2023, 21:00:42) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: 11.6.112 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Ti Laptop GPU Nvidia driver version: 517.00 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2400 DeviceID=CPU0 Family=198 L2CacheSize=11776 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2400 Name=12th Gen Intel(R) Core(TM) i712800H ProcessorType=3 Revision= Versi",2023-09-28T12:30:18Z,triaged,closed,2,12,https://github.com/pytorch/pytorch/issues/110213, ,"So the reason for this like you said is because of masking out an entire row, similar to this issue:  CC(SDPA produces NaN with padding mask). There was a change though from 2.0 to 2.1, which is that we can now run fused attention kernels with arbitrary attention masks. This means that your code likely was being run on the math path before and is now running with the fused kernel. The fused attention kernels use the iterative softmax algorithm and for large negative values it will produce all NaNs as output while regular softmax for large negative values will evenly distribute the attention among all entries. It would be very costly to check if an entire row is masked at runtime. I could see this being a check in debug mode though however. As a workaround I would try running your code with mem effiecient attention turned off via the context manager: https://github.com/pytorch/pytorch/blob/81da6db74a6930cfb882cdd9476310ac07386c97/torch/backends/cuda/__init__.pyL275","Thank you.  Turning off mem. effiecient attention looks like to me destorying the purpose of using `scaled_dot_product_attention`? I guess we will have to do something manually on masks before calling `scaled_dot_product_attention`. (as you can see, left padding + causal mask will cause shorter sequences having all places being masked for leading tokens)"," I am wondering if you have an other suggestion apart from disabling the memoryefficient kernel path. My current understanding is that efficient batched inference with padding support is not a byproduct of  CC(Add support for ALiBi/relative positional biases to the fast path for Transformers), contrary to what I thought before. Is that correct? To me (for inference),  it is not a big deal that some `nan` appear (they appear in meaningless positions anyway), however it is a big issue that `nan` propagate to all positions at later SDPA calls. A solution could be to override nan with some dummy value to avoid `nan` to propagate, but that is surely inefficient. see https://www.diffchecker.com/4UwU6uKK/ (math vs memefficient) !image !image !image",Solution: attend to at least a token even for padding. This does not influence the result given that softmax is computed on the last dimension,"> attend to at least a token even for padding This could fix the issue, and yes we don't care about the output at those padding places. But in terms of testing/debugging, it would be difficult if we don't keep what has been done in torch 2.0 or before. i.e. if previously it `evenly distributes the attention among all entries`, it's better to do so in our custom manipulation too. Otherwise, when we check (forward) outputs given by torch 2.0 and torch 2.1 and see it is different, users or developers need to remember those differences are at the padding places and are OK.","What I did in https://github.com/huggingface/transformers/pull/26572 is attend to all tokens equally (basically having `[0, 0, 0, 0, 0]` instead of `[inf, inf, inf, inf, inf]` on padding rows), maybe it works with regard to keeping an even distribution of the attention among all entries for padding rows.","OK, very nice! Could you check they give the same outputs with this (on SDPA GPU + torch 2.1) and when running without SDPA (or with SDPA on CPU) for a sequence with no attention at all? ",Will do!,Interestingly this issue happens only in fp32 but not in fp16  maybe `65504` is not a small enough value as softmax may be computed in fp32,"> Solution: attend to at least a token even for padding. This does not influence the result given that softmax is computed on the last dimension Hi , , I know I'm kinda late to the party, but using an `attn_mask` that is attempting to ignore certain rows entirely still leads to NaNs that propagates throughout all layers of the neural network. This is the case whether FlashAttention v1 or v2 is used in Pytorch >= 2.1. My case involves a crossattention layer where the query and keys both have padded elements. I don't want any of the padded elements to be attended to. Setting first column of the `attn_mask` to `False` using `attn_mask[:, :, 0] = False` as  suggested is one way of preventing the problem. This feels quite hacky, and I'm not sure if it's the right solution, as the model now attends to the first element of the key, even for padded elements in the query (?).  I think we should establish a guideline for this case from now so that people like me don't get stuck for a day trying to resolve the issue.  Thank you for your time!","Hi , I am only seeing your message now. You should probably open an other issue with a repro/example. In transformers, we use https://github.com/huggingface/transformers/blob/25245ec26dc29bcf6102e1b4ddd0dfd02e720cf5/src/transformers/modeling_attn_mask_utils.pyL189 which unmasks entirely padding rows in the source dimension from (batch_size, source_length, target_length), effectively dealing with padding in the source dimension."
385,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ATen] Remove ATen.h includes from transformers)ï¼Œ å†…å®¹æ˜¯ (  CC([ATen] Remove ATen.h includes from transformers) The kernel files here in particular are quite slow to compile and don't use anything from  `ATen.h`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ATen] Remove ATen.h includes from transformers,  CC([ATen] Remove ATen.h includes from transformers) The kernel files here in particular are quite slow to compile and don't use anything from  `ATen.h`.,2023-09-28T04:19:33Z,open source Merged ciflow/trunk topic: not user facing ciflow/periodic,closed,0,6,https://github.com/pytorch/pytorch/issues/110199,tagging  as a reviewer, Many of these kernels are regularly synched from the https://github.com/facebookresearch/xformers library. You should submit this PR to xformers there as well.," I reverted the change to `generate_kernels.py` and add the flag in the build system instead, I also see that the kernel header file doesn't include `ATen.h` in the `xformers` version so that change seems fine. https://github.com/facebookresearch/xformers/blob/23cc20a29e5a88e06932b9b0a6a6fd23fa77bc54/xformers/csrc/attention/cuda/fmha/kernel_forward.hL10L13 All of the other files seem to be PyTorchonly so I don't think any change is required there.",If it passes periodic `buck` build (which mimic internal build system) then please go ahead and merge it , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1972,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(cudaMallocAsync cause too much fragmentation.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I try to use CUDAâ€™s builtin asynchronous allocator by setting `PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync`. Then I encounter OOM with over 20GB memory 'missing':  This dose not like what the post saying: If a memory allocation request made using cudaMallocAsync canâ€™t be serviced due to fragmentation of the corresponding memory pool, the CUDA driver defragments the pool by remapping unused memory in the pool to a contiguous portion of the GPUâ€™s virtual address space. Remapping existing pool memory instead of allocating new memory from the OS also helps keep the applicationâ€™s memory footprint low. This task can barely run with PyTorch's allocator, yet, still with significant fragmentation:   Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Alibaba Group Enterprise Linux Server 7.2 (Paladin) (x86_64) GCC version: (GCC) 8.3.1 20190311 (Red Hat 8.3.13) Clang version: Could not collect CMake version: version 3.27.4 Libc version: glibc2.32 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform:  Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 470.154 cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.9.1 /usr/lib64/libcudnn_adv_infer.so.8.9.1 /usr/lib64/libcudnn_adv_train.so.8.9.1 /usr/lib64/libcudnn_cnn_infer.so.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,cudaMallocAsync cause too much fragmentation.," ğŸ› Describe the bug I try to use CUDAâ€™s builtin asynchronous allocator by setting `PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync`. Then I encounter OOM with over 20GB memory 'missing':  This dose not like what the post saying: If a memory allocation request made using cudaMallocAsync canâ€™t be serviced due to fragmentation of the corresponding memory pool, the CUDA driver defragments the pool by remapping unused memory in the pool to a contiguous portion of the GPUâ€™s virtual address space. Remapping existing pool memory instead of allocating new memory from the OS also helps keep the applicationâ€™s memory footprint low. This task can barely run with PyTorch's allocator, yet, still with significant fragmentation:   Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Alibaba Group Enterprise Linux Server 7.2 (Paladin) (x86_64) GCC version: (GCC) 8.3.1 20190311 (Red Hat 8.3.13) Clang version: Could not collect CMake version: version 3.27.4 Libc version: glibc2.32 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform:  Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 470.154 cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.9.1 /usr/lib64/libcudnn_adv_infer.so.8.9.1 /usr/lib64/libcudnn_adv_train.so.8.9.1 /usr/lib64/libcudnn_cnn_infer.so.",2023-09-28T02:32:05Z,module: cuda module: memory usage triaged module: CUDACachingAllocator,open,0,1,https://github.com/pytorch/pytorch/issues/110194, 
729,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `lazy_clone_storage` to create COW storages)ï¼Œ å†…å®¹æ˜¯ (This PR relands CC(Reland: implement a function to convert a storage to copyonwrite) but accounts for the changes in CC(Reorganize and rename COW files and APIs). Also, the function for creating COW storages is called `lazy_clone_storage` in this PR, instead of `try_ensure` NOTE: COW storages do not actually copy on write yet, they just have the COW deleter and deleter context applied to them Part of CC(Implement Copyonwrite (COW) tensors)   CC(Add `lazy_clone_storage` to create COW storages) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add `lazy_clone_storage` to create COW storages,"This PR relands CC(Reland: implement a function to convert a storage to copyonwrite) but accounts for the changes in CC(Reorganize and rename COW files and APIs). Also, the function for creating COW storages is called `lazy_clone_storage` in this PR, instead of `try_ensure` NOTE: COW storages do not actually copy on write yet, they just have the COW deleter and deleter context applied to them Part of CC(Implement Copyonwrite (COW) tensors)   CC(Add `lazy_clone_storage` to create COW storages) ",2023-09-28T00:42:01Z,module: internals open source Merged Reverted ciflow/trunk topic: not user facing ciflow/periodic,closed,0,28,https://github.com/pytorch/pytorch/issues/110192,"To fix the memory leak that caused CC(Reland: implement a function to convert a storage to copyonwrite) to be reverted, I modified the `different_context` test to make sure the `new std::byte[5]` gets deleted I wasn't able to find out why the other failure happened though, where the `try_ensure` call (now called `lazy_clone_storage`) returned a null storage in the `no_context` test. Seemingly, the StorageImpl that gets created for that test can in some cases be given a UniqueVoidPtr context that does not equal the data pointer, when it's run in the Metainternal CI. I'm not sure why that would happen, because they are supposed to be equal in the default casethat is true in all the public CI runs. Maybe the failing internal test is overriding `GetCPUAllocator()` with an allocator that adds a context to the data pointer? I added an assert to that test to check the context before the `lazy_clone_storage` call, so if that assert fails internally, we'll at least know for sure if the context of the StorageImpl is actually getting set to something other than the data pointer",Did you just delete the try ensure tests from this PR? Do you want to land it as is or try to diagnose the problem?,"fwiw, with your PR, I get this failure: ","> Did you just delete the try ensure tests from this PR? Do you want to land it as is or try to diagnose the problem? No, I didn't delete the tests, I renamed them from `try_ensure` to `lazy_clone_storage`. I would like to try to fix the problem","oh I see. OK, so then it's the `no_context` test which is still failing. How do you want to tackle this? Is the new error enough?","The new error comes from the check that I added to the `no_context` test which checks that `original_storage`'s data pointer and context are equal to each other:  My guess is that something in the Metainternal test suite must be changing the allocator that `GetCPUAllocator()` returns, probably by calling `SetCPUAllocator`. This allocator sets the context to something that does not equal the data pointer, which COW does not support. I think the fix is just to use `GetDefaultCPUAllocator()` instead, since that does not change when `SetCPUAllocator` is called",", I'm guessing that the test will pass internally now after I changed it to use `GetDefaultCPUAllocator()` instead of `GetCPUAllocator()`",testing it rn,no cigar  ,"From the error message, the data pointer is 0x10 (16) bytes greater than the context pointer. I realized that this happens with the default CPU allocator on mobile builds:  https://github.com/pytorch/pytorch/blob/5183760ca52ea7dbd892ebc88cabdd43ebc81cbe/c10/core/CPUAllocator.cppL166L176 https://github.com/pytorch/pytorch/blob/5183760ca52ea7dbd892ebc88cabdd43ebc81cbe/c10/core/alignment.hL7L11 https://github.com/pytorch/pytorch/blob/5183760ca52ea7dbd892ebc88cabdd43ebc81cbe/c10/core/CPUAllocator.cppL71L72 https://github.com/pytorch/pytorch/blob/5183760ca52ea7dbd892ebc88cabdd43ebc81cbe/c10/core/CPUAllocator.cppL134L139 So it seems like that is probably what is happening in the failing test. Could you confirm if the internal CI is using a mobile build? To fix this, we should be able to just change all `data_ptr.get() == data_ptr.get_context()` checks to add in the `c10::gAlignment` in the `C10_MOBILE` case.","yup, it's a mobile build","Well, adding a `storage.allocator() == c10::GetDefaultMobileCPUAllocator()` check to `c10/core/impl/cow/COW.cpp` made the bazel build fail because `CPUAllocator` is its own bazel library that is not included in `base`. So I guess we'll have to separate COW into its own library after all to use the `CPUAllocator` library",This is ready for another internal CI run," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,I'm going to revert this and immediately remerge to work around some importing issues.," revert m ""revert to work around some importing issues"" c weird", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.," merge f ""immediately remerging temp reverted PR"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m ""Breaking internal builds,  please support the author providing further details"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.," or , let me know what the error was and I can work on fixing it",,Oh I see what's wrong. I'll submit a fixed PR shortly
1980,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(H100 Compatibility - PyTorch with CUDA 12.2 and lower)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi,   Context I want to finetune Llama270Bchathf with any dataset on an Nvidia H100 instance running with CUDA 12.2 v2. To finetune it, i chose autotrainadvanced with Python 3.10.  First try For the first try, i've simply made a venv and installed autotrainadvanced, then run :  So far, everything has gone successfully... After that, i'm running my train command :  And then, i've got this error :   What i tried to handle it I tried so many things to handle this PyTorch issue :   Install older CUDA Driver : 22.1 (which is supported by the latest PyTorch Nightly version)  Build PyTorch from source in a venv as it is suggested following the PyTorch's repo process  Build with and without conda/mkl  Build on different CUDA Versions  Conclusion Always this same warning saying me that the PyTorch version isn't compatible for sm_90 capabilities (H100). And ... as reported by ML Engineer at Nvidia :  CC(CUDA Capability sm_90 for H100 GPUs)issuecomment1673709633 !image I'm gonna post this also on the PyTorch repo but if someone got the same issue, and fixed it, i won't say no to a little help. If you need deeper context, let me know and i'll provide it. Many thanks.  Versions Collecting environment information... /scratch/torchbuild/lib/python3.10/sitepackages/torch/cuda/__init__.py:173: UserWarning: NVIDIA H100 PCIe with CUDA capability sm_90 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86. If you want to use the NVIDIA H100 PCIe GPU with PyTorch, please check the instructions at https://pytorch.org/getstarted/locally/   warnings.warn(incompatible_device_warn.format(device_name, capability, "" "".join()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,H100 Compatibility - PyTorch with CUDA 12.2 and lower," ğŸ› Describe the bug Hi,   Context I want to finetune Llama270Bchathf with any dataset on an Nvidia H100 instance running with CUDA 12.2 v2. To finetune it, i chose autotrainadvanced with Python 3.10.  First try For the first try, i've simply made a venv and installed autotrainadvanced, then run :  So far, everything has gone successfully... After that, i'm running my train command :  And then, i've got this error :   What i tried to handle it I tried so many things to handle this PyTorch issue :   Install older CUDA Driver : 22.1 (which is supported by the latest PyTorch Nightly version)  Build PyTorch from source in a venv as it is suggested following the PyTorch's repo process  Build with and without conda/mkl  Build on different CUDA Versions  Conclusion Always this same warning saying me that the PyTorch version isn't compatible for sm_90 capabilities (H100). And ... as reported by ML Engineer at Nvidia :  CC(CUDA Capability sm_90 for H100 GPUs)issuecomment1673709633 !image I'm gonna post this also on the PyTorch repo but if someone got the same issue, and fixed it, i won't say no to a little help. If you need deeper context, let me know and i'll provide it. Many thanks.  Versions Collecting environment information... /scratch/torchbuild/lib/python3.10/sitepackages/torch/cuda/__init__.py:173: UserWarning: NVIDIA H100 PCIe with CUDA capability sm_90 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86. If you want to use the NVIDIA H100 PCIe GPU with PyTorch, please check the instructions at https://pytorch.org/getstarted/locally/   warnings.warn(incompatible_device_warn.format(device_name, capability, "" "".join(",2023-09-27T12:53:08Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/110153, Python output when calling torch ,You need CUDA11.8 or newer for H100 support. You installed the 11.7 binaries. We actually dropped support for 11.7 in the latest release anyhow., umm i really dont know why this version is in my binaries cuz i didn't installed it myself. I'm fixing it and keep you in touch. thanks,"Hi again,  It's working fine for the build :    BUT when i'm installing ultralytics or setting up autotrain it changes my torch module version to torch==2.0.1 ""compiled with CUDA 11.7""  How can i force to use the pytorch version i just built and not reinstalling one."
2030,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The masking matrix seems to be wrong in the ""torch.nn.functional.scaled_dot_product_attention"" function)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug It seems to be a logic error about the flash attention in transformer model in the implementation of ""torch.nn.functional.scaled_dot_product_attention"" function (named SDPA below). *But I am not sure because current implementation may consider other situations. This problem can be solved by specify the attn_mask parameter in SDPA function. But if it is a logic error, maybe it should be fixed for more convenient to use.*  Situation: I use the GPT to generate text with **the KVcache** and **more than more tokens**.  For example, I have the KVcache of the first 4 tokens (t_0, t_1, t_2, t_3) denoted as K[:4] and V[:4], and have two new tokens (t_4, t_5) to infer the 7th token. With the (t_4, t_5) tokens, I can calculate their QKV denoted as Q[4:6], K[4:6] and V[4:6].  Correct procession: We should concatenate the KVs of above tokens and current tokens as K[:6] and V[:6], and next conduct the masking attention about Q[4:6], K[:6] and V[:6] with a masking matrix [[True, True, True, True, True, False], [True, True, True, True, True, True]].  Problem: I read the document, and it seems that the masking matrix in the SDPA implemention is [[True, False, False, False, False, False], [True, True, False, False, False, False]], resulting wrong result.  Below it my code to verify my guess:    Versions Collecting environment information... PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: TencentOS Server 3.1 (Final) (x86_64) GCC version: (GCC) 8.5.0 20210514 (TencentOS 8.5.018) Clang version: 13.0.1 (Red Hat 13.0.12.module+el8.6.0+37+eac49f58) CMake version: version 3.25.0 Libc version: glibc2.28 Python version: 3.10.13 (main, Sep 11 2023, 1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"The masking matrix seems to be wrong in the ""torch.nn.functional.scaled_dot_product_attention"" function"," ğŸ› Describe the bug It seems to be a logic error about the flash attention in transformer model in the implementation of ""torch.nn.functional.scaled_dot_product_attention"" function (named SDPA below). *But I am not sure because current implementation may consider other situations. This problem can be solved by specify the attn_mask parameter in SDPA function. But if it is a logic error, maybe it should be fixed for more convenient to use.*  Situation: I use the GPT to generate text with **the KVcache** and **more than more tokens**.  For example, I have the KVcache of the first 4 tokens (t_0, t_1, t_2, t_3) denoted as K[:4] and V[:4], and have two new tokens (t_4, t_5) to infer the 7th token. With the (t_4, t_5) tokens, I can calculate their QKV denoted as Q[4:6], K[4:6] and V[4:6].  Correct procession: We should concatenate the KVs of above tokens and current tokens as K[:6] and V[:6], and next conduct the masking attention about Q[4:6], K[:6] and V[:6] with a masking matrix [[True, True, True, True, True, False], [True, True, True, True, True, True]].  Problem: I read the document, and it seems that the masking matrix in the SDPA implemention is [[True, False, False, False, False, False], [True, True, False, False, False, False]], resulting wrong result.  Below it my code to verify my guess:    Versions Collecting environment information... PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: TencentOS Server 3.1 (Final) (x86_64) GCC version: (GCC) 8.5.0 20210514 (TencentOS 8.5.018) Clang version: 13.0.1 (Red Hat 13.0.12.module+el8.6.0+37+eac49f58) CMake version: version 3.25.0 Libc version: glibc2.28 Python version: 3.10.13 (main, Sep 11 2023, 1",2023-09-27T07:36:48Z,triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/110144,"I would argue that the masking matrix is not ""wrong"" rather it chose between two possible patterns for the meaning of causal when seq_q != seq_kv. For more discussion see this issue:   CC([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment)",Thanks for your clarification! The explanation in your mentioned issue is quite clear.  I will set the attn_mask parameter to set the masking matrix for my use. 
417,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GPT2ForSequenceClassification, LayoutLMForSequenceClassification: ""torch._dynamo.exc.Unsupported: call_function BuiltinVariable(setattr) [HFPretrainedConfigVariable(), ConstantVariable(str), ConstantVariable(str)] {}"")ï¼Œ å†…å®¹æ˜¯ (Repro:  Error:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,"GPT2ForSequenceClassification, LayoutLMForSequenceClassification: ""torch._dynamo.exc.Unsupported: call_function BuiltinVariable(setattr) [HFPretrainedConfigVariable(), ConstantVariable(str), ConstantVariable(str)] {}""",Repro:  Error:  ,2023-09-26T19:28:41Z,triaged oncall: pt2 module: dynamo,closed,0,1,https://github.com/pytorch/pytorch/issues/110096,fixed
2054,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 18 is not supported.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug from transformers import AutoModelForCausalLM, AutoTokenizer import torch DEVICE = 'cuda:0' baichuan_base_path = ""./modules/Baichuan213BBase"" tokenizer = AutoTokenizer.from_pretrained(baichuan_base_path, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(baichuan_base_path, device_map=""auto"",                                              trust_remote_code=True) inputs = tokenizer('ç™»é¹³é›€æ¥¼>ç‹ä¹‹æ¶£\nå¤œé›¨å¯„åŒ—>', return_tensors='pt') inputs = inputs.to(DEVICE) onnx_file_path = ""./onnx/Baichuan213BBase.onnx"" torch.onnx.export(     model,     (inputs['input_ids'], inputs['attention_mask']),   è¾“å…¥æ•°æ®     onnx_file_path,       verbose=True,       opset_version=18,     input_names=[""input_ids"", ""attention_mask""],   è¾“å…¥åç§°     output_names=[""output""],       dynamic_axes={         'input_ids': {0: 'batch_size', 1: 'sequence_length'},         'attention_mask': {0: 'batch_size', 1: 'sequence_length'},         'output': {0: 'batch_size', 1: 'sequence_length'}     } )  Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 3.27.5 Libc version: glibc2.17 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01160.90.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A800SXM480GB GPU 1: NVIDIA A800SXM480GB GPU 2: NVIDIA A800SXM480GB GPU 3: NVIDIA A800SXM480GB GPU 4: NVIDIA A800SXM480GB GPU 5: NVI)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX opset version 18 is not supported.," ğŸ› Describe the bug from transformers import AutoModelForCausalLM, AutoTokenizer import torch DEVICE = 'cuda:0' baichuan_base_path = ""./modules/Baichuan213BBase"" tokenizer = AutoTokenizer.from_pretrained(baichuan_base_path, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(baichuan_base_path, device_map=""auto"",                                              trust_remote_code=True) inputs = tokenizer('ç™»é¹³é›€æ¥¼>ç‹ä¹‹æ¶£\nå¤œé›¨å¯„åŒ—>', return_tensors='pt') inputs = inputs.to(DEVICE) onnx_file_path = ""./onnx/Baichuan213BBase.onnx"" torch.onnx.export(     model,     (inputs['input_ids'], inputs['attention_mask']),   è¾“å…¥æ•°æ®     onnx_file_path,       verbose=True,       opset_version=18,     input_names=[""input_ids"", ""attention_mask""],   è¾“å…¥åç§°     output_names=[""output""],       dynamic_axes={         'input_ids': {0: 'batch_size', 1: 'sequence_length'},         'attention_mask': {0: 'batch_size', 1: 'sequence_length'},         'output': {0: 'batch_size', 1: 'sequence_length'}     } )  Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 3.27.5 Libc version: glibc2.17 Python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01160.90.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.2.140 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A800SXM480GB GPU 1: NVIDIA A800SXM480GB GPU 2: NVIDIA A800SXM480GB GPU 3: NVIDIA A800SXM480GB GPU 4: NVIDIA A800SXM480GB GPU 5: NVI",2023-09-26T10:01:43Z,module: onnx triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/110070,"Recently, I have been studying the conversion of various LLM to onnx. Maybe I have some problems with the conversion code. If possible, please tell me the correct method of converting Baichuan2 to onnx. Thank you very much.",It works in nightly.  CC(Support for 'aten::unflatten' operator when exporting to ONNX opset version 12)issuecomment1670143184,"Hello, I updated pytorch to version 2.1 without the above error, but there is a new problem Traceback (most recent call last):   File ""/root/anaconda3/envs/ry_pytorch_gpu/lib/python3.10/sitepackages/torch/onnx/utils.py"", line 1686, in _export     _C._check_onnx_proto(proto) RuntimeError: Unrecognized attribute: axes for operator ReduceMean ==> Context: Bad node spec for node. Name: /model/layers.0/input_layernorm/ReduceMean OpType: ReduceMean The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/data01/ry/pt2onnx/pt2onnx.py"", line 138, in      convert_llama2_onnx()   File ""/data01/ry/pt2onnx/pt2onnx.py"", line 102, in convert_llama2_onnx     torch.onnx.export(   File ""/root/anaconda3/envs/ry_pytorch_gpu/lib/python3.10/sitepackages/torch/onnx/utils.py"", line 516, in export     _export(   File ""/root/anaconda3/envs/ry_pytorch_gpu/lib/python3.10/sitepackages/torch/onnx/utils.py"", line 1688, in _export     raise errors.CheckerError(e) from e torch.onnx.errors.CheckerError: Unrecognized attribute: axes for operator ReduceMean ==> Context: Bad node spec for node. Name: /model/layers.0/input_layernorm/ReduceMean OpType: ReduceMean"
427,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add IWYU pragma for umbrella header)ï¼Œ å†…å®¹æ˜¯ (Summary: Include cleaner tools would benefit from IWYU pragmas that define implementation headers and exported headers.   See for more information.  Test Plan: sandcastle_pass Differential Revision: D49632855)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add IWYU pragma for umbrella header,Summary: Include cleaner tools would benefit from IWYU pragmas that define implementation headers and exported headers.   See for more information.  Test Plan: sandcastle_pass Differential Revision: D49632855,2023-09-26T09:03:04Z,fb-exported Stale,closed,0,4,https://github.com/pytorch/pytorch/issues/110067,"   :x: The email address for the commit (daa6af8cf286d89e83a44cb2981c913c8ef8dff4) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D49632855," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
689,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo][nn_module_guards] Config flag to disable nn_module_guards)ï¼Œ å†…å®¹æ˜¯ (  CC([dynamo][guardslog] Print nn module guard saved dict versions for debugging)  CC([dynamo][nn_module_guards] Config flag to disable nn_module_guards)  CC([dynamo][guardslog] Do not print duplicate guard entries) This flag is requested by  who is seeing recompilations with simple gpt experiments. We are observing recompilations because `_parameters` ordered dict keeps changing from run to run, and its unclear why that is happening. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[dynamo][nn_module_guards] Config flag to disable nn_module_guards,"  CC([dynamo][guardslog] Print nn module guard saved dict versions for debugging)  CC([dynamo][nn_module_guards] Config flag to disable nn_module_guards)  CC([dynamo][guardslog] Do not print duplicate guard entries) This flag is requested by  who is seeing recompilations with simple gpt experiments. We are observing recompilations because `_parameters` ordered dict keeps changing from run to run, and its unclear why that is happening. ",2023-09-25T21:35:01Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/110039, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
509,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reland: implement a function to convert a storage to copy-on-write)ï¼Œ å†…å®¹æ˜¯ (Relands CC(implement a function to convert a storage to copyonwrite) In addition, the `impl_cow_context` library is combined into the base c10 core library, and COW unit tests are combined into just one binary. Part of CC(Implement Copyonwrite (COW) tensors) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Reland: implement a function to convert a storage to copy-on-write,"Relands CC(implement a function to convert a storage to copyonwrite) In addition, the `impl_cow_context` library is combined into the base c10 core library, and COW unit tests are combined into just one binary. Part of CC(Implement Copyonwrite (COW) tensors) ",2023-09-25T17:10:44Z,module: internals open source Merged Reverted ciflow/trunk topic: not user facing ciflow/periodic,closed,0,15,https://github.com/pytorch/pytorch/issues/110022, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: periodic / macos12py3x8664 / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcpupy3.10gcc9bazeltest / buildandtest (default, 1, 1, linux.4xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: periodic / macos12py3x8664 / test (default, 1, 4, macos12) Details for Dev Infra team Raised by workflow job "," merge f ""flaky CI failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m ""New tests are failing in internal CI"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,Following tests are failing in internal CI:  This one is memory leaked: 
2030,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Static quantization for Transformer block : AttributeError 'function' object has no attribute 'is_cuda')ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm trying to apply static quantization to a model using a `nn.TransformerEncoderLayer`. But when running the model, I get the following error :    Here is a Colab notebook reproducing the issue : https://colab.research.google.com/drive/14jJBQk6DSn6DJxfOa8gx5zhpol9vwBMp?usp=sharing Here is the script reproducing the issue :    Versions Collecting environment information... PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.4 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.120+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          2 Online CPU(s) list:             0,)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Static quantization for Transformer block : AttributeError 'function' object has no attribute 'is_cuda'," ğŸ› Describe the bug I'm trying to apply static quantization to a model using a `nn.TransformerEncoderLayer`. But when running the model, I get the following error :    Here is a Colab notebook reproducing the issue : https://colab.research.google.com/drive/14jJBQk6DSn6DJxfOa8gx5zhpol9vwBMp?usp=sharing Here is the script reproducing the issue :    Versions Collecting environment information... PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.27.4 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.120+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          2 Online CPU(s) list:             0,",2023-09-25T03:08:08Z,module: nn triaged,open,0,3,https://github.com/pytorch/pytorch/issues/109987,I can confirm that the same error happens when using dynamic quantization. Here is a Colab reproducing the issue with dynamic quantization : https://colab.research.google.com/drive/1_MLSRqwFtDTNDDwp8k_WYzALX4KJUAZW?usp=sharing And here is the script used :  ,"On the latest version of `pytorch`, dynamic quantization seems to work. However if I try to JIT script the models, it fails. You can reproduce by simply adding the following code in the script mentioned earlier :    I get the following error :  > RuntimeError:  method cannot be used as a value:   File ""/Users/usr/miniconda3/envs/transfo/lib/python3.8/sitepackages/torch/nn/modules/transformer.py"", line 659                 self.norm2.weight,                 self.norm2.bias,                 self.linear1.weight,                 ~~~~~~~~~~~~~~~~~~~ < HERE                 self.linear1.bias,                 self.linear2.weight,","Workaround :  By commenting this whole branch of the code, the code seems to work, and I can run the JIT model. Will keep this issue open until a proper solution is found."
737,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use github.com/apssouza22/chatflow as a conversational layer. It would enable actual API requests to be carried out from natural language inputs.)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Adding this conversational UI would enable people to 'talk' directly with the backend and API requests to be carried out more effectively. RAG can help with some of the problems function calling by language models face at the moment.  Alternatives none  Additional context I'm trying to accelerate the adoption of natural language interfaces. https://youtu.be/r3cegH2kviQ)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Use github.com/apssouza22/chatflow as a conversational layer. It would enable actual API requests to be carried out from natural language inputs.," ğŸš€ The feature, motivation and pitch Adding this conversational UI would enable people to 'talk' directly with the backend and API requests to be carried out more effectively. RAG can help with some of the problems function calling by language models face at the moment.  Alternatives none  Additional context I'm trying to accelerate the adoption of natural language interfaces. https://youtu.be/r3cegH2kviQ",2023-09-25T01:02:09Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/109984,"Hey! I'm not sure what is the exact thing you're proposing to do here. If you have a more concrete proposal, feel free to reopen this issue with these details, thanks!"
381,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory access fault with AMD Rocm)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When using Pytorch with Rocm, trying to train or infer with an upscaling model, I get this error:   Versions PyTorch: 2.2.0.dev20230920+rocm5.6 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Memory access fault with AMD Rocm," ğŸ› Describe the bug When using Pytorch with Rocm, trying to train or infer with an upscaling model, I get this error:   Versions PyTorch: 2.2.0.dev20230920+rocm5.6 ",2023-09-22T23:48:40Z,needs reproduction module: rocm triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/109929,"If relevant, add a minimal example so that we can reproduce the error by running the code. Please run the following and paste the output below. ",  please provide simple script for us to reproduce and include output of python collect_env .  We will close this issue if we don't get response after 3 weeks after the issue was opened. You can open the issue if you have the details afterwards.,Sorry for the long time to answer. The issue happens anytime I try to upscale an image using Chainner or train a model using Trainner. Basically anytime I use Pytorch for anything actually... It didn't happen with Pytorch 2.0.1 with Rocm 5.4. Here is the output :  ,Can you try to set HSA_OVERRIDE_GFX_VERSION=10.3.0 environment variable and try again?," Does the above environment variable override work for you? Please update, otherwise, we will close the issue.","Sorry, we need more clarity on this issue. Please reopen an issue with reproducible steps and error messages. And also, try to override your environment with the above suggestion. We need to close this as we did not see follow up in more than 3 weeks."
1316,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add torch.library.impl_abstract)ï¼Œ å†…å®¹æ˜¯ (  CC([optests] Excise unused operator_compile_check)  CC(Add torch.library.impl_abstract) Changelog:  torch.library.impl_abstract optionally accepts a torch.library.Library   object. If passed in, then the lifetime of the registration is tied to   the Library object.  we've also changed torch.library.impl_abstract to work on all   operators, including overloads.  we refactored the `torch._custom_ops.*` and `torch._custom_op.*`   impl_abstract APIs and put them under torch._library. This is the   final resting place for them. I will followup with deleting   all the `torch._custom_ops.*` stuff later.  There is a new ""SimpleOperatorRegistry"" where we actually collect the   abstract_impl. We will expand this to also hold the other   torch._custom_ops.* APIs when we move those to torch.library NB: Previously we had designed `impl_abstract` assuming a very highlevel Pythononly custom op API. We've revisited that since; now, impl_abstract works for all custom ops, no matter python or C++, no matter the schema. The new refactored design reflects this better. Test Plan:  existing and new tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add torch.library.impl_abstract,"  CC([optests] Excise unused operator_compile_check)  CC(Add torch.library.impl_abstract) Changelog:  torch.library.impl_abstract optionally accepts a torch.library.Library   object. If passed in, then the lifetime of the registration is tied to   the Library object.  we've also changed torch.library.impl_abstract to work on all   operators, including overloads.  we refactored the `torch._custom_ops.*` and `torch._custom_op.*`   impl_abstract APIs and put them under torch._library. This is the   final resting place for them. I will followup with deleting   all the `torch._custom_ops.*` stuff later.  There is a new ""SimpleOperatorRegistry"" where we actually collect the   abstract_impl. We will expand this to also hold the other   torch._custom_ops.* APIs when we move those to torch.library NB: Previously we had designed `impl_abstract` assuming a very highlevel Pythononly custom op API. We've revisited that since; now, impl_abstract works for all custom ops, no matter python or C++, no matter the schema. The new refactored design reflects this better. Test Plan:  existing and new tests",2023-09-22T20:32:23Z,Merged ciflow/trunk release notes: composability ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/109912,NB: this PR is kind of long. I'm happy to attempt to split it up if necessary. It might be easier to just review all of the green lines.,I will... eventually review it... it will just be faster if it's shorter ğŸ“¦ ,Okay let me split, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
290,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DALLE2_pytorch: ""torch._dynamo.exc.Unsupported: call_method NNModuleVariable() eval [] {}"")ï¼Œ å†…å®¹æ˜¯ (Repro:  Error:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"DALLE2_pytorch: ""torch._dynamo.exc.Unsupported: call_method NNModuleVariable() eval [] {}""",Repro:  Error:  ,2023-09-22T14:44:30Z,triaged oncall: pt2 module: inductor,open,0,0,https://github.com/pytorch/pytorch/issues/109885
1965,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor][cpu] performance regression)ï¼Œ å†…å®¹æ˜¯ (new_perf_regression: 20230920 nightly release vs 20230917 nightly release Note: multi threads secnario for models above first *, single thread for models between two * new_perf_regression                name       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       *       *       *       *       *       *       *       *       *       *       *       *       *       *       *                 llama       1       0.077857       0.416423326       0.032421470892381996       39.62569       1       0.318457       0.102005337       0.032484313605009       38.0415       0.24       1.0       0.24       0.96                 *       *       *       *       *       *       *       *       *       *       *       *       *       *       *          SW info                SW       Nightly commit       Main commit                       Pytorch       00ae5fa       1b3e5b5                 Torchbench       /       ffbbebb9                 torchaudio       475b6ae       ede4309                 torchtext       142d029       45e4b8c                 torchvision       8636bf3       4ac707a                 torchdata       eb9bf61       d76d92c                 dynamo_benchmarks       0200b11       /           Reference SW info                SW       Nightly commit       Main commit                       Pytorch       0de2555                        Torchbench       /       ffbbebb9                 torchaudio       475b6ae                      )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[inductor][cpu] performance regression,"new_perf_regression: 20230920 nightly release vs 20230917 nightly release Note: multi threads secnario for models above first *, single thread for models between two * new_perf_regression                name       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       *       *       *       *       *       *       *       *       *       *       *       *       *       *       *                 llama       1       0.077857       0.416423326       0.032421470892381996       39.62569       1       0.318457       0.102005337       0.032484313605009       38.0415       0.24       1.0       0.24       0.96                 *       *       *       *       *       *       *       *       *       *       *       *       *       *       *          SW info                SW       Nightly commit       Main commit                       Pytorch       00ae5fa       1b3e5b5                 Torchbench       /       ffbbebb9                 torchaudio       475b6ae       ede4309                 torchtext       142d029       45e4b8c                 torchvision       8636bf3       4ac707a                 torchdata       eb9bf61       d76d92c                 dynamo_benchmarks       0200b11       /           Reference SW info                SW       Nightly commit       Main commit                       Pytorch       0de2555                        Torchbench       /       ffbbebb9                 torchaudio       475b6ae                      ",2023-09-22T09:53:50Z,triaged oncall: pt2 module: inductor oncall: cpu inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/109874,([inductor] realize_into should not alias src and dst),Zheng Could you help to take a look of this issue?,"We found 2 commits:  https://github.com/pytorch/pytorch/pull/108126  https://github.com/pytorch/pytorch/pull/107614 caused this regression. After these 2 PRs, there are several extra buffer created with memory copy."," Similar issue as https://github.com/pytorch/pytorch/pull/112440 and I think this issue is backend irreverent.    Similar fixes by adding `unsafe_alias=True` like https://github.com/pytorch/pytorch/pull/112440 also fixes this issue https://github.com/pytorch/pytorch/blob/87ea6fb84471c09795672de542622515f81af3d7/torch/_inductor/lowering.pyL4790    But I don't think it's safe by adding this flag `return mutate_to(dst, src, unsafe_alias=True)` during the lowering of `copy_` .    The rootcause is extra buffer copy introduced which are not necessary in this case    Generated code before regression is: https://gist.github.com/lesliefangintel/a5923cb16b93e029242b99c6c4eff77e    Generated code after regression is: https://gist.github.com/lesliefangintel/d48f2fa719609ff6e8ea55d83a1cb163    As you can see extra memory copies after the regression:  Take `buf288`, `buf289` and `arg76_1` as example:  `arg76_1` is a input buffer, here we will do `copy_(arg76_1, buf288)` which seems will change the input buffer.  `buf288` is also a user of `arg76_1`, so `buf288` is realized firstly here https://github.com/pytorch/pytorch/blob/87ea6fb84471c09795672de542622515f81af3d7/torch/_inductor/ir.pyL2685  Then `buf289` is realized as a copy of `buf288` https://github.com/pytorch/pytorch/blob/87ea6fb84471c09795672de542622515f81af3d7/torch/_inductor/ir.pyL2698L2709  `buf289` is also `MutationLayout` of `arg76_1` https://github.com/pytorch/pytorch/blob/87ea6fb84471c09795672de542622515f81af3d7/torch/_inductor/ir.pyL2710L2711  Before regression, there is no `buf289`. We will directly mark `buf288` as the `MutationLayout` of `arg76_1`.    **How to fix**:    Per the comment here https://github.com/pytorch/pytorch/blob/87ea6fb84471c09795672de542622515f81af3d7/torch/_inductor/ir.pyL2690L2695  and discussion with  here https://github.com/pytorch/pytorch/pull/110787discussion_r1350375182, if we want to skip the copy from `buf288` to `buf 289`, we need to ensure `dst` (`arg76_1`) will not be further mutated again.     Per my understanding, since marking `buf288` copy to `buf289` is still during the graph lowering process, we are not sure whether `dst` (`arg76_1`) will be mutated again. So, we are not able to do it during the `GraphLowering`.    Is it possible to do it during the scheduler? During the `scheduler`, We possible can know that      `buf289` is the only mutation of  `arg76_1`      `buf289` depends on `buf288`.      How can we know `buf289` is just a copy of `buf288`?         If we know that, can we directly remove `buf289`? After we remove `buf289`, how to mark `buf288` as the mutation of `arg76_1`?",Expected fixing by https://github.com/pytorch/pytorch/pull/116899,"Close this issue as per  CC(TorchInductor CPU Performance Dashboard)issuecomment1916066691, the llama single thread speedup is 1.392433x now."
2016,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(an issue occurs while `loss.backward()`: You are trying to call the hook of a dead module)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Can be reproduced in: https://github.com/EthanChenplus/dp_with_llm/blob/master/maincsverror3new.ipynb As above, `trainer.training_step` can work ok as well as `loss.backward()`  However if we use trainer.train(), some errors happend: !image !image  Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: Could not collect CMake version: version 3.27.5 Libc version: glibc2.31 Python version: 3.9.18  (main, Aug 30 2023, 03:49:32)  [GCC 12.3.0] (64bit runtime) Python platform: Linux5.4.0162genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 GPU 3: NVIDIA GeForce RTX 3090 GPU 4: NVIDIA GeForce RTX 3090 GPU 5: NVIDIA GeForce RTX 3090 GPU 6: NVIDIA GeForce RTX 3090 GPU 7: NVIDIA GeForce RTX 3090 Nvidia driver version: 525.125.06 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.1 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8.0.5 /u)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,an issue occurs while `loss.backward()`: You are trying to call the hook of a dead module," ğŸ› Describe the bug Can be reproduced in: https://github.com/EthanChenplus/dp_with_llm/blob/master/maincsverror3new.ipynb As above, `trainer.training_step` can work ok as well as `loss.backward()`  However if we use trainer.train(), some errors happend: !image !image  Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: Could not collect CMake version: version 3.27.5 Libc version: glibc2.31 Python version: 3.9.18  (main, Aug 30 2023, 03:49:32)  [GCC 12.3.0] (64bit runtime) Python platform: Linux5.4.0162genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 GPU 3: NVIDIA GeForce RTX 3090 GPU 4: NVIDIA GeForce RTX 3090 GPU 5: NVIDIA GeForce RTX 3090 GPU 6: NVIDIA GeForce RTX 3090 GPU 7: NVIDIA GeForce RTX 3090 Nvidia driver version: 525.125.06 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.1 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8.0.5 /u",2023-09-21T08:25:52Z,needs reproduction module: autograd module: nn triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/109778,Thanks for the report. Nonfull backward hooks have been deprecated in favor of https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_backward_hooktorch.nn.Module.register_full_backward_hook This error should only happen if you use nonfull backward hooks. Is there any reason you aren't using full backward hooks?,Closing for now since nonfull backward hooks are deprecated,  I'm sorry for the delayed response. I was off for the past two weeks. I double checked my code as they were using `transformers` and I haven't find any nonfull backward hooks.  This is very strange since the entry point of the function is `loss.backward()`.,"Could you provide a selfcontained snippet that can reproduce this issue? If nonfull backward hooks aren't being used, then the other way this could happen is if you are loading a state dict during backward."," Thank you! I will try my best to find a selfcontained code snippet. However, it may depend on transformers or dp_transformers. Please give me some time, and thank you for your patience.","  The problem may lie in version dependency and coupling relationship between `torch`, `transformers`, `dp_transformers` and `opacus`. I tried to recreate the environment with the packages listed below, and the issue was resolved. Here are the `YAML` file and dependency `.txt` for the environment, as well as the installation command. Thank you for keeping track of this issue. install command:  channels:    defaults    condaforge    pytorch    nvidia  Additional information can be found in: https://github.com/microsoft/dptransformers/issues/35 glm_lora.txt glm_lora_yaml.zip"
1981,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(LLaMA-2 70b model convert from PyTorch to ONNX format )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug error in converting pytorch llama2 format to onnx format  results below   Versions /home/onnxruntimedev/miniconda3/lib/python3.9/sitepackages/torch/cuda/__init__.py:173: UserWarning: NVIDIA H100 80GB HBM3 with CUDA capability sm_90 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86. If you want to use the NVIDIA H100 80GB HBM3 GPU with PyTorch, please check the instructions at https://pytorch.org/getstarted/locally/   warnings.warn(incompatible_device_warn.format(device_name, capability, "" "".join(arch_list), device_name)) Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.27.5 Libc version: glibc2.31 Python version: 3.9.2 (default, Mar  3 2021, 20:02:32)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.15.073genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 530.30.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /u)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,LLaMA-2 70b model convert from PyTorch to ONNX format ," ğŸ› Describe the bug error in converting pytorch llama2 format to onnx format  results below   Versions /home/onnxruntimedev/miniconda3/lib/python3.9/sitepackages/torch/cuda/__init__.py:173: UserWarning: NVIDIA H100 80GB HBM3 with CUDA capability sm_90 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86. If you want to use the NVIDIA H100 80GB HBM3 GPU with PyTorch, please check the instructions at https://pytorch.org/getstarted/locally/   warnings.warn(incompatible_device_warn.format(device_name, capability, "" "".join(arch_list), device_name)) Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.2) 9.4.0 Clang version: Could not collect CMake version: version 3.27.5 Libc version: glibc2.31 Python version: 3.9.2 (default, Mar  3 2021, 20:02:32)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.15.073genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 530.30.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /u",2023-09-21T03:50:22Z,module: onnx triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/109768,"Hi, this is not `torch.onnx.export` API that we provide. Would you raise an error to whichever repo providing this feature, and maybe repro it with our API (torch.onnx.export)?","Hi  , Closing this issue as stale. Feel free to use `torch.onnx.dynanmo_export`, we have support for llama2 at this point. Feel free to message or reopen issue if necessary. "
574,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Extend storage access error api for untyped_storage())ï¼Œ å†…å®¹æ˜¯ (  CC(Extend storage access error api for untyped_storage()) In cudagraph trees, we invalidate tensors at some point and drop their storage. Then, when they are accessed with .data_ptr(), a custom error message is thrown. Previously, this invalidation didn't also make untyped_storage()/storage() error which could result in a segfault.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Extend storage access error api for untyped_storage(),"  CC(Extend storage access error api for untyped_storage()) In cudagraph trees, we invalidate tensors at some point and drop their storage. Then, when they are accessed with .data_ptr(), a custom error message is thrown. Previously, this invalidation didn't also make untyped_storage()/storage() error which could result in a segfault.  ",2023-09-20T22:57:04Z,Merged ciflow/trunk topic: not user facing module: inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/109750, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalcuda11.8py3.10gcc9 / test (distributed, 1, 3, linux.8xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""flakey distributed test"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
314,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(rename nanogpt_generate to nanogpt to also support train)ï¼Œ å†…å®¹æ˜¯ (  CC(rename nanogpt_generate to nanogpt to also support train) : D49522940)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,rename nanogpt_generate to nanogpt to also support train,  CC(rename nanogpt_generate to nanogpt to also support train) : D49522940,2023-09-20T22:33:14Z,Merged ciflow/trunk release notes: releng module: dynamo ciflow/inductor,closed,0,14,https://github.com/pytorch/pytorch/issues/109746,"If we are renaming this model, should we also remove `nanogpt_generate` at the same time?",I'll include the updated torchbench commit pin from https://github.com/pytorch/benchmark/pull/1911 in this PR to also remove `nanogpt_generate` here," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," For some reason, stable_diffusion_text_encoder is running twice, passes the first but fails on the second with infra_error  Also stable_diffusion_unet OOMs during training, so I had to disable it",Do you have the error message for the text encoder failure? curious about that also how was the unet ooming exactly? if it was on accuracy checks then there's an explicit skip for accuracy checks for large models that's better than an outright skip,"stable_diffusion_text_encoder infra error: https://github.com/pytorch/pytorch/actions/runs/6278447316/job/17052570700  stable_diffusion_unet OOM was also in the same job as the above log (inductor_torchbench_dynamic_cpu_accuracy). Currently, I disabled it for cpu only, it seems to pass in the cuda accuracy test (https://github.com/pytorch/pytorch/actions/runs/6278447316/job/17053309644). Is it better to disable accuracy check alltogether?"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch rebase origin/main` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1899,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensor indexing error when using 'mps')ï¼Œ å†…å®¹æ˜¯ (We are training a DETR model using transformers and it works well on any machine with a GPU+CUDA. Running it on a Mac only works if we use the ""cpu"" accelerator. With 'mps' it throws an error (see full stack below): ValueError: boxes1 must be in [x0, y0, x1, y1] (corner) format, but got tensor([], device='mps:0', size=(0, 4)) Using Lightning v2.0.9 on a MacBook Pro M2 Max 64GB  Bug was first report to PyTorch Lightning but the team suggested that it was a PyTorch issue.    Versions PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.5.2 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.5.2arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] numpy==1.25.2 [pip3] pytorchlightning==2.0.9 [pip3] torch==2.0.1 [pip3] torchmetrics==1.1.2 [pip3] torchvision==0.15.2 [conda] numpy                     1.25.2                   pypi_0    pypi [conda] pytorchlightning         2.0.9                    pypi_0    pypi [conda] torch                     2.0.1                    pypi_0    pypi [conda] torchmetrics              1.1.2                    pypi_0    pypi [conda] torchvision               0.15.2                   pypi_0    pypi )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Tensor indexing error when using 'mps',"We are training a DETR model using transformers and it works well on any machine with a GPU+CUDA. Running it on a Mac only works if we use the ""cpu"" accelerator. With 'mps' it throws an error (see full stack below): ValueError: boxes1 must be in [x0, y0, x1, y1] (corner) format, but got tensor([], device='mps:0', size=(0, 4)) Using Lightning v2.0.9 on a MacBook Pro M2 Max 64GB  Bug was first report to PyTorch Lightning but the team suggested that it was a PyTorch issue.    Versions PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.5.2 (arm64) GCC version: Could not collect Clang version: 15.0.0 (clang1500.0.40.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.5.2arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] numpy==1.25.2 [pip3] pytorchlightning==2.0.9 [pip3] torch==2.0.1 [pip3] torchmetrics==1.1.2 [pip3] torchvision==0.15.2 [conda] numpy                     1.25.2                   pypi_0    pypi [conda] pytorchlightning         2.0.9                    pypi_0    pypi [conda] torch                     2.0.1                    pypi_0    pypi [conda] torchmetrics              1.1.2                    pypi_0    pypi [conda] torchvision               0.15.2                   pypi_0    pypi ",2023-09-20T14:08:18Z,needs reproduction triaged module: mps,closed,0,2,https://github.com/pytorch/pytorch/issues/109716,"Can you please try latest torch nightly? Also, it would really help if you(or someone else) can provide a smaller repro example, where MPS backend does not return what is expected",Installing torch2.2.0.dev20230920 torchaudio2.2.0.dev20230920 torchvision0.17.0.dev20230920 seems to have fixed the issue. Thank you  
692,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Regression on 2.1 RC RoCm: data parallel error on `torch._C._broadcast_coalesced`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I noticed today a bug in data parallel on RoCm system that is not present on NVIDIA ones. Reproduction: run the following script `CUDA_VISIBLE_DEVICES=0,1 python run_dp.py`:  raises  on torch 2.1 RC (`2.1.0+rocm5.6`), while no error is raised on 2.1 RC (`2.1.0+cu118`) on an NVIDIA system. On 2.0.1, there is no issue (specifically on `rocm/pytorch:rocm5.6_ubuntu20.04_py3.8_pytorch_2.0.1`).  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Regression on 2.1 RC RoCm: data parallel error on `torch._C._broadcast_coalesced`," ğŸ› Describe the bug Hi, I noticed today a bug in data parallel on RoCm system that is not present on NVIDIA ones. Reproduction: run the following script `CUDA_VISIBLE_DEVICES=0,1 python run_dp.py`:  raises  on torch 2.1 RC (`2.1.0+rocm5.6`), while no error is raised on 2.1 RC (`2.1.0+cu118`) on an NVIDIA system. On 2.0.1, there is no issue (specifically on `rocm/pytorch:rocm5.6_ubuntu20.04_py3.8_pytorch_2.0.1`).  Versions  ",2023-09-20T10:10:03Z,high priority module: binaries in progress module: rocm oncall: releng triaged,closed,0,17,https://github.com/pytorch/pytorch/issues/109709,"Since the issue is NOT happening on `pytorch2.0.1+ROCm 5.6`, it seems some pytorch change is affecting ROCm.","I was able to reproduce this issue on an MI100. It is due to a bug in the wheel build logic, that was fixed for nightly wheels.",Here is the cherrypick for this issue: https://github.com/pytorch/builder/pull/1546 . Rocm wheel was rebuild last week with this fix. amd could you please confirm this is resolved ?,"Q: is `oncall: releng` the right keyword here? Or perhaps `oncall: binaries`, as it affects trunk as well?","Yes, I tested RC wheels and the RCCL error (due to some required files not being found) is fixed in PR https://github.com/pytorch/builder/pull/1546. However, the testcase in this PR now fails with a different error.  The same error is observed in PyTorch nightly wheels, so I am not sure if this is a 2.1 release blocker. We are still investigating.",amd could you please include the new error after https://github.com/pytorch/builder/pull/1546  here for reference ?,Tentatively setting 2.1.1. milestone, Here it is: `:0:rocdevice.cpp            :2778: 1125517855952 us: 284  : [tid:0x7f713b807700] Callback: Queue 0x7f6c9c200000 aborting with error : HSA_STATUS_ERROR_EXCEPTION: An HSAIL operation resulted in a hardware exception. code: 0x1016`  This is the new error I was referring to. I see this with both nightly and 2.1 RC wheels. Let's look into this with priority so we can try to fix this for 2.1.1 release.,amd Do we have a fix for this issue ? Should we remove it from milestone for 2.1.1 ?,fyi: the code runs fine on the rocm's nightly docker image on mi250 and mi210. ,"Also tested with below pytorch nightly wheel on mi210, the code is fine. pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/rocm5.7","Hi, ,   is there a RC2 wheel I can test too? Right now, the nightly wheel is good.  Please kindly share the link so that I can test on AMD GPUs. Thanks.", RC wheels should be available at https://download.pytorch.org/whl/test/rocm5.6/, Confirmed the code finished fine using the above RC wheel on MI210: !image !image,Resolving this now as the fix has been confirmed,"Hi, How did you guys install the transformers on AMD ROCM/5.6? That would be really helpful if anybody knows.","cuanschutz Transformers should just work out of the box with PyTorch for ROCm (https://pytorch.org/getstarted/locally/), provided that your GPU is supported by ROCm (https://rocm.docs.amd.com/projects/installonlinux/en/latest/reference/systemrequirements.html). Feel free to open an issue in transformers repo if you hit any issue."
1988,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Test results for transformer change as the batch size changes)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I have obtained a pth file by training the encoder of transformer, but when I run the test, the batch size is different, and the predicted result will also change. May I ask why? And how to solve this problem?  Versions Collecting environment information... PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 ä¸“ä¸šç‰ˆ GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.17 (default, Jul  5 2023, 20:44:21) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=1700 DeviceID=CPU0 Family=198 L2CacheSize=4096 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=1700 Name=12th Gen Intel(R) Core(TM) i71255U ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchvision==0.14.1 [pip3] tritonclient==2.36.0 [conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main [conda] cpuonly                   2.0                           0    pytorch [conda] mkl                       2023.1.0         h8bd8f75_46356    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main [conda] mklservice               2.4.0            py38h2bbff1b_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Test results for transformer change as the batch size changes," ğŸ› Describe the bug I have obtained a pth file by training the encoder of transformer, but when I run the test, the batch size is different, and the predicted result will also change. May I ask why? And how to solve this problem?  Versions Collecting environment information... PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 ä¸“ä¸šç‰ˆ GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.17 (default, Jul  5 2023, 20:44:21) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=1700 DeviceID=CPU0 Family=198 L2CacheSize=4096 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=1700 Name=12th Gen Intel(R) Core(TM) i71255U ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchvision==0.14.1 [pip3] tritonclient==2.36.0 [conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main [conda] cpuonly                   2.0                           0    pytorch [conda] mkl                       2023.1.0         h8bd8f75_46356    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main [conda] mklservice               2.4.0            py38h2bbff1b_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/",2023-09-20T02:04:44Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/109676
1948,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.optim.Adafactor)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Models are getting harder to fit on a limited number of GPUs and ADAM doesn't help since its memory overhead is 2N where N is the number of parameters in a model !image We don't like to merge optimizers in core because they rarely stand the test of time but ADAM has and a memory efficient alternative that's been in use at many startups I've talked to, Twitter and larger companies https://github.com/googleresearch/t5x/blob/main/t5x/adafactor.py, has been Adafactor, see the discussion here  https://twitter.com/HamelHusain/status/1702004478369743125  it's also come up in a github issue before here  CC(assert callable(unaltered_fn)) Assigning to myself since I want a starter task in optimizers  Alternatives There is a working implementation in fairseq which Huggingface has borrowed https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adafactor.pyL66 which is a good starting point  Additional context You might be thinking why merge yet another optimizer? Optimizers are plagued by lack of reproducibility and sensitivity to hyperparameters  However, ADAM has stood the test of time but ADAM also has a high memory overhead, for each parameter you need to store the first and second moment so if your model has N parameters then your optimizer state is 2N. Also few people change its hyperparameters in fact the hyperparams have remained the same since torch days * https://github.com/pytorch/pytorch/blob/main/torch/optim/adam.py * https://github.com/torch/optim/blob/master/adam.lua So as Blueberries projects move towards finetuning after PyTorch conference and as more members of the community try to fit larger models on their GPUs it's critical we find memory efficient alterna)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,torch.optim.Adafactor," ğŸš€ The feature, motivation and pitch Models are getting harder to fit on a limited number of GPUs and ADAM doesn't help since its memory overhead is 2N where N is the number of parameters in a model !image We don't like to merge optimizers in core because they rarely stand the test of time but ADAM has and a memory efficient alternative that's been in use at many startups I've talked to, Twitter and larger companies https://github.com/googleresearch/t5x/blob/main/t5x/adafactor.py, has been Adafactor, see the discussion here  https://twitter.com/HamelHusain/status/1702004478369743125  it's also come up in a github issue before here  CC(assert callable(unaltered_fn)) Assigning to myself since I want a starter task in optimizers  Alternatives There is a working implementation in fairseq which Huggingface has borrowed https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adafactor.pyL66 which is a good starting point  Additional context You might be thinking why merge yet another optimizer? Optimizers are plagued by lack of reproducibility and sensitivity to hyperparameters  However, ADAM has stood the test of time but ADAM also has a high memory overhead, for each parameter you need to store the first and second moment so if your model has N parameters then your optimizer state is 2N. Also few people change its hyperparameters in fact the hyperparams have remained the same since torch days * https://github.com/pytorch/pytorch/blob/main/torch/optim/adam.py * https://github.com/torch/optim/blob/master/adam.lua So as Blueberries projects move towards finetuning after PyTorch conference and as more members of the community try to fit larger models on their GPUs it's critical we find memory efficient alterna",2023-09-19T05:20:14Z,feature module: optimizer triaged needs research,closed,0,13,https://github.com/pytorch/pytorch/issues/109581,"Thanks for opening an issuethis is intriguing! I agree that the optimizer footprint for Adam is quite sasquatch and now the question is whether Adafactor would be the strongest candidate here. On one end of the spectrum, as you've mentioned, we have vanilla SGD without momentum which takes up no state but takes longer to converge.  On the other hand, we have Adam, which takes up 2X params memory (and more runtime) but converges faster. Skimming https://openreview.net/pdf?id=Sf1NlV2r6PO seems to suggest that there is coordinate clipping that could be done in order to make SGD more effective, though I haven't yet dug into the details. It is interesting this paper also compared against Adafactor, which is a positive sign for Adafactor. Some stats on GitHub code search when searching for  regex `/[^AZaz]Optim\(/`:    . Knowing all this, I would like to answer the following: 1. is Adafactor _the_ candidate we are going for? Do other alternatives (e.g., SGD with clipping) fail to compare? Are there other optimizers in this realm that I am not aware of? 2. If we want to support Adafactor, should we be adding an entirely new Optimizer(torch.optim.Optimizer)? Should it live in core vs pytorchlabs? Maybe down the road, we'd be able to bundle a collection of foreach ops with some global state and bypass the need for big O Optimizer entirely.","Whether adafactor is in labs or core doesn't matter too much to me personally, I can add it in labs until we have more convincing first hand evidence it works better The clipping paper is quite new with 1 citation so not sure how many people have independently reproduced the results For Adafactor specifically the argument I'm making is not a popularitybased one for its inclusion, it seems to be disproportionately used at startups that know how to train large models  EDIT: Rethe point on foreach ops, at a high level that sounds good I guess you can save memory both by defining your optimizer state but also how you apply it, those feel like complementary techniques to me? ","I think adding an Optimizer subclass in pytorchlabs is a great first stepand I'm happy to help review that. The function of foreach ops is definitely a quarterbaked complementary idea that requires much more design so it is fine to not worry about that for this issue. What I was thinking was what  has mentioned with the new post_accumulate_grad_hook taking in a functionwe could define the optimizer step as a function with global state through that without the need of subclassing Optimizer. Unsure if this would be lighter weight, but I do think the standard today is still to subclass Optimizer.","I want to understand the relation between FSDP and Adafactor, so I am sharing my initial thoughts. Please point out any errors!  TL;DR  Existing FSDP cannot compose with Adafactor, forcing a choice between FSDP + Adam vs. DDP vs. Adafactor.  For >= 4 GPUs, FSDP + Adam uses less memory for Llama7B. A similar rule can be derived for other models.  There is a possible a path for perparametersharding FSDP to compose with Adafactor. cc:    Can Adafactor compose with FSDP? Under existing PyTorch FSDP's flatparameter sharding, it is **not possible** without some shape recovery algorithm. Under perparameter sharding, it requires one allreduce per `nn.Parameter`.   Flatparameter sharding flattens several parameters together, losing the required tensor structure. Existing Fairscale/PyTorch FSDP uses this.  Perparametersharding shards each parameter individually on dim0.  Adafactor requires computing a mean over the row state (t5x, fairseq), which, under perparameter sharding, would be sharded across data parallel workers. Thus, this mean computation would require an allreduce.  Should I use Adafactor or Today's PyTorch FSDP?  Assuming that FSDP and Adafactor cannot compose, we consider two options: FSDP + Adam vs. DDP + Adafactor. **Setup** Notation:  `N`: number of GPUs  `M`: model's total numel  `A`: Adafactor states numel (function of parameter shapes)  `B`: transformer block numel  `E`: nontransformerblock numel (`M  L * B` for `L` transformer blocks) For Llama7B, `M = 6.7e9`, `A = 2.8e6`, `B = 2.0e8`, and `E = 2.6e8`. **Analysis** The two options have the same activation memory (since we assume same local batch size), so we only need to consider the modelstate memory: parameters, gradients, and optimizer states.  For simplicity, the table assumes no mixed precision. Mixed precision would only decrease the contribution from the `(3+1/N)B + E` term in FSDP + Adam, favoring FSDP. For brevity, I omit the explanation for `(3 + 1/N)B + E`; let me know if you want me to write it out here. Since `M`, `A`, `B`, and `E` are known (for Llama7B), we solve for `N` in `4M/N + (3 + 1/N)B + E = 2M + A` to get `N ~= 2.15`. This suggests that as long as you have >2 GPUs, you would prefer to use FSDP + Adam. Practically, there are two **caveats**: 1. Existing FSDP uses `(4 + 1/N)B` due to an implementation limitation, though this only changes `N` to `2.18`, not affecting the conclusion. 2. Existing FSDP uses `recordStream`, which can hold onto memory longer than necessary depending on _GPU kernel timing_, increasing the `(3 + 1/N)B` term. This makes it difficult to upper bound its memory usage analytically.       That being said, we generally do not see FSDP holding onto >3 layers' memory. Relaxing `(3 + 1/N)B` to `(32 + 1/N)B`, conservatively representing FSDP holding onto _8 layers' memory_, `N` still solves to `= 4 GPUs, we prefer to use FSDP + Adam than DDP + Adafactor.  What if FSDP Could Compose with Adafactor? We would achieve a `2M/N` > `A` reduction in modelstate memory, which is significant for small `N`. How might we achieve this? We are proposing an FSDP rewrite that uses perparameter sharding ( CC([RFC] Per-Parameter-Sharding FSDP)) that includes avoiding both practical caveats above. The proposal includes exposing FSDPsharded parameters as `DTensor`s with `(Shard(0),)` placement, providing an interface for nonpointwise optimizer integration. In theory, `DTensor` could implement `torch.mean` (if not already) to include the required allreduce without user intervention (cc: ). A critical consideration is optimizer state dict. For Adafactor optimizer states, the column state would be replicated, and the row state would be sharded on dim0. This can fit nicely into the `DTensor` paradigm. If `DTensor` and the Adafactor implementation could work such that the row state becomes `(Shard(0),)` placement and the column state becomes `(Replicate,)` placement without intervention, then FSDP and nonFSDP usages could share the same implementation (again, cc: ). From a performance standpoint, overlapping the allreduce with computation can be desirable. However, the importance depends on the hardware setup, especially if FSDP + Adafactor is targeting a small number of GPUs within one host with fast network bandwidth. One option would be to leave overlapping to `torch.compile`, but we should brainstorm around this overlapping requirement early."," thank you for this analysis. If we look strictly at memory, it looks like FSDP + Adafactor would be the clear winner in the future (`2M/N + (3 + 1/N)B + E + A`). So I'm all for the FSDP rewrite along with your suggestions regarding DTensor mentioned above (I don't quite understand the `(Shard(0), )` row vs column state in DTensor, but I figure  gets it) as maintaining Adafactor being the same across nonFSDP + FSDP is crucial for composability. To pick your brain a little bit, since this calculation was on llama7b, do you see this changing a lot for llama70b/across other LLMs?","> To pick your brain a little bit, since this calculation was on llama7b, do you see this changing a lot for llama70b/across other LLMs? It depends. It might be easiest to rerun the calculations for the specific LLM and cluster. Generally, increasing `M` correlates with increasing `N` to meet the memory requirement. However, for this memoryconstrained context, we probably would start applying other memory saving techniques like (Q)LoRA, which changes the formulas. If we use LoRA, then there is lower optimizer state memory, which means that Adafactor gives less absolute savings.","Thanks for the detailed summary ! > If DTensor and the Adafactor implementation could work such that the row state becomes (Shard(0),) placement and the column state becomes (Replicate,) placement without intervention, then FSDP and nonFSDP usages could share the same implementation Yeah that's a natural fit to DTensor where we can control the optimizer state to make one be replication and the other be sharded! There possibly be more nonpointwise optimizer states needs this flexibility going forward, so this looks like a important starter for us! > maintaining Adafactor being the same across nonFSDP + FSDP is crucial for composability I looked at the code pointer for row/col states, I feel if `torch.zeros` would take `device_mesh` and `placements` and when it does it give back a DTensor, this might makes it easier to maintain this uniformly. right now dtensor need to separately maintain those tensor constructors, . The other option is to make a TorchDispatchMode that allocates those states, and enable this torch_dispatch mode under distributed context, this way we can also maintain same code. ","Given that jax.Array has gone to being distributed by default, I am conceptually in favor of the idea that torch.Tensor should also be distributed by default. However, this means that DTensor must ensure that a nondistributed DTensor is actually just a plain Tensor, and that the APIs are substitutable in these cases. I don't think this is impossible to do, but it is Work and would need to be done. What I do not want is having both Tensor and single node DTensor floating around, a distinction without meaning.",Any update on this? FSDP + AdaFactor will be greatly appreciated :pray:,sf no progress yet but I'm still looking to merge this,Any progress on this yet?,Nope but I am planning to get to it in a few weeks,"The forloop implementation has landed and uses significantly less memory, as promised. https://github.com/pytorch/pytorch/pull/129905 I'll be following up this issue with another tracker for ways Adafactor can be improved + what is missing in terms of support."
452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Fix mps to cpu copy with storage offset)ï¼Œ å†…å®¹æ˜¯ (  CC([MPS] Fix sort with empty tensor.)  CC([MPS] Fix nll_loss with default ignore_index)  CC([MPS] Fix mps to cpu copy with storage offset) Fix CC(Target indices are ignored when writing to a CPU tensor from an MPS tensor))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MPS] Fix mps to cpu copy with storage offset,  CC([MPS] Fix sort with empty tensor.)  CC([MPS] Fix nll_loss with default ignore_index)  CC([MPS] Fix mps to cpu copy with storage offset) Fix CC(Target indices are ignored when writing to a CPU tensor from an MPS tensor),2023-09-18T23:18:36Z,open source Merged release notes: mps ciflow/mps,closed,0,0,https://github.com/pytorch/pytorch/issues/109557
2010,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Mismatch between PyTorch and onnxruntime when converting TransformerEncoder to onnx)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I convert a model containing TransformerEncoder, I notice that the outputs I get in PyTorch and onnxruntimes are different.    Versions PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.9.16 (main, Mar  1 2023, 18:30:21) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Laptop GPU Nvidia driver version: 532.09 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Revision= Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] pytorchlightning==1.9.4 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchmetrics==0.11.3 [pip3] torchvision==0.14.1 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.8.0              h09e9e62_11    condaforge [conda] mkl                       2021.4.0           haa95532_640   [conda] mklservice               2.4.0            py39h2bbff1b_0   [conda] mkl_fft                   1.3.1            py39h277e83a_0   [conda] mkl_random                1.2.2            py39hf11a4ad_0   [conda] numpy                     1.23.5           py39h3b20f71_0   [conda] numpybase                1.23.5           py39h4da318b_0   [conda] pytorch                   1.13.1          py3.9_cuda11.6_cudnn8_0    pytorch [conda] pytorchcuda              11.6   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Mismatch between PyTorch and onnxruntime when converting TransformerEncoder to onnx," ğŸ› Describe the bug When I convert a model containing TransformerEncoder, I notice that the outputs I get in PyTorch and onnxruntimes are different.    Versions PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Microsoft Windows 10 Enterprise GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.9.16 (main, Mar  1 2023, 18:30:21) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.19045SP0 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Laptop GPU Nvidia driver version: 532.09 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Revision= Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] pytorchlightning==1.9.4 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchmetrics==0.11.3 [pip3] torchvision==0.14.1 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.8.0              h09e9e62_11    condaforge [conda] mkl                       2021.4.0           haa95532_640   [conda] mklservice               2.4.0            py39h2bbff1b_0   [conda] mkl_fft                   1.3.1            py39h277e83a_0   [conda] mkl_random                1.2.2            py39hf11a4ad_0   [conda] numpy                     1.23.5           py39h3b20f71_0   [conda] numpybase                1.23.5           py39h4da318b_0   [conda] pytorch                   1.13.1          py3.9_cuda11.6_cudnn8_0    pytorch [conda] pytorchcuda              11.6   ",2023-09-18T18:45:15Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/109532,I was missing `model.eval()`
666,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] supports QLora finetuning)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Currently FSDP is rejecting tensor parameters with dtype unit8. is_floating_point() only allows one of the (torch.float64, torch.float32, torch.float16, and torch.bfloat16)    Since in Qlora setup, the base model is loaded as unit8 dtype. This makes qlora + fsdp imcompatible.   Alternatives I have considered simply removed this dtype check but concerned about unknown impact.   Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,[FSDP] supports QLora finetuning," ğŸš€ The feature, motivation and pitch Currently FSDP is rejecting tensor parameters with dtype unit8. is_floating_point() only allows one of the (torch.float64, torch.float32, torch.float16, and torch.bfloat16)    Since in Qlora setup, the base model is loaded as unit8 dtype. This makes qlora + fsdp imcompatible.   Alternatives I have considered simply removed this dtype check but concerned about unknown impact.   Additional context _No response_ ",2023-09-16T17:15:17Z,feature triaged module: fsdp,open,3,1,https://github.com/pytorch/pytorch/issues/109440,I don't think this should be impossible. But we may want to consider what the longterm comm compression story for int dtype would be.
678,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Build error in third_party/ideep)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug This issue occurs when attempting to build PyTorch from the source code at commit aed9bee. Interestingly, it was not encountered when building PyTorch from the source at commit 2764ead. The root cause of the issue is currently unclear. **Steps to reproduce:** To reproduce this issue, follow these steps:  **Console Output:** Here is the console output showing the encountered errors during the build process:   Versions PyTorch main branch. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Build error in third_party/ideep," ğŸ› Describe the bug This issue occurs when attempting to build PyTorch from the source code at commit aed9bee. Interestingly, it was not encountered when building PyTorch from the source at commit 2764ead. The root cause of the issue is currently unclear. **Steps to reproduce:** To reproduce this issue, follow these steps:  **Console Output:** Here is the console output showing the encountered errors during the build process:   Versions PyTorch main branch. ",2023-09-15T19:06:52Z,module: cpu triaged module: third_party,closed,0,1,https://github.com/pytorch/pytorch/issues/109397,The issue is solved removing the `pytorchinstall` folder. 
1259,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make Fx Generating Incorrect Graph For GPTQ model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I'm trying to generate the fx graph for the Falcon7bGPTQ model but the graph generated is not correct. If we run the fx graph with the inputs then the results generated from the fx graph and from directly running the model are different. Also, if we run the same fx graph again it would generate different results and after 23 executions the fx graph will generate NANs while the PyTorch execution would give the same results every time. To reproduce the issue, run the following code:  To run this code you would also need to install:  If you see an error like this:  Then change the`value_layer_ ` to `value_layer_ .to(torch.float32)` to here: https://huggingface.co/TheBloke/falcon7binstructGPTQ/blob/main/modelling_RW.pyL280. You will find this file at the location: `~/.cache/huggingface/modules/transformers_modules/TheBloke/falcon7binstructGPTQ/d6ce55f4e840bbbd596d1a65f64888f0a3c3326b/modelling_RW.py` After these changes and installation, you should be able to reproduce the issue.  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Make Fx Generating Incorrect Graph For GPTQ model," ğŸ› Describe the bug Hi, I'm trying to generate the fx graph for the Falcon7bGPTQ model but the graph generated is not correct. If we run the fx graph with the inputs then the results generated from the fx graph and from directly running the model are different. Also, if we run the same fx graph again it would generate different results and after 23 executions the fx graph will generate NANs while the PyTorch execution would give the same results every time. To reproduce the issue, run the following code:  To run this code you would also need to install:  If you see an error like this:  Then change the`value_layer_ ` to `value_layer_ .to(torch.float32)` to here: https://huggingface.co/TheBloke/falcon7binstructGPTQ/blob/main/modelling_RW.pyL280. You will find this file at the location: `~/.cache/huggingface/modules/transformers_modules/TheBloke/falcon7binstructGPTQ/d6ce55f4e840bbbd596d1a65f64888f0a3c3326b/modelling_RW.py` After these changes and installation, you should be able to reproduce the issue.  Versions  ",2023-09-15T16:48:43Z,triaged module: fx module: ProxyTensor,open,0,4,https://github.com/pytorch/pytorch/issues/109386,Can you try exporting it with torch.export.export and see how that works?,> Can you try exporting it with torch.export.export and see how that works? It throws this error: ,"wow, what an exciting error message lol","make_fx isn't guaranteed to be sound, there are plenty of ways to cause silent unsoundness. torch.export, on the other hand, has a much stronger soundness guarantee. If I were in your shoes, I'd see if I could workaround / fix problems until I got it exporting all the way through."
1667,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP crashes when submodule calls method that isn't `forward()`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am getting various runtime errors given an FSDP module that wraps multiple children modules, where in the forward pass, we invoke a submodule's nonforward method. The autowrap policy wraps each submodule separately. The minimal example below should make this more clear: Run with (at least 2 GPUs): `torchrun standalone nnodes 1 nprocpernode 2   This results in the following error message:   Further context: I'm working on a project where we take the patch features from a (frozen) Vision Transformer backbone and transform them into a different latent space where they're used to decode other modalities (e.g., depth).  This gist provides an annotated example that reflects our setup a bit better: https://gist.github.com/siddk/db3e8808bed2a9cb90ae62b5338de68d  **Some other things I tried (to help speed along debugging)  all of this is in the linked Gist**:      **Setting `use_orig_params=True` results in a different error at the same Conv2D call (`RuntimeError: weight should have at least three dimensions`)       **Freezing the ViT** (as required in our original setup) results in yet another error at the Conv2D call (`RuntimeError: GET was unable to find an engine to execute this computation`) Interestingly, if we monkey patch the `vit` instance such that `vit.forward = vit.forward_features` and call `self.vit(imgs)` in `Net.forward()`  **all of these bugs disappear!**   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,FSDP crashes when submodule calls method that isn't `forward()`," ğŸ› Describe the bug I am getting various runtime errors given an FSDP module that wraps multiple children modules, where in the forward pass, we invoke a submodule's nonforward method. The autowrap policy wraps each submodule separately. The minimal example below should make this more clear: Run with (at least 2 GPUs): `torchrun standalone nnodes 1 nprocpernode 2   This results in the following error message:   Further context: I'm working on a project where we take the patch features from a (frozen) Vision Transformer backbone and transform them into a different latent space where they're used to decode other modalities (e.g., depth).  This gist provides an annotated example that reflects our setup a bit better: https://gist.github.com/siddk/db3e8808bed2a9cb90ae62b5338de68d  **Some other things I tried (to help speed along debugging)  all of this is in the linked Gist**:      **Setting `use_orig_params=True` results in a different error at the same Conv2D call (`RuntimeError: weight should have at least three dimensions`)       **Freezing the ViT** (as required in our original setup) results in yet another error at the Conv2D call (`RuntimeError: GET was unable to find an engine to execute this computation`) Interestingly, if we monkey patch the `vit` instance such that `vit.forward = vit.forward_features` and call `self.vit(imgs)` in `Net.forward()`  **all of these bugs disappear!**   Versions  ",2023-09-15T16:31:52Z,triaged module: fsdp,open,2,3,https://github.com/pytorch/pytorch/issues/109385,"Hi . This is a known limitation of FSDP. Our design relies on `nn.Module.forward()` to designate the compute for which FSDP should allgather parameters. This issue has shown up previously for HuggingFace's `generate()` methods. If you have a way to workaround this for now (e.g. monkey patching), then that would be the shortest path for now.",Awesome  thanks ; I'm lucky in that I found the monkey patching thing to work just as I was writing up the minimal example for the bug report... would be super great to add this to the docs somewhere so others don't fall into the same trap!,Facing the same problem today. Thanks for the open issue and discussion.
1471,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(UnsupportedOperatorError: Exporting the operator 'aten::lgamma' to ONNX opset version 18 is not supported.)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Unable to export to ONNX a model that uses the operation 'aten::lgamma'.  Alternatives I have tried adding the operation to symbolic_opsetxx.py as per this guide, and see [this issue]  CC(UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX is not supported.) but it won't work.  Additional context I have tried  1.  `operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,` can load on Netron tmodule_model.onnx.zip but > Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from onnx_model.onnx failed:Fatal error: org.pytorch.aten:ATen(1) is not a registered function/op 2. And this  but  > /usr/local/lib/python3.10/distpackages/torch/onnx/_internal/jit_utils.py in onnxscript_op(self, onnx_fn, outputs, *raw_args, **kwargs) > 		    140          NOTE(titaiwang): This is using class attributes, and it needs to be updated > 		    141          if onnxscript makes any change on these. > 		> 142         symbolic_name = f""{onnx_fn.opset.domain}::{onnx_fn.opname}"" > 		    143         opset_version = onnx_fn.opset.version > 		    144  > 		 > 		AttributeError: 'OnnxFunction' object has no attribute 'opname' )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,UnsupportedOperatorError: Exporting the operator 'aten::lgamma' to ONNX opset version 18 is not supported.," ğŸš€ The feature, motivation and pitch Unable to export to ONNX a model that uses the operation 'aten::lgamma'.  Alternatives I have tried adding the operation to symbolic_opsetxx.py as per this guide, and see [this issue]  CC(UnsupportedOperatorError: Exporting the operator 'aten::unflatten' to ONNX is not supported.) but it won't work.  Additional context I have tried  1.  `operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK,` can load on Netron tmodule_model.onnx.zip but > Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from onnx_model.onnx failed:Fatal error: org.pytorch.aten:ATen(1) is not a registered function/op 2. And this  but  > /usr/local/lib/python3.10/distpackages/torch/onnx/_internal/jit_utils.py in onnxscript_op(self, onnx_fn, outputs, *raw_args, **kwargs) > 		    140          NOTE(titaiwang): This is using class attributes, and it needs to be updated > 		    141          if onnxscript makes any change on these. > 		> 142         symbolic_name = f""{onnx_fn.opset.domain}::{onnx_fn.opname}"" > 		    143         opset_version = onnx_fn.opset.version > 		    144  > 		 > 		AttributeError: 'OnnxFunction' object has no attribute 'opname' ",2023-09-15T12:52:30Z,module: onnx triaged,closed,0,7,https://github.com/pytorch/pytorch/issues/109369, looks like we removed opname or something? Do we need to add it back for bc?, thanks for testing different solutions. Could you share your PyTorch version?,To unblock you may do ,>  thanks for testing different solutions. Could you share your PyTorch version? ,The pytorch version is too old to be compatible with your onnxscript version: https://github.com/pytorch/pytorch/blob/f3d14018430b312ac80d1a829b067f27a719fda6/torch/onnx/_internal/jit_utils.pyL143 It's working with nightly.,We will be fixing this with https://github.com/microsoft/onnxscript/pull/1062. You should be able to get the updated version with `pip install upgrade onnxscript` by the end of tomorrow.,"have you slovedï¼Ÿ I meet the same problemï¼Œ my onnx.model have generated , i want to verify that how it works code like: model_path = 'simple_bev.onnx' print(""begin inferenceSession"") session = ort.InferenceSession(model_path) erro: onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from simple_bev.onnx failed:Fatal error: org.pytorch.aten:instance_norm(1) is not a registered function/op"
716,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] Fix triton compiler error in multilayer any)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] Fix triton compiler error in multilayer any) Fixes CC(Failure using compile for HF transformers `google/flanul2` and `google/flant5xl` when batch size is not a power of 2) When we have a split reduction and the tensor is not an even multiple of the split size, we use `ops.masked` to pad to an even multiple. In the case here we generated:  which implicitly promotes our boolean value to `int32`. The fix is to give the default value the same dtype as `result`. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[inductor] Fix triton compiler error in multilayer any,"  CC([inductor] Fix triton compiler error in multilayer any) Fixes CC(Failure using compile for HF transformers `google/flanul2` and `google/flant5xl` when batch size is not a power of 2) When we have a split reduction and the tensor is not an even multiple of the split size, we use `ops.masked` to pad to an even multiple. In the case here we generated:  which implicitly promotes our boolean value to `int32`. The fix is to give the default value the same dtype as `result`. ",2023-09-14T22:16:05Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor release notes: inductor,closed,0,16,https://github.com/pytorch/pytorch/issues/109325, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/peterbell10/612/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/109325`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (aot_eager_torchbench, 1, 1, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/peterbell10/612/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/109325`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge i," Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 2, linux.2xlarge) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," drci (Please ignore this, I's testing Dr.CI)"
1187,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(aten::squeeze exported to ONNX as an `If` node)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I am exporting to ONNX llama from transformers implementation, and don't understand why `If` nodes are inserted for the export of this `squeeze` operator, where the first dims are always of shape `1` and `1`: https://github.com/huggingface/transformers/blob/7c63e6fc8c34dcf8b0121eaee776f41ccf3b1137/src/transformers/models/llama/modeling_llama.pyL182 It appears we are going into this path: https://github.com/pytorch/pytorch/blob/b6a1d3fb97ca8eeccf15a4c495fdd1af4b197f88/torch/onnx/symbolic_opset11.pyL937 because the symbolic helper `_get_tensor_sizes` give us here `x_type.varyingSizes() = [None, None, None, None]` . Is there a way to hint that the shapes are constant hardcoded to `1`? The intermediate captured graph is (the aten::sub is not in the original code base  added for debugging):  A workaround is to use `cos = cos[0, 0]`  wondering if there is anything better. Thank you!    Versions torch 2.0.1, opset_version = 12)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,aten::squeeze exported to ONNX as an `If` node," ğŸ› Describe the bug Hi, I am exporting to ONNX llama from transformers implementation, and don't understand why `If` nodes are inserted for the export of this `squeeze` operator, where the first dims are always of shape `1` and `1`: https://github.com/huggingface/transformers/blob/7c63e6fc8c34dcf8b0121eaee776f41ccf3b1137/src/transformers/models/llama/modeling_llama.pyL182 It appears we are going into this path: https://github.com/pytorch/pytorch/blob/b6a1d3fb97ca8eeccf15a4c495fdd1af4b197f88/torch/onnx/symbolic_opset11.pyL937 because the symbolic helper `_get_tensor_sizes` give us here `x_type.varyingSizes() = [None, None, None, None]` . Is there a way to hint that the shapes are constant hardcoded to `1`? The intermediate captured graph is (the aten::sub is not in the original code base  added for debugging):  A workaround is to use `cos = cos[0, 0]`  wondering if there is anything better. Thank you!    Versions torch 2.0.1, opset_version = 12",2023-09-14T12:56:02Z,module: onnx triaged,open,2,4,https://github.com/pytorch/pytorch/issues/109292,probably related  CC(Export to ONNX of nop-squeeze errors out in ONNXRT)issuecomment628223303,I wonder if the `.setType()` method helps here?, all shapes are static until you say otherwise using `input_names` and `dynamic_shapes` arguments. Do you have a full repro we could try? does it require a specific transformers version? You could also try the new ONNX exporter and let us know if that works for you: quick torch.onnx.dynamo_export API tutorial. We have exported llama v2 with this API on our CI and it works fine,any update?
1124,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] scale down RBLOCK for occupancy)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] scale down RBLOCK for occupancy) For large reduction (with large xnumel and rnumel), we potentially need run large number of thread blocks. Occupancy matters here since with larger occupancy we can run more blocks on each SM and we may need less number of waves to run the entire kernel on the GPU.  Number of registers used by each thread can limit the occupancy. For A100, it's safe to say that register usage does not limit occupancy only if each thread use <= 32 registers. This PR leverage this observation and reduce RBLOCK (thus reduce registers used by each thread) if thread usage limit occupancy for large reduction. The scenario mentioned can happen for the softmax kernel used in transformers. Here are some results get from devgpu:  PLBartForCausalLM we improve from 1.88x (58.7ms) to 2.00x (55.82ms)  TrOCRForCausalLM we improve from 1.45x (92.9ms) to 1.51x (89.12ms) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[inductor] scale down RBLOCK for occupancy,"  CC([inductor] scale down RBLOCK for occupancy) For large reduction (with large xnumel and rnumel), we potentially need run large number of thread blocks. Occupancy matters here since with larger occupancy we can run more blocks on each SM and we may need less number of waves to run the entire kernel on the GPU.  Number of registers used by each thread can limit the occupancy. For A100, it's safe to say that register usage does not limit occupancy only if each thread use <= 32 registers. This PR leverage this observation and reduce RBLOCK (thus reduce registers used by each thread) if thread usage limit occupancy for large reduction. The scenario mentioned can happen for the softmax kernel used in transformers. Here are some results get from devgpu:  PLBartForCausalLM we improve from 1.88x (58.7ms) to 2.00x (55.82ms)  TrOCRForCausalLM we improve from 1.45x (92.9ms) to 1.51x (89.12ms) ",2023-09-14T06:27:38Z,Merged module: inductor ciflow/inductor,closed,4,8,https://github.com/pytorch/pytorch/issues/109275,Very nice :),Here is a perf test link  HF 1.80x > 1.82x .  TB 1.67x > 1.68x. It's not like noise since I see      hf_GPT2_large 1.75x > 1.88x     BERT_pytorch: 3.04x > 3.11x     drq: 1.48x > 1.53x  TIMM 1.69x > 1.70x . This can be noise. Compilation time increases a bit  HF 83s > 87s  TB 67s > 68s  TIMM 109s > 111s I hope we can trade the compilation time with some perf wins.,Why would there be compilation time increases? Can we get runtime improvements without the compilation time increase?,"> Why would there be compilation time increases? Can we get runtime improvements without the compilation time increase? That's because we generate extra configs to compile and benchmark. We could skip benchmarking an old config if we reduce its RBLOCK, but  that may not change the overall compilation time much since I think compared to compilation, benchmarking is not that slow  that may cause bad perf for some models. The changed config is not guaranteed to be better than the old config in every case.  I discussed with Elias offline the idea to make the parallel compilation tasks more finergrain.  Currently each tasks compiles all configs for a kernel. But maybe let each task compiles a single config can speedup things. Hopefully I can trade some compilation time here","Hmm, I realized the extra configs are all get compiled by the main process (not any of the child process for parallel compiling). The reason is the parallel compiling subprocess don't have CUDA initialized. We can not load the cubin (in triton) so the number of registers are not available. Not sure if there is anyway to workaround.","> The reason is the parallel compiling subprocess don't have CUDA initialized. We can not load the cubin (in triton) so the number of registers are not available. In theory there is one way to work around:  with 'v' ptxas will report the number of registers used.  But in inductor stack, ptxas is called by triton. We'll need to change triton to make numusedregisters information available without loading the cubin."," merge f ""test failure not related"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
414,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] Enable mypy checking for torch/_inductor/bounds.py)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] Enable mypy checking for torch/_inductor/bounds.py) Summary: Add type hints and enable mypy checking for torch/_inductor/bounds.py Test Plan: lintrunner )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,[inductor] Enable mypy checking for torch/_inductor/bounds.py,  CC([inductor] Enable mypy checking for torch/_inductor/bounds.py) Summary: Add type hints and enable mypy checking for torch/_inductor/bounds.py Test Plan: lintrunner ,2023-09-14T02:29:12Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/109271, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 2, 3, macosm112), trunk / winvs2019cpupy3 / test (default, 3, 3, windows.4xlarge.nonephemeral) Details for Dev Infra team Raised by workflow job ","For the test failures, I see:  > ModuleNotFoundError: No module named 'sympy'  , any idea what that's about?","I think that error is not there anymore, so let's try to merge this again", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1768,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ONNX exporter issue: fails to add conversions exporting T5 Transformer model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug To reproduce, run the following script:   Observe result:  Resulting ONNX does fail in ORT if attempted. Expected behavior Working ONNX file should be generated. The problem is that some arguments to transformer functions are coming in as float that should have been long integers. Torch eager and TorchScript are doing implicit conversion on the fly, while ONNX exporter does not add explicit casts, and that code fails later in ORT because ONNX does not have implicit conversion mechanism.   Versions PyTorch version: 2.1.0a0+4136153 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.24.1 Libc version: glibc2.35 Python version: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.046genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A5000 .... Versions of relevant libraries: [pip3] k2==1.24.3.dev20230725+cuda12.1.torch2.1.0a0 [pip3] numpy==1.22.2 [pip3] pytorchlightning==1.9.0 [pip3] pytorchquantization==2.1.2 [pip3] torch==2.1.0a0+4136153 [pip3] torchcluster==1.6.1 [pip3] torchgeometric==2.3.1 [pip3] torchsparse==0.6.17 [pip3] torchsplineconv==1.2.2 [pip3] torchtensorrt==1.5.0.dev0 > pip show onnxruntimegpu Name: onnxruntimegpu Version: 1.15.0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,ONNX exporter issue: fails to add conversions exporting T5 Transformer model," ğŸ› Describe the bug To reproduce, run the following script:   Observe result:  Resulting ONNX does fail in ORT if attempted. Expected behavior Working ONNX file should be generated. The problem is that some arguments to transformer functions are coming in as float that should have been long integers. Torch eager and TorchScript are doing implicit conversion on the fly, while ONNX exporter does not add explicit casts, and that code fails later in ORT because ONNX does not have implicit conversion mechanism.   Versions PyTorch version: 2.1.0a0+4136153 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.24.1 Libc version: glibc2.35 Python version: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.046genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA RTX A5000 .... Versions of relevant libraries: [pip3] k2==1.24.3.dev20230725+cuda12.1.torch2.1.0a0 [pip3] numpy==1.22.2 [pip3] pytorchlightning==1.9.0 [pip3] pytorchquantization==2.1.2 [pip3] torch==2.1.0a0+4136153 [pip3] torchcluster==1.6.1 [pip3] torchgeometric==2.3.1 [pip3] torchsparse==0.6.17 [pip3] torchsplineconv==1.2.2 [pip3] torchtensorrt==1.5.0.dev0 > pip show onnxruntimegpu Name: onnxruntimegpu Version: 1.15.0",2023-09-13T19:06:32Z,module: onnx triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/109235,I got the same issue when trying to export mask2former from hugging face. With torch version 2.1.0 one of cast operation got float and raises error. When downgrade to torch 2.0 everything works fine. ,"hey   , Going to close out this issue as stale  it's been a while. Feel free to try our new `torch.onnx.dynamo_export()` api, and maybe that will help if you're still struggling with export. If not, feel free to reopen the issue / create a new one  we'll get to it this time :) "
580,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failure using compile for HF transformers `google/flan-ul2` and `google/flan-t5-xl` when batch size is not a power of 2)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The error is reproduced with the following snippet:  When the batch size (first dim of `toks`) is a power of 2, everything works. However if it is not (as in the above example), we get the error below.   Error logs   Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Failure using compile for HF transformers `google/flan-ul2` and `google/flan-t5-xl` when batch size is not a power of 2," ğŸ› Describe the bug The error is reproduced with the following snippet:  When the batch size (first dim of `toks`) is a power of 2, everything works. However if it is not (as in the above example), we get the error below.   Error logs   Minified repro _No response_  Versions  ",2023-09-13T12:57:16Z,triaged oncall: pt2 module: inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/109196,"One additional info: this error is not observed using nightly `2.1.0.dev20230730+cu118`, so it is something that crept in fairly recently. ",Does this work without compile? , can you post the contents of  `/tmp/torchinductor_1000790000/e6/ce6z47qglnn53vil2auo2wljkqq6g5ttulum3pbujkkqzkthpnhd.py`,"Sorry for late reply, haven't had much time last few weeks. Thank you for looking into this.  > Does this work without compile?   yes I just doublechecked > can you post the contents of /tmp/torchinductor_1000790000/e6/ce6z47qglnn53vil2auo2wljkqq6g5ttulum3pbujkkqzkthpnhd.py  it's rather huge so I attached it:  cbn6nqtdevf2ioumzjuajl3nj7m4f2lyqicavemkesd4ol5alpqg.py.zip"
2039,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DDP - ""No backend type associated with device type cpu"" with new Model Phi 1.5 despite everything loaded on GPUs)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When training a new Model PHI 1.5 with Transformers via accelerate/axolotl, I get the following error `No backend type associated with device type cpu`. (I am running on 2 RTX 4090s without P2P). Training with only 1 GPU runs fine and training for Llamabased models also runs fine with the same setup and accelerate. Debugging shows, that `DistributedDataParallel` has:  Full stacktrace:   Versions Collecting environment information... /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          AuthenticAMD Model name:                         AMD Ryzen 9 7950X 16Core Processor CPU family:                         25 Model:                              97 Thread(s) per core:                 2 Core(s) per socket:                 16 Socket(s):                          1 Stepping:                           2 BogoMIPS:                           8983.47 Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"DDP - ""No backend type associated with device type cpu"" with new Model Phi 1.5 despite everything loaded on GPUs"," ğŸ› Describe the bug When training a new Model PHI 1.5 with Transformers via accelerate/axolotl, I get the following error `No backend type associated with device type cpu`. (I am running on 2 RTX 4090s without P2P). Training with only 1 GPU runs fine and training for Llamabased models also runs fine with the same setup and accelerate. Debugging shows, that `DistributedDataParallel` has:  Full stacktrace:   Versions Collecting environment information... /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                       x86_64 CPU opmode(s):                     32bit, 64bit Address sizes:                      48 bits physical, 48 bits virtual Byte Order:                         Little Endian CPU(s):                             32 Online CPU(s) list:                031 Vendor ID:                          AuthenticAMD Model name:                         AMD Ryzen 9 7950X 16Core Processor CPU family:                         25 Model:                              97 Thread(s) per core:                 2 Core(s) per socket:                 16 Socket(s):                          1 Stepping:                           2 BogoMIPS:                           8983.47 Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl",2023-09-12T12:14:58Z,oncall: distributed,open,0,2,https://github.com/pytorch/pytorch/issues/109103," It seems that there are some buffers reside on CPU, can you check the device types of all the buffers?",">  It seems that there are some buffers reside on CPU, can you check the device types of all the buffers? Can you tell me how to do that? I checked everything in the Debugger and it was set to Cuda. However probably you are right  multiGPU training works now with Deepspeed Zero2, and even when disabling all CPU Offloading in the config, the GPU / CPU utilization looks as if there is some offloading happening. Is it possible that there is something in the custom code (see here ) that triggers ""forced"" CPU offloading? (sorry, I am still new to this area and distributed training..)"
802,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP + PEFT Prompt Tuning Issue)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug In PEFT like prompt tuning, it's common to insert trainable parameters into an LLM while keeping the rest of the model frozen. With FSDP, I'm trying to do the following:  with strict=False, I'd expect it to ignore the extra embedding I added, and I can just log a warning that it's not initialized (its not expected to be initialized). With local module this works fine, but with FSDP it hits a crash: P824863048 Maybe a fix is to ensure that if `strict=False`, the pre/post load hooks don't crash on missing param FQN issues and instead log warnings.  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",peft,FSDP + PEFT Prompt Tuning Issue," ğŸ› Describe the bug In PEFT like prompt tuning, it's common to insert trainable parameters into an LLM while keeping the rest of the model frozen. With FSDP, I'm trying to do the following:  with strict=False, I'd expect it to ignore the extra embedding I added, and I can just log a warning that it's not initialized (its not expected to be initialized). With local module this works fine, but with FSDP it hits a crash: P824863048 Maybe a fix is to ensure that if `strict=False`, the pre/post load hooks don't crash on missing param FQN issues and instead log warnings.  Versions main ",2023-09-11T23:26:00Z,oncall: distributed triaged module: fsdp,closed,1,1,https://github.com/pytorch/pytorch/issues/109077,https://github.com/pytorch/pytorch/pull/109116 fixes the issue. Thanks varma for implementing the fix.
525,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reland 2: Add PyObject preservation for UntypedStorage)ï¼Œ å†…å®¹æ˜¯ (Relands CC(Reland: Add PyObject preservation for UntypedStorage) after it was reverted. This PR makes the new `ignore_hermetic_tls` argument of `check_pyobj` optional to avoid causing a compilation error in torchdistx Part of CC(PyObject preservation and resurrection for `StorageImpl`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Reland 2: Add PyObject preservation for UntypedStorage,Relands CC(Reland: Add PyObject preservation for UntypedStorage) after it was reverted. This PR makes the new `ignore_hermetic_tls` argument of `check_pyobj` optional to avoid causing a compilation error in torchdistx Part of CC(PyObject preservation and resurrection for `StorageImpl`) ,2023-09-11T18:22:11Z,module: internals open source Merged Reverted ciflow/trunk release notes: python_frontend ciflow/periodic,closed,0,11,https://github.com/pytorch/pytorch/issues/109039, merge, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m 'Sorry for reverting your change but it is failing linter job in trunk, probably due to a landrace' c landrace", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to   Raised by https://github.com/pytorch/pytorch/actions/runs/6162982433, merge i," Merge started Your change will be merged while ignoring the following 4 checks: pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 3, 5, linux.g5.4xlarge.nvidia.gpu, unstable), pull / linuxfocalcuda12.1py3.10gcc9sm86 / test (default, 5, 5, linux.g5.4xlarge.nvidia.gpu, unstable), periodic / linuxfocalcuda11.8py3.9gcc9 / test (multigpu, 1, 1, linux.g5.12xlarge.nvidia.gpu, unstable), periodic / macos12py3x8664 / test (default, 2, 4, macos12) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
350,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch._dynamo.exc.Unsupported: call_method ListVariable() index [ConstantVariable(str)] {})ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  gives  This showed up in torchrec_dlrm   Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,torch._dynamo.exc.Unsupported: call_method ListVariable() index [ConstantVariable(str)] {}, ğŸ› Describe the bug  gives  This showed up in torchrec_dlrm   Versions main ,2023-09-11T16:48:09Z,good first issue triaged oncall: pt2 module: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/109031,did you try upgrading your pytorch version in your local machine?... pip install upgrade torch let me know if that helps,"I think we need to add the index case after the `__contains__` one, in   `torch/_dynamo/variables/lists.py` Just need to check options I guess, it seems easy enough, I can work on that", sgtm lmk if you need help
1621,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(functorch: fallthrough on calls to custom size/stride/storage_offset calls)ï¼Œ å†…å®¹æ˜¯ (The problem (that  pointed out) is that functorch assumes that when it create a TensorImpl (like `TensorWrapper`, code), it doesn't reenter the dispatcher. However, if the inner tensor that we hold is a tensor subclass with custom size/strides, then calls like `sym_storage_offset()` get plumbed to `__torch_dispatch__` as `torch.ops.aten.sym_storage_offset.default`, which is a real op registered to the dispatcher (here). , let me know if you think there's a better alternative  in this PR, I just manually audited the ops in that file, and registered fallthroughs to them for the functorch keys (which normally run backend fallbacks).   CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(_return_and_correct_aliasing: fix for schemas with mutable tensor in kwargs)  CC(Make FunctionalTensor subclass to be more like functorch (interaction with ZeroTensor + Conjugate key))  CC(fix subclass custom sizes dynamic shapes caching)  CC(fix infinite loop with primtorch and .to(meta))  CC(python functionalization: support higher order ops)  CC(custom ops: don't error if autograd input is a tensor subclass)  CC(functorch: fallthrough on calls to custom size/stride/storage_offset calls))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,functorch: fallthrough on calls to custom size/stride/storage_offset calls,"The problem (that  pointed out) is that functorch assumes that when it create a TensorImpl (like `TensorWrapper`, code), it doesn't reenter the dispatcher. However, if the inner tensor that we hold is a tensor subclass with custom size/strides, then calls like `sym_storage_offset()` get plumbed to `__torch_dispatch__` as `torch.ops.aten.sym_storage_offset.default`, which is a real op registered to the dispatcher (here). , let me know if you think there's a better alternative  in this PR, I just manually audited the ops in that file, and registered fallthroughs to them for the functorch keys (which normally run backend fallbacks).   CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(_return_and_correct_aliasing: fix for schemas with mutable tensor in kwargs)  CC(Make FunctionalTensor subclass to be more like functorch (interaction with ZeroTensor + Conjugate key))  CC(fix subclass custom sizes dynamic shapes caching)  CC(fix infinite loop with primtorch and .to(meta))  CC(python functionalization: support higher order ops)  CC(custom ops: don't error if autograd input is a tensor subclass)  CC(functorch: fallthrough on calls to custom size/stride/storage_offset calls)",2023-09-11T14:21:28Z,Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/109024
519,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(GPT2ForSequenceClassification fails accuracy in cpu inference after HF version upgrade)ï¼Œ å†…å®¹æ˜¯ (After upgrading the HF version used in CI (https://github.com/pytorch/pytorch/pull/107400), we see `fail_accuracy` for `GPT2ForSequenceClassification` when running cpu test, https://github.com/pytorch/pytorch/actions/runs/6133028313/job/16644698906 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,GPT2ForSequenceClassification fails accuracy in cpu inference after HF version upgrade,"After upgrading the HF version used in CI (https://github.com/pytorch/pytorch/pull/107400), we see `fail_accuracy` for `GPT2ForSequenceClassification` when running cpu test, https://github.com/pytorch/pytorch/actions/runs/6133028313/job/16644698906 ",2023-09-11T13:15:19Z,module: cpu triaged oncall: pt2,closed,0,12,https://github.com/pytorch/pytorch/issues/109019,"Test on my local system, it looks like pass the accuracy check:  I am using   May I know should I upgrade `transformers` to some specific commit to reproduce this failure?", any comments on how to reproduce the failures?,,"Thanks for the help. I thin I can reproduce this failure now  CMD: `TRANSFORMERS_OFFLINE=1 OMP_NUM_THREADS=1 numactl C 00 membind=0 python benchmarks/dynamo/huggingface.py accuracy float32 dcpu n50 noskip dashboard batchsize 1 threads 1 only GPT2ForSequenceClassification inference freezing timeout 9000 backend=inductor output=/tmp/inductor_single_test_st.csv`  When use `transformer v4.30.2`, it can pass the accuracy check  When use `transformer 6c26faa159b79a42d7fa46cb66e2d21523351987`, the accuracy check will fail.",Further investigation seems this transformer commit: https://github.com/huggingface/transformers/commit/f10452271802573fe6e19442631113c4c23a2c70 (PR: https://github.com/huggingface/transformers/pull/24979) makes some change to model script which causes this failure.,"The root cause for this issue is after the model code changed, there are negative value tensor([1]) used in another tensor's index.  Here is a example to reproduce this failure:    Draft PR: https://github.com/pytorch/pytorch/pull/111118 to fix this issue.",Discussed with . Close PR https://github.com/pytorch/pytorch/pull/111118. And PR https://github.com/pytorch/pytorch/pull/108690 is expected to fix this issue also.,https://github.com/pytorch/pytorch/pull/108690 landed and verified with latest PyTorch master: 4f42edfb6e5b703eec2a14b8933090646702c5a2   may I know is there any thing else we need to follow up to close this issue?,I wonder why didn't CI go from failing accuracy to passing when submitting that PR...,"> CC(Move negative index checking to common.py  Fix issue 97365) landed and verified with latest PyTorch master: 4f42edf >  >  >  >  may I know is there any thing else we need to follow up to close this issue? Let's take this test out of the CI skip list, https://github.com/pytorch/pytorch/blob/247f39f6039deffebb49b3fbe41b29d9e3a6bfb4/benchmarks/dynamo/common.pyL250","> > CC(Move negative index checking to common.py  Fix issue 97365) landed and verified with latest PyTorch master: 4f42edf > >  > >  > >  > >      > >        > >      > >  > >        > >      > >  > >      > >    > >  may I know is there any thing else we need to follow up to close this issue? >  > Let's take this test out of the CI skip list, >  > https://github.com/pytorch/pytorch/blob/247f39f6039deffebb49b3fbe41b29d9e3a6bfb4/benchmarks/dynamo/common.pyL250 PR: https://github.com/pytorch/pytorch/pull/112100 to remove it from CI skip list, please help to take a look  ",https://github.com/pytorch/pytorch/pull/112100 merged to take `GPT2ForSequenceClassification` out of the CI skip list
1975,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add Lambert W function as torch.special.lambertw)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Implement the Lambert W function as a core mathematical function available in pytorch.  Suggest to put this in `torch.special.lambertw` to follow pattern in `scipy.special.lambertw` and `tfp.special.lambertw`.  Lambert W function (wiki, and Corless et al, 1996) is the solution to the equation `x * exp(x) = z`, ie `W(z)=x` with multiple branches (real and complex).  It has many applications in physics & math and skewed/heavytailed distribution modeling; recently this also gained attention in the deep learning literature as a way to add Gaussianization (outlier dampeninig / remove heavy tails) layers (Goerg 2011 & Goerg 2015, Wiese et al, 2019, Li 2023, amongst others).  Adding it here as a core `pytorch.special.lambertw` functionality would open up ability for pytorch developers to implement any advances involving Lambert W functions; currently blocked by lack of implementation compared to other standard frameworks/libraries (Boost C++, TensorFlow, scipy, R, Matlab, Julia): * Boost C++ implementation: https://www.boost.org/doc/libs/1_69_0/libs/math/doc/html/math_toolkit/lambert_w.html * `sklearn` like `gaussianize` Transformer: https://github.com/gregversteeg/gaussianize * `LambertW` R package: https://github.com/gmgeorg/LambertW; based on C++ implementation in `lamW` (https://github.com/cran/lamW/blob/master/src/lambertW.cpp) * TensorFlow implementation of `lambertw`: https://www.tensorflow.org/probability/api_docs/python/tfp/math/lambertw * MATLAB (https://www.mathworks.com/help/symbolic/lambertw.html) * Julia (https://docs.juliahub.com/LambertW/7mpiq/0.4.5/autodocs/) Particularly it would unblock implemented Lambert W x F distributions in `torch.distributions.*` (akin to thei)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add Lambert W function as torch.special.lambertw," ğŸš€ The feature, motivation and pitch Implement the Lambert W function as a core mathematical function available in pytorch.  Suggest to put this in `torch.special.lambertw` to follow pattern in `scipy.special.lambertw` and `tfp.special.lambertw`.  Lambert W function (wiki, and Corless et al, 1996) is the solution to the equation `x * exp(x) = z`, ie `W(z)=x` with multiple branches (real and complex).  It has many applications in physics & math and skewed/heavytailed distribution modeling; recently this also gained attention in the deep learning literature as a way to add Gaussianization (outlier dampeninig / remove heavy tails) layers (Goerg 2011 & Goerg 2015, Wiese et al, 2019, Li 2023, amongst others).  Adding it here as a core `pytorch.special.lambertw` functionality would open up ability for pytorch developers to implement any advances involving Lambert W functions; currently blocked by lack of implementation compared to other standard frameworks/libraries (Boost C++, TensorFlow, scipy, R, Matlab, Julia): * Boost C++ implementation: https://www.boost.org/doc/libs/1_69_0/libs/math/doc/html/math_toolkit/lambert_w.html * `sklearn` like `gaussianize` Transformer: https://github.com/gregversteeg/gaussianize * `LambertW` R package: https://github.com/gmgeorg/LambertW; based on C++ implementation in `lamW` (https://github.com/cran/lamW/blob/master/src/lambertW.cpp) * TensorFlow implementation of `lambertw`: https://www.tensorflow.org/probability/api_docs/python/tfp/math/lambertw * MATLAB (https://www.mathworks.com/help/symbolic/lambertw.html) * Julia (https://docs.juliahub.com/LambertW/7mpiq/0.4.5/autodocs/) Particularly it would unblock implemented Lambert W x F distributions in `torch.distributions.*` (akin to thei",2023-09-09T15:26:22Z,triaged module: special topic: new features,open,1,5,https://github.com/pytorch/pytorch/issues/108948,Any updates or guidance here?," Below is one implementation to get the principal branch with positive inputs. The approximation takes only a few iterations to converge.  It takes some effort to make it a standard activation function.  Reference: Iacono, R., Boyd, J.P. New approximations to the principal realvalued branch of the Lambert Wfunction. Adv Comput Math 43, 1403â€“1436 (2017). https://doi.org/10.1007/s1044401795303  Benchmark against `scipy.special.lambertw`.  Computation time (s):    !benchmark","  Thanks for the reference. I have an implementation already done that has much lower error rates (1e15 compared to 1e8 here and works w/ negative inputs >= 1/e as well), at roughly same runtime (using more accurate approximations than Iacono, R., Boyd, J.P.; see also tensorflow reference)  !image The request here was generally to just add this function as a standard repertoire in torch (like in many other existing libraries/frameworks).  Adding it specifically as an activation function can come as follow up; primarily users would benefit for being able to use the `lambertw` function  just like any other function in `torch`, say, `torch.log` or `torch.exp` . Happy to share the github repo implementation for reference","For reference, made the **torchlambertw** repo public now  https://github.com/gmgeorg/torchlambertw I still think all of the available functionality there is just what's missing today in the core torch library. ","Bump, I think this can be useful"
911,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo] Translate ""value in tensor"" into something more efficient)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug In native CPython, a `value in tensor` tensor must proceed by iterating over tensor using the sequence protocol, testing each element one by one. However, with Dynamo, we can translate this into something more efficient like `torch.any(value == tensor)`, which gives up short circuiting to get a vectorized comparison that also avoids specializing on the numel of tensor. This specialization impedes batch dynamic shapes in  CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0) Because `in` is a single bytecode (`COMPARE_OP`) this should be easy to match against in Dynamo.  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[Dynamo] Translate ""value in tensor"" into something more efficient"," ğŸ› Describe the bug In native CPython, a `value in tensor` tensor must proceed by iterating over tensor using the sequence protocol, testing each element one by one. However, with Dynamo, we can translate this into something more efficient like `torch.any(value == tensor)`, which gives up short circuiting to get a vectorized comparison that also avoids specializing on the numel of tensor. This specialization impedes batch dynamic shapes in  CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0) Because `in` is a single bytecode (`COMPARE_OP`) this should be easy to match against in Dynamo.  Versions main ",2023-09-08T20:54:05Z,good first issue triaged oncall: pt2 module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/108904,I think it's already doing that though. Or am I mistaken here? https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/variables/tensor.pyL633L642,"You're right, maybe I misdiagnosed the specialization in the parent issue ğŸ¤” "
687,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(nanogpt_generate and clip pass regression)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug These models regressed from 9/01 > 9/02. Filing this issue to track / further triage. https://hud.pytorch.org/benchmark/blueberries/inductor_no_cudagraphs?startTime=Fri,%2001%20Sep%202023%2018:26:13%20GMT&stopTime=Fri,%2008%20Sep%202023%2018:26:13%20GMT&granularity=hour&mode=inference&dtype=bfloat16&lBranch=main&lCommit=ffc0c46092efef1f3b51becc8f5770abcec5c23a&rBranch=main&rCommit=591cb776af68e43f41cba1a29b6219341b75d951  Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,nanogpt_generate and clip pass regression," ğŸ› Describe the bug These models regressed from 9/01 > 9/02. Filing this issue to track / further triage. https://hud.pytorch.org/benchmark/blueberries/inductor_no_cudagraphs?startTime=Fri,%2001%20Sep%202023%2018:26:13%20GMT&stopTime=Fri,%2008%20Sep%202023%2018:26:13%20GMT&granularity=hour&mode=inference&dtype=bfloat16&lBranch=main&lCommit=ffc0c46092efef1f3b51becc8f5770abcec5c23a&rBranch=main&rCommit=591cb776af68e43f41cba1a29b6219341b75d951  Versions master ",2023-09-08T18:28:22Z,triaged module: inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/108884,Seems to have been resolved
389,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Reland] increase clang-tidy coverage in torch/csrc)ï¼Œ å†…å®¹æ˜¯ (Reland  PR CC(increase clangtidy coverage in torch/csrc) since there was a time gap between this PR and other PRs in terms of torch/csrc modifications    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Reland] increase clang-tidy coverage in torch/csrc,Reland  PR CC(increase clangtidy coverage in torch/csrc) since there was a time gap between this PR and other PRs in terms of torch/csrc modifications    ,2023-09-08T16:52:20Z,open source Merged ciflow/trunk release notes: releng,closed,0,17,https://github.com/pytorch/pytorch/issues/108875, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `clang_tidy_coverage4_reland` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout clang_tidy_coverage4_reland && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `clang_tidy_coverage4_reland` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout clang_tidy_coverage4_reland && git pull rebase`)", rebase, merge, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC([Reland] increase clangtidy coverage in torch/csrc), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,  rebase b main, started a rebase job onto refs/remotes/origin/main. Check the current status here,"Successfully rebased `clang_tidy_coverage4_reland` onto `refs/remotes/origin/main`, please pull locally before adding more changes (for example, via `git checkout clang_tidy_coverage4_reland && git pull rebase`)", Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
402,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Validate that storage have enough memory allocated)ï¼Œ å†…å®¹æ˜¯ (Check that storage have enough memory allocated to contain all tensor data (according to its metadata). Fixes CC(Heap buffer overflow with `torch::load` on fuzzy data) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Validate that storage have enough memory allocated,Check that storage have enough memory allocated to contain all tensor data (according to its metadata). Fixes CC(Heap buffer overflow with `torch::load` on fuzzy data) ,2023-09-08T15:42:38Z,oncall: jit triaged open source Stale release notes: mobile,closed,0,2,https://github.com/pytorch/pytorch/issues/108872,"Hi! I tried to fix CC(Heap buffer overflow with `torch::load` on fuzzy data) in this PR. The issue is that malformed tensor could be constructed during deserialization process. The constructed tensor's storage is read from raw binary data, and it have specific size of allocated memory: https://github.com/pytorch/pytorch/blob/d38379f9f1ec95c59d959c89b666fa04d9654b75/caffe2/serialize/inline_container.ccL342 But tensor also have metadata about storage  numel and itemsize which could be inconsistent with storage size. Because actual storage memory allocation size is not used anywhere (only tensor metadata used to access tensor's data) there could be outofbuffer accesses. I tried to validate that tensor storage have enough memory allocated to contain all its elements (numel * itemsize), but it turns out that storage could be of size 0 bytes. While its reasonable for metatensors, its still shows up on other tests (test_save_load_with_saved_traced_inputs). Does these situation with zero bytes allocated for tensor's storage is normal? Or is there more suitable way to validate that storage size corresponds to tensor data?","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1570,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add torch::Library::request_pyimport)ï¼Œ å†…å®¹æ˜¯ (Stack from ghstack:  CC(Add torch::Library::request_pyimport) We want to be able to define custom ops in Python and C++: a user may put things like meta kernels in Python but the backend kernels in C++. When someone is using a custom op from Python, we want to ensure that both the C++ shared library and the Python kernels are loaded to avoid partiallyinitialized custom ops. When someone is using a custom op from C++, we assume they just want to use the C++ kernels. The design we use to prevent partiallyinitialized custom ops in Python is that a C++ torch::Library object can import a python module by calling `request_pyimport`, which will import the module if Python is available. This ensures that when the C++ TORCH_LIBRARY block gets loaded, the Python module will also be imported. Implementation details:  Due to the libtorchlibtorch_python split, we have a PyImportsHandler indirection. PyImportsHandler has a method that imports a module in Python.  When `import torch` happens, we initialize the PyImportsHandler  If a user wants to load a shared library with custom ops that calls `request_pyimport`, they must do so AFTER `import torch` happens. If they do it before, we'll raise an error on `import torch`.  The PyImportsHandler is protected by a mutex: TORCH_LIBRARY static initialization may be multithreaded. Test Plan:  new test)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add torch::Library::request_pyimport,"Stack from ghstack:  CC(Add torch::Library::request_pyimport) We want to be able to define custom ops in Python and C++: a user may put things like meta kernels in Python but the backend kernels in C++. When someone is using a custom op from Python, we want to ensure that both the C++ shared library and the Python kernels are loaded to avoid partiallyinitialized custom ops. When someone is using a custom op from C++, we assume they just want to use the C++ kernels. The design we use to prevent partiallyinitialized custom ops in Python is that a C++ torch::Library object can import a python module by calling `request_pyimport`, which will import the module if Python is available. This ensures that when the C++ TORCH_LIBRARY block gets loaded, the Python module will also be imported. Implementation details:  Due to the libtorchlibtorch_python split, we have a PyImportsHandler indirection. PyImportsHandler has a method that imports a module in Python.  When `import torch` happens, we initialize the PyImportsHandler  If a user wants to load a shared library with custom ops that calls `request_pyimport`, they must do so AFTER `import torch` happens. If they do it before, we'll raise an error on `import torch`.  The PyImportsHandler is protected by a mutex: TORCH_LIBRARY static initialization may be multithreaded. Test Plan:  new test",2023-09-07T15:05:55Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/108771
1461,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SDPA with nested backend: expose a way to avoid recomputing data layout information)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Hi, `torch.nn.functional.scaled_dot_product_attention` has a very nice torch.nested backend that allows to use e.g. flash attention with variable length inputs. In general, this is useful when doing training (finetuning) with padding, or during batched inference. However, profiling SDPA and looking at the code, it happens that some overhead exists for the torch.nested backend, notably https://github.com/pytorch/pytorch/blob/49e964cad6c6f0241e08adb619bec637763888a8/aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cppL228 which computes cumulative sequence length and maximum sequence length, especially this to operation. However, those are usually constants over different layers. !image (with code borrowed from the nice path in  CC(SDPA with nested tensor is not faster than non-nested), here max_seqlen=4096, pad percentage=0.4, batch_size=16, head_dim=128, num_heads=32) As computing those stats only once is recommended for flash attention, it would be neat to be able to indeed compute them only once using PyTorch interface. Related: https://github.com/huggingface/transformers/pull/25598  Alternatives /  Additional context / )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,SDPA with nested backend: expose a way to avoid recomputing data layout information," ğŸš€ The feature, motivation and pitch Hi, `torch.nn.functional.scaled_dot_product_attention` has a very nice torch.nested backend that allows to use e.g. flash attention with variable length inputs. In general, this is useful when doing training (finetuning) with padding, or during batched inference. However, profiling SDPA and looking at the code, it happens that some overhead exists for the torch.nested backend, notably https://github.com/pytorch/pytorch/blob/49e964cad6c6f0241e08adb619bec637763888a8/aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cppL228 which computes cumulative sequence length and maximum sequence length, especially this to operation. However, those are usually constants over different layers. !image (with code borrowed from the nice path in  CC(SDPA with nested tensor is not faster than non-nested), here max_seqlen=4096, pad percentage=0.4, batch_size=16, head_dim=128, num_heads=32) As computing those stats only once is recommended for flash attention, it would be neat to be able to indeed compute them only once using PyTorch interface. Related: https://github.com/huggingface/transformers/pull/25598  Alternatives /  Additional context / ",2023-09-07T14:41:59Z,triaged module: sdpa,open,2,1,https://github.com/pytorch/pytorch/issues/108770,"Hey , thanks for the request! We are aware that the repeated `cumulative_and_max_seq_len` computation is inefficient and are actively working on a solution to this problem. The TL;DR is that we are introducing an alternative layout to nested tensors that would maintain the cumulative / max sequence lengths as an alternative to the general strided nested tensor metadata. The idea is that you can call into SDPA with nested tensors in this layout and the metadata will be passed directly to the underlying flash attention kernel with no conversion computation needed. Would this be sufficient for your use case or do you have other concerns?"
860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor][cpu][amp] model YituTechConvBert MemoryError: std::bad_alloc )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug bash inductor_single_run.sh single inference performance **huggingface YituTechConvBert** amp first static default 0   Versions SW info               SW       Nightly commit       Main commit                       Pytorch       73c794d       4a9c6f1                 Torchbench       /       770d5cf7                 torchaudio       dc83b38       66f661d                 torchtext       c11d758       60bea66                 torchvision       58366ab       a6dea86                 torchdata       1d231d1       757c032                 dynamo_benchmarks       f228c8b       /          )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Inductor][cpu][amp] model YituTechConvBert MemoryError: std::bad_alloc , ğŸ› Describe the bug bash inductor_single_run.sh single inference performance **huggingface YituTechConvBert** amp first static default 0   Versions SW info               SW       Nightly commit       Main commit                       Pytorch       73c794d       4a9c6f1                 Torchbench       /       770d5cf7                 torchaudio       dc83b38       66f661d                 torchtext       c11d758       60bea66                 torchvision       58366ab       a6dea86                 torchdata       1d231d1       757c032                 dynamo_benchmarks       f228c8b       /          ,2023-09-07T10:22:02Z,triaged oncall: pt2,closed,0,5,https://github.com/pytorch/pytorch/issues/108761,"Hi SYD, May I know the difference of `Nightly Commit` and `Main commit`? Which one should I use to reproduce this issue?",I have tested with above commits and can't reproduce this error. Here is my env  and the test log ,try on SPR ? Also you can access docker env via `docker pull ccrregistry.caas.intel.com/pytorch/pt_inductor:2023_09_02_static_default_local`,It looks like a functional issue that the system failed to allocate a memory block. I don't think it will relate to platform difference of SPR or CPX.   Can this issue be reliably reproduced when running the above command you provided?    May it relate to some software version? Such as Jemalloc version and configuration parameters.,"The issue could not been stable reproduced, pass in 0908 nightly, closing "
831,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Support None in fx.args as torchlib inputs)ï¼Œ å†…å®¹æ˜¯ (  CC([ONNX] Support None in fx.args as torchlib inputs) Prior to this PR, if None is returned from intermediate nodes, it will crashes the export because None is not expected to be passed into `_fill_tensor_shape_type`, and raise beartype roar. The function fills in shape and type to TorchScriptTensor according to its info from FX graph. This is discovered after https://github.com/microsoft/onnxscript/pull/1043 is supported. The op specifically generates None in one of its inputs, but the only output from it being consumed is the first one (not None). Reference test from a TorchBench model: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[ONNX] Support None in fx.args as torchlib inputs,"  CC([ONNX] Support None in fx.args as torchlib inputs) Prior to this PR, if None is returned from intermediate nodes, it will crashes the export because None is not expected to be passed into `_fill_tensor_shape_type`, and raise beartype roar. The function fills in shape and type to TorchScriptTensor according to its info from FX graph. This is discovered after https://github.com/microsoft/onnxscript/pull/1043 is supported. The op specifically generates None in one of its inputs, but the only output from it being consumed is the first one (not None). Reference test from a TorchBench model: ",2023-09-06T22:04:53Z,module: onnx open source Merged ciflow/trunk release notes: onnx topic: improvements,closed,0,3,https://github.com/pytorch/pytorch/issues/108708,"> The op specifically generates None in one of its inputs, but the only output from it being consumed is the first one (not None). Do you mean an input or output is None? If output which one?", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
716,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(updated pyinterpreter fix)ï¼Œ å†…å®¹æ˜¯ (  CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(python functionalization: support higher order ops)  CC(updated pyinterpreter fix)  CC(fix subclass custom sizes dynamic shapes caching)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(python functionalization: add helpers, functionalize_sync and mirror_autograd_meta)  CC(Add TorchDispatch version of functionalization))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,updated pyinterpreter fix,"  CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(python functionalization: support higher order ops)  CC(updated pyinterpreter fix)  CC(fix subclass custom sizes dynamic shapes caching)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(python functionalization: add helpers, functionalize_sync and mirror_autograd_meta)  CC(Add TorchDispatch version of functionalization)",2023-09-06T14:30:41Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/108655
1994,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.onnx.export does not trace all outputs for the HF BLOOM model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I try to export HF BLOOM model using `torch.onnx.export` only the first output is traced, the other outputs are ignored. Reproduction:  In this scenario the output of the model is a nested tuple of dozens of tensors (logits + cache) and the cache is missing in the onnx model output. The same code works fine with other models such as Llama or BART. Tested on both stable and nightly.  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230904+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.052genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.128 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 Nvidia driver version: 525.85.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.onnx.export does not trace all outputs for the HF BLOOM model," ğŸ› Describe the bug When I try to export HF BLOOM model using `torch.onnx.export` only the first output is traced, the other outputs are ignored. Reproduction:  In this scenario the output of the model is a nested tuple of dozens of tensors (logits + cache) and the cache is missing in the onnx model output. The same code works fine with other models such as Llama or BART. Tested on both stable and nightly.  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230904+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: Could not collect CMake version: version 3.27.1 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.052genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 12.2.128 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 Nvidia driver version: 525.85.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                    ",2023-09-06T09:05:20Z,module: onnx triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/108640,"Hi  , Please feel free to try exporting bloom with the `torch.onnx.dynamo_export` api  should work! Closing the issue in the meantime, feel free to reopen if there are any issues. "
498,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Back out ""Faster gc_count update for CUDACachingAllocator"")ï¼Œ å†…å®¹æ˜¯ (Summary: Original commit changeset: 1d04ae368fd8 Original Phabricator Diff: D48481557 Test Plan: llm inference service can encounter a segfault underload. it no longer does after backing out the diff. Reviewed By: houseroad Differential Revision: D49003404)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"Back out ""Faster gc_count update for CUDACachingAllocator""",Summary: Original commit changeset: 1d04ae368fd8 Original Phabricator Diff: D48481557 Test Plan: llm inference service can encounter a segfault underload. it no longer does after backing out the diff. Reviewed By: houseroad Differential Revision: D49003404,2023-09-06T07:50:28Z,fb-exported Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/108633,This pull request was **exported** from Phabricator. Differential Revision: D49003404," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
498,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Back out ""Faster gc_count update for CUDACachingAllocator"")ï¼Œ å†…å®¹æ˜¯ (Summary: Original commit changeset: 1d04ae368fd8 Original Phabricator Diff: D48481557 Test Plan: llm inference service can encounter a segfault underload. it no longer does after backing out the diff. Reviewed By: houseroad Differential Revision: D49003404)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"Back out ""Faster gc_count update for CUDACachingAllocator""",Summary: Original commit changeset: 1d04ae368fd8 Original Phabricator Diff: D48481557 Test Plan: llm inference service can encounter a segfault underload. it no longer does after backing out the diff. Reviewed By: houseroad Differential Revision: D49003404,2023-09-06T07:25:54Z,fb-exported,closed,0,3,https://github.com/pytorch/pytorch/issues/108631," :x:  login:  . The commit (0363f7606246647f07c3570df83ed44968927fa1) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D49003404," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
821,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add TORCH_API to Expose RPC module functions for RPC module device extension)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER At present, our team refer to the existing rpc backend tensorpipe backend, and implement our own rpc communication backend in our extension package. We found that these functions are not exposed during development, and direct use will cause our extension package to appear undefined symbol problem. Add the TORCH_API macro to the functions required to implement the custom tensorpipe agent in the rpc module to expose them to developersï¼Œat the same time, we think this risk is very controllable and hope it can be merged into the version 2.1.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Add TORCH_API to Expose RPC module functions for RPC module device extension,"Fixes ISSUE_NUMBER At present, our team refer to the existing rpc backend tensorpipe backend, and implement our own rpc communication backend in our extension package. We found that these functions are not exposed during development, and direct use will cause our extension package to appear undefined symbol problem. Add the TORCH_API macro to the functions required to implement the custom tensorpipe agent in the rpc module to expose them to developersï¼Œat the same time, we think this risk is very controllable and hope it can be merged into the version 2.1.",2023-09-06T02:28:25Z,open source release notes: distributed (rpc),closed,0,0,https://github.com/pytorch/pytorch/issues/108617
801,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( Add TORCH_API to expose RPC module functions for RPC module device extension)ï¼Œ å†…å®¹æ˜¯ (At present, I refer to the existing rpc backend tensorpipe backend, and implement our own rpc communication backend in our extension package. We found that these functions are not exposed during development, and direct use will cause our extension package to appear undefined symbol problem. Add the TORCH_API macro to the functions required to implement the custom tensorpipe agent in the rpc module to expose them to developersï¼Œat the same time, we think this risk is very controllable and hope it can be merged into the version 2.1. ,   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent, Add TORCH_API to expose RPC module functions for RPC module device extension,"At present, I refer to the existing rpc backend tensorpipe backend, and implement our own rpc communication backend in our extension package. We found that these functions are not exposed during development, and direct use will cause our extension package to appear undefined symbol problem. Add the TORCH_API macro to the functions required to implement the custom tensorpipe agent in the rpc module to expose them to developersï¼Œat the same time, we think this risk is very controllable and hope it can be merged into the version 2.1. ,   ",2023-09-05T03:29:07Z,triaged open source Merged ciflow/trunk release notes: distributed (rpc),closed,0,4,https://github.com/pytorch/pytorch/issues/108553, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `main` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout main && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
883,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inductor cpp wrapper: fix codegen of positional args with default value)ï¼Œ å†…å®¹æ˜¯ (  CC(Inductor cpp wrapper: fix codegen of positional args with default value) Fixes  CC([inductor][cpu] AssertionError: ordered_kwargs_for_cpp_kernel has to be provided). Cpp wrapper has functionality regression on `llama` and `tnt_s_patch16_224` due to recent support of scaled dot product flash attention in inductor. The schema of this OP is as follows:  For `llama` and `tnt_s_patch16_224`, the OP is called in the below way, where the three positional args with default values are not passed (`float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False`).  This PR fixes the cpp wrapper support for this case. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Inductor cpp wrapper: fix codegen of positional args with default value,"  CC(Inductor cpp wrapper: fix codegen of positional args with default value) Fixes  CC([inductor][cpu] AssertionError: ordered_kwargs_for_cpp_kernel has to be provided). Cpp wrapper has functionality regression on `llama` and `tnt_s_patch16_224` due to recent support of scaled dot product flash attention in inductor. The schema of this OP is as follows:  For `llama` and `tnt_s_patch16_224`, the OP is called in the below way, where the three positional args with default values are not passed (`float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False`).  This PR fixes the cpp wrapper support for this case. ",2023-09-05T03:07:19Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/108552,The CI failures are unrelated to the current PR.,The CI failures are caused by HttpClient timeout and Service unavailable., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1990,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(nn.Transformer has dropout layers that BERT / GPT-2 do not have)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue The docstring of nn.TransformerEncoder reads: ""Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters."" However, `TransformerEncoderLayer` (and `TransformerDecoderLayer`) has a dropout layer in between the two linear layers that come after the attention layers: * https://github.com/pytorch/pytorch/blob/51c2e22/torch/nn/modules/transformer.pyL723 * https://github.com/pytorch/pytorch/blob/51c2e22/torch/nn/modules/transformer.pyL874 BERT does not have this: * https://github.com/googleresearch/bert/blob/master/modeling.pyL872 So the docstring is subtly wrong, at least when planning to use the model for training. For more context, also GPT2 does not have this: * https://github.com/openai/gpt2/blob/master/src/model.pyL118 The original ""Attention is all you need"" has it in Transformer Base v2 and v3, but not v1: * https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.pyL1850 The official TensorFlow tutorial does not have it: * https://www.tensorflow.org/text/tutorials/transformerthe_feed_forward_network But the Annotated Transformer has it: * http://nlp.seas.harvard.edu/annotatedtransformer/ An alternative to changing the docstring would be extending the `TransformerEncoderLayer` implementation to optionally accept a dictionary for `dropout` with entries ""residual"", ""mlp"", ""attention"" for the three types of dropout that are employed in the model. But I don't know how much `nn.Transformer` is used as compared to custom implementations.  Suggest a potential alternative/fix A correct version would be: ""Users can build the BERT (...) model with corresponding parameters, except that BERT does not employ dropout between the feedforward )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,nn.Transformer has dropout layers that BERT / GPT-2 do not have," ğŸ“š The doc issue The docstring of nn.TransformerEncoder reads: ""Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters."" However, `TransformerEncoderLayer` (and `TransformerDecoderLayer`) has a dropout layer in between the two linear layers that come after the attention layers: * https://github.com/pytorch/pytorch/blob/51c2e22/torch/nn/modules/transformer.pyL723 * https://github.com/pytorch/pytorch/blob/51c2e22/torch/nn/modules/transformer.pyL874 BERT does not have this: * https://github.com/googleresearch/bert/blob/master/modeling.pyL872 So the docstring is subtly wrong, at least when planning to use the model for training. For more context, also GPT2 does not have this: * https://github.com/openai/gpt2/blob/master/src/model.pyL118 The original ""Attention is all you need"" has it in Transformer Base v2 and v3, but not v1: * https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.pyL1850 The official TensorFlow tutorial does not have it: * https://www.tensorflow.org/text/tutorials/transformerthe_feed_forward_network But the Annotated Transformer has it: * http://nlp.seas.harvard.edu/annotatedtransformer/ An alternative to changing the docstring would be extending the `TransformerEncoderLayer` implementation to optionally accept a dictionary for `dropout` with entries ""residual"", ""mlp"", ""attention"" for the three types of dropout that are employed in the model. But I don't know how much `nn.Transformer` is used as compared to custom implementations.  Suggest a potential alternative/fix A correct version would be: ""Users can build the BERT (...) model with corresponding parameters, except that BERT does not employ dropout between the feedforward ",2023-09-04T16:01:31Z,module: docs module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/108522
1655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multi-Head Attention: Only require attn_mask if actually needed)ï¼Œ å†…å®¹æ˜¯ (`torch.nn.MultiheadAttention` internally uses `scaled_dot_product_attention()`. For the common case of causal attention, the latter accepts an `is_causal` flag, which computes causal attention inside the kernel without having to compute an attention mask in memory. Still, `torch.nn.MultiheadAttention` and `torch.nn.functional.multi_head_attention_forward()` ask for an attention mask whenever `is_causal=True` is given:  This short PR tightens the check for a missing attention mask so it is not required when it would be set to `None` 11 lines later anyway. Disclaimer: I currently do not have a development setup for PyTorch and will rely on the CI, sorry. As an aside, the docstring of `torch.nn.functional.multi_head_attention_forward()` currently reads: > is_causal: If specified, applies a causal mask as attention mask, and ignores >        attn_mask for computing scaled dot product attention. This suggests that the mask is completely ignored, while it is actually still required when either `need_weights` or `key_padding_mask` is given. This could be fixed either by updating the docstring, or by creating a causal `attn_mask` on the fly when needed, and not ever complaining about a missing mask. The latter would be convenient, but it would hide to the user the opportunity to precompute the mask once and reuse it in the case of fixed sequence lengths or multiple samesize transformer layers.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Multi-Head Attention: Only require attn_mask if actually needed,"`torch.nn.MultiheadAttention` internally uses `scaled_dot_product_attention()`. For the common case of causal attention, the latter accepts an `is_causal` flag, which computes causal attention inside the kernel without having to compute an attention mask in memory. Still, `torch.nn.MultiheadAttention` and `torch.nn.functional.multi_head_attention_forward()` ask for an attention mask whenever `is_causal=True` is given:  This short PR tightens the check for a missing attention mask so it is not required when it would be set to `None` 11 lines later anyway. Disclaimer: I currently do not have a development setup for PyTorch and will rely on the CI, sorry. As an aside, the docstring of `torch.nn.functional.multi_head_attention_forward()` currently reads: > is_causal: If specified, applies a causal mask as attention mask, and ignores >        attn_mask for computing scaled dot product attention. This suggests that the mask is completely ignored, while it is actually still required when either `need_weights` or `key_padding_mask` is given. This could be fixed either by updating the docstring, or by creating a causal `attn_mask` on the fly when needed, and not ever complaining about a missing mask. The latter would be convenient, but it would hide to the user the opportunity to precompute the mask once and reuse it in the case of fixed sequence lengths or multiple samesize transformer layers.",2023-09-04T13:29:29Z,triaged open source Stale,closed,0,4,https://github.com/pytorch/pytorch/issues/108517,The committers listed above are authorized under a signed CLA.:white_check_mark: login: f0k / name: Jan SchlÃ¼ter  (5d0d6d0b7e0c88af31787d5306c4baa646e1ffcf)," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Reading `MultiheadAttention.forward()`, I saw that I forgot the ""fast path"", which calls `torch._native_multi_head_attention` instead of `torch.nn.functional.multi_head_attention_forward()`. The fast path uses `attn_mask` without ever checking for `is_causal`. So calling `MultiheadAttention.forward(..., is_causal=True, attn_mask=None)` when all preliminaries are met for the fast path will currently not give an error, and not give the same result as when passing an attention mask. The PR currently only touches `torch.nn.functional.multi_head_attention_forward()`, removing the need to pass an attention mask in exactly the cases it is currently guaranteed to be ignored anyway. By itself, this seems reasonable. However, it may lead users to pass `is_causal_True, attn_mask=None` to `MultiheadAttention.forward()`, which is a slippery slope  it will work fine during training, but will break inference if the fast path is hit. So in addition to updating the docstring for `torch.nn.functional.multi_head_attention_forward()`, I guess this PR should also clarify whether `MultiheadAttention.forward()` should require `attn_mask` when `is_causal=True`, and add the appropriate checks. My hunch would be to allow `is_causal=True, attn_mask=None`, but not enter the fast path in this case.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
451,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Silent correctness issue if input tensors have existing storage offsets)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug This is already fixed on main by https://github.com/pytorch/pytorch/pull/108168  Fails with  The existing storage_offset is discarded by inductor.  Versions release/2.1 branch )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Silent correctness issue if input tensors have existing storage offsets, ğŸ› Describe the bug This is already fixed on main by https://github.com/pytorch/pytorch/pull/108168  Fails with  The existing storage_offset is discarded by inductor.  Versions release/2.1 branch ,2023-09-02T02:24:40Z,high priority triaged module: correctness (silent) oncall: pt2 module: inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/108472
1017,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`SymInt` input doesn't get optimized out from `torch.compiled()` graph even if unused)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug We have Dynamo backend defined similar to IPEX which traces and freezes the model:  I'm running the Llama model from Transformers repo tag tag: v4.30.1 with following script:  one of the graph that `torch.compile()` produces is:  Here second argument is `s0 : torch.SymInt` which isn't used later, I think it should be optimized out by DeadCodeElimination, I tried to call `eliminate_dead_code` on model, it doesn't do anything. This is troublesome since `orch.jit.trace` doesn't support `SymInt` inputs.  This bug occurs many times in this model, I pasted only one subgraph where is occurs since it is short.  Problem doesn't occur on v2.0.0 tag, but happens on `400c4de53bb7b36066aef381313ed71e4a877e95`  Versions main branch )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`SymInt` input doesn't get optimized out from `torch.compiled()` graph even if unused," ğŸ› Describe the bug We have Dynamo backend defined similar to IPEX which traces and freezes the model:  I'm running the Llama model from Transformers repo tag tag: v4.30.1 with following script:  one of the graph that `torch.compile()` produces is:  Here second argument is `s0 : torch.SymInt` which isn't used later, I think it should be optimized out by DeadCodeElimination, I tried to call `eliminate_dead_code` on model, it doesn't do anything. This is troublesome since `orch.jit.trace` doesn't support `SymInt` inputs.  This bug occurs many times in this model, I pasted only one subgraph where is occurs since it is short.  Problem doesn't occur on v2.0.0 tag, but happens on `400c4de53bb7b36066aef381313ed71e4a877e95`  Versions main branch ",2023-09-01T21:06:26Z,good first issue triaged oncall: pt2 module: dynamic shapes,open,0,1,https://github.com/pytorch/pytorch/issues/108446,`eliminate_dead_code` doesn't eliminate dead inputs. The addition of the SymInt input is intentional. You'll need to muck around with  I think I'd be OK with adding another config knob for this but this is NOT the happy path for Dynamo.
1004,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Torch compile generates incorrect graph on Llama model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug We have Dynamo backend defined similar to IPEX which traces and freezes the model (however the problem is general):  I'm running the Llama model from Transformers repo tag `tag: v4.30.1` with following script:  When model gets to the our dynamo backend Pytorch throws an error (inside `torch.jit.trace`):  The problem lies in this part (I printed the model with `print_readable()` call in our backend:  and__7 = 1 & eq_4;  eq_4 = None < this line is wrong there is an Tensor on left hand side and bool on the right hand side, the code generated by torch.dynamo is incorrect. Problem doesn't occur on `v2.0.0` tag, but happens on `400c4de53bb7b36066aef381313ed71e4a877e95` The original code from the model in this place is:   Versions main branch )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Torch compile generates incorrect graph on Llama model," ğŸ› Describe the bug We have Dynamo backend defined similar to IPEX which traces and freezes the model (however the problem is general):  I'm running the Llama model from Transformers repo tag `tag: v4.30.1` with following script:  When model gets to the our dynamo backend Pytorch throws an error (inside `torch.jit.trace`):  The problem lies in this part (I printed the model with `print_readable()` call in our backend:  and__7 = 1 & eq_4;  eq_4 = None < this line is wrong there is an Tensor on left hand side and bool on the right hand side, the code generated by torch.dynamo is incorrect. Problem doesn't occur on `v2.0.0` tag, but happens on `400c4de53bb7b36066aef381313ed71e4a877e95` The original code from the model in this place is:   Versions main branch ",2023-09-01T20:04:58Z,triaged module: regression oncall: pt2 module: dynamic shapes,closed,0,2,https://github.com/pytorch/pytorch/issues/108442, are you working on this?,Seems to work correctly in 2.4
620,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Implement `mul` operation for complex types)ï¼Œ å†…å®¹æ˜¯ (  CC([MPS] Implement `mul` operation for complex types)  CC([MPS] Add complex `add`/`sub`)  CC([MPS][BE] Add `dispatch_sync_with_rethrow`) Using existing BinaryKernel template Add `mul` as well as `kron` and `outer` to list of MPS ops that support complex types This should add all the missing ops mentioned in  CC(Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[MPS] Implement `mul` operation for complex types,  CC([MPS] Implement `mul` operation for complex types)  CC([MPS] Add complex `add`/`sub`)  CC([MPS][BE] Add `dispatch_sync_with_rethrow`) Using existing BinaryKernel template Add `mul` as well as `kron` and `outer` to list of MPS ops that support complex types This should add all the missing ops mentioned in  CC(Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators),2023-09-01T03:42:01Z,Merged ciflow/trunk release notes: mps ciflow/mps,closed,0,4,https://github.com/pytorch/pytorch/issues/108395, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"With this PR, does this mean pytorch supports all complex operations on MPS devices? e.g. tensor multiplications, additions, and various linear algebra operations?",
709,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP always puts parameters to fp32 when loading state_dict, even if state_dict has bf16 params)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug In cases where a nonFSDP trained model is loaded with FSDP and the state_dict to load is bf16, loading into FSDP sharded_state_dict will make the parameters to fp32. In my particular case, I am converting LLaMA weights to PyTorch FSDP format for FSDP training, but the checkpoint I'm working with is in bf16. This could cause issues in cases where users want to benchmark numerical equivalence.  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"FSDP always puts parameters to fp32 when loading state_dict, even if state_dict has bf16 params"," ğŸ› Describe the bug In cases where a nonFSDP trained model is loaded with FSDP and the state_dict to load is bf16, loading into FSDP sharded_state_dict will make the parameters to fp32. In my particular case, I am converting LLaMA weights to PyTorch FSDP format for FSDP training, but the checkpoint I'm working with is in bf16. This could cause issues in cases where users want to benchmark numerical equivalence.  Versions main ",2023-08-31T23:02:32Z,oncall: distributed,open,0,2,https://github.com/pytorch/pytorch/issues/108381,varma are you using torch.compile?,I am not chuang 
1958,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor][cpu] Perf regression)ï¼Œ å†…å®¹æ˜¯ (perf regression found  compare with 2023_08_22 nightly Repro bash inductor_single_test.sh multiple inference performance suite model float32 first dynamic cpp 0 new_perf_regression                name       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       doctr_det_predictor       1       1.069458       0.148391054       0.158697999828732       33.396333       1       1.503562       0.106578389       0.160247215721618       37.416409       0.71       1.01       0.72       1.12                 pytorch_unet       1       0.862569       0.310560677       0.267880012599213       18.169774       1       1.057315       0.24839536899999998       0.262632149574235       27.68669       0.82       0.98       0.8       1.52                 *       *       *       *       *       *       *       *       *       *       *       *       *       *       *                 doctr_det_predictor       1       0.652977       3.336484332       2.178647529656364       38.657023       1       1.20895       1.828299074       2.2103221655123       36.253952       0.54       1.01       0.55       0.94                 pytorch_unet       1       0.915661       5.48157092       5.01926071017812       20.518048       1       0.998196       4.898655984       4.889818808604864       29.142998       0.92       0.97       0.89       1.42                 *       *       *       *       *       *       *       *       *       *       *       *       * )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[inductor][cpu] Perf regression,perf regression found  compare with 2023_08_22 nightly Repro bash inductor_single_test.sh multiple inference performance suite model float32 first dynamic cpp 0 new_perf_regression                name       batch_size_new       speed_up_new       inductor_new       eager_new       compilation_latency_new       batch_size_old       speed_up_old       inductor_old       eager_old       compilation_latency_old       Ratio Speedup(New/old)       Eager Ratio(old/new)       Inductor Ratio(old/new)       Compilation_latency_Ratio(old/new)                       doctr_det_predictor       1       1.069458       0.148391054       0.158697999828732       33.396333       1       1.503562       0.106578389       0.160247215721618       37.416409       0.71       1.01       0.72       1.12                 pytorch_unet       1       0.862569       0.310560677       0.267880012599213       18.169774       1       1.057315       0.24839536899999998       0.262632149574235       27.68669       0.82       0.98       0.8       1.52                 *       *       *       *       *       *       *       *       *       *       *       *       *       *       *                 doctr_det_predictor       1       0.652977       3.336484332       2.178647529656364       38.657023       1       1.20895       1.828299074       2.2103221655123       36.253952       0.54       1.01       0.55       0.94                 pytorch_unet       1       0.915661       5.48157092       5.01926071017812       20.518048       1       0.998196       4.898655984       4.889818808604864       29.142998       0.92       0.97       0.89       1.42                 *       *       *       *       *       *       *       *       *       *       *       *       * ,2023-08-31T03:14:18Z,triaged oncall: pt2 oncall: cpu inductor,closed,0,13,https://github.com/pytorch/pytorch/issues/108324,"pytorch_unet regression starts from this commit: https://github.com/pytorch/pytorch/pull/107640 In the lowering of `cat`, `to_dtype` will be called: https://github.com/pytorch/pytorch/blob/10c646295d3512237cfb3ab44aa21dfcc9832441/torch/_inductor/lowering.pyL962 Before https://github.com/pytorch/pytorch/pull/107640, `to_dtype` will add extra copy: https://github.com/pytorch/pytorch/blob/3022a395f37a3a82fe72a4e752a9aa7fe0243558/torch/_inductor/lowering.pyL500L502 While after this PR, no more copy: https://github.com/pytorch/pytorch/blob/10c646295d3512237cfb3ab44aa21dfcc9832441/torch/_inductor/lowering.pyL501L504 The generated code are different. Need further check why the removal of extra copy has brought performance regression.",llama regression is caused by https://github.com/pytorch/pytorch/pull/107578.  ,"`doctr_det_predictor` regression: we added `freezing` and `export TORCHINDUCTOR_FREEZING=1` when running the dynamic shapes benchmark starting from 0825, which caused this regression. By adding freezing, deconv + relu will be fused and the primitive of this fusion is quite slow compared with running deconv and relu separately. Similar as  CC([Inductor][cpu] torchbench model doctr_det_predictor perf regression) Need to upgrade oneDNN to >= v3.2 to fix this issue. The current version in PyTorch is 3.1.1.","> llama regression is caused by CC([Inductor] Add new fused_attention pattern matcher). , w. The regression is caused by using the fused SDPA (flashattention basedSDPA). Due to the small sequence length (32) in the model, the blocking is not able to be applied. However, to reduce memory access, the fused SDPA does more calculations compared to the unfused SDPA. Hence, the regression is expected and we need to tune a threshold for the input shape for using the fused SDPA.","`pytorch_unet `:  I checked the report on 20231129 and the performance of `pytorch_unet ` inductorinferencefloat32dynamiccppwrapper is 0.226223875 (multithread), 4.773578356 (singlethread) and seems the regression has been fixed. However, in the report on 20231204, the performance becomes 0.330448337(multithread), 5.891143518(singlethread). Could you please help doubleconfirm it? If that's the case, could you please help submit a new issue for `pytorch_unet` by providing the guilty commit between 20231129 and 20231204?","`doctr_det_predictor`  I checked the latest report on 20231204 and the performance of doctr_det_predictor inductorinferencefloat32dynamiccppwrapper is 0.096308208(multithread), 1.897524131(singlethread) and seems the regression has been fixed. Could you please help doubleconfirm it?","Have double checked that `doctr_det_predictor` perf issue has been fixed by the oneDNN 3.3.2 upgrade.  time in s  2.752925339  please help to double check the `pytorch_unet` model with  check whether if the expected drop due to the build flag change. If not, please create a new one for it and close this one.","> Have double checked that `doctr_det_predictor` perf issue has been fixed by the oneDNN 3.3.2 upgrade. >  > time in s	oneDNN v3.3.2	oneDNN v3.1.1 > multithread	0.093045079	0.136664209 > singlethread	1.936846355	2.752925339 >  please help to double check the `pytorch_unet` model with  check whether if the expected drop due to the build flag change. If not, please create a new one for it and close this one. I have checked that the speedup of FP32 singlethread and multithreads are both `0.97` for building flag change. Hence, this is not the guilty commit for `pytorch_unet` drop.",I'll reassign this issue to   since the perf regression of `doctr_det_predictor` and `pytorch_unet` have all been fixed on the latest main branch. The only left issue is `llama` and the fix is needed as specified in  CC([inductor][cpu] Perf regression)issuecomment1751604591.,"**Update (20231220):** According to the `Torchinductorinductorinferencefloat32dynamiccppReport(AWS)_2023_12_18`, `pytorch_unet ` perf is 0.216017387 (multithread), 4.681508366(singlethread). The regression is fixed.  ","The root cause of llama regression is the same as  CC([inductor][cpu] performance regression). The application of flash attention introduces more extra buffer creations. Expected to be fixed by https://github.com/pytorch/pytorch/pull/116899, with which the perf comes back.  eager:  0.03427387704141438  inductor:  0.024897337891161442  speedup: 1.377x.","In addition, I have tried to disable flash attention with different KV size thresholds, for llama and stable diffusion. It couldn't help and the perf would drop more or less.  ","Close this issue as per  CC(TorchInductor CPU Performance Dashboard)issuecomment1916066655 , llama bs32 multi thread speed up is 1.36908x now."
2007,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor][cpu] AssertionError: ordered_kwargs_for_cpp_kernel has to be provided)ï¼Œ å†…å®¹æ˜¯ (AssertionError: ordered_kwargs_for_cpp_kernel has to be provided, affect model llama and tnt_s_patch16_224(only occurs in single core) Repro bash inductor_single_test.sh multiple inference performance torchbench llama float32 first static cpp 0 Errorr MSG  loading model: 0it [00:00, ?it/s] loading model: 0it [00:00, ?it/s]cpu eval llama ERROR:common:Backend dynamo failed in warmup() Traceback (most recent call last): File ""/workspace/pytorch/benchmarks/dynamo/common.py"", line 2226, in warmup fn(model, example_inputs) File ""/workspace/pytorch/torch/_dynamo/eval_frame.py"", line 328, in _fn return fn(*args, **kwargs) File ""/workspace/pytorch/torch/_dynamo/eval_frame.py"", line 490, in catch_errors return callback(frame, cache_entry, hooks, frame_state) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 641, in _convert_frame result = inner_convert(frame, cache_size, hooks, frame_state) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 133, in _fn return fn(*args, **kwargs) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert return _compile( File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 569, in _compile guarded_code = compile_inner(code, one_graph, hooks, transform) File ""/workspace/pytorch/torch/_dynamo/utils.py"", line 189, in time_wrapper r = func(*args, **kwargs) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 491, in compile_inner out_code = transform_code_object(code, transform) File ""/workspace/pytorch/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object transformations(instructions, code_options) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 458, in transform tracer.run() F)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[inductor][cpu] AssertionError: ordered_kwargs_for_cpp_kernel has to be provided,"AssertionError: ordered_kwargs_for_cpp_kernel has to be provided, affect model llama and tnt_s_patch16_224(only occurs in single core) Repro bash inductor_single_test.sh multiple inference performance torchbench llama float32 first static cpp 0 Errorr MSG  loading model: 0it [00:00, ?it/s] loading model: 0it [00:00, ?it/s]cpu eval llama ERROR:common:Backend dynamo failed in warmup() Traceback (most recent call last): File ""/workspace/pytorch/benchmarks/dynamo/common.py"", line 2226, in warmup fn(model, example_inputs) File ""/workspace/pytorch/torch/_dynamo/eval_frame.py"", line 328, in _fn return fn(*args, **kwargs) File ""/workspace/pytorch/torch/_dynamo/eval_frame.py"", line 490, in catch_errors return callback(frame, cache_entry, hooks, frame_state) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 641, in _convert_frame result = inner_convert(frame, cache_size, hooks, frame_state) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 133, in _fn return fn(*args, **kwargs) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert return _compile( File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 569, in _compile guarded_code = compile_inner(code, one_graph, hooks, transform) File ""/workspace/pytorch/torch/_dynamo/utils.py"", line 189, in time_wrapper r = func(*args, **kwargs) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 491, in compile_inner out_code = transform_code_object(code, transform) File ""/workspace/pytorch/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object transformations(instructions, code_options) File ""/workspace/pytorch/torch/_dynamo/convert_frame.py"", line 458, in transform tracer.run() F",2023-08-31T03:04:55Z,triaged oncall: pt2 module: inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/108323
927,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torchbench mfu mem bandwidth+ refactors)ï¼Œ å†…å®¹æ˜¯ ( EDIT:  I'm angrily refactoring, we can probably just merge by rebasing at https://github.com/pytorch/pytorch/pull/108296/commits/6019d5a559d9a7ef167e5b151614057f5619f5e4 or I might just do https://github.com/pytorch/pytorch/pull/108296/commits/34145b169ac234ca64ca2cd669101a699245521d in a seperate PR EDIT 2: These numbers are quite low so either its well known for stable diffusion or a bug in my measurement, will baseline with llama instead EDIT 3: I will research and summarize other implementations of the metric, try to find a denominator for MFU and finally run a larger sweep, it is expected for most blueberries models that mfu will be low since we are doing bs=1 so need to find other things )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,torchbench mfu mem bandwidth+ refactors," EDIT:  I'm angrily refactoring, we can probably just merge by rebasing at https://github.com/pytorch/pytorch/pull/108296/commits/6019d5a559d9a7ef167e5b151614057f5619f5e4 or I might just do https://github.com/pytorch/pytorch/pull/108296/commits/34145b169ac234ca64ca2cd669101a699245521d in a seperate PR EDIT 2: These numbers are quite low so either its well known for stable diffusion or a bug in my measurement, will baseline with llama instead EDIT 3: I will research and summarize other implementations of the metric, try to find a denominator for MFU and finally run a larger sweep, it is expected for most blueberries models that mfu will be low since we are doing bs=1 so need to find other things ",2023-08-30T22:54:25Z,Stale topic: not user facing module: dynamo ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/108296,"Hi  I have a csv now generated here in `inductor.csv`, I'm still verifying the correctness of the metrics but we can talk about dashboarding now ","> Hi  I have a csv now generated here in `inductor.csv`, I'm still verifying the correctness of the metrics but we can talk about dashboarding now >  >  This looks good to me. The two new field `mfu` and `memory_bandwidth` will be ingested automatically into https://console.rockset.com/collections/details/inductor.torch_dynamo_perf_stats.  The current dashboard can pick them up.  I will start working on that using some dummy data for these two fields. To actually populate them with real data, we will need to run one around of perf benchmark with this PR using your dev branch `msaroufim/mfu2` https://github.com/pytorch/pytorch/actions/workflows/inductorperftestnightly.yml.  Only results from that workflow is uploaded to Rockset.", rebase,The numbers are low but i suspect this might be expected for stable diffusion so running https://github.com/pytorch/pytorch/actions/runs/6303288952 to see if this applies to all models , started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/108296/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/6303282896,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","I'm no longer spending time on the compilers dashboard so gonna close this in case someone else wants to pick this up, the main issue was the numbers seemed too low but that might be expected behavior"
441,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] Ignore sympy.PolynomialError while simplifying)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] Ignore sympy.PolynomialError while simplifying) Fixes CC(jx_nest_base + max_autotune fails with `sympy.polys.polyerrors.PolynomialError: noncommutative expressions are not supported``))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[inductor] Ignore sympy.PolynomialError while simplifying,  CC([inductor] Ignore sympy.PolynomialError while simplifying) Fixes CC(jx_nest_base + max_autotune fails with `sympy.polys.polyerrors.PolynomialError: noncommutative expressions are not supported``),2023-08-30T20:20:40Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/108280, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1224,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Transformer performance drop due to slow PyTorch GEMMs)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug While trying to debug our transformer performance, we see the following problem. When looking at the performance of different matrix multiplications using `torch.nn.functional.linear()`, we see a huge drop in performance for GEMMs of size (2048, 4, 82432)x(82432, 20608) through (2048, 4,  89088)x(89088, 22272) as displayed in the below figures.  !image !image In the area of bad performance, there is a large drop in L2 cache hit rate, as well as a shifting in the tiling dimensions of the kernel call (128x256x64 to 256x128x64). When investigating this using Cutlass, we saw that it does not share the performance drop PyTorch does for these GEMM sizes. Why do we see this poor performance in GEMMs, and how can we avoid this poor performance? full kernel names outside of dip: `ampere_fp16_s16816gemm_fp16_128x256_ldg8_f2f_stages_64x3_tn` within dip: `ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_64x3_tn` full script used to produce figures:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Transformer performance drop due to slow PyTorch GEMMs," ğŸ› Describe the bug While trying to debug our transformer performance, we see the following problem. When looking at the performance of different matrix multiplications using `torch.nn.functional.linear()`, we see a huge drop in performance for GEMMs of size (2048, 4, 82432)x(82432, 20608) through (2048, 4,  89088)x(89088, 22272) as displayed in the below figures.  !image !image In the area of bad performance, there is a large drop in L2 cache hit rate, as well as a shifting in the tiling dimensions of the kernel call (128x256x64 to 256x128x64). When investigating this using Cutlass, we saw that it does not share the performance drop PyTorch does for these GEMM sizes. Why do we see this poor performance in GEMMs, and how can we avoid this poor performance? full kernel names outside of dip: `ampere_fp16_s16816gemm_fp16_128x256_ldg8_f2f_stages_64x3_tn` within dip: `ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_64x3_tn` full script used to produce figures:   Versions  ",2023-08-30T20:08:29Z,module: performance module: cuda triaged,open,0,2,https://github.com/pytorch/pytorch/issues/108277,"Thanks for reporting the issue ! Are you using `2.0.1+cu117` or `1.13.0+cu116` as both seem to be installed? I also assume you've tested the same workload using the current nightly+cu121 release? Could you also let me know which models are affected by this, please? ","Hey, sorry for the long delay here. I have confirmed the issue with both `2.0.1` and `1.13.0`. I have not tested with the nightly release, I will see if i can find time to do that. This is from the mlp layer of a transformer, so any mlp layer where `input=4*output`."
1890,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( RuntimeError: NYI: Named tensors are not supported with the tracer)ï¼Œ å†…å®¹æ˜¯ (Repro 1:  Error:  Repro2:  Error 2:   CC(RuntimeError: NYI: Named tensors are not supported with the tracer) RuntimeError: NYI: Named tensors are not supported with the tracer CC(jit tracer doesn't work with unflatten layer) jit tracer doesn't work with unflatten layer CC(when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.) when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.     This bug was closed but exists. Multiple comments on it still showing error. This is addressed Likely fixes the following issues (but untested) CC(Named tensor in tracer) Named tensor in tracer CC(allow single nontuple sequence to trigger advanced indexing) [Bug] torch.onnx.errors.UnsupportedOperatorError when convert mask2former to onnx Fix zero dimensioned tensors when used with jit.trace They are currently assigned an empty set for names {} this is not the same as ""no name"" so jit.trace bails with   ""NYI: Named tensors are not supported with the tracer"" This happens when I am trying to save a nontrivial model as onnx but the simplest repro I have seen is 48054 above which has been added as test/jit/test_zero_dim_tensor_trace.py Test plan:   New unit test added   Broken scenarios tested locally   CI Fixes CC(RuntimeError: NYI: Named tensors are not supported with the tracer)  label ""topic: not user facing"")è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi, RuntimeError: NYI: Named tensors are not supported with the tracer,"Repro 1:  Error:  Repro2:  Error 2:   CC(RuntimeError: NYI: Named tensors are not supported with the tracer) RuntimeError: NYI: Named tensors are not supported with the tracer CC(jit tracer doesn't work with unflatten layer) jit tracer doesn't work with unflatten layer CC(when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.) when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.     This bug was closed but exists. Multiple comments on it still showing error. This is addressed Likely fixes the following issues (but untested) CC(Named tensor in tracer) Named tensor in tracer CC(allow single nontuple sequence to trigger advanced indexing) [Bug] torch.onnx.errors.UnsupportedOperatorError when convert mask2former to onnx Fix zero dimensioned tensors when used with jit.trace They are currently assigned an empty set for names {} this is not the same as ""no name"" so jit.trace bails with   ""NYI: Named tensors are not supported with the tracer"" This happens when I am trying to save a nontrivial model as onnx but the simplest repro I have seen is 48054 above which has been added as test/jit/test_zero_dim_tensor_trace.py Test plan:   New unit test added   Broken scenarios tested locally   CI Fixes CC(RuntimeError: NYI: Named tensors are not supported with the tracer)  label ""topic: not user facing""",2023-08-30T15:28:33Z,oncall: jit open source onnx-needs-info,closed,2,15,https://github.com/pytorch/pytorch/issues/108238,"  :white_check_mark: login: warrenburch / name: Warren Burch  (6cd18d9363cee02da9b18e37734eec9133b49082, c17b1db2a97bf672da94b6b0330c7c1eb7164f88) :x: The email address for the commit (0694472b8a4a6b0f424cd6c9959fd27dbd3e01f6) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."," label ""topic: not user facing""","thanks Richard, sorry for delay I was out of the country. I have addressed your feedback and also found a couple of other places this pattern is used incorrectly. Cheers Warren On Thu, Sep 14, 2023 at 7:26â€¯AM Richard Zou ***@***.***> wrote: > ***@***.**** commented on this pull request. >  > > In test/test_zero_dim_tensor_trace.py > : > > > +class TestZeroDimTensorTrace(TestCase): > +    def test_bugrepro(self): > + > +        import torch > +        def f(x): > +            return x[x > 0] > +        jf = torch.jit.trace(f, torch.tensor(2., device=""cpu"")) > > Please move these tests to test/test_jit.py > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >",Any chance this could get fixed? I'm having the issue  CC(RuntimeError: NYI: Named tensors are not supported with the tracer)),"I have submitted the PR and it is waiting on review. I dont know how long it takes to get reviewed. In the meantime you could just try pulling my branch ( https://github.com/warrenburch/pytorch/tree/user/warrenb/FixZeroDimensionTensorsInTracer) and using that, you would have to rebuild pytorch yourself though which if you havent done it before can be a little involved. Cheers Warren On Fri, Oct 13, 2023 at 10:22â€¯AM some16 ***@***.***> wrote: > Any chance this could get fixed? I'm having the issue CC(RuntimeError: NYI: Named tensors are not supported with the tracer) >  > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Hi Richard (), I have tried changing my name to match and have filled out the individual CLA several times. I am not sure what I need to do to get this to go through. Any help greatly appreciated!  Cheers Warren",can you squash all the commits on your branch and then forcepush to it?," Thanks Richard, I have squashed and pushed. I have filled out the CLA form (several times) but it doesnt seem to like it. I'm at a loss at this point how to proceed. Cheers Warren",  can you try opening a new PR and seeing what happens?,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",Reopening as this issue is still relevant,Closing and pushing a new PR for this: https://github.com/pytorch/pytorch/pull/118393,"And the fix is still good. I had so many problems getting this accepted because of process I just gave up when I moved on to another project.Â Itâ€™s be great if someone who is able to make the repo changes to this upWarrenOn Jan 26, 2024, at 9:35 AM, Thiago Crepaldi ***@***.***> wrote:ï»¿ Closing and pushing a new PR for this â€”Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>","Excuse the autocorrect mess in the mail (on a tiny screen). I think the intention is clearWarrenOn Jan 26, 2024, at 9:45 AM, Warren Burch ***@***.***> wrote:ï»¿And the fix is still good. I had so many problems getting this accepted because of process I just gave up when I moved on to another project.Â Itâ€™s be great if someone who is able to make the repo changes to this upWarrenOn Jan 26, 2024, at 9:35 AM, Thiago Crepaldi ***@***.***> wrote:ï»¿ Closing and pushing a new PR for this â€”Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>","Thanks Thiago!WarrenOn Jan 26, 2024, at 9:35 AM, Thiago Crepaldi ***@***.***> wrote:ï»¿ Closing and pushing a new PR for this â€”Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>"
2036,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Compile] Running Llama2 with torch.compile and FSDP results in Type mismatch assert in LlamaRotaryEmbedding )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Running Torch.compile with Llama7B and FSDP mixed precision, results in assert during first forward pass of training: (you can repro by going to https://github.com/lessw2020/llamarecipes/tree/rotary_embeddings and run ""bash run.sh"")  ~~~  File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/torch/_subclasses/meta_utils.py"", line 42, in assert_eq     assert a == b, f""{a} != {b}"" AssertionError: torch.float32 != torch.bfloat16 ~~~ from this section (full trace below): ~~~    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/transformers/models/llama/modeling_llama.py"", line 123, in forward     self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),  ~~~  Effectively there is a type mismatch but at least in adding some debugging to the Rotary cache and the incoming tensors, everything is all fp32.   Here's the full stack trace:  ~~~ Training Epoch0:   0% 0/48 [01:11     fire.Fire(main)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/fire/core.py"", line 141, in Fire     component_trace = _Fire(component, args, parsed_flag_args, context, name)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/fire/core.py"", line 475, in _Fire     component, remaining_args = _CallAndUpdateTrace(   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/fire/core.py"", line 691, in _CallAndUpdateTrace     component = fn(*varargs, **kwargs)   File ""/data/home/less/llama_rotary/llama_finetuning.py"", line 245, in main     results = train(   File ""/data/home/le)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[Compile] Running Llama2 with torch.compile and FSDP results in Type mismatch assert in LlamaRotaryEmbedding ," ğŸ› Describe the bug Running Torch.compile with Llama7B and FSDP mixed precision, results in assert during first forward pass of training: (you can repro by going to https://github.com/lessw2020/llamarecipes/tree/rotary_embeddings and run ""bash run.sh"")  ~~~  File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/torch/_subclasses/meta_utils.py"", line 42, in assert_eq     assert a == b, f""{a} != {b}"" AssertionError: torch.float32 != torch.bfloat16 ~~~ from this section (full trace below): ~~~    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/transformers/models/llama/modeling_llama.py"", line 123, in forward     self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),  ~~~  Effectively there is a type mismatch but at least in adding some debugging to the Rotary cache and the incoming tensors, everything is all fp32.   Here's the full stack trace:  ~~~ Training Epoch0:   0% 0/48 [01:11     fire.Fire(main)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/fire/core.py"", line 141, in Fire     component_trace = _Fire(component, args, parsed_flag_args, context, name)   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/fire/core.py"", line 475, in _Fire     component, remaining_args = _CallAndUpdateTrace(   File ""/data/home/less/miniconda3/lib/python3.9/sitepackages/fire/core.py"", line 691, in _CallAndUpdateTrace     component = fn(*varargs, **kwargs)   File ""/data/home/less/llama_rotary/llama_finetuning.py"", line 245, in main     results = train(   File ""/data/home/le",2023-08-30T03:15:21Z,high priority oncall: distributed triaged months module: fsdp oncall: pt2 pt2d-triage-nov2024,open,0,8,https://github.com/pytorch/pytorch/issues/108211,FWIW I couldn't repro this by just compiling the `LlamaRotaryEmbedding` from HuggingFace on a single GPU without FSDP,"I think the core issue is the mixed precision aspect of FSDP and how compile is interacting with it.  Per IBM, if you move your weights to pure BF16 (so no mixed precision) then this issue goes away (though they are then reporting it errors out with a stride mismatch..but we'll get to that after this is resolved). ",Horace: There's another issue with DDP. Is there anyone signed up to own FSDP + torch.compile / DDP + torch.compile? This is basically  ,> Is there anyone signed up to own FSDP + torch.compile / DDP + torch.compile I'm currently working on this.,I am working on compile + FSDP. Tracked w/ meta internal posts. ,Sounds good  we could sync later on this as needed.,"We are working on compile + FSDP which is preferred over graphbreak FSDP. We aim to have it ready at ""prototype"" release stage by end of H1.","Ran into the same issue, will this be fixed in an incoming release? And, wondering if there is any workaround fix? Thanks. "
1110,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([fbcode] reduce overhead in `split_with_sizes_nested`)ï¼Œ å†…å®¹æ˜¯ (Summary: we are seeing for certain cases, ""aten::chunk"" is causing overhead for transformer encoder when nested tensor is being used. As suggested by cpuhrsch, we may reduce this overhead by working on the pointers as opposed to tensor elements. This is expected to remove those small CPU `at::select` calls. also updated a bit more to use `at::empty_like` to replace `clone` to avoid filling overhead and `moved` tensors (although tensor copy should be lightweight). Test Plan: UT also profiled with the a postray model. D48409754   CUDA_VISIBLE_DEVICES=2 buck2 run @//mode/devnosan content_understanding/inference/benchmarking:pt2_benchmark  dumppath /tmp/inference_benchmark_5b_bs64_eager_fix_nnnt pt2mode NA skipcheckparity skipmemsnapshot batchsize 64  {F1079141849} vs before  {F1079128718} the overhead of `aten::chunk` is gone (20X) Differential Revision: D48810365)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[fbcode] reduce overhead in `split_with_sizes_nested`,"Summary: we are seeing for certain cases, ""aten::chunk"" is causing overhead for transformer encoder when nested tensor is being used. As suggested by cpuhrsch, we may reduce this overhead by working on the pointers as opposed to tensor elements. This is expected to remove those small CPU `at::select` calls. also updated a bit more to use `at::empty_like` to replace `clone` to avoid filling overhead and `moved` tensors (although tensor copy should be lightweight). Test Plan: UT also profiled with the a postray model. D48409754   CUDA_VISIBLE_DEVICES=2 buck2 run @//mode/devnosan content_understanding/inference/benchmarking:pt2_benchmark  dumppath /tmp/inference_benchmark_5b_bs64_eager_fix_nnnt pt2mode NA skipcheckparity skipmemsnapshot batchsize 64  {F1079141849} vs before  {F1079128718} the overhead of `aten::chunk` is gone (20X) Differential Revision: D48810365",2023-08-30T01:24:41Z,fb-exported,closed,0,4,https://github.com/pytorch/pytorch/issues/108207,"   :x: The email address for the commit (7b8ab5a182b81e5b059d0cca4c32ba414fe9d664) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D48810365," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.",Can close this and merge https://github.com/pytorch/pytorch/pull/108213 instead
1490,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AdaptiveMaxPool documentation is not detailed)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue The documentation does not give insights into how the assignments are performed for AdaptiveMaxPool2d (and other AdaptiveMaxPoolXd). While it is clear what happens in simple cases where the input shape is divisible by the input shape, the behavior when it is not divisible (i.e., where it is actually necessary) is not clarified. For example, for a 3x3 input and a 2x2 output, it is not obvious that each output is the maximum of 2x2 inputs. (It could also be maxima over 2x2, 2x1, 1x2, and 1x1 input elements.) My source for the behavior is: https://github.com/pytorch/pytorch/blob/51861cc9b19d9c483598e39932661822a826d3a2/aten/src/ATen/native/AdaptiveMaxPooling2d.cpp   Suggest a potential alternative/fix The following could be added: For each element of the batch and each channel, given an input shape of $H_{in}\times W_{in}$ and a desired output of shape $H_{out}\times W_{out}$, `AdaptiveMaxPool2d` is defined as follows:  $$Y[i, j] = \max\left\\{ X[a, b] \ |\  \left\lfloor \frac{i \cdot H_{in}}{H_{out}} \right\rfloor \leq a < \left\lceil \frac{(i+1) \cdot H_{in}}{H_{out}} \right\rceil \ ,\  \left\lfloor \frac{j \cdot W_{in}}{W_{out}} \right\rfloor \leq b < \left\lceil \frac{(j+1) \cdot W_{in}}{W_{out}} \right\rceil   \right\\}$$ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AdaptiveMaxPool documentation is not detailed," ğŸ“š The doc issue The documentation does not give insights into how the assignments are performed for AdaptiveMaxPool2d (and other AdaptiveMaxPoolXd). While it is clear what happens in simple cases where the input shape is divisible by the input shape, the behavior when it is not divisible (i.e., where it is actually necessary) is not clarified. For example, for a 3x3 input and a 2x2 output, it is not obvious that each output is the maximum of 2x2 inputs. (It could also be maxima over 2x2, 2x1, 1x2, and 1x1 input elements.) My source for the behavior is: https://github.com/pytorch/pytorch/blob/51861cc9b19d9c483598e39932661822a826d3a2/aten/src/ATen/native/AdaptiveMaxPooling2d.cpp   Suggest a potential alternative/fix The following could be added: For each element of the batch and each channel, given an input shape of $H_{in}\times W_{in}$ and a desired output of shape $H_{out}\times W_{out}$, `AdaptiveMaxPool2d` is defined as follows:  $$Y[i, j] = \max\left\\{ X[a, b] \ |\  \left\lfloor \frac{i \cdot H_{in}}{H_{out}} \right\rfloor \leq a < \left\lceil \frac{(i+1) \cdot H_{in}}{H_{out}} \right\rceil \ ,\  \left\lfloor \frac{j \cdot W_{in}}{W_{out}} \right\rfloor \leq b < \left\lceil \frac{(j+1) \cdot W_{in}}{W_{out}} \right\rceil   \right\\}$$ ",2023-08-29T23:55:40Z,module: docs module: nn triaged actionable module: pooling,open,0,1,https://github.com/pytorch/pytorch/issues/108197,Thanks  we would accept a PR for this that improves the docs.
512,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add FakeTensor support to torch._utils._rebuild_tensor)ï¼Œ å†…å®¹æ˜¯ (There are two scenarios: * Scenario 1: The checkpoint was saved with pytorch = 1.6 Repro Scenario 1:  Error:  Repro scenario 2:  Error:  This PR adds the ability to create fake tensors during torch.load (when fake mode is active) by changing the storage's device to 'meta'. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add FakeTensor support to torch._utils._rebuild_tensor,There are two scenarios: * Scenario 1: The checkpoint was saved with pytorch = 1.6 Repro Scenario 1:  Error:  Repro scenario 2:  Error:  This PR adds the ability to create fake tensors during torch.load (when fake mode is active) by changing the storage's device to 'meta'. ,2023-08-29T21:05:15Z,triaged open source Merged Reverted ezyang's list ciflow/trunk module: fakeTensor ciflow/inductor release notes: dynamo,closed,0,42,https://github.com/pytorch/pytorch/issues/108186,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Hi, once you add support for `Storage` in schemas:  Then I think you'll need to add a custom op impl for `aten.set_.source_Storage_storage_offset` in fake_tensor.py and this should work, without having to make changes to `torch._utils._rebuild_tensor`","> Hi, once you add support for `Storage` in schemas:  a/torch/fx/operator_schemas.py +++ b/torch/fx/operator_schemas.py @@ 51,6 +51,7 @@ _type_eval_globals = {'Tensor' : torch.Tensor, 'Device' : torch.device, 'Layout' 'number' : numbers.Number, 'Future' : torch.jit.Future, 'AnyEnumType' : enum.Enum, 'QScheme' : torch.qscheme, '**torch**': _FakeGlobalNamespace(), 'NoneType': type(None), >  > *  >  > Then I think you'll need to add a custom op impl for `aten.set_.source_Storage_storage_offset` in fake_tensor.py and this should work, without having to make changes to `torch._utils._rebuild_tensor` Thanks, I will give it a try soon",Any updates on this?,I am coming back to this soon and try Elias suggestion,"> > Hi, once you add support for `Storage` in schemas:  a/torch/fx/operator_schemas.py +++ b/torch/fx/operator_schemas.py @@ 51,6 +51,7 @@ _type_eval_globals = {'Tensor' : torch.Tensor, 'Device' : torch.device, 'Layout' 'number' : numbers.Number, 'Future' : torch.jit.Future, 'AnyEnumType' : enum.Enum, 'QScheme' : torch.qscheme, '**torch**': _FakeGlobalNamespace(), 'NoneType': type(None), > >  > > *  > >  > > Then I think you'll need to add a custom op impl for `aten.set_.source_Storage_storage_offset` in fake_tensor.py and this should work, without having to make changes to `torch._utils._rebuild_tensor` >  > Thanks, I will give it a try soon what would the custom op for `aten.set_.source_Storage_storage_offset` do differently than the existing implementation? I am back on this task"," sent me:  The problem with implementing set_ support for FakeTensorMode is .... it's not entirely well defined. Specifically, we don't have a concept of a userfacing FakeStorage (and cannot implement one, because we don't have a `__torch_dispatch__` or subclass override mechanism for storages). You might be able to get it to work by doing a little bit of special casing: if you have a real storage as an argument to fake set_, you could try converting it to a meta storage on the spot (since we DO have a mapping of real to ""fake"" storages aka storages with meta device.) The other obvious fix is to rewrite all of the code here to not use the Storage API at all. This is not too difficult to do for `set_` because we have an overload that takes a Tensor as input. But how would we arrange for a Tensor to be passed in as an input to `_rebuild_tensor`? It also seems questionable. Must ponder some more...",https://github.com/pytorch/pytorch/pull/118780 helps The next problem is that you get a CPU storage fed into set_. With this patch on top of my PR it works  So it seems to me that you should teach the storage deserialization logic to deserialize into meta storage (with no real data) when fake tensor mode is enabled.,"As a bonus, you get to avoid wasting time deserializing data on disk that you're going to throw out later!","> CC(Support printing storage while FakeTensorMode is enabled) helps >  > The next problem is that you get a CPU storage fed into set_. With this patch on top of my PR it works >  >  >  > So it seems to me that you should teach the storage deserialization logic to deserialize into meta storage (with no real data) when fake tensor mode is enabled. Indeed this patch seems to do the trick, with the exception that it always use `meta` device and `torch.empty`. We should do it only when fake mode is detected, right? Push a new revision with that change. Thanks", merge,> thankee Thanks for the review. I have a repro for a similar issue. there is a device mismatch between `meta` and `cpu` for this sample  , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / test (default, 2, 3, windows.4xlarge.nonephemeral) Details for Dev Infra team Raised by workflow job ",A single Windows CI failed with:  which might be related to the changes in serialization  although there is no obvious connection that I could spot," merge f ""unrelated executorch quantization error"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",This needs to be reverted failing internal test after landing. Error: ," revert m=""Diff reverted internally"" c=""ghfirst"" This Pull Request has been reverted by a revert inside Meta. To reland this change, please open another pull request, assign the same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).)", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,> This needs to be reverted failing internal test after landing. Error: >  >  Is there a repro I can use to make sure this will be fixed?,Hi  . Here is more detailed log: ,"> Hi  . Here is more detailed log: >  >  Just added `register_artifact(""not_implemented"", ""Logs dispatches not implemented by Fake Tensor"")` to `torch/_logging/_registrations.py` as the error message suggested.   Can you import this PR and verify it works now?", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,  Same Internal breakage. Unlanding this diff.  Please don't reland until we confirm its safe. Error:  Log: ," rever c ghfirst m ""Reverted Internally"""
453,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Properly skip fast path in TransformerEncoder/MHA if autocast is enabled on CPU)ï¼Œ å†…å®¹æ˜¯ (Fix CC(`RuntimeError: expected scalar type BFloat16 but found Float` with `torch.nn.TransformerEncoder`)   CC(Properly skip fast path in TransformerEncoder/MHA if autocast is enabled on CPU))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Properly skip fast path in TransformerEncoder/MHA if autocast is enabled on CPU,Fix CC(`RuntimeError: expected scalar type BFloat16 but found Float` with `torch.nn.TransformerEncoder`)   CC(Properly skip fast path in TransformerEncoder/MHA if autocast is enabled on CPU),2023-08-29T19:42:23Z,Stale,closed,0,1,https://github.com/pytorch/pytorch/issues/108178,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
992,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problems hit when upgrading the version of HF used in CI)ï¼Œ å†…å®¹æ˜¯ ( upgraded the HF version used on CI to overcome a common Export+AOTInductor failure on HF, https://github.com/pytorch/pytorch/pull/107400. However, the upgrading revealed several other issues,  [ ] test_hf_bert_ddp_inductor failure, `python test/distributed/test_dynamo_distributed.py k test_hf_bert_ddp_inductor`  [ ] several benchmarks failures with dynamic shapes ON, also reported in  CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0)  [ ] increased graph break count, see https://github.com/pytorch/pytorch/pull/107400/files . Most of the affected benchmarks overlap with those failed at dynamic shapes, so likely caused by the same issue. Performance drop because of additional graph breaks. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Problems hit when upgrading the version of HF used in CI," upgraded the HF version used on CI to overcome a common Export+AOTInductor failure on HF, https://github.com/pytorch/pytorch/pull/107400. However, the upgrading revealed several other issues,  [ ] test_hf_bert_ddp_inductor failure, `python test/distributed/test_dynamo_distributed.py k test_hf_bert_ddp_inductor`  [ ] several benchmarks failures with dynamic shapes ON, also reported in  CC([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0)  [ ] increased graph break count, see https://github.com/pytorch/pytorch/pull/107400/files . Most of the affected benchmarks overlap with those failed at dynamic shapes, so likely caused by the same issue. Performance drop because of additional graph breaks. ",2023-08-29T14:07:18Z,triaged oncall: pt2 module: dynamic shapes,closed,0,1,https://github.com/pytorch/pytorch/issues/108145,xref https://github.com/huggingface/transformers/pull/25581
2023,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment)ï¼Œ å†…å®¹æ˜¯ ( Current Behavior: The `torch.nn.functional.scaled_dot_product_attention` function currently applies causal masking to the top left corner of the attention matrix. This well defined when seqlen_q equals seqlen_kv, but an alignment choice needs to made when they are not.  Current alignment choice If seqlen_q != seqlen_kv and causal=True, the causal mask is aligned to the the topleft corner. For example, if seqlen_q = 2 and seqlen_kv = 5, the causal mask (1 = keep, 0 = masked out) is:  If seqlen_q = 5 and seqlen_kv = 2, the causal mask is:   Proposal: I propose changing the default behavior of the scaled_dot_product_attention function's causal masking to align with the bottom right corner of the attention matrix. This change would be backward incompatible for inputs when seqlen_q does not equal seqlen_kv, as it would shift the masking pattern. For example, if seqlen_q = 2 and seqlen_kv = 5, the causal mask (1 = keep, 0 = masked out) is:  If seqlen_q = 5 and seqlen_kv =  2, the causal mask is:  _If the row of the mask is all zero, the output will be zero._  Benefits: This choice of mask is beneficial when performing autoregressive decoding for LLMs.  For example lets say that your prefill prompt consisted of 4 tokens and you had a max KV cache size of 8. Then at step 1 of decoding, your query sequence length would be size 1 (index position 5) and your KV input would be of size 5. The query's seqlen is 1 but it really represents the 5 token in the sequence. So the mask for this attention should be:  With the existing implementation of causal, the produced attention mask would be.     Drawbacks: Backward compatibility: The proposed change would break backward compatibility for cases where seqlen_q doesn't equal seqlen_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment," Current Behavior: The `torch.nn.functional.scaled_dot_product_attention` function currently applies causal masking to the top left corner of the attention matrix. This well defined when seqlen_q equals seqlen_kv, but an alignment choice needs to made when they are not.  Current alignment choice If seqlen_q != seqlen_kv and causal=True, the causal mask is aligned to the the topleft corner. For example, if seqlen_q = 2 and seqlen_kv = 5, the causal mask (1 = keep, 0 = masked out) is:  If seqlen_q = 5 and seqlen_kv = 2, the causal mask is:   Proposal: I propose changing the default behavior of the scaled_dot_product_attention function's causal masking to align with the bottom right corner of the attention matrix. This change would be backward incompatible for inputs when seqlen_q does not equal seqlen_kv, as it would shift the masking pattern. For example, if seqlen_q = 2 and seqlen_kv = 5, the causal mask (1 = keep, 0 = masked out) is:  If seqlen_q = 5 and seqlen_kv =  2, the causal mask is:  _If the row of the mask is all zero, the output will be zero._  Benefits: This choice of mask is beneficial when performing autoregressive decoding for LLMs.  For example lets say that your prefill prompt consisted of 4 tokens and you had a max KV cache size of 8. Then at step 1 of decoding, your query sequence length would be size 1 (index position 5) and your KV input would be of size 5. The query's seqlen is 1 but it really represents the 5 token in the sequence. So the mask for this attention should be:  With the existing implementation of causal, the produced attention mask would be.     Drawbacks: Backward compatibility: The proposed change would break backward compatibility for cases where seqlen_q doesn't equal seqlen_",2023-08-28T23:47:02Z,module: bc-breaking module: nn triaged topic: bc breaking module: sdpa,open,7,5,https://github.com/pytorch/pytorch/issues/108108,"Hi  . Is/would this proposal under working in progress? I checked with torch 2.4 (RC) and torch 2.5 (nightly) and it looks like they are still using the current mask (as in torch 2.3). I also have a question about the benefit mentioned > For example lets say that your prefill prompt consisted of 4 tokens and you had a max KV cache size of 8. Then at step 1 of decoding, your query sequence length would be size 1 (index position 5) and your KV input would be of size 5. The query's seqlen is 1 but it really represents the 5 token in the sequence. So the mask for this attention should be: In this case, query sequence length would be size 1, and KV (cache) length is of size 8, statically allocated. Now even if with the proposal (that is the same with current flash attention >= 2.1), the attention mask would be  > 1 1 1 1 1 1 1 1 and the query still attends to the last 3 tokens (position index  6, 7, 8 , 1based indexing). I think this has to be addressed in flash attention library too in order to use static KV cache. Let me know if I make some mistake here, thank you.","Flash attention now have `flash_attn_with_kvcache` that could deal with the training 0 in the mask by using the argument `cache_seqlens` + the length of new key/value. However, if pytorch ever want to update sdpa with FA2, it's likely we will have a `scaled_dot_product_attention_with_kvcache` rather adding extra arguments into the existing `flash_attn_with_kvcache` ?","OK, I see in CC(Disable FlashAttenion for is_causal=True when seqlen q not equal kv) > // for nonsquare masks. We will not support nonsquare masks for causal w/ FAV2 and CC([RFC] Scaled Dot Product Attention  API Changes). Not sure if it's still WIP.",  You can find the mechanism for using lowerrightcausal masking here: https://github.com/pytorch/pytorch/blob/f86dbae2471bcfddd68d11b8baa3c952fcf8b2e9/torch/nn/attention/bias.pyL321 You are also right that for static kv_cache we would need a different mechanism for supporting this. We currently dont support sdpa_with_kv_cache. The existing plan has been to do something using NestedTensor  That being said we still might  ship a kv_cache enabled variant of sdpa,"Hi  Thanks a lot for the above information. I have one more question: currently `CausalBias` has `causal_upper_left` and `causal_lower_right`. While the support of `sdpa_with_kv_cache` is not yet available, do you think it's even possible for a user to subclass `CausalBias` and `CausalVariant` in order to mark some trailing part to be `False` (i.e. not to attend). Looking into the source code: > out = torch.ops.aten._scaled_dot_product_flash_attention(...) it seems this is still impossible as `torch.ops.aten._scaled_dot_product_flash_attention` doesn't accept a mask argument or something similar to `cu_seqlens_k` argument."
586,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Sequence Parallel for non-Pairwise Parallel and uneven MLP layers.)ï¼Œ å†…å®¹æ˜¯ (Hi all, Is there a way to apply SequenceParallel to RowwiseParallel and ColwiseParallel layers similar to how it is applied to PairwiseParallel?  I typically will swap the layers with Apexcompatible ones but I am interested in using `parallelize_module` to remove that dependency. Thank you, Enrico  Versions main (2.1.0a0+git8688965)  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Sequence Parallel for non-Pairwise Parallel and uneven MLP layers.,"Hi all, Is there a way to apply SequenceParallel to RowwiseParallel and ColwiseParallel layers similar to how it is applied to PairwiseParallel?  I typically will swap the layers with Apexcompatible ones but I am interested in using `parallelize_module` to remove that dependency. Thank you, Enrico  Versions main (2.1.0a0+git8688965)  ",2023-08-28T23:41:05Z,oncall: distributed triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/108106,cc:  ," yes you can. You just passing different prepare_input and prepare_out to the parallel_style. For example, for sequence parallel can be:  This way we call allgather and reducescatter for you internally.",">  yes you can. You just passing different prepare_input and prepare_out to the parallel_style. For example, for sequence parallel can be: >  >  >  > This way we call allgather and reducescatter for you internally. Hi , Thank you for the response. To outline an example for comprehension, say you have an Attention mechanism with sequence parallelism and a column and row parallel layer in Apex:  If we would like to apply PyTorch Sequence Parallelism to Colwise and Rowwise Parallel linear layers it would follow something like:  Or in the case of Sequence Parallelism and an MLP/FF layer with 3 Apex linear layers:  Then the matching PyTorch Sequence Parallel and Tensor Parallel modules would be:  And to confirm, we should **not** be doing:  I greatly appreciate your help. Thank you, Enrico"," if you want to use PyTorch native solution, you don't need the `ColumnParallelLinear` and `RowParallelLinear` We will handle the collectives for you. And by specifying different `_prepare_input` and `_prepare_output` you essentially are switching from sequence parallel and tensor parallel.","Thank you for the help. >  if you want to use PyTorch native solution, you don't need the `ColumnParallelLinear` and `RowParallelLinear` We will handle the collectives for you. And by specifying different `_prepare_input` and `_prepare_output` you essentially are switching from sequence parallel and tensor parallel."
416,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix test_mem_efficient_attention_vs_math_ref_grads tolerance from test_transformers.py)ï¼Œ å†…å®¹æ˜¯ (Tolerance currently too low, triggering test failures via numerical mismatch in NVIDIA internal testing for certain H100, A16, A40 configs. cc:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Fix test_mem_efficient_attention_vs_math_ref_grads tolerance from test_transformers.py,"Tolerance currently too low, triggering test failures via numerical mismatch in NVIDIA internal testing for certain H100, A16, A40 configs. cc:   ",2023-08-28T20:45:18Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/108094,Ready for review, Are we still waiting on 's review or can someone from Meta approve and merge? This PR has been ready for a month now and  has already approved on our end., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
635,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Intra-graph reordering pass on Inductor scheduler IR (based on #100762))ï¼Œ å†…å®¹æ˜¯ (This PR implements intragraph communication reordering pass on Inductor scheduler IR, based on Horace's previous PR CC(Communication reordering pass). Main algorithm: 1. Greedily moves waits as late as possible (i.e. until we reach a use) 2. Greedily moves comms as early as possible (i.e. until we reach an input) 3. Move computes following simple heuristics to improve overlap. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Intra-graph reordering pass on Inductor scheduler IR (based on #100762),"This PR implements intragraph communication reordering pass on Inductor scheduler IR, based on Horace's previous PR CC(Communication reordering pass). Main algorithm: 1. Greedily moves waits as late as possible (i.e. until we reach a use) 2. Greedily moves comms as early as possible (i.e. until we reach an input) 3. Move computes following simple heuristics to improve overlap. ",2023-08-28T20:06:03Z,Merged ciflow/trunk module: inductor module: dynamo ciflow/inductor release notes: inductor,closed,0,12,https://github.com/pytorch/pytorch/issues/108091," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Meta InternalOnly Changes Check Details for Dev Infra team Raised by workflow job "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hi,   Wonderful work, may i ask a question about this pass? Usually the users may put the collective ops to a new cuda Stream for overlapping with the compute ops in the current Stream, but it looks like the Stream ops(for example, `torch.cuda.set_stream`) will be ignored by the `torch.compile`. Here the inductor will schedule the collective ops on the graph to another Stream? or both collective ops and compute ops will execute on the same Stream? Thank you."
302,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Intra-graph communication reordering pass on Inductor scheduler IR (based on #100762))ï¼Œ å†…å®¹æ˜¯ (Differential Revision: D47657156 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Intra-graph communication reordering pass on Inductor scheduler IR (based on #100762),Differential Revision: D47657156 ,2023-08-26T04:46:35Z,fb-exported Stale module: inductor ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/108001,This pull request was **exported** from Phabricator. Differential Revision: D47657156," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1714,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with Huggingface accelerate + FSDP,  No model update unless prepare is  done together for model, optimizer, lr_schedular)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am using HF accelerate with FSDP. HF recommends to first do model prepare (accelerate.prepare()) and then do optimizer prepare. But when I use that way it does not update the model  parameters, i.e. model does not train. While debugging that model sharding has happen, gradient is also calculated.  Here is code snippet (setting 1):   However,  the parameter update works, i.e. model training happens fine if I  prepare together.   Though the code is working but i get the warning message  `WARNING  FSDP Warning: When using FSDP, it is efficient and recommended to call prepare for the model before creating the optimizer` This setting is doing model parameters update (working version  setting 2):      `model, train_dataloader, valid_dataloader, optimizer, lr_scheduler = accelerator.prepare( model, train_dataloader, valid_dataloader, optimizer, lr_scheduler) ` Initialization of acclerate is same for both, as follows:  The accelerate config file for FSDP is    Versions  Other relevant libraries version  I have tried that with other version of python (3.9) and also other version of accelerate (0.18, 0.19) and transformers (4.24.0), i find the same (model doesn't train with setting 1).  When I change the accelerate config file for no fsdp, the setting 1 works fine.  The accelerate config without FSDP in which setting 1 does training is.  Can some one help to fix this ? )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Issue with Huggingface accelerate + FSDP,  No model update unless prepare is  done together for model, optimizer, lr_schedular"," ğŸ› Describe the bug I am using HF accelerate with FSDP. HF recommends to first do model prepare (accelerate.prepare()) and then do optimizer prepare. But when I use that way it does not update the model  parameters, i.e. model does not train. While debugging that model sharding has happen, gradient is also calculated.  Here is code snippet (setting 1):   However,  the parameter update works, i.e. model training happens fine if I  prepare together.   Though the code is working but i get the warning message  `WARNING  FSDP Warning: When using FSDP, it is efficient and recommended to call prepare for the model before creating the optimizer` This setting is doing model parameters update (working version  setting 2):      `model, train_dataloader, valid_dataloader, optimizer, lr_scheduler = accelerator.prepare( model, train_dataloader, valid_dataloader, optimizer, lr_scheduler) ` Initialization of acclerate is same for both, as follows:  The accelerate config file for FSDP is    Versions  Other relevant libraries version  I have tried that with other version of python (3.9) and also other version of accelerate (0.18, 0.19) and transformers (4.24.0), i find the same (model doesn't train with setting 1).  When I change the accelerate config file for no fsdp, the setting 1 works fine.  The accelerate config without FSDP in which setting 1 does training is.  Can some one help to fix this ? ",2023-08-24T04:00:06Z,oncall: distributed triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/107843,This issue seems better for the HuggingFace accelerate repo?, Does this issue needs to be transferred to Huggingface accelerate ? Please let me know. ,Yes. Please transfer it.
880,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Support constant tensors in FakeMode exporting)ï¼Œ å†…å®¹æ˜¯ (  CC([ONNX] Support constant tensors in FakeMode exporting) Fixes  CC([ONNX] ONNX doesn't support exporting the models with constant tensors in FakeMode)  Constant tensors was wrongly recognized as weights and buffers, and then was detached from its default value during `to_model_proto`. This PR fixes the bug and pick up Bloom CI test back successfully. NOTE: nonpersistent buffer and weights has different situation and is not fixed by this PR.  Reduce transformers model size by modifying their config parameters to speed up CI tests. (Unrelated to this PR title) Corresponding change with https://github.com/microsoft/onnxscript/pull/1023)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Support constant tensors in FakeMode exporting,"  CC([ONNX] Support constant tensors in FakeMode exporting) Fixes  CC([ONNX] ONNX doesn't support exporting the models with constant tensors in FakeMode)  Constant tensors was wrongly recognized as weights and buffers, and then was detached from its default value during `to_model_proto`. This PR fixes the bug and pick up Bloom CI test back successfully. NOTE: nonpersistent buffer and weights has different situation and is not fixed by this PR.  Reduce transformers model size by modifying their config parameters to speed up CI tests. (Unrelated to this PR title) Corresponding change with https://github.com/microsoft/onnxscript/pull/1023",2023-08-24T01:01:55Z,module: onnx open source Merged ciflow/trunk release notes: onnx topic: bug fixes,closed,0,2,https://github.com/pytorch/pytorch/issues/107836, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
529,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] bugfix - make module setattr more restrictive)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor][ac] preserve recompute tags through pattern matching)  CC([dynamo] bugfix  make module setattr more restrictive) A check got missed in https://github.com/pytorch/pytorch/pull/106092 Fixes  CC( Error using compile with transformers: 'NoneType' object has no attribute 'node') )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[dynamo] bugfix - make module setattr more restrictive,  CC([inductor][ac] preserve recompute tags through pattern matching)  CC([dynamo] bugfix  make module setattr more restrictive) A check got missed in https://github.com/pytorch/pytorch/pull/106092 Fixes  CC( Error using compile with transformers: 'NoneType' object has no attribute 'node') ,2023-08-23T22:57:57Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,11,https://github.com/pytorch/pytorch/issues/107828, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch rebase origin/main` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: inductorperiodic / cuda12.1py3.10gcc9sm86periodicdynamobenchmarks / test (dynamo_eager_huggingface, 1, 1, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge," merge f ""expecting ROCm tests to pass"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
511,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable Mypy Checking in torch/_inductor/bounds.py)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Enable Mypy Checking in torch/_inductor)  Summary: As suggested in  CC(Enable Mypy Checking in torch/_inductor) mypy checking is enabled in torch/_inductor/bounds.py After Fix: mypy followimports=skip torch/_inductor/bounds.py Success: no issues found in 1 source file )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,Enable Mypy Checking in torch/_inductor/bounds.py,Fixes CC(Enable Mypy Checking in torch/_inductor)  Summary: As suggested in  CC(Enable Mypy Checking in torch/_inductor) mypy checking is enabled in torch/_inductor/bounds.py After Fix: mypy followimports=skip torch/_inductor/bounds.py Success: no issues found in 1 source file ,2023-08-23T07:53:58Z,triaged open source Stale module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/107769," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
858,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( Error using compile with transformers: 'NoneType' object has no attribute 'node')ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm hitting an error with the following code using latest nightly build:  which produces:  what I find quite strange is that if I uncomment the line that calls `model.forward` once before calling the `compiled_forward`, the error disappears and everything works as expected.  Note: I had tried compile with the same model some weeks ago and encountered a different error (see: CC(Error using torch.compile with HF transformers and model `mosaicml/mpt7b`)) but this new error seems significantly different.  Error logs _No response_  Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer, Error using compile with transformers: 'NoneType' object has no attribute 'node'," ğŸ› Describe the bug I'm hitting an error with the following code using latest nightly build:  which produces:  what I find quite strange is that if I uncomment the line that calls `model.forward` once before calling the `compiled_forward`, the error disappears and everything works as expected.  Note: I had tried compile with the same model some weeks ago and encountered a different error (see: CC(Error using torch.compile with HF transformers and model `mosaicml/mpt7b`)) but this new error seems significantly different.  Error logs _No response_  Minified repro _No response_  Versions  ",2023-08-22T18:47:54Z,triaged oncall: pt2 module: dynamo,closed,0,1,https://github.com/pytorch/pytorch/issues/107721,Would be fixed by https://github.com/pytorch/pytorch/pull/107828
695,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Retire FXSymbolicTracer in FX exporter)ï¼Œ å†…å®¹æ˜¯ (FXSymbolicTracer is a pioneer of exploring fake tensor export, which does successfully export large scale transformers, like BLOOM, and GPT2. However, it's not using dynamo export, but `torch.fx._symbolic_trace.Tracer` which is not actively maintained. Besides, there are patches used to only cover FXSymbolicTracer, which is not exposed to public API, and will never be.. Now, with the fake mode api in fx exporter becomes mature. We should retire FXSymbolicTracer.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Retire FXSymbolicTracer in FX exporter,"FXSymbolicTracer is a pioneer of exploring fake tensor export, which does successfully export large scale transformers, like BLOOM, and GPT2. However, it's not using dynamo export, but `torch.fx._symbolic_trace.Tracer` which is not actively maintained. Besides, there are patches used to only cover FXSymbolicTracer, which is not exposed to public API, and will never be.. Now, with the fake mode api in fx exporter becomes mature. We should retire FXSymbolicTracer.",2023-08-22T17:12:08Z,module: onnx triaged onnx-triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/107714, I am leaving this one to you.
654,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`torch._int_mm` may yield wrong results starting cuda 12.1 update 1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `torch._int_mm` may yield wrong results starting cuda 12.1 update 1 The tests and functionality were reenabled for versions other than cuda 11.7 in https://github.com/pytorch/pytorch/pull/106840, and failures were later discovered by our internal CI. The issue has been reported to cublas team. Ref: /4252478 Tested on A100. Short reproduce:   Versions main cuda 12.1 update 1 A100  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`torch._int_mm` may yield wrong results starting cuda 12.1 update 1," ğŸ› Describe the bug `torch._int_mm` may yield wrong results starting cuda 12.1 update 1 The tests and functionality were reenabled for versions other than cuda 11.7 in https://github.com/pytorch/pytorch/pull/106840, and failures were later discovered by our internal CI. The issue has been reported to cublas team. Ref: /4252478 Tested on A100. Short reproduce:   Versions main cuda 12.1 update 1 A100  ",2023-08-22T08:23:07Z,high priority triage review module: cuda triaged module: cublas module: correctness (silent),closed,1,6,https://github.com/pytorch/pytorch/issues/107671,Should we update the code to fallback to another impl when detecting this specific cuda version?,"Even though this is a private function, since 2.1 will ship with CUDA 12.1, we most likely want a mitigation before then.","The results are correct for cuda 12.1, and started to become wrong in cuda 12.1 update 1. It depends on which cuda 12.1 minor version you will be using. For the mitigation, we can change this cuda version guard at compile time just like it was before https://github.com/pytorch/pytorch/blob/b0e93e206c5e5ddaba3072032aa9e92cc542edab/aten/src/ATen/native/cuda/Blas.cppL682",We indeed migrated to 12.1.1 due to  CC(`torch.linalg.eigh` fails on GPU) Updating that compile flag will make `_int_mm` raise error for all the release build is that correct?,"If you mean test failures, we can skip those tests based on cuda runtime versions. For functionality concern, an error will unfortunately be raised, at least it's better than silent wrong output. https://github.com/pytorch/pytorch/blob/b0e93e206c5e5ddaba3072032aa9e92cc542edab/aten/src/ATen/native/cuda/Blas.cppL701L707","I've discussed this with cublas team. There are something wrong with our implementation here, https://github.com/pytorch/pytorch/blob/cd031f13ba16d68f999144a73c21908db8199934/aten/src/ATen/cuda/CUDABlas.cppL1025L1026 The alpha and beta values here should be of `scaleType`, that is `int32_t` instead of `int8_t` or `float`. I've verified that after fixing this type, the numerical results are correct in the latest cuda 12.2"
487,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`RuntimeError: expected scalar type BFloat16 but found Float` with `torch.nn.TransformerEncoder`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Runtime error occurred when running `torch.nn.TransformerEncoder` in AMP scope. This issue occurs for both when `enable_nested_tensor` is `True` and `False`.   Error message:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`RuntimeError: expected scalar type BFloat16 but found Float` with `torch.nn.TransformerEncoder`, ğŸ› Describe the bug Runtime error occurred when running `torch.nn.TransformerEncoder` in AMP scope. This issue occurs for both when `enable_nested_tensor` is `True` and `False`.   Error message:   Versions  ,2023-08-22T05:46:29Z,module: nn triaged module: amp (automated mixed precision),open,0,4,https://github.com/pytorch/pytorch/issues/107663,"FYI. another issue (shown below) will most likely occur once this issue gets fixed, under the same environment with the same script when `enable_nested_tensor = True`. ","The issue here is that the `TransformerEncoder` fast path should be disabled when autocast is enabled. See here https://github.com/pytorch/pytorch/blob/78810d78e82f8e18dbc1c049a2b92e559ab567b2/torch/nn/modules/transformer.pyL350L351 However, `torch.is_autocast_enabled()` doesn't work on CPU, for CPU we should use `torch.is_autocast_cpu_enabled` instead   so this is not being properly checked on CPU. If this code sample is changed to   your script should run without error. I will send a PR to fix this. I haven't looked into the issue in the second comment yet.           ",It looks like the PR CC(Properly skip fast path in TransformerEncoder/MHA if autocast is enabled on CPU) was closed as stale without any comment from the maintainers. What can we do to help get this reviewed and merged?,"> It looks like the PR CC(Properly skip fast path in TransformerEncoder/MHA if autocast is enabled on CPU) was closed as stale without any comment from the maintainers. >  > What can we do to help get this reviewed and merged? I was getting a similar issue with not being able to use Autocast on a transformerEncoderLayer when using both nested tensors (batched) and src_key_padding_mask together. I'm temporarily having to use fully Stochastic gradient decent (no batch dimension) to get around this. Ideally, there'd be support for both of these with autocast. Just for clarity, having those two in place without autocast also works. tests have been done with both CPU and Nvidia GPUs."
719,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] Separate to_{dtype,device} from lowering to avoid copying)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] Fix bug in input mutation)  CC([inductor] Separate to_{dtype,device} from lowering to avoid copying) These lowerings must copy even when they are noops in order to preserve correctness in the presense of mutations. However, `to_dtype` and `to_device` are also used in various lowerings as a helper function where it is okay to alias. So, I've split these into two functions and allow the helper functions to alias which saves some unnecessary copies. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[inductor] Separate to_{dtype,device} from lowering to avoid copying","  CC([inductor] Fix bug in input mutation)  CC([inductor] Separate to_{dtype,device} from lowering to avoid copying) These lowerings must copy even when they are noops in order to preserve correctness in the presense of mutations. However, `to_dtype` and `to_device` are also used in various lowerings as a helper function where it is okay to alias. So, I've split these into two functions and allow the helper functions to alias which saves some unnecessary copies. ",2023-08-21T21:35:57Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/107640, merge i," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
359,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(defaultdict) items [] {})ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug occurs in torchrec FusedEmbeddingBagCollection  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(defaultdict) items [] {}, ğŸ› Describe the bug occurs in torchrec FusedEmbeddingBagCollection  Versions main ,2023-08-21T14:32:39Z,good first issue triaged oncall: pt2 module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/107595,proximal cause is the keys of the defaultdict are tuples of enums,"Repro ~~~ import torch from collections import defaultdict from enum import Enum class Color(Enum):     RED = 1     GREEN = 2     BLUE = 3 class Shape(Enum):     RECTANGLE = 1     SQUARE = 2 d = defaultdict(int) d[(Color.RED, Shape.RECTANGLE)] = 2 def fn(x):     for k, v in d.items():         x = x * v     return x opt_fn = torch.compile(fn, backend=""eager"", fullgraph=True) opt_fn(torch.randn(4)) ~~~","This error is caused by the program trying to call the items method on a defaultdict object, and it's not supported by PyTorch's dynamic tracing mechanism. import torch from collections import defaultdict from enum import Enum class Color(Enum):     RED = 1     GREEN = 2     BLUE = 3 class Shape(Enum):     RECTANGLE = 1     SQUARE = 2 d = defaultdict(int) d[(Color.RED, Shape.RECTANGLE)] = 2 def fn(x):      Convert defaultdict to a regular dictionary     d_dict = dict(d)     for k, v in d_dict.items():         x = x * v     return x  Use torch.jit.script to create a TorchScript module opt_fn = torch.jit.script(fn)  Call the TorchScript module result = opt_fn(torch.randn(4)) print(result)",We're expecting this to be fixed by the stack at https://github.com/pytorch/pytorch/pull/111196 Maybe. I'm actually not convinced,"I'd expect this to work with that stack, or in a simple follow up if not"
670,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Run transformers.OPTForCausalLM(config=config) occurs 'GraphModule' object has no attribute 'compile_subgraph_reason')ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I try to convert whole opt model fw/bw without any break. It seems same to closed issue  CC(Torch Dynamo Error when Compiling/Exporting Module which Calls a Helper Function), which is not solved in nighty version! test.py  occurs:  break atï¼š   Versions extraindexurl https://download.pytorch.org/whl/nightly/cu118 pre torch==2.1.0.dev20230820+cu118 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Run transformers.OPTForCausalLM(config=config) occurs 'GraphModule' object has no attribute 'compile_subgraph_reason'," ğŸ› Describe the bug I try to convert whole opt model fw/bw without any break. It seems same to closed issue  CC(Torch Dynamo Error when Compiling/Exporting Module which Calls a Helper Function), which is not solved in nighty version! test.py  occurs:  break atï¼š   Versions extraindexurl https://download.pytorch.org/whl/nightly/cu118 pre torch==2.1.0.dev20230820+cu118 ",2023-08-21T12:27:49Z,triaged oncall: pt2 oncall: export,closed,0,6,https://github.com/pytorch/pytorch/issues/107587,transformers             4.31.0  changed by the commit causes the problem:https://github.com/huggingface/transformers/commit/a28325e25ef399e1a616d67f50ca05abba931163,"Little cleaned up script is ~~~ import transformers import torch._dynamo torch._dynamo.config.suppress_errors = False def make_data(model, device):     batch_size = 1     seq_len = 16     input = torch.randint(         low=0, high=model.config.vocab_size, size=(batch_size, seq_len), device=device     )     label = torch.randint(low=0, high=model.config.vocab_size, size=(batch_size, seq_len),                           device=device)     return input, label device = torch.device('cuda') config = transformers.AutoConfig.from_pretrained(""facebook/opt125m"") config.tie_word_embeddings = False model = transformers.OPTForCausalLM(config=config) model.to(device) optimized_model = torch.compile(model, backend='eager', fullgraph=True) data = make_data(model, device) model.zero_grad(set_to_none=True) with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):     optimized_model(data[0]) ~~~ And error is  ~~~   File ""/data/users/anijain/pytorch/torch/_dynamo/variables/functions.py"", line 90, in call_function     return tx.inline_user_function_return(   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 598, in inline_user_function_return     result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 2179, in inline_call     return cls.inline_call_(parent, func, args, kwargs)   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 2286, in inline_call_     tracer.run()   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 724, in run     and self.step()   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 688, in step     getattr(self, inst.opname)(inst)   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 370, in inner     raise exc.UserError( torch._dynamo.exc.UserError: Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow from user code:    File ""/home/anijain/local/miniconda3/envs/pytorch3.10/lib/python3.10/sitepackages/transformers/models/opt/modeling_opt.py"", line 944, in forward     outputs = self.model.decoder(   File ""/data/users/anijain/pytorch/torch/nn/modules/module.py"", line 1527, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/anijain/local/miniconda3/envs/pytorch3.10/lib/python3.10/sitepackages/transformers/models/opt/modeling_opt.py"", line 688, in forward     if dropout_probability < self.layerdrop: ~~~ Looking more into it.","A much shorter repro is ~~~ import torch import random def fn(x):     prob = torch.rand([])  Fails      prob = random.uniform(0, 1)  Works     if prob < 0.5:         return x * 2     return x * 3 opt_fn = torch.compile(fn, backend=""eager"", fullgraph=True) opt_fn(torch.randn(4)) ~~~ Error ~~~   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 2074, in run     super().run()   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 724, in run     and self.step()   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 688, in step     getattr(self, inst.opname)(inst)   File ""/data/users/anijain/pytorch/torch/_dynamo/symbolic_convert.py"", line 370, in inner     raise exc.UserError( torch._dynamo.exc.UserError: Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow from user code:    File ""/data/users/anijain/pytorch/examples/drop.py"", line 8, in fn     if prob < 0.5: ~~~"," (xref'd above) to solve a part of this problem. ://github.com/pytorch/pytorch/issues/102794 From the discussion, it seems that if we have to peek at the tensor value, its a legit dynamic control flow, and we will graph break. The fix here is to change the model itself. We can use `torch.where` where applicable, or `torch.cond` more generally.",Gong is your question answered from above?,"Closing due to inactivity, feel free to reopen. Looks like it has been resolved"
823,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] Add new fused_attention pattern matcher)ï¼Œ å†…å®¹æ˜¯ (Add new fused_attention pattern matcher for Inductor, in order to make more models call the op SDPA. The following models would call SDPA due to the added pattern: For HuggingFace  AlbertForMaskedLM  AlbertForQuestionAnswering  BertForMaskedLM  BertForQuestionAnswering  CamemBert  ElectraForCausalLM  ElectraForQuestionAnswering  LayoutLMForMaskedLM  LayoutLMForSequenceClassification  MegatronBertForCausalLM  MegatronBertForQuestionAnswering  MobileBertForMaskedLM  MobileBertForQuestionAnswering  RobertaForCausalLM  RobertaForQuestionAnswering  YituTechConvBert For TorchBench  llama )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[Inductor] Add new fused_attention pattern matcher,"Add new fused_attention pattern matcher for Inductor, in order to make more models call the op SDPA. The following models would call SDPA due to the added pattern: For HuggingFace  AlbertForMaskedLM  AlbertForQuestionAnswering  BertForMaskedLM  BertForQuestionAnswering  CamemBert  ElectraForCausalLM  ElectraForQuestionAnswering  LayoutLMForMaskedLM  LayoutLMForSequenceClassification  MegatronBertForCausalLM  MegatronBertForQuestionAnswering  MobileBertForMaskedLM  MobileBertForQuestionAnswering  RobertaForCausalLM  RobertaForQuestionAnswering  YituTechConvBert For TorchBench  llama ",2023-08-21T08:59:24Z,open source Merged ciflow/trunk topic: not user facing intel module: inductor ciflow/inductor,closed,1,2,https://github.com/pytorch/pytorch/issues/107578, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2008,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(haloï¼ŒI continue pretrain llama2-13B model ï¼Œbut save state_dict is about 50GB file)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  def training_function(args):      get some base rank info      metric = evaluate.load(""glue"", ""mrpc"")     world_size = os.getenv(""WORLD_SIZE"")     rank = os.getenv(""RANK"")     local_rank = os.getenv(""LOCAL_RANK"")     assert world_size is not None, f""WORLD_SIZE is needed {world_size}""     assert rank is not None, f""RANK is needed {rank}""     assert local_rank is not None, f""RANK is needed {local_rank}""     world_size = int(world_size)     rank = int(rank)     local_rank = int(local_rank)      Instantiate the model (we build the model here so that the seed also control new weights initialization)      Load the pretrained model and setup its configuration     model = LlamaForCausalLM.from_pretrained(             args.model_name_and_path,             load_in_8bit=True if args.quantization else None,             device_map=""auto"" if args.quantization else None,             return_dict=True         )     model.train()      Initialize accelerator     gradient_accumulation_steps = args.gradient_accumulation_steps     if args.with_tracking:         accelerator = Accelerator(             gradient_accumulation_steps=gradient_accumulation_steps, mixed_precision=args.mixed_precision, log_with=""tensorboard"", project_dir=args.project_dir)     else:         accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps, mixed_precision=args.mixed_precision)          We need to initialize the trackers we use, and also store our configuration     if args.with_tracking:         run = os.path.split(__file__)[1].split(""."")[0]         tracker_cfg = {             'lr': args.lr,             'per_device_batch_size': args.batch_size,             'seed': args.seed,             'num_epoch': args.num_ep)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,haloï¼ŒI continue pretrain llama2-13B model ï¼Œbut save state_dict is about 50GB file," ğŸ› Describe the bug  def training_function(args):      get some base rank info      metric = evaluate.load(""glue"", ""mrpc"")     world_size = os.getenv(""WORLD_SIZE"")     rank = os.getenv(""RANK"")     local_rank = os.getenv(""LOCAL_RANK"")     assert world_size is not None, f""WORLD_SIZE is needed {world_size}""     assert rank is not None, f""RANK is needed {rank}""     assert local_rank is not None, f""RANK is needed {local_rank}""     world_size = int(world_size)     rank = int(rank)     local_rank = int(local_rank)      Instantiate the model (we build the model here so that the seed also control new weights initialization)      Load the pretrained model and setup its configuration     model = LlamaForCausalLM.from_pretrained(             args.model_name_and_path,             load_in_8bit=True if args.quantization else None,             device_map=""auto"" if args.quantization else None,             return_dict=True         )     model.train()      Initialize accelerator     gradient_accumulation_steps = args.gradient_accumulation_steps     if args.with_tracking:         accelerator = Accelerator(             gradient_accumulation_steps=gradient_accumulation_steps, mixed_precision=args.mixed_precision, log_with=""tensorboard"", project_dir=args.project_dir)     else:         accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps, mixed_precision=args.mixed_precision)          We need to initialize the trackers we use, and also store our configuration     if args.with_tracking:         run = os.path.split(__file__)[1].split(""."")[0]         tracker_cfg = {             'lr': args.lr,             'per_device_batch_size': args.batch_size,             'seed': args.seed,             'num_epoch': args.num_ep",2023-08-21T07:46:21Z,oncall: distributed triaged,open,0,2,https://github.com/pytorch/pytorch/issues/107575,I didn't run your code but FWIW 50GB doesn't seem too surprising to me at a high level  LLAMA 13B is about 25GB on disk and if you're using Adam you're gonna need about 50GB  Double checking what you're actually saving and it should make things clearer https://tinkerd.net/blog/machinelearning/distributedtraining/,"Thanks   for answer my questionã€‚ https://tinkerd.net/blog/machinelearning/distributedtraining/ this blog is very usefull for me and i learn more about distrubute training. I compute llama2 model params and compares saved model files, its right. my loaded model is huggingdace formmat, i test if don't assign torch_dtype=torch.float16 it params is torch.float32 and saved file is about 48GB. Thanks  "
825,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(caching keys+values in TransformerDecoderLayer for faster inference)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch In autoregressive generation, at step k, we only need to compute the new token k+1, based on all the previous ones.  This can be done in $O(k  d)$ for each step, if we cache previous examples. However, the current nn.TransformerDecoderLayer (and Encoder) does not support this. Therefore, currently the most efficient inference method would be  in $O(k^2  d)$ for each step.  Alternatives add an option to feed in previously calculated keys+values, and if they appear, avoid some unneeded computations.  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,caching keys+values in TransformerDecoderLayer for faster inference," ğŸš€ The feature, motivation and pitch In autoregressive generation, at step k, we only need to compute the new token k+1, based on all the previous ones.  This can be done in $O(k  d)$ for each step, if we cache previous examples. However, the current nn.TransformerDecoderLayer (and Encoder) does not support this. Therefore, currently the most efficient inference method would be  in $O(k^2  d)$ for each step.  Alternatives add an option to feed in previously calculated keys+values, and if they appear, avoid some unneeded computations.  Additional context _No response_ ",2023-08-21T07:25:47Z,module: nn triaged,open,2,4,https://github.com/pytorch/pytorch/issues/107573,We have been discussing having a builtin KV cache but unclear whether this should be in core or more of an example for people to follow. seems useful enough to have something in core IMHO,Thank you! I agree of course.  It's also worth mentioning that it would be hard to implement a workaround. I couldn't think of a workaround in order to support KV caching.  If you have some ideas I would be happy to know.,KV cache of `TransformerDecoderLayer` should be a standard support by pytorch just like causal mask.,I think also a KV cache is a must have.
302,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Storage size calculation overflowed when torch.nn.Upsample)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Due to arg_1 = 1250999896765   Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Storage size calculation overflowed when torch.nn.Upsample, ğŸ› Describe the bug Due to arg_1 = 1250999896765   Versions ,2023-08-20T17:52:10Z,triage review module: int overflow,closed,0,2,https://github.com/pytorch/pytorch/issues/107553,"Adding triage review as I'm not sure why this issue wasn't closed, as this is an expected behavior, isn't it? If not, let's create an umbrella issue for 64+ bit numel tensors and close those as duplicates","OK, closing as expected behavior. Please do not hesitate to comment (or open a new issue) if you believe PyTorch should behave differently"
1704,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Storage size calculation overflowed when running torch.nn.functional.interpolate)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Due to a very large integer variable.  Output:   Versions PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.9.15 (main, Nov 24 2022, 14:31:59) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 Ti Nvidia driver version: 525.60.11 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] blas 1.0 mkl [conda] cudatoolkit 11.3.1 h2bc3f7f_2 [conda] ffmpeg 4.3 hf484d3e_0 pytorch [conda] mkl 2021.4.0 h06a4308_640 [conda] mklservice 2.4.0 py39h7f8727e_0 [conda] mkl_fft 1.3.1 py39hd3c417c_0 [conda] mkl_random 1.2.2 py39h51133e4_0 [conda] numpy 1.23.5 py39h14f4228_0 [conda] numpybase 1.23.5 py39h31eccc5_0 [conda] pytorch 1.12.0 py3.9_cuda11.3_cudnn8.3.2_0 pytorch [conda] pytorchmutex 1.0 cuda pytorch [conda] torchaudio 0.12.0 py39_cu113 pytorch [conda] torchvision 0.13.0 py39_cu113 pytorch)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Storage size calculation overflowed when running torch.nn.functional.interpolate," ğŸ› Describe the bug Due to a very large integer variable.  Output:   Versions PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.9.15 (main, Nov 24 2022, 14:31:59) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 Ti Nvidia driver version: 525.60.11 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] blas 1.0 mkl [conda] cudatoolkit 11.3.1 h2bc3f7f_2 [conda] ffmpeg 4.3 hf484d3e_0 pytorch [conda] mkl 2021.4.0 h06a4308_640 [conda] mklservice 2.4.0 py39h7f8727e_0 [conda] mkl_fft 1.3.1 py39hd3c417c_0 [conda] mkl_random 1.2.2 py39h51133e4_0 [conda] numpy 1.23.5 py39h14f4228_0 [conda] numpybase 1.23.5 py39h31eccc5_0 [conda] pytorch 1.12.0 py3.9_cuda11.3_cudnn8.3.2_0 pytorch [conda] pytorchmutex 1.0 cuda pytorch [conda] torchaudio 0.12.0 py39_cu113 pytorch [conda] torchvision 0.13.0 py39_cu113 pytorch",2023-08-20T17:37:18Z,triaged module: int overflow,open,0,0,https://github.com/pytorch/pytorch/issues/107552
409,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE]: Add PYI files to ruff lintrunner)ï¼Œ å†…å®¹æ˜¯ (Due to a bug with the lintrunner yaml, PYI files were not convered. This PR builds off CC([BE]: Update ruff to 0.285) covers all PYI files and adds one noqa to fix a B006 bugprone error. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE]: Add PYI files to ruff lintrunner,"Due to a bug with the lintrunner yaml, PYI files were not convered. This PR builds off CC([BE]: Update ruff to 0.285) covers all PYI files and adds one noqa to fix a B006 bugprone error. ",2023-08-19T22:05:59Z,open source better-engineering module: amp (automated mixed precision) Merged ciflow/trunk release notes: onnx release notes: quantization topic: not user facing ciflow/mps,closed,0,26,https://github.com/pytorch/pytorch/issues/107524,If you want to stack like this use ghstack. I'll review after the dep merges and you rebase past it, rebase,"> If you want to stack like this use ghstack. I'll review after the dep merges and you rebase past it Yeah, I need to figure how to get this setup.", started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC([BE]: Add PYI files to ruff lintrunner), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC([BE]: Add PYI files to ruff lintrunner), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC([BE]: Add PYI files to ruff lintrunner), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", rebase b main, started a rebase job onto refs/remotes/origin/main. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/main pull/107524/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/5918807515, Deps are merged and PR is rebased., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3_8clang8xla / test (xla, 1, 1, linux.12xlarge)  Lint / Test collect_env (without_torch)  Lint / Test collect_env (older_python_version) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge ic 'flakes', merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 3 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3_8clang8xla / test (xla, 1, 1, linux.12xlarge)  Lint / Test collect_env (without_torch)  Lint / Test collect_env (older_python_version) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge ic, merge i," Merge started Your change will be merged while ignoring the following 5 checks: pull / linuxfocalpy3_8clang8xla / test (xla, 1, 1, linux.12xlarge), pull / linuxjammypy3.9clang12asan / test (default, 1, 6, linux.4xlarge), Lint / Test collect_env (without_torch), Lint / Test collect_env (older_python_version), trunk / linuxfocalrocm5.6py3.8 / test (default, 1, 3, linux.rocm.gpu) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", drci
385,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE]: Apply PYI autofixes to various types)ï¼Œ å†…å®¹æ˜¯ (Applies some autofixes from the ruff PYI rules to improve the typing of PyTorch. I haven't enabled most of these ruff rules yet as they do not have autofixes. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE]: Apply PYI autofixes to various types,Applies some autofixes from the ruff PYI rules to improve the typing of PyTorch. I haven't enabled most of these ruff rules yet as they do not have autofixes. ,2023-08-19T19:44:30Z,open source Merged ciflow/trunk release notes: distributed (pipeline) module: dynamo ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/107521, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,Found a bug in ruff and flagged it upstream: https://github.com/astralsh/ruff/issues/6695, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,Curious why `Any` > `object`? Just for my learning, Edit: meant to copy this link: https://beta.ruff.rs/docs/rules/anyeqneannotation/
679,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([nightly][jit] bad constant exponent (e+38.f) in default_program fused_mul_div_add)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug OS: Linux Ubuntu 22.04 GPU: Nvidia T4 pytorch: 2.1.0.dev20230817+cu118 `torch.jit.trace()` generates C++ cuda code which contains bad constants  for example `3.402823466385289e+38.f`. The issue is in exponent part of the number. `e+38.f` is bad. It should be `e+38f` (without dot btw 38 and f). Error exists in Pytorch 2.1.0 Nightly (2.0.1 works fine) Example code:  Error message   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[nightly][jit] bad constant exponent (e+38.f) in default_program fused_mul_div_add, ğŸ› Describe the bug OS: Linux Ubuntu 22.04 GPU: Nvidia T4 pytorch: 2.1.0.dev20230817+cu118 `torch.jit.trace()` generates C++ cuda code which contains bad constants  for example `3.402823466385289e+38.f`. The issue is in exponent part of the number. `e+38.f` is bad. It should be `e+38f` (without dot btw 38 and f). Error exists in Pytorch 2.1.0 Nightly (2.0.1 works fine) Example code:  Error message   Versions  ,2023-08-19T02:37:47Z,oncall: jit triaged,open,0,5,https://github.com/pytorch/pytorch/issues/107503,Do you wanna send a PR? torchscript is on maintenance mode so unlikely someone will fix but I'd be happy to merge your fix, Which sections of the file structure should I look into?  maybe fuser? I am really struggling with this issue.,"Hi   I am also affected by the same issue.  I am happy to devote some cycles to fixing it, but can you give some initial pointers and where the culprit code is likely to be?  Thanks in advance..","Unlike Linux, I have found on Windows, the same issue happens, even when using 2.0.1.","torch.jit is on maintenance mode, that's ok, but torch.export is still not stable...  I guess that's a mistake."
954,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add torch::Library::require_pyimport)ï¼Œ å†…å®¹æ˜¯ (  CC(Add torch::Library::require_pyimport) It is possible for users to register kernels for a custom operator in both C++ and Python. This PR:  allows C++ Library to specify a ""python module"" that it is associated with via `m.require_pyimport(""foo.bar.baz"")`  changes `torch.ops.load_library(""lib.so"")` to also import all Python modules that were specified from the staticallyloaded libraries in lib.so. The mechanism behind this is:  `m.require_pyimport(str)` puts the string into a global list  `torch.ops.load_library` reads from the global list, imports every string in the list as a module, and clears the list.  static initialization might be multithreaded so there is a mutex that protects the global list. Test Plan:  new test)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add torch::Library::require_pyimport,"  CC(Add torch::Library::require_pyimport) It is possible for users to register kernels for a custom operator in both C++ and Python. This PR:  allows C++ Library to specify a ""python module"" that it is associated with via `m.require_pyimport(""foo.bar.baz"")`  changes `torch.ops.load_library(""lib.so"")` to also import all Python modules that were specified from the staticallyloaded libraries in lib.so. The mechanism behind this is:  `m.require_pyimport(str)` puts the string into a global list  `torch.ops.load_library` reads from the global list, imports every string in the list as a module, and clears the list.  static initialization might be multithreaded so there is a mutex that protects the global list. Test Plan:  new test",2023-08-18T19:30:03Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/107476
1525,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow storage() to work on python tensor subclasses, but error on future data accesses)ï¼Œ å†…å®¹æ˜¯ (This was discussed in feedback from the original version of my ""reorder proxy/fake"" PR. This PR allows calls to `tensor.untyped_storage()` to **always** return a python storage object to the user. Previously, we would error loudly if we detected that the storage had a null dataptr. Instead, I updated the python bindings for the python storage methods that I saw involve data access, to throw an error later, only if you try to access those methods (e.g. `storage.data_ptr()` will now raise an error if the data ptr is null).   CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(Fix aliasing, metadata and edge case handling for FunctionalTensor, introduce some new wrapper tensor APIs)  CC(Add TorchDispatch version of functionalization)  CC(reorder proxy / fake modes so they always run last)  CC(better support for fakeifying and dynamoing through torch_dispatch subclasses (with dynamic shapes))  CC(allow result of at::for_blob to advertise as resizeable (for tensor subclasses))  CC(Allow storage() to work on python tensor subclasses, but error on future data accesses))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Allow storage() to work on python tensor subclasses, but error on future data accesses","This was discussed in feedback from the original version of my ""reorder proxy/fake"" PR. This PR allows calls to `tensor.untyped_storage()` to **always** return a python storage object to the user. Previously, we would error loudly if we detected that the storage had a null dataptr. Instead, I updated the python bindings for the python storage methods that I saw involve data access, to throw an error later, only if you try to access those methods (e.g. `storage.data_ptr()` will now raise an error if the data ptr is null).   CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(Fix aliasing, metadata and edge case handling for FunctionalTensor, introduce some new wrapper tensor APIs)  CC(Add TorchDispatch version of functionalization)  CC(reorder proxy / fake modes so they always run last)  CC(better support for fakeifying and dynamoing through torch_dispatch subclasses (with dynamic shapes))  CC(allow result of at::for_blob to advertise as resizeable (for tensor subclasses))  CC(Allow storage() to work on python tensor subclasses, but error on future data accesses)",2023-08-17T22:48:41Z,Merged release notes: python_frontend,closed,0,0,https://github.com/pytorch/pytorch/issues/107417
2026,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(better support for fakeifying and dynamoing through torch_dispatch subclasses (with dynamic shapes))ï¼Œ å†…å®¹æ˜¯ (There is already some support for plumbing `__torch_dispatch__` tensor subclasses through dynamo, but this PR beefs it up a bit and adds a test. In particular: (1) Fakeifying tensor subclasses didn't properly set autograd metadata (requires_grad, is_leaf) on the newly fakeified wrapper subclass. I don't actually have a test for this in this PR, but it's tested pretty heavily later in my aot autograd tests (2) Fakeifying tensor subclasses didn't properly track source information for dynamic shapes on the inner tensors. I added a new `WrapperSubclassFieldSource` subclass, that represents a source coming from a tensor field on a wrapper subclass, which I use in the fakeifying logic, and again in symbolic_shapes.py to generate proper guards. (3) `_make_wrapper_subclass()` marginally updated this code to work better with dynamic shapes. One thing that's a bit weird about `_make_wrapper_subclass`: it has two overloads, and the first explicitly does not support dynamic shapes (and the second.. does not support kwargs). I think that later we probably want to consolidate / at least make the first overload work with dynamic shapes, but I didn't want to handle that in this PR (so these smaller changes seemed like a strict improvement).   CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(python functionalization: add helpers, functionalize_sync and mirror_autograd_meta)  CC(Add TorchDispatch version of functionalization)  CC(wrapper subclasses: support noncpu device for dynamic shape overload)  CC(add dynamic shapes support for subclasses that overri)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,better support for fakeifying and dynamoing through torch_dispatch subclasses (with dynamic shapes),"There is already some support for plumbing `__torch_dispatch__` tensor subclasses through dynamo, but this PR beefs it up a bit and adds a test. In particular: (1) Fakeifying tensor subclasses didn't properly set autograd metadata (requires_grad, is_leaf) on the newly fakeified wrapper subclass. I don't actually have a test for this in this PR, but it's tested pretty heavily later in my aot autograd tests (2) Fakeifying tensor subclasses didn't properly track source information for dynamic shapes on the inner tensors. I added a new `WrapperSubclassFieldSource` subclass, that represents a source coming from a tensor field on a wrapper subclass, which I use in the fakeifying logic, and again in symbolic_shapes.py to generate proper guards. (3) `_make_wrapper_subclass()` marginally updated this code to work better with dynamic shapes. One thing that's a bit weird about `_make_wrapper_subclass`: it has two overloads, and the first explicitly does not support dynamic shapes (and the second.. does not support kwargs). I think that later we probably want to consolidate / at least make the first overload work with dynamic shapes, but I didn't want to handle that in this PR (so these smaller changes seemed like a strict improvement).   CC(torch.compile DTensor E2E)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(Update AOTAutograd to use FunctionalTensorMode instead of C++ functionalization)  CC(python functionalization: add helpers, functionalize_sync and mirror_autograd_meta)  CC(Add TorchDispatch version of functionalization)  CC(wrapper subclasses: support noncpu device for dynamic shape overload)  CC(add dynamic shapes support for subclasses that overri",2023-08-17T22:48:33Z,Merged ciflow/trunk release notes: fx module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/107415
539,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update HF version to commit hash (6c26faa))ï¼Œ å†…å®¹æ˜¯ (Some errors in the torchinductor hf benchmarks should be fixed in the most recent release (for example, this line no longer exists). Additionally, I landed a commit (6c26faa) to the HF transformers repro to fix one of the graph breaks. This PR results in 76% pass rate for the export + aot inductor HF benchmark! )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Update HF version to commit hash (6c26faa),"Some errors in the torchinductor hf benchmarks should be fixed in the most recent release (for example, this line no longer exists). Additionally, I landed a commit (6c26faa) to the HF transformers repro to fix one of the graph breaks. This PR results in 76% pass rate for the export + aot inductor HF benchmark! ",2023-08-17T18:33:50Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,10,https://github.com/pytorch/pytorch/issues/107400,TestMultiProc::test_hf_bert_ddp_inductor failure looks real. I wonder how large accuracy difference it is.,"> TestMultiProc::test_hf_bert_ddp_inductor failure looks real. I wonder how large accuracy difference it is.  , can we mark this test as xfail and then have someone to look into the problem?","if there is some urgency around unblocking the HF upgrade then sure, but ideally fix before land rather than xfail.", merge i," Merge started Your change will be merged while ignoring the following 3 checks: pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 2, linux.2xlarge, unstable), pull / linuxfocalpy3.11clang10 / test (dynamo, 1, 2, linux.2xlarge, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (inductor, 1, 1, linux.g5.4xlarge.nvidia.gpu, unstable) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","The merge job was canceled. If you believe this is a mistake, then you can re trigger it through pytorchbot.", merge i," Merge started Your change will be merged while ignoring the following 3 checks: pull / linuxfocalpy3.8clang10 / test (dynamo, 1, 2, linux.2xlarge, unstable), pull / linuxfocalpy3.11clang10 / test (dynamo, 1, 2, linux.2xlarge, unstable), inductor / cuda12.1py3.10gcc9sm86 / test (inductor, 1, 1, linux.g5.4xlarge.nvidia.gpu, unstable) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",There's a lot of new graph breaks as shown in https://github.com/pytorch/pytorch/pull/109148 :( Why do we need this bump right now? ,"I am going to kick off 2 runs, with and without this PR. "
1014,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(benchmark: convert output of fp64 to torch.float64)ï¼Œ å†…å®¹æ˜¯ (This PR adds converting the output of fp64 to torch.float64 before checking for accuracy. Why we need this change? For llama of torchbench, it converts output to float before returning it. https://github.com/pytorch/benchmark/blob/bad4e9ac19852f320c0d21e97f526e0c2838633e/torchbenchmark/models/llama/model.pyL241 While in the correctness checker, it will not compare the res results with fp64_ref if the fp64_ref.dtype is not torch.float64. So llama fails the accuracy check in the lowprecision case, even though res is closer to fp64_ref than ref. https://github.com/pytorch/pytorch/blob/e108f33299e4ea8fd39a1a81cf5ba6f3b509b6cb/torch/_dynamo/utils.pyL1025   CC(benchmark: higher tolerance for RobertaForQuestionAnswering)  CC(benchmark: convert output of fp64 to torch.float64) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,benchmark: convert output of fp64 to torch.float64,"This PR adds converting the output of fp64 to torch.float64 before checking for accuracy. Why we need this change? For llama of torchbench, it converts output to float before returning it. https://github.com/pytorch/benchmark/blob/bad4e9ac19852f320c0d21e97f526e0c2838633e/torchbenchmark/models/llama/model.pyL241 While in the correctness checker, it will not compare the res results with fp64_ref if the fp64_ref.dtype is not torch.float64. So llama fails the accuracy check in the lowprecision case, even though res is closer to fp64_ref than ref. https://github.com/pytorch/pytorch/blob/e108f33299e4ea8fd39a1a81cf5ba6f3b509b6cb/torch/_dynamo/utils.pyL1025   CC(benchmark: higher tolerance for RobertaForQuestionAnswering)  CC(benchmark: convert output of fp64 to torch.float64) ",2023-08-17T10:46:53Z,open source Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/107375, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
681,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DataParallel scatter method split tensor wrong)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I use dataparallel with two gpus. And I found half of tensors changed to zero tensors after scatter method.  Debugpy will step into forward method in torch.nn.parallel.data_parallel.py. Code is here.  After ""inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)"", kwargs is split into two parts. !image So the bug is that the second kwargs scattered into cuda:1 changed into zero tensors.  Versions collect_env.py output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DataParallel scatter method split tensor wrong," ğŸ› Describe the bug I use dataparallel with two gpus. And I found half of tensors changed to zero tensors after scatter method.  Debugpy will step into forward method in torch.nn.parallel.data_parallel.py. Code is here.  After ""inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)"", kwargs is split into two parts. !image So the bug is that the second kwargs scattered into cuda:1 changed into zero tensors.  Versions collect_env.py output  ",2023-08-17T10:31:31Z,module: cuda triaged module: data parallel,open,0,1,https://github.com/pytorch/pytorch/issues/107374,Are you able to do a simple program that transfers a cuda tensor from cuda0 to cuda1?
1966,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation Fault when importing torch)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Segmentation fault when importing torch. torch has been installed with `pip` in a `conda` virtual environment. All other packages were installed with `pip`, installed libraries are as follows:  accelerate==0.21.0  bitsandbytes==0.41.1  huggingfacehub==0.16.4  kaggle==1.5.16  langchain==0.0.266  tokenizers==0.13.3  transformers  out:  running `python q X faulthandler c ""import torch""` gives following output:  running  gives following stack traces: link to stack traces PS. this error occurred suddenly when I reran my script, I have tried to clear the `pycache` files but it did not do any favor. PS2. I think this error would be gone with creating a new venv and installing pytorch in it. However, I'm looking for a better solution.  Versions PyTorch version: N/A (because it throw segfault, currently 2.0.1 has been installed) Is debug build: N/A CUDA used to build PyTorch: N/A (11.7 has been installed) ROCM used to build PyTorch: N/A  OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.27 Python version: 3.9.17 (main, Jul  5 2023, 20:41:20)  [GCC 11.2.0] (64bit runtime) Python platform: Linux4.15.0202genericx86_64withglibc2.27 Is CUDA available: N/A CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080 Nvidia driver version: 470.161.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              8 Online CPU(s) list: 07 Thread(s) per core:  2 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Segmentation Fault when importing torch," ğŸ› Describe the bug Segmentation fault when importing torch. torch has been installed with `pip` in a `conda` virtual environment. All other packages were installed with `pip`, installed libraries are as follows:  accelerate==0.21.0  bitsandbytes==0.41.1  huggingfacehub==0.16.4  kaggle==1.5.16  langchain==0.0.266  tokenizers==0.13.3  transformers  out:  running `python q X faulthandler c ""import torch""` gives following output:  running  gives following stack traces: link to stack traces PS. this error occurred suddenly when I reran my script, I have tried to clear the `pycache` files but it did not do any favor. PS2. I think this error would be gone with creating a new venv and installing pytorch in it. However, I'm looking for a better solution.  Versions PyTorch version: N/A (because it throw segfault, currently 2.0.1 has been installed) Is debug build: N/A CUDA used to build PyTorch: N/A (11.7 has been installed) ROCM used to build PyTorch: N/A  OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.27.2 Libc version: glibc2.27 Python version: 3.9.17 (main, Jul  5 2023, 20:41:20)  [GCC 11.2.0] (64bit runtime) Python platform: Linux4.15.0202genericx86_64withglibc2.27 Is CUDA available: N/A CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080 Nvidia driver version: 470.161.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              8 Online CPU(s) list: 07 Thread(s) per core:  2 ",2023-08-17T09:54:37Z,module: crash triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/107371,can you run it under gdb and report the trace there?,> can you run it under gdb and report the trace there? the report can be found in this gist  ,I don't have much to suggest besides reinstalling your env. Maybe reinstalling torch will help.,"Reinstalling environment with all the packages helped. However, I was looking for something more robust because this happened several times for me but for now I will close the issue.   ","almost the same bug report with Current thread 0x00007fe5a306f740 (most recent call first):   File """", line 228 in _call_with_frames_removed   File """", line 1108 in create_module   File """", line 565 in module_from_spec   File """", line 666 in _load_unlocked   File """", line 986 in _find_and_load_unlocked   File """", line 1007 in _find_and_load   File ""/home/ai/anaconda3/envs/tao_torch/lib/python3.9/sitepackages/torch/__init__.py"", line 235 in    File """", line 228 in _call_with_frames_removed   File """", line 790 in exec_module   File """", line 680 in _load_unlocked   File """", line 986 in _find_and_load_unlocked   File """", line 1007 in _find_and_load   File """", line 1 in ",Try recreating your venv with desired version of pytorch.    
381,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor][cpu] torchbench model llama perf regression on 2023_08_13 nightly release)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug tracked in  CC(TorchInductor CPU Performance Dashboard)issuecomment1678596899         name / )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[Inductor][cpu] torchbench model llama perf regression on 2023_08_13 nightly release, ğŸ› Describe the bug tracked in  CC(TorchInductor CPU Performance Dashboard)issuecomment1678596899         name / ,2023-08-17T02:54:02Z,triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/107351,The latest code seems to fix this performance regression and get the same speed up as the ref data.
957,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Implement `polar` via metal shader)ï¼Œ å†…å®¹æ˜¯ (Use `view_as_real` to cast complex into a pair of floats and then it becomes just another binary operator. Enable `polar` and `view_as_complex` consistency tests, but skip `test_output_grad_match_polar_cpu` as `mul` operator is yet not supported Remove redundant `ifdef __OBJC__` and capture and rethrow exceptions captured during `createCacheBlock` block.  Fixes  CC(MPS dispatch blocks often fail to propagate C++ exceptions back to the caller) TODOs(in followup PRs):    Implement backwards (requires complex mul and sgn)    Measure the perf impact of computing the strides on the fly rather than ahead of time (unrelated to this PR) Partially addresses  CC(Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[MPS] Implement `polar` via metal shader,"Use `view_as_real` to cast complex into a pair of floats and then it becomes just another binary operator. Enable `polar` and `view_as_complex` consistency tests, but skip `test_output_grad_match_polar_cpu` as `mul` operator is yet not supported Remove redundant `ifdef __OBJC__` and capture and rethrow exceptions captured during `createCacheBlock` block.  Fixes  CC(MPS dispatch blocks often fail to propagate C++ exceptions back to the caller) TODOs(in followup PRs):    Implement backwards (requires complex mul and sgn)    Measure the perf impact of computing the strides on the fly rather than ahead of time (unrelated to this PR) Partially addresses  CC(Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators)",2023-08-16T20:42:37Z,Merged ciflow/trunk release notes: mps ciflow/mps,closed,1,3,https://github.com/pytorch/pytorch/issues/107324, does it solve the LLM case for you?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
811,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Patch release before v2.1.0?)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi PyTorch team ğŸ‘‹  we've run into a bug in our tool Casanovo where `NaN`s would randomly appear in the output of our Transformer layers only with `torch.no_grad()` (https://github.com/NobleLab/casanovo/issues/186). The issue exists as of v2.0.1, but as far as we can tell has been fixed in the nightly builds. Are there plans for a new patch release before v2.1.0, which looks like it is currently scheduled for October? If you'd like, I'm also happy to document the bug further, but I suspect it has been addressed as a at least a subset of another issue. Thanks!  Versions N/A )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Patch release before v2.1.0?," ğŸ› Describe the bug Hi PyTorch team ğŸ‘‹  we've run into a bug in our tool Casanovo where `NaN`s would randomly appear in the output of our Transformer layers only with `torch.no_grad()` (https://github.com/NobleLab/casanovo/issues/186). The issue exists as of v2.0.1, but as far as we can tell has been fixed in the nightly builds. Are there plans for a new patch release before v2.1.0, which looks like it is currently scheduled for October? If you'd like, I'm also happy to document the bug further, but I suspect it has been addressed as a at least a subset of another issue. Thanks!  Versions N/A ",2023-08-16T16:44:46Z,module: binaries triaged release notes: releng,closed,0,3,https://github.com/pytorch/pytorch/issues/107303," can you please share a reproducer, as well as whether issues happens on CUDA, cpu or MPS, using eager mode or torch.compile feature? This will help us determine severity of the issue and whether or not it warrants patch release (though to be frank, most likely it is not, at this point)",Hi   you can find the torchonly reproducer in this colab notebook where we produce a NaN output from a nonNaN input and a single `TransformerEncoderLayer` initialized with nonNaN weights. The issue happens only on CUDA and not on cpu. We suspect that Fast path calculation in `forward()` might be the culprit. Thanks!,"Considering that 2.1.0 is just a few weeks away, closing this one. (One can try/evaluate v2.1.0 release candidate by running `pip install torch==2.1.0 extraindexurl https://download.pytorch.org/whl/test/cu121` for example) Please note, that with v2.1.0, plans is to have 2 patch releases before v2.2.0. "
457,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(WAR by avoid querying device before env mutation)ï¼Œ å†…å®¹æ˜¯ (  CC(WAR by avoid querying device before env mutation) We should probably fix  CC(RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED) properly but this works around the problem Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,WAR by avoid querying device before env mutation,  CC(WAR by avoid querying device before env mutation) We should probably fix  CC(RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED) properly but this works around the problem Signedoffby: Edward Z. Yang  ,2023-08-16T15:48:49Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/107301, merge i," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1010,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('MPS' training Issue(s) with NanoGPT: -Inf, NaN's)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I train Karpathy's NanoGPT model, setting device='mps,', after ~2500 iterations the weight's get corrupted with Inf, NaNs.  If I reduce the blocksize to 4, the model trains without errors. The model trains without errors when device='cpu' and blocksize doesn't matter. https://github.com/karpathy/nanoGPT This is running on MacOS 13.5 on a 2021 iMac 27"" with an AMD Radeon Pro 5700 XT GPU. The Inf seem to start in this layer:  model.py:        torch.autograd.set_detect_anomaly(True) dbl 3/14/23 Is there a way to configure 'MPS' to generate trace logs of resource usage to see it's it's an 'MPS' resource exhaustion issue? I reported this previously on the Pytorch forum. https://discuss.pytorch.org/t/trainingissuesinmpsinfandnan/175244   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,"'MPS' training Issue(s) with NanoGPT: -Inf, NaN's"," ğŸ› Describe the bug When I train Karpathy's NanoGPT model, setting device='mps,', after ~2500 iterations the weight's get corrupted with Inf, NaNs.  If I reduce the blocksize to 4, the model trains without errors. The model trains without errors when device='cpu' and blocksize doesn't matter. https://github.com/karpathy/nanoGPT This is running on MacOS 13.5 on a 2021 iMac 27"" with an AMD Radeon Pro 5700 XT GPU. The Inf seem to start in this layer:  model.py:        torch.autograd.set_detect_anomaly(True) dbl 3/14/23 Is there a way to configure 'MPS' to generate trace logs of resource usage to see it's it's an 'MPS' resource exhaustion issue? I reported this previously on the Pytorch forum. https://discuss.pytorch.org/t/trainingissuesinmpsinfandnan/175244   Versions  ",2023-08-16T12:55:24Z,triaged module: mps,open,0,0,https://github.com/pytorch/pytorch/issues/107294
593,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(model.forward() get error with torch.compile() when using huggingface llama)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Found that the torch.compile(model) lead to ""Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'."" error when learning alpacalora. Here is a minimal repro. When I commented out the torch.compile(), things are good.  The complete output   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,model.forward() get error with torch.compile() when using huggingface llama," ğŸ› Describe the bug Found that the torch.compile(model) lead to ""Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'."" error when learning alpacalora. Here is a minimal repro. When I commented out the torch.compile(), things are good.  The complete output   Versions  ",2023-08-15T23:28:14Z,triaged oncall: pt2 module: dynamo,closed,0,14,https://github.com/pytorch/pytorch/issues/107269, would you happen to know if there is a smaller variant of the model that still repros the issue?,"Hi, thanks for your reply. I'll try on 7b now.",  I've tried that it also fails on huggyllama/llama7b,"Try it on a nightly, we fixed a lot of these on main.",Tried nightly. Still has same bug., could you take a look at this one or reassign?,"Hi, Did you solve the problem? I think this might related to the incompatibility between qlora and torch.compile(). I finetuned huggingface llama with Lora and torch_compile, and it works.  However, when I tried qlora, I had the same error as you showed.","Hi, Teeemio. I've not solved this question since I'm now a torch professor. This issue seems to be forgot by them for a long time.",I see.. They said that inputs to the compiler are now fake tensors. I think there may be some issues between model wrapping and input since this error happens when dealing with the input embedding which is the very start of the model.," Hey, can you help us with this issue?","I tried running the repro on the issue with a nightly from yesterday. There are a few graph breaks and recompiles, but it runs properly for me. If you're still hitting this error, are you hitting it with the same repro linked in the issue?","I also face a similar issue, and it seems that this bug may be caused by the CrossEntropyLoss and the view of tensor.", do you have a repro for your problem? Feel file to file a fresh issue,Closing this because of staleness but feel free to file a fresh issue.
919,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Add huggingface models into CI tests)ï¼Œ å†…å®¹æ˜¯ (  CC([ONNX] Support nonpersistent buffers in FakeMode export)  CC([ONNX] Add huggingface models into CI tests) 1. Add a list of HF models to CI tests. The PR intends to build them from Config, but some of them are not supported with Config. NOTE: Loaded from pretrained model could potentially hit uint8/bool conflict when a newer version of transformers is used.      Dolly has torch.fx.Node in OnnxFunction attribute, which is currently not supported.      Falcon and MPT has unsupported user coding to Dynamo. 2. Only update GPT2 exporting with real tensor to Config, as FakeMode rises unequal input errors between PyTorch and ORT. The reason is that nonpersistent buffer is not supported))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Add huggingface models into CI tests,"  CC([ONNX] Support nonpersistent buffers in FakeMode export)  CC([ONNX] Add huggingface models into CI tests) 1. Add a list of HF models to CI tests. The PR intends to build them from Config, but some of them are not supported with Config. NOTE: Loaded from pretrained model could potentially hit uint8/bool conflict when a newer version of transformers is used.      Dolly has torch.fx.Node in OnnxFunction attribute, which is currently not supported.      Falcon and MPT has unsupported user coding to Dynamo. 2. Only update GPT2 exporting with real tensor to Config, as FakeMode rises unequal input errors between PyTorch and ORT. The reason is that nonpersistent buffer is not supported)",2023-08-15T18:42:23Z,module: onnx open source Merged ciflow/trunk release notes: onnx topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/107247,"Remember to create issues against proper repos (pytorch, hf, onnx, or ort) for the errors/problems encountered during the test.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
758,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Relax not exist assertion for 'register_pytree_node')ï¼Œ å†…å®¹æ˜¯ (  CC([ONNX] Relax not exist assertion for 'register_pytree_node')  CC([ONNX] Set 'Generic[Diagnostic]' as base class for 'DiagnosticContext')  CC([ONNX] Fix diagnostic log and add unittest)  CC([ONNX] Public diagnostic options for 'dynamo_export') To not conflict with potential existing workaround or solution outside of exporter. Latest huggingface/transformers main (>4.31) patches PyTorch PyTree with support over `ModelOutput` class. `_PyTreeExtensionContext` is kept to support prior versions of transformers.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Relax not exist assertion for 'register_pytree_node',  CC([ONNX] Relax not exist assertion for 'register_pytree_node')  CC([ONNX] Set 'Generic[Diagnostic]' as base class for 'DiagnosticContext')  CC([ONNX] Fix diagnostic log and add unittest)  CC([ONNX] Public diagnostic options for 'dynamo_export') To not conflict with potential existing workaround or solution outside of exporter. Latest huggingface/transformers main (>4.31) patches PyTorch PyTree with support over `ModelOutput` class. `_PyTreeExtensionContext` is kept to support prior versions of transformers.,2023-08-15T17:55:26Z,open source Merged ciflow/trunk release notes: onnx topic: improvements,closed,0,5,https://github.com/pytorch/pytorch/issues/107245, merge, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
608,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Fix memory leak when exporting models)ï¼Œ å†…å®¹æ˜¯ (This commit fixes a memory leak caused by creating a new PyListObject using PyDict_Items() and not releasing that list later. This often prevented the entire model from being deallocated even when all python references to it have gone out of scope. Here is a repro script:  Results with this commit:  With this commit:  Fixes CC(Python memory leak in torch.onnx.export of HF GPT2))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Fix memory leak when exporting models,This commit fixes a memory leak caused by creating a new PyListObject using PyDict_Items() and not releasing that list later. This often prevented the entire model from being deallocated even when all python references to it have gone out of scope. Here is a repro script:  Results with this commit:  With this commit:  Fixes CC(Python memory leak in torch.onnx.export of HF GPT2),2023-08-15T17:55:08Z,open source Merged ciflow/trunk release notes: jit release notes: onnx topic: bug fixes,closed,0,7,https://github.com/pytorch/pytorch/issues/107244," label ""topic: not user facing""","Looks like a bunch of unrelated errors, I will wait for them to be resolved.", merge," Merge failed **Reason**: Approval needed from one of the following: hhbyyh, itang00, digantdesai, weifengpy, mikekgfb, ... Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ",, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2026,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `BertForMaskedLM`, `BertForQuestionAnswering`, `CamemBert`, `RobertaForCausalLM`, `RobertaForQuestionAnswering` in HF fail with `Constraints violated` using transformers `v4.31.0` but work well with transformers `4.30.2`. The failure is caused by this change in transformers: https://github.com/huggingface/transformers/pull/24510 and more exactly this function: warn_if_padding_and_no_attention_mask. When running the benchmark, the batch dim is marked as dynamic here. But due to the HF model change of adding warn_if_padding_and_no_attention_mask, the batch dim in the generated code becomes a Constant instead of a dynamic dim, which has ended up with this failure:   Minified repro You could change `BertForMaskedLM` in the below line to other model names to reproduce the corresponding failures. You'll need to install transformers `v4.31.0` to reproduce the failure.   Versions Collecting environment information... PyTorch version: 2.1.0a0+git236eda4 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.24.3 Libc version: glibc2.31 Python version: 3.8.13 (default, Oct 21 2022, 23:50:54)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.078genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0," ğŸ› Describe the bug `BertForMaskedLM`, `BertForQuestionAnswering`, `CamemBert`, `RobertaForCausalLM`, `RobertaForQuestionAnswering` in HF fail with `Constraints violated` using transformers `v4.31.0` but work well with transformers `4.30.2`. The failure is caused by this change in transformers: https://github.com/huggingface/transformers/pull/24510 and more exactly this function: warn_if_padding_and_no_attention_mask. When running the benchmark, the batch dim is marked as dynamic here. But due to the HF model change of adding warn_if_padding_and_no_attention_mask, the batch dim in the generated code becomes a Constant instead of a dynamic dim, which has ended up with this failure:   Minified repro You could change `BertForMaskedLM` in the below line to other model names to reproduce the corresponding failures. You'll need to install transformers `v4.31.0` to reproduce the failure.   Versions Collecting environment information... PyTorch version: 2.1.0a0+git236eda4 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.24.3 Libc version: glibc2.31 Python version: 3.8.13 (default, Oct 21 2022, 23:50:54)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.078genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:     ",2023-08-15T01:59:42Z,triaged oncall: pt2 module: dynamic shapes module: dynamo,open,0,10,https://github.com/pytorch/pytorch/issues/107200,I think we should fix this in huggingface. I'll file them a bug.,"Interestingly, on my copy of PyTorch I get `numel: integer multiplication overflow` instead","> Interestingly, on my copy of PyTorch I get `numel: integer multiplication overflow` instead May I know which Pytorch commit you're using?",I think it was ed0782125a5a768e098d2a3f59c792db0ec5dc15,Similar error message in another 5 models tracked  CC(TorchInductor CPU Performance Dashboard)issuecomment1678363694 doctr_det_predictor  hf_Bert hf_Bert_large hf_T5_generate  llamaï¼ˆonly in multithreadsï¼‰,> I think it was ed07821 Oh that's weird since I tried with PyTorch of this commit + transformers `4.31.0` and still got `torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated!`. Let me try to see if anything else in the environment could lead to this difference.,Local repro cmd: ,I think it is best to fix this in HF,"We could fix it in Dynamo with  CC([Dynamo] Translate ""value in tensor"" into something more efficient)","OK so it turns out the specialization actually has nothing to do with what I originally thought it was (`tok in tensor`) test, and actually it's just a problem with the benchmarking setup. Repro command from Bin gives:  so `batch_size` is constant.  This is because we had a graph break and on resumption, `batch_size` was an input integer, and so we don't manage to tag it as dynamic. The ""right"" fix is to make us not graph break but this cannot easily be done "
426,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([opinfo] Add cases to slice_scatter to improve coverage)ï¼Œ å†…å®¹æ˜¯ (`slice_scatter` accepts `end > L` and negative dim. Previously we did not have test cases for them but they come up in real uses. Based on https://github.com/microsoft/onnxscript/pull/995)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[opinfo] Add cases to slice_scatter to improve coverage,`slice_scatter` accepts `end > L` and negative dim. Previously we did not have test cases for them but they come up in real uses. Based on https://github.com/microsoft/onnxscript/pull/995,2023-08-14T12:20:29Z,triaged open source Stale topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/107134,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1155,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Revert ""[AOTInductor] Remove call to aot_autograd when receiving ExportedProgram"")ï¼Œ å†…å®¹æ˜¯ (  CC(Revert ""[AOTInductor] Remove call to aot_autograd when receiving ExportedProgram"") This reverts CC([AOTInductor] Remove call to aot_autograd when receiving ExportedProgram). There are two main problems with that PR: 1) It is not calling TorchInductor FX passes.  Most notably pre_grad_passes that need to run *before* AotAutograd, and also joint_graph_passes that need to run using a custom partition function in AotAutograd. It is not using the inductorspecific partitioning logic in AotAutograd. 2) People are hitting issues (I added  to the chat with more details) where they are calling TorchInductor with the wrong decomp table.  Inductor relies on a different set of decomps that `export()` and will break in nasty ways if the decomps don't match.  If we are going to break things out into multiple steps, we need much more strict checks that the correct decomps are being used. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,"Revert ""[AOTInductor] Remove call to aot_autograd when receiving ExportedProgram""","  CC(Revert ""[AOTInductor] Remove call to aot_autograd when receiving ExportedProgram"") This reverts CC([AOTInductor] Remove call to aot_autograd when receiving ExportedProgram). There are two main problems with that PR: 1) It is not calling TorchInductor FX passes.  Most notably pre_grad_passes that need to run *before* AotAutograd, and also joint_graph_passes that need to run using a custom partition function in AotAutograd. It is not using the inductorspecific partitioning logic in AotAutograd. 2) People are hitting issues (I added  to the chat with more details) where they are calling TorchInductor with the wrong decomp table.  Inductor relies on a different set of decomps that `export()` and will break in nasty ways if the decomps don't match.  If we are going to break things out into multiple steps, we need much more strict checks that the correct decomps are being used. ",2023-08-13T20:39:34Z,ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/107103, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/jansel/162/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/107103`)"
297,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for GET_YIELD_FROM_ITER, YIELD_FROM, SEND)ï¼Œ å†…å®¹æ˜¯ (  CC(Add support for GET_YIELD_FROM_ITER, YIELD_FROM, SEND) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Add support for GET_YIELD_FROM_ITER, YIELD_FROM, SEND","  CC(Add support for GET_YIELD_FROM_ITER, YIELD_FROM, SEND) ",2023-08-10T19:42:56Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/106986," merge f ""CI passes, 3 unrelated failures"" "," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 2591d17ce2322ed412b850bc08e5d01ca0fc5c26` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
625,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Python memory leak in torch.onnx.export of HF GPT2)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Network memory cannot be reclaimed by python after `torch.onnx.export` of a HF transformers GPT2 model.  Expected behavior: `del model` and `gc.collect()` after `torch.onnx.export` should free up all resources tied to the model (namely its weights). This is reproducible with larger 6.7B models as well where `torch.onnx.export` leaks a more problematic 26GB.   Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Python memory leak in torch.onnx.export of HF GPT2, ğŸ› Describe the bug Network memory cannot be reclaimed by python after `torch.onnx.export` of a HF transformers GPT2 model.  Expected behavior: `del model` and `gc.collect()` after `torch.onnx.export` should free up all resources tied to the model (namely its weights). This is reproducible with larger 6.7B models as well where `torch.onnx.export` leaks a more problematic 26GB.   Versions ,2023-08-10T16:50:52Z,module: onnx module: memory usage triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/106976,"Still occurs in today's nightly, 2.1.0.dev20230810+cu118"
328,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add torch/_C/_onnx.pyi to ONNX exporter's .github/merge_rules.yaml)ï¼Œ å†…å®¹æ˜¯ (Adds `orch/_C/_onnx.pyi` to `ONNX exporter` team at `.github/merge_rules.yaml`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add torch/_C/_onnx.pyi to ONNX exporter's .github/merge_rules.yaml,Adds `orch/_C/_onnx.pyi` to `ONNX exporter` team at `.github/merge_rules.yaml`,2023-08-10T15:48:51Z,module: onnx module: infra topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/106975,Dupe of https://github.com/pytorch/pytorch/pull/106927
548,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor][cpu] gptj6B & llama model perf regression)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Result 1: use_dynamo = True Model  0.76,1.45,0.65,0.65,0.65,    For reproduce: bash env_prepare.sh bfloat16 llm_bench_spr scripts  Versions Pytorch: 	4734e4d transformers: 	4.31.0 TORCH_VISION: 	ffdb719 TORCH_TEXT: 	3f2c002 TORCH_AUDIO: 	ed89176 TORCH_DATA: 	76bcef9 TORCH_BENCH: 	0b7147fd )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Inductor][cpu] gptj6B & llama model perf regression," ğŸ› Describe the bug   Result 1: use_dynamo = True Model  0.76,1.45,0.65,0.65,0.65,    For reproduce: bash env_prepare.sh bfloat16 llm_bench_spr scripts  Versions Pytorch: 	4734e4d transformers: 	4.31.0 TORCH_VISION: 	ffdb719 TORCH_TEXT: 	3f2c002 TORCH_AUDIO: 	ed89176 TORCH_DATA: 	76bcef9 TORCH_BENCH: 	0b7147fd ",2023-08-10T04:07:47Z,triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/106945,Fixed by https://github.com/pytorch/pytorch/pull/107123.
280,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `_onnx.pyi` to ONNX merge rules)ï¼Œ å†…å®¹æ˜¯ (Followup after https://github.com/pytorch/pytorch/pull/106379)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add `_onnx.pyi` to ONNX merge rules,Followup after https://github.com/pytorch/pytorch/pull/106379,2023-08-10T00:32:55Z,Merged topic: not user facing,closed,1,2,https://github.com/pytorch/pytorch/issues/106927," merge f ""Lint is green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
805,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Model Test List for FX-to-ONNX Exporters for FakeTensorMode and dynamic=True)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Some test candidates to consider  LLaMA: https://huggingface.co/metallama/Llama27b or https://huggingface.co/openlmresearch/open_llama_7b_v2  Whisper: https://huggingface.co/openai/whisperlargev2  FlanT5: https://huggingface.co/google/flant5xxl  Dolly: https://huggingface.co/databricks/dollyv212b  Falcon: https://huggingface.co/tiiuae/falcon40b  MPT: https://huggingface.co/mosaicml/mpt7b  Bloom: https://huggingface.co/bigscience/bloom  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[ONNX] Model Test List for FX-to-ONNX Exporters for FakeTensorMode and dynamic=True," ğŸš€ The feature, motivation and pitch Some test candidates to consider  LLaMA: https://huggingface.co/metallama/Llama27b or https://huggingface.co/openlmresearch/open_llama_7b_v2  Whisper: https://huggingface.co/openai/whisperlargev2  FlanT5: https://huggingface.co/google/flant5xxl  Dolly: https://huggingface.co/databricks/dollyv212b  Falcon: https://huggingface.co/tiiuae/falcon40b  MPT: https://huggingface.co/mosaicml/mpt7b  Bloom: https://huggingface.co/bigscience/bloom  Alternatives _No response_  Additional context _No response_ ",2023-08-09T19:17:31Z,module: onnx triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/106897,I am working on `bigscience/bloom560m` now,Smaller ones  https://huggingface.co/openlmresearch/open_llama_3b_v2 (Done)  https://huggingface.co/openai/whispertiny (Done)  https://huggingface.co/google/flant5small (Done)  https://huggingface.co/databricks/dollyv23b (onnxscrip input issue)  https://huggingface.co/tiiuae/falcon7b (AssertionError: Mutating module attribute seq_len_cached during export.)  https://huggingface.co/mosaicml/mpt7b (torch._dynamo.exc.UserError: Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow)  https://huggingface.co/bigscience/bloom560m (Done),1. runtime needs to run small HF models. (Use others model) 2. no runtime needs to test regular size HF models. (Use config)
641,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo x FSDP][3/x] TypedStorage and storage_offset)ï¼Œ å†…å®¹æ˜¯ (  CC([FSDP][WIP] [Do not review] Trace FSDP)  CC([Dynamo x FSDP][5/x] Fix bug in __class__ sources)  CC([Dynamo x FSDP][4/x] Add support for params, buffers, submodules on FSDPManagedNNModuleVariable)  CC([Dynamo x FSDP][3/x] TypedStorage and storage_offset)  CC([Dynamo x FSDP][2/x] Small changes to distributed to make it dynamo friendly)  CC([Dynamo x FSDP][1/x] Builder support for deque, appendleft) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Dynamo x FSDP][3/x] TypedStorage and storage_offset,"  CC([FSDP][WIP] [Do not review] Trace FSDP)  CC([Dynamo x FSDP][5/x] Fix bug in __class__ sources)  CC([Dynamo x FSDP][4/x] Add support for params, buffers, submodules on FSDPManagedNNModuleVariable)  CC([Dynamo x FSDP][3/x] TypedStorage and storage_offset)  CC([Dynamo x FSDP][2/x] Small changes to distributed to make it dynamo friendly)  CC([Dynamo x FSDP][1/x] Builder support for deque, appendleft) ",2023-08-09T18:32:59Z,Stale module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/106888,Needs tests.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
717,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([forward-fix] Fix multigpu varying tensor optim tests)ï¼Œ å†…å®¹æ˜¯ (Forward fixes https://github.com/pytorch/pytorch/pull/106615 by increasing tolerance in the test. The capturable implementation for foreach simply varies due to a different order of operations when updating params. I had also attempted to compare against fp64 but that introduced more disparity in the other optimizer configs. It is worth trying the fp64 comparison at a later point, but let's get the test passing first.   CC([forwardfix] Fix multigpu varying tensor optim tests))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[forward-fix] Fix multigpu varying tensor optim tests,"Forward fixes https://github.com/pytorch/pytorch/pull/106615 by increasing tolerance in the test. The capturable implementation for foreach simply varies due to a different order of operations when updating params. I had also attempted to compare against fp64 but that introduced more disparity in the other optimizer configs. It is worth trying the fp64 comparison at a later point, but let's get the test passing first.   CC([forwardfix] Fix multigpu varying tensor optim tests)",2023-08-09T18:24:22Z,Merged ciflow/trunk release notes: foreach_frontend topic: not user facing ciflow/periodic,closed,0,2,https://github.com/pytorch/pytorch/issues/106887, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1996,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RPC all_gather doesn't work with dynamic world size (world_size=None))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The `rpc.api._all_gather(obj)` function doesn't work when the rpc cluster is initialized with `world_size=None`. Here is a test code that can reproduce the problem:  Output:  `Rank 0` only collects its own device count, the other ranks throw the `eof` error and doesn't exit. Specifying the `world_size` as `4` makes the code behave as expected.    Versions PyTorch version: 1.13.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0150genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10 GPU 1: NVIDIA A10 GPU 2: NVIDIA A10 GPU 3: NVIDIA A10 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                      )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,RPC all_gather doesn't work with dynamic world size (world_size=None)," ğŸ› Describe the bug The `rpc.api._all_gather(obj)` function doesn't work when the rpc cluster is initialized with `world_size=None`. Here is a test code that can reproduce the problem:  Output:  `Rank 0` only collects its own device count, the other ranks throw the `eof` error and doesn't exit. Specifying the `world_size` as `4` makes the code behave as expected.    Versions PyTorch version: 1.13.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0150genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10 GPU 1: NVIDIA A10 GPU 2: NVIDIA A10 GPU 3: NVIDIA A10 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                      ",2023-08-09T08:05:50Z,oncall: distributed module: rpc,open,0,0,https://github.com/pytorch/pytorch/issues/106851
1342,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multiprocess DataLoader doesn't work with sparse tensor as it'll try to access the underlying storage)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Summary When using `DataLoader` with multiprocess loading to load a dataset with sparse tensor elements, it'll try to access the underlying storage of the tensor, but sparse tensor (COO, CSF etc) doesn't support accessing storage. I've put the **minimal reproduction sample** in this colab notebook: https://colab.research.google.com/drive/16q_tzyUz5ylZSCcpzhJ52pxVSUpMZXMscrollTo=o0KeaWnVz9Hm&uniqifier=1   Case 1: default collation When using default collate (auto_collate=True here), to collate on a sparse tensor, it'll attempt to access the `elem._typed_storage` here, thus hitting error:   Case 2: Manual collation Without auto collation, (set `batch_size=None` so that it'll use `default_convert` method, OR provide a `collate_fn`) we get around the issue in default_collate in case 1; But later on when the worker process is feeding into the `worker_result_queue` the loaded data, it'll again attemp to access the underlying storage, thus hitting  So that anyway we cannot do multiprocess loading with sparse tensor.  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Multiprocess DataLoader doesn't work with sparse tensor as it'll try to access the underlying storage," ğŸ› Describe the bug  Summary When using `DataLoader` with multiprocess loading to load a dataset with sparse tensor elements, it'll try to access the underlying storage of the tensor, but sparse tensor (COO, CSF etc) doesn't support accessing storage. I've put the **minimal reproduction sample** in this colab notebook: https://colab.research.google.com/drive/16q_tzyUz5ylZSCcpzhJ52pxVSUpMZXMscrollTo=o0KeaWnVz9Hm&uniqifier=1   Case 1: default collation When using default collate (auto_collate=True here), to collate on a sparse tensor, it'll attempt to access the `elem._typed_storage` here, thus hitting error:   Case 2: Manual collation Without auto collation, (set `batch_size=None` so that it'll use `default_convert` method, OR provide a `collate_fn`) we get around the issue in default_collate in case 1; But later on when the worker process is feeding into the `worker_result_queue` the loaded data, it'll again attemp to access the underlying storage, thus hitting  So that anyway we cannot do multiprocess loading with sparse tensor.  Versions  ",2023-08-09T01:36:56Z,module: sparse module: dataloader triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/106837,hi! have you found a workaround?,"I am also running into this problem! It looks like there are similar limitations with nested tensors ( CC(`torch.nested.nested_tensor` does not work within multiprocess `DataLoader`)), however that is labelled as high priority. I think this is also important as it looks like an essential feature. In my workflow, I currently have to either convert the sparse tensors to dense (thus defeating the purpose of using them in the first place) or perform additional computations during training since these cannot be precomputed and stored. Thanks!"," as a workaround you can also try retrieving both the values and indices tensors from the sparse tensor in question (`t.values(), t.indices()`), pass/store them and reconstruct the sparse tensor from those!",+1. The workaround is fine but feels so unnecessary to do every single batch like this?,The PR https://github.com/pytorch/pytorch/pull/112842 fixes the case 1 by throwing  (because stacking of sparse tensors is currently not supported) and the case 2 by making it work.
944,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Make several ops unranked to avoid graph recompilation)ï¼Œ å†…å®¹æ˜¯ ( Made `bmm`, `cat`, `softmax`, `copy_cast` and `index_select` unranked. This improves the performance of LLMs and other networks, because these graphs would be invariant to shape changes, and therefore, they won't recompile.   Binary ops: Flatten tensors bigger than 4D to 1D, and reuse existing buffer for inplace operations (for offset 0).  Cache the `MPSGraphExecutable` in `MPSCachedGraph`. These executables have the typeInference disabled which should help with recompilation issues on MPSGraph.  Handle transposes in second batch of matrices in `bmm`. This helps with performance in case the second batch (i.e., `other` argument) was transposed (like the case with `QxK.t()` in Transformers). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[MPS] Make several ops unranked to avoid graph recompilation," Made `bmm`, `cat`, `softmax`, `copy_cast` and `index_select` unranked. This improves the performance of LLMs and other networks, because these graphs would be invariant to shape changes, and therefore, they won't recompile.   Binary ops: Flatten tensors bigger than 4D to 1D, and reuse existing buffer for inplace operations (for offset 0).  Cache the `MPSGraphExecutable` in `MPSCachedGraph`. These executables have the typeInference disabled which should help with recompilation issues on MPSGraph.  Handle transposes in second batch of matrices in `bmm`. This helps with performance in case the second batch (i.e., `other` argument) was transposed (like the case with `QxK.t()` in Transformers). ",2023-08-08T19:45:30Z,triaged open source Stale module: mps release notes: mps ciflow/mps,closed,0,6,https://github.com/pytorch/pytorch/issues/106813,"> Looks like this changes regresses behavior with channel last (likely due to the typo in BinaryOps.mm) >  > Also, please do not introduce new skips, as this sounds like a regression, nor remove XFAILS unless you expect them to be fixed somehow. It's not a regression. It preexisted and has nothing to do with Binary ops. I removed those changes from this PR as someone else already had landed them in another PR: https://github.com/pytorch/pytorch/blob/main/torch/testing/_internal/common_modules.pyL3409","> Looks like this changes regresses behavior with channel last (likely due to the typo in BinaryOps.mm) >  which test was regressing?  > Also, please do not introduce new skips, as this sounds like a regression, nor remove XFAILS unless you expect them to be fixed somehow. The softmin started passing with the unranked fix.  , Are you okay with the PR now ?","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",  should we revive this one?,"Now that we have testing we should. Though I want to land few changes that reenables more testing, as potentially breaking something for MacOS 14, before 15 is out is not good"," :x:  login:  / name: Ramin Azarmehr . The commit (4f0cc52948739e688da61f087fcfd3d2e73b4016, 7995fab55eca4d5ab566b4778fd2126d1fafdda6, 6965182ed5e6ef8f56e16e07ac3b68041e3e2db8, aa4fe430fb907aafb25c8a95a1f94e19f8d1f035) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."
1865,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Conv2d yields incorrect result using different batch size)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Conv2d yield incorrect result with different batch size with GPU (but correct with CPU), see the following code:  Output:  so far I find this bug only when input H == W == 64, haven't test for other configurations. Looking forward to solving this bug  Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Pro GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.11  (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Nvidia driver version: 516.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2904 DeviceID=CPU0 Family=205 L2CacheSize=1536 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2904 Name=Intel(R) Core(TM) i59400F CPU @ 2.90GHz ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] torch==2.0.1+cu117 [pip3] torchaudio==2.0.2+cu117 [pip3] torchvision==0.15.2+cu117 [conda] numpy                     1.24.4                   pypi_0    pypi [conda] torch                     2.0.1+cu117              pypi_0    pypi [conda] torchaudio                2.0.2+cu117              pypi_0    pypi [conda] torchvision               0.15.2+cu117             pypi_0    pypi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Conv2d yields incorrect result using different batch size," ğŸ› Describe the bug Conv2d yield incorrect result with different batch size with GPU (but correct with CPU), see the following code:  Output:  so far I find this bug only when input H == W == 64, haven't test for other configurations. Looking forward to solving this bug  Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Pro GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.11  (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Nvidia driver version: 516.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2904 DeviceID=CPU0 Family=205 L2CacheSize=1536 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2904 Name=Intel(R) Core(TM) i59400F CPU @ 2.90GHz ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.4 [pip3] torch==2.0.1+cu117 [pip3] torchaudio==2.0.2+cu117 [pip3] torchvision==0.15.2+cu117 [conda] numpy                     1.24.4                   pypi_0    pypi [conda] torch                     2.0.1+cu117              pypi_0    pypi [conda] torchaudio                2.0.2+cu117              pypi_0    pypi [conda] torchvision               0.15.2+cu117             pypi_0    pypi",2023-08-08T02:55:00Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/106749,"This bug appers in many version on Windows (torch2.0.1+cu117, torch1.12.1+cu116, torch1.11+cu113) and in certain version on Linux (torch2.0.0+cu118) as far as I tested",There's another that causes incorrect result:  output:  Tested on Linux + V100 + (torch1.9.0+cu111),https://discuss.pytorch.org/t/slightlydifferentresultsfordifferentbatchsizes/43458/4
643,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Hugging Face safetensor does not work with FakeTensorMode)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When certain HF models are loaded within Fake mode, the `load_state_dict` API tries to load data using `safetensors`, which fail to determine the tensor shape and crash  The error is:  Looking at the torch/csrc/utils/tensor_new.cpp code, I found this   Versions pytorch                         main branch () safetensors                   0.3.1 transformers                  4.30.0 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Hugging Face safetensor does not work with FakeTensorMode," ğŸ› Describe the bug When certain HF models are loaded within Fake mode, the `load_state_dict` API tries to load data using `safetensors`, which fail to determine the tensor shape and crash  The error is:  Looking at the torch/csrc/utils/tensor_new.cpp code, I found this   Versions pytorch                         main branch () safetensors                   0.3.1 transformers                  4.30.0 ",2023-08-07T22:46:02Z,triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,open,0,6,https://github.com/pytorch/pytorch/issues/106732,"I have proposed https://github.com/huggingface/safetensors/pull/318 to `safetensors`, but need to confirm whether this is the best way to detect a HF's safetensor checkpoint is being loaded within fake mode and whether using `torch.empty(tuple(shape_list), dtype=torch.dtype)` is the way to go to load a fakefied checkpoint",The PR seems a bit questionable. What underlying torch APIs are being used in get_tensor? Preferably we would fix whatever is broken there.,It' s running basically this: https://gist.github.com/Narsil/3edeec2669a5e94e4707aa0f901d2282 (With a bit of sugar to handle torch =2.0 and UntypedStorage related changes),"OK, so the key pain point is the Storage APIs, which are not torch dispatch overrideable. I propose we monkeypatch (either via TorchFunctionMode or directly).","> OK, so the key pain point is the Storage APIs, which are not torch dispatch overrideable. I propose we monkeypatch (either via TorchFunctionMode or directly). Thanks, folks. I am trying the `TorchFuncMode` approach. By logging `tryexcept` all `func` and handling the one that raises, I kind of found `torch.ops.aten.set_.source_Storage` as the culprit, so I patched it. It kind of makes sense given the issue is related to untype storage. However, now another `func == torch.ops.aten.select.int` raises and I don't think it should. By executing `func(*args, **kwargs)` within `pdb`, I got `*** IndexError: select(): index 0 out of range for tensor of size [0] at dimension 0` was raised. i guess that means the `torch.ops.aten.set_.source_Storage` patching wasn't good enough. Ideas? ","I'm not sure why your code is failing, but you need to fix ByteStorage.from_file to not create a real storage first, that's the first order of business."
466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enabling Transformer fast path for not batch_first (MHA, TE, TEL))ï¼Œ å†…å®¹æ˜¯ (Summary: The fast path for the `forward()` method in `MultiheadAttention`, `TE`, `TEL` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Differential Revision: D48095703 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Enabling Transformer fast path for not batch_first (MHA, TE, TEL)","Summary: The fast path for the `forward()` method in `MultiheadAttention`, `TE`, `TEL` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Differential Revision: D48095703 ",2023-08-05T20:20:52Z,oncall: distributed enhancement fb-exported Stale release notes: nn,closed,0,15,https://github.com/pytorch/pytorch/issues/106668,This pull request was **exported** from Phabricator. Differential Revision: D48095703,This pull request was **exported** from Phabricator. Differential Revision: D48095703,This pull request was **exported** from Phabricator. Differential Revision: D48095703," any suggestion how to handle the FSDP fail?  I think (i.e., conjecture) the underlying cause is a numerical difference between fastpath vs. standard execution  I think this is becawe avoided running into this so far because this test ran with batch_first=False, and until this diff we used the fastpath with batch_first=True only. (fastpath is only on for inference with no_grad(), so eval() vs eval+no_grad() exercises two different paths) cc: varma    ","missed this comment before, but it sounds like you've found out that its an issue of eval vs train accuracy for this model, is that right?",This pull request was **exported** from Phabricator. Differential Revision: D48095703,"> missed this comment before, but it sounds like you've found out that its an issue of eval vs train accuracy for this model, is that right? The underlying issue is that model.eval() vs model.eval() with no_grad() triggers different computational kernels.  The only way to control this in a sane way is via a backend context manager for choosing between these backends.  I added this in CC(Create fastpath backend context manager, similar to SDPA kernel backend manager)",This pull request was **exported** from Phabricator. Differential Revision: D48095703,"> missed this comment before, but it sounds like you've found out that its an issue of eval vs train accuracy for this model, is that right? Yep  the way that ""validation"" is performed is that the test runs with train mode, and with eval/no_grad  the latter triggers fastpath with the present bathc, but because we;re looking at different implementations, we can't expect bitexact answers. One possible solution is a context manager that gives more control over the kernel chosen, similar for what we do for SDPA  CC(Create fastpath backend context manager, similar to SDPA kernel backend manager) is an implementation of this context manager for backend selection",This pull request was **exported** from Phabricator. Differential Revision: D48095703,This pull request was **exported** from Phabricator. Differential Revision: D48095703,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",This pull request was **exported** from Phabricator. Differential Revision: D48095703,This pull request was **exported** from Phabricator. Differential Revision: D48095703,This pull request was **exported** from Phabricator. Differential Revision: D48095703
2002,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([docs] URL and link format proposal to make function page URLs more concise)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug These links end up copypasted in messengers for chat with colleagues, so making them have less bloat would be nice. If the links are concise, one can also do quick lookups without going to relatively slower loading https://pytorch.org/docs  e.g. by typing https://pytorch.org/docs/torch.div or https://pytorch.org/docs/div https://docs.pytorch.org/torch.div or https://docs.pytorch.org/div. These pages can also be autogenerated or symlinked for all function names at least for the torch/tensor functions (and such pages can list all functions with the same name e.g. torch.sigmoid, F.sigmoid which may be good by itself  CC([docs] torch.sigmoid to make clear equivalence relations to other sigmoid functions) or https://discuss.pytorch.org/t/variousquantizedquantizableintrinsicmodulespurpose/183562 for LinearReLU)  If we go to https://pytorch.org/docs/stable/torch.html, we have links like https://pytorch.org/docs/stable/generated/torch.div.htmltorch.div I'm proposing that instead these function links should be more concise and not include the hash. Going further, there should be symlinks https://pytorch.org/docs/torch.div.html or https://pytorch.org/docs/torch.div or even https://docs.pytorch.org/torch.div (and https://docs.pytorch.org/div) When doing simple search like `torch.div` we get:  Clicking the first hit will get us to https://pytorch.org/docs/stable/generated/torch.div.html?highlight=torch+divtorch.div Also, the highlight seems to have no effect now (as opposed to the Google's https://pytorch.org/docs/stable/generated/torch.div.html:~:text=torch.div https://stackoverflow.com/questions/62161819/whatexactlyisthetextlocationhashinanurl). So maybe worth dropping the highligh part from the Sphinx s)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,[docs] URL and link format proposal to make function page URLs more concise," ğŸ› Describe the bug These links end up copypasted in messengers for chat with colleagues, so making them have less bloat would be nice. If the links are concise, one can also do quick lookups without going to relatively slower loading https://pytorch.org/docs  e.g. by typing https://pytorch.org/docs/torch.div or https://pytorch.org/docs/div https://docs.pytorch.org/torch.div or https://docs.pytorch.org/div. These pages can also be autogenerated or symlinked for all function names at least for the torch/tensor functions (and such pages can list all functions with the same name e.g. torch.sigmoid, F.sigmoid which may be good by itself  CC([docs] torch.sigmoid to make clear equivalence relations to other sigmoid functions) or https://discuss.pytorch.org/t/variousquantizedquantizableintrinsicmodulespurpose/183562 for LinearReLU)  If we go to https://pytorch.org/docs/stable/torch.html, we have links like https://pytorch.org/docs/stable/generated/torch.div.htmltorch.div I'm proposing that instead these function links should be more concise and not include the hash. Going further, there should be symlinks https://pytorch.org/docs/torch.div.html or https://pytorch.org/docs/torch.div or even https://docs.pytorch.org/torch.div (and https://docs.pytorch.org/div) When doing simple search like `torch.div` we get:  Clicking the first hit will get us to https://pytorch.org/docs/stable/generated/torch.div.html?highlight=torch+divtorch.div Also, the highlight seems to have no effect now (as opposed to the Google's https://pytorch.org/docs/stable/generated/torch.div.html:~:text=torch.div https://stackoverflow.com/questions/62161819/whatexactlyisthetextlocationhashinanurl). So maybe worth dropping the highligh part from the Sphinx s",2023-08-05T11:42:07Z,module: docs triaged enhancement,open,0,1,https://github.com/pytorch/pytorch/issues/106664,"Btw gradio does this style of URLs: https://www.gradio.app/docs/checkbox, so for many cases we can directly navigate to the docs page without even typing search"
544,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Will torch.sparse.mm support multiplying two boolean matrices?)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I have two huge matrices. They are sparse Boolean matrices. I call torch.sparse.mm and found that it cannot be successful. Will the multiplication of bool sparse matrices be supported in the future?  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Will torch.sparse.mm support multiplying two boolean matrices?," ğŸš€ The feature, motivation and pitch I have two huge matrices. They are sparse Boolean matrices. I call torch.sparse.mm and found that it cannot be successful. Will the multiplication of bool sparse matrices be supported in the future?  Alternatives _No response_  Additional context _No response_ ",2023-08-05T09:52:28Z,module: sparse triaged module: boolean tensor,open,0,0,https://github.com/pytorch/pytorch/issues/106662
507,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable transformer.py fastpath for not batch_first for TE & TEL)ï¼Œ å†…å®¹æ˜¯ (Summary: TE  & TEL support for not batch_first transpose position of (variable) seqlen and batch position, prior to converting inputs to nested tensor, and undo the transformation after.  Also, in TElayer. Test Plan: sandcastle Differential Revision: D47989561)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Enable transformer.py fastpath for not batch_first for TE & TEL,"Summary: TE  & TEL support for not batch_first transpose position of (variable) seqlen and batch position, prior to converting inputs to nested tensor, and undo the transformation after.  Also, in TElayer. Test Plan: sandcastle Differential Revision: D47989561",2023-08-05T03:51:53Z,fb-exported Stale,closed,0,4,https://github.com/pytorch/pytorch/issues/106659," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.",This pull request was **exported** from Phabricator. Differential Revision: D47989561,"Likely abandon, because MHA and TE/TEL need to be handled in sync.  See CC(Enabling Transformer fast path for not batch_first (MHA, TE, TEL)) for merged diff.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1999,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(no_grad() changes output of TransformerDecoder module during evaluation )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The TransformerDecoder module gives different outputs at validation time depending on whether gradient calculations are being performed.  Minimal example:    ` **Output:** > tensor(1.8914, grad_fn=) > tensor(1.8914) > tensor(1.8914, grad_fn=) > tensor(2.9639) **Expected output:**  All four outputs should be the same  whether or not the computation graph is stored shouldn't affect the output of the model.  The difference isn't caused by unexpected train vs. test logic somewhere in the TransformerDecoderLayer (eg. a hidden dropout or batch norm), since the model behaves as expected when run in eval model but with gradients.   Versions **Output of collect_env.py:**  PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.25.2 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.109+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is X)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,no_grad() changes output of TransformerDecoder module during evaluation ," ğŸ› Describe the bug The TransformerDecoder module gives different outputs at validation time depending on whether gradient calculations are being performed.  Minimal example:    ` **Output:** > tensor(1.8914, grad_fn=) > tensor(1.8914) > tensor(1.8914, grad_fn=) > tensor(2.9639) **Expected output:**  All four outputs should be the same  whether or not the computation graph is stored shouldn't affect the output of the model.  The difference isn't caused by unexpected train vs. test logic somewhere in the TransformerDecoderLayer (eg. a hidden dropout or batch norm), since the model behaves as expected when run in eval model but with gradients.   Versions **Output of collect_env.py:**  PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.4.01ubuntu1~22.04) 11.4.0 Clang version: 14.0.01ubuntu1.1 CMake version: version 3.25.2 Libc version: glibc2.35 Python version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64bit runtime) Python platform: Linux5.15.109+x86_64withglibc2.35 Is CUDA available: False CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is X",2023-08-04T18:47:08Z,module: nn triaged,open,0,2,https://github.com/pytorch/pytorch/issues/106630,"**Note**: Reverting to torch==1.12.1+cu116 gives expected behavior, so issue looks like it was introduced recently. ","I tracked this down to the following changes: v1.12.0 (https://github.com/pytorch/pytorch/blob/v1.12.0/torch/nn/modules/activation.pyL1088):  v1.12.1 (https://github.com/pytorch/pytorch/blob/v1.12.1/torch/nn/modules/activation.pyL1090): Works, consistent output v2.0.0 (https://github.com/pytorch/pytorch/blob/v2.0.0/torch/nn/modules/activation.pyL1116): Inconsistent output, but can be made consistent by adding the following patch: "
808,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(test_fused_sdp_choice in test_transformers.py fix)ï¼Œ å†…å®¹æ˜¯ (sdp dispatcher prioritizes flash attention over efficient attention: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/cuda/sdp_utils.cppL684L687, and flash attention is enabled for sm75+: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/cuda/sdp_utils.cppL625. Thus, the unit test `test_fused_sdp_choice` from `test_transformers.py` which is failing on T4 (sm75) should have this `SM80OrLater` check changed to `SM75OrLater`: https://github.com/pytorch/pytorch/blob/main/test/test_transformers.pyL1914L1917.  cc:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,test_fused_sdp_choice in test_transformers.py fix,"sdp dispatcher prioritizes flash attention over efficient attention: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/cuda/sdp_utils.cppL684L687, and flash attention is enabled for sm75+: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/cuda/sdp_utils.cppL625. Thus, the unit test `test_fused_sdp_choice` from `test_transformers.py` which is failing on T4 (sm75) should have this `SM80OrLater` check changed to `SM75OrLater`: https://github.com/pytorch/pytorch/blob/main/test/test_transformers.pyL1914L1917.  cc:   ",2023-08-03T23:50:08Z,open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/106587, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
634,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] Use shape env bounds in inductor bounds.py (#106175))ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] Use shape env bounds in inductor bounds.py (106175)) Summary: If constrained ranges are available, use them in bounds.py before value range analysis (to enable Int64 > Int32 optimization). Test Plan: New unit test in test_torchinductor.py to mark a tensor as dynamic, then constrain with constrain_as_size (as outlined in  CC(Use Shape Env Bounds in Inductor Bounds.by)) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",dspy,[inductor] Use shape env bounds in inductor bounds.py (#106175),"  CC([inductor] Use shape env bounds in inductor bounds.py (106175)) Summary: If constrained ranges are available, use them in bounds.py before value range analysis (to enable Int64 > Int32 optimization). Test Plan: New unit test in test_torchinductor.py to mark a tensor as dynamic, then constrain with constrain_as_size (as outlined in  CC(Use Shape Env Bounds in Inductor Bounds.by)) ",2023-08-03T19:18:41Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/106568, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
446,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(use no_grad() consistently for testing transformer trace construction)ï¼Œ å†…å®¹æ˜¯ (Summary: check trace runs with no_grad() and grad or not impacts transformer trace construction. use no_grad() consistently Test Plan: sandcastle and github ci  Differential Revision: D48020889)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,use no_grad() consistently for testing transformer trace construction,Summary: check trace runs with no_grad() and grad or not impacts transformer trace construction. use no_grad() consistently Test Plan: sandcastle and github ci  Differential Revision: D48020889,2023-08-03T04:52:37Z,fb-exported Merged topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/106523,This pull request was **exported** from Phabricator. Differential Revision: D48020889," merge f ""all tests passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
2004,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PT2.1 Feature Proposal] SDPA (Scaled-Dot Product Attention) CPU Optimization)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch This ticket is as part of PT 2.1 feature proposal process.  Motivation As LLM tends to accept a large batch size and a long context length, the requirement of large memory may lead to OOM issues or result in bad performance. To reduce memory usage and provide a substantial speedup for attentionrelated models, it is important to optimize SDPA. The fused SDPA, e.g. flash attention, is one type of the optimized SDPA algorithms designed for memorybound problems, with better parallelism and memory access patterns. In PT 2.0, there exist both the basic unfused SDPA and the fused SDPA for CUDA, while only the unfused SDPA has CPU implementation. To fill the gap between CPU and CUDA, it is proposed to optimize SDPA by implementing the fused SDPA for CPU in PT 2.1.  Implementation We submitted PRs for CPU SDPA optimization and demonstrated up to 3x performance speedup on attentionrelated benchmarks. Here are the detailed implementation items:  	The flash attention CPU kernel is added, in which both forward and backward paths are implemented for data types float32 and bfloat16.  	Write an SDPA selecting function for CPU to automatically choose one SDPA implementation among several ones. The following will be nice to have for PT 2.1:  	Support data type of float16.  	Enable the SDPA graph rewriting for Inductor.  	Further blockrelated tuning for the fused SDPA.  Performance All validations are run on SPR machine.  NanoGPT's SDPA kernel Using benchmark repo, with one socket. Dtype  1.049479  Alternatives _No response_  Additional context  Related PRs Flash attention Implementation: [ CC([cpu] implement scaled dot product flash attention) Flash attention forward](https://github.com/pytorch/)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[PT2.1 Feature Proposal] SDPA (Scaled-Dot Product Attention) CPU Optimization," ğŸš€ The feature, motivation and pitch This ticket is as part of PT 2.1 feature proposal process.  Motivation As LLM tends to accept a large batch size and a long context length, the requirement of large memory may lead to OOM issues or result in bad performance. To reduce memory usage and provide a substantial speedup for attentionrelated models, it is important to optimize SDPA. The fused SDPA, e.g. flash attention, is one type of the optimized SDPA algorithms designed for memorybound problems, with better parallelism and memory access patterns. In PT 2.0, there exist both the basic unfused SDPA and the fused SDPA for CUDA, while only the unfused SDPA has CPU implementation. To fill the gap between CPU and CUDA, it is proposed to optimize SDPA by implementing the fused SDPA for CPU in PT 2.1.  Implementation We submitted PRs for CPU SDPA optimization and demonstrated up to 3x performance speedup on attentionrelated benchmarks. Here are the detailed implementation items:  	The flash attention CPU kernel is added, in which both forward and backward paths are implemented for data types float32 and bfloat16.  	Write an SDPA selecting function for CPU to automatically choose one SDPA implementation among several ones. The following will be nice to have for PT 2.1:  	Support data type of float16.  	Enable the SDPA graph rewriting for Inductor.  	Further blockrelated tuning for the fused SDPA.  Performance All validations are run on SPR machine.  NanoGPT's SDPA kernel Using benchmark repo, with one socket. Dtype  1.049479  Alternatives _No response_  Additional context  Related PRs Flash attention Implementation: [ CC([cpu] implement scaled dot product flash attention) Flash attention forward](https://github.com/pytorch/",2023-08-03T03:11:17Z,,closed,3,0,https://github.com/pytorch/pytorch/issues/106519
1985,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(split one nn.linear to two nn.linear, get different output)ï¼Œ å†…å®¹æ˜¯ ( Describe the bug when I split one nn.linear to two nn.linear, I get different outputs.  Code import torch.nn.functional as F import torch batch_size = 1 input_dim = 4096 output_dim = 4096 torch.set_default_tensor_type(torch.cuda.HalfTensor) input_tensor = torch.randn(batch_size, input_dim) weight_matrix = torch.randn(output_dim, input_dim) output_tensor = F.linear(input_tensor, weight_matrix) print(output_tensor.shape) output_tensor_half1 = F.linear(input_tensor, weight_matrix[0:output_dim//2,:]) print(output_tensor_half1.shape) print(""output_tensor_half1 diff:"", (output_tensor[:,0:output_dim//2]  output_tensor_half1).abs().max()) output_tensor_half2 = F.linear(input_tensor, weight_matrix[output_dim//2:,:]) print(output_tensor_half2.shape) print(""output_tensor_half2 diff:"", (output_tensor[:,output_dim//2:]  output_tensor_half2).abs().max())  result torch.Size([1, 4096]) torch.Size([1, 2048]) output_tensor_half1 diff: tensor(0.0625) torch.Size([1, 2048]) output_tensor_half2 diff: tensor(0.1250)  System Info Package               Version   accelerate            0.19.0 addict                2.4.0 aiohttp               3.8.4 aiosignal             1.3.1 asynctimeout         4.0.2 attrs                 23.1.0 bcrypt                4.0.1 boto3                 1.26.164 botocore              1.29.164 certifi               2023.5.7 cffi                  1.15.1 cfgv                  3.3.1 charsetnormalizer    2.1.1 click                 8.1.3 colossalai            0.3.0 contexttimer          0.3.3 contourpy             1.0.7 cpmkernels           1.0.11 cryptography          41.0.1 cycler                0.11.0 datasets              2.12.0 decorator             5.1.1 dill                  0.3.6 distlib               0.3.6 eva)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"split one nn.linear to two nn.linear, get different output"," Describe the bug when I split one nn.linear to two nn.linear, I get different outputs.  Code import torch.nn.functional as F import torch batch_size = 1 input_dim = 4096 output_dim = 4096 torch.set_default_tensor_type(torch.cuda.HalfTensor) input_tensor = torch.randn(batch_size, input_dim) weight_matrix = torch.randn(output_dim, input_dim) output_tensor = F.linear(input_tensor, weight_matrix) print(output_tensor.shape) output_tensor_half1 = F.linear(input_tensor, weight_matrix[0:output_dim//2,:]) print(output_tensor_half1.shape) print(""output_tensor_half1 diff:"", (output_tensor[:,0:output_dim//2]  output_tensor_half1).abs().max()) output_tensor_half2 = F.linear(input_tensor, weight_matrix[output_dim//2:,:]) print(output_tensor_half2.shape) print(""output_tensor_half2 diff:"", (output_tensor[:,output_dim//2:]  output_tensor_half2).abs().max())  result torch.Size([1, 4096]) torch.Size([1, 2048]) output_tensor_half1 diff: tensor(0.0625) torch.Size([1, 2048]) output_tensor_half2 diff: tensor(0.1250)  System Info Package               Version   accelerate            0.19.0 addict                2.4.0 aiohttp               3.8.4 aiosignal             1.3.1 asynctimeout         4.0.2 attrs                 23.1.0 bcrypt                4.0.1 boto3                 1.26.164 botocore              1.29.164 certifi               2023.5.7 cffi                  1.15.1 cfgv                  3.3.1 charsetnormalizer    2.1.1 click                 8.1.3 colossalai            0.3.0 contexttimer          0.3.3 contourpy             1.0.7 cpmkernels           1.0.11 cryptography          41.0.1 cycler                0.11.0 datasets              2.12.0 decorator             5.1.1 dill                  0.3.6 distlib               0.3.6 eva",2023-08-03T01:37:16Z,,closed,0,5,https://github.com/pytorch/pytorch/issues/106504,"If I want to obtain exactly the same output, what should I do? ",This is the result of the nondeterministic nature of CUDA operations. What you can do in order to enforce deterministic behavior is setting the CUBLAS_WORKSPACE_CONFIG environment variable at the top of your program as follows: ,"Thank you for your reply. I try this and get the result:  result torch.Size([1, 4096]) torch.Size([1, 2048]) output_tensor_half1 diff: tensor(0.0625) torch.Size([1, 2048]) output_tensor_half2 diff: tensor(0.1250) It could be that I expressed myself incorrectly, what I hope to obtain is a diff of 0.","Sorry, my bad. The solution I proposed works for torch version 2.0.1. For torch version 1.13.1 that you currently use I could not find a solution that forces deterministic behavior properly.","Ok, still very much appreciated."
701,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enabling Transformer fast path for not batch_first)ï¼Œ å†…å®¹æ˜¯ (Summary: The fast path for the `forward()` method in `MultiheadAttention` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Previously https://github.com/pytorch/pytorch/pull/85576 (approved by Christian but went stale and autoclosed) Test Plan: Added unit test for fast path for both values of `batch_first` producing identical outputs. Test location  `//caffe2/test/test_native_mha.py` Differential Revision: D47982531)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Enabling Transformer fast path for not batch_first,Summary: The fast path for the `forward()` method in `MultiheadAttention` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Previously https://github.com/pytorch/pytorch/pull/85576 (approved by Christian but went stale and autoclosed) Test Plan: Added unit test for fast path for both values of `batch_first` producing identical outputs. Test location  `//caffe2/test/test_native_mha.py` Differential Revision: D47982531,2023-08-02T15:11:30Z,fb-exported Stale release notes: nn,closed,0,10,https://github.com/pytorch/pytorch/issues/106462,This pull request was **exported** from Phabricator. Differential Revision: D47982531,This pull request was **exported** from Phabricator. Differential Revision: D47982531,This pull request was **exported** from Phabricator. Differential Revision: D47982531,This pull request was **exported** from Phabricator. Differential Revision: D47982531,This pull request was **exported** from Phabricator. Differential Revision: D47982531,This pull request was **exported** from Phabricator. Differential Revision: D47982531,This pull request was **exported** from Phabricator. Differential Revision: D47982531,This pull request was **exported** from Phabricator. Differential Revision: D47982531,"Likely abandon, because MHA and TE/TEL need to be handled in sync.  See CC(Enabling Transformer fast path for not batch_first (MHA, TE, TEL)) for merged diff.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
2032,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`Unsupported: call_method SizeVariable() numel [] {}` when compiling a function sampling from Categorical)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Compiling the simple function below  fails with `Unsupported: call_method SizeVariable() numel [] {}`  Error logs Traceback (most recent call last):   File ""/scratch/ahmedk/hard_ar_sl/ExprStarcraftShortestPath_hard/compile_sampling.py"", line 8, in      func()   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 286, in _fn     return fn(*args, **kwargs)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 439, in catch_errors     return callback(frame, cache_size, hooks, frame_state)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 127, in _fn     return fn(*args, **kwargs)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 360, in _convert_frame_assert     return _compile(   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 180, in time_wrapper     r = func(*args, **kwargs)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 430, in _compile     out_code = transform_code_object(code, transform)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1000, in transform_code_object     transformations(instructions, code_options)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 415, in transform     tracer.run()   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2020, in run     super().run()   File ""/space/ah)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,`Unsupported: call_method SizeVariable() numel [] {}` when compiling a function sampling from Categorical," ğŸ› Describe the bug Compiling the simple function below  fails with `Unsupported: call_method SizeVariable() numel [] {}`  Error logs Traceback (most recent call last):   File ""/scratch/ahmedk/hard_ar_sl/ExprStarcraftShortestPath_hard/compile_sampling.py"", line 8, in      func()   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 286, in _fn     return fn(*args, **kwargs)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py"", line 439, in catch_errors     return callback(frame, cache_size, hooks, frame_state)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 127, in _fn     return fn(*args, **kwargs)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 360, in _convert_frame_assert     return _compile(   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 180, in time_wrapper     r = func(*args, **kwargs)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 430, in _compile     out_code = transform_code_object(code, transform)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1000, in transform_code_object     transformations(instructions, code_options)   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/convert_frame.py"", line 415, in transform     tracer.run()   File ""/space/ahmedk/anaconda3/envs/pt2/lib/python3.10/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2020, in run     super().run()   File ""/space/ah",2023-08-01T20:49:50Z,good first issue triaged oncall: pt2 release notes: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/106407,torch.Size supports numel method and we apparently have not implemented it. Should be straightforward to add support for.,  this issue is fixed right? Seems to work ok on torch==2.0.1+cu118 with numel. ,"Your sample code is different; it looks like here you need to support `torch.Size([1, 2]).numel()` and similar."
1746,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Transformer.generate_square_subsequent_mask has nan values on MPS device)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Square subsequent mask has nan values in place of zeroes when created on MPS device. To reproduce:  prints:  However, the correct tensor is printed when generated on CPU,  `nn.modules.transformer.Transformer.generate_square_subsequent_mask(4, device='cpu')` prints:  After looking into the `Transformer.generate_square_subsequent_mask` function, the problem appears to come from `torch.triu` when called with `diagonal=1`, since `torch.full` works as expected on MPS device.  Versions PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.9.17 (main, Jul  5 2023, 15:35:09)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] numpy==1.25.0 [pip3] torch==2.0.1 [pip3] torchvision==0.15.2 [conda] numpy                     1.25.0                   pypi_0    pypi [conda] torch                     2.0.1                    pypi_0    pypi [conda] torchvision               0.15.2                   pypi_0    pypi  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Transformer.generate_square_subsequent_mask has nan values on MPS device," ğŸ› Describe the bug Square subsequent mask has nan values in place of zeroes when created on MPS device. To reproduce:  prints:  However, the correct tensor is printed when generated on CPU,  `nn.modules.transformer.Transformer.generate_square_subsequent_mask(4, device='cpu')` prints:  After looking into the `Transformer.generate_square_subsequent_mask` function, the problem appears to come from `torch.triu` when called with `diagonal=1`, since `torch.full` works as expected on MPS device.  Versions PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.9.17 (main, Jul  5 2023, 15:35:09)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] numpy==1.25.0 [pip3] torch==2.0.1 [pip3] torchvision==0.15.2 [conda] numpy                     1.25.0                   pypi_0    pypi [conda] torch                     2.0.1                    pypi_0    pypi [conda] torchvision               0.15.2                   pypi_0    pypi  ",2023-07-31T22:21:52Z,triaged module: NaNs and Infs module: mps,open,0,1,https://github.com/pytorch/pytorch/issues/106341,Also just ran into this very specific bug  Python 3.11.9 torch version 2.3.1 macbook pro m3 2023
1872,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`torch.nn.modules.MultiheadAttention` yields different graph under pre_dispatch tracing)ï¼Œ å†…å®¹æ˜¯ (**The problem** Repro posted at the bottom of the issue. Why does `pre_dispatch` tracing yield a different graph? Answer: (1) `pre_dispatch` tracing uses `PreDispatchTorchFunctionMode` so that it can properly intercept and capture autograd and autocast API's during tracing, like no_grad/enable_grad (here) (2) `MultiheadAttention` has a series of checks to decide whether or not it can use a fastpath kernel during inference. One of them: it will ignore the fast path if any `TorchFunctionMode` is active (here) repro:  **Solutions** The main question here is: ""Is `MultiheadAttention` doing something unusual that we're okay with applying a oneoff fix for""? User code can do all sorts of branching on whether or not its inputs have a `__torch_function__` or `__torch_dispatch__` enabled (or if there is a mode active), which can cause us to execute a different set of code when we trace compared to running eager mode. (1) oneoff fix for `MultiheadAttention`: we could augment the check here to ignore the case when a pre_dispatch tracing TorchFunctionMode is active. This is actually a bit annoying, since we'd still probably like that check to return true if pre_dispatch is tracing **and** any other torch_function modes are active. (2) Agree that ""when users ask if torchfunction is enabled in some form, they're really asking about **user** torch_function code, and not our tracing infra  augment `has_torch_function(...)` to explicitly ignore the torchfunctionmode used by pre_dispatch tracing. Open to suggestions  I don't have a strong opinion, although (2) is probably easier to implement. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`torch.nn.modules.MultiheadAttention` yields different graph under pre_dispatch tracing,"**The problem** Repro posted at the bottom of the issue. Why does `pre_dispatch` tracing yield a different graph? Answer: (1) `pre_dispatch` tracing uses `PreDispatchTorchFunctionMode` so that it can properly intercept and capture autograd and autocast API's during tracing, like no_grad/enable_grad (here) (2) `MultiheadAttention` has a series of checks to decide whether or not it can use a fastpath kernel during inference. One of them: it will ignore the fast path if any `TorchFunctionMode` is active (here) repro:  **Solutions** The main question here is: ""Is `MultiheadAttention` doing something unusual that we're okay with applying a oneoff fix for""? User code can do all sorts of branching on whether or not its inputs have a `__torch_function__` or `__torch_dispatch__` enabled (or if there is a mode active), which can cause us to execute a different set of code when we trace compared to running eager mode. (1) oneoff fix for `MultiheadAttention`: we could augment the check here to ignore the case when a pre_dispatch tracing TorchFunctionMode is active. This is actually a bit annoying, since we'd still probably like that check to return true if pre_dispatch is tracing **and** any other torch_function modes are active. (2) Agree that ""when users ask if torchfunction is enabled in some form, they're really asking about **user** torch_function code, and not our tracing infra  augment `has_torch_function(...)` to explicitly ignore the torchfunctionmode used by pre_dispatch tracing. Open to suggestions  I don't have a strong opinion, although (2) is probably easier to implement. ",2023-07-31T14:09:01Z,triaged module: __torch_function__ pre_dispatch tracing oncall: export,open,0,4,https://github.com/pytorch/pytorch/issues/106302,"From offline talk with : we can do a oneoff fix for `MultiheadAttention`, and revisit if we see this popping up in a few places over time.","To be sure  is ""user is asking about user torch_function"" in has_torch_function, violated, because of https://fburl.com/code/i7vrf6wv?"," if I understand your question correctly, then yes  that function will return true if there are **any** active TorchFunctionModes. Why does multihead attention care? It doesn't want to risk running the fastpath native op if there are any torch function modes active, because a user that implements a custom torch function (probably) doesn't know how to handle that fused native op. But in our case with pre_dispatch, we're perfectly happy handling that native op in our pre_dispatch tracing code: we just want to trace it into the graph.",">  if I understand your question correctly, then yes  that function will return true if there are **any** active TorchFunctionModes. Why does multihead attention care? It doesn't want to risk running the fastpath native op if there are any torch function modes active, because a user that implements a custom torch function (probably) doesn't know how to handle that fused native op. But in our case with pre_dispatch, we're perfectly happy handling that native op in our pre_dispatch tracing code: we just want to trace it into the graph. Got it. "
542,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(inductor: support linear fusion when multi linear using same input)ï¼Œ å†…å®¹æ˜¯ (  CC(inductor: support linear fusion when multi linear using same input) For  model, there has a pattern that multi linear using the same input and input dim > 2: , this PR update the pattern to make the linar+silu can be fused(we first need remove view ops, and then apply fusion patterns). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,inductor: support linear fusion when multi linear using same input,"  CC(inductor: support linear fusion when multi linear using same input) For  model, there has a pattern that multi linear using the same input and input dim > 2: , this PR update the pattern to make the linar+silu can be fused(we first need remove view ops, and then apply fusion patterns). ",2023-07-31T12:39:11Z,open source Merged ciflow/trunk module: inductor ciflow/inductor release notes: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/106300, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1712,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support CUDA Unified Memory)ï¼Œ å†…å®¹æ˜¯ ( Motivation Occasionally we found some overlarge samples (e.g. long sequence during training a Transformerbased model) that could cause OOM. Although carefully filtering them out or applying some manuallydesigned transformation can reduce OOM possibility, these methods can hardly eliminate the issue completely.  Resolution here This PR adds support for CUDA Unified Memory, or UVM, to PyTorch, such that we effectively makes device RAM ""swappable"" to system RAM. We usually have plenty of system RAM (e.g., several terabytes) compared to device RAM (tens of gigabytes), therefore UVM brings us a much large safe margin before hitting OOM. The feature can be optionally enabled by setting `PYTORCH_CUDA_USE_UVM=1` in environment variables.  Performance implications UVM degrades CUDA performance, but how much does it affects depends on the actual workload. From our earlier experience, UVM hardly harms CUDA kernels, however, D2H/H2D memcpy are slowed down to a certain degree. Therefore, for memcpyheavy workload, UVM can makes things much slower. However, for training workload with an adequately large batch size, the performance should degrade much. This is also empirically verified by QLoRA: Efficient Finetuning of Quantized LLMs (""Paged Optimizers""). This makes UVM appealing to training workload.  Possibly related  CC(Bring CudaPluggableAllocator to feature parity with the Native Allocator) seems related, which also tried to add UVM support to PyTorch, albeit via a difference approach. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,Support CUDA Unified Memory," Motivation Occasionally we found some overlarge samples (e.g. long sequence during training a Transformerbased model) that could cause OOM. Although carefully filtering them out or applying some manuallydesigned transformation can reduce OOM possibility, these methods can hardly eliminate the issue completely.  Resolution here This PR adds support for CUDA Unified Memory, or UVM, to PyTorch, such that we effectively makes device RAM ""swappable"" to system RAM. We usually have plenty of system RAM (e.g., several terabytes) compared to device RAM (tens of gigabytes), therefore UVM brings us a much large safe margin before hitting OOM. The feature can be optionally enabled by setting `PYTORCH_CUDA_USE_UVM=1` in environment variables.  Performance implications UVM degrades CUDA performance, but how much does it affects depends on the actual workload. From our earlier experience, UVM hardly harms CUDA kernels, however, D2H/H2D memcpy are slowed down to a certain degree. Therefore, for memcpyheavy workload, UVM can makes things much slower. However, for training workload with an adequately large batch size, the performance should degrade much. This is also empirically verified by QLoRA: Efficient Finetuning of Quantized LLMs (""Paged Optimizers""). This makes UVM appealing to training workload.  Possibly related  CC(Bring CudaPluggableAllocator to feature parity with the Native Allocator) seems related, which also tried to add UVM support to PyTorch, albeit via a difference approach. ",2023-07-28T09:27:41Z,module: cuda triaged open source release notes: cuda module: CUDACachingAllocator,closed,1,25,https://github.com/pytorch/pytorch/issues/106200,The committers listed above are authorized under a signed CLA.:white_check_mark: login: 0x804d8000 / name: Luo Bo  (348df22db099eee57d1c9a8f85eb86171cf9fc4a)," label ""release notes: CUDA: Added Unified Memory support."""," label ""module: CUDACachingAllocator""","We're going to have to work out some testing strategy, at least.","Hi  , I revised the patch accordingly, would you mind taking a look at it again?","One thing I'm not sure about, overall design wise, is whether or not to do this as a global flag or not. I understand that having it as a global flag is convenient for the use case ""I just want to change a single flag and suddenly my program can use more memory without OOMing"", but the underlying cudaMallocManaged function can in principle be used in a more finegrained manner by having a different allocator. This is what the previous approach you linked to does, and it also appears to be how TF implemented it: https://github.com/tensorflow/tensorflow/commit/cd4f5840 Related  CC(Support for NVIDIA UVM technology) If we do go the global flag route, instead of having a completely separate envvar you should unify the handling with PYTORCH_CUDA_ALLOC_CONF see CachingAllocatorConfig in c10/cuda/CUDACachingAllocator.cpp  ","Iâ€™m a little concerned with the ""new allocator"" way. Unlikely CUDA stream ordered allocator (cudaMallocAsync, which we already supported as a separate allocator backend), UVM does not natively recognize CUDA stream dependencies. This means to work with CUDA streams, we either have to pay the cost of heavy devicewide synchronization implied by cudaFree, or have to implement another ""caching"" layer for UVM allocator (and duplicate the caching allocator for little benefit).  Use PYTORCH_CUDA_ALLOC_CONF to enable UVM sounds reasonable to me, Iâ€™ll see if I can get this working.  ",Tensorflow enables CUDA UVM by setting a memory limit >=1 so they also have it as a single global flag (for the TF.session at least)  ,Now UVM is enabled by setting `use_uvm=True` in `PYTORCH_CUDA_ALLOC_CONF`.  ,"Without prefetching I would assume to see a poor performance, so do we have any real measurements besides the linked paper? Also, why doesn't `torch.cuda.CUDAPluggableAllocator` work for this use case?  ahmed  Could you take a look at it, too, please?","Unless weâ€™re over subscribing device memory, prefetching is not relevant here. Because we always explicitly copy tensors to device via cudaMemcpy before using them, tensors are always resident on device memory regardless of whether UVM is used.  As I said in the â€œPerformance implicationsâ€ section, CUDA kernels donâ€™t see a slow down with UVM, assuming we manually copied data before using them (which is the case in PyTorch). The difference shows up  when doing H2D/D2H copy, these copies do see a performance hit. Iâ€™m not sure why itâ€™s the case, maybe UVM needs more interaction with the driver or something.  So the key here is to avoid frequent D2H/H2D copy. In the training scenario, we naturally not â€œcudaMemcpy API boundâ€, so things should be fine. Personally I tested the patch with a nonstandard GPT model, with B=1 I saw a 2x slow down, however, once I increased B to 32, it reaches 96% throughout of nonUVM version. ","The issue with torch.cuda.CUDAPluggableAllocator is that itâ€™s not currently as full featured as the â€œnativeâ€ allocator, see CC(Bring CudaPluggableAllocator to feature parity with the Native Allocator). From my experience I also saw some issue such as torch.cuda.memory_allocated() doesnâ€™t work. Iâ€™m ok with the pluggable allocator way if it works seamlessly as the native one. ","If people like the global flag, I guess I can deal with the global flag. I'd like it if NVIDIA peeps could verify the rest of the implementation.",I'm taking a look at this now. Let me compose a response.," CC(Bring CudaPluggableAllocator to feature parity with the Native Allocator) raises a good concern and I think what it's saying is CUDAPluggableAllocator needs a little more work such that external allocators can be a dropin replacement for the builtin allocators in PyTorch. To elaborate a little,  When `torch.backends.cudnn.benchmark = True`, cudnn backend will hit the benchmark path that uses `cacheInfo`. Since only alloc and free are provided with the pluggable allocator, cacheInfo call raises exception (in pluggable allocator) when cudnn benchmark is used, rightfully so  cacheInfo's job is to provide a reasonable estimate for cudnn workspace size and different allocators can implement their own logic for cacheInfo (e.g. CudaMallocAsyncAllocator's cacheInfo is different than CudaCachingAllocator's). We could add a way for the user to provide the cacheInfo function, just like they do for alloc and free (may be also refactor the cacheInfo function because a pluggable allocator can be a noncaching allocator).  Functions related to memory statistics such as torch.cuda.memory_allocated(), memory_snapshot etc. are also allocator specific.  So even though CUDAPluggableAllocator is a good use case for this PR, it's understandable to enable UVM with the environment variable. Questions for  :   how should users interpret memory_stats / memory_allocated with your PR when `use_uvm` is true? Can you give some examples? Do I understand correctly that with your PR, cudaMallocManaged will ""spill"" the memory to cpu memory  when gpu memory is exhausted?   Is there any change in the chaching mechanism of the CUDACachingAllocator when using uvm? I see you have `cudaMallocManaged` in `cudaMallocMaybeCapturing` which is taking the cached path. When memory is oversubscribed and part of the memory is on cpu, is that covered in the gpu cached segment? I'm not sure about the implications of that yet, but it sounds wrong if the CUDACachingAllocator is supposed to have cached allocs that are for gpu memory, but now with uvm it's both cpu and gpu? May be  can comment.  Can you comment on this issue about using cudaMallocManaged (from the fbgemm_gpu repo ):        Does this happen with your PR?  is this a concern for this PR?  `cudaMallocManaged` allocates using page faults. Can we see some benchmarks/testing that sheds some light on if there is an overhead? For instance, you can use nsight profile with the options `cudaumcpupagefaults` and `cudaumgpupagefaults`."," a) Now `memory_allocated` may get bigger than `device_props.total_memory`, when oversubscription occurs. For people who use UVM, I think they should already be expecting this, as the only reason they enable UVM is for oversubscription. The rest fields may be interpreted as if no UVM is involved. b) Sort of. Which memory blocks are ""spilled"" are dynamically determined, in a pseudoLRU way. Memory may get migrated backandforth between device memory and system memory. You may think device memory as a ""huge cache"" for system memory when UVM is used.  I think the most benefit of caching allocator is avoiding the heavyweight `cudaFree`, which incurs a devicewide synchronization. Regardless of the allocation mechanism, I think we always want such benefit. So yes, I think caching allocator should be used here, not merely for ""caching some device memory"", but for ""caching memory allocated from cudart so that we don't have to pay the cost of `cudaFree` on each deallocation"". Another practical reason, or rather, limitation, that we want to treat all memory allocated via UVM the same way (being cached) is that we cannot tell which part of memory are on device and which are on system memory, as the placement is dynamically determined, therefore we can hardly treat them separately.  Honestly I don't have much to comment on this issue, the reasons are: 1) I have no experience with multithreaded training. If multiple devices are used, we run multiple instances of PyTorch (i.e., multiple python processes) via `torchrun`, assigning each instance a device. 2) With UVM enabled, device memory allocated by PyTorch are all ""migrateble"" and can be migrated to host memory were there such a need, so we have much greater headroom for things that needs ""pinned device memory"", including cuda context initialization, cudnn & cublas workspace, ... . Considering that usually we have tens of gigabytes allocated via PyTorch and all of them can be migrated away for make room for cuda context initialization, it's highly unlikely we can still run out of device memory and fail the initialization.  It relies on page faults only if we do not explicit copy memory between device and host. This is not case with PyTorch. We already have necessary `cudaMemcpyAsync` in place, so effectively all page faults / prefetching / ... are ""done manually"", so page faults is not relevant here, just as how prefetching is not relevant. This can be empirically verified with the snippet below.  If no extra argument is provide, we indeed incur page faults:  However, if extra argument is provided (and invokes `cudaMemcpy`) when running the program, we see all page faults happens during `cudaMemcpy` (the green part), and no page fault during kernel execution (happens at 1.335s):  This also echos the effects I posted earlier, that unless we're ""memcpybound"", UVM does not harm performance significantly: > Personally I tested the patch with a nonstandard GPT model, with B=1 I saw a 2x slow down, however, once I increased B to 32, it reaches 96% throughout of nonUVM version.","> Now memory_allocated may get bigger than device_props.total_memory Can you please show using a pytorch snippet and the output you see with your PR? > b) Sort of. Which memory blocks are ""spilled"" are dynamically determined, in a pseudoLRU way. Memory may get migrated backandforth between device memory and system memory. Can you elaborate what you mean by dynamically determined and pseudoLRU way? Memory migrating backandforth is the performance downside of cudaMallocManaged that we want to avoid, and that's why cudaMemAdvise and cudaMemPrefetchAsync are there, so that users can provide migration policies. Also, what happens in a backward call? > we want to treat all memory allocated via UVM the same way (being cached) is that we cannot tell which part of memory are on device and which are on system memory, as the placement is dynamically determined, therefore we can hardly treat them separately. This is what I'm concerned with the design. If a user says allocate my tensor on `device=""cuda""` and pytorch has migrated part of that memory to ""cpu"" by using cudaMallocManaged, aren't we breaking the user's intent? From 's blog, ""the distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device"". > It relies on page faults only if we do not explicit copy memory between device and host.  Yes. I think the point here is it page faults. Lets say you have 24 GB device memory and you need a 30 GB tensor, when you allocate a 30 GB tensor with cudaMallocManaged and access it in your kernel, won't you page fault? That's why it's a little surprising that there is no usage of cudaMemAdvise or cudaMemPrefetchAsync in this PR.  For the attached paper, I see they indeed use cudaMemPrefetchAsync: https://github.com/TimDettmers/bitsandbytes/blob/18e827d666fa2b70a12d539ccedc17aa51b2c97c/bitsandbytes/optim/optimizer.pyL314L322. So I think we need to see more results from your PR. For the GPT model, was it an inference run or fine tuning? ","I think I see your point now. You're concerned about the performance when the user does oversubscribe VRAM, right? Yes, they will suffer. Let me reiterate the motivation of this PR: > Occasionally we found some overlarge samples (e.g. long sequence during training a Transformerbased model) that could cause OOM. Although carefully filtering them out or applying some manuallydesigned transformation can reduce OOM possibility, these methods can hardly eliminate the issue completely. So the use of UVM here is not intended as a solution for constantly VRAM oversubscription, but an onceandforall mitigation for occasional OOMs caused by overlarge sequence or alike. Upon seeing such inputs, the expected behavior changes from:  OOM occurs and the training process crashes, requiring manually intervening. to:  Oversubscription occurs and the training speed of **this** batch slows down, due to UM page faults, but no interruption to the training process would occur, and the rest (batches before and batches after) trains at the nearoptimal (noUM) speed [1]. In case the training workload is constantly larger than VRAM, instead of manually prefetching, CPU offload may be a better choice. Either way we'd be limited by PCIe bandwidth, and with explicitly copying (offloading) between device and host, we should get the optimal result. To conclude, the PR is a onceandforall mitigation for occasional OOMs, not a solution for workloads that are constantly uses more VRAM than physically available. Hopefully this clarifies something for your concern CC(Don't support legacy Python) & CC(PEP8). As for CC(PEP8): No it's not PyTorch who migrates the UVM memory, it's the CUDA driver. PyTorch has no control on the placement policy (except for `cudaMemAdvise` or alike). For user intent, we still allocate the memory in the CUDA way, and the data does reside on CUDA driver (except for occasional oversubscription). Also, the contract (I mean, interface to the user) that ""that doesn't work with any other device"" should still hold. Accessing a CUDA tensor, even if it's allocated with UVM and ""physically"" accessible, without first migrating (in PyTorch's sense this time, i.e., `tensor.cpu()`) it to CPU, is still a violation of our design. [2] As for the GPT I mentioned, it was inference, with no VRAM oversubscription. So I was testing that the ""normal"" case does run at the expected speed, not testing the speed of the occasionally oversubscribed ""degraded"" case.  The snippet asked:   1: Actually we may even get better training speed than nonUM case, as, as a side effect, UVM ""mitigates"" memory fragmentation issue. Those ""fragmented but not used"" memory blocks will likely be eventually migrated to system memory by CUDA driver, leaving more VRAM for us to use and allowing a larger batch size. 2: Well we may some day need to revise this design as vendors are trying to ""unify"" the memory space, such as NVIDIA's Grace Hopper Superchip, AMD's MI300, ... . But this is not relevant to this PR.","Oh I see why you said ""pytorch has migrated part of that memory"". Did you mean explicitly migrating the memory via `cudaMemAdvise`? We don't do that in this PR. So migration is totally up to the CUDA driver.  Normally we want all data reside on device memory. Migrating part of data to cpu should only happen when some exceptionally large input is somehow given by the data loader. Yes I'm aware that we can do some fancy batching strategy so that the input sizes are under a certain limit, but devising a perfect strategy (100% no OOM) is much harder than a good enough one (99% of the time no over subscription). Besides, a perfect safe strategy usually requires such a strict batch size rule that often overestimates VRAM actually required, leading to an unnecessary small batch size, and slows down the overall training speed. With this PR, we only need a good enough strategy, leaving the occasional OOM for UVM to handle, and only pay the cost of being slow of that single batch. ","I would like to add one more topic to keep in mind when discussion this change: the interaction between the caching allocator and this setting. > I think the most benefit of caching allocator is avoiding the heavyweight cudaFree, which incurs a devicewide synchronization. Regardless of the allocation mechanism, I think we always want such benefit. So yes, I think caching allocator should be used here, It is important to keep in mind that our caching allocator is ""very greedy"" today and relies on OOM signal to stabilise. If it cannot find a very good spot for the new allocation, it will very happily use cudaMalloc to get more memory. And ONLY if this cudaMalloc report an OOM, will it start to do the hard work of clearing blocks to allow merging them and call cudaFree etc. Using unified memory here would remove this OOM signal altogether, allowing the allocator to always be able to allocate more blocks. This will most likely increase the GPU memory used in general (and so your network that used to fit in device memory will now start using paging!) and add a worst case scenario where you keep allocating memory (without ever reusing your blocks) until you run out of RAM."," Thanks for the reminder.  Yes Iâ€™m aware of that behavior, and Iâ€™ve been â€œblessedâ€ by it. After allocating terabytes of memory, a CUDA kernel miserably raised an â€œillegal memory accessâ€ errorâ€¦ This is now mitigated by the addition of c10/cuda/CUDACachingAllocator.cpp:1385. When UVM is enabled, the allocator now proactively reclaims memory if there has been too much of memory being reserved. ","Turning on unified memory increases the complexity of the mental model of performance for running networks. Previously the performance of a network was uncoupled to how much memory has been allocated.  With unified memory, OOMs are disabled entirely but slowdowns will be introduced that get worse with more memory. I can imagine having situations where a developer increases memory inadvertently and is left with a hardtodebug slowdown. Unified memory is not immediately compatible with expandable_segments, which is also trying to enable more complete use of memory at the limits, so I can see places where it will not be clear how to choose which approach to use. Until we see large adoption of unified memory to solve memory problems, this allocator seems more appropriate as a custom allocator where it is very clear that nonstandard behavior has been enabled.  So I think the right path forward is to fix the issues that make the pluggable allocator unsuitable for this case. In particular, it makes sense for the pluggable allocator to fallback to the existing allocator for functionality it doesn't implement, and for the the pluggable allocator to be able to override how noncached memory is obtained from cuda even when falling back to the existing caching approach. For unified memory, the custom allocator would then use the standard implementations of cached_alloc/cached_free, but override uncached_alloc/uncached_free to back that memory with managed memory.","So I guess the preferred way is to revising the pluggable allocator, closing the PR then.  For anyone who is interested in the patch, I'll keep it in my fork's uvm branch. Thank you all for the valuable comments. ğŸ™","Hi, uh, I know I'm a bit late to the party, but the actual reasons I suggested using the pluggable allocator were: 1. so other allocators that pop up in the future could, in theory, be used next to seamlessly, and 2. I thought it'd require less code, which seems like a misconception on my part.  Thanks for the link, I'm checking it out right now ","> So I guess the preferred way is to revising the pluggable allocator, closing the PR then. >  > For anyone who is interested in the patch, I'll keep it in my fork's uvm branch. >  > Thank you all for the valuable comments. ğŸ™ Thanks man! I can finally EasyOCR an 300 dpi A4 page on an NVIDIA GT 1030 with only 2 GB within a reasonable time on an AMD FX6300 without AVX2!"
810,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo.export] symbolic_shapes.GuardOnDataDependentSymNode)ï¼Œ å†…å®¹æ˜¯ (This was observed when exporting `BertForMaskedLM`. It turned out the model code is doing a input value check to produce a warning. Hence an intuitive workaround appears to be applying `._dynamo.assume_constant_result` on the checker function. That indeed worked around the issue and enables export. I'm wondering if it is technically feasible to create an ""unsafe"" config that, when opt in, automatically applies `assume_constant_result` onto the call that triggers `GuardOnDataDependentSymNode`, to essentially consider it a constant and prevent the graph break?   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[dynamo.export] symbolic_shapes.GuardOnDataDependentSymNode,"This was observed when exporting `BertForMaskedLM`. It turned out the model code is doing a input value check to produce a warning. Hence an intuitive workaround appears to be applying `._dynamo.assume_constant_result` on the checker function. That indeed worked around the issue and enables export. I'm wondering if it is technically feasible to create an ""unsafe"" config that, when opt in, automatically applies `assume_constant_result` onto the call that triggers `GuardOnDataDependentSymNode`, to essentially consider it a constant and prevent the graph break?   ",2023-07-28T00:32:49Z,triaged oncall: pt2 module: dynamic shapes,closed,1,7,https://github.com/pytorch/pytorch/issues/106183," and I were actually discussing this for unrelated reasons. There are a few things going on here: 1. If the conditional happens inside Dynamo'ed Python bytecode, I agree it should be possible to do this automatically. However, don't really like `assume_constant_result` for this purpose. The problem is that you need to be absolutely certain that the function doesn't have any important pieces to trace. In your example, it's pretty easy to see, but imagine if the warning function was inlined, there's no logical place to put the `assume_constant_result`. Instead, what we have to do is when we do a conditional that hits a GuardOnDataDependentSymNode error, we should speculate both sides of the conditional, and if one of them hits an error, we can assume that we must go down the nonerror path (and trace a runtime assert). If some model changes are permitted, we can use a special assertstyle construct to indicate this . If the conditional happens inside PyTorch core code, esp in C++, we can't play the automatic speculate trick from (1) easily (technically, it possible to do using the same idea as in https://gist.github.com/ezyang/192c1b5eb57ff95a46d3b50aa46e3193, but you have to repeatedly rerun the *entire* trace toptobottom to get to change your mind midway through since we don't have first class continuations / amb). But the special assert construct which says ""if this is unhinted, just assume it's always true/false"" works just as well in this case. We should add it. I'm probably going to do this soon, as it's blocking some export cases I'm working on lol.",," Thank you for responses. Regarding the automatic solution I was visioning its application in a much smaller scope. From an overly simplified point of view to amend unhinted symbol with the hint value. For generality we wouldn't want the call to be made a constant in the graph completely by `assume_constant_result`, we just need similar mechanism to produce a hint value, and I'd assume this would be guarded and we can emit warnings or asserts. However, I'm not familiar with when/how/where dynamo handles the concrete run and what it requires so this idea may not be feasible from the start lol. On the other hand for conditional speculation, I wonder if it would become a problem that the search space grows exponentially with the number of expressions, and that the erroring may only occur at a much later stage, or may not occur at all for either branch...","But you... can't really soundly give unbacked ints a hint. You have no way to guard on it! The guard implies that you have to invalidate the graph, but you can't check the guard on entry because you don't have the data. If you have some sort of bailout strategy this is more likely to work, but you need to reset the analysis with a different hint when the hint gets violated. > On the other hand for conditional speculation, I wonder if it would become a problem that the search space grows exponentially with the number of expressions, and that the erroring may only occur at a much later stage, or may not occur at all for either branch... Yes, this is why the special assert construct (which knows one branch is impossible) is a good idea.",I want to note that we have `expect_true` now and are able to infer runtime asserts when the opposite path would error. IT doesn't help for this particular case though because it's a warning invocation., is this still an issue? I ran your repro and don't see the error you posted above anymore.,"Closing, this seems to be resolved in upstream transformers with code changes "
343,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Compiled Autograd] Improve nyi error messages)ï¼Œ å†…å®¹æ˜¯ (  CC([Compiled Autograd] Refactor duplicate symint handling)  CC([Compiled Autograd] Improve nyi error messages) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[Compiled Autograd] Improve nyi error messages,  CC([Compiled Autograd] Refactor duplicate symint handling)  CC([Compiled Autograd] Improve nyi error messages) ,2023-07-27T22:57:32Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/106176, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.9clang12asan / test (default, 4, 6, linux.4xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1961,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch compile changes model output)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug For some reason, torch compile changes model output. I checked multiple transformer models, here is one example:  Same orders of magnitude in difference for CPU and any other precisions (float16, bfloat16) with/without torch.cuda.amp.autocast decorator.  Transformers versions:  Related huggingface issue: https://github.com/huggingface/transformers/issues/25155  Versions PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01034oraclex86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 515.105.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch compile changes model output," ğŸ› Describe the bug For some reason, torch compile changes model output. I checked multiple transformer models, here is one example:  Same orders of magnitude in difference for CPU and any other precisions (float16, bfloat16) with/without torch.cuda.amp.autocast decorator.  Transformers versions:  Related huggingface issue: https://github.com/huggingface/transformers/issues/25155  Versions PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01034oraclex86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA A100SXM480GB GPU 4: NVIDIA A100SXM480GB GPU 5: NVIDIA A100SXM480GB GPU 6: NVIDIA A100SXM480GB GPU 7: NVIDIA A100SXM480GB Nvidia driver version: 515.105.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):     ",2023-07-27T22:29:20Z,triaged oncall: pt2,closed,0,5,https://github.com/pytorch/pytorch/issues/106171,"This should be expected since torch.compile will use different kernels underlying and it may change numerics in some sense. But if you try: > (Pdb) torch.allclose(out_compiled, out_not_compiled, atol=1e3, rtol=1e3) This will return True.", Is this also expected using float16? On T4 free colab GPU? Adding autocast context manager does not help...,There are a few things you can try: 1. try print part of the logits with and without torch.compile to spot check if they are close enough 2. try to use larger rtol argument for torch.allclose as well. Default one is 1e5 and maybe you can try 1e3 It's possible that half float/bfloat16 need further loosing the tolerance compared to float32.," is this still an issue? Another way to check this is compare eager/torch.compile to float64, it's likely that torch.compile will be close to the float64 baseline.","Closing due to inactivity, feel free to reopen"
860,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(is causal hints for transformer)ï¼Œ å†…å®¹æ˜¯ (Summary: make is_causal hint flags available for the top level transformer module. It's debatable whether this is useful  at present we autodetect causal masks for src and tgt masks in transformer encoder and decoder, respectively. is_causal flags available woul enable users to shortcut this check by asserting whether they mask is causal, or not. I am putting this diff up for discussion, not as a solution.  Not doing anything may be the right solution, unless there is strong (datadriven) user demand.  it appears the consensus is to move ahead with this, as per discussions below.      Test Plan: sandcastle Differential Revision: D47373260)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,is causal hints for transformer,"Summary: make is_causal hint flags available for the top level transformer module. It's debatable whether this is useful  at present we autodetect causal masks for src and tgt masks in transformer encoder and decoder, respectively. is_causal flags available woul enable users to shortcut this check by asserting whether they mask is causal, or not. I am putting this diff up for discussion, not as a solution.  Not doing anything may be the right solution, unless there is strong (datadriven) user demand.  it appears the consensus is to move ahead with this, as per discussions below.      Test Plan: sandcastle Differential Revision: D47373260",2023-07-27T16:34:35Z,fb-exported Merged ciflow/trunk release notes: nn,closed,0,14,https://github.com/pytorch/pytorch/issues/106143,This pull request was **exported** from Phabricator. Differential Revision: D47373260,"cc'ing  as we had similar API design questions for things like ""is this matrix singular"". IIRC our resolution was that adding these kinds of hints made sense and we should be willing to do it.","I don't remember that the specific conversation about ""is the matrix singular"", but I remember that sort of conversation appeared in two contexts: `linalg.lstsq` has a `driver` arg that lets you choose the algorithm that will be used. We wrote in its docs a comprehensive guide on how to choose the correct `driver` arg for your application. The other discussion we were having was about writing a set of tensor subclasses to mark that certain input matrices have some properties (e.g. they are symmetric or Hermitian, or SPD, or singular...). Then, the function would choose the most efficient algorithm available for a given set of inputs. The user would need to initially ""tag"" matrices with these classes, and then the algorithms could potentially return tagged matrices. This approach is nice, because it decouples the properties of the inputs from the API, so it's more extensible. Now, it just makes sense if you expect this API to be extended.",See also CC(Implement `is_causal` API for `Transformer`).,"I think this change makes sense to have consistency between the Transformer modules. To me, it makes sense that these flags should be available when calling a `Transformer` as well. Otherwise you lose access to these flags if you instantiate a `Transformer`; due to the defaults, users do not need to worry about them (just as before), but can still supply the flags for performance improvements.",This pull request was **exported** from Phabricator. Differential Revision: D47373260,This pull request was **exported** from Phabricator. Differential Revision: D47373260,This pull request was **exported** from Phabricator. Differential Revision: D47373260,This pull request was **exported** from Phabricator. Differential Revision: D47373260,This pull request was **exported** from Phabricator. Differential Revision: D47373260,This pull request was **exported** from Phabricator. Differential Revision: D47373260,> Please fix lint Fixed!," merge Get Outlook for iOS ________________________________ From: mikaylagawarecki ***@***.***> Sent: Thursday, August 3, 2023 6:03:25 PM To: pytorch/pytorch ***@***.***> Cc: Michael Gschwind ***@***.***>; Author ***@***.***> Subject: Re: [pytorch/pytorch] is causal hints for transformer (PR CC(is causal hints for transformer))  approved this pull request. Thanks! â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.â€ŠMessage ID:  ZjQcmQRYFpfptBannerStart This Message Is From an External Sender ZjQcmQRYFpfptBannerEnd  approved this pull request. Thanks! â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***>", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
318,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pt2] add meta for `cholesky_inverse`)ï¼Œ å†…å®¹æ˜¯ (Stack from ghstack:  CC([pt2] add meta for `cholesky_inverse`)  CC([pt2] add meta for `cholesky`))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[pt2] add meta for `cholesky_inverse`,Stack from ghstack:  CC([pt2] add meta for `cholesky_inverse`)  CC([pt2] add meta for `cholesky`),2023-07-27T08:04:38Z,open source Merged ciflow/trunk topic: not user facing ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/106120,From the list in  CC(Remaining functions without meta registrations), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(llama model failed for dynamic shape path)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When running the dynamic shape path for the Llama model, there report an assert error:   Error logs _No response_  Minified repro   Versions Collecting environment information... PyTorch version: 2.1.0a0+gitdcacff5 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.13.030genericx86_64withglibc2.31 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] bertpytorch==0.0.1a4 [pip3] clipanytorch==2.5.2 [pip3] CoCapytorch==0.0.7 [pip3] dalle2pytorch==1.12.4 [pip3] emapytorch==0.2.2 [pip3] functorch==1.14.0a0+b71aa0b [pip3] intelextensionforpytorch==2.1.0+git785443c [pip3] mypy==1.4.1 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.24.3 [pip3] opencliptorch==2.20.0 [pip3] pytorchtransformers==1.2.0 [pip3] pytorchtriton==2.1.0+9e3e10c5ed [pip3] pytorchwarmup==0.1.1 [pip3] rotaryembeddingtorch==0.2.1 [pip3] torch==2.1.0a0+git0391d8d [pip3] torchfidelity==0.3.0 [pip3] torchscatter==2.1.1+pt20cpu [pip3] torchsparse==0.6.17+pt20cpu [pip3] torchstruct==0.5 [pip3] torchaudio==2.1.0a0+d5b2996 [pip3] torchdata==0.7.0a0+f2bfd3d [pip3] torchmetrics==0.11.4 [pip3] torchrecnightly==2023.3.23 [pip3] torchtext==0.16.0a0+60bea66 [pip3] torchvision==0.16.0a0+d24db8c [pip3] ve)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,llama model failed for dynamic shape path," ğŸ› Describe the bug When running the dynamic shape path for the Llama model, there report an assert error:   Error logs _No response_  Minified repro   Versions Collecting environment information... PyTorch version: 2.1.0a0+gitdcacff5 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.13.030genericx86_64withglibc2.31 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] bertpytorch==0.0.1a4 [pip3] clipanytorch==2.5.2 [pip3] CoCapytorch==0.0.7 [pip3] dalle2pytorch==1.12.4 [pip3] emapytorch==0.2.2 [pip3] functorch==1.14.0a0+b71aa0b [pip3] intelextensionforpytorch==2.1.0+git785443c [pip3] mypy==1.4.1 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.24.3 [pip3] opencliptorch==2.20.0 [pip3] pytorchtransformers==1.2.0 [pip3] pytorchtriton==2.1.0+9e3e10c5ed [pip3] pytorchwarmup==0.1.1 [pip3] rotaryembeddingtorch==0.2.1 [pip3] torch==2.1.0a0+git0391d8d [pip3] torchfidelity==0.3.0 [pip3] torchscatter==2.1.1+pt20cpu [pip3] torchsparse==0.6.17+pt20cpu [pip3] torchstruct==0.5 [pip3] torchaudio==2.1.0a0+d5b2996 [pip3] torchdata==0.7.0a0+f2bfd3d [pip3] torchmetrics==0.11.4 [pip3] torchrecnightly==2023.3.23 [pip3] torchtext==0.16.0a0+60bea66 [pip3] torchvision==0.16.0a0+d24db8c [pip3] ve",2023-07-27T04:32:29Z,needs reproduction triaged oncall: pt2 module: dynamic shapes,open,0,18,https://github.com/pytorch/pytorch/issues/106110,`hf_T5_generate` accuracy check in MultiThreads also observed.   `,"I can't easily run the command, it fails for me with  can someone run the model with `TORCH_LOGS=dynamic` as the error message suggests and post the logs?",running  and get the following log: ,"> I can't easily run the command, it fails for me with >  >  >  > can someone run the model with `TORCH_LOGS=dynamic` as the error message suggests and post the logs? It seems torchbenchmark issue, torchbenchmark used by us is .","The key line is `[20230727 22:16:12,052] torch.fx.experimental.symbolic_shapes: [INFO] 0.2: eval Eq(32, s0) [guard added] at benchmark/torchbenchmark/models/llama/model.py:126 in forward (utils/_device.py:76 in __torch_function__)`. But even checking out your commit hash I'm in the middle of nowhere for this line:  The framework stack kind of messed up and dropped us in a useless frame. Try cranking this up to give a full stack trace by editing the relevant log line in symbolic_shapes.py"," Is it still relevant? If not, can you please close the issue?","fangintel, please help doublecheck it.","Sure, I will double confirm whether it's still a issue or not",I can reproduce this issue with latest PyTorch(1e60174891c21e8de9a813eb2a454ac9819b4a50) and TorchBench(fa41cb26a39c112f72f55b3ccacc2bbc502e2649)  Reproduce CMD is: `python m torch.backends.xeon.run_cpu node_id 0 benchmarks/dynamo/torchbench.py performance float32 dcpu inference n5 inductor timeout 9000 dynamicshapes dynamicbatchonly only llama`  Detail error msg is: ,If you run with TORCH_LOGS=dynamic it will tell you why the specialization occurred,"Hi , update some progress for this issue, especially for why `[20230727 22:16:12,052] torch.fx.experimental.symbolic_shapes: [INFO] 0.2: eval Eq(32, s0) [guard added] at benchmark/torchbenchmark/models/llama/model.py:126 in forward (utils/_device.py:76 in __torch_function__)`  This guard is generated by model script at `keys = self.cache_k[:bsz, : start_pos + seqlen]`     The hint of input bs is `32`: `DEFAULT_EVAL_BSIZE = 32`    The max bs to generate `self.cache_k` is also `32`: `max_batch_size: int = 32`      If we change the max bs to generate `self.cache_k` from `32` to `33`. The issue **disappears**. Looks like we try to specialize the variable `set_replacement s0 = 32` when equal at priority.  Simplify this issue into a small test case which can reproduce this issue: https://gist.github.com/lesliefangintel/657e13fbd90941a1972c43ed6d11e25e ","This is the thing where slicing into a tensor behaves inconsistently if you slice to the very end, versus slicing before the end. Is a bit of a pain to fix. Current approach would be to make the quantity in question unbacked, but will need some work. A repro would let me take a look directly.",> * https://gist.github.com/lesliefangintel/657e13fbd90941a1972c43ed6d11e25e Thanks for the comment . Here are the repro: https://gist.github.com/lesliefangintel/657e13fbd90941a1972c43ed6d11e25e,"So, if you don't mind having a specialized kernel for 32, you can work around this using `maybe_mark_dynamic`, which won't hard error when we do the bs=32 specialization, but will give you a generic kernel for everything else",I fiddled around with the unbacked variant and we seem to be missing juice for it. I'll file some issues.,"> So, if you don't mind having a specialized kernel for 32, you can work around this using `maybe_mark_dynamic`, which won't hard error when we do the bs=32 specialization, but will give you a generic kernel for everything else Thanks for the suggestion.  Since the issue happens when running llama in `torchbench` which hardcode to use `mark_dynamic` instead of `maybe_mark_dynamic`, do you think if we can change it in `torchbench`?  Another workaround comes to my mind is maybe we can do some llama specific workaround as changing the max bs for `cache_k` from 32 to larger values like 64 in torchbench? Any preference for the workaround?",I think the latter gives us more insight into the true performance characteristics of the model.,PR: https://github.com/pytorch/benchmark/pull/2283 to fix this issue in torchbench.
542,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add support for mutated params)ï¼Œ å†…å®¹æ˜¯ (  CC(More descriptive graph diagram names in svg)  CC(add support for mutated params)  CC([Dynamo] Ignore noop tensor assignment)  CC(run freezing in nightly) Previously, this didn't work because of the warmup run. Now that we do not run warmup, and then execution on one inductor invocation this works. llama inference 1.6>4.4 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,add support for mutated params,"  CC(More descriptive graph diagram names in svg)  CC(add support for mutated params)  CC([Dynamo] Ignore noop tensor assignment)  CC(run freezing in nightly) Previously, this didn't work because of the warmup run. Now that we do not run warmup, and then execution on one inductor invocation this works. llama inference 1.6>4.4 ",2023-07-27T01:10:00Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/106098, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
779,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo] Ignore no-op tensor assignment)ï¼Œ å†…å®¹æ˜¯ (  CC(More descriptive graph diagram names in svg)  CC(add support for mutated params)  CC([Dynamo] Ignore noop tensor assignment)  CC(run freezing in nightly) Ignore noop `self.attr = self.attr` on NN Modules when attr is a Tensor attribute.  This comes from a llama pattern. Normally, when a set attr occurs on an nn module we turn it into an `UnspecializedNNModuleVariable` which prevents static buffers and parameters. In subsequent pr i will add support for cudagraph mutation of buffers/params, which with this pr takes llama 1.6x > 4.4x in inference )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[Dynamo] Ignore no-op tensor assignment,"  CC(More descriptive graph diagram names in svg)  CC(add support for mutated params)  CC([Dynamo] Ignore noop tensor assignment)  CC(run freezing in nightly) Ignore noop `self.attr = self.attr` on NN Modules when attr is a Tensor attribute.  This comes from a llama pattern. Normally, when a set attr occurs on an nn module we turn it into an `UnspecializedNNModuleVariable` which prevents static buffers and parameters. In subsequent pr i will add support for cudagraph mutation of buffers/params, which with this pr takes llama 1.6x > 4.4x in inference ",2023-07-26T23:51:42Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/106092, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1084,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Specifying parameters for RNN classes (for signature helpers) and document RNNBase)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue The RNN, LSTM, and GRU classes are initialized with *args and **kwargs, despite the docstring specifying a list of possible arguments. This can be a mild annoyance for those using editors with signature helpers and may confuse users about PyTorch's implementation of RNNs. Additionally, there is no documentation of the RNNBase class to explain its purpose in the PyTorch codebase.   Suggest a potential alternative/fix Trivial changes to the RNN.py file to specify the initialization parameters of each RNN class with appropriate type signatures. This will benefit those using editors with signature helpers. Write a short docstring for the RNNBase class that (1) explains that it is a base implementation of all RNNs and (2) directs users to other documentation. I'd be happy to make these changes! )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Specifying parameters for RNN classes (for signature helpers) and document RNNBase," ğŸ“š The doc issue The RNN, LSTM, and GRU classes are initialized with *args and **kwargs, despite the docstring specifying a list of possible arguments. This can be a mild annoyance for those using editors with signature helpers and may confuse users about PyTorch's implementation of RNNs. Additionally, there is no documentation of the RNNBase class to explain its purpose in the PyTorch codebase.   Suggest a potential alternative/fix Trivial changes to the RNN.py file to specify the initialization parameters of each RNN class with appropriate type signatures. This will benefit those using editors with signature helpers. Write a short docstring for the RNNBase class that (1) explains that it is a base implementation of all RNNs and (2) directs users to other documentation. I'd be happy to make these changes! ",2023-07-26T23:04:07Z,module: docs module: nn module: rnn triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/106085
1966,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] Enable async all-reduce for HSDP)ï¼Œ å†…å®¹æ˜¯ (  CC([FSDP][Docs] Add note on `NCCL_CROSS_NIC=1` for HSDP)  CC([FSDP] Enable async allreduce for HSDP)  CC([FSDP] Break up `_post_backward_hook` into smaller funcs) **Overview** This PR runs the HSDP allreduce as async so that it can overlap with both allgather and reducescatter, which can lead to slight endtoend speedups when the sharding process group is fully intranode. Previously, the allreduce serializes with reducescatter, so it can only overlap with one allgather. For some clusters (e.g. our AWS cluster), `NCCL_CROSS_NIC=1` improves internode allreduce times when overlapped with intranode allgather/reducescatter. **Experiment**   Example 'before' trace      Example 'after' trace    For the 6encoderlayer, 6decoder layer transformer with `d_model=8192`, `nhead=64` on 4 nodes / 32 40 GB A100s via AWS, the endtoend iteration times are as follows (with AG == allgather, RS == reducescatter, AR == allreduce; bandwidth reported as algorithmic bandwidth):  Reference FSDP:      **1160 ms / iteration**      ~23 ms / encoder AG/RS > 24.46 GB/s bandwidth      ~40 ms / decoder AG/RS > 26.5 GB/s bandwidth      50 GB/s theoretical internode bandwidth  Baseline 8way HSDP (only overlap AR with AG)  intranode AG/RS, internode AR:      **665 ms / iteration**      ~3 ms / encoder AG/RS > 187.5 GB/s bandwidth      ~5 ms / decoder AG/RS > 212 GB/s bandwidth      ~30 ms / encoder AR > 2.34 GB/s bandwidth      ~55 ms / decoder AR > 2.65 GB/s bandwidth      300 GB/s theoretical intranode bandwidth  New 8way HSDP (overlap AR with AG and RS)  intranode AG/RS, internode AR:      **597 ms / iteration**      ~3 ms / encoder AG/RS > 187.5 GB/s bandwidth      ~6.2 ms / decoder AG/RS > 170.97 GB/s bandwidth (slower)      ~23 ms / encoder AR)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[FSDP] Enable async all-reduce for HSDP,"  CC([FSDP][Docs] Add note on `NCCL_CROSS_NIC=1` for HSDP)  CC([FSDP] Enable async allreduce for HSDP)  CC([FSDP] Break up `_post_backward_hook` into smaller funcs) **Overview** This PR runs the HSDP allreduce as async so that it can overlap with both allgather and reducescatter, which can lead to slight endtoend speedups when the sharding process group is fully intranode. Previously, the allreduce serializes with reducescatter, so it can only overlap with one allgather. For some clusters (e.g. our AWS cluster), `NCCL_CROSS_NIC=1` improves internode allreduce times when overlapped with intranode allgather/reducescatter. **Experiment**   Example 'before' trace      Example 'after' trace    For the 6encoderlayer, 6decoder layer transformer with `d_model=8192`, `nhead=64` on 4 nodes / 32 40 GB A100s via AWS, the endtoend iteration times are as follows (with AG == allgather, RS == reducescatter, AR == allreduce; bandwidth reported as algorithmic bandwidth):  Reference FSDP:      **1160 ms / iteration**      ~23 ms / encoder AG/RS > 24.46 GB/s bandwidth      ~40 ms / decoder AG/RS > 26.5 GB/s bandwidth      50 GB/s theoretical internode bandwidth  Baseline 8way HSDP (only overlap AR with AG)  intranode AG/RS, internode AR:      **665 ms / iteration**      ~3 ms / encoder AG/RS > 187.5 GB/s bandwidth      ~5 ms / decoder AG/RS > 212 GB/s bandwidth      ~30 ms / encoder AR > 2.34 GB/s bandwidth      ~55 ms / decoder AR > 2.65 GB/s bandwidth      300 GB/s theoretical intranode bandwidth  New 8way HSDP (overlap AR with AG and RS)  intranode AG/RS, internode AR:      **597 ms / iteration**      ~3 ms / encoder AG/RS > 187.5 GB/s bandwidth      ~6.2 ms / decoder AG/RS > 170.97 GB/s bandwidth (slower)      ~23 ms / encoder AR",2023-07-26T22:17:20Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: improvements,closed,0,6,https://github.com/pytorch/pytorch/issues/106080," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","I think this is functionally correct but there's something very unfortunate going on here, which is the fact that you had to duplicate the `_post_backward_use_sharded_grad_views` etc calls through the other stream path so that you could get everything in the correct stream context. One way to reduce the duplication is to have everything uniformly use the new stream, even if you're not HSDP. But this may have undesirable performance side effects for the nonHSDP case (since you're uselessly creating a new stream and then immediately syncing it back. Another possibility is to rewrite the code in continuation passing style, where  is a callback that is passed to `_reduce_grad`, and then you can home it where it needs to live. Although, `_reduce_grad` is only called in one spot, so you could also literally just swing these calls into `_reduce_grad` (the annoyance being that it still needs to be a function call so that you can get it in `_reduce_grad_no_shard`.) I don't think it is a big deal and you can land as is, but it definitely will be a tax for future modifications to this code.",I went with the callback style (but did not explicitly pass it into `_reduce_grad` / `_reduce_grad_no_shard`). I agree that the callback is less brittle to future changes. I also added some comments.,"Rebased `gh/awgu/443/orig` onto `refs/remotes/origin/viable/strict` because CC([FSDP][Docs] Add note on `NCCL_CROSS_NIC=1` for HSDP) was rebased, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/106080`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
441,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dynamo Not Preserving Bufferness)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Will result in 0 static inputs / no buffers passed to aot_autograd. This is a minified repro of llama issue I'm looking at that prevents cudagraph speed ups (1.6x > 4.4 on TB inference)    Versions master)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Dynamo Not Preserving Bufferness, ğŸ› Describe the bug  Will result in 0 static inputs / no buffers passed to aot_autograd. This is a minified repro of llama issue I'm looking at that prevents cudagraph speed ups (1.6x > 4.4 on TB inference)    Versions master,2023-07-26T18:06:45Z,triaged oncall: pt2 module: dynamo,closed,0,8,https://github.com/pytorch/pytorch/issues/106060,"This repro doesn't run as is, a modified script to better illustrate the issue runs fine  Please resubmit as a failing unit test?",changing to register buffer is the fix to get what Elias wants.,  Still gives 0 fixed/ 0 buffers,"I think because of the set attr on the Graph Module we convert it to unspecialized, and dont preserver bufferness:  This is kind of a funny case, because the getattr and the setattr are the same variable, so it's a noop. E.g. we would also make an unspecialized module on `nn_module.buffer = nn_module.buffer`. ","> This is kind of a funny case, because the getattr and the setattr are the same variable, so it's a noop. E.g. we would also make an unspecialized module on nn_module.buffer = nn_module.buffer. I think they are different in dynamo,  may trigger side effect.",I guess we can special case this if you really want to....,I submitted pr,Fix: https://github.com/pytorch/pytorch/pull/106092
1008,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torchbench pin upd: hf auth token, clip, whisper, llamav2, sd)ï¼Œ å†…å®¹æ˜¯ (Includes stable diffusion, whisper, llama7b and clip To get this to work I had to Pass in hf auth token to all ci jobs, github does not pass in secrets from parent to child automatically. There's a likelihood HF will rate limit us in case please revert this PR and I'll work on adding a cache next   `hf_Bert` and `hf_Bert_large` are both failing on some dynamic shape looking error which I'm not sure how to debug yet so for now felt a bit gross but added a skip since others are building on top this work   `llamav2_7b_16h` cannot pass through accuracy checks cause it OOMs on deepcloning extra inputs this seems to make it not need to show up in expected numbers csv, will figure this when we update the pin with https://github.com/pytorch/benchmark/pull/1803 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"torchbench pin upd: hf auth token, clip, whisper, llamav2, sd","Includes stable diffusion, whisper, llama7b and clip To get this to work I had to Pass in hf auth token to all ci jobs, github does not pass in secrets from parent to child automatically. There's a likelihood HF will rate limit us in case please revert this PR and I'll work on adding a cache next   `hf_Bert` and `hf_Bert_large` are both failing on some dynamic shape looking error which I'm not sure how to debug yet so for now felt a bit gross but added a skip since others are building on top this work   `llamav2_7b_16h` cannot pass through accuracy checks cause it OOMs on deepcloning extra inputs this seems to make it not need to show up in expected numbers csv, will figure this when we update the pin with https://github.com/pytorch/benchmark/pull/1803 ",2023-07-26T05:21:51Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,26,https://github.com/pytorch/pytorch/issues/106009,CI failures looks related,"TODO:  * [x] clip and whisper just need their requirements.txt updated * [x] llama v2 needs a hf auth token https://github.com/pytorch/pytorch/actions/runs/5664989636/job/15349967063?pr=106009step:15:1803   do you mind if I add my own as a github secret? I did so in torchbench CI * [x] stable diffusion and hf t5 breakages are not expected, some weird thing going on because of moving out from canary models"," do you know if hf tokens are rate limited? If not, they sure, go ahead and do it","Reached out with my token offline, I can't add github secrets AFAIK to pytorch", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `msaroufimpatch7` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout msaroufimpatch7 && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.9clang12asan / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxjammypy3.9clang12asan / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `msaroufimpatch7` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout msaroufimpatch7 && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , feel free to ignore ASAN failures as they are unrelated to what you are trying to achieve here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxdocs / builddocsfunctorchfalse Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , ic, merge i, Merge started Your change will be merged while ignoring the following 1 checks: pull / linuxdocs / builddocsfunctorchfalse Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: inductor / cuda11.8py3.10gcc7sm86 / test (inductor_timm, 2, 2, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""previous commits show this is safe"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," https://github.com/pytorch/pytorch/actions/runs/5753300329/job/15597676255 started to fail with  Improvement: 3 models have accuracy status improved:     clip hf_Whisper stable_diffusion If it's expected, please follow the instructions in https://github.com/pytorch/pytorch/actions/runs/5753300329/job/15597676255   "
515,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE] Improve test_transformers test structure)ï¼Œ å†…å®¹æ˜¯ ( Summary We have a vast majority of test that only run on cuda. Decorating with  causes pytest to instantiate 2x the tests and skip half of them. This overhead is non trivial when the tests cross larger like it has for this file. This breaks up the cuda only tests into a separate class)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[BE] Improve test_transformers test structure, Summary We have a vast majority of test that only run on cuda. Decorating with  causes pytest to instantiate 2x the tests and skip half of them. This overhead is non trivial when the tests cross larger like it has for this file. This breaks up the cuda only tests into a separate class,2023-07-25T16:12:29Z,better-engineering Merged ciflow/trunk topic: not user facing,closed,3,5,https://github.com/pytorch/pytorch/issues/105938, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1478,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Flip default on `add_zero_attn` in `torch.nn.MultiheadAttention` to `True`)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I'm working on implementing a modified version of Softmax to be used in Transformer models that could greatly improve the performance of these models across the board. Evan Miller outlines the change in the blog post linked below, and it is an extremely simple one. This is my first issue on the pytorch library and I am happy to begin working on it. From what I know, this would involve: 1. Creating a new function within torch.nn.functional that resembles the torch.nn.functional.softmax code. 2. Add similar functionality to the next layers of code that inherit from torch.nn.functional. The quantized version comes to mind. 3. Add an nn.Module that while similar to nn.Softmax, uses the new torch.nn.functional.softmax_one code to be written for point 1. Credit to Evan Miller for this enlightening discovery: https://www.evanmiller.org/attentionisoffbyone.html  Alternatives A possible alternative could be me publishing my own nn.Module as a separate function.  Additional context This improvement is something that, while it would require retraining for Transformer models that want to use this new optimization, would not introduce any breaking changes to the torch library. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Flip default on `add_zero_attn` in `torch.nn.MultiheadAttention` to `True`," ğŸš€ The feature, motivation and pitch I'm working on implementing a modified version of Softmax to be used in Transformer models that could greatly improve the performance of these models across the board. Evan Miller outlines the change in the blog post linked below, and it is an extremely simple one. This is my first issue on the pytorch library and I am happy to begin working on it. From what I know, this would involve: 1. Creating a new function within torch.nn.functional that resembles the torch.nn.functional.softmax code. 2. Add similar functionality to the next layers of code that inherit from torch.nn.functional. The quantized version comes to mind. 3. Add an nn.Module that while similar to nn.Softmax, uses the new torch.nn.functional.softmax_one code to be written for point 1. Credit to Evan Miller for this enlightening discovery: https://www.evanmiller.org/attentionisoffbyone.html  Alternatives A possible alternative could be me publishing my own nn.Module as a separate function.  Additional context This improvement is something that, while it would require retraining for Transformer models that want to use this new optimization, would not introduce any breaking changes to the torch library. ",2023-07-25T15:56:49Z,module: nn triaged module: sdpa,open,0,4,https://github.com/pytorch/pytorch/issues/105934,It appears this may already exist? https://twitter.com/SamuelMullr/status/1683582347793530884 Maybe we pivot this issue towards changing the default value from `False` to `True`?,"Hi , thanks for the issue!  As you mentioned, this feature implemented by the `use_zero_attn` flag in `torch.nn.MultiheadAttention`.  For backwardscompatibility reasons, we might not want to flip the default at the current moment. However, if there is more interest in this in the form of activity on this issue or papers that re this optimization we would revisit this! ","Is this truly adding 1 to the denominator of the softmax probability function? Or this is an alternative solution to accomplish the same end goal? I see the K,V values are getting the zero tensor concatenated, but I'm not sure it is an equivalent solution."," Under the Softmax One and Quiet Attention section in the blog post you linked >  you can think of there being an extra zerovalued entry in ğ‘¥  (as exp(0)=1), as well as a zero vector in the ğ‘‰ matrix that attenuates the result. Where the $x$ discussed is $\frac{QK^{T}}{\sqrt{d}}$ leads me to believe this might be equivalent. That is a good point though, feel free to write out the math to disprove this! :) "
378,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix check issue for replace_params_with_constants)ï¼Œ å†…å®¹æ˜¯ (  CC(fix check issue for replace_params_with_constants) fix check issue for replace_params_with_constants to make llama mode const folding work. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,fix check issue for replace_params_with_constants,  CC(fix check issue for replace_params_with_constants) fix check issue for replace_params_with_constants to make llama mode const folding work. ,2023-07-25T09:01:42Z,open source Merged ciflow/trunk module: inductor ciflow/inductor release notes: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/105909, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
390,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update serialization in storage reduce)ï¼Œ å†…å®¹æ˜¯ (The serialization protocol used in the reduce method of the current storage object is the lagecy protocol, and it is better to update it to the default zip new protocol)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Update serialization in storage reduce,"The serialization protocol used in the reduce method of the current storage object is the lagecy protocol, and it is better to update it to the default zip new protocol",2023-07-25T06:19:50Z,open source topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/105898," label ""topic: not user facing"""
511,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(JIT input aliasing does not support aten::fill_)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug åœ¨å°†deformbledeträ»£ç ä½¿ç”¨TensorBoardè¿›è¡Œæ¨¡å‹ç»“æ„è¾“å‡ºæ—¶å‡ºç°ï¼š  deformable_deträ»£ç å¦‚ä¸‹ï¼š  è°ƒç”¨ä»£ç engine.pyä¸­å¦‚ä¸‹ï¼š   Versions ä»£ç æ¥æº https://github.com/xunull/readDeformableDETR  åªæ›´æ”¹äº†ä¸Šé¢ä¸¤ä¸ªæ–‡ä»¶çš„ä»£ç   å…¶ä½™æ–‡ä»¶ä»£ç æœªæ›´æ”¹   è¿è¡Œæ­¥éª¤1ã€cd models/ops  2ã€ python setup.py build install   3ã€cd ../../  4ã€python  main.py )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,JIT input aliasing does not support aten::fill_, ğŸ› Describe the bug åœ¨å°†deformbledeträ»£ç ä½¿ç”¨TensorBoardè¿›è¡Œæ¨¡å‹ç»“æ„è¾“å‡ºæ—¶å‡ºç°ï¼š  deformable_deträ»£ç å¦‚ä¸‹ï¼š  è°ƒç”¨ä»£ç engine.pyä¸­å¦‚ä¸‹ï¼š   Versions ä»£ç æ¥æº https://github.com/xunull/readDeformableDETR  åªæ›´æ”¹äº†ä¸Šé¢ä¸¤ä¸ªæ–‡ä»¶çš„ä»£ç   å…¶ä½™æ–‡ä»¶ä»£ç æœªæ›´æ”¹   è¿è¡Œæ­¥éª¤1ã€cd models/ops  2ã€ python setup.py build install   3ã€cd ../../  4ã€python  main.py ,2023-07-24T03:51:54Z,oncall: jit,open,0,6,https://github.com/pytorch/pytorch/issues/105821,"Hello, I've gone through this issue and parsed the following:  You are working from DeformableDETR and changed some code to include lines like `if reference.shape[1] == 4`.  This makes the code no longer traceable, because we do not have jit support for aten::fill_.  Thus, your request is either for us to support aten::fill_ or to give you a workaround. Currently, we do not have plans to implement more features for JIT, as we are trying to migrate to newer PT2 infrastructure. Thus,  `aten::fill_`?  I'd recommend working with DeformableDETR to derive a workaround in the meantime.","Please give me a workaround,Thank you very much. &nbsp;åŸå§‹é‚®ä»¶&nbsp; å‘ä»¶äºº: ""Jane (Yuan) ***@***.***&gt;;  å‘é€æ—¶é—´: 2023å¹´7æœˆ24æ—¥(æ˜ŸæœŸä¸€) æ™šä¸Š11:20 æ”¶ä»¶äºº: ***@***.***&gt;;  æŠ„é€: ***@***.***&gt;; ***@***.***&gt;;  ä¸»é¢˜: Re: [pytorch/pytorch] JIT input aliasing does not support aten::fill_ (Issue CC(JIT input aliasing does not support aten::fill_)) Hello, I've gone through this issue and parsed the following: You are working from DeformableDETR and changed some code to include lines like if reference.shape[1] == 4. This makes the code no longer traceable, because we do not have jit support for aten::fill_. Thus, your request is either for us to support aten::fill_ or to give you a workaround. Currently, we do not have plans to implement more features for JIT, as we are trying to migrate to newer PT2 infrastructure. Thus, ::fill_? I'd recommend working with DeformableDETR to derive a workaround in the meantime. â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;"," CC([JIT] Bool Should Subtype NumberType) This is basically the issue where TorchScript  doesn't know that booleans are a type of scalar. Workaround  where `fill_()` is getting called, replace the boolean with an integer  `1` for true, `0` for false. Or int(True) would work. Minimal repro: ","Thank you for your answer. &nbsp;åŸå§‹é‚®ä»¶&nbsp; å‘ä»¶äºº: ""David ***@***.***&gt;;  å‘é€æ—¶é—´: 2023å¹´7æœˆ26æ—¥(æ˜ŸæœŸä¸‰) ä¸Šåˆ7:28 æ”¶ä»¶äºº: ***@***.***&gt;;  æŠ„é€: ***@***.***&gt;; ***@***.***&gt;;  ä¸»é¢˜: Re: [pytorch/pytorch] JIT input aliasing does not support aten::fill_ (Issue CC(JIT input aliasing does not support aten::fill_)) CC([JIT] Bool Should Subtype NumberType) This is basically the issue where TorchScript  doesn't know that booleans are a type of scalar. Workaround  where fill_() is getting called, replace the boolean with an integer  1 for true, 0 for false. Or int(True) would work. Minimal repro:  import torch def fn(x):     x.fill_(True)     return x + 2  fn_s = torch.jit.script(fn) x = torch.zeros((4, 4), dtype=torch.bool) fn_s = torch.jit.trace(fn, (x,)) fn_s(x) â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***&gt;","> Thank you for your answer. > [â€¦]() > &nbsp;åŸå§‹é‚®ä»¶&nbsp; å‘ä»¶äºº: ""David ***@***.***&gt;; å‘é€æ—¶é—´: 2023å¹´7æœˆ26æ—¥(æ˜ŸæœŸä¸‰) ä¸Šåˆ7:28 æ”¶ä»¶äºº: ***@***.***&gt;; æŠ„é€: ***@***.***&gt;; ***@***.***&gt;; ä¸»é¢˜: Re: [pytorch/pytorch] JIT input aliasing does not support aten::fill_ (Issue CC(JIT input aliasing does not support aten::fill_)) CC([JIT] Bool Should Subtype NumberType) This is basically the issue where TorchScript doesn't know that booleans are a type of scalar. Workaround  where fill_() is getting called, replace the boolean with an integer  1 for true, 0 for false. Or int(True) would work. Minimal repro: import torch def fn(x): x.fill_(True) return x + 2  fn_s = torch.jit.script(fn) x = torch.zeros((4, 4), dtype=torch.bool) fn_s = torch.jit.trace(fn, (x,)) fn_s(x) â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you authored the thread.Message ID: ***@***.***&gt; same problem with you,have you solved it?  i have no idea to fix it.  ",torch1.12.1cu116 works for me.
1986,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Errors while trying to finetune compiled transformers model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm switching to Pytorch 2.0.1 and want to compile the model for training times improvement. I'm trying to compile the model according to the docs, but it does not work and throws an error.  The code looks this:  Here is the link to colab version (""Pytorch API"" section)  Error logs   Minified repro _No response_  Versions PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: 14.0.01ubuntu1 CMake version: version 3.25.2 Libc version: glibc2.35 Python version: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.109+x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 525.105.17 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          2 Online CPU(s) list:             0,1 Vendor ID:                       GenuineIntel Model name:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Errors while trying to finetune compiled transformers model," ğŸ› Describe the bug I'm switching to Pytorch 2.0.1 and want to compile the model for training times improvement. I'm trying to compile the model according to the docs, but it does not work and throws an error.  The code looks this:  Here is the link to colab version (""Pytorch API"" section)  Error logs   Minified repro _No response_  Versions PyTorch version: 2.0.1+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: 14.0.01ubuntu1 CMake version: version 3.25.2 Libc version: glibc2.35 Python version: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.109+x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 525.105.17 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   46 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          2 Online CPU(s) list:             0,1 Vendor ID:                       GenuineIntel Model name:   ",2023-07-23T15:31:23Z,needs reproduction triaged oncall: pt2 module: dynamo dynamo-must-fix,open,0,11,https://github.com/pytorch/pytorch/issues/105802,The colab link seems to be broken. Can you provide a standalone script for repro?," hm, that's weird, for me it opens even in incognito mode. But here is the standalone script: ","Hmm, I'm getting this error instead: ","Indeed, you are right. The error I pasted appears if I run all the cells one by one in this colab, even the model and the arguments are recreated, and that's pretty weird. Regarding the error you are facing (another colab) it goes away if I comment the line with `torch.compile` call, so the compilation still does not work as expected and as stated in the docs","I tried this a bit. Without torch.compile on the model, a row of training data looks like  but after applying torch.compile on the model, a row of training data looks like  Keys other than label are all get lost. This smells like relevant to dynamo. ?"," thanks for the nice summary, this now reminds me a lot of this issue  CC(torch.compile specializes on output name)","Help me debug this :) I get `ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']` Is there a smaller repro I can look at? ", the smallest one is this one,"same issue, any solution here?",LittleBoy Do you have a repro that I can try on my end?, is  CC(Errors while trying to finetune compiled transformers model)issuecomment1650216293 working for you?
421,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] aten::erfinv bug fix: add storage offset buffers to handle slicing)ï¼Œ å†…å®¹æ˜¯ (A bug fix of a recently merged PR per comment: https://github.com/pytorch/pytorch/pull/101507discussion_r1271393706 The follow test would fail without this bug fix: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MPS] aten::erfinv bug fix: add storage offset buffers to handle slicing,A bug fix of a recently merged PR per comment: https://github.com/pytorch/pytorch/pull/101507discussion_r1271393706 The follow test would fail without this bug fix: ,2023-07-23T14:24:33Z,triaged open source Merged release notes: mps ciflow/mps,closed,0,8,https://github.com/pytorch/pytorch/issues/105801,Updated PR to include new slice and view tests in test_mps.py.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",what's the hold up on this one?,I think it was just missed in their notifications...,"Hmm, I think that's the first time I see this PR... looking"," merge f ""MPS + Lint is green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes).  Please use `f` as last resort and instead consider `i/ignorecurrent` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge. Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
2038,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DDP , error . [c10d] The client socket has timed out after 900s while trying to connect to (XX.XX.XX.XX, 8514).)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug code: python m torch.distributed.launch use_env nproc_per_node=${GPUS_PER_NODE} nnodes=${WORKER_CNT} node_rank=${RANK} \           master_addr=""172.22.48.199"" master_port=${MASTER_PORT} cn_clip/training/main.py \ error: [W socket.cpp:601] [c10d] The IPv6 network addresses of (XX.XX.XX.XX, 8514) cannot be retrieved (gai error: 2  Name or service not known). [E socket.cpp:860] [c10d] The client socket has timed out after 900s while trying to connect to (XX.XX.XX.XX, 8514). Traceback (most recent call last):   File ""/mnt/e/wsldata/conda/lib/python3.9/runpy.py"", line 197, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/mnt/e/wsldata/conda/lib/python3.9/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 196, in      main()   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 192, in main     launch(args)   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 177, in launch     run(args)   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/run.py"", line 785, in run     elastic_launch(   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 241, in launch_agent     result = agent.run()   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/elastic/metrics/api)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"DDP , error . [c10d] The client socket has timed out after 900s while trying to connect to (XX.XX.XX.XX, 8514)."," ğŸ› Describe the bug code: python m torch.distributed.launch use_env nproc_per_node=${GPUS_PER_NODE} nnodes=${WORKER_CNT} node_rank=${RANK} \           master_addr=""172.22.48.199"" master_port=${MASTER_PORT} cn_clip/training/main.py \ error: [W socket.cpp:601] [c10d] The IPv6 network addresses of (XX.XX.XX.XX, 8514) cannot be retrieved (gai error: 2  Name or service not known). [E socket.cpp:860] [c10d] The client socket has timed out after 900s while trying to connect to (XX.XX.XX.XX, 8514). Traceback (most recent call last):   File ""/mnt/e/wsldata/conda/lib/python3.9/runpy.py"", line 197, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/mnt/e/wsldata/conda/lib/python3.9/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 196, in      main()   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 192, in main     launch(args)   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launch.py"", line 177, in launch     run(args)   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/run.py"", line 785, in run     elastic_launch(   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 134, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/launcher/api.py"", line 241, in launch_agent     result = agent.run()   File ""/mnt/e/wsldata/conda/envs/cnclip/lib/python3.9/sitepackages/torch/distributed/elastic/metrics/api",2023-07-22T05:22:37Z,oncall: distributed,open,0,0,https://github.com/pytorch/pytorch/issues/105782
2015,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([TransformerEngine&Kineto] `RuntimeError: *tag_it == Tag::Tensor INTERNAL ASSERT FAILED`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I'm running some tests on H100 with TransformerEngine, and this simple code causes an assertion failure when enabling fp8 (corresponding issue in TransformerEngine: https://github.com/NVIDIA/TransformerEngine/issues/336):  Repro   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230721+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01033awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.2.91 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Ad)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[TransformerEngine&Kineto] `RuntimeError: *tag_it == Tag::Tensor INTERNAL ASSERT FAILED`," ğŸ› Describe the bug Hello, I'm running some tests on H100 with TransformerEngine, and this simple code causes an assertion failure when enabling fp8 (corresponding issue in TransformerEngine: https://github.com/NVIDIA/TransformerEngine/issues/336):  Repro   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230721+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01033awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.2.91 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3 GPU 1: NVIDIA H100 80GB HBM3 GPU 2: NVIDIA H100 80GB HBM3 GPU 3: NVIDIA H100 80GB HBM3 GPU 4: NVIDIA H100 80GB HBM3 GPU 5: NVIDIA H100 80GB HBM3 GPU 6: NVIDIA H100 80GB HBM3 GPU 7: NVIDIA H100 80GB HBM3 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.9.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.9.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Ad",2023-07-21T16:18:47Z,oncall: profiler,closed,0,1,https://github.com/pytorch/pytorch/issues/105748,Minimal repro here  Will try to write a fix within the next few days
480,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error using torch.compile with HF transformers and model `mosaicml/mpt-7b`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Below is a reproducible example of what is failing. It seems to relate to `rearrange` function from `einops` package.  Tagging  (at the suggestion of )   Error logs   Versions  Conda env:  pip freeze:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Error using torch.compile with HF transformers and model `mosaicml/mpt-7b`, ğŸ› Describe the bug Below is a reproducible example of what is failing. It seems to relate to `rearrange` function from `einops` package.  Tagging  (at the suggestion of )   Error logs   Versions  Conda env:  pip freeze:  ,2023-07-20T20:09:56Z,triaged oncall: pt2 module: dynamic shapes,closed,0,4,https://github.com/pytorch/pytorch/issues/105686,This is being discussed here  CC(Regression in Dalle2 due to dynamic shapes),"I can confirm that this issue is resolved by the change to `einops` proposed by  here: https://github.com/arogozhnikov/einops/pull/260/files I'ma able to use this patched einops as a temporary workaround, but since that PR was closed, it seems like a longterm solution is still needed. ",The real POR for einops hashing of symints is to support a hash on symints that specializes to the hashed value.,I believe this has been resolved in latest einops release: https://github.com/arogozhnikov/einops/releases/tag/v0.7.0rc2
1399,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I have attempted to run Llama 2 on Mseries (M1/M2) Mac GPUs here: https://github.com/Samyak2/llamamps  Current status The models loads correctly but inference fails because:  [ ] The `ComplexFloat` dtype is not supported in MPS yet (Closest existing issue I found:  CC(FFT operators are not supported on MPS device))  [ ] The `aten::view_as_complex` operator is not supported in MPS yet ( CC(General MPS op coverage tracking issue))  [ ] The `aten::polar.out` operator is not supported in MPS yet. This can be worked around by setting `PYTORCH_ENABLE_MPS_FALLBACK=1` which runs the operator on CPU instead. For full performance, this operator would need to be supported too. There may be more operators and types that may need to be supported. I have not dug further on this since it crashes due to `ComplexFloat` not being supported.  Alternatives There have been forks of Llama to make it work on CPU instead. Examples: https://github.com/b0kch01/llamacpu These will leave a lot of performance on the table though.  Additional context Failure logs for context (from https://github.com/Samyak2/llamamps):  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators," ğŸš€ The feature, motivation and pitch I have attempted to run Llama 2 on Mseries (M1/M2) Mac GPUs here: https://github.com/Samyak2/llamamps  Current status The models loads correctly but inference fails because:  [ ] The `ComplexFloat` dtype is not supported in MPS yet (Closest existing issue I found:  CC(FFT operators are not supported on MPS device))  [ ] The `aten::view_as_complex` operator is not supported in MPS yet ( CC(General MPS op coverage tracking issue))  [ ] The `aten::polar.out` operator is not supported in MPS yet. This can be worked around by setting `PYTORCH_ENABLE_MPS_FALLBACK=1` which runs the operator on CPU instead. For full performance, this operator would need to be supported too. There may be more operators and types that may need to be supported. I have not dug further on this since it crashes due to `ComplexFloat` not being supported.  Alternatives There have been forks of Llama to make it work on CPU instead. Examples: https://github.com/b0kch01/llamacpu These will leave a lot of performance on the table though.  Additional context Failure logs for context (from https://github.com/Samyak2/llamamps):  ",2023-07-20T15:31:09Z,triaged module: mps,closed,3,10,https://github.com/pytorch/pytorch/issues/105665,"Hi, Sounds like the exact same thing as llama1: you can just update the apply_rotary_emb function not to use complex. You will just have to write the equivalent of the complex multiplication by hand.","> Hi, >  > Sounds like the exact same thing as llama1: you can just update the apply_rotary_emb function not to use complex. You will just have to write the equivalent of the complex multiplication by hand. Thanks for the tip ! I tried implementing this but something is clearly wrong because all the outputs are `?? ?? ??`.  Here's my implementation:  Here's the original code for reference.","After some more investigation, my rewritten implementation of `apply_rotary_emb` seems correct. The output tensors are exactly the same as the original version. I'm not sure why the outputs are the way they are. The branch with these changes is here if anyone wants to try it out: https://github.com/Samyak2/llamamps/pull/1",I got something to work here btw in case anyone is interested https://github.com/karpathy/llama2.c/pull/14/  EDIT:   I did try doing this rewrite but then M1 was about 10x slower than just doing complex on CPU so might need to look into this a bit more  lemme know if you see any obvious issues,Do you have sample inputs for which you see this slowdown? Doing manual complex is expected to be slower than builtin when available in hw :/ ,"Not sure how helpful it'll be but here's the inputs for the real and complex versions https://gist.github.com/msaroufim/57199b3dcfda2fcd10f8d7baf8a739f2 And just some quick benchmark showed * Real version on MPS: apply_rotary_emb took 0.0017440319061279297 seconds * Complex version on CPU: apply_rotary_emb took 5.5789947509765625e05 seconds Is complex support not supported by Apple hardware or is it just that someone needs to write more code in PyTorch? If it's the latter, could I presumably do it as a side project (12 dedicated days) EDIT: I saw you already outlined how people can do it here  CC(General MPS op coverage tracking issue) lemme poke around","> Not sure how helpful it'll be but here's the inputs for the real and complex versions https://gist.github.com/msaroufim/57199b3dcfda2fcd10f8d7baf8a739f2 >  > And just some quick benchmark showed >  > * Real version on MPS: apply_rotary_emb took 0.0017440319061279297 seconds > * Complex version on CPU: apply_rotary_emb took 5.5789947509765625e05 seconds >  > Is complex support not supported by Apple hardware or is it just that someone needs to write more code in PyTorch? If it's the latter, could I presumably do it as a side project (12 dedicated days) >  > EDIT: I saw you already outlined how people can do it here CC(General MPS op coverage tracking issue) lemme poke around  thanks for the updates . We do have complex support in MPSGraph MacOS 14 . We need to enable it in PyTorch stack . If you are willing, it will be great if you take up the effort of adding complex support . If you have questions we are happy to help you with it ", would it be possible to add some more detail/link to docs about the complex support in MPSGraph MacOS 14?  There is the example pr for adding ops that already exist in the MPS back end  do you think following this is still helpful?,"Hi all, any updates on this?",Closing this one as Complex support for limited ops mentioned here has been added by https://github.com/pytorch/pytorch/pull/107324 and general complex op support has been enabled for MacOS 14 by https://github.com/pytorch/pytorch/pull/115513
2006,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] is unsupported)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The following code yields an `Unsupported` error:  By contrast the analogous code using lists does work:  As does the version where you're adding to a nonconstant tuple:  Notably, this bug causes the HuggingFace `transformers` implementation of the Llama model to fail to compile when `fullgraph=True`, since it uses `+=` on an empty tuple.  Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.27 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.19.042genericx86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 10.2.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A40 GPU 1: NVIDIA A40 GPU 2: NVIDIA A40 GPU 3: NVIDIA A40 GPU 4: NVIDIA A40 GPU 5: NVIDIA A40 GPU 6: NVIDIA A40 GPU 7: NVIDIA A40 Nvidia driver version: 515.105.01 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              96 Online CPU(s) list: 095 Thread(s) per core:  2 Core(s) per socket:  24 Socket(s):           2 NUMA node(s):        8 Vendor ID:           AuthenticAMD CPU family:          25 Model:               1 Model name:          AMD EPYC 7413 24Core Processor Stepping:            1 CPU MHz:             1861.845 CPU max MHz:         3630.8101 CPU min MHz:         1500.0000 BogoMIPS:            5300.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] is unsupported"," ğŸ› Describe the bug The following code yields an `Unsupported` error:  By contrast the analogous code using lists does work:  As does the version where you're adding to a nonconstant tuple:  Notably, this bug causes the HuggingFace `transformers` implementation of the Llama model to fail to compile when `fullgraph=True`, since it uses `+=` on an empty tuple.  Versions PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.27 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.19.042genericx86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 10.2.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A40 GPU 1: NVIDIA A40 GPU 2: NVIDIA A40 GPU 3: NVIDIA A40 GPU 4: NVIDIA A40 GPU 5: NVIDIA A40 GPU 6: NVIDIA A40 GPU 7: NVIDIA A40 Nvidia driver version: 515.105.01 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              96 Online CPU(s) list: 095 Thread(s) per core:  2 Core(s) per socket:  24 Socket(s):           2 NUMA node(s):        8 Vendor ID:           AuthenticAMD CPU family:          25 Model:               1 Model name:          AMD EPYC 7413 24Core Processor Stepping:            1 CPU MHz:             1861.845 CPU max MHz:         3630.8101 CPU min MHz:         1500.0000 BogoMIPS:            5300.",2023-07-20T04:55:46Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/105643,Fixed in nightly
817,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP with gradient checkpointing lead to redundant allgathers during backward)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug While training huggingface Llama 13B with FSDP with full shard and gradient checkpointing enabled on a single node, I observed that the backward pass has two allgathers per layer. Compared to non checkpointed training, this additional allgather also affects the overlap between the reduce scatter and gradient computation.  I think ideally only one allgather is needed to gather the weights per FSDP module.  Trace: (browm is reduce scatter, blue is allgather) !allgather   Versions I'm using pytorch nightly: `2.1.0.dev20230709+cu121` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,FSDP with gradient checkpointing lead to redundant allgathers during backward," ğŸ› Describe the bug While training huggingface Llama 13B with FSDP with full shard and gradient checkpointing enabled on a single node, I observed that the backward pass has two allgathers per layer. Compared to non checkpointed training, this additional allgather also affects the overlap between the reduce scatter and gradient computation.  I think ideally only one allgather is needed to gather the weights per FSDP module.  Trace: (browm is reduce scatter, blue is allgather) !allgather   Versions I'm using pytorch nightly: `2.1.0.dev20230709+cu121` ",2023-07-20T03:48:09Z,triaged module: fsdp,open,0,6,https://github.com/pytorch/pytorch/issues/105635,"How are you applying checkpointing? Also, if you could share the trace file, that could be helpful.",Huggingface's llama has builtin checkpointing. I simply turned it on via a flag.  https://github.com/huggingface/transformers/blob/9ef5256dfb28f5419649af8ea94e82573a6b2e77/src/transformers/models/llama/modeling_llama.pyL690L696,Here's the trace of a single worker. Let me know if you need all workers.  llama.tar.gz,"Thanks! I will find some time to look at the traces soon, but in the short term, could you try: 1. Do not enable checkpointing via the builtin flag 2. Instead use the following: ","With your new code, the problem seems to disappear. Hmm interesting !image","There is some complexity in how FSDP and activation checkpointing interoperate. The FSDP changes in https://github.com/pytorch/pytorch/pull/105090 might help your original use case work, but I am not sure. We have not landed those changes yet though."
1743,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.nn.TransformerDecoderLayer lacks parameter validation check)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Description: torch.nn.TransformerDecoderLayer lacks parameter validation check, and when invalid values are given, they are used in subsequent computations, leading to errors such as division by zero.  Examples: input:  error_message:   Versions PS D:\PythonProjects\venv\Lib\sitepackages\torch\utils> python collect_env.py Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 å®¶åº­ä¸­æ–‡ç‰ˆ GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU Nvidia driver version: 532.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2200 DeviceID=CPU0 Family=207 L2CacheSize=16384 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2200 Name=13th Gen Intel(R) Core(TM) i913900HX ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==2.0.1+cu117 [pip3] torchaudio==2.0.2+cu117 [pip3] torchvision==0.15.2+cu117 [conda] Could not collect )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.nn.TransformerDecoderLayer lacks parameter validation check," ğŸ› Describe the bug  Description: torch.nn.TransformerDecoderLayer lacks parameter validation check, and when invalid values are given, they are used in subsequent computations, leading to errors such as division by zero.  Examples: input:  error_message:   Versions PS D:\PythonProjects\venv\Lib\sitepackages\torch\utils> python collect_env.py Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 å®¶åº­ä¸­æ–‡ç‰ˆ GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU Nvidia driver version: 532.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture=9 CurrentClockSpeed=2200 DeviceID=CPU0 Family=207 L2CacheSize=16384 L2CacheSpeed= Manufacturer=GenuineIntel MaxClockSpeed=2200 Name=13th Gen Intel(R) Core(TM) i913900HX ProcessorType=3 Revision= Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==2.0.1+cu117 [pip3] torchaudio==2.0.2+cu117 [pip3] torchvision==0.15.2+cu117 [conda] Could not collect ",2023-07-20T02:41:58Z,module: nn triaged,open,0,2,https://github.com/pytorch/pytorch/issues/105632,"Would the solution here be to add an 'if num_heads < 1: raise ValueError(""..."")' in the MultiHeadAttention class? Would we want a type check here too?","> Would the solution here be to add an 'if num_heads < 1: raise ValueError(""..."")' in the MultiHeadAttention class? Would we want a type check here too? Sounds like a good solution"
259,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reenable UFMT on pyi)ï¼Œ å†…å®¹æ˜¯ (  CC(Reenable UFMT on pyi) Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Reenable UFMT on pyi,  CC(Reenable UFMT on pyi) Signedoffby: Edward Z. Yang ,2023-07-19T18:20:16Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/105577, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
304,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(tts_angular: fail_to_run, torch._dynamo.exc.Unsupported: call_method NNModuleVariable() flatten_parameters [] {})ï¼Œ å†…å®¹æ˜¯ (Repro:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"tts_angular: fail_to_run, torch._dynamo.exc.Unsupported: call_method NNModuleVariable() flatten_parameters [] {}",Repro:  ,2023-07-19T13:46:01Z,triaged oncall: pt2 dynamo-nn-modules,open,0,1,https://github.com/pytorch/pytorch/issues/105532,",  any update on this? It still repros"
1002,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implement NEON version of ERF())ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Currently, the fp32 `erf()` function on Intel machines goes through either the AVX512 path or the AVX2 path . These implementations are much faster than the corresponding std math & SLEEF implementations (as described in CC([cpu] implement erf based on oneDNN algorithm for aten::Vec)). But the same function on ARM takes the std::erf() path, or the SLEEF path, but that is hard to invoke ( CC(Enable SLEEF on ARM)).  Since the `erf()` function is a hotspot for transformer models (GELU activation), I propose adding a NEON intrinsics implementation for `erf()` as well. I see around 5060% speed improvements after doing so. Posting this here for discussion/comments before raising a PR.  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Implement NEON version of ERF()," ğŸš€ The feature, motivation and pitch Currently, the fp32 `erf()` function on Intel machines goes through either the AVX512 path or the AVX2 path . These implementations are much faster than the corresponding std math & SLEEF implementations (as described in CC([cpu] implement erf based on oneDNN algorithm for aten::Vec)). But the same function on ARM takes the std::erf() path, or the SLEEF path, but that is hard to invoke ( CC(Enable SLEEF on ARM)).  Since the `erf()` function is a hotspot for transformer models (GELU activation), I propose adding a NEON intrinsics implementation for `erf()` as well. I see around 5060% speed improvements after doing so. Posting this here for discussion/comments before raising a PR.  Alternatives _No response_  Additional context _No response_ ",2023-07-18T19:49:00Z,module: build module: sleef oncall: mobile module: arm topic: improvements,closed,0,2,https://github.com/pytorch/pytorch/issues/105493," if you have neonaccelerated ERF implementation, please do not hesitate to propose a PR and ask me to review it","Thanks , I have raised the PR CC(Implement NEON accelerated implementation of ERF())"
978,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Specifying `FakeTensorMode` for Custom Backends)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When specifying the `FakeTensorMode` to be used for custom backend implementations, the utility `fake_tensor_unsupported` is very useful for indicating _no_ fake tensors should be allowed, but I could not find similar utilities for specifying custom fake modes, for instance `FakeTensorMode(allow_non_fake_inputs=True)`. An attempt was made to set the fake mode directly in the tracing context upon entry into the backend function (see Minified repro), however this causes an error upon completion of the compilation. What is the recommended way to set the `FakeTensorMode` for a custom backend?  Error logs   Minified repro Below is a demo of how the `FakeTensorMode` was set in the backend.   Versions **Relevant Versions**  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Specifying `FakeTensorMode` for Custom Backends," ğŸ› Describe the bug When specifying the `FakeTensorMode` to be used for custom backend implementations, the utility `fake_tensor_unsupported` is very useful for indicating _no_ fake tensors should be allowed, but I could not find similar utilities for specifying custom fake modes, for instance `FakeTensorMode(allow_non_fake_inputs=True)`. An attempt was made to set the fake mode directly in the tracing context upon entry into the backend function (see Minified repro), however this causes an error upon completion of the compilation. What is the recommended way to set the `FakeTensorMode` for a custom backend?  Error logs   Minified repro Below is a demo of how the `FakeTensorMode` was set in the backend.   Versions **Relevant Versions**  ",2023-07-18T17:58:33Z,feature triaged oncall: pt2 module: fakeTensor,open,0,17,https://github.com/pytorch/pytorch/issues/105485,"I was able to get around the above error by adding a `ShapeEnv` to the `FakeTensorMode`, as so:  Still curious if the above is the best way to set the fake mode for custom backends, or if there is a preferable alternative.","Additionally, with the following lines in the backend:  I see this error, on certain models:  When those lines are commented out in the backend, no such error appears."," Don't do this. The fake mode needs to be consistent with any fake tensors you're getting from Dynamo, and you're also modifying the state in a nonscoped way. Can you tell us more about why you want to `allow_non_fake_inputs`? Dynamo should be consistently fakeifying everything, so it should not be necessary to set this.","  that makes sense. The main reason for `allow_non_fake_inputs` is to allow Tensor constant freezing within the graph. When I try to access an `nn.Parameter` or other constant Tensor in the graph at compiletime, the value observed is always a `FakeTensor`, so the data cannot be accessed. For instance, when freezing the weight and bias of a convolution layer, I would try `weight.data` (where `weight` is the `nn.Parameter` of an `nn.Convolution` instance, represented in the graph via a `get_attr`), but this returns a `FakeTensor`. With `allow_non_fake_inputs`, I was able to get around this issue and successfully freeze constant tensors in the graph.","It sounds like the real request here is for freezing.  Is that correct ? We have a mechanism for freezing built into inductor right now: `import torch._inductor.config.freezing`.  There was another request for freezing here:  CC(Support an option to keep weights as attributes in aot_autograd). We could make abstract out freezing and make that an option for backends, .","The request is related to freezing and to the referenced issue, but is slightly different. I am already using the `_inductor` freezing utilities to transform the `placeholder` nodes into `get_attr` nodes. The issue is that in order to register these frozen nodes with the backend, I need to access their component data at compiletime, which the inductor utility does not enable.","> I need to access their component data at compiletime, which the inductor utility does not enable. The data should be present on the GraphModule. You should be able to get the tensors in the same way we are setting them here.  Also, related to your above comment about `allow_non_fake_inputs`, does the following pattern solve your issue ?  https://github.com/pytorch/pytorch/blob/main/torch/_inductor/freezing.pyL176L178","This makes sense, and thank you for the reference  I will test this out. To verify, instead of the following, would it be an issue to simply set `TracingContext.get().fake_mode.allow_non_fake_inputs = True` in the main function of the backend? Or is it preferable to only switch on `allow_non_fake_inputs` in the specific location it is used? https://github.com/pytorch/pytorch/blob/73e145532788fd4071a41ea431883bef9633809d/torch/_inductor/freezing.pyL176L178","I tested out adding the above code to my freezing functionality, and I see the following error:  The above is caused by these two specific locations in the code 1, 2, where the model is invoked during compilation. The `forward` call encounters some of the real tensor constants such as the convolution weight, causing the above error. If I wrap those two calls in the following decorator, they succeed:  Is the above the recommended way to have this work? An alternative that also works is setting `TracingContext.get().fake_mode.allow_non_fake_inputs = True` from inside the main backend function.","Oh, I see, the problem is you're trying to actually to call the real model with real parameters with fake inputs ğŸ¤”  I can't think of a better recommended way to do this, besides fakeifying the model object first before running with fake inputs. The main hazard is if you mutate the parameters in the model, but being parameters I guess this is unlikely.","Thanks for the feedback   on the related topic of freezing tensors, in the backend, we currently use AOT autograd `aot_module_simplified` to lower the graph to ATen ops, however the following code makes it difficult to freeze tensors within the graph: https://github.com/pytorch/pytorch/blob/0ab74044c2775970d3bc3668454a3152ae18ea82/torch/_functorch/aot_autograd.pyL3765L3769 The reason is that the returned forward function comes with the `params_flat` as an automatic input, meaning if the `GraphModule` is modified to freeze certain inputs, the returned `forward` function will be incorrect. Would it be recommended to use `aot_export_module`, `aot_module`, or `aot_function` in this case instead?","Yes, you should use one of those variants instead. Stock AOTAutograd will make unrestricted changes to the calling convention as it assumes it is also in control of the call site. The other aot_ variants have more restricted calling convention changes they'are allowed to make.","Hello  I wanted to check if there are any recent alternatives to the following method for temporarily modifying the `FakeTensorMode` in torch compile backends. I noticed that this code no longer shows up in the location referenced above in Torch Inductor. I was wondering if there might be a context manager such as `with allow_non_fake_inputs(...)`, which could enable this behavior? ","We probably won't gratuitously break the monkeypatching here but the ""proper"" way is to properly fakeify params/inputs before running things in fake mode. Maybe one day I'll convince  to pull the switch on `allow_non_fake_inputs=True` by default lol.",I'd consider switching the default. It was mostly in the context of dynamo and inductor that turning that flag on would hide bugs.,"olive,  has this been resolved?","We do still use `with unittest.mock.patch.object(fake_mode, ""allow_non_fake_inputs"", True), fake_mode:`, which is functional for now  are there any alternatives available currently?"
794,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP loading with a partial state triggers KeyError)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug In finetuning cases, you might want to save a subset of your model to reduce the size of your checkpoints. This is particularly important when techniques such as LoRA are used with very large models. The suggested way to do this is to filter the keys of the model's `state_dict` However, this seems to break FSDP loading:   A related feature request of mine is  CC(FSDP saving with a partial `state_dict` triggers assertion) where I asked if FSDP could be made to work if the model didn't include all layers in the `state_dict`  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,FSDP loading with a partial state triggers KeyError," ğŸ› Describe the bug In finetuning cases, you might want to save a subset of your model to reduce the size of your checkpoints. This is particularly important when techniques such as LoRA are used with very large models. The suggested way to do this is to filter the keys of the model's `state_dict` However, this seems to break FSDP loading:   A related feature request of mine is  CC(FSDP saving with a partial `state_dict` triggers assertion) where I asked if FSDP could be made to work if the model didn't include all layers in the `state_dict`  Versions  ",2023-07-18T00:32:00Z,triaged module: fsdp,open,1,1,https://github.com/pytorch/pytorch/issues/105379,
792,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`torch.linalg.eigh` fails on GPU and corrupts memory)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `torch.linalg.eigh` fails on some large lowrank float32 matrices on GPU, but succeeds on CPU or when cast to float64. (See similar issue at  CC(`torch.linalg.eigh` fails on GPU)). After failing, the matrix cannot be accessed again without causing a CUDA illegal memory access error. An example matrix that fails can be found here: rank7_idx0.1.3.0_iter100_factor.pt.zip This matrix was generated when applying the Shampoo optimizer to HF T5 finetuning.  cusolver eigendecomposition error:  CUDA illegal memory access error:    Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,`torch.linalg.eigh` fails on GPU and corrupts memory," ğŸ› Describe the bug `torch.linalg.eigh` fails on some large lowrank float32 matrices on GPU, but succeeds on CPU or when cast to float64. (See similar issue at  CC(`torch.linalg.eigh` fails on GPU)). After failing, the matrix cannot be accessed again without causing a CUDA illegal memory access error. An example matrix that fails can be found here: rank7_idx0.1.3.0_iter100_factor.pt.zip This matrix was generated when applying the Shampoo optimizer to HF T5 finetuning.  cusolver eigendecomposition error:  CUDA illegal memory access error:    Versions ",2023-07-17T21:33:34Z,triage review module: cuda triaged module: linear algebra,closed,3,1,https://github.com/pytorch/pytorch/issues/105359,"I can repro this issue. Now, let's take the SVD of the matrix. Its smallest singular values are  It's clear that this matrix is *very* close to being singular. In particular, this falls under https://pytorch.org/docs/main/notes/numerical_accuracy.htmlextremalvaluesinlinalg, so it is expected. As recommended there, if you are working with very singular matrices, there is two ways of going about it: 1. The quick and dirty: Perform the operation in `float64` and hope for the best 2. Apply some preconditioner to your matrix that regularises it. In other words, it moves its spectrum towards a more amenable one. Option 1. works in this case. `print(torch.linalg.eigh(factor_matrix.to(torch.float64)))` just succeeds. All this being said, cusolver should not crash in an unrecoverable way if possible. ://github.com/pytorch/pytorch/issues/94772, as you mentioned. Let's continue the discussion there."
389,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inductor cpp wrapper: support torch.complex64)ï¼Œ å†…å®¹æ˜¯ (  CC(Inductor cpp wrapper: support torch.complex64) Add `torch.complex64` into the supported dtype list of cpp wrapper to fix CPU cpp wrapper failure on llama. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Inductor cpp wrapper: support torch.complex64,  CC(Inductor cpp wrapper: support torch.complex64) Add `torch.complex64` into the supported dtype list of cpp wrapper to fix CPU cpp wrapper failure on llama. ,2023-07-17T05:26:05Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/105305, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/chunyuanw/54/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/105305`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
697,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exercise subclass of TransformerEncoderLayer)ï¼Œ å†…å®¹æ˜¯ (Summary: Exercise subclass of TransformerEncoderLayer Additional unit tests for change in CC(Move static checks of layers[0] (e.g., isinstance check) to model build time) to show correct e2e operation (cf. issue CC(Subclassed `TransformerEncoderLayer` breaks scripting of TransformerEncoder)) Also: remove batch_first from list of TS module constants where it is not used to resolve torchscripting warning Test Plan: saqndcastle, github Differential Revision: D47503004)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Exercise subclass of TransformerEncoderLayer,"Summary: Exercise subclass of TransformerEncoderLayer Additional unit tests for change in CC(Move static checks of layers[0] (e.g., isinstance check) to model build time) to show correct e2e operation (cf. issue CC(Subclassed `TransformerEncoderLayer` breaks scripting of TransformerEncoder)) Also: remove batch_first from list of TS module constants where it is not used to resolve torchscripting warning Test Plan: saqndcastle, github Differential Revision: D47503004",2023-07-16T23:44:51Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,13,https://github.com/pytorch/pytorch/issues/105297,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004,This pull request was **exported** from Phabricator. Differential Revision: D47503004," merge f ""1 unrelated failure, as per github actions"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
2063,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Torch.compile Error: RuntimeError: aten::_conj() Expected a value of type 'Tensor' for argument 'self' but instead found type 'complex'.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Training code     manual_seed(args.seed)     torch.backends.cudnn.benchmark = True     with open(args.model_path+'/config.yaml') as f:         config = ConfigDict(yaml.load(f, Loader=yaml.FullLoader))     config.training.num_steps = args.num_steps     trainset = MSSDatasets(config, args.data_root)     train_loader = DataLoader(         trainset,          batch_size=config.training.batch_size,          shuffle=True,          num_workers=args.num_workers,          pin_memory=args.pin_memory     )     model = TFC_TDF_net(config)     model = torch.compile(model)     model.train()     device_ids = args.device_ids     if type(device_ids)==int:         device = torch.device(f'cuda:{device_ids}')         model = model.to(device)     else:         device = torch.device(f'cuda:{device_ids[0]}')         model = nn.DataParallel(model, device_ids=device_ids).to(device)     optimizer = Adam(model.parameters(), lr=config.training.lr)     print('Train Loop')     scaler = GradScaler()         for batch in tqdm(train_loader):            y = batch.to(device)         x = y.sum(1)   mixture            if config.training.target_instrument is not None:             i = config.training.instruments.index(config.training.target_instrument)             y = y[:,i]         with torch.cuda.amp.autocast():                     y_ = model(x)                loss = nn.MSELoss()(y_, y)          scaler.scale(loss).backward()         if config.training.grad_clip:             nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)           scaler.step(optimizer)         scaler.update()         optimizer.zero_grad(set_to_none=True)     state_dict = model.state_dict() if type(device_ids)==int else model.module.state_di)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Torch.compile Error: RuntimeError: aten::_conj() Expected a value of type 'Tensor' for argument 'self' but instead found type 'complex'.," ğŸ› Describe the bug Training code     manual_seed(args.seed)     torch.backends.cudnn.benchmark = True     with open(args.model_path+'/config.yaml') as f:         config = ConfigDict(yaml.load(f, Loader=yaml.FullLoader))     config.training.num_steps = args.num_steps     trainset = MSSDatasets(config, args.data_root)     train_loader = DataLoader(         trainset,          batch_size=config.training.batch_size,          shuffle=True,          num_workers=args.num_workers,          pin_memory=args.pin_memory     )     model = TFC_TDF_net(config)     model = torch.compile(model)     model.train()     device_ids = args.device_ids     if type(device_ids)==int:         device = torch.device(f'cuda:{device_ids}')         model = model.to(device)     else:         device = torch.device(f'cuda:{device_ids[0]}')         model = nn.DataParallel(model, device_ids=device_ids).to(device)     optimizer = Adam(model.parameters(), lr=config.training.lr)     print('Train Loop')     scaler = GradScaler()         for batch in tqdm(train_loader):            y = batch.to(device)         x = y.sum(1)   mixture            if config.training.target_instrument is not None:             i = config.training.instruments.index(config.training.target_instrument)             y = y[:,i]         with torch.cuda.amp.autocast():                     y_ = model(x)                loss = nn.MSELoss()(y_, y)          scaler.scale(loss).backward()         if config.training.grad_clip:             nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)           scaler.step(optimizer)         scaler.update()         optimizer.zero_grad(set_to_none=True)     state_dict = model.state_dict() if type(device_ids)==int else model.module.state_di",2023-07-16T14:48:37Z,high priority triaged module: complex actionable module: functionalization oncall: pt2 module: pt2-dispatcher,closed,0,11,https://github.com/pytorch/pytorch/issues/105290,Which version  you are using . 1. There is no torch.compile available . 2. Check your return type ing Torch.compile Error: RuntimeError: aten::_conj()  because this expects the value the of type tensors .  Can you put the type (print(type(Value)) to check the type of the value you are inserting ,"Do you get the same error when you try `backend=""eager""` or `backend=""aot_eager""` in the `torch.compile` call?"," No error with backend=""eager"" but for backend=""aot_eager"" i get this error >   File ""/usr/local/envs/mdxnet/lib/python3.11/sitepackages/torch/_ops.py"", line 437, in __call__     return self._op(*args, **kwargs or {})            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ torch._dynamo.exc.BackendCompilerFailed: backend='aot_eager' raised: RuntimeError: aten::_conj() Expected a value of type 'Tensor' for argument 'self' but instead found type 'complex'. Position: 0 Value: 1j Declaration: aten::_conj(Tensor(a) self) > Tensor(a) Cast error details: Unable to cast 1j to Tensor", any thoughts?,"Just wanna add another data point to this discussion, we haven't been super highly prioritizing complex numbers because the argument was that people can rewrite their code to avoid using complex numbers using eulers identity So I tried to do that here  CC(Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators)issuecomment1646961668 for rotary embeddings and got 10x slowdowns so in theory the rewrite argument is maybe fine but then we should be a more prescriptive about how to do those rewrites. Good starting point is probably rotary positional embeddings",hi   please do you have any further comments for getting torch.compile to work with my code?,It seems to me that there's an op using complex numbers that we don't support yet. It's hard for me to further investigate without having a runnable code example (can you provide code with hardcoded config and random input?). You can try seeing the output when setting the envvar `TORCH_LOGS=aot_graphs` to see if you can find the problematic op.,"Hey  if this is still breaking for you, do you mind including a selfcontained repro? That will make diagnosing easier. The repro above isn't runnable(e.g. args doesn't exist)",This problem also affects AudioLM. https://gist.github.com/ezyang/6914fb5dedff6598dc673288898a7498 has the graph going into AOTAutograd and a stack trace Full repro code: gist.github.com/ezyang/64c24c9fc5529f3afed4ee4266f6adc5 but it fails before you get to this error; you need to bypass a different error by disabling compile in `vector_quantize_pytorch/vector_quantize_pytorch.py` in `EuclideanCodebook.forward` See also https://docs.google.com/document/d/14uWNDXa10I_Dpq1KxYShJwYmjHT0xbmv38ZWYCN40NE/edit,Putting this back into the queue for a bit.,"Very simple min repro:  The problem is roughly: (1) the python arg parser converts 1j to a tensor, and the autograd engine sees a call to `aten.mul.Tensor(self, other)` (2) the derivative rule says to compute `other.conj()` (3) during tracing, this results in `aten._conj(other)` (4) however: `other` here is a scalartensor. Today, we convert scalartensors back into scalars when plumbing them into __torch_dispatch__. The input to `FunctionalTensorMode.__torch_dispatch__` becomes a python scalar (`1j` in the repro above)`, and it fails when we attempt to redispatch to `aten._conj` on the python scalar. Still thinking about what the right thing to do here is... somewhere, we want to arrange for the scalar to become a tensor again."
409,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add PyObject_CallMethodNoArgs to pythoncapi_compat.h)ï¼Œ å†…å®¹æ˜¯ (  CC(Compiled autograd)  CC(Misc visibility changes for compiled autograd)  CC(Move TypeAndSize out of /generated/)  CC(Add PyObject_CallMethodNoArgs to pythoncapi_compat.h))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Add PyObject_CallMethodNoArgs to pythoncapi_compat.h,  CC(Compiled autograd)  CC(Misc visibility changes for compiled autograd)  CC(Move TypeAndSize out of /generated/)  CC(Add PyObject_CallMethodNoArgs to pythoncapi_compat.h),2023-07-16T04:04:43Z,Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/105285, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1886,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Copying to a mps tensor produces incorrect results)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug While working on fixing a broken test for `torch.linalg.vander` in CC(Add cumprod support for device mps), because it should be enabled by cumprod, I noticed that the operator was occasionally inconsistent with the cpu version. I tracked down the inconsistency to a bug in copying to a tensor. After distilling it down here is a small repro:  Running the following code I see:  It appears that the copy of the unsqueezed tensor is broken in mps.  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230526 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: version 3.24.2 Libc version: N/A Python version: 3.9.12 (main, Apr  5 2022, 01:52:34)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS13.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==2.1.0.dev20230526 [pip3] torchaudio==2.1.0.dev20230527 [pip3] torchvision==0.16.0.dev20230526 [conda] numpy                     1.24.3                   pypi_0    pypi [conda] torch                     2.1.0.dev20230526          pypi_0    pypi [conda] torchaudio                2.1.0.dev20230527          pypi_0    pypi [conda] torchvision               0.16.0.dev20230526          pypi_0    pypi )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Copying to a mps tensor produces incorrect results," ğŸ› Describe the bug While working on fixing a broken test for `torch.linalg.vander` in CC(Add cumprod support for device mps), because it should be enabled by cumprod, I noticed that the operator was occasionally inconsistent with the cpu version. I tracked down the inconsistency to a bug in copying to a tensor. After distilling it down here is a small repro:  Running the following code I see:  It appears that the copy of the unsqueezed tensor is broken in mps.  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230526 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: version 3.24.2 Libc version: N/A Python version: 3.9.12 (main, Apr  5 2022, 01:52:34)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS13.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==2.1.0.dev20230526 [pip3] torchaudio==2.1.0.dev20230527 [pip3] torchvision==0.16.0.dev20230526 [conda] numpy                     1.24.3                   pypi_0    pypi [conda] torch                     2.1.0.dev20230526          pypi_0    pypi [conda] torchaudio                2.1.0.dev20230527          pypi_0    pypi [conda] torchvision               0.16.0.dev20230526          pypi_0    pypi ",2023-07-15T21:12:57Z,triaged module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/105277,Okay I spent a little time today digging further and I think my first example is kind of misleading. Here's better code:  when run on the cpu this produces:  and when run on mps this produces:  it looks like the problem is that the src tensor is transposed and does not copy down the entire axis on the mps version when both tensors are ints. However when the src is an int and the dst is a float the tranpose problem at least appears to go away:  cpu output:  mps version: ,"Thanks for the detailed analysis  , taking a look at the PR ",Sounds good  let me know if you need anything else from me.
1146,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Refactor causal mask generation and detection for nn.transformer)ï¼Œ å†…å®¹æ˜¯ (Summary: * Create a private globalscope function _generate_subsequent because static class attribute member functions not supported by TorchScript resulting in torchscripting errors. * Make TransformerEncoder and TransformerDecoder consistent w.r.t. is_causal handling by calling _detect_casual_mask * Clarify documentation that is_causal is a hint * Move causal mask detection into a method _detect_causal_mask  * only accept inputsize compatible causal mask as causal mask * update _generate_subsequent_causal_mask to include factory kwargs for dtype and device:    avoid extra copies & conversions by passing directly to torch.full. Test Plan: sandcastle & github CICD Continuation of CC(Refactor causal mask generation and detection for nn.transformer) (due to a tooling issue) which is a continuationinpart of https://github.com/pytorch/pytorch/pull/98327 by   Differential Revision: D47427117)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Refactor causal mask generation and detection for nn.transformer,Summary: * Create a private globalscope function _generate_subsequent because static class attribute member functions not supported by TorchScript resulting in torchscripting errors. * Make TransformerEncoder and TransformerDecoder consistent w.r.t. is_causal handling by calling _detect_casual_mask * Clarify documentation that is_causal is a hint * Move causal mask detection into a method _detect_causal_mask  * only accept inputsize compatible causal mask as causal mask * update _generate_subsequent_causal_mask to include factory kwargs for dtype and device:    avoid extra copies & conversions by passing directly to torch.full. Test Plan: sandcastle & github CICD Continuation of CC(Refactor causal mask generation and detection for nn.transformer) (due to a tooling issue) which is a continuationinpart of https://github.com/pytorch/pytorch/pull/98327 by   Differential Revision: D47427117,2023-07-15T06:42:02Z,fb-exported Merged ciflow/trunk release notes: nn suppress-bc-linter,closed,0,44,https://github.com/pytorch/pytorch/issues/105265,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117,This pull request was **exported** from Phabricator. Differential Revision: D47427117
275,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add XPU support for storage resize_)ï¼Œ å†…å®¹æ˜¯ (We'd like to add XPU device support for storage resize_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add XPU support for storage resize_,We'd like to add XPU device support for storage resize_ ,2023-07-15T03:14:30Z,triaged open source Merged ciflow/trunk release notes: python_frontend topic: improvements,closed,1,5,https://github.com/pytorch/pytorch/issues/105262, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ","Hi  May I know do we need release notes: label here? Could you please help add label, thanks! ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
707,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Copying over Wanchao's changes from https://github.com/pytorch/pytorch/pull/103146)ï¼Œ å†…å®¹æ˜¯ (  CC(fix AsyncCollectiveTensor interaction with view ops, provide torch.compile support)  CC(torch.compile DTensor E2E)  CC(Copying over Wanchao's changes from https://github.com/pytorch/pytorch/pull/103146)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(reorder proxy / fake modes so they always run last) to add some E2E proofofconcept tests for DTensor and torch.compile )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Copying over Wanchao's changes from https://github.com/pytorch/pytorch/pull/103146,"  CC(fix AsyncCollectiveTensor interaction with view ops, provide torch.compile support)  CC(torch.compile DTensor E2E)  CC(Copying over Wanchao's changes from https://github.com/pytorch/pytorch/pull/103146)  CC([not ready for review yet] torch.compile support for parseSemiStructuredTensor)  CC(AOTDispatch subclass)  CC(reorder proxy / fake modes so they always run last) to add some E2E proofofconcept tests for DTensor and torch.compile ",2023-07-14T18:51:39Z,Stale module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/105235,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
365,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Some buffers in `named_buffers()` are missing in `state_dict()`)ï¼Œ å†…å®¹æ˜¯ ( Is this expected behavior or bug?  (blocks CC(Allow (temporarily?) nonfake input during ONNX export with fake mode)) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Some buffers in `named_buffers()` are missing in `state_dict()`, Is this expected behavior or bug?  (blocks CC(Allow (temporarily?) nonfake input during ONNX export with fake mode)) ,2023-07-14T18:44:25Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/105233,Maybe the buffer was registered with `persistent=False`?, Thanks! This is expected behavior then. Closing. ref: https://pytorch.org/docs/stable/generated/torch.nn.Module.htmltorch.nn.Module.register_buffer
1959,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Export+AOTInductor issue tracker)ï¼Œ å†…å®¹æ˜¯ (Updated (October 23)  14K github models (72%) (https://github.com/jansel/pytorchjitparitybench)    [x] CC([aotinductor] 14k models: TypeError: make_boxed_func..g() missing 1 required positional argument: 'args' )    [ ] CC([export] 14k models: AssertionError: graphcaptured input  2, of type , is not among original inputs of types)    [ ] 153 errors like: AssertionError: Failed to produce a graph during tracing. Tracing through 'f' must produce a single graph. (example ./generated/test_lucidrains_imagen_pytorch.py:UpsampleCombiner  pytest ./generated/test_lucidrains_imagen_pytorch.py k test_007) ( CC([AOTInductor] 14k models: AssertionError: Failed to produce a graph during tracing. Tracing through 'f' must produce a single graph.))    [ ] 130 errors like: AssertionError: Mutating module attribute min_val during export. (example ./generated/test_NVlabs_GroupViT.py:Transformer  pytest ./generated/test_NVlabs_GroupViT.py k test_004) ( CC(convit_base: AssertionError: Mutating module attribute rel_indices during export.))    [ ] 81 errors like: AssertionError: original output  2 is None, but only the following types are supported: (, , , ) (example ./generated/test_elbayadm_attn2d.py:GridGatedMAX  pytest ./generated/test_elbayadm_attn2d.py k test_011) ( CC([AOTInductor] 14k models: AssertionError: original output #2 is None, but only the following types are supported))    [ ] 75 errors like: Unsupported: setattr(UserDefinedObjectVariable)  (example ./generated/test_XPixelGroup_BasicSR.py:SFTUpBlock  pytest ./generated/test_XPixelGroup_BasicSR.py k test_029)    [ ] 68 errors like: Unsupported: call_function BuiltinVariable(setattr) [TensorVariable(), ConstantVariable(str), ConstantVariable(bool)] {} (example ./generated)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Export+AOTInductor issue tracker,"Updated (October 23)  14K github models (72%) (https://github.com/jansel/pytorchjitparitybench)    [x] CC([aotinductor] 14k models: TypeError: make_boxed_func..g() missing 1 required positional argument: 'args' )    [ ] CC([export] 14k models: AssertionError: graphcaptured input  2, of type , is not among original inputs of types)    [ ] 153 errors like: AssertionError: Failed to produce a graph during tracing. Tracing through 'f' must produce a single graph. (example ./generated/test_lucidrains_imagen_pytorch.py:UpsampleCombiner  pytest ./generated/test_lucidrains_imagen_pytorch.py k test_007) ( CC([AOTInductor] 14k models: AssertionError: Failed to produce a graph during tracing. Tracing through 'f' must produce a single graph.))    [ ] 130 errors like: AssertionError: Mutating module attribute min_val during export. (example ./generated/test_NVlabs_GroupViT.py:Transformer  pytest ./generated/test_NVlabs_GroupViT.py k test_004) ( CC(convit_base: AssertionError: Mutating module attribute rel_indices during export.))    [ ] 81 errors like: AssertionError: original output  2 is None, but only the following types are supported: (, , , ) (example ./generated/test_elbayadm_attn2d.py:GridGatedMAX  pytest ./generated/test_elbayadm_attn2d.py k test_011) ( CC([AOTInductor] 14k models: AssertionError: original output #2 is None, but only the following types are supported))    [ ] 75 errors like: Unsupported: setattr(UserDefinedObjectVariable)  (example ./generated/test_XPixelGroup_BasicSR.py:SFTUpBlock  pytest ./generated/test_XPixelGroup_BasicSR.py k test_029)    [ ] 68 errors like: Unsupported: call_function BuiltinVariable(setattr) [TensorVariable(), ConstantVariable(str), ConstantVariable(bool)] {} (example ./generated",2023-07-14T13:21:55Z,triaged tracker,open,0,19,https://github.com/pytorch/pytorch/issues/105217,"We recently added the export pass rate at CC([benchmark][export] Add torch.export passrate for TB/TIMM benchmarks), looks like the number is consistent.","> We recently added the export pass rate at CC([benchmark][export] Add torch.export passrate for TB/TIMM benchmarks), looks like the number is consistent. My understanding is we are measuring export+eager there, and here we are checking export+AOTInductor. Is there an existing issue tracking the HF export issue? If not, I can create one.","> My understanding is we are measuring export+eager there, and here we are checking export+AOTInductor. Is there an existing issue tracking the HF export issue? If not, I can create one. Correct, I means they are consistent. I'm not sure if there is an issue.  ",'s working on kwargs support rn.,I'm working on kwarg: see D47478481,"  no GH issue that I know of, already cced Yidi", CC(Export does not support dict as inputs) created,Latest dashboard  result after https://github.com/pytorch/pytorch/pull/105496,Latest dashboard result after https://github.com/pytorch/pytorch/pull/106564,I will take the one below if nobody is looking at it. Thanks. ,"> I will take the one below if nobody is looking at it. Thanks. >  >  SGTM, thanks! For repro, you can look into https://github.com/jansel/pytorchjitparitybench",Cannot repro  ,The one below is not reproducible: ,couldn't repro the error ,failed to repro this: ,cannot reproduce this: ,couldn't reproduce: , I created several issues for tracking 14Kgithubmodels errors that I was able to reproduce. ,Is this still valid?
2065,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor.utils] - using torch.compile w/ FSDP, generates wall of repetitive warnings ""using Triton random, expect differences from eager"")ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Summary  running with FSDP and torch.compile will generate screenfuls of the same single warning: ""[WARNING]"" using Triton random, expect difference from eager"" This is propogated over and over, and then duplicated * every rank. The effect for the user is this:  This really wants to be a one and done warning.  Make the point, and move on.  Steps to reproduce: 1  git clone transformer framework repo: https://github.com/lessw2020/transformer_framework.git 2  dependencies = pip install transformers datasets vitpytorch 3  you can run T5 (defaults to small) by ""bash run_training.sh"" (note this is a torchrun script, check gpu count but defaults to 4). 4  That should run T5 and show the warning wall.   Versions Versions PyTorch version: 2.1.0.dev20230713+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.9.16  (main, Feb 1 2023, 21:39:03) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.01036awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 GPU models and configuration: GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 525.85.12 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==1.0.0 [pip3] numpy==1.24.3 [pip3] pytorchtriton==2.1.0+440fd1bf20 [pip3] torch==2.1.0.dev20230628+cu121 [pip3] torchmodelarchiver==0.5.3b20220226 [pip3] torchworkflowarchiver==0.2.8b20230512 [pip3] torchau)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[Inductor.utils] - using torch.compile w/ FSDP, generates wall of repetitive warnings ""using Triton random, expect differences from eager"""," ğŸ› Describe the bug Summary  running with FSDP and torch.compile will generate screenfuls of the same single warning: ""[WARNING]"" using Triton random, expect difference from eager"" This is propogated over and over, and then duplicated * every rank. The effect for the user is this:  This really wants to be a one and done warning.  Make the point, and move on.  Steps to reproduce: 1  git clone transformer framework repo: https://github.com/lessw2020/transformer_framework.git 2  dependencies = pip install transformers datasets vitpytorch 3  you can run T5 (defaults to small) by ""bash run_training.sh"" (note this is a torchrun script, check gpu count but defaults to 4). 4  That should run T5 and show the warning wall.   Versions Versions PyTorch version: 2.1.0.dev20230713+cu121 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.9.16  (main, Feb 1 2023, 21:39:03) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.01036awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 GPU models and configuration: GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 525.85.12 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==1.0.0 [pip3] numpy==1.24.3 [pip3] pytorchtriton==2.1.0+440fd1bf20 [pip3] torch==2.1.0.dev20230628+cu121 [pip3] torchmodelarchiver==0.5.3b20220226 [pip3] torchworkflowarchiver==0.2.8b20230512 [pip3] torchau",2023-07-14T05:00:35Z,module: logging triaged oncall: pt2 module: inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/105204,"Right now, we're logging the message for every graph (https://github.com/pytorch/pytorch/blob/fb376f80a2f2ceb68322d27433ce22a87c9793e5/torch/_inductor/lowering.pyL1457). You could also try disabling the `developer_warnings` inductor config (see https://github.com/pytorch/pytorch/blob/fb376f80a2f2ceb68322d27433ce22a87c9793e5/torch/_inductor/utils.pyL727). Do you have an expectation for when the warning should be logged?  ","Ok that makes sense re: every graph.   For FSDP, you are going to graph break at every collective... which means a lot of warnings b/c we have a lot of collectives, and hence the entire wall of warnings in the console. re: expectation  personally I would be fine not seeing it at all, I don't think the info conveyed is very surprising/valuable.   Perhaps go from showing by default to only showing when some form of debug flag is flipped, otherwise just have a global 'one and done' warning.  Note that per graph ultimately makes minimal sense though, esp for distributed now integrating with torch.compile where atm every collective generates a graph break.  Thanks for the info above.  I'll play with the utils/dev_warning and add that to the best practices for now for people using FSDP + compile to work around it.","could we just make the warning `once globally` instead of `once per graph`?  I am also fine just disabling the warning if others agree.  This should be alleviated by the fsdp tracing effort too, but that's not going to land soon enough to avoid doing a quick fix for this now.",Let's just lower this to logging.INFO.
1120,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([EZ][BE] Fix the massively annoying strict-weak-ordering issue.)ï¼Œ å†…å®¹æ˜¯ (Summary: kip_fist_pump Running any EgoOCR workflow in nonopt modes was breaking with https://fburl.com/strictweakordering Painstakingly found out that the stable_sort comparator in the generate_proposals caffe2 op was the issue due to numerical imprecision. This was causing Word Detector model to barf with the error. Adding explicit handling for the irreflexivity property fixes this annoying strictweakordering issue that has bugged me and several others(https://fb.workplace.com/groups/1405155842844877/permalink/7079705785389826/) for a while. We can finally run all OCR workflows in nonopt mode! :) Test Plan: Debugged this with `fdb disableautobreakpoints secondarydebugger=lldb buck2 run mode/devsand ai_demos/server_model_zoo/models/ego_ocr_e2e_prod:ego_ocr_e2e_prod_binary` and running `breakpoint set E c++` in the lldb terminal. Differential Revision: D47446816)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[EZ][BE] Fix the massively annoying strict-weak-ordering issue.,Summary: kip_fist_pump Running any EgoOCR workflow in nonopt modes was breaking with https://fburl.com/strictweakordering Painstakingly found out that the stable_sort comparator in the generate_proposals caffe2 op was the issue due to numerical imprecision. This was causing Word Detector model to barf with the error. Adding explicit handling for the irreflexivity property fixes this annoying strictweakordering issue that has bugged me and several others(https://fb.workplace.com/groups/1405155842844877/permalink/7079705785389826/) for a while. We can finally run all OCR workflows in nonopt mode! :) Test Plan: Debugged this with `fdb disableautobreakpoints secondarydebugger=lldb buck2 run mode/devsand ai_demos/server_model_zoo/models/ego_ocr_e2e_prod:ego_ocr_e2e_prod_binary` and running `breakpoint set E c++` in the lldb terminal. Differential Revision: D47446816,2023-07-13T23:54:55Z,caffe2 fb-exported Merged ciflow/trunk topic: not user facing,closed,0,11,https://github.com/pytorch/pytorch/issues/105189,The committers listed above are authorized under a signed CLA.:white_check_mark: login: debowin / name: Debojeet Chatterjee  (8411d0e4dab76282797d7e2e51e9a29f64fb0b5c),This pull request was **exported** from Phabricator. Differential Revision: D47446816,/easycla,This pull request was **exported** from Phabricator. Differential Revision: D47446816,This pull request was **exported** from Phabricator. Differential Revision: D47446816,This pull request was **exported** from Phabricator. Differential Revision: D47446816,This pull request was **exported** from Phabricator. Differential Revision: D47446816," merge ""rocm failure is not related"""," merge m ""rocm failure is not related""", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2051,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DO NOT MERGE][NCCL][CUDA][CUDA Graphs] Set watchdog runtime capture mode to thread local to handle cleaning straggling work)ï¼Œ å†…å®¹æ˜¯ (A more minimal alternative to CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture) and CC([NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work) that should not unwittingly suppress errors during graph captures. I've tested this locally and it appears to fix the repro used to test CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture) and CC([NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work), while checking that the watchdog is attempting to call `cudaEventQuery` (on an event recorded before the capture) in the middle of a capture on another thread. Xposting summary from CC([NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work) here: 1. The issue is a crash caused by the watchdog thread calling `cudaEventQuery` on work that was enqueued before a graph capture. 2. The initial proposal was to change the capture mode of the thread performing the capture to the ""thread local"" mode, so that other threads querying events (the watchdog) don't crash as the events in question were created before the actual capture and therefore shouldn't interfere with the capture. 3. Many such as  and  were concerned with how we changed the capture mode of the capturing thread in this PR (by globally changing the constructor), as we could potentially miss errors/failures caused by always using this more lenient mode. 4.  pointed out that it is really just the watchdog thread that we are concerned with issuing a disallowed `cudaEventQuery` call, so another potential solution is to just have the watchdog thread operate in a m)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[DO NOT MERGE][NCCL][CUDA][CUDA Graphs] Set watchdog runtime capture mode to thread local to handle cleaning straggling work,"A more minimal alternative to CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture) and CC([NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work) that should not unwittingly suppress errors during graph captures. I've tested this locally and it appears to fix the repro used to test CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture) and CC([NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work), while checking that the watchdog is attempting to call `cudaEventQuery` (on an event recorded before the capture) in the middle of a capture on another thread. Xposting summary from CC([NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work) here: 1. The issue is a crash caused by the watchdog thread calling `cudaEventQuery` on work that was enqueued before a graph capture. 2. The initial proposal was to change the capture mode of the thread performing the capture to the ""thread local"" mode, so that other threads querying events (the watchdog) don't crash as the events in question were created before the actual capture and therefore shouldn't interfere with the capture. 3. Many such as  and  were concerned with how we changed the capture mode of the capturing thread in this PR (by globally changing the constructor), as we could potentially miss errors/failures caused by always using this more lenient mode. 4.  pointed out that it is really just the watchdog thread that we are concerned with issuing a disallowed `cudaEventQuery` call, so another potential solution is to just have the watchdog thread operate in a m",2023-07-13T22:11:43Z,module: cuda triaged module: nccl open source module: cuda graphs ciflow/trunk topic: not user facing ciflow/periodic ciflow/inductor,closed,0,25,https://github.com/pytorch/pytorch/issues/105182, rebase main, help, rebase b main, started a rebase job onto refs/remotes/origin/main. Check the current status here,"Successfully rebased `nccl_watchdog_threadlocal` onto `refs/remotes/origin/main`, please pull locally before adding more changes (for example, via `git checkout nccl_watchdog_threadlocal && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `nccl_watchdog_threadlocal` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout nccl_watchdog_threadlocal && git pull rebase`)",A distributed test seems to be timing out:  does not. If the failure is still there afterwards will check if minimally changing the capture mode to the region around the event query helps.,Testing a theory now that somehow the CUDA runtime API call in the watchdog is causing the M60 runner to time out on this distributed test.,Any theories for the failure?, CC(Kill process in `wait_for_process` if `SIGINT` fails to terminate it) hopefully improves the RTT on test timingout/feedback: , rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `nccl_watchdog_threadlocal` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout nccl_watchdog_threadlocal && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `nccl_watchdog_threadlocal` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout nccl_watchdog_threadlocal && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/105182/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/5802975149, Do you want to do a manual rebase of this? The test failure is to do with SyncBatchNorm and seems interesting.,"Rebased, sorry for the noise around this","Thank you, it seems this PR fixes the issue seen we've seen in  CC(Repro for non-deterministic ""operation not permitted when stream is capturing"" crash) ! Can you expand on the meaning of the `[DO NOT MERGE]` on this PR, and whether we should treat it as a warning against pulling it into our 2.1.0 build to fix the issues we're encountering? Thanks!","Thanks  , the current proposed fix is here: https://github.com/pytorch/pytorch/pull/110665pullrequestreview1662802051. This PR was breaking distributed tests when last attempted, hence the [DO NOT MERGE] tag."
1022,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Prefer bound_sympy over sympy_interp)ï¼Œ å†…å®¹æ˜¯ (  CC(Assert that we can compute the bounds for guards using rational numbers)  CC(Prefer bound_sympy over sympy_interp)  CC(Bound just size variables in bound_sympy) This is the first PR towards simplifying sympy_interp, and more generally, simplifying the implementation of ValueRangeAnalysis for SymPy expressions. In general, it would be conteptually good to have a minimal subset of operations that conform our SymPy expressions, let that be guards or indexing expressions. This would allow us to reason better about SymPy guards and potentially have invariants like knowing that guards are continuous piecewise rational functions. If this were the case, we could operate on them using exact arithmetic and completely avoid precision errors like the one found in  CC(Incorrect ValueRanges analysis))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Prefer bound_sympy over sympy_interp,"  CC(Assert that we can compute the bounds for guards using rational numbers)  CC(Prefer bound_sympy over sympy_interp)  CC(Bound just size variables in bound_sympy) This is the first PR towards simplifying sympy_interp, and more generally, simplifying the implementation of ValueRangeAnalysis for SymPy expressions. In general, it would be conteptually good to have a minimal subset of operations that conform our SymPy expressions, let that be guards or indexing expressions. This would allow us to reason better about SymPy guards and potentially have invariants like knowing that guards are continuous piecewise rational functions. If this were the case, we could operate on them using exact arithmetic and completely avoid precision errors like the one found in  CC(Incorrect ValueRanges analysis)",2023-07-13T11:10:23Z,open source Merged release notes: fx ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/105138
1069,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.load fails under FakeTensorMode for GPT2 model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug EDIT: There were 2 issues described here. The second seems to be resolved by main branch (1/25/2024) Model loading works when called outside `FakeTensorMode` context, but it fails when called within it. This becomes relevant after https://github.com/pytorch/pytorch/pull/100017/ in which we can fakefy input and model parameters before calling `torch._dynamo.export(fake_model, *fake_args, fake_mode=fake_mode, **fake_kwargs)` with the preinstantiated `FakeTensorMode`. **Repro:**  This is the full error:  By patching `torch._utils._rebuild_tensor` with a context manager like ** Repro **    it does work, but I wonder if this is the right way of doing this. My expectation was that under a fake mode context, real tensors would never be created so having to hack `torch._util._rebuild_tensor` was unexpected.   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.load fails under FakeTensorMode for GPT2 model," ğŸ› Describe the bug EDIT: There were 2 issues described here. The second seems to be resolved by main branch (1/25/2024) Model loading works when called outside `FakeTensorMode` context, but it fails when called within it. This becomes relevant after https://github.com/pytorch/pytorch/pull/100017/ in which we can fakefy input and model parameters before calling `torch._dynamo.export(fake_model, *fake_args, fake_mode=fake_mode, **fake_kwargs)` with the preinstantiated `FakeTensorMode`. **Repro:**  This is the full error:  By patching `torch._utils._rebuild_tensor` with a context manager like ** Repro **    it does work, but I wonder if this is the right way of doing this. My expectation was that under a fake mode context, real tensors would never be created so having to hack `torch._util._rebuild_tensor` was unexpected.   ",2023-07-12T18:30:25Z,triaged ezyang's list oncall: pt2 module: fakeTensor module: dynamo release notes: dynamo module: pt2-dispatcher,closed,0,4,https://github.com/pytorch/pytorch/issues/105077, ," For the second part, after the hack, I have asserted all `*args`, `**kwargs` and `model.parameters()` and `model.buffers()` are all fake in the beginning of the `torch._dynamo.export`. Maybe new layers/parameters are created during tracing? Because `torch._dynamo.export` is executed outside the `FakeTensorMode` context, that could explain the ""mix"" of existing usercreated fake tensors and real tensors in the model. In that case, even after CC(Add symbolic tracing support to torch._dynamo.export (fake input + weights)), we can't achieve 100% symbolic tracing as dynamically created parmeters aren't converted yet.", Can you check if the issue still exists? https://github.com/pytorch/pytorch/pull/105468 could possible fix this.,"This is sti >  Can you check if the issue still exists? CC(Distinguish between outer and inner fake mode in Dynamo) could possible fix this. Yes, it still repros after CC(Distinguish between outer and inner fake mode in Dynamo) (which is actually merged as CC(Support torch.onnx.dynamo_export within FakeTensorMode))"
1976,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inductor backend for CPU inference extremely slow)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug related forum post https://discuss.pytorch.org/t/inductorbackendforcpuinferenceextremelyslow/183881 I was trying to compare using `torch.compile` to other optimization methods and doing this on CPU results in pretty unexpected behavior to me. I was attempting to compile a huggingface transformers model for text classification. Here are my machine specs  When I follow this tutorial, I can see the speedup when I just run the same input over and over again with `timeit` like the tutorial does. I see about a 40% speedup which I guess is expected. Where things become weird is when I try to run it on an actual set of unique texts (rather than the same input over and over). In this case the model starts to take an extremely long time to run, upwards of 10s per sample. Can someone help me understand what is going on here? Here is the code I am testing this with ``` Additionally my VM also kept crashing which made me think it was running out of memory. I watched `htop` during the execution loop. each iteration the memory would jump up by 1G. So it must be compiling a new model on each iteration? How do I avoid the massive recompile times in an inference loop? If I were serving this model wouldn't each inference call just act like the next iteration in an inference loop so I would expect the same behavior?  Error logs no error, just very very slow  Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.28 Pytho)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Inductor backend for CPU inference extremely slow," ğŸ› Describe the bug related forum post https://discuss.pytorch.org/t/inductorbackendforcpuinferenceextremelyslow/183881 I was trying to compare using `torch.compile` to other optimization methods and doing this on CPU results in pretty unexpected behavior to me. I was attempting to compile a huggingface transformers model for text classification. Here are my machine specs  When I follow this tutorial, I can see the speedup when I just run the same input over and over again with `timeit` like the tutorial does. I see about a 40% speedup which I guess is expected. Where things become weird is when I try to run it on an actual set of unique texts (rather than the same input over and over). In this case the model starts to take an extremely long time to run, upwards of 10s per sample. Can someone help me understand what is going on here? Here is the code I am testing this with ``` Additionally my VM also kept crashing which made me think it was running out of memory. I watched `htop` during the execution loop. each iteration the memory would jump up by 1G. So it must be compiling a new model on each iteration? How do I avoid the massive recompile times in an inference loop? If I were serving this model wouldn't each inference call just act like the next iteration in an inference loop so I would expect the same behavior?  Error logs no error, just very very slow  Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.0.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.28 Pytho",2023-07-12T17:59:17Z,triaged oncall: pt2 module: dynamic shapes,closed,0,4,https://github.com/pytorch/pytorch/issues/105075,", could you help check the shapes of the unique input path? if the shapes are always changed, you can try to turn on the dynamic shapes feature to reduce the compile time( and ). ",thank you  . When I try that I get this error ,Need a nightly,ah ok thanks  . I installed the nightly and set those config options and now it appears to work  so a very small speed up. Thank you!
440,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Internal inductor error in compiled NumPy with global variables)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Consider a toy example  hits an assertion  Removing the global variable   makes the error go away.    Versions Same as  CC(Graph break in len(numpy_array) in compiled NumPy))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Internal inductor error in compiled NumPy with global variables, ğŸ› Describe the bug Consider a toy example  hits an assertion  Removing the global variable   makes the error go away.    Versions Same as  CC(Graph break in len(numpy_array) in compiled NumPy),2023-07-12T17:52:14Z,triaged module: numpy oncall: pt2 module: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/105074, , can I take over this one?,Yeah go ahead
481,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix `hash_storage`'s padding calculation)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Minifier fails when storage size is not divisible by 2). The existing implementation attempts to make `x.numel() % 4 == 0` by appending `x.numel() % 4` zeros. This is backwards, e.g if `x.numel() % 4 == 1`, we need to append `[0, 0, 0]`, not `[0]`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fix `hash_storage`'s padding calculation,"Fixes CC(Minifier fails when storage size is not divisible by 2). The existing implementation attempts to make `x.numel() % 4 == 0` by appending `x.numel() % 4` zeros. This is backwards, e.g if `x.numel() % 4 == 1`, we need to append `[0, 0, 0]`, not `[0]`.",2023-07-12T03:51:58Z,triaged open source Merged ciflow/trunk topic: bug fixes release notes: dynamo,closed,0,4,https://github.com/pytorch/pytorch/issues/105036, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
392,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Minifier fails when storage size is not divisible by 2)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug With `TORCHDYNAMO_REPRO_AFTER=""aot""` + `TORCHDYNAMO_REPRO_LEVEL=4`, the following snippet:  , yields the following error:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Minifier fails when storage size is not divisible by 2," ğŸ› Describe the bug With `TORCHDYNAMO_REPRO_AFTER=""aot""` + `TORCHDYNAMO_REPRO_LEVEL=4`, the following snippet:  , yields the following error:   Versions  ",2023-07-12T03:49:07Z,oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/105035,This is an easy fix  the padding calculation used in `hash_storage` is incorrect. I will put up a PR to fix it shortly.
606,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(mypy hints internal error while trying to check types, but lint runner hides it)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Run `mypy torch/_export/exported_program.py` and it will fail with:   But `mypy_linter` adapter will swallow this error, which results in `lintrunner` aborting lint check at random subset of files and typing error popping up if run on files individually, but got occluded by the INTERNAL ERROR  Versions CI /pytorchdevinfra)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"mypy hints internal error while trying to check types, but lint runner hides it"," ğŸ› Describe the bug Run `mypy torch/_export/exported_program.py` and it will fail with:   But `mypy_linter` adapter will swallow this error, which results in `lintrunner` aborting lint check at random subset of files and typing error popping up if run on files individually, but got occluded by the INTERNAL ERROR  Versions CI /pytorchdevinfra",2023-07-11T04:43:42Z,high priority module: typing module: ci triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/104940,"Tentatively grabbing for myself. `mypy1.x.y` does not have this problem, but it uncovers gazillion of new lint violations, so to solve the issue two things must be done:   Update mypy   Fix `mypy_linter` to never ignore internal errors",.
939,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Errors when converting LLaMA to ONNX using dynamo export)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug While exporting LLaMA from PyTorch to ONNX using the dynamo exporter, the following error occurs.  Here is the code used to export LLaMA.  This error appears to arise from how `torch.full` is used. The documentation says that `fill_value` should be a scalar value. Hugging Face's implementation defines `fill_value` as `torch.tensor(...)` though.  After trying a quick workaround to change `mask` to `torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)`, another error occurs.  Note: There was an initial error with `tabulate` that I resolved with this PR. The PR change is in my version of torch.  Versions Torch: v2.1.0.dev20230630+cu118 Transformers: v4.30.0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Errors when converting LLaMA to ONNX using dynamo export," ğŸ› Describe the bug While exporting LLaMA from PyTorch to ONNX using the dynamo exporter, the following error occurs.  Here is the code used to export LLaMA.  This error appears to arise from how `torch.full` is used. The documentation says that `fill_value` should be a scalar value. Hugging Face's implementation defines `fill_value` as `torch.tensor(...)` though.  After trying a quick workaround to change `mask` to `torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)`, another error occurs.  Note: There was an initial error with `tabulate` that I resolved with this PR. The PR change is in my version of torch.  Versions Torch: v2.1.0.dev20230630+cu118 Transformers: v4.30.0",2023-07-10T20:17:04Z,module: onnx triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/104903,"Below is a minified repro. Hinting that the issue is from decomposing `aten._local_scalar_dense.default`, which is included in the decomposition_table used by exporter ","So looking at the location of error, it turns out it is by design unsupported.   However it can be supported by turning on `dynamic shapes` and `capture_scalar_outputs`.  This will now give me `Unsupported FX nodes: {'call_function': ['aten._local_scalar_dense.default']}`, which will be fixed by https://github.com/microsoft/onnxscript/pull/847. vaishnavi can you give it a try?","I pulled in the latest ONNX Script changes from the main branch to include that PR change and turned on `dynamic_shapes` and `capture_scalar_outputs`. The `aten._local_scalar_dense.default` error does not appear but the `Cannot find symbolic function for aten::index.Tensor, which should be registered under aten.index.Tensor.` error still appears.",vaishnavi thanks for confirming.   `aten.index.Tensor`,"Need https://github.com/pytorch/pytorch/pull/105040, https://github.com/microsoft/onnxscript/pull/862 and https://github.com/microsoft/onnxscript/issues/493",Closing as completed
462,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix AttributeError(""'constexpr' object has no attribute 'type'""))ï¼Œ å†…å®¹æ˜¯ (  CC(Fix AttributeError(""'constexpr' object has no attribute 'type'"")) Fixes CC(Inductor codegen error: AttributeError(""'constexpr' object has no attribute 'type'"")  HuggingFace llama) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"Fix AttributeError(""'constexpr' object has no attribute 'type'"")","  CC(Fix AttributeError(""'constexpr' object has no attribute 'type'"")) Fixes CC(Inductor codegen error: AttributeError(""'constexpr' object has no attribute 'type'"")  HuggingFace llama) Signedoffby: Edward Z. Yang  ",2023-07-09T00:03:39Z,Merged ciflow/trunk topic: bug fixes module: inductor ciflow/inductor release notes: inductor,closed,0,11,https://github.com/pytorch/pytorch/issues/104831," merge f ""unrelated wheel problem"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 0c6264d9993a6da91d279e548a62901ff9b39417` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: inductor / linuxfocalcpupy3.8gcc7inductor / test (inductor_huggingface_dynamic_cpu_accuracy, 1, 1, linux.12xlarge) Details for Dev Infra team Raised by workflow job "," merge f ""unrelated infra failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 1cb72864f3cfe29067165b22ed073f80b136d82c` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1001,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Dynamic shapes tests fail when not simplifying `Mod` expressions.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Some tests in _test/dynamo/dynamic_shapes.py_ fail if SymPy stops simplifying `Mod` operations. The easiest way to reproduce this behavior is: 1. Modify SymPy `Mod` implementation at _sympy/core/mod.py_:  2. Run: `DynamicShapesExportTests.test_capture_symbolic_tracing_dynamic_shapes`   Raw Log   **Note:** the error occurred when trying to print `Ne(Mod(s0, 2*s0), 0)`. Without the SymPy change (i.e. turning simplification of `Mod` expressions on), the expression is simplified to true. **Expected:** the tests should pass regardless of SymPy simplification.  Other Tests There are a few other tests that fail, when I run _dynamo/test_dynamic_shapes.py_:   Versions On commit: `db1ac4e29b0f557711190c8d49d4afb5da1844e8` (July 6). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Dynamic shapes tests fail when not simplifying `Mod` expressions.," ğŸ› Describe the bug Some tests in _test/dynamo/dynamic_shapes.py_ fail if SymPy stops simplifying `Mod` operations. The easiest way to reproduce this behavior is: 1. Modify SymPy `Mod` implementation at _sympy/core/mod.py_:  2. Run: `DynamicShapesExportTests.test_capture_symbolic_tracing_dynamic_shapes`   Raw Log   **Note:** the error occurred when trying to print `Ne(Mod(s0, 2*s0), 0)`. Without the SymPy change (i.e. turning simplification of `Mod` expressions on), the expression is simplified to true. **Expected:** the tests should pass regardless of SymPy simplification.  Other Tests There are a few other tests that fail, when I run _dynamo/test_dynamic_shapes.py_:   Versions On commit: `db1ac4e29b0f557711190c8d49d4afb5da1844e8` (July 6). ",2023-07-08T15:03:15Z,triaged oncall: pt2,closed,0,8,https://github.com/pytorch/pytorch/issues/104826, ,"The first issue you quoted here is a oneoff issue with the fake mode export interface I worked on with . The root cause of the problem is that when we fakeify the tensor manually here  we didn't assign sources to x or the model. This then bypasses the source assignment Dynamo would have done, and consequently guard creation doesn't know what to do. This is a separate bug that would need to be fixed but if  isn't running into trouble with it I'm not in a rush to fix it. The other issues seem to be dupes of this one, OR numpy ndarray related (). Hard to say more without the error message.",They are all similar to each other.  DynamicShapesMiscTests.test_numpy_ndarray_graph_break    DynamicShapesMiscTests.test_numpy_ndarray_graph_break_dynamic_shapes    DynamicShapesMiscTests.test_numpy_ndarray_graph_break_with_multiple_outputs    DynamicShapesMiscTests.test_numpy_ndarray_graph_break_with_multiple_outputs_dynamic_shapes    DynamicShapesMiscTests.test_tensor_interacts_with_numpy_ndarray    DynamicShapesMiscTests.test_tensor_interacts_with_numpy_ndarray_dynamic_shapes  ,"The numpy frame count change is pretty strange, will need to run with some logs and see what's going on.","Yes. Even weirder: this error only occurs when I execute all of _test_dynamic_shapes.py_ tests. Whenever I filter so as to run only, for example, `test_numpy_ndarray_graph_break`, they pass.",Ok. This is super strange. I will rebase and rerun the tests.,The hysteresis problem could be due to not clearing cache,Turns out `test_capture_symbolic_tracing_dynamic_shapes` was really relying on `Mod` ( already commented on that). The others were unrelated.
601,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.func.jvp fails with BERT training)ï¼Œ å†…å®¹æ˜¯ (As the title suggests just as a fun experiment I am trying to training a huggingface BERT model using forward gradient. I am getting this error in jvp computation. > `RuntimeError: wrapper>level().value() <= current_level INTERNAL ASSERT FAILED at ""../aten/src/ATen/functorch/ADInterpreters.cpp"":39, please report a bug to PyTorch. escaped?` Here is the minimal implementation  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.func.jvp fails with BERT training,"As the title suggests just as a fun experiment I am trying to training a huggingface BERT model using forward gradient. I am getting this error in jvp computation. > `RuntimeError: wrapper>level().value() <= current_level INTERNAL ASSERT FAILED at ""../aten/src/ATen/functorch/ADInterpreters.cpp"":39, please report a bug to PyTorch. escaped?` Here is the minimal implementation  ",2023-07-08T03:37:16Z,module: autograd triaged actionable module: forward ad module: functorch,open,0,1,https://github.com/pytorch/pytorch/issues/104933,"Hi , I am trying to reproduce the error you're facing but getting a segmentation fault. Could you please share the version of each library you're using? I am getting the following when I try to execute the shared script: "
375,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Miscellaneous fixes for HuggingFace accelerated-pytorch-transformers-generation)ï¼Œ å†…å®¹æ˜¯ (  CC(Miscellaneous fixes for HuggingFace acceleratedpytorchtransformersgeneration) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Miscellaneous fixes for HuggingFace accelerated-pytorch-transformers-generation,  CC(Miscellaneous fixes for HuggingFace acceleratedpytorchtransformersgeneration) Signedoffby: Edward Z. Yang  ,2023-07-07T15:48:21Z,Stale module: inductor module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/104777,I'll probably split this up into a few PRs...,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
599,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot resume training when using fused AdamW )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I'm not sure if this is more of a HuggingFace issue or a PyTorch one. When I resume training using standard AdamW, the Trainer works without issues, but using the fused implementation of AdamW leads to this error. For context, the code trains (from scratch) without issues.   Versions * PyTorch 2.0.0 with CUDA integration * Transformers 4.30.1 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Cannot resume training when using fused AdamW ," ğŸ› Describe the bug Hi, I'm not sure if this is more of a HuggingFace issue or a PyTorch one. When I resume training using standard AdamW, the Trainer works without issues, but using the fused implementation of AdamW leads to this error. For context, the code trains (from scratch) without issues.   Versions * PyTorch 2.0.0 with CUDA integration * Transformers 4.30.1 ",2023-07-07T11:06:52Z,module: optimizer,closed,0,2,https://github.com/pytorch/pytorch/issues/104767,"Never mind, I discovered it's a problem with how `transformers` and `torch` integrate with each other, so this can be closed now.",This should be fixed by moving torch past this commit as well https://github.com/pytorch/pytorch/pull/102619
2009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Torch Filename Storage hangs on ""file_system"" sharing strategy after in-place fill)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug On MacOS Mseries machines (AND linux machines using 'file_system' sharing) if you use an inplace fill operation on a Linear layer before launching torch multiprocessing, then shared filename storage will fail to allocate on sufficiently large batches of data. The problem also appears if you move a normal tensor into shared memory on the host. This is a problem because the torch DataLoader relies on torch Queues which in turn rely on the ForkingPickler which in turn relies on shared filename storage.  The expected output here would be to see:  The actual output is as follows, with the program then hanging forever rather than exiting:   Versions **MacOS** Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.8.16  (default, Feb  1 2023, 16:01:13)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] numpy==1.24.2 [pip3] torch==2.0.1 [pip3] torchaudio==2.0.2 [pip3] torchinfo==1.7.2 [pip3] torchview==0.2.6 [pip3] torchvision==0.15.2 [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.0.1                    pypi_0    pypi [conda] torchaudio                2.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Torch Filename Storage hangs on ""file_system"" sharing strategy after in-place fill"," ğŸ› Describe the bug On MacOS Mseries machines (AND linux machines using 'file_system' sharing) if you use an inplace fill operation on a Linear layer before launching torch multiprocessing, then shared filename storage will fail to allocate on sufficiently large batches of data. The problem also appears if you move a normal tensor into shared memory on the host. This is a problem because the torch DataLoader relies on torch Queues which in turn rely on the ForkingPickler which in turn relies on shared filename storage.  The expected output here would be to see:  The actual output is as follows, with the program then hanging forever rather than exiting:   Versions **MacOS** Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.8.16  (default, Feb  1 2023, 16:01:13)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] numpy==1.24.2 [pip3] torch==2.0.1 [pip3] torchaudio==2.0.2 [pip3] torchinfo==1.7.2 [pip3] torchview==0.2.6 [pip3] torchvision==0.15.2 [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.0.1                    pypi_0    pypi [conda] torchaudio                2.",2023-07-07T05:36:36Z,module: multiprocessing triaged module: mps,open,0,5,https://github.com/pytorch/pytorch/issues/104761,I tried it and get the same results. If you change the number down from 128 to 4 it works. My hunch is that the process is running out of memory (stack or heap) with the larger numbers. I am not sure how to prove this and get metrics and see the out of memory. ,The weird thing is that if you get rid of the inplace fill then you can crank the remote batch size up to millions without issue,Another update: This is not just a MacOS problem. The same code also freezes on Linux if using the 'file_system' sharing method. I've updated the initial comment / code to reflect this,This problem might actually not be related to shared_memory at all. Even the following hangs (again on both linux and Mac): ,I Agree with Davidradl
515,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inductor codegen error: AttributeError(""'constexpr' object has no attribute 'type'"") - HuggingFace llama)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Repro script: https://gist.github.com/ezyang/321e9ab5ead6e5fdfecadbe5dfd4a963 Fails with:  Repro was extracted from https://github.com/fxmarty/acceleratedpytorchtransformersgeneration/pull/14  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"Inductor codegen error: AttributeError(""'constexpr' object has no attribute 'type'"") - HuggingFace llama", ğŸ› Describe the bug Repro script: https://gist.github.com/ezyang/321e9ab5ead6e5fdfecadbe5dfd4a963 Fails with:  Repro was extracted from https://github.com/fxmarty/acceleratedpytorchtransformersgeneration/pull/14  Versions main ,2023-07-07T04:12:57Z,high priority oncall: pt2,closed,1,2,https://github.com/pytorch/pytorch/issues/104759,"Issue is that Triton doesn't support `tl.broadcast_to(1, [XBLOCK, YBLOCK])`. We generate it here: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/triton.pyL1055 So, we have a check right above it that's supposed to catch this case, but in this case, `index_str` was 1, but `index` was `(x3//16) + 1`, which after simplification becomes 1.  Right solution is probably to add a simplify_indexing here !image",This fix works
2051,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch._dynamo.exc.InternalTorchDynamoError: Could not run 'aten::_local_scalar_dense' with arguments from the 'Meta' backend)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am trying to get the following to run the following code to compile a BERT model   On exporting the compiled code to ONNX, I get the following error:  It says that CUDA is supported, and the system I am running it on has CUDA. I wonder why it still throws an error.  Versions Collecting environment information... PyTorch version: 2.1.0a0+gitc42de84 Is debug build: False CUDA used to build PyTorch: 12.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.076genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 12.2.91 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4080 Nvidia driver version: 535.54.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   39 bits physical, 48 bits virtual CPU(s):                          8 Online CPU(s) list:             07 Thread(s) per core:              2 Core(s) per socket:              4 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       GenuineIntel CPU family:                      6 Model:                           158 Model name:                      Intel(R) Core(TM) i77700K CPU @ 4.20GHz Stepping:                        9 CPU MHz:                         4200.0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch._dynamo.exc.InternalTorchDynamoError: Could not run 'aten::_local_scalar_dense' with arguments from the 'Meta' backend," ğŸ› Describe the bug I am trying to get the following to run the following code to compile a BERT model   On exporting the compiled code to ONNX, I get the following error:  It says that CUDA is supported, and the system I am running it on has CUDA. I wonder why it still throws an error.  Versions Collecting environment information... PyTorch version: 2.1.0a0+gitc42de84 Is debug build: False CUDA used to build PyTorch: 12.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.076genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 12.2.91 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4080 Nvidia driver version: 535.54.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   39 bits physical, 48 bits virtual CPU(s):                          8 Online CPU(s) list:             07 Thread(s) per core:              2 Core(s) per socket:              4 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       GenuineIntel CPU family:                      6 Model:                           158 Model name:                      Intel(R) Core(TM) i77700K CPU @ 4.20GHz Stepping:                        9 CPU MHz:                         4200.0",2023-07-06T23:37:55Z,triaged oncall: pt2 oncall: export,closed,1,3,https://github.com/pytorch/pytorch/issues/104748, / export., when I run this on nightly I get a different error now: `RuntimeError: Detected that you are using FX to torch.jit.trace a dynamooptimized function. This is not supported at the moment.` Let me know if you get the same thing and I can convert this to a feature request.,"Closing due to inactivity, feel free to reopen"
1059,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Error reporting uses formal parameter names of downstream C++ function)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When calling sdpa, I get the error message `RuntimeError: The size of tensor a (256) must match the size of tensor b (2) at nonsingleton dimension 2`  there are no parameters a and b in sdpa.  Likely it's the formal parameter names of a function called from inside sdpa, which does not relate to the visible Python call stack.  (Partial stack backtrace below.)  Can we / Should we check parameters inside sdpa and use proper sdpa formal parameter names?  I think this behavior has shown for other functions, although getting the order of tensor dimensions right when updating arbitrary transformer code feels an order of magnitude harder.  No concern about having to do that per se, but it would be nice to have errors expressed in terms of sdpa to help debug.   Versions fbcode trunk )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Error reporting uses formal parameter names of downstream C++ function," ğŸ› Describe the bug When calling sdpa, I get the error message `RuntimeError: The size of tensor a (256) must match the size of tensor b (2) at nonsingleton dimension 2`  there are no parameters a and b in sdpa.  Likely it's the formal parameter names of a function called from inside sdpa, which does not relate to the visible Python call stack.  (Partial stack backtrace below.)  Can we / Should we check parameters inside sdpa and use proper sdpa formal parameter names?  I think this behavior has shown for other functions, although getting the order of tensor dimensions right when updating arbitrary transformer code feels an order of magnitude harder.  No concern about having to do that per se, but it would be nice to have errors expressed in terms of sdpa to help debug.   Versions fbcode trunk ",2023-07-06T22:09:37Z,module: nn triaged,open,0,5,https://github.com/pytorch/pytorch/issues/104739, ,"Depends on where the error is coming from, if it is coming the in the composite path then it would be pretty hard to pass the names down to one of the called functions. setting `TORCH_SHOW_CPP_STACKTRACES=1` can help pinpoint the error","Yes, I don't think we have a way to pass names that should be reported with formal parameters to give ""pythonlevel correct"" error names.  The two options I see are either duplicating the check at the beginning of SDPA, or... just living with it (There are also the TORCH_SHOW_CPP_STACKTRACES=1 which helps diagnose, but at the end of the day doesn't help debug of Python code) ","To add insult to injury we add up in `sdpa_math()`.    Also, D47243587 needs some work to handle key padding mask correctly as NT.",Yeah my reading of the tea leaves on this one is add_ only happends when a mask is present in the math path. I don't know if regular users would be able to get this though
2011,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(No supported gcc/g++ host compiler found. (torchCuda11.8 + transformer + deepspeed ))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Environment setup error when loading cpuadam.  to reproduce:   The compiler of the gpu is not accessed. So when I run the run_classification_w.py script, the output gives error that just loading the cpu adam with deepspeed has issues.  See the highlighted parts:    1. run_classification_w.py:  > import logging > import os > import sys > import warnings > from dataclasses import dataclass, field > from random import randint > from typing import Optional >  > import datasets > import evaluate > import numpy as np > from datasets import DatasetDict, load_dataset >  > import transformers > from transformers import ( >     AutoConfig, >     AutoFeatureExtractor, >     AutoModelForAudioClassification, >     HfArgumentParser, >     Trainer, >     TrainingArguments, >     set_seed, > ) > from transformers.trainer_utils import get_last_checkpoint > from transformers.utils import check_min_version, send_example_telemetry > from transformers.utils.versions import require_version >  >  > logger = logging.getLogger(__name__) >  > from huggingface_hub import login > login(token=""..."") >  > import deepspeed > deepspeed.ops.op_builder.CPUAdamBuilder().load()  **error is raised at line: **: `deepspeed.ops.op_builder.CPUAdamBuilder().load()` 2. output:   â€¦ > [20230706 17:55:36,866] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect) > [20230706 17:55:37,457] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only. > Detected CUDA_VISIBLE_DEVICES=0: setting include=localhost:0 > [20230706 17:55:37,485] [INFO] [runner.py:555:main] cmd = /home/flckv/.conda/envs/_w/bin/python u m deepspeed.launcher.launch world_i)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,No supported gcc/g++ host compiler found. (torchCuda11.8 + transformer + deepspeed )," ğŸ› Describe the bug Environment setup error when loading cpuadam.  to reproduce:   The compiler of the gpu is not accessed. So when I run the run_classification_w.py script, the output gives error that just loading the cpu adam with deepspeed has issues.  See the highlighted parts:    1. run_classification_w.py:  > import logging > import os > import sys > import warnings > from dataclasses import dataclass, field > from random import randint > from typing import Optional >  > import datasets > import evaluate > import numpy as np > from datasets import DatasetDict, load_dataset >  > import transformers > from transformers import ( >     AutoConfig, >     AutoFeatureExtractor, >     AutoModelForAudioClassification, >     HfArgumentParser, >     Trainer, >     TrainingArguments, >     set_seed, > ) > from transformers.trainer_utils import get_last_checkpoint > from transformers.utils import check_min_version, send_example_telemetry > from transformers.utils.versions import require_version >  >  > logger = logging.getLogger(__name__) >  > from huggingface_hub import login > login(token=""..."") >  > import deepspeed > deepspeed.ops.op_builder.CPUAdamBuilder().load()  **error is raised at line: **: `deepspeed.ops.op_builder.CPUAdamBuilder().load()` 2. output:   â€¦ > [20230706 17:55:36,866] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect) > [20230706 17:55:37,457] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only. > Detected CUDA_VISIBLE_DEVICES=0: setting include=localhost:0 > [20230706 17:55:37,485] [INFO] [runner.py:555:main] cmd = /home/flckv/.conda/envs/_w/bin/python u m deepspeed.launcher.launch world_i",2023-07-06T14:54:07Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/104708,"Hey , in general you'll have more luck resolving environmentrelated issues by posting on the PyTorch Forums. Note that the issues here are reserved for bugs within PyTorch itself. Closing for now but feel free to reopen if a PyTorch bug is identified.","hi ,  I am not able to use the PyTorch Forums for months now.  I get this error: ""New registrations are not allowed from your IP address (maximum limit reached). Contact a staff member.""  I reported this error months ago without any change here:  CC(discuss.pytorch.org signup issue)issuecomment1575685718 "
1799,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Calling all on empty MPS tensor yields wrong result)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Minimal example:  fails, since `x_cpu.all()` (correctly) yields `true`, while `x_mps.all()` yields `false`.  Versions PyTorch version: 2.1.0.dev20230702 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3 (x86_64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.26.3 Libc version: N/A Python version: 3.8.16 (default, Mar  1 2023, 21:19:10)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] flake8==6.0.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.24.3 [pip3] torch==2.1.0.dev20230702 [pip3] torchaudio==2.1.0.dev20230702 [pip3] torchmetrics==0.11.4 [pip3] torchvision==0.16.0.dev20230702 [conda] blas                      1.0                         mkl [conda] mkl                       2023.1.0         h59209a4_43558 [conda] numpy                     1.24.3                   pypi_0    pypi [conda] torch                     2.1.0.dev20230702          pypi_0    pypi [conda] torchaudio                2.1.0.dev20230702          pypi_0    pypi [conda] torchmetrics              0.11.4                   pypi_0    pypi [conda] torchvision               0.16.0.dev20230702          pypi_0    pypi )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Calling all on empty MPS tensor yields wrong result," ğŸ› Describe the bug Minimal example:  fails, since `x_cpu.all()` (correctly) yields `true`, while `x_mps.all()` yields `false`.  Versions PyTorch version: 2.1.0.dev20230702 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3 (x86_64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.26.3 Libc version: N/A Python version: 3.8.16 (default, Mar  1 2023, 21:19:10)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Max Versions of relevant libraries: [pip3] flake8==6.0.0 [pip3] mypyextensions==1.0.0 [pip3] numpy==1.24.3 [pip3] torch==2.1.0.dev20230702 [pip3] torchaudio==2.1.0.dev20230702 [pip3] torchmetrics==0.11.4 [pip3] torchvision==0.16.0.dev20230702 [conda] blas                      1.0                         mkl [conda] mkl                       2023.1.0         h59209a4_43558 [conda] numpy                     1.24.3                   pypi_0    pypi [conda] torch                     2.1.0.dev20230702          pypi_0    pypi [conda] torchaudio                2.1.0.dev20230702          pypi_0    pypi [conda] torchmetrics              0.11.4                   pypi_0    pypi [conda] torchvision               0.16.0.dev20230702          pypi_0    pypi ",2023-07-06T09:32:50Z,triaged module: mps,closed,0,4,https://github.com/pytorch/pytorch/issues/104694,I am investigating.,"I see that the numpy all() is defined as `In numpy, we can check that whether none of the elements of given array is zero or not with the help of numpy.all() function. In this function pass an array as parameter. If any of one element of the passed array is zero then it returns False otherwise it returns True boolean value. ` And an empty array for all() returns True. So I agree the cpu case is correct. I notice that the code in aten/native/mps/operations/reduce_ops.mm has code  So for a zero length input tensor it should return True   but we see False in the reported case. I notice that operation count_nonzero() is also inconsistent for these empty tensors with cpu reporting 0 and mps reporting a large number. ",I notice that https://github.com/pytorch/pytorch/blob/main/test/test_mps.py has a few commented out tests for empty tensors . ,I am not sure if anyone can assign this to me as I am looking at it.
1994,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] ValueError: `FlatParameter` requires uniform `requires_grad`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi! I am trying to wrap a transformer model with `torch.distributed.fsdp.FullyShardedDataParallel`. Since I am working with the PEFT setting, in each Attention or FFN layer, there exist both `requires_grad=True` and `requires_grad=False` parameters.  When I try to wrap the model with FSDP, the following ERROR occurs:   The following is a minimal example to reproduce the error:   I assume this error is related to  CC([FSDP] `use_orig_params=True`: allow non-uniform `requires_grad` during init), but I've already set `use_orig_params=True`. I also find that the model can be successfully wrapped when set   so I think this could be related to the `transformer_auto_wrap_policy`.  Versions Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.2 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0150genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 GPU 4: NVIDIA RTX A6000 GPU 5: NVIDIA RTX A6000 GPU 6: NVIDIA RTX A6000 GPU 7: NVIDIA RTX A6000 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_a)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[FSDP] ValueError: `FlatParameter` requires uniform `requires_grad`," ğŸ› Describe the bug Hi! I am trying to wrap a transformer model with `torch.distributed.fsdp.FullyShardedDataParallel`. Since I am working with the PEFT setting, in each Attention or FFN layer, there exist both `requires_grad=True` and `requires_grad=False` parameters.  When I try to wrap the model with FSDP, the following ERROR occurs:   The following is a minimal example to reproduce the error:   I assume this error is related to  CC([FSDP] `use_orig_params=True`: allow non-uniform `requires_grad` during init), but I've already set `use_orig_params=True`. I also find that the model can be successfully wrapped when set   so I think this could be related to the `transformer_auto_wrap_policy`.  Versions Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.2 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0150genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 GPU 4: NVIDIA RTX A6000 GPU 5: NVIDIA RTX A6000 GPU 6: NVIDIA RTX A6000 GPU 7: NVIDIA RTX A6000 Nvidia driver version: 535.54.03 cuDNN version: Probably one of the following: /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_a",2023-07-06T06:04:01Z,triaged module: fsdp,closed,0,9,https://github.com/pytorch/pytorch/issues/104690,"Thanks for submitting this issue! We are actively investigating how to improve our parameterefficient finetuning support. For now, here are a few comments:  If you want to avoid the `ValueError` with `use_orig_params=True`, you will need to use a nightly release since the change  (https://github.com/pytorch/pytorch/pull/98221) was not part of the 2.0.1 release.  If you pass the frozen parameters to `ignored_parameters`, then FSDP will not shard them. That may not be the behavior you want.      JFYI, we removed the `ignored_parameters` constructor argument and replaced it with `ignored_states`.     https://github.com/pytorch/pytorch/blob/8d65635378b828b4272cddbcc6c9f99be6e06146/torch/distributed/fsdp/fully_sharded_data_parallel.pyL361  Even if you use `use_orig_params=True` on the nightly release, you will see unexpectedly high memory usage. This is because the gradient memory will still be as if you did not freeze parameters (as a limitation of our flat parameter design).  What you can try is to do something like the following for now: ","> Thanks for submitting this issue! We are actively investigating how to improve our parameterefficient finetuning support. >  > For now, here are a few comments: >  > * If you want to avoid the `ValueError` with `use_orig_params=True`, you will need to use a nightly release since the change  ([[FSDP] Allow nonuniform `requires_grad` for `use_orig_params=True` CC([FSDP] Allow nonuniform `requires_grad` for `use_orig_params=True`)](https://github.com/pytorch/pytorch/pull/98221)) was not part of the 2.0.1 release. > * If you pass the frozen parameters to `ignored_parameters`, then FSDP will not shard them. That may not be the behavior you want. >    >   * JFYI, we removed the `ignored_parameters` constructor argument and replaced it with `ignored_states`. >     https://github.com/pytorch/pytorch/blob/8d65635378b828b4272cddbcc6c9f99be6e06146/torch/distributed/fsdp/fully_sharded_data_parallel.pyL361 > * Even if you use `use_orig_params=True` on the nightly release, you will see unexpectedly high memory usage. This is because the gradient memory will still be as if you did not freeze parameters (as a limitation of our flat parameter design). > * What you can try is to do something like the following for now: >  >  Thanks for your reply! It solves my problem. Just still a bit confused about ""passing the frozen parameters to ignored_parameters may not be the behavior I want"". I thought that, now that the frozen parameters do not incur the costly optimizer states & gradients to save in memory, I may prefer to  always hold a complete (unshared) version of the parameters in each rank to avoid the communication cost in each iteration. Is my understanding wrong? If it is, in what condition should I use the `ignored_states` argument? Thanks!",">  I thought that, now that the frozen parameters do not incur the costly optimizer states & gradients to save in memory, I may prefer to always hold a complete (unshared) version of the parameters in each rank to avoid the communication cost in each iteration. Is my understanding wrong? I think if holding the full unsharded parameters in GPU memory is tolerable, then passing them to `ignored_states` is okay. However, I would be careful on whether doing so is actually okay. Let us try do some backoftheenvelope calculation.   Assuming an optimizer like Adam that requires 2x optimizer states per parameter, the model memory is normally **4x** the parameter memory (1x from parameters, 1x from gradients, and 2x from the optimizer states).  If only **10%** of your model parameters are trainable, then you expect this to decrease to **1.3x** (1x from parameters, 0.1x from gradients, and 0.2x from optimizer states).  If you use FSDP, then ideally you shard this 1.3x over your GPUs. Assuming **8** GPUs, maybe you end up with **~0.3x** (giving some buffer for allgathering and intermediates etc.).  However, if you do not shard the model parameters (like you propose), then you end up with something like **1.1x** because you only shard the gradients/optimizer states, which are already reduced from have only 10% of the model being trainable. In that case, FSDP is not doing much. Given the existing FSDP design there are a few things to note:  For `use_orig_params=False`, you must wrap frozen parameters separately. (I have a PR open to improve the error messaging here.)  For `use_orig_params=True`, you _can_ wrap frozen parameters with nonfrozen parameters together, but you will use gradient memory equivalent to not freezing any parameters.      Let us revisit our example for this case. We would end up with **~0.43x** memory (0.1 * 2x / 8 = 0.025x for optimizer states; 1x parameters > ~0.2x after sharding over 8 GPUs; and 1x gradients > ~0.2x after sharding over 8 GPUs). This is still less memory than not sharding the model parameters (though it requires more communication).  In summary:  For max memory savings, we should prefer to wrap frozen parameters separately (e.g. to get closer to the ~0.3x modelstate memory usage).  If we absolutely cannot, we can use `use_orig_params=True` and pay the cost of higher gradient memory usage (e.g. to still get ~0.43x memory usage).  If we can afford to not shard the model parameters (e.g. using ~1.1x memory), then we may follow your approach of ignoring the frozen parameters to save on FSDP communication. Let me know if anything is confusing or if I made a mistake somewhere!","> > I thought that, now that the frozen parameters do not incur the costly optimizer states & gradients to save in memory, I may prefer to > > always hold a complete (unshared) version of the parameters in each rank to avoid the communication cost in each iteration. Is my understanding wrong? >  > I think if holding the full unsharded parameters in GPU memory is tolerable, then passing them to `ignored_states` is okay. However, I would be careful on whether doing so is actually okay. Let us try do some backoftheenvelope calculation. >  > * Assuming an optimizer like Adam that requires 2x optimizer states per parameter, the model memory is normally **4x** the parameter memory (1x from parameters, 1x from gradients, and 2x from the optimizer states). > * If only **10%** of your model parameters are trainable, then you expect this to decrease to **1.3x** (1x from parameters, 0.1x from gradients, and 0.2x from optimizer states). > * If you use FSDP, then ideally you shard this 1.3x over your GPUs. Assuming **8** GPUs, maybe you end up with **~0.3x** (giving some buffer for allgathering and intermediates etc.). > * However, if you do not shard the model parameters (like you propose), then you end up with something like **1.1x** because you only shard the gradients/optimizer states, which are already reduced from have only 10% of the model being trainable. In that case, FSDP is not doing much. >  > Given the existing FSDP design there are a few things to note: >  > * For `use_orig_params=False`, you must wrap frozen parameters separately. (I have a PR open to improve the error messaging here.) > * For `use_orig_params=True`, you _can_ wrap frozen parameters with nonfrozen parameters together, but you will use gradient memory equivalent to not freezing any parameters. >    >   * Let us revisit our example for this case. We would end up with **~0.43x** memory (0.1 * 2x / 8 = 0.025x for optimizer states; 1x parameters > ~0.2x after sharding over 8 GPUs; and 1x gradients > ~0.2x after sharding over 8 GPUs). This is still less memory than not sharding the model parameters (though it requires more communication). >  > In summary: >  > * For max memory savings, we should prefer to wrap frozen parameters separately (e.g. to get closer to the ~0.3x modelstate memory usage). > * If we absolutely cannot, we can use `use_orig_params=True` and pay the cost of higher gradient memory usage (e.g. to still get ~0.43x memory usage). > * If we can afford to not shard the model parameters (e.g. using ~1.1x memory), then we may follow your approach of ignoring the frozen parameters to save on FSDP communication. >  > Let me know if anything is confusing or if I made a mistake somewhere! Thank you sooooooooo much for your detailed explanation!! It is very helpful and has cleared up all my confusion. Thank you very much!","Hi  , thanks for your very detailed explanations!  If the lora implementation is like this:  what would you recommend doing? Since the same module has both trainable and nontrainable parameters","khanna  I think if you cannot change the module, then your best bet is to apply `FullyShardedDataParallel` to `LoRALinear` with `use_orig_params=True`. You will use more gradient memory, but otherwise, things should still work.","We are working on a new FSDP that would address this issue more fundamentally. E.g., in khanna 's case, we can apply FSDP to `LoRALinear` _and_ use the expected amount of memory. Example unit test: https://github.com/pytorch/pytorch/blob/58047205ed098c04ec045e66fc39dcc70b60600b/test/distributed/_composable/fsdp/test_fully_shard_frozen.pyL36","> We are working on a new FSDP that would address this issue more fundamentally. E.g., in khanna 's case, we can apply FSDP to `LoRALinear` _and_ use the expected amount of memory. >  > Example unit test: >  > https://github.com/pytorch/pytorch/blob/58047205ed098c04ec045e66fc39dcc70b60600b/test/distributed/_composable/fsdp/test_fully_shard_frozen.pyL36 Hello, may I ask which version supports this capability? I encountered this problem in 2.0.1"," For addressing this error in existing FSDP, you want a newer version, e.g. >= 2.2. For the new FSDP, you want the latest nightlies."
453,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch._dynamo.export does not work with bert model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm interested in using aot inductor. However, I'm getting an error while running the following script. Note torch.compile is able to generate one graph for this model.  Error message:     Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch._dynamo.export does not work with bert model," ğŸ› Describe the bug I'm interested in using aot inductor. However, I'm getting an error while running the following script. Note torch.compile is able to generate one graph for this model.  Error message:     Versions  ",2023-07-06T00:25:53Z,triaged ezyang's list oncall: pt2 oncall: export,closed,1,7,https://github.com/pytorch/pytorch/issues/104678, ,It looks like pytree doesn't know how to serialize dataclass. One workaround is to manually register it like  Is it possible to support dataclass in pytree similar to namedtuple and OrderedDict? or it needs to be done by the user.,I think in principle we could support dataclasses directly. Maybe someone should send a patch.,There's already a PR and intensive discussions about adding dataclass to pytree and there are concerns that someone may want dataclass to be something opaque and cannot be flattened. https://github.com/pytorch/pytorch/pull/93214.,I am working on fixing export to not require pytree support on inputs/outputs. More soon. Cc  , is this still an issue? I just ran on nightly and I no longer see the assertion.,It works now. Thanks!
329,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Specifying kwargs for torch.pow causes graph break)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug see title  Error logs  gives   Minified repro _No response_  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Specifying kwargs for torch.pow causes graph break, ğŸ› Describe the bug see title  Error logs  gives   Minified repro _No response_  Versions main ,2023-07-05T19:53:28Z,triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/104656,Related issue:  CC(Accept SymInts and SymFloats For Scalar Inputs), can we close this as a dupe of  CC(Accept SymInts and SymFloats For Scalar Inputs) ? ,Yup it's dupe
338,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add LlamaForCausalLM to HuggingFace suite)ï¼Œ å†…å®¹æ˜¯ (  CC(Add LlamaForCausalLM to HuggingFace suite)  CC(Upgrade HuggingFace to v4.30.2) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Add LlamaForCausalLM to HuggingFace suite,  CC(Add LlamaForCausalLM to HuggingFace suite)  CC(Upgrade HuggingFace to v4.30.2) Signedoffby: Edward Z. Yang  ,2023-07-05T17:51:12Z,Stale module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/104641,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
817,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile fails with ""INTERNAL ASSERT FAILED"" when compiling GPT-2)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am trying an experiment to run the open source GPT2 model in pytorch through torch.compile to get an optimized, fused graph and them examine that. On doing so, I get internal assertions firing which ask me to report the bug to pytorch. Could someone help explain why the bug is firing, and if so, what a workaround would be?  torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised: RuntimeError: list_trace[idx] == nullptr INTERNAL ASSERT FAILED at ""../torch/csrc/jit/frontend/tracer.cpp"":1016, please report a bug to PyTorch.   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"torch.compile fails with ""INTERNAL ASSERT FAILED"" when compiling GPT-2"," ğŸ› Describe the bug I am trying an experiment to run the open source GPT2 model in pytorch through torch.compile to get an optimized, fused graph and them examine that. On doing so, I get internal assertions firing which ask me to report the bug to pytorch. Could someone help explain why the bug is firing, and if so, what a workaround would be?  torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised: RuntimeError: list_trace[idx] == nullptr INTERNAL ASSERT FAILED at ""../torch/csrc/jit/frontend/tracer.cpp"":1016, please report a bug to PyTorch.   ",2023-07-04T21:26:11Z,module: onnx triaged oncall: pt2,closed,0,12,https://github.com/pytorch/pytorch/issues/104610,"I do not get this error when running this code using a cpu. It fails in the export for me. I see the pytorch code does   your error is failing on the second assert.  My thoughts are: This function is given a size and an index, and the index can be less than the size. The second assert seems to be assuming that list_trace elements would be from 0 to idx1 and that attempting to address the element at idx would null pointer.  But if the index idx can be less than the size, then I assume there are indexable elements up to size, so addressing element at idx will not nullptr  if so then this assert should be removed as it does not make sense.      ","My thought is this assert might be firing on GPUs too because of the same reason you mentioned. GPUs might have more kernel fusion and compiler optimizations than CPUs, leading to more graph modifications during the trace. Do you think that is possible? How do we double check your hypothesis and rerun?"," the easiest test would be to remove the assert and test to see at least if this test scenario works. I guess addressing past the end of a valid object could give a malicious user access to content they should not have access to. I think the assert should be changed to  `AT_ASSERT(list_trace[size] == nullptr);` which would police this sort of malicious scenario. I have not got easy access to a GPU environment, can you make this change and test locally? It would be good to know if it fails later  especially in a related way.","  So I ran the above code on a GPU, removed the assert, and rebuilt PyTorch. I then hit the following  Looks like there is something more intrinsic in the pytorch that is being flagged. But as you said, we are failing in the torch.onnx.export now and not the GPU after the removal of the following line of code  ","As an experiment, I commented out the following asserts just to see what happens.  I end up getting the following error:  Not sure why torch.onnx.export is not liking the export. I thought OperatorExportTypes.ONNX_ATEN_FALLBACK took care of the concern, but looks like not.",  an attempt to export a `torch.compile`ed model to onnx?," combination is not officially support. In order to make FX graphs to be supported by ONNX exporter, we need to run decomposition, functionalization and order transformations that neither of the APIs above support. In particular, `torch.onnx.export` is not meant to support Dynamo at al. `torch.onnx.dynamo_export` should be used instead. This is our new experimental ONNX exporter that can work with `torch._dynamo.export`. `torch.compile` is not yet supported too. "," Are we able to export any optimized models? I was hoping to do symbolic analysis, do automatic graph transformations (like torch.compile) using make_fx and then export them. At least, is it possible to visualize the optimized graph using Netron or some other popular visualization app?",">  Are we able to export any optimized models? I was hoping to do symbolic analysis, do automatic graph transformations (like torch.compile) using make_fx and then export them. At least, is it possible to visualize the optimized graph using Netron or some other popular visualization app? Not with `torch.onnx.export`. It is not meant to be used with `make_fx`, `torch.compile`, `torch.export` or any other Dynamo API. This is a by design limitation because `torch.onnx.export` is based on `TorchScript` graphs while `torch.compile` and `torch.export` are based on FX `GraphModule` graphs. FX graphs are meant to replace `TorchScript`, not to augment each other. `torch.onnx.dynamo_export` on the other hand does call `torch._dynamo.export` (similar to `torch.export`) and does handle FX graphs. We will provide APIs to allow users to optimize both the FX and ONNX graphs, but that should come officially with PyTorch 2.1","As mentioned before, `torch.onnx.export` was not designed to support neither `torch.compile` or `torch.export`. There is no plan to add such support as well However, the proposed model can be exported to onnx using `torch.onnx.dynamo_export` which is compatible with `torch.export` Here is the repro  The ONNX file look like  Note the resulting graph was not optimized by `torch.compile`, but ONNX Runtime (or a post processing custom script) could further optimize the resulting model", is your issue resolved?,"Hey  , I've gone ahead and closed the issue as we haven't heard back from you in a while! Feel free to reopen the issue if necessary."
411,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`torch.compile` slowing down transformer model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `torch.compile` is slowing down the execution of `SwinModel` from the transformers library for me on cpu:   Error logs   Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`torch.compile` slowing down transformer model, ğŸ› Describe the bug `torch.compile` is slowing down the execution of `SwinModel` from the transformers library for me on cpu:   Error logs   Minified repro _No response_  Versions  ,2023-07-04T14:04:53Z,oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/104601,"Hi  torch.compile is a JIT compilation approach for graph mode DAG generation, so it also requires ""warmup"" forward passes (like using torch.jit). Adding a for loop at the compiled model forward func, you can see the following forward passes (after 2nd) much faster.","> Hi  torch.compile is a JIT compilation approach for graph mode DAG generation, so it also requires ""warmup"" forward passes (like using torch.jit). Adding a for loop at the compiled model forward func, you can see the following forward passes (after 2nd) much faster. ah I see, a bit lazy from me ğŸ˜… think I'm just used to jax's jit which only ever seems to need 1 ""warmup"" pass "
457,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Revert ""Specialize storage_offset - Does not cover automatic dynamic (#104204)"")ï¼Œ å†…å®¹æ˜¯ (  CC(Enable automatic_dynamic_shapes by default)  CC(Revert ""Specialize storage_offset  Does not cover automatic dynamic (104204)"") This reverts commit 803c14490b189f9b755ecb9f2a969876088ea243. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Revert ""Specialize storage_offset - Does not cover automatic dynamic (#104204)""","  CC(Enable automatic_dynamic_shapes by default)  CC(Revert ""Specialize storage_offset  Does not cover automatic dynamic (104204)"") This reverts commit 803c14490b189f9b755ecb9f2a969876088ea243. ",2023-07-04T13:30:05Z,module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/104599
1984,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Transformer seeing future tokens despite subsequent mask?)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I've been experimenting with Transformer and noticed something odd. I use the correct subsequent mask, but future tokens still influence the decoder. Here's my code:  I'd expect the output for the first 10 positions in out to be identical to out_limited, but they're not. Is this a known issue?  Versions Collecting environment information... PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.17 (main, Jun 28 2023, 12:03:49)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0135genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3080 GPU 1: NVIDIA GeForce RTX 3080 GPU 2: NVIDIA GeForce RTX 2070 SUPER GPU 3: NVIDIA GeForce RTX 2070 SUPER Nvidia driver version: 510.47.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Transformer seeing future tokens despite subsequent mask?," ğŸ› Describe the bug I've been experimenting with Transformer and noticed something odd. I use the correct subsequent mask, but future tokens still influence the decoder. Here's my code:  I'd expect the output for the first 10 positions in out to be identical to out_limited, but they're not. Is this a known issue?  Versions Collecting environment information... PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.17 (main, Jun 28 2023, 12:03:49)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0135genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3080 GPU 1: NVIDIA GeForce RTX 3080 GPU 2: NVIDIA GeForce RTX 2070 SUPER GPU 3: NVIDIA GeForce RTX 2070 SUPER Nvidia driver version: 510.47.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bi",2023-07-04T10:02:10Z,,closed,1,3,https://github.com/pytorch/pytorch/issues/104595,"Hi, what is the difference you got with 1.13.1? I tried with 20230704 nightly and found the diff is almost zero (no larger than 2e8).","I got almost the same (2e8). But still, I would expect it will be 0.0 ",My viewpoint is this would not be an issue. Float point numbers have precision limits so completely identical results are not always available even if mathematically equal. Please refer  https://pytorch.org/docs/stable/notes/numerical_accuracy.html
2022,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Exprted Graph has Different Behavior from Eager Mode for CUDA FP16 Tensor Times a Number)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Use below module to produce the issue. Cast the model to FP16 module (model.to(torch.float16)) and provide FP16 tensor as input, and run on CUDA device.  Here x is FP16 tensor on device, scale is CPU scalar number, the print(x.dtype) on eager mode shows torch.float16 as output, means the Mul CUDA kernel is computed on FP16 (though the scale is float type). But the exported graph is:  Which shows that the Mul Op is computed on float, and cause all following inputs casted to float type. This behavior is different from the torch runing behavior, and since there are lots of Cast nodes inserted to the graph, and most of the computes are on float dtype, which is slower than on FP16 dtype, the graph execution is very inefficient, especially for some big transformer models.  Versions PyTorch version: 2.1.0.dev20230523+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.8.5 (default, Sep  4 2020, 07:30:14)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Tesla V100DGXS32GB GPU 1: Tesla V100DGXS32GB GPU 2: Tesla V100DGXS32GB GPU 3: Tesla V100DGXS32GB Nvidia driver version: 525.85.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.8.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.8.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.8.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.8.0)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Exprted Graph has Different Behavior from Eager Mode for CUDA FP16 Tensor Times a Number," ğŸ› Describe the bug Use below module to produce the issue. Cast the model to FP16 module (model.to(torch.float16)) and provide FP16 tensor as input, and run on CUDA device.  Here x is FP16 tensor on device, scale is CPU scalar number, the print(x.dtype) on eager mode shows torch.float16 as output, means the Mul CUDA kernel is computed on FP16 (though the scale is float type). But the exported graph is:  Which shows that the Mul Op is computed on float, and cause all following inputs casted to float type. This behavior is different from the torch runing behavior, and since there are lots of Cast nodes inserted to the graph, and most of the computes are on float dtype, which is slower than on FP16 dtype, the graph execution is very inefficient, especially for some big transformer models.  Versions PyTorch version: 2.1.0.dev20230523+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.8.5 (default, Sep  4 2020, 07:30:14)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Tesla V100DGXS32GB GPU 1: Tesla V100DGXS32GB GPU 2: Tesla V100DGXS32GB GPU 3: Tesla V100DGXS32GB Nvidia driver version: 525.85.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.8.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.8.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.8.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.8.0",2023-07-04T09:55:34Z,module: onnx triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/104594
662,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Init_rpc() errors when running the test code in the TorchPRC document on two different machines )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I ran the test code in https://pytorch.org/docs/1.13/rpc.html on **two different machines** and got the following errors: worker0: python torchrpc_master.py  worker1: python torchrpc_slave.py  But the same code runs fine on the same machine. I want to know what causes it? This is my code: torchrpc_master.py  torchrpc_master.py   Versions torch version: 1.13.1 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Init_rpc() errors when running the test code in the TorchPRC document on two different machines , ğŸ› Describe the bug I ran the test code in https://pytorch.org/docs/1.13/rpc.html on **two different machines** and got the following errors: worker0: python torchrpc_master.py  worker1: python torchrpc_slave.py  But the same code runs fine on the same machine. I want to know what causes it? This is my code: torchrpc_master.py  torchrpc_master.py   Versions torch version: 1.13.1 ,2023-07-04T02:23:52Z,triaged module: rpc,open,0,1,https://github.com/pytorch/pytorch/issues/104568,"I met the same problem as you , have you solved it yet?"
978,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([torch.compile] Guards failures due to storage offsets in new nightly )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I just wanted to quickly report this issue. In the newest nightly, static code that used to compile without problems broke for me. The model takes views into a tensor as inputs  (see simplified example below). I turns out that with the newest nightly, it is recompiling on every storage offset. The problem is fixed if the view is cloned before being handed to the model.  Error logs   Minified repro A sketch of the problem (but not directly runnable):         input_ids = torch.randint(0, 256, (1024,))         for seq_idx in range(0, max(1, input_ids.shape[1]  2)), 2):             model_inputs = input_ids[:, seq_idx : seq_idx + 3]            compiled_model(model_inputs) Workaround:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[torch.compile] Guards failures due to storage offsets in new nightly ," ğŸ› Describe the bug Hi, I just wanted to quickly report this issue. In the newest nightly, static code that used to compile without problems broke for me. The model takes views into a tensor as inputs  (see simplified example below). I turns out that with the newest nightly, it is recompiling on every storage offset. The problem is fixed if the view is cloned before being handed to the model.  Error logs   Minified repro A sketch of the problem (but not directly runnable):         input_ids = torch.randint(0, 256, (1024,))         for seq_idx in range(0, max(1, input_ids.shape[1]  2)), 2):             model_inputs = input_ids[:, seq_idx : seq_idx + 3]            compiled_model(model_inputs) Workaround:   Versions  ",2023-07-04T00:24:04Z,triaged oncall: pt2 module: dynamic shapes,closed,0,4,https://github.com/pytorch/pytorch/issues/104563,Cc  ,"I think this is because https://github.com/pytorch/pytorch/pull/104204 is too aggro. I suspect that Inductor actually generates polymorphic code for storage offsets automatically, so we should always treat these as ""dynamic"" no matter what.",The likely source for this issue was reverted.  would you confirm that on a nightly containing bfd995f0d6bf87262613b5e89d871832ca9e9938 this no longer repros ?," Yeah, the recompilation does not occur on the 6th of July nightly."
542,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work)ï¼Œ å†…å®¹æ˜¯ (An alternative to CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture) where we relax the capture mode to allow watchdogs to clean up (via `cudaEventQuery`) previously enqueued work. CC  who proposed this alternative   A   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[NCCL][CUDA][CUDA Graphs] Change capture mode to thread local to handle watchdogs cleaning straggling work,An alternative to CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture) where we relax the capture mode to allow watchdogs to clean up (via `cudaEventQuery`) previously enqueued work. CC  who proposed this alternative   A   ,2023-07-03T21:57:37Z,module: cuda triaged module: nccl open source module: cuda graphs ciflow/trunk topic: not user facing ciflow/periodic ciflow/inductor,closed,0,29,https://github.com/pytorch/pytorch/issues/104555,"I can share another place where `cudaStreamCaptureModeThreadLocal` is used: nccltests. https://github.com/NVIDIA/nccltests/blob/1a5f551ffd6e3271982b03a9d5653a3f6ba545fa/src/common.cuL422L429 It documents two other reasons: (i) For NCCL multithread mode to work with CUDA Graph;  (ii) For the first P2P operation to work with CUDA Graph (because P2P's ondemand connect may call CUDA malloc APIs). Case (i) is less common to PyTorch users who use Python APIs. However, there can be still users who use the C++ `ProcessGroupNCCL` APIs and run it in multithread mode (1 thread per GPU, multiple threads per process). Case (ii) can be hidden if user does a warmup of the P2P operation before the graph capture. But I think over time it may be good to relax this assumption.","Some context for  in case you see this PR first before https://github.com/pytorch/pytorch/pull/104487, this is being considered as an alternative to the fix in CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture). Our current main concern is that using a thread local capture mode will affect the visibility of what is included in the capture, and a trivially bad case of this would be excluding some kernel launches that happen in other threads (such as in the case of autograd/backward?). I'm also asking internally at NVIDIA about the implications of the thread local capture mode.",Asked around NV regarding this issue and the response is that thread local captures wouldn't affect visibility so I think we're good on that end.,"In general, `cudaEventQuery` calls are not allowed for events created during the capture. CC(NCCL process group: avoid workEnqueue when capturing cuda graph) addressed this case by gating the work to be queried by the watchdog behind a check of the current capture status. However, it appears that in the global capture mode, `cudaEventQuery` is also disallowed when called by other threads, even on events created before the capture. So this change in mode is meant to allow the watchdog thread to cleanup straggling work that was created _before_ the capture, a case not handled by CC(NCCL process group: avoid workEnqueue when capturing cuda graph). The alternative would be to force all watchdogs to ""finish"" in order to prevent any future `cudaEventQuery` calls before starting a capture, but this change appears to be cleaner.","Would it be worth adding a ctor arg to CudaGraphs to specify capture mode, or have some other way of changing the capture mode? As you've mentioned in the other thread, if we're capturing the backward or doing other multithreading, this might lead to a regression in detected bad cudagraph events. ","If the constructor would use the strictest ""global"" mode by default we would be breaking existing user code that was written before recent changes to the watchdog e.g., CC(Rewrite NCCL watchdog to more reliably throw timeout) which introduces places where `cudaEventQuery` could be called where it previously would not (I believe this is caused by calling `workEnquque` by default where it was not called previously). The response I got from the graphs team is that thread local capture mode isn't the default because CUDA doesn't have visibility into whether there isn't any synchronization happening between threads in a capture. We might consider adding a debugging mode where the graph capture mode can be set to global, but for this approach I think it might be too strict to use as a default.","CUDAGraphs is already very difficult to debug. I would lean against making it even more difficult to debug in the general case (can't detect if another thread in the backward does something that would throw) because of new NCCL changes.  , any thoughts here ?","> , any thoughts here ? EDIT: Ok this comment is completely wrong as I didn't read the doc properly. Please disregard! Answering again below ~First, I expect that this PR will break anyone capturing the fw+bw jointly. So it doesn't sound good asis for sure. I'm surprised our CI didn't catch that btw!~ ~We could add this to the TLS states we preserve across worker threads: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/ThreadLocalState.h to fix this particular problem.~ ~It is not clear to me though how to preserve this state across threads? Especially with the nontrivial wrapper we have above cudagraph to make it work with multiple cudagraphs!~ ~In general, I agree with  concern that we are not the best citizens in terms of careful state preservation when threading is used, so making this the new default is likely to break quite a few things. Keeping the default as global and having this as an optin for some users that have issues with side threads sounds a lot safer for sure.~"," Do you have an explanation as to what would be broken by the thread local capture mode in a combined fw+bwd capture? Note that the capture mode does not control the _visibility_ of kernels launched on other threads to the capture, but rather just what is disallowed by the API. However, if the main concern is that we could be opening a previously unopened can of worms debuggingwise, then please also have a look at CC([NCCL][CUDA][CUDA Graphs] Flush enqueued work before starting a graph capture) which is an alternative that lets us keep global capture mode as the default. (also CC  )","The definition for this function is nontrivial so I'm going to try and repeat here so you can tell me if it is correct. In this world, each thread can be in either Global/ThreadLocal/Relaxed mode and, independently, each call to captureBegin can also be tagged with Global/ThreadLocal/Relaxed. You end up with 2x (if the call is in the local thread or not) 3x3 (for the cross product of thread state and captureBegin state) possibility (18 total!). We could do two tables of all cases but I assume that was clear in your head from the beginning and my ascii art foo is bad. So my understanding of what this changes do is as follows:  For any unsafe API call within the current thread: nothing changes: it will still raise  For any other thread that is running with the default setting (Global): nothing changes: it will still raise  For any other thread where the user changed the setting to Relaxed: nothing changes: it still doesnt raise.  For any other thread where the user changed the setting to ThreadLocal: THIS CHANGED: it used to raise and now will pass just fine. So I assume that the background NCCL thread is configured in ThreadLocal mode and that's why this fixes your issue? Also we don't want to change that thread to Relaxed as that would be too unsafe in general for a NCCL thread? I am understanding this properly?","> Do you have an explanation as to what would be broken by the thread local capture mode in a combined fw+bwd capture? Note that the capture mode does not control the visibility of kernels launched on other threads to the capture, but rather just what is disallowed by the API I don't think anything will break, it just makes it possible for a multithread backward to capture a CUDAGraph that isn't valid without error, where it was not before. ","> I don't think anything will break, it just makes it possible for a multithread backward to capture a CUDAGraph that isn't valid without error, where it was not before. But if my understanding is correct, since we don't change the state of our autograd worker thread, they should have the default state (Global) and thus will still error out in this case! (the second case in my comment above is not changed).","> But if my understanding is correct, since we don't change the state of our autograd worker thread, they should have the default state (Global) and thus will still error out in this case! (the second case in my comment above is not changed). My interpretation of the docs was that if the local thread is instantiated with ThreadLocal it will not error on bad events for other threads that are ThreadGlobal. That is why this PR fixes their use case.","Ok, I'm not sure if my interpretation in https://github.com/pytorch/pytorch/pull/104555issuecomment1631553124 is correct or not,  could you please correct me here? Are the 4 cases described there properly identified?","Yes, I think  's understanding is basically how I am interpreting things as well. Empirically with this PR: 1. The watchdog thread is probably started before any graph capture (so it operates in ""global mode""). 2. Another thread (the main thread)? starts a capture with thread local mode. 3. The watchdog thread issues a `cudaEventQuery`, but now  does not apply, so we are still OK.  3a. The local thread is not the watchdog thread is still not issuing `cudaEventQuery` calls, so that's why we are safe on that end.","> The watchdog thread is probably started before any graph capture (so it operates in ""global mode""). Does this imply that if the watchdog thread was started after the graph capture started, it would not be in Global mode? > My interpretation of the docs was that if the local thread is instantiated with ThreadLocal it will not error on bad events for other threads that are ThreadGlobal. That is why this PR fixes their use case. Sounds like  you agree with  statement here? After this change, any other threads will be able to call into unsafe APIs.  what do you think about changing the watchdog thread to put it in Relaxed mode instead of changing the capture setting?", I will investigate whether changing the capture mode of just the watchdog thread resolves the issue. Is there a reason you would prefer changing the mode to relaxed vs. just thread local which in theory would also resolve the issue?,Quick question: what does applying a capture mode to the watchdog thread mean? The watchdog thread is not capturing anything. ," that's a great question and hopefully I can summarize the path of reasoning that got us here. 1. The crash is caused by the watchdog thread calling `cudaEventQuery` on work that was enqueued before a graph capture. 2. The initial proposal was to change the capture mode of the thread performing the capture to the ""thread local"" mode, so that other threads querying events (the watchdog) don't crash as the events in question were created before the actual capture and therefore shouldn't interfere with the capture. 3. Many such as  and  were concerned with how we changed the capture mode of the capturing thread in this PR (by globally changing the constructor), as we could potentially miss errors/failures caused by always using this more lenient mode. 4.  pointed out that it is really just the watchdog thread that we are concerned with issuing a disallowed `cudaEventQuery` call, so another potential solution is to just have the watchdog thread operate in a more lenient mode. This seems to be fairly minimal change that shouldn't have unexpected effects on the correctness or debuggability of code involving graph captures. To clarify, while the watchdog thread itself is not capturing anything, since the watchdog thread never calls `cudaThreadExchangeStreamCaptureMode`, it is operating with the ""global"" mode when another thread is in the middle of a capture, and hence crash the capture because `cudaEventQuery` is disallowed in this scenario. 4a. Our current understanding is the following (assuming the `cudaEventQuery` is for an event outside of a capture). capturing thread mode  I will try out the approach in CC(PEP8) tomorrow and open a PR if it appears to work. We would implement this by e.g., calling `cudaThreadExchangeStreamCaptureMode` to change its mode to the thread local mode at the start of the watchdog thread.", thanks for the detailed writeup! My understanding from the doc is that if the capturing thread is global and the watchdog thread is thread local then it WILL fail! You need to have the watchdog thread in relaxed mode for its call to cudaEventQuery to be ignored by our global capturing thread.," I did test that thread local seems to work without crashing, opened CC([DO NOT MERGE][NCCL][CUDA][CUDA Graphs] Set watchdog runtime capture mode to thread local to handle cleaning straggling work) My read of   was that as the local thread (fhe watchdog) does not have an ongoing capture sequence, it can issue ""potentially unsafe API calls"" such as `cudaEventQuery` if the event was not recorded during a capture sequence.", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `eqypatch19` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout eqypatch19 && git pull rebase`)", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/104555/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/6015006234,"> Isnt CC([DO NOT MERGE][NCCL][CUDA][CUDA Graphs] Set watchdog runtime capture mode to thread local to handle cleaning straggling work) the plan on record to fix this? I think so, if we can get to the bottom of the SyncBatchNorm test failure on it. (I just tried to rebase this one as I thought it had had interesting test failures which I can no longer see.)","> Isnt CC([DO NOT MERGE][NCCL][CUDA][CUDA Graphs] Set watchdog runtime capture mode to thread local to handle cleaning straggling work) the plan on record to fix this? Yes, CC([DO NOT MERGE][NCCL][CUDA][CUDA Graphs] Set watchdog runtime capture mode to thread local to handle cleaning straggling work) is the plan on record.   do you mind if I close this PR in favor of CC([DO NOT MERGE][NCCL][CUDA][CUDA Graphs] Set watchdog runtime capture mode to thread local to handle cleaning straggling work)? They use the same methodology so I figure we can keep just one of them. Feel free to reopen if needed."
2000,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile failed when using dynamic input shapes on generation tasks.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug torch.compile failed when using dynamic input shapes on generation tasks. It seems that related to this PR 103600 because the error disappear when I revert this change.  Hi  , would you please check it? Thanks! BTW, the bug also exists when I use the newest version pytorch=2.1.0.dev20230702+cpu Command: TORCHDYNAMO_REPORT_GUARD_FAILURES=1 TORCH_SHOW_CPP_STACKTRACES=1 OMP_NUM_THREADS=56 numactl C 055 membind 0 python3 xxx.py Python script  The bug is as follows   Versions Collecting environment information...                                                                                                                                                                         PyTorch version: 2.1.0a0+git1b16ac7                                                                                                                                                                           Is debug build: False                                                                                                                                                                                         CUDA used to build PyTorch: None                                                                                                                                                                              ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 11.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.28 Python version: 3.9.16 (main, May 15 2023, 23:46:34)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.16.0rc8intelnext01534g53cb5f883cf7x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A G)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile failed when using dynamic input shapes on generation tasks.," ğŸ› Describe the bug torch.compile failed when using dynamic input shapes on generation tasks. It seems that related to this PR 103600 because the error disappear when I revert this change.  Hi  , would you please check it? Thanks! BTW, the bug also exists when I use the newest version pytorch=2.1.0.dev20230702+cpu Command: TORCHDYNAMO_REPORT_GUARD_FAILURES=1 TORCH_SHOW_CPP_STACKTRACES=1 OMP_NUM_THREADS=56 numactl C 055 membind 0 python3 xxx.py Python script  The bug is as follows   Versions Collecting environment information...                                                                                                                                                                         PyTorch version: 2.1.0a0+git1b16ac7                                                                                                                                                                           Is debug build: False                                                                                                                                                                                         CUDA used to build PyTorch: None                                                                                                                                                                              ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 11.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.28 Python version: 3.9.16 (main, May 15 2023, 23:46:34)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.16.0rc8intelnext01534g53cb5f883cf7x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A G",2023-07-03T01:37:59Z,triaged oncall: pt2 module: dynamic shapes,closed,0,3,https://github.com/pytorch/pytorch/issues/104515,"feng you're running with `torch._dynamo.config.assume_static_by_default = False`, can you instead try with this True and `automatic_dynamic_shapes = True`? Thanks.","Hi  . Thanks for your reply. I tried   , but the error still exists. I also tried to delete these configs, but the error is still the same.",it seems cpu backend issue.
1981,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Performance issue of training GPTLMHeadModel with FSDP)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am training GPTLMHeadModel (7B) from flash_attn.models.gpt with FSDP on 16 A100 GPUs and find it slow.  The model is wrapped with transformer_auto_wrap_policy. In comparison, when training LLaMA (7B) model with FSDP, the average iteration time is about 5.3s (local_bsz=8, global_bsz=128). However, when training GPTLMHeadModel (7B) with FSDP, the average iteration time increases to about 7.3s.  To delve into the reason, I check the tensor board and find that for GPTLMHeadModel, there are many CUDA FREE operations during the forward computation of CrossEntropyLoss, as well as an extremely long clone kernel and a contiguous kernel. It seems that when calculating the huge tensor ([bsz, seq_len1, vocab_size]) during CrossEntropyLoss, the memory runs out and FSDP begin to frequently free the memory. I also check the torch CUDA memory summary and find that the number of cudaMalloc retries is 46. However, for LLaMA model, there is no frequent CUDA FREE during forward pass, and the number of cudaMalloc retries is 0.  This situation is perplexing because these two models should have the same parameter size and nearly identical training memory consumption, yet the memory allocation behavior of FSDP is markedly different between them.  I am interested in understanding the mechanism of memory allocation and deallocation in FSDP, specifically under what circumstances FSDP will invoke the CUDA FREE operation to release memory. Here is the tensorboard of GPTLMHeadModel.  !gpthead_tensorboard pic Here is the CUDA memory summary for GPTLMHeadModel and LLaMA. !gpt_head pic !llama pic  Versions PyTorch version: 2.1.0.dev20230522+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTor)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Performance issue of training GPTLMHeadModel with FSDP," ğŸ› Describe the bug I am training GPTLMHeadModel (7B) from flash_attn.models.gpt with FSDP on 16 A100 GPUs and find it slow.  The model is wrapped with transformer_auto_wrap_policy. In comparison, when training LLaMA (7B) model with FSDP, the average iteration time is about 5.3s (local_bsz=8, global_bsz=128). However, when training GPTLMHeadModel (7B) with FSDP, the average iteration time increases to about 7.3s.  To delve into the reason, I check the tensor board and find that for GPTLMHeadModel, there are many CUDA FREE operations during the forward computation of CrossEntropyLoss, as well as an extremely long clone kernel and a contiguous kernel. It seems that when calculating the huge tensor ([bsz, seq_len1, vocab_size]) during CrossEntropyLoss, the memory runs out and FSDP begin to frequently free the memory. I also check the torch CUDA memory summary and find that the number of cudaMalloc retries is 46. However, for LLaMA model, there is no frequent CUDA FREE during forward pass, and the number of cudaMalloc retries is 0.  This situation is perplexing because these two models should have the same parameter size and nearly identical training memory consumption, yet the memory allocation behavior of FSDP is markedly different between them.  I am interested in understanding the mechanism of memory allocation and deallocation in FSDP, specifically under what circumstances FSDP will invoke the CUDA FREE operation to release memory. Here is the tensorboard of GPTLMHeadModel.  !gpthead_tensorboard pic Here is the CUDA memory summary for GPTLMHeadModel and LLaMA. !gpt_head pic !llama pic  Versions PyTorch version: 2.1.0.dev20230522+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTor",2023-07-02T13:34:57Z,triaged module: fsdp,closed,0,5,https://github.com/pytorch/pytorch/issues/104509,,"Are you passing `limit_all_gathers=True` to FSDP? If not, could you try that and report back?","> Are you passing `limit_all_gathers=True` to FSDP? If not, could you try that and report back? Thanks very much. I tried passing `limit_all_gathers=True` to FSDP and it worked. The average iteration time of GPTLMHead reduces to 5.0s, and the number of cudaMalloc retries becomes 0. I wonder if `limit_all_gathers=True` is a recommended setting to most of the models? Will this setting bring any side effect for some cases?"," `limit_all_gathers=True` is recommended, and we should/will make it default soon. For LLMs, it is pretty safe to always set it to `True`. The only case where you may want it as `False` is if you have an exceptionally CPU bound workload and want to issue allgathers way in advance to allow the CPU thread to continue issuing later kernels. *However*, our current design with `limit_all_gathers=False` still may suffer from the `cudaMalloc` retries you saw in that case. In other words, in the ideal case, we should always set `limit_all_gathers=True`, *and* we should offer something fundamentally different to deal with CPUboundedness systematically (e.g. a public API to prefetch allgathers explicitly that does not risk `cudaMalloc` retries).",Got it. Thanks again!
464,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Generate `nearbyint` for Round in tensorexpr llvm codegen, match `torch.round` result)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(`torch.round` behaves inconsistently when running a traced model), which matches the behavior of `torch.round` (doc)  â€œround half to evenâ€  Using the repro code, the output is correct:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Generate `nearbyint` for Round in tensorexpr llvm codegen, match `torch.round` result","Fixes CC(`torch.round` behaves inconsistently when running a traced model), which matches the behavior of `torch.round` (doc)  â€œround half to evenâ€  Using the repro code, the output is correct:  ",2023-06-29T17:45:53Z,open source Merged NNC ciflow/trunk release notes: jit,closed,0,10,https://github.com/pytorch/pytorch/issues/104430, merge rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `llvm_codegen_round` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout llvm_codegen_round && git pull rebase`)"," Merge failed **Reason**: Approval needed from one of the following: idning, bigfootjon, lessw2020, PratsBhatt, MaigoAkisame, ... Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", could you please help to check who should review and approve before merging? Thank you.,thanks nrv !, merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `llvm_codegen_round` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout llvm_codegen_round && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2056,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED at HIPGuardImplMasqueradingAsCUDA.h:60, please report a bug to PyTorch)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I get the following runtime error when trying to use the Koala LLM and Microsoft Guidance :   json {     ""id"": ""{{id}}"",     ""description"": ""{{description}}"",     ""name"": ""{{gen 'name'}}"",     ""age"": {{gen 'age' pattern='[09]+' stop=','}},     ""armor"": ""{{select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}"",     ""weapon"": ""{{select 'weapon' options=valid_weapons}}"",     ""class"": ""{{gen 'class'}}"",     ""mantra"": ""{{gen 'mantra' temperature=0.7}}"",     ""strength"": {{gen 'strength' pattern='[09]+' stop=','}},     ""items"": [{{geneach 'items' num_iterations=5 join=', '}}""{{gen 'this' temperature=0.7}}""{{/geneach}}] } Expected behavior : This code should return the json completed by the LLM I use this model but get the same error with others : https://huggingface.co/TheBloke/koala7BGPTQ4bit128g/tree/main The llama_inference library comes from GPTQforLLaMa (Cuda branch) : https://github.com/qwopqwop200/GPTQforLLaMa  Versions PyTorch version: 2.0.1+rocm5.4.2 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22803474e8620 OS: Linux Mint 21.1 (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.075genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.5.119 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Radeon RX 6800 XT Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22803 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: ArchitectureÂ :                   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED at HIPGuardImplMasqueradingAsCUDA.h:60, please report a bug to PyTorch"," ğŸ› Describe the bug I get the following runtime error when trying to use the Koala LLM and Microsoft Guidance :   json {     ""id"": ""{{id}}"",     ""description"": ""{{description}}"",     ""name"": ""{{gen 'name'}}"",     ""age"": {{gen 'age' pattern='[09]+' stop=','}},     ""armor"": ""{{select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}"",     ""weapon"": ""{{select 'weapon' options=valid_weapons}}"",     ""class"": ""{{gen 'class'}}"",     ""mantra"": ""{{gen 'mantra' temperature=0.7}}"",     ""strength"": {{gen 'strength' pattern='[09]+' stop=','}},     ""items"": [{{geneach 'items' num_iterations=5 join=', '}}""{{gen 'this' temperature=0.7}}""{{/geneach}}] } Expected behavior : This code should return the json completed by the LLM I use this model but get the same error with others : https://huggingface.co/TheBloke/koala7BGPTQ4bit128g/tree/main The llama_inference library comes from GPTQforLLaMa (Cuda branch) : https://github.com/qwopqwop200/GPTQforLLaMa  Versions PyTorch version: 2.0.1+rocm5.4.2 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22803474e8620 OS: Linux Mint 21.1 (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.075genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.5.119 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Radeon RX 6800 XT Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22803 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: ArchitectureÂ :                   ",2023-06-29T13:44:13Z,module: rocm triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/104411,  What is your gfx target? can you run below command ? Thanks. ,"  Can you upgrade your version to ROCm 5.6 or plus, and let you know your issue then. If we don't get reply in a week, we will close this. ",No response from reporter. Closing.
556,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`test_storage_meta_errors_cpu` should fail but passes in CI)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug CI signal on  https://github.com/pytorch/pytorch/pull/104355 is green, but testing on https://github.com/pytorch/pytorch/actions/runs/5409497721/jobs/9829920184 is also green, which should not be possible. And indeed local run of the test fails with `TypeError`:   Versions CI /pytorchdevinfra)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,`test_storage_meta_errors_cpu` should fail but passes in CI," ğŸ› Describe the bug CI signal on  https://github.com/pytorch/pytorch/pull/104355 is green, but testing on https://github.com/pytorch/pytorch/actions/runs/5409497721/jobs/9829920184 is also green, which should not be possible. And indeed local run of the test fails with `TypeError`:   Versions CI /pytorchdevinfra",2023-06-29T13:40:38Z,module: ci triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/104410,"An even more ridiculous example:  Edit, though this one is easy: `test_segment_reductions` were never run in OSS :/","Actually, just a benign case of not running CPU tests on GPU (I.e. that one must not have `` decorator to expose the issue)"
718,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make `torch._dynamo` lazy-importable)ï¼Œ å†…å®¹æ˜¯ (Use PEP562 to import `_dynamo` and `_inductor` only when needed.  Remove redundant imports from tests  Add `test_lazy_imports_are_lazy` to make sure they will not get imported by accident   ğŸ¤– Generated by Copilot at bae8e90 > _Sing, O Muse, of the daring deeds of PyTorch, the swift and fiery_ > _framework of deep learning, that with skill and cunning wrought_ > _many wonders of dynamic compilation, using the hidden powers_ > _of `_dynamo` and `_inductor`, the secret modules of LLVM and MLIR._ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Make `torch._dynamo` lazy-importable,"Use PEP562 to import `_dynamo` and `_inductor` only when needed.  Remove redundant imports from tests  Add `test_lazy_imports_are_lazy` to make sure they will not get imported by accident   ğŸ¤– Generated by Copilot at bae8e90 > _Sing, O Muse, of the daring deeds of PyTorch, the swift and fiery_ > _framework of deep learning, that with skill and cunning wrought_ > _many wonders of dynamic compilation, using the hidden powers_ > _of `_dynamo` and `_inductor`, the secret modules of LLVM and MLIR._ ",2023-06-28T19:38:14Z,Merged ciflow/trunk release notes: python_frontend topic: improvements module: inductor module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/104368, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2031,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ReduceLROnPlateau will throw IndexError: list index out of range with modified optimizer's param_groups.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am aware of similar posts which already notify this issue:   CC(ReduceLROnPlateau will fail when add new parameter group to the optimizer)   CC(Poor support of `Optimizer.add_param_group`)  And a lightning issue: https://github.com/LightningAI/lightning/issues/8727 But it seems that the error should be caught sooner to help the user understand what's going on. Using ReduceLROnPlateau: https://github.com/pytorch/pytorch/blob/044a8e3305bdff28780cdab757b859abf2fc76d9/torch/optim/lr_scheduler.pyL913 If we use Optimizer.add_param_group) on the optimizer attached to the scheduler while the ReduceLrOnPlateau is instantiated, the method: _reduce_lr(self, epoch) once called while throw an **IndexError: list index out of range** due to this line [`new_lr = max(old_lr * self.factor, self.min_lrs[i]`](https://github.com/pytorch/pytorch/blob/044a8e3305bdff28780cdab757b859abf2fc76d9/torch/optim/lr_scheduler.pyL1036C12L1036C64) which tries to access an index of `min_lrs` that does not exist. This is something that usually happens in a finetuning process when we unfreeze some layers of a network and add a new group of parameters to the optimizer attached to the scheduler. The property `min_lrs` of the ReduceLROnPlateau instance is already populated at this point based on the previous number of existing groups in the optimizer ( length of the param_groups) and will not be updated accordingly. To make it clearer that the problem comes from the fact that `len(self.min_lrs) != len(optimizer.param_groups)` we could notify the user with some ValueError encapsulated in a property.  This is a general idea and could even be dealt with using a list extension that match length of the param_groups instead of raising an )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,ReduceLROnPlateau will throw IndexError: list index out of range with modified optimizer's param_groups.," ğŸ› Describe the bug I am aware of similar posts which already notify this issue:   CC(ReduceLROnPlateau will fail when add new parameter group to the optimizer)   CC(Poor support of `Optimizer.add_param_group`)  And a lightning issue: https://github.com/LightningAI/lightning/issues/8727 But it seems that the error should be caught sooner to help the user understand what's going on. Using ReduceLROnPlateau: https://github.com/pytorch/pytorch/blob/044a8e3305bdff28780cdab757b859abf2fc76d9/torch/optim/lr_scheduler.pyL913 If we use Optimizer.add_param_group) on the optimizer attached to the scheduler while the ReduceLrOnPlateau is instantiated, the method: _reduce_lr(self, epoch) once called while throw an **IndexError: list index out of range** due to this line [`new_lr = max(old_lr * self.factor, self.min_lrs[i]`](https://github.com/pytorch/pytorch/blob/044a8e3305bdff28780cdab757b859abf2fc76d9/torch/optim/lr_scheduler.pyL1036C12L1036C64) which tries to access an index of `min_lrs` that does not exist. This is something that usually happens in a finetuning process when we unfreeze some layers of a network and add a new group of parameters to the optimizer attached to the scheduler. The property `min_lrs` of the ReduceLROnPlateau instance is already populated at this point based on the previous number of existing groups in the optimizer ( length of the param_groups) and will not be updated accordingly. To make it clearer that the problem comes from the fact that `len(self.min_lrs) != len(optimizer.param_groups)` we could notify the user with some ValueError encapsulated in a property.  This is a general idea and could even be dealt with using a list extension that match length of the param_groups instead of raising an ",2023-06-28T18:25:10Z,module: optimizer triaged actionable module: LrScheduler,closed,0,12,https://github.com/pytorch/pytorch/issues/104361," Thanks for the report and for reraising this issue! We would be happy to review a PR that would fix this discrepancy. Before you go for it, I do have a questionwhen adding a param group, do you typically just want to have the same min_lr across all param groups, or would you perhaps start with a different min_lr? If I understood your solution correctly, it would set everything to the same min_lr if a noncontainer was given (we should probably be more precise with typing in the actual implementation), and it would just override the list if a list/tuple were provided. Is this the right understanding?","The point is that we should force the param min_lrs length to match the one of self.optimizer.param_groups but there is no way for the scheduler to know when the number of param group gets modified by the optimizer (as the optimizer is another instance). In order to make sure that min_lrs length match, we could (with a getter logic) expand a list with the lenght of param_groups having the same min_lrs for each element (in the case where there was a single min_lrs) and maybe raise and error if min_lrs is already a list with different min_lrs as we can't choose which value should be assigned to fill the list. There is no clear cut implementation and should be a design choice clearly documented.","Ah, I had interpreted your code as: we should error that lists are different lengths but also offer people an easy api to call into if they just want to extend the list to be the same for every element. I'm not sure the second is actually something people normally want, so erroring out (with documentation) would already be an upgrade. Whether we provide more APIs for people to easily match the min_lrs length would follow as a future step, if it's even desirable.",Yes this could be it.,So a good first actionable step would be erroring with a better error than we currently have!,any plans about this ? I still have this error  ," The original consensus was to raise a better error saying that the list lengths don't match. Here, I'd imagine a workaround to be to instantiate the LRScheduler after all param groups have been added to the optimizer (including frozen params with requires_grad=False). We had briefly discussed the design for how to better handle this but did not come to a satisfying conclusion. Was there a specific solution you're looking for?","  I am using Pytorch Lightning and their BackboneFinetuning callback (see here). The way it's implemented is that at a given epoch, it will unfreeze the backbone (make the layers trainable), and add a new parameter group to the optimizer.   For the moment I use it as is, so it doesn't work with ReduceLROnPlateau. If you tell me that it's gonna stay like that, I might try to subclass this callback to see if I can make it work, but it might not be straightforward.","Ah I see. Here is something we could do: 1. Add an API to LRScheduler, call it `refresh` that refreshes the LRScheduler/reinits it based on any changes observed in its optimizer member. 2. Change pytorchlightning code to always call `lrscheduler.refresh(...)` after adding a param group (or modifying the optimizer in a stateful way). This should fix the discrepancy in a very explicit waydo you have thoughts?","  Yeah, I think that's a great idea! The `finetune_function` of Pytorch Lightning has access to the pl_module, and we can access the LRScheduler through `pl_module.lr_schedulers()`, so we should be able to call `pl_module.lr_schedulers().refresh()` directly. Would love to see that implemented, thanks :)"," ~I opened a PR here: https://github.com/pytorch/pytorch/pull/137632~ Never mind, I decided to go with a more automatic approach for the majority of users: https://github.com/pytorch/pytorch/pull/137637 Could you try it out and give some feedback?",Thank you  ! It's even better like that because it doesn't require to change anything in the pytorch lightning code :) I tested it and it seems to work well on my side! Looking forward to seeing this released :)
463,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(restores torch inductor coverage to three tests)ï¼Œ å†…å®¹æ˜¯ (Restores torch inductor coverage on the following tests:  `test_hook_remove`  `test_copy_many_to_one`  `test_typed_storage_deprecation_warning` See CC(skipIfTorchInductor Tracking Issue ) for tracking of torch inductor skipped tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,restores torch inductor coverage to three tests,Restores torch inductor coverage on the following tests:  `test_hook_remove`  `test_copy_many_to_one`  `test_typed_storage_deprecation_warning` See CC(skipIfTorchInductor Tracking Issue ) for tracking of torch inductor skipped tests,2023-06-28T17:21:32Z,open source Stale topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/104358,The committers listed above are authorized under a signed CLA.:white_check_mark: login: georgehorrell / name: George Horrell  (3b8c6729b84124b0e83661255653ee324634844d),The three tests pass for me locally but I wasn't exhaustive about testing all devices. Will this PR eventually go through CI/CD? I don't see any indication in the HUD that it's pending testing., label ciflow/inductor,> The three tests pass for me locally but I wasn't exhaustive about testing all devices. Will this PR eventually go through CI/CD? I don't see any indication in the HUD that it's pending testing. I approved the CI. Approval is needed because you're a firsttime contributor currently.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
283,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix UntypedStorage pin error)ï¼Œ å†…å®¹æ˜¯ (Summary: Fixes:  Test Plan: Sandcastle Differential Revision: D47093797)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix UntypedStorage pin error,Summary: Fixes:  Test Plan: Sandcastle Differential Revision: D47093797,2023-06-28T16:32:25Z,fb-exported Merged ciflow/trunk release notes: memory format topic: improvements topic: bug fixes,closed,0,7,https://github.com/pytorch/pytorch/issues/104355,This pull request was **exported** from Phabricator. Differential Revision: D47093797,This pull request was **exported** from Phabricator. Differential Revision: D47093797,This pull request was **exported** from Phabricator. Differential Revision: D47093797,Test failure is unrelated., drci, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1596,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Meff Attn Bias )ï¼Œ å†…å®¹æ˜¯ ( Summary  Review Points  Automatically pad tensors to create aligned masks when seqlen_kv is not multiple of 16. This will cause memory spike ~ 2 * attn_mask size which could in theory be big.  At appears though that doing this + mem_eff is faster than no_pad + math. SO seems to be worth it  Using expand to view the attn_mask in 4d. This is a little different to how we enforce q,k,v to be viewed in 4d prior to calling. Also not supprint b*n_heads, seq_lenq, seq_lenkv case.  Should enable, CC(Add support for ALiBi/relative positional biases to the fast path for Transformers)  Profiling I ran a bunch of comparisons between sdpa.MATH and sdp.MemEffAttention.  I added a attn_bias of shape (1, 1, seqlen_q, seqln_k). For these experiments seqlen_q == seqlen_k. These were all ran on an a100 80gb gpu. Configs:     The function calls `sdpa(input**).sum().backward()`.    I calculated the geomean speedup of the efficient attention path of the math path for all these configs:    `Geomean Speedup: 1.977` An example comparision with batchsize = 8, num_heads = 32, embed_dim = 64, and dtype = torch.float16: !attn_mask_compare_bsz_8_num_heads_32_embed_dim_64_dtype_fp16  This was done using the current state of the branch where we force alignment of mask when the last dim is not divisible by 16, which shows up in seq_len = 15 and 555 case. The full data can be found here: attn_mask_sweep.csv )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Meff Attn Bias ," Summary  Review Points  Automatically pad tensors to create aligned masks when seqlen_kv is not multiple of 16. This will cause memory spike ~ 2 * attn_mask size which could in theory be big.  At appears though that doing this + mem_eff is faster than no_pad + math. SO seems to be worth it  Using expand to view the attn_mask in 4d. This is a little different to how we enforce q,k,v to be viewed in 4d prior to calling. Also not supprint b*n_heads, seq_lenq, seq_lenkv case.  Should enable, CC(Add support for ALiBi/relative positional biases to the fast path for Transformers)  Profiling I ran a bunch of comparisons between sdpa.MATH and sdp.MemEffAttention.  I added a attn_bias of shape (1, 1, seqlen_q, seqln_k). For these experiments seqlen_q == seqlen_k. These were all ran on an a100 80gb gpu. Configs:     The function calls `sdpa(input**).sum().backward()`.    I calculated the geomean speedup of the efficient attention path of the math path for all these configs:    `Geomean Speedup: 1.977` An example comparision with batchsize = 8, num_heads = 32, embed_dim = 64, and dtype = torch.float16: !attn_mask_compare_bsz_8_num_heads_32_embed_dim_64_dtype_fp16  This was done using the current state of the branch where we force alignment of mask when the last dim is not divisible by 16, which shows up in seq_len = 15 and 555 case. The full data can be found here: attn_mask_sweep.csv ",2023-06-28T00:18:04Z,module: nn enhancement Merged Reverted ciflow/trunk release notes: nn module: inductor ciflow/inductor,closed,6,17,https://github.com/pytorch/pytorch/issues/104310,, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""PR introduced cuda OOM issue"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 10 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8clang10onnx  pull / linuxfocalpy3.8gcc7  pull / linuxdocs  pull / linuxfocalpy3.9clang7asan  pull / linuxbionicpy3_8clang8xla Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," looks very good. I guess enabling the 1MHK mode would be even faster for the padding case? Do you know if it is planned? `_scaled_dot_product_efficient_attention_cuda` always passes `c10::nullopt` for `seqstart_q` & `seqstart_k`, thus I believe the full sequence is always used. Maybe 1MHK can be enabled with nestedtensor? Though this commit https://github.com/facebookresearch/xformers/commit/cfea89fab5e42a99f961d445ac2b06d4b43126cediff6ffc547e102d4e367e017498c0697c9be0b1901c9f940fef64a2a5c49b613c0b seem to hint that flash v2 is always faster than the other xformers paths.",">  looks very good. I guess enabling the 1MHK mode would be even faster for the padding case? Do you know if it is planned? `_scaled_dot_product_efficient_attention_cuda` always passes `c10::nullopt` for `seqstart_q` & `seqstart_k`, thus I believe the full sequence is always used. >  > Maybe 1MHK can be enabled with nestedtensor? Though this commit facebookresearch/xformersdiff6ffc547e102d4e367e017498c0697c9be0b1901c9f940fef64a2a5c49b613c0b seem to hint that flash v2 is always faster than the other xformers paths. Yup, thats exactly what we are going to look into"
425,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(WIP - Do not merge - Revert ""Specialize storage_offset - Does not cover automatic dynamic â€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦( CC(Specialize storage_offset  Does not cover automatic dynamic))"" This reverts commit 803c14490b189f9b755ecb9f2a969876088ea243. Fixes ISSUE_NUMBER )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"WIP - Do not merge - Revert ""Specialize storage_offset - Does not cover automatic dynamic â€¦","â€¦( CC(Specialize storage_offset  Does not cover automatic dynamic))"" This reverts commit 803c14490b189f9b755ecb9f2a969876088ea243. Fixes ISSUE_NUMBER ",2023-06-27T14:57:46Z,ciflow/trunk module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/104261," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
2108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([proposal] ""Name"" string attribute for modules, parameters, buffers, tensors for more pleasant debugging (especially for graph printouts / export / studying compiled generated code))ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch This is useful for debugging complex treestructured models to be able to `.name` and understand where a module is found within the whole model tree This idea is currently used only for parameters within `state_dict()` serialization. I suggest that enabling `.name` attribute/property would be useful for debugging and various formatting/debugprinting in the general context. Such `.name` could be used in `__str__` implementations of modules and tensors, could be used for creating easiervisualizable ONNX graphs. Access to module name might also be useful for various finergrained logic hacks in module hooks (although maybe not the best coding practice in all cases  but for hacks might be okay). I propose to: 1. introduce a `.name` property (backed by `._name` attribute which may not exist if we wrongly torch.load and old model file) or just a `.name` attribute if the deserializingoldmoduleobjectswithoutsomeattributes is not a problem. It should be an empty string by default. 2. introduce an instance `.set_names_recursively()` (modulo bikeshedding) method on `torch.nn.Module` which would go around and produce names similar to what's now found in `state_dict()` keys formatting I also propose to support setting/getting such attribute for any tensor. It's understood that most tensors won't have it assigned and the user would need to set it manually for any usefulness, but it's still good (e.g. can be set for tensorstobereturnedfromafunction). Propagation of such tensor names is a more complex task, and I propose that it's out of scope as the feature is already useful if only manual tensor names are supported (for the cases useful for debugging). Alternatively a `tensor.name()` / `tensor)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"[proposal] ""Name"" string attribute for modules, parameters, buffers, tensors for more pleasant debugging (especially for graph printouts / export / studying compiled generated code)"," ğŸš€ The feature, motivation and pitch This is useful for debugging complex treestructured models to be able to `.name` and understand where a module is found within the whole model tree This idea is currently used only for parameters within `state_dict()` serialization. I suggest that enabling `.name` attribute/property would be useful for debugging and various formatting/debugprinting in the general context. Such `.name` could be used in `__str__` implementations of modules and tensors, could be used for creating easiervisualizable ONNX graphs. Access to module name might also be useful for various finergrained logic hacks in module hooks (although maybe not the best coding practice in all cases  but for hacks might be okay). I propose to: 1. introduce a `.name` property (backed by `._name` attribute which may not exist if we wrongly torch.load and old model file) or just a `.name` attribute if the deserializingoldmoduleobjectswithoutsomeattributes is not a problem. It should be an empty string by default. 2. introduce an instance `.set_names_recursively()` (modulo bikeshedding) method on `torch.nn.Module` which would go around and produce names similar to what's now found in `state_dict()` keys formatting I also propose to support setting/getting such attribute for any tensor. It's understood that most tensors won't have it assigned and the user would need to set it manually for any usefulness, but it's still good (e.g. can be set for tensorstobereturnedfromafunction). Propagation of such tensor names is a more complex task, and I propose that it's out of scope as the feature is already useful if only manual tensor names are supported (for the cases useful for debugging). Alternatively a `tensor.name()` / `tensor",2023-06-27T10:29:24Z,feature triaged needs design,open,0,11,https://github.com/pytorch/pytorch/issues/104247,These module names could also be useful for autoannotation of NVTX ranges for profiling and for making IR/FX code printouts more semantic and easier to follow (so maybe not only eager mode would benefit), btw names (when they exist) might be useful for the FLOP counter for better presentation :) https://github.com/pytorch/pytorch/pull/95751,"from Alban: this can all theoretically be down out of tree. Since these features seem useful but experimental, it probably makes sense for them to live in a 3rd party lib (with the possibility of eventually being upstreamed if the community finds them useful).","I would say that most people just roll their own `.name` attributes like this when having to do complex debugging (e.g. having access to a module name in hook can also be useful) / profiling as setting an attribute recursively is quite simple. And for graph export / flop counting, this feature should probably be incore. Having it in a noncore package has no advantages compared to ""rollrecursivefunctionyourself"" IMO :)",One problem that would need to be resolved in core is what to do if a tensor shows up in multiple places in the module hierarchy. There is not a unique name to assign in that case.,"I think that it was recommended even not to reuse ReLU/stateless module instances for this reason?  to not confuse various rewrite techniques / graph / dataflow analysis and hooks ğŸ˜€  So I think this is not the problem, the name in this case can be underscoreconcatenated for instance (could also append some global integer module id like onnx export does if any name clashes or just add some suffix indicating that the module is used in several places). Since this is only for various debugging / exports / nvtx range label gen and would require explicit manual optin to assign names, the user would actually see sth is strange and actually might refactor this. Also, by default I would maybe only assign `.name` of Parameters/Buffers when called set_names_recursively  and not of regular tensors. Tensor names can be assigned as output of some ops, but I would say that by default this can be switched off if it causes some more overhead. Could be turned on with some context manager, but even without this being able to assign a name to tensor from the user code somewhere and then have this name reappear in tensor `__str__` is super cool",A little bit related:  CC([ONNX] FX produce valid node names in models),"Having a single code to assign names would simplify all sorts of vis/export utils (including onnx export) and make them more legible, e.g. https://github.com/pytorch/pytorch/pull/107018","If `.name` exists, one can roll a very simple NaN detection as a forward module hook (which accepts the module instance as argument). It's possible to do now manually, but I think, it's a great usecase to have a `name` field by default (even if empty string) and an util function to set them recursively (maybe even in torch.nn.util)","For instance, it appears from the docs that anomaly detection mode would print detected stuff only in backward, but sometimes we'd like to get some info logged before backward starts (as code might fail before the backward). With global hook and a name attribute, it can be quite easy to have a similar rudimentary debugging (two modes: with a global hook or with registering permodule hooks): ",Somewhat related on preserving original module `names` during transformations:  https://github.com/pytorch/vision/issues/8054
652,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DTensor][Random Ops] enable inplace dropout for nanoGPT benchmark)ï¼Œ å†…å®¹æ˜¯ (  CC([DTensor][Random Ops] enable inplace dropout for nanoGPT benchmark)  CC([DTensor][Random] Add optional arguments to parallelize_module to better control the RNG state in tensor parallel)  CC([DTensor][TP][Random] Introduce TensorParallelRNGTracker to integrate parallel RNG state with Tensor Parallel)  CC([DTensor][Random] Introduce CudaRNGStateTracker to maintain parallel RNG state for DTensor))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[DTensor][Random Ops] enable inplace dropout for nanoGPT benchmark,  CC([DTensor][Random Ops] enable inplace dropout for nanoGPT benchmark)  CC([DTensor][Random] Add optional arguments to parallelize_module to better control the RNG state in tensor parallel)  CC([DTensor][TP][Random] Introduce TensorParallelRNGTracker to integrate parallel RNG state with Tensor Parallel)  CC([DTensor][Random] Introduce CudaRNGStateTracker to maintain parallel RNG state for DTensor),2023-06-26T22:52:31Z,Stale ciflow/trunk ciflow/periodic release notes: distributed (dtensor),closed,0,1,https://github.com/pytorch/pytorch/issues/104215,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
310,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Specialize storage_offset - Does not cover automatic dynamic)ï¼Œ å†…å®¹æ˜¯ (  CC(Specialize storage_offset  Does not cover automatic dynamic) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Specialize storage_offset - Does not cover automatic dynamic,  CC(Specialize storage_offset  Does not cover automatic dynamic) ,2023-06-26T19:15:31Z,Merged Reverted ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/104204, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hi ,  it seems this PR broke one of the CI jobs https://github.com/pytorch/pytorch/actions/runs/5386259326/jobs/9776853834step:15:3546](https://github.com/pytorch/pytorch/actions/runs/5386259326/jobs/9776853834logs)"")  Can we easily FF this?","> Hi , >  > it seems this PR broke one of the CI jobs https://github.com/pytorch/pytorch/actions/runs/5386259326/jobs/9776853834step:15:3546](https://github.com/pytorch/pytorch/actions/runs/5386259326/jobs/9776853834logs)"") >  > Can we easily FF this? vision mask rcnn is usually unstable. I don't trust this CI signal, because it did not come up on the PR. ",But it did show up on the PR... ," revert c ignoredsignal m ""also due to  CC([torch.compile] Guards failures due to storage offsets in new nightly )""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
411,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([WIP][nanoGPT][DTensor] enable inplace dropout for nanoGPT benchmark)ï¼Œ å†…å®¹æ˜¯ (  CC([WIP][nanoGPT][DTensor] enable inplace dropout for nanoGPT benchmark)  WHY noninplace dropout seems not working with DTensor. need to further investigate.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[WIP][nanoGPT][DTensor] enable inplace dropout for nanoGPT benchmark,  CC([WIP][nanoGPT][DTensor] enable inplace dropout for nanoGPT benchmark)  WHY noninplace dropout seems not working with DTensor. need to further investigate.,2023-06-26T18:12:54Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/104200
1894,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Torchscript with dynamic quantization produces inconsistent model outputs)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I've been experimenting with torchscript and dynamic quantization and often have the issue that results of the models that are dynamically quantized are not consistent between Python and Java. To reproduce the issue I created a fork of the python javademo: https://github.com/westphaljan/javademo. To setup you need to download libtorch and set its the location in `build.gradle` (https://github.com/westphaljan/javademo/blob/master/build.gradleL16) Download Link: https://download.pytorch.org/libtorch/cpu/libtorchsharedwithdeps1.13.1%2Bcpu.zip I created a simple dummy model with one linear layer and export it unquantized and quantized here: https://github.com/westphaljan/javademo/blob/master/create_dummy_models.py (The code can also be run using the dependencies defined in https://github.com/westphaljan/javademo/blob/master/requirements.txt but I also commited the dummy models) Python:  You can run the java code with `./gradlew run`. Java:  As you can see the output of the unquantized model is perfectly consistent while the output of the dynamically quantized model is slightly inconsistent. It might seem insignificant but with larger models like a transformer it becomes more obvious (differences usually already in the first decimal place). Am I misunderstanding something conceptually? I thought as the code is compiled down to C it should be the same even when using dynamic quantization. Note: I made sure that Python and Java use the same version of Torch `1.13.1` which is the latest published mvn version (https://mvnrepository.com/artifact/org.pytorch/pytorch_java_only)  Versions 1.13.1 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Torchscript with dynamic quantization produces inconsistent model outputs," ğŸ› Describe the bug Hello, I've been experimenting with torchscript and dynamic quantization and often have the issue that results of the models that are dynamically quantized are not consistent between Python and Java. To reproduce the issue I created a fork of the python javademo: https://github.com/westphaljan/javademo. To setup you need to download libtorch and set its the location in `build.gradle` (https://github.com/westphaljan/javademo/blob/master/build.gradleL16) Download Link: https://download.pytorch.org/libtorch/cpu/libtorchsharedwithdeps1.13.1%2Bcpu.zip I created a simple dummy model with one linear layer and export it unquantized and quantized here: https://github.com/westphaljan/javademo/blob/master/create_dummy_models.py (The code can also be run using the dependencies defined in https://github.com/westphaljan/javademo/blob/master/requirements.txt but I also commited the dummy models) Python:  You can run the java code with `./gradlew run`. Java:  As you can see the output of the unquantized model is perfectly consistent while the output of the dynamically quantized model is slightly inconsistent. It might seem insignificant but with larger models like a transformer it becomes more obvious (differences usually already in the first decimal place). Am I misunderstanding something conceptually? I thought as the code is compiled down to C it should be the same even when using dynamic quantization. Note: I made sure that Python and Java use the same version of Torch `1.13.1` which is the latest published mvn version (https://mvnrepository.com/artifact/org.pytorch/pytorch_java_only)  Versions 1.13.1 ",2023-06-26T16:52:51Z,oncall: jit oncall: quantization low priority triaged,open,1,4,https://github.com/pytorch/pytorch/issues/104195,"the underlying operators for static and dynamic quantization (and the corresponding kernels) are different so there might be difference in the numerics when you compare static and dynamic quantization. Moreover in dynamic quantization, the activations are quantized on the fly which can also result in different numerics compared to static.  https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.htmlwhatisdynamicquantization might help. Closing this issue, since I don't think its a bug. You can also use the forums https://discuss.pytorch.org/c/quantization/17 for similar questions in the future.","Unfortunately, I can't access the link https://fb.workplace.com/work/knowledge/4359408054107776 as it seems to be an internal Facebook thread. Also I'm not sure why you are comparing static and dynamic quantization. In the scenario above I use the same scripted and dynamically quantized model on the same archtitecture (CPU, x86_64) only executing it once in Python and once in Java. From my understanding the same kernels should be used and dynamic quantization should also be deterministic even though the activations are quantized on the fly. Can you confirm that my assumption is correct and elaborate a little bit more on your comment?","> Unfortunately, I can't access the link https://fb.workplace.com/work/knowledge/4359408054107776  sorry this was a wrong paste, updated the original link. I think I misunderstood the problem originally and understand it better now, thanks for clarifying. One difference could be the representation of data types being different in Python vs Java for the scale factor values. In Python the float dtype is default 64 bits and in Java it is 32 bits.","Thanks. How is it possible that the scale factors have different dtypes? Shouldn't be the same C++ Code used and therefore also have the same dtype regardless of the programming language that uses torchscript? If the dtype however differs in both languages, is it possible to configure this somewhere?"
1141,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.embedding: Trying to convert BFloat16 to the MPS backend but it does not have support for that dtype.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Code to reproduce  results in   Probably here: https://github.com/pytorch/pytorch/blob/b2277075b0fe5cf085d369a313863f64c6fb50c3/aten/src/ATen/native/mps/OperationUtils.mmL30L44 I wasn't able to test this on nightly, because apparently it's been blocked currently: https://github.com/pytorch/pytorch/blob/31f311a816c026bbfca622d6121d6a7fab44260d/aten/src/ATen/mps/EmptyTensor.cppL46 BF16 support is added to the OS version (macOS Sonoma) I use recently, referred from here (with timestamp): https://developer.apple.com/wwdc23/10050?time=590 > Starting with macOS Sonoma, MPSGraph adds support for a new data type, bfloat16. https://developer.apple.com/wwdc23/10050?time=659 > Adding Automatic Mixed Precision support to your network is a very easy process. First, add autocast. Both float16 and bfloat16 are supported.   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.embedding: Trying to convert BFloat16 to the MPS backend but it does not have support for that dtype.," ğŸ› Describe the bug Code to reproduce  results in   Probably here: https://github.com/pytorch/pytorch/blob/b2277075b0fe5cf085d369a313863f64c6fb50c3/aten/src/ATen/native/mps/OperationUtils.mmL30L44 I wasn't able to test this on nightly, because apparently it's been blocked currently: https://github.com/pytorch/pytorch/blob/31f311a816c026bbfca622d6121d6a7fab44260d/aten/src/ATen/mps/EmptyTensor.cppL46 BF16 support is added to the OS version (macOS Sonoma) I use recently, referred from here (with timestamp): https://developer.apple.com/wwdc23/10050?time=590 > Starting with macOS Sonoma, MPSGraph adds support for a new data type, bfloat16. https://developer.apple.com/wwdc23/10050?time=659 > Adding Automatic Mixed Precision support to your network is a very easy process. First, add autocast. Both float16 and bfloat16 are supported.   Versions  ",2023-06-26T14:55:11Z,triaged enhancement module: bfloat16 module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/104191,"bfloat16 support is not yet available in MPS (Sonoma has not been released yet), though will be available eventually","Zhang we have a PR to add the AutoCast https://github.com/pytorch/pytorch/pull/99272 , which will add BFloat16 support as this one merges.","Closing this one, as BFloat support has been added to MPS in https://github.com/pytorch/pytorch/pull/119641"
2014,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP Reshard-only Post-Backward Multi-Grad Hooks Need to be Conditionally Re-Registered)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug In order to support common finetuning patterns wherein a subset of model parameters are thawed after `n` finetuning steps, FSDP reshardonly postbackward multigrad hooks need to be conditionally reregistered as normal postbackward hooks (and attached to `AccumulateGrad`). Absent the proposed changes below (or a functionally similar implementation), thawing model parameters after `n` finetuning steps triggers a `SystemError` on `run_backward`:   To reproduce Update the existing `test_fsdp_fine_tune.py::TestFSDPFineTune::test_backward_reshard_hooks` test as follows:    Potential resolution While I will be away the next couple weeks and so unfortunately won't be able to submit/collaborate on any PR associated with this issue in that time, one possible fix is to patch `_register_post_backward_hooks` as follows:  After applying that fix, the extended test above should pass again. I opened this issue as a bug given the prevalence of the aforementioned finetuning pattern but feel free to relabel it as an enhancement request if you think that is a more appropriate classification. Thanks again for all your continued contributions, the PyTorch Distributed team is awesome!! :rocket: :tada:  varma  Versions PyTorch version: 2.1.0a0+git3a823e4 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0152genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING se)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,FSDP Reshard-only Post-Backward Multi-Grad Hooks Need to be Conditionally Re-Registered," ğŸ› Describe the bug In order to support common finetuning patterns wherein a subset of model parameters are thawed after `n` finetuning steps, FSDP reshardonly postbackward multigrad hooks need to be conditionally reregistered as normal postbackward hooks (and attached to `AccumulateGrad`). Absent the proposed changes below (or a functionally similar implementation), thawing model parameters after `n` finetuning steps triggers a `SystemError` on `run_backward`:   To reproduce Update the existing `test_fsdp_fine_tune.py::TestFSDPFineTune::test_backward_reshard_hooks` test as follows:    Potential resolution While I will be away the next couple weeks and so unfortunately won't be able to submit/collaborate on any PR associated with this issue in that time, one possible fix is to patch `_register_post_backward_hooks` as follows:  After applying that fix, the extended test above should pass again. I opened this issue as a bug given the prevalence of the aforementioned finetuning pattern but feel free to relabel it as an enhancement request if you think that is a more appropriate classification. Thanks again for all your continued contributions, the PyTorch Distributed team is awesome!! :rocket: :tada:  varma  Versions PyTorch version: 2.1.0a0+git3a823e4 Is debug build: False CUDA used to build PyTorch: 12.1 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.4 Libc version: glibc2.31 Python version: 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0152genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING se",2023-06-24T21:10:23Z,triaged module: fsdp,closed,0,0,https://github.com/pytorch/pytorch/issues/104148
1985,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(PyTorch2.0 ROCM LayerNorm HIP error: invalid configuration)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am trying to  inference a protein structure graph transformer model using PyTorch2 with rocm5.4.3/5.5.1 on MI210 GPUs.  When I scale the batch size pass 250 per gpu I get the error below. The error does not exist when we use PyTorch 1.13 and we can scale to 1500 batch size per gpu on the MI210s. However, It works with PyTorch2 on our A100 GPUs. So it seems to be specific to PyTorch2.0 on ROCM.  I have begun working with  on the issue. He can discuss more about the current state of diagnosing and debugging the issue.    Versions Collecting environment information... /scratch/cluster/danny305/miniconda3/envs/amd/lib/python3.8/sitepackages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML   warnings.warn(""Can't initialize NVML"") PyTorch version: 2.0.1+rocm5.4.2 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22803474e8620 OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.071genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Instinct MI210 Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22804 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):       )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,PyTorch2.0 ROCM LayerNorm HIP error: invalid configuration," ğŸ› Describe the bug I am trying to  inference a protein structure graph transformer model using PyTorch2 with rocm5.4.3/5.5.1 on MI210 GPUs.  When I scale the batch size pass 250 per gpu I get the error below. The error does not exist when we use PyTorch 1.13 and we can scale to 1500 batch size per gpu on the MI210s. However, It works with PyTorch2 on our A100 GPUs. So it seems to be specific to PyTorch2.0 on ROCM.  I have begun working with  on the issue. He can discuss more about the current state of diagnosing and debugging the issue.    Versions Collecting environment information... /scratch/cluster/danny305/miniconda3/envs/amd/lib/python3.8/sitepackages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML   warnings.warn(""Can't initialize NVML"") PyTorch version: 2.0.1+rocm5.4.2 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22803474e8620 OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.071genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Instinct MI210 Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22804 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):       ",2023-06-24T18:55:11Z,module: rocm triaged,open,0,6,https://github.com/pytorch/pytorch/issues/104147, Verbose log showing the failing hipLaunchKernel call with hipErrorInvalidConfiguration  amd , would you be able to try our latest nightly wheel that has rocm 5.5? `pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/rocm5.5`,Just tried the nightly build:  torch                        2.1.0.dev20230627+rocm5.5  torchaudio                   2.1.0.dev20230627+rocm5.5  torchvision                  0.16.0.dev20230627+rocm5.5 Same error: ,  amd  Is there a fix available to try? Should we try the ROCm 5.6 pytorch 2.0 wheel from `https://repo.radeon.com/rocm/manylinux/rocmrel5.6/` here? CC:   ,"Yes, please try the ROCm5.6 wheels, since this looks like something that could have regressed in HIP/ROCm, assuming the layernorm kernels are launched with the same config in PyTorch 1.13 vs 2.0.  Would you be able to provide a simple reduced testcase which just calls the layernorm op with the same inputs that you provide in your app? I expect it should fail even in a standalone testcase with the same HIP configuration error. That would really help us dig into this faster.", are you able to create a smaller reproducer?  Or provide us with the input shapes that were used with layernorm so we can try to reproduce?
1968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP SHARDED_STATE_DICT Runs into CPU OOM)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Running FSDP on PT 2.0.1 for a 7B model during the checkpointing using SHARDED_STATE_DICT it runs into CUDA OOM. A side note, I am using PEFT LORA on top of FSDP although this is not expected to impact the checkpointing part.  on 4 A10 gpus memory stats during training are: reserved memory is 16.7559 GB and allocated memory is 3.4788GB **Error Logs** ```bash Training Epoch{epoch}:   0% (main, May 10 2023, 18:58:44) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.01033awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 525.85.12 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          48 Online CPU(s) list:             047 Thread(s) per core:              2 Core(s) per socket:              24 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       AuthenticAMD CPU family:                      23 Model:                           49 Model name:                      AMD EPYC 7R32 Stepping:                        0 CPU MHz:                         2799.888 BogoMIPS:                        5599.77 Hypervisor vendor:               KVM Virtualization type:             full L1d cache:                       768 KiB L1i cache:                       768 KiB L2 cache:              )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",peft,FSDP SHARDED_STATE_DICT Runs into CPU OOM," ğŸ› Describe the bug Running FSDP on PT 2.0.1 for a 7B model during the checkpointing using SHARDED_STATE_DICT it runs into CUDA OOM. A side note, I am using PEFT LORA on top of FSDP although this is not expected to impact the checkpointing part.  on 4 A10 gpus memory stats during training are: reserved memory is 16.7559 GB and allocated memory is 3.4788GB **Error Logs** ```bash Training Epoch{epoch}:   0% (main, May 10 2023, 18:58:44) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.01033awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 525.85.12 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          48 Online CPU(s) list:             047 Thread(s) per core:              2 Core(s) per socket:              24 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       AuthenticAMD CPU family:                      23 Model:                           49 Model name:                      AMD EPYC 7R32 Stepping:                        0 CPU MHz:                         2799.888 BogoMIPS:                        5599.77 Hypervisor vendor:               KVM Virtualization type:             full L1d cache:                       768 KiB L1i cache:                       768 KiB L2 cache:              ",2023-06-24T15:17:56Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/104143
2017,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Is memory-efficient FSDP initialization intended to be possible with torch.device('meta')?)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Looking at the FSDP API, I was expecting that we could use the torch meta device in conjunction with the `param_init_fn` and `sync_module_states` arguments to `FSDP` to instantiate a large model in a memoryefficient manner. Specifically, I was imagining a workflow like:  Only loading model parameters on the rank 0 device (load on the meta device on the others)  Use a param_init_fn like module.to_empty(device=f'cuda:{rank}') when creating the FSDP module on nonrank 0 devices  Use sync_module_states=True when creating the FSDP instance to sync parameter values with rank 0 For example:     import contextlib     import transformers     import functools     import torch     from torch.distributed.fsdp import FullyShardedDataParallel as FSDP     init_context = contextlib.nullcontext() if local_rank == 0 else torch.device('meta')     with init_context:         model = transformers.AutoModelForCausalLM.from_pretrained(             'EleutherAI/pythia160m', low_cpu_mem_usage=True)     if local_rank == 0:         print(model)     model_auto_wrap_policy = functools.partial(transformer_auto_wrap_policy, transformer_layer_cls={model.gpt_neox.layers[0].__class__},)     model = FSDP(model,                  auto_wrap_policy=model_auto_wrap_policy,                  sharding_strategy=ShardingStrategy.FULL_SHARD,                  cpu_offload=CPUOffload(offload_params=False),                  backward_prefetch=BackwardPrefetch.BACKWARD_PRE,                  device_id=local_rank,                  sync_module_states=True,                  param_init_fn=lambda mod: mod.to_empty(device=f'cuda:{local_rank}')) But this code fails on the `FSDP()` line. It seems the model sharding happens before modules on the meta device )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Is memory-efficient FSDP initialization intended to be possible with torch.device('meta')?," ğŸ› Describe the bug Looking at the FSDP API, I was expecting that we could use the torch meta device in conjunction with the `param_init_fn` and `sync_module_states` arguments to `FSDP` to instantiate a large model in a memoryefficient manner. Specifically, I was imagining a workflow like:  Only loading model parameters on the rank 0 device (load on the meta device on the others)  Use a param_init_fn like module.to_empty(device=f'cuda:{rank}') when creating the FSDP module on nonrank 0 devices  Use sync_module_states=True when creating the FSDP instance to sync parameter values with rank 0 For example:     import contextlib     import transformers     import functools     import torch     from torch.distributed.fsdp import FullyShardedDataParallel as FSDP     init_context = contextlib.nullcontext() if local_rank == 0 else torch.device('meta')     with init_context:         model = transformers.AutoModelForCausalLM.from_pretrained(             'EleutherAI/pythia160m', low_cpu_mem_usage=True)     if local_rank == 0:         print(model)     model_auto_wrap_policy = functools.partial(transformer_auto_wrap_policy, transformer_layer_cls={model.gpt_neox.layers[0].__class__},)     model = FSDP(model,                  auto_wrap_policy=model_auto_wrap_policy,                  sharding_strategy=ShardingStrategy.FULL_SHARD,                  cpu_offload=CPUOffload(offload_params=False),                  backward_prefetch=BackwardPrefetch.BACKWARD_PRE,                  device_id=local_rank,                  sync_module_states=True,                  param_init_fn=lambda mod: mod.to_empty(device=f'cuda:{local_rank}')) But this code fails on the `FSDP()` line. It seems the model sharding happens before modules on the meta device ",2023-06-22T06:37:44Z,triaged module: fsdp,open,0,14,https://github.com/pytorch/pytorch/issues/104026,Just checking in  to see if you had any thoughts on this :),"Why do you only use the metadevice context for rank 0? I recently landed a PR https://github.com/pytorch/pytorch/pull/104189 that should help work memoryefficient FSDP initialization (correctly). My understanding is that you can do the following:  Construct the model on meta device for all ranks.  Define a `param_init_fn` that (1) calls `module.to_empty(device=torch.device(""cuda""), recurse=False)` and (2) conditions on the `nn.Module` class and initializes its parameters/buffers using the desired initialization function (e.g. `torch.nn.init.xavier_uniform_()`)  FSDP will call this `param_init_fn` on all FSDPmanaged parameters exactly once as it proceeds through the auto wrapping.  Pass that `param_init_fn` to the FSDP constructor. `sync_module_states=True` should work as well. For more context, the order of operations in FSDP should be like: (1) allocate uninitialized GPU memory for an FSDP module via `to_empty(recurse=False)`; (2) initialize those values using the `param_init_fn`; (3) sync the values via broadcast from rank 0; (4) construct FSDP's flat parameter and shard it. This process proceeds bottomup through the module tree. Let me know if anything is unclear or if you hit any issues!","Thanks for this ! Those steps totally make sense for training a model from scratch. However, I'm interested in the usecase of finetuning a pretrained model, i.e., loaded with `from_pretrained`. That's why I load the model on the meta device for any rank that's *not* 0. On rank 0, I actually load the full pretrained model parameters (into CPU RAM), using `nullcontext()` instead of the torch meta device context manager. This way, I was hoping to only load one copy of the pretrained parameters (on rank 0), and then use `to_empty()` to allocate uninitialized GPU memory on other ranks, skip initializing them, and then sync the parameter values by broadcasting the pretrained values loaded on rank 0. Is my logic for how this process should work going wrong somewhere?","Ah, my bad! I misread/misunderstood. I may need to replicate your environment to look into this further. Before that though, could you share the stack trace of the error (when you say that it errored in the `FSDP()` line)? I think your thinking should be correct. I would expect to do something like the following to work:  On rank 0, `param_init_fn=None` (but should not matter since rank 0 should not have any metadevice parameters)  On nonzero ranks, `param_init_fn` calls `module.to_empty(device=torch.device(""cuda""), recurse=False)`  On all ranks, `sync_module_states=True` It may be possible that https://github.com/pytorch/pytorch/pull/104189 helps fix your issue because I believe it was landed after this issue was opened.","In this method, all non rank 0 devices need to wait for rank 0 to load the checkpoint then broadcast. How can we initialize big models faster by letting all the ranks to load their part of the checkpoint?"," Thanks for your response, and sorry for being so slow on this! I'm hoping tomorrow I'll get a chance to try to reproduce the issue after https://github.com/pytorch/pytorch/pull/104189 and share a full stack trace if it's still showing up.  That would be ideal, though I assume you'll need to preshard your model checkpoint and load only the correct shard. Are you implying with your comment that you've gotten this rank0only initialization method to work correctly, just without the checkpoint sharding you've described?",Yes. It's working. , thanks for sharing this code sample! What version of torch/CUDA are you using? I'm not finding torchdistx to be compatible with recent version of PyTorch (I assume that's how `deferred_init` and `materialize_module` are defined).," I've cleaned up my example to make it selfcontained and rerun from nightly (`2.1.0.dev20230717+cu118`). Depending on how I create the meta tensor models, FSDP either crashes or hangs on the first forward pass (`model(**batch).loss`). See full code below. The error is  Here is my best attempt at a minimal repro: ","The only approach that has worked for me is to create the model on CPU **before** the call to `torch.multiprocessing.spawn` and pass the model as an argument to `spawn()`. This approach seems to leverage the fact that PyTorch tensors will automatically move into shared memory when used with multiprocessing. One note is that I need to call `ulimit n 64000` to raise the open file limit in some cases, or else the multiprocessing call fails with `too many open files`. Also I'm not sure how to get this to work with `torch.distributed.run`, since it requires passing a custom arg (the model) to the `mp.spawn` call. This approach actually seems pretty elegant, as it doesn't require any FSDPrelated changes, but I'm unsure if it's the ""right"" way to do things (or if it is even intended behavior).","mitchell, would you be willing to share your example that uses `torch.multiprocessing.spawn`? I bumped up against the same issue tonight: https://github.com/pytorch/examples/issues/1179.", check out this repo: https://github.com/ericmitchell/directpreferenceoptimization/blob/main/train.pyL111 The trainer class takes the sharedmemory model and creates an FSDP instance (once in each worker process).,Just following up curious if you have any thoughts on this !,The same issue here!
587,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('MPS' Issue Running HuggingFace Transformer Pix2Struct Model)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am running the transformer model Pix2StructForConditionalGeneration, Pix2StructProcessor on MacOS 13.4 on an iMac 27"" 2020 with an AMD Radeon Pro 5700XT. The code to run the Transformer is here: https://huggingface.co/google/pix2structai2dbase The code runs with 'CPU' but with 'MPS' gets:   Here's the code:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,'MPS' Issue Running HuggingFace Transformer Pix2Struct Model," ğŸ› Describe the bug I am running the transformer model Pix2StructForConditionalGeneration, Pix2StructProcessor on MacOS 13.4 on an iMac 27"" 2020 with an AMD Radeon Pro 5700XT. The code to run the Transformer is here: https://huggingface.co/google/pix2structai2dbase The code runs with 'CPU' but with 'MPS' gets:   Here's the code:   Versions  ",2023-06-21T15:32:08Z,triaged module: mps,open,0,1,https://github.com/pytorch/pytorch/issues/103966,Same error with: 2.0.0.dev20230211 
774,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to unwrap after auto_wrap in FSDP?)ï¼Œ å†…å®¹æ˜¯ (I am currently finetuning a LLM (LLaMA) and would like to retrieve the gradients of each weight (parameter) after every gradient update. However, I notice that weights are (auto) wrapped into stuff like â€œ_fsdp_wrapped_module._flat_paramâ€ during training. I need to map these wrapped weights to the original LLaMA architecture such as â€œself_attn.v_projâ€. Any code examples? I guess â€œsummon_full_params()â€ might be the function that I look for, but I am not sure if that is correct. I also have difficulty using this function. Thanks a lot for any help! )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,How to unwrap after auto_wrap in FSDP?,"I am currently finetuning a LLM (LLaMA) and would like to retrieve the gradients of each weight (parameter) after every gradient update. However, I notice that weights are (auto) wrapped into stuff like â€œ_fsdp_wrapped_module._flat_paramâ€ during training. I need to map these wrapped weights to the original LLaMA architecture such as â€œself_attn.v_projâ€. Any code examples? I guess â€œsummon_full_params()â€ might be the function that I look for, but I am not sure if that is correct. I also have difficulty using this function. Thanks a lot for any help! ",2023-06-21T11:27:10Z,oncall: distributed triaged module: fsdp,open,0,3,https://github.com/pytorch/pytorch/issues/103962,"Unfortunately, we do not have great support for gathering the gradients. I added an option like `summon_full_params(model, offload_to_cpu=False, with_grads=True)` (note the `offload_to_cpu=False`), which requires passing `use_orig_params=True` to the FSDP constructor. If you have significant memory pressure, then the `offload_to_cpu=False` may be problematic. In that case, you can also try `summon_full_params(..., recurse=False)` to gather the gradients layer by layer.",Did you find a good way to accomplish this? Stuck in a similar situation. ,> Did you find a good way to accomplish this? Stuck in a similar situation. We are working on some new things. It would be great if you could tell me more about your use case. Thanks!
1119,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_gpt2_tiny (__main__.TestFxToOnnxWithOnnxRuntime_op_level_debug_True_dynamic_shapes_False))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_gpt2_tiny` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `onnx/test_fx_to_onnx_with_onnxruntime.py`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,DISABLED test_gpt2_tiny (__main__.TestFxToOnnxWithOnnxRuntime_op_level_debug_True_dynamic_shapes_False),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_gpt2_tiny` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `onnx/test_fx_to_onnx_with_onnxruntime.py`",2023-06-21T06:40:20Z,module: onnx triaged module: flaky-tests skipped,closed,0,4,https://github.com/pytorch/pytorch/issues/103950, do you have insights how to fetch the flaky failure logs? Looking through recent examples it shows logs from May which clearly isn't recent failures? > 20230510T00:02:49.7560062Z FAILED [2.7391s],"I'm not sure how the recent example link doesn't work, but the link to the most recent flaky trunk job looks right to me https://github.com/pytorch/pytorch/runs/14423285784.  In this example, I can see that this test `onnx/test_fx_to_onnx_with_onnxruntime.py::TestFxToOnnxWithOnnxRuntime_op_level_debug_True_dynamic_shapes_False::test_gpt2_tiny` succeeded after 2 retries:  20230621T01:57:54.8706271Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/transformers/models/gpt2/modeling_gpt2.py"", line 832, in forward 20230621T01:57:54.8706371Z     inputs_embeds = self.wte(input_ids) 20230621T01:57:54.8706378Z  20230621T01:57:54.8706440Z  20230621T01:57:54.8706753Z Traceback (most recent call last): 20230621T01:57:54.8706759Z  20230621T01:57:54.8707092Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/onnx/_internal/fx/op_validation.py"", line 58, in validate_op_between_ort_torch 20230621T01:57:54.8707258Z     expected_outputs = node.target(*torch_args, **torch_kwargs)   type: ignore[operator] 20230621T01:57:54.8707264Z  20230621T01:57:54.8707514Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_ops.py"", line 429, in __call__ 20230621T01:57:54.8707622Z     return self._op(*args, **kwargs or {}) 20230621T01:57:54.8707628Z  20230621T01:57:54.8707729Z IndexError: index out of range in self 20230621T01:57:54.8707734Z  20230621T01:57:54.8707795Z  20230621T01:57:54.8710710Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/transformers/models/gpt2/modeling_gpt2.py"", line 833, in forward 20230621T01:57:54.8710826Z     position_embeds = self.wpe(position_ids) 20230621T01:57:54.8710832Z  20230621T01:57:54.8710891Z  20230621T01:57:54.8711197Z Traceback (most recent call last): 20230621T01:57:54.8711203Z  20230621T01:57:54.8711531Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/onnx/_internal/fx/op_validation.py"", line 58, in validate_op_between_ort_torch 20230621T01:57:54.8711748Z     expected_outputs = node.target(*torch_args, **torch_kwargs)   type: ignore[operator] 20230621T01:57:54.8711753Z  20230621T01:57:54.8712002Z   File ""/opt/conda/envs/py_3.8/lib/python3.8/sitepackages/torch/_ops.py"", line 429, in __call__ 20230621T01:57:54.8712108Z     return self._op(*args, **kwargs or {}) 20230621T01:57:54.8712114Z  20230621T01:57:54.8712213Z IndexError: index out of range in self 20230621T01:57:54.8712219Z  20230621T01:57:54.8712279Z   Any thoughts on the meaning of the failure?","Thanks for pointer, looks like two failures were due to connection timeouts loading tokenizer. Any suggestions? Should it be cached beforehand like the model as well? ",Thank you for the fix!  It's indeed the issue here.
612,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reland: Add PyObject preservation for UntypedStorage)ï¼Œ å†…å®¹æ˜¯ (This relands CC(Add PyObject preservation for UntypedStorage) after CC(Back out ""Add PyObject preservation for UntypedStorage (97470)"") reverted it. This PR attempts to fix the internal failure by avoiding an unnecessary intermediate storage buffer allocation in `c10::newStorageImplFromRefcountedDataPtr`. Part of CC(PyObject preservation and resurrection for `StorageImpl`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Reland: Add PyObject preservation for UntypedStorage,"This relands CC(Add PyObject preservation for UntypedStorage) after CC(Back out ""Add PyObject preservation for UntypedStorage (97470)"") reverted it. This PR attempts to fix the internal failure by avoiding an unnecessary intermediate storage buffer allocation in `c10::newStorageImplFromRefcountedDataPtr`. Part of CC(PyObject preservation and resurrection for `StorageImpl`) ",2023-06-20T18:45:52Z,module: internals triaged open source Merged Reverted ciflow/trunk release notes: python_frontend ciflow/periodic,closed,0,36,https://github.com/pytorch/pytorch/issues/103907,"CI failures are upstream. , could we have someone run this with the model that produced the OOM error?"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",I wouldn't expect this to fix the other error you mentioned here: https://github.com/pytorch/pytorch/pull/102553issuecomment1585436879 Let me know if that one comes up again. I might need some more info about what makes it happen,I think we also need to come up with a solution for that error too,", do you know if this successfully fixed the OOM error or not?","no, I'm still trying to get people to give me access to rerun the flows. IF they still don't respond to me, try your best to fix the other issue, and I will just reland this and test it in prod.","Ok sounds good. I still haven't figured out the other issue. According to that trace you sent me, there is some situation where `tensor.untyped_storage()` can return a tuple, which is what causes the error. I haven't figured out why though",", I've applied my proposed fix for the HermeticPyObjectTLS issue that caused the Metainternal model to fail. I also included a test case for it which previously failed before the fix. I'd like to know what you think about it and confirm if it actually fixes the internal failure"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",Just want to confirm that https://github.com/pytorch/pytorch/pull/103907/commits/bcb33c8a951a995f4f0cb75d27c810f93240e17e are the critical changes ,"One thing I notice, looking at this PR, is we don't fix the equivalent potential problem on Tensor. I'm not sure I want to pay another field on THPVariable just to track if something was allocated hermetically or not, but it would be nice to add enough debugging info one way or another to make it easy to tell if the Tensor situation has occurred (I think hypothetically it could happen in some rare cases, e.g., you have an operator that clears the grad field on a tensor). Looking at THPVariable   it's already got some random goop in it, maybe one more field isn't so bad.",> Just want to confirm that bcb33c8 are the critical changes Yep that's right,Confirmed this passes the internal model,"just waiting on any last changes you want to add, or i can start landing immediately",I'm about to push a few changes shortly,Ok those are all the changes I wanted to add, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Meta InternalOnly Changes Check Details for Dev Infra team Raised by workflow job ", merge i, Merge started Your change will be merged while ignoring the following 1 checks: Meta InternalOnly Changes Check Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 2, 3, macosm112) Details for Dev Infra team Raised by workflow job ","Looks like one of the checks I added to Tensor made a unit test fail. `THPVariable_Check` fails if the object is not a subclass of `torch.Tensor`, and the failing unit test is defining its own subclass of `torch._C._TensorBase`, so the check fails. For now I'll just remove the check", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: Meta InternalOnly Changes Check Details for Dev Infra team Raised by workflow job ", merge i," Merge started Your change will be merged while ignoring the following 3 checks: pull / linuxjammypy3.8gcc11 / test (docs_test, 1, 1, linux.2xlarge), pull / linuxdocs / builddocspythonfalse, Meta InternalOnly Changes Check Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: 1 jobs have failed, first few of them are: periodic / linuxfocalcuda11.8py3.9gcc9 / test (multigpu, 1, 1, linux.g5.12xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "
1593,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Please consider the SCFA/dynamic flash attention for your implementation of scaled dot product attention)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Sparse Causal Flash Attention as implmented here and described in this paper claims to  > extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashingbased attention. This leads to implementations with no computational complexity overhead and a multifold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2.0Ã— and 3.3Ã— for sequences of respectively 8k and 16k tokens. Adding these implementations to main Pytorch might lead to Transformers being able to use both flash attention, PEFT/LoRA and methods for handling long contexts  Alternatives 1. Leaving everything as is and finalising the current implementation to allow handling attention masks (see issue CC(Add support for ALiBi/relative positional biases to the fast path for Transformers) ) 2. Relying on flash_attn by HazyResearch, which works great on its own but apparently does not work with LoRA  Additional context I'm just not sure functions written in triton are readily convertible into C++ functions. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Please consider the SCFA/dynamic flash attention for your implementation of scaled dot product attention," ğŸš€ The feature, motivation and pitch Sparse Causal Flash Attention as implmented here and described in this paper claims to  > extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashingbased attention. This leads to implementations with no computational complexity overhead and a multifold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2.0Ã— and 3.3Ã— for sequences of respectively 8k and 16k tokens. Adding these implementations to main Pytorch might lead to Transformers being able to use both flash attention, PEFT/LoRA and methods for handling long contexts  Alternatives 1. Leaving everything as is and finalising the current implementation to allow handling attention masks (see issue CC(Add support for ALiBi/relative positional biases to the fast path for Transformers) ) 2. Relying on flash_attn by HazyResearch, which works great on its own but apparently does not work with LoRA  Additional context I'm just not sure functions written in triton are readily convertible into C++ functions. ",2023-06-19T11:06:27Z,module: sparse triaged topic: new features module: sdpa,open,0,1,https://github.com/pytorch/pytorch/issues/103836,The barrier to entry for bringing in implementations to PyTorch is high. Since this paper has only come out on June 1 it does not meet this barrier. There are two options. 1.) Luckily since the code is written in Triton it should be usable and hackable for end users. 2.) Write an implementation of this kernel using exisitng PyTorch ops and then use torch.compile.  I will leave this issue open and if more people request this feature then we can revisit
237,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Doc] Fix torch.UntypedStorage.mps() doc)ï¼Œ å†…å®¹æ˜¯ (Fix doc typo.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Doc] Fix torch.UntypedStorage.mps() doc,Fix doc typo.,2023-06-17T09:15:22Z,open source Merged ciflow/trunk topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/103797,The committers listed above are authorized under a signed CLA.:white_check_mark: login: LamForest / name: GeT_Left  (a78864bc5f9df8ad3cc8d1f94197f0615ff472d5)," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""Docstring fix only""","The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot."," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1023,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SDPA produces NaN with padding mask)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm implementing padding support directly on my LLM model. To do so, I add extra rows to the boolean attention mask with all `False` values. However, calling `torch.nn.functional.scaled_dot_product_attention` returns NaNs. An equivalent and naive implementation also does (see `naive_fsdpa_1`). But if I use `torch.finfo(att.dtype).min` as the masked fill value, the result is correct (see `naive_fsdpa_2`) Here's the repro:  This seems to suggest that there's a precision issue. Is this expected or known? Could `scaled_dot_product_attention` be fixed? Side question: do I lose flash attn if I append the padding mask like this? Thank you for your help. Maybe linked:  CC(scaled_dot_product_attention produces NaN when input has NaN in masked-out positions)  Versions Current master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,SDPA produces NaN with padding mask," ğŸ› Describe the bug I'm implementing padding support directly on my LLM model. To do so, I add extra rows to the boolean attention mask with all `False` values. However, calling `torch.nn.functional.scaled_dot_product_attention` returns NaNs. An equivalent and naive implementation also does (see `naive_fsdpa_1`). But if I use `torch.finfo(att.dtype).min` as the masked fill value, the result is correct (see `naive_fsdpa_2`) Here's the repro:  This seems to suggest that there's a precision issue. Is this expected or known? Could `scaled_dot_product_attention` be fixed? Side question: do I lose flash attn if I append the padding mask like this? Thank you for your help. Maybe linked:  CC(scaled_dot_product_attention produces NaN when input has NaN in masked-out positions)  Versions Current master ",2023-06-16T13:59:29Z,triage review triaged,closed,7,33,https://github.com/pytorch/pytorch/issues/103749,I would suggest you to try this code once and let me know  >  , could you please help debug this issue ?,>  could you please help debug this issue ? Sure.,"I have run into this as well and I think the issue occurs when all logits for a piece end up being masked (a piece that attends to nothing). Softmax will then get an array consisting of only `inf`, which returns NaNs:  The NaNs then propagate and it's over. Either softmax should return a uniform distribution for such input or the attention mask should should use a very negative value (eg. `torch.finfo(att.dtype).min`), which also results in a uniform distribution in such cases. You can work around this by giving the sdp function an array of floats, these are then directly added to the logits.","maybe  ? (I couldn't find an SDPA label, not sure what it should go under)","Initially responded to CC(`scaled_dot_product_attention` produces `nan`s with boolean `attn_mask` with zero rows.) with: Correct, the problem is a completely masked out row. This is indicating that this batch, head query entry should attend to nothing in the keys which doesn't make the most sense. !image Which causes the indeterminate: 0/0 fo the softmax calc. The motivation for the boolean mask was to help people with their padding, and masking code which is typically used for creating a batch of varying sequences to perform SDPA on. They otheroption, encouraged for inference, is to use NestedTensors. This function is still in Beta. And I would be curious for arguments against the boolean mask."," Responded with:  >"" , the point being is that boolean mask result makes sense, the floating point mask does not... To me, at least...""",For the context. Boolean mask behavior makes sense to me. What is the reason behind floating point masks behaving differently as per the following example?  ,"My follow up would be, I the float version also makes sense to me. You are turning all the False values to 0 when you convert from boolean to float. So your attn _mask will look like the following.  The ""0"" row is indicating that, from the softmax calc, all entries in the key should be attended too equally", can you make this an easier snipppet to run?," , I am not sure I follow.  Have a look at the following example:  Could you please explain why ""mask"" is added to the input of `softmax`? It just makes no sense to me, so that is why I am confused. There should be some motivation behind it, I guess? Otherwise, in the floating case, `attn_mask` seems to be a wrong name since masking is actually not performed...", I would be happy to take a look at this example but can you make it a python script and not the output of a jupyter notebook?,"> For the context. Boolean mask behavior makes sense to me. What is the reason behind floating point masks behaving differently as per the following example?   The floating point mask does work correctly. From the docs: > A float mask of the same type as query, key, value that **is added** to the attention score. Since a float mask is added to the logits, in order to mask out the the values for softmax, you need to set them to a very large negative value. So, the correct mask is not  But something like:  In fact, PyTorch does something similar internally if you use a bool mask, when converting the bool mask to floats. Translated to Python code it does the following (it uses a masked fill, but same principle):  However, this is where the issue arises. If a token has an attention mask consisting of only `False` (does not attend to another token), softmax will return `NaN`. However, if you use the minimum value for a dtype, it will return a uniform distribution.","> This is indicating that this batch, head query entry should attend to nothing in the keys which doesn't make the most sense. It does make sense once you start combining attention masks and causal masks. You cannot pass both to he Torch SDP implementation, plus causal masks are currently broken if the key and value lengths are not the same ( CC(Ambiguitiy in causalmask in scaled_dot_product_attention)). So the only way to use Torch SDP currently in there cases is to combine the causal and attention masks and pass the intersection to `scaled_dot_product_attention`. These allfalse masks occur in that scenario. Suppose that a sequence has padding at the end. Then the last padding piece does not attend to earlier tokens (due to the causal mask). That by itself would be fine, because the piece would still attend to itself. However, since it is padding, it is also masked out by the attention mask. So, the result is a piece/query that does not attend to any key. This is how we ran into this issue. However, now we are using the float mask and mask the logits by setting them to `torch.finfo(dtype).min` rather than `inf`.", Thank you for the comment I am just seeing the one above.  TBH I am not entriely sure what a row of `False` values **SHOULD** produce a uniform distribution of nan.   any thoughts?,", by problem is with `attn_mask.to(torch.bool)` and `attn_mask.to(torch.float)` not being semantically equivalent even if `attn_mask` is just `{0, 1}` binary. Besides, in the floating case, I struggle to understand why `attn_mask` is a mask, not a weight, since it is not used for masking... Ah, OK, but I am fine if the user is aware that in the floating case it is the user who is responsible for setting up negative numbers in the complement and setting selected values to zero. Still yet, it could be done in a similar fashion as for the boolean case. That way the user is spared from setting things explicitly.","> , by problem is with attn_mask.to(torch.bool) and attn_mask.to(torch.float) not being semantically equivalent even if attn_mask is just {0, 1} binary. Most libraries mask logits by adding a large negative value, so it does make it easy to plug Torch SDP into existing libraries. If they were semantically the same, it wouldn't make sense to accept float tensors at all. This is quite a different issue than the bug reported in this issue though."," , the issue is with `softmax`, so, it seems, it is safe to close it...","> > , by problem is with attn_mask.to(torch.bool) and attn_mask.to(torch.float) not being semantically equivalent even if attn_mask is just {0, 1} binary. >  > Most libraries mask logits by adding a large negative value, so it does make it easy to plug Torch SDP into existing libraries. If they were semantically the same, it wouldn't make sense to accept float tensors at all. What are these libraries if I may ask? just curious, but would love to understand the difference between boolean and floating point masks. Given that `attn_mask` is called a mask, it seems like floating point mask does make sense if, similar to `bool`, only the values for which `attn_mask != 0` are selected?...","> What are these libraries may I ask, just curious? It's pretty common. One example is Hugging Face Transformers: https://github.com/huggingface/transformers/blob/6ce6d62b6f20040129ec9831e7c4f6576402ea42/src/transformers/modeling_utils.pyL902 https://github.com/huggingface/transformers/blob/6ce6d62b6f20040129ec9831e7c4f6576402ea42/src/transformers/models/bert/modeling_bert.pyL355 You can find plenty more with variations of https://github.com/search?q=%22%281.0++attention_mask%29%22+*+%22torch.finfo%28dtype%29.min%22&type=code"," , all right, that makes sense to me. Thank you for the examples! But now we have a problem when `attn_mask` is a sparse tensor. In that case boolean/floating semantics are unified by interpreting not specified values as `inf`. This, however, is different from the dense case where in the floating point case it is up to the user to set `inf` explicitly.  ,  , this begs the question: should BSR mask be dispatched under the `torch.nn.functional.scaled_dot_product_attention`? It seems to me that no, it has to be a standalone function because of the issues described in the previous paragraph."," , there is an issue about `softmax` which is unfortunately closed,  CC(Pass a tensor of `-inf` to `nn.Softmax` and return a tensor of `nan`). I think we can still fix it if proposition  CC(Pass a tensor of `-inf` to `nn.Softmax` and return a tensor of `nan`)issuecomment1601407472 is accepted. This one is quite easy to detect, however....","Landing CC(Pass a tensor of `inf` to `nn.Softmax` and return a tensor of `nan`) might be too big of a change. For the specific issue described in the top post, if possible, I would suggest going with using a very negative value instead of inf when a boolean mask is passed."," , using large negative values may still overflow when computing `x  x.max()` inside `softmax` unless it handles it properly, no?"," It seems to me this should be okay. The different semantics is permissible as it stems from the behavior of `torch.sparse.softmax` where the namespace `torch.sparse` allows us to have documented departures in semantics under densesparse conversions. The behavior would still be similar in a predictable, logical way.  That is, for a bsr mask with bool data type the `inf` would be used automatically in the case of materialized `false` values and unspecified values, and for the float bsr mask the user would need to set materialized values to `inf`. The assumption of the value of the unspecified elements is a ""feature"" of the  `sparse.softmax` which has special masking semantics. When the mask is applied weather it be bool or float, the result which eventually makes its way into the softmax has float datatype and the unspecified values are treated the same way. Now with a block sparse mask if the intention is to partially mask some blocks then those specified values must be set appropriately, as you would have to in the case of a dense mask, where all values are specified.  I don't think that the small differences in behaviour is unexpected enough to require `scaled_dot_product_attention` to be implemented for bsr masks under a `sparse` namespace variant. ",I've got the same bug. It's taken me all day to find this.,Essentially the same Issue:  CC(nn.MultiheadAttention causes gradients to become NaN under some use cases),"another small example/repro:  In LLMs we have a workaround for this case using:  but it's unfortunate and a bit confusing since the grad of nan_to_num would cause problems during training too. I can see semantic arguments about why we might not want to change the behavior in softmax, but I don't think those arguments necessarily translate to this particular function. Can we change the behavior of just the sdpa kernels to return 0's instead of nans? The function is documented as beta. Maybe we'd also add the `torch.nan_to_num` to the ""equivalent to the following"" portion of the doc?","I think this is expected behaviour. Look at softmax() of an entirely masked row. You're either going to get inf, Nan or all 1s. The last one is bad because you get past to future leakage. At least the first two ""break"" your training loop. By the way, this observation is courtesy of  "," ah, it isn't my observation, this is just a common early gotcha when it comes to transformer masking. yea it is expected behavior i agree (for softmax, dunno about sdpa)"
1385,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_gpt2_tiny (__main__.TestFxToOnnxWithOnnxRuntime_op_level_debug_True_dynamic_shapes_True))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_gpt2_tiny` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `onnx/test_fx_to_onnx_with_onnxruntime.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/onnx/test_fx_to_onnx_with_onnxruntime.py 1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0) headers: {} )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,DISABLED test_gpt2_tiny (__main__.TestFxToOnnxWithOnnxRuntime_op_level_debug_True_dynamic_shapes_True),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_gpt2_tiny` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `onnx/test_fx_to_onnx_with_onnxruntime.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/onnx/test_fx_to_onnx_with_onnxruntime.py 1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0) headers: {} ",2023-06-16T09:41:48Z,triaged module: flaky-tests skipped oncall: pt2 module: dynamic shapes,closed,0,0,https://github.com/pytorch/pytorch/issues/103743
2030,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] activation checkpointing with CheckpointImpl.NO_REENTRANT fail on flash-attention GPTLMHeadModel)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Enable FSDP with activation checkpointing on GPTLMHeadModel. Got bellow error when I use CheckpointImpl.NO_REENTRANT  If I use CheckpointImpl.REENTRANT instead of CheckpointImpl.NO_REENTRANT, FSDP work well code like:   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230522+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.1 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: version 3.19.6 Libc version: glibc2.31 Python version: 3.8.8 (default, Feb 24 2021, 21:46:12)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.4.0113genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB GPU 4: NVIDIA A100SXM440GB GPU 5: NVIDIA A100SXM440GB GPU 6: NVIDIA A100SXM440GB GPU 7: NVIDIA A100SXM440GB Nvidia driver version: 470.129.06 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.1.1 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_train.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_i)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[FSDP] activation checkpointing with CheckpointImpl.NO_REENTRANT fail on flash-attention GPTLMHeadModel," ğŸ› Describe the bug Enable FSDP with activation checkpointing on GPTLMHeadModel. Got bellow error when I use CheckpointImpl.NO_REENTRANT  If I use CheckpointImpl.REENTRANT instead of CheckpointImpl.NO_REENTRANT, FSDP work well code like:   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230522+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.1 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: version 3.19.6 Libc version: glibc2.31 Python version: 3.8.8 (default, Feb 24 2021, 21:46:12)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.4.0113genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB GPU 4: NVIDIA A100SXM440GB GPU 5: NVIDIA A100SXM440GB GPU 6: NVIDIA A100SXM440GB GPU 7: NVIDIA A100SXM440GB Nvidia driver version: 470.129.06 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.1.1 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_train.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_i",2023-06-16T01:47:47Z,high priority module: autograd triaged actionable,closed,0,7,https://github.com/pytorch/pytorch/issues/103726,.,Smaller reproduction: ,"There are probably many workarounds here, for example not making the tensor you save requires_grad, but maybe there's a possibility to provide a better error message.","My understanding of the assumption here is that only Tensors that are input/output should ever be saved. And since these input/output are always in the graph, if they are leafs, they must have a grad_accumulator that is kept alive by this graph. Maybe we should be more explicit in refusing noninput/output from save_for_backward. Also I guess that we don't want our checkpoint code to do this, or at least the temporary graph we create should keep the grad_accumulator alive properly?","From offline discussion with , the plan is:  saved tensor hooks should support saving intermediates   improved error checking when intermediates are saved AND we double backward ","> Maybe we should be more explicit in refusing noninput/output from save_for_backward. Yeah, sometimes for efficiency of formula computation, it can be convenient to save not input/output but something else as well...",>  it can be convenient to save not input/output but something else as well its always free though to just return that intermediate as an output and not do anything with it afterwards
2011,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯((fsdp) Support for accessing unsharded parameters for methods other than `forward()`)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Certain models require execution methods other than `forward()`. A good example family of models are generation models, for example Huggingface's `transformers.PreTrainedModel`. Here, `PreTrainedModel.forward()` is used for training, but during evaluation (which is often in the validation part of the training loop), users need to call `PreTrainedModel.generate()`. Once the model is FSDPwrapped, however, the user cannot use methods other than `forward()` to access unsharded parameters. As reported in  CC([FSDP] caffe2 error in forward method when using fsdp) , for EncoderDecoder models, calling `PreTrainedModel.generate()` with FSDPwrapped model will result in an error due to a direct access to a module (`_fsdp_wrapped_module.encoder`) that is not FSDPwrapped. The recommendation in this issue was to individually FSDPwrap `encoder`, but this is not a feasible solution for the case of T5 models, because embedding parameters should be shared across Encoder and Decoder; this results in errors during training. For this reason, HuggingFace Transformer team recommends a hack https://github.com/huggingface/transformers/issues/21667 which is not to wrap `encoder`, but to run `PreTrainedModel.forward()` (with an arbitrary input) before `PreTrainedModel.generate()`, because `forward()`'s side effect seems to populate unsharded parameters.  In order to support methods other than `forward()`, we could generalize `FullyShardedDataParallel.forward()` to support any function between `_pre_forward()` and `_post_forward()`, making a singleline change to https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/fully_sharded_data_parallel.pyL784L806 :  Of course, such a copypaste approac)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,(fsdp) Support for accessing unsharded parameters for methods other than `forward()`," ğŸš€ The feature, motivation and pitch Certain models require execution methods other than `forward()`. A good example family of models are generation models, for example Huggingface's `transformers.PreTrainedModel`. Here, `PreTrainedModel.forward()` is used for training, but during evaluation (which is often in the validation part of the training loop), users need to call `PreTrainedModel.generate()`. Once the model is FSDPwrapped, however, the user cannot use methods other than `forward()` to access unsharded parameters. As reported in  CC([FSDP] caffe2 error in forward method when using fsdp) , for EncoderDecoder models, calling `PreTrainedModel.generate()` with FSDPwrapped model will result in an error due to a direct access to a module (`_fsdp_wrapped_module.encoder`) that is not FSDPwrapped. The recommendation in this issue was to individually FSDPwrap `encoder`, but this is not a feasible solution for the case of T5 models, because embedding parameters should be shared across Encoder and Decoder; this results in errors during training. For this reason, HuggingFace Transformer team recommends a hack https://github.com/huggingface/transformers/issues/21667 which is not to wrap `encoder`, but to run `PreTrainedModel.forward()` (with an arbitrary input) before `PreTrainedModel.generate()`, because `forward()`'s side effect seems to populate unsharded parameters.  In order to support methods other than `forward()`, we could generalize `FullyShardedDataParallel.forward()` to support any function between `_pre_forward()` and `_post_forward()`, making a singleline change to https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/fully_sharded_data_parallel.pyL784L806 :  Of course, such a copypaste approac",2023-06-15T16:38:12Z,triaged module: fsdp,open,1,5,https://github.com/pytorch/pytorch/issues/103682,Tagging  (sorry I don't know a better method of tagging a person in github),Thanks for the detailed issue. I will work on scoping the design here and plan to address this in the upcoming half :), Thanks for taking a look! Looking forward to hear back!,"> but to run PreTrainedModel.forward() (with an arbitrary input) before PreTrainedModel.generate(), because forward()'s side effect seems to populate unsharded parameters. Regarding this,  I believe this is mainly because we don't reshard root parameters after forward for any of our sharding schemes. If the unsharded parameters aren't part of the root FSDP, this wouldn't work either.",varma Thanks for explaining why forward has such a side effect. Hope we build a more reliable solution!
1032,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([GHF] Auth when trying to fetch labels)ï¼Œ å†…å®¹æ˜¯ (There were few merge bot failures reported recently due to HTTP/403 error:  https://github.com/pytorch/pytorch/actions/runs/5269083146/jobs/9526693976step:6:80  https://github.com/pytorch/pytorch/actions/runs/5272750075/jobs/9535376256step:6:93 Which likely stems from the fact that `_fetch_url` method did not try to pass the auth token even when it was available and as result was ratelimited to 60 requests per hour, according to Resources in the REST API Refactor `gh_fetch_url` into `gh_fetch_url_and_headers` and use it from `request_for_labels` to utilize auth token, if available, which bumps rate limit to 1000 per hour as well as print more actionable message when rate limit is exceeded.   ğŸ¤– Generated by Copilot at 499b805 > _`gh_fetch_url` splits_ > _returns headers and body_ > _wrapper function_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[GHF] Auth when trying to fetch labels,"There were few merge bot failures reported recently due to HTTP/403 error:  https://github.com/pytorch/pytorch/actions/runs/5269083146/jobs/9526693976step:6:80  https://github.com/pytorch/pytorch/actions/runs/5272750075/jobs/9535376256step:6:93 Which likely stems from the fact that `_fetch_url` method did not try to pass the auth token even when it was available and as result was ratelimited to 60 requests per hour, according to Resources in the REST API Refactor `gh_fetch_url` into `gh_fetch_url_and_headers` and use it from `request_for_labels` to utilize auth token, if available, which bumps rate limit to 1000 per hour as well as print more actionable message when rate limit is exceeded.   ğŸ¤– Generated by Copilot at 499b805 > _`gh_fetch_url` splits_ > _returns headers and body_ > _wrapper function_",2023-06-15T15:55:41Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/103679," merge f ""Lint is green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1062,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(""Y.getIntrusivePtr()->set_storage(X.getIntrusivePtr()->storage()); "" in C++ is not supported)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch There is no set_storage() support for getIntrusivePtr() object. I want to copy from X to Y where X and Y are from different devices (i.e. X.device() != Y.device()). However, two devices share the global memory address with each other, so I prefer just moving the storage point from X to Y to avoid expensive data copy. The following code in C++ doesn't work for me, as it fails for undefined `set_storage()`:  If I perform a shallow copy using:  It is also not what I want, because it will change Y's device to be X's device as well, making following computation of Y not performed on original Y's expected device. Any alternative way to share the storage link. or any supporting plans?  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"""Y.getIntrusivePtr()->set_storage(X.getIntrusivePtr()->storage()); "" in C++ is not supported"," ğŸš€ The feature, motivation and pitch There is no set_storage() support for getIntrusivePtr() object. I want to copy from X to Y where X and Y are from different devices (i.e. X.device() != Y.device()). However, two devices share the global memory address with each other, so I prefer just moving the storage point from X to Y to avoid expensive data copy. The following code in C++ doesn't work for me, as it fails for undefined `set_storage()`:  If I perform a shallow copy using:  It is also not what I want, because it will change Y's device to be X's device as well, making following computation of Y not performed on original Y's expected device. Any alternative way to share the storage link. or any supporting plans?  Alternatives _No response_  Additional context _No response_ ",2023-06-15T13:17:59Z,module: cpp triaged,open,0,0,https://github.com/pytorch/pytorch/issues/103671
440,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch._dynamo.exc.Unsupported: torch.* op returned non-Tensor bool call_method is_contiguous on DynamicShapesFunctionTests.test_is_contiguous_memory_format_dynamic_shapes)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Run the test with `automatic_dynamic_shapes=True`.  Versions main)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,torch._dynamo.exc.Unsupported: torch.* op returned non-Tensor bool call_method is_contiguous on DynamicShapesFunctionTests.test_is_contiguous_memory_format_dynamic_shapes, ğŸ› Describe the bug  Run the test with `automatic_dynamic_shapes=True`.  Versions main,2023-06-14T19:48:24Z,good first issue triaged module: dynamic shapes,closed,0,4,https://github.com/pytorch/pytorch/issues/103618,How can I reproduce? When I run the command it does not want to recognize the automatic_dynamic_shapes=True attribute.  ![Uploading Screenshot 20230625 at 9.53.45 PM.pngâ€¦](),You can patch in https://github.com/pytorch/pytorch/pull/103623 and then delete `expectedFailureDynamic` from the test,"  Ran this test with the 'expectedFailureDynamic' decorator removed, automatic_dynamic_shapes was already True in my config.py file on the latest repo pull I just did, everything seems to pass. Is this issue just outdated? ",Thanks for confirming it's fixed!
497,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Continue simplifying dynamic shapes tests)ï¼Œ å†…å®¹æ˜¯ (  CC(Switch dynamic_shapes to True by default)  CC(Delete ifdyn and ifunspec combinators)  CC(Continue simplifying dynamic shapes tests) Remove the static by default / no automatic dynamic configuration as this is about to become the default. Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Continue simplifying dynamic shapes tests,  CC(Switch dynamic_shapes to True by default)  CC(Delete ifdyn and ifunspec combinators)  CC(Continue simplifying dynamic shapes tests) Remove the static by default / no automatic dynamic configuration as this is about to become the default. Signedoffby: Edward Z. Yang  ,2023-06-14T13:53:56Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/103592, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1960,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DDP+Gloo+torch.bool weight matrix)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Using Hugging Face transformers models GPTJ wrapped with the torch DistributedDataParallel where its bias matrix was implemented with the torch.bool type, and got 'Invalid scalar type' error message. But if I changed its bias matrix to torch.uint8, it can run successfully. A relevant issue can be found here: DDP + gloo + gpt2.    Versions Collecting environment information... PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.17 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01160.80.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:          x86_64 CPU opmode(s):        32bit, 64bit Byte Order:            Little Endian CPU(s):                96 Online CPU(s) list:   095 Thread(s) per core:    2 Core(s) per socket:    24 Socket(s):             2 NUMA node(s):          2 Vendor ID:             GenuineIntel CPU family:            6 Model:                 85 Model name:            Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz Stepping:              7 CPU MHz:               1000.012 CPU max MHz:           3700.0000 CPU min MHz:           1000.0000 BogoMIPS:              4200.00 Virtualization:        VTx L1d cache:             32K L1i cache:            )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DDP+Gloo+torch.bool weight matrix," ğŸ› Describe the bug Using Hugging Face transformers models GPTJ wrapped with the torch DistributedDataParallel where its bias matrix was implemented with the torch.bool type, and got 'Invalid scalar type' error message. But if I changed its bias matrix to torch.uint8, it can run successfully. A relevant issue can be found here: DDP + gloo + gpt2.    Versions Collecting environment information... PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.544) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.17 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux3.10.01160.80.1.el7.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:          x86_64 CPU opmode(s):        32bit, 64bit Byte Order:            Little Endian CPU(s):                96 Online CPU(s) list:   095 Thread(s) per core:    2 Core(s) per socket:    24 Socket(s):             2 NUMA node(s):          2 Vendor ID:             GenuineIntel CPU family:            6 Model:                 85 Model name:            Intel(R) Xeon(R) Gold 6252 CPU @ 2.10GHz Stepping:              7 CPU MHz:               1000.012 CPU max MHz:           3700.0000 CPU min MHz:           1000.0000 BogoMIPS:              4200.00 Virtualization:        VTx L1d cache:             32K L1i cache:            ",2023-06-14T09:18:09Z,oncall: distributed,closed,0,2,https://github.com/pytorch/pytorch/issues/103585, Seems torch.bool is not supported in some collectives on GLOO. ,"Yes, please check this: https://github.com/pytorch/pytorch/blame/dbc8eb2a8fd894fbc110bbb9f70037249868afa8/torch/csrc/distributed/c10d/ProcessGroupGloo.cppL98"
818,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Oncall][MTPG] Fix flaky test multi_threaded - test_broadcast_object_list)ï¼Œ å†…å®¹æ˜¯ (This test(https://github.com/pytorch/pytorch/blob/8340762211e3b55caa178bac748bd902249f6fc0/test/distributed/test_multi_threaded_pg.pyL133 ) is failing on internal sandbox with the following error msg:  Internal error report: https://www.internalfb.com/intern/test/562950031915334?ref_report_id=0  We believe this is because we no longer perform barrier after init (see https://github.com/pytorch/pytorch/pull/99937).  This PR temporarily turn back on  to avoid flaky test for the time being, but we should look into it to find a way to properly do this.  cc.   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Oncall][MTPG] Fix flaky test multi_threaded - test_broadcast_object_list,"This test(https://github.com/pytorch/pytorch/blob/8340762211e3b55caa178bac748bd902249f6fc0/test/distributed/test_multi_threaded_pg.pyL133 ) is failing on internal sandbox with the following error msg:  Internal error report: https://www.internalfb.com/intern/test/562950031915334?ref_report_id=0  We believe this is because we no longer perform barrier after init (see https://github.com/pytorch/pytorch/pull/99937).  This PR temporarily turn back on  to avoid flaky test for the time being, but we should look into it to find a way to properly do this.  cc.   ",2023-06-14T02:42:17Z,Merged ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/103568, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8gcc7 / test (distributed, 2, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," f ""unrelated test failure"""," merge f ""unrelated test failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1988,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([functorch] [FakeTensorMode, meta tensor] + aot_autograd Bug.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am trying to use FakeTensor and aot_autograd to capture the computation graph, but I met below errors. Can anyone help me out?  FakeTensorMode case In this case, I got errors like `TypeError: Multiple dispatch failed for 'torch._ops.aten.t.default'; all __torch_dispatch__ handlers returned NotImplemented`.    torchdistx fake_mode case If I replace the `with FakeTensorMode():` by `with fake_mode()` in torchdistx, it gets below errors:   Meta Tensor case In meta tensor case, below code works for small models. But for some complicated models like Bert, it raises Errors shown below.    Versions PyTorch version: 2.1.0.dev20230612 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (x86_64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.24.0 Libc version: N/A Python version: 3.9.13 (main, Aug 25 2022, 18:29:29)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Intel(R) Core(TM) i78850H CPU @ 2.60GHz Versions of relevant libraries: [pip3] functorch==0.2.1 [pip3] numpy==1.23.2 [pip3] torch==2.1.0.dev20230612 [pip3] torchdistx==0.3.0.dev0+cpu [conda] functorch                 0.2.1                    pypi_0    pypi [conda] numpy                     1.23.2                   pypi_0    pypi [conda] torch                     2.1.0.dev20230612          pypi_0    pypi [conda] torchdistx                0.3.0.dev0+cpu       )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[functorch] [FakeTensorMode, meta tensor] + aot_autograd Bug."," ğŸ› Describe the bug I am trying to use FakeTensor and aot_autograd to capture the computation graph, but I met below errors. Can anyone help me out?  FakeTensorMode case In this case, I got errors like `TypeError: Multiple dispatch failed for 'torch._ops.aten.t.default'; all __torch_dispatch__ handlers returned NotImplemented`.    torchdistx fake_mode case If I replace the `with FakeTensorMode():` by `with fake_mode()` in torchdistx, it gets below errors:   Meta Tensor case In meta tensor case, below code works for small models. But for some complicated models like Bert, it raises Errors shown below.    Versions PyTorch version: 2.1.0.dev20230612 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (x86_64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.24.0 Libc version: N/A Python version: 3.9.13 (main, Aug 25 2022, 18:29:29)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Intel(R) Core(TM) i78850H CPU @ 2.60GHz Versions of relevant libraries: [pip3] functorch==0.2.1 [pip3] numpy==1.23.2 [pip3] torch==2.1.0.dev20230612 [pip3] torchdistx==0.3.0.dev0+cpu [conda] functorch                 0.2.1                    pypi_0    pypi [conda] numpy                     1.23.2                   pypi_0    pypi [conda] torch                     2.1.0.dev20230612          pypi_0    pypi [conda] torchdistx                0.3.0.dev0+cpu       ",2023-06-13T12:28:56Z,triaged oncall: pt2 module: fakeTensor module: aotdispatch,closed,0,8,https://github.com/pytorch/pytorch/issues/103505,Assigning  arbitrarily," are you able to use `aot_export_module` instead for your use case? I got this example to work for me:  `aot_export_module` is meant for exporting modules. `aot_export_joint_simple` is (currently) intended for a more specific use case around higher order ops. One important difference is that `aot_export_module` will give you a pure, functional graph, where the parameters on the module are lifted into graph inputs. It also returns a `GraphSignature` object that tells you info about which graph inputs correspond to parameters. I still think we should fix this though  `aot_export_joint_simple` should fail when passed fake inputs (although it's not currently very well tested in the code base today)."," Thank you.  Your code works for me, too. I think currently `FakeTensorMode + aot_export_module` is enough for me. It not only has the joint forwardbackward graph, but also has the information of parameters and their corresponding  gradients in the backward propagation.  I try some other modules like torch.nn.TransformerEncoderLayer and it also works. It think this is a good news. Expecting more examples and explanations in the future documents. ğŸ˜†  "," I met another bug when using `torch.nn.TransformerEncoder`. `torch.nn.TransformerEncoderLayer` works for me, but below code doesn't. Any idea to solve this?  ","I dug a little, and it looks like in this code:  We end up with the fake modes on the sample inputs and the parameters having two different id's. That's pretty unexpected (and breaks AOTAutograd's assumption that there is only one fake mode floating around). The example passes when I only use `torch.nn.TransformerEncoderLayer(1024, 16, 4096)` instead of the entire `TransformerEncoder`, hmm.","Looks like this `deepcopy` is the culprit (link). `TransformerEncoder` deepcopies the layers that were passed into it, and it looks like when we deepcopy a layer, we end up with a different fake mode from the original module (which is probably not the behavior we want).",This looks like a fake tensor issue  created a dedicated issue for it here:  CC(nn.module deepcopy() does not preserve fake tensor mode),Fixed as part of https://github.com/pytorch/pytorch/pull/104476
464,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6047) of binary: /home/win10-ubuntu/anaconda3/envs/vicuna-7b/bin/python)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug !image  Versions Finetune vicuna7b error Finetuning commandsï¼š !image But I got an errorï¼š !image )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6047) of binary: /home/win10-ubuntu/anaconda3/envs/vicuna-7b/bin/python, ğŸ› Describe the bug !image  Versions Finetune vicuna7b error Finetuning commandsï¼š !image But I got an errorï¼š !image ,2023-06-13T09:50:22Z,oncall: distributed triaged,open,0,2,https://github.com/pytorch/pytorch/issues/103498,"Who has encountered similar problems, can you help me? Would be greatly appreciated","Hi, can you please provide a selfcontained repro of your issue as well as a paste (not screenshot) of the error message? thanks!"
1970,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory efficient SDP yields wrong gradients)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The gradients of samples for models with and without memory efficient SPD should be nearly identical, but are in practice often very different. I have a code example here: https://gist.github.com/lengstrom/fd091020d1cf8e22b2e0caa01c2e9255. `optimum` transforms the model to use memory efficient SDP  the gradients go back to matching when I disable memory efficient SDP and enable math SDP. Considering a GPTNeoX model, we fix a sample and record the gradient of the memory efficient SDP model and the gradient of the original model. We then measure the cosine similarity (a vector similarity metric, 0 = uncorrelated, 1 = perfectly correlated) and find that both: (a) the cosine similarity of gradients between the MESDP and standard models is not 1.0 and furthermore (b) on some parameter groups, this cosine similarity is very low. For example, see `gpt_neox.layers.0.input_layernorm.weight`  the similarity is `0.45` on this parameter group You can see the logs that show this here (from the script above):  This code should perfectly reproduce with just pytorch (nightly), optimumÂ (latest  this library is unversioned?), datasets (2.11.1), and transformers (4.29.2) installed. The full output is commented in the gist. This issue was explored more in https://github.com/huggingface/optimum/issues/1091: it doesn't arise with the 160m Pythia model (only 70m).  Versions  Collecting environment information... PyTorch version: 2.1.0.dev20230609+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.35 Python vers)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Memory efficient SDP yields wrong gradients," ğŸ› Describe the bug The gradients of samples for models with and without memory efficient SPD should be nearly identical, but are in practice often very different. I have a code example here: https://gist.github.com/lengstrom/fd091020d1cf8e22b2e0caa01c2e9255. `optimum` transforms the model to use memory efficient SDP  the gradients go back to matching when I disable memory efficient SDP and enable math SDP. Considering a GPTNeoX model, we fix a sample and record the gradient of the memory efficient SDP model and the gradient of the original model. We then measure the cosine similarity (a vector similarity metric, 0 = uncorrelated, 1 = perfectly correlated) and find that both: (a) the cosine similarity of gradients between the MESDP and standard models is not 1.0 and furthermore (b) on some parameter groups, this cosine similarity is very low. For example, see `gpt_neox.layers.0.input_layernorm.weight`  the similarity is `0.45` on this parameter group You can see the logs that show this here (from the script above):  This code should perfectly reproduce with just pytorch (nightly), optimumÂ (latest  this library is unversioned?), datasets (2.11.1), and transformers (4.29.2) installed. The full output is commented in the gist. This issue was explored more in https://github.com/huggingface/optimum/issues/1091: it doesn't arise with the 160m Pythia model (only 70m).  Versions  Collecting environment information... PyTorch version: 2.1.0.dev20230609+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.35 Python vers",2023-06-12T21:58:31Z,triaged,closed,1,5,https://github.com/pytorch/pytorch/issues/103462,"I can reproduce the issue with the flash and memefficient kernel (on pytorch 2.0.1): pythia70m !image Interestingly gradients seem indeed to start to drift just before SDPA, see here and here !image !image",,Is Flash being run in fp16 and then compared to math and memefficient being run in fp32?   how easy would it be to run the same experiment with FlashAttention directly? I want to know if it is integration error or fundamental to FlashAttention. That being said we do not expect bitwise identical results between the same paths: see this test,"Memefficient is run in fp32, compared to fp32 eager implementation. Flash is run in fp16, compared to fp16 eager implementation. I'll try to see if I can reproduce on a minimal example.","Hi, So, some difference in results is expected between the different implementations. This is due to the iterative normalization in the flashattention / memoryefficient attention, which amplified the differences for lowerprecision floating points. To give an idea, the baseline memoryefficient implementation has the following tolerances for different floating points on the backward pass. Take this difference in numerics for a single layer, and amplify it over different layers and you can get gradients which might be fairly different indeed."
1915,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Asynchronous CUDA AveragedModel)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch This is a proposal to improve the efficiency of CUDA `AveragedModel` used in EMA and SWA. (Followup of https://github.com/pytorch/pytorch/pull/94820). I would provide the implementation but would like feedback/approval before opening a PR. Currently the EMA/SWA weight update is done on the default stream, same as all the other GPU work. Because EMA/SWA weights are typically updated at the end of a training iteration (after an optimizer step), and are not needed until the end of the next iteration, we can actually run the EMA/SWA update in parallel of the forward/backward/optimizer step, virtually eliminating the overhead of EMA/SWA in many cases. This can be done by using a separate dedicated CUDA stream to perform the weight update. This is how it is done in the NeMo framework:  Stream creation: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL234  Synchronization between the dedicated stream and the main stream: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL259  Weight update in the dedicated stream: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL261  API to manually synchronize the dedicated stream: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL310 If the team is interested in extending the current `AveragedModel` class with an optional asynchronous feature, let me know and I'll work on a PR.  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Asynchronous CUDA AveragedModel," ğŸš€ The feature, motivation and pitch This is a proposal to improve the efficiency of CUDA `AveragedModel` used in EMA and SWA. (Followup of https://github.com/pytorch/pytorch/pull/94820). I would provide the implementation but would like feedback/approval before opening a PR. Currently the EMA/SWA weight update is done on the default stream, same as all the other GPU work. Because EMA/SWA weights are typically updated at the end of a training iteration (after an optimizer step), and are not needed until the end of the next iteration, we can actually run the EMA/SWA update in parallel of the forward/backward/optimizer step, virtually eliminating the overhead of EMA/SWA in many cases. This can be done by using a separate dedicated CUDA stream to perform the weight update. This is how it is done in the NeMo framework:  Stream creation: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL234  Synchronization between the dedicated stream and the main stream: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL259  Weight update in the dedicated stream: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL261  API to manually synchronize the dedicated stream: https://github.com/NVIDIA/NeMo/blob/a87702a522387da0aac62dc1f90a88a8e0bfc7cc/nemo/collections/common/callbacks/ema.pyL310 If the team is interested in extending the current `AveragedModel` class with an optional asynchronous feature, let me know and I'll work on a PR.  Alternatives _No response_  Additional context _No response_ ",2023-06-12T18:59:18Z,module: optimizer triaged needs research,open,1,3,https://github.com/pytorch/pytorch/issues/103449, Who would be the most relevant person in the team to opine on this?,"To clarify, this would purely be a performance upgrade, right? I wonder if inductor would in the future be able to automatically assign parallel streams for copying data and compute. cc.   would you know? If that is not a plan for the future for inductor, we are willing to review a PR that would add this in eager for AveragedModel.",Yes this is planned for inductor.
1706,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.nn.TransformerEncoder.forward() references attn_mask, which is not an argument.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue The docs for torch.nn.TransformerEncoder.forward() includes this description of `is_causal`.  > is_causal (Optional[bool]) â€“ If specified, applies a causal mask as mask (optional) and ignores attn_mask for computing scaled dot product attention. Default: False. However, `attn_mask` is not an argument in this method. There are two arguments that pertain to masks: 1. `mask` 2. `src_key_padding_mask`  Suggest a potential alternative/fix I think it would be clearer to refer to one of these arguments by name; if `is_causal` does pertain to either mask, then the documentation should explain what it does pertain to. Because of the inconsistent naming, it's not obvious whether either or both of these mask arguments are related to `is_causal`.  I *believe* the intention is that `is_causal` is a convenient substitute for `mask` in certain usages. Because a large number of transformer usages are about nexttoken prediction tasks, the `is_causal` flag lets PyTorch generate `mask` on behalf of users so that the model only attends to previous tokens. If I'm correct, the fix could be as simple as > is_causal (Optional[bool]) â€“ If specified, applies a causal mask as `mask` (optional) and ignores `mask` for computing scaled dot product attention. Default: False. But if I'm not correct, then the docs should be revised for clarity. What is `attn_mask` and how does it relate to `torch.nn.TransformerEncoder.forward()`?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"torch.nn.TransformerEncoder.forward() references attn_mask, which is not an argument."," ğŸ“š The doc issue The docs for torch.nn.TransformerEncoder.forward() includes this description of `is_causal`.  > is_causal (Optional[bool]) â€“ If specified, applies a causal mask as mask (optional) and ignores attn_mask for computing scaled dot product attention. Default: False. However, `attn_mask` is not an argument in this method. There are two arguments that pertain to masks: 1. `mask` 2. `src_key_padding_mask`  Suggest a potential alternative/fix I think it would be clearer to refer to one of these arguments by name; if `is_causal` does pertain to either mask, then the documentation should explain what it does pertain to. Because of the inconsistent naming, it's not obvious whether either or both of these mask arguments are related to `is_causal`.  I *believe* the intention is that `is_causal` is a convenient substitute for `mask` in certain usages. Because a large number of transformer usages are about nexttoken prediction tasks, the `is_causal` flag lets PyTorch generate `mask` on behalf of users so that the model only attends to previous tokens. If I'm correct, the fix could be as simple as > is_causal (Optional[bool]) â€“ If specified, applies a causal mask as `mask` (optional) and ignores `mask` for computing scaled dot product attention. Default: False. But if I'm not correct, then the docs should be revised for clarity. What is `attn_mask` and how does it relate to `torch.nn.TransformerEncoder.forward()`?",2023-06-12T14:25:34Z,triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/103431,"Hey, this docstring was recently updated and is reflected in the unstable docs (https://pytorch.org/docs/main/generated/torch.nn.TransformerEncoder.htmltransformerencoder) it will be reflected in the next release.  Alternatively, you could toggle the dropdown here (arrow beside 2.0) to see these docs   Closing this issue as this has ben fixed. Feel free to reopen if you have further questions"
1994,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(mps and cpu give far different results when training a transformer.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug when training a transfromer model with mac CPU this is the resulting loss function log 994,tensor(13.4385),2.598125166893005,2.676572799682617 1988,tensor(13.2934),2.5872674012184143,2.6137495040893555 2000,tensor(13.3104),2.5885423374176026,2.628082513809204 2982,tensor(12.8851),2.5560683608055115,2.675567865371704 3976,tensor(12.6861),2.54050742149353,2.4734363555908203 4000,tensor(12.8042),2.549770863056183,2.584007501602173 4970,tensor(12.6474),2.537447814941406,2.528282403945923 when using the MPS, same code and data 994,tensor(1.4541),0.3743850642442703,0.42320576310157776 1988,tensor(1.4568),0.3762083804607391,0.3572617173194885 2000,tensor(1.4540),0.37433584868907926,0.3778816759586334 2982,tensor(1.4524),0.3732233664393425,0.3656735122203827 3976,tensor(1.4476),0.36990807741880416,0.3881590962409973 4000,tensor(1.4447),0.36788938045501707,0.3148590922355652 4970,tensor(1.4531),0.373729208111763,0.3972059488296509 test.zip  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230608 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.11.3 (main, Apr 19 2023, 18:49:55) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Pro Versions of relevant libraries: [pip3] numpy==1.24.3 [pip)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,mps and cpu give far different results when training a transformer.," ğŸ› Describe the bug when training a transfromer model with mac CPU this is the resulting loss function log 994,tensor(13.4385),2.598125166893005,2.676572799682617 1988,tensor(13.2934),2.5872674012184143,2.6137495040893555 2000,tensor(13.3104),2.5885423374176026,2.628082513809204 2982,tensor(12.8851),2.5560683608055115,2.675567865371704 3976,tensor(12.6861),2.54050742149353,2.4734363555908203 4000,tensor(12.8042),2.549770863056183,2.584007501602173 4970,tensor(12.6474),2.537447814941406,2.528282403945923 when using the MPS, same code and data 994,tensor(1.4541),0.3743850642442703,0.42320576310157776 1988,tensor(1.4568),0.3762083804607391,0.3572617173194885 2000,tensor(1.4540),0.37433584868907926,0.3778816759586334 2982,tensor(1.4524),0.3732233664393425,0.3656735122203827 3976,tensor(1.4476),0.36990807741880416,0.3881590962409973 4000,tensor(1.4447),0.36788938045501707,0.3148590922355652 4970,tensor(1.4531),0.373729208111763,0.3972059488296509 test.zip  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230608 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.4 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.11.3 (main, Apr 19 2023, 18:49:55) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Pro Versions of relevant libraries: [pip3] numpy==1.24.3 [pip",2023-06-09T20:55:01Z,triaged module: mps,closed,2,6,https://github.com/pytorch/pytorch/issues/103343,"I abstracted the code in the zip file.   run ""python demo.py train_seq eval_seq ""  to reproduce the data shown.     replace ""cpu"" with ""mps"" in the code between runs.","Hi  , I ran your code and data on cuda platform and the result is consistency with those on cpu, so I suppose it is a mps device issue.","> Hi  , I ran your code and data on cuda platform and the result is consistency with those on cpu, so I suppose it is a mps device issue. yes,  the mps code is totally not working correctly for training the  SimpleTransformers model.   I hope it is fixed soon. I also ran it on an NVIDIA system to verify that the issue is with the  MPS implementation.","more insight   If i create a model on NVIDIA, then open it on a Mac/mps system with map_location=torch.device('mps') then the loss values are close to that of the NVIDIA machine.   So there may be something related to initializing the model on the mps system.",this still behaves this way with current pytorch/transformers/ os 14.1.2,"the current build from source 16 Dec 2023 appears to have corrected this issue.   I assume that it will propogate to a release. I did not check in changes but some updates from others discussing this topic, e.g. https://github.com/watarungurunnn  identified here  CC(multi_head_attention_forward generates different values on MPS compared to CPU)  appear to have corrected the issue. 1 epoch MPS loss is 2.5,   1 epoch NVIDIA loss is 2.7.   because of the randomness involved in the training process these are comparable, where before they were different by closer to 10x "
816,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The static checks of the TransformerEncoder should consider ...)ï¼Œ å†…å®¹æ˜¯ (**Transformer support num_encoder_layers=0 as PyTorch 1.6** The static checks of the TransformerEncoder should consider num_layers to avoid IndexError. The `Transformer` can be used to implement all the capabilities of the `TransformerEncoder` and `TransformerDecoder`. Therefore it is also possible to set the `Transformer` initialization parameter `num_encoder_layers` to `0`. In this case the current static check (`first_layer = self.layers[0]`) will throw an exception `IndexError: index 0 is out of range`. This pr is to fix this bug of outofbounds array access. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,The static checks of the TransformerEncoder should consider ...,**Transformer support num_encoder_layers=0 as PyTorch 1.6** The static checks of the TransformerEncoder should consider num_layers to avoid IndexError. The `Transformer` can be used to implement all the capabilities of the `TransformerEncoder` and `TransformerDecoder`. Therefore it is also possible to set the `Transformer` initialization parameter `num_encoder_layers` to `0`. In this case the current static check (`first_layer = self.layers[0]`) will throw an exception `IndexError: index 0 is out of range`. This pr is to fix this bug of outofbounds array access. ,2023-06-09T18:14:59Z,module: nn triaged open source module: edge cases bug,closed,0,8,https://github.com/pytorch/pytorch/issues/103335,"   :x: The email address for the commit (9496ebb0ecab3727b883145faa98f74b3c846732) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","   :x: The email address for the commit (9496ebb0ecab3727b883145faa98f74b3c846732) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."," label ""topic: Transformer support num_encoder_layers=0 as PyTorch 1.6"""," label ""bug"""," label ""module: nn""", ,This is fixed by CC(Replace use of `first_layer` in init with `encoder_layer` argument to init)
816,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The static checks of the TransformerEncoder should consider ...)ï¼Œ å†…å®¹æ˜¯ (**Transformer support num_encoder_layers=0 as PyTorch 1.6** The static checks of the TransformerEncoder should consider num_layers to avoid IndexError. The `Transformer` can be used to implement all the capabilities of the `TransformerEncoder` and `TransformerDecoder`. Therefore it is also possible to set the `Transformer` initialization parameter `num_encoder_layers` to `0`. In this case the current static check (`first_layer = self.layers[0]`) will throw an exception `IndexError: index 0 is out of range`. This pr is to fix this bug of outofbounds array access. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,The static checks of the TransformerEncoder should consider ...,**Transformer support num_encoder_layers=0 as PyTorch 1.6** The static checks of the TransformerEncoder should consider num_layers to avoid IndexError. The `Transformer` can be used to implement all the capabilities of the `TransformerEncoder` and `TransformerDecoder`. Therefore it is also possible to set the `Transformer` initialization parameter `num_encoder_layers` to `0`. In this case the current static check (`first_layer = self.layers[0]`) will throw an exception `IndexError: index 0 is out of range`. This pr is to fix this bug of outofbounds array access. ,2023-06-09T18:14:59Z,module: nn triaged open source module: edge cases bug,closed,0,8,https://github.com/pytorch/pytorch/issues/103335,"   :x: The email address for the commit (9496ebb0ecab3727b883145faa98f74b3c846732) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","   :x: The email address for the commit (9496ebb0ecab3727b883145faa98f74b3c846732) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."," label ""topic: Transformer support num_encoder_layers=0 as PyTorch 1.6"""," label ""bug"""," label ""module: nn""", ,This is fixed by CC(Replace use of `first_layer` in init with `encoder_layer` argument to init)
442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] Add missing fields for THPPyInterpreterFrame.)ï¼Œ å†…å®¹æ˜¯ (Fixes  CC([dynamo] Python 3.11: AttributeError: 'torch._C.dynamo.eval_frame._PyInterpreterFrame' object has no attribute 'f_lineno') Test Plan: Before the fix:   got result:  After the fix:  Got Result:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[dynamo] Add missing fields for THPPyInterpreterFrame.,Fixes  CC([dynamo] Python 3.11: AttributeError: 'torch._C.dynamo.eval_frame._PyInterpreterFrame' object has no attribute 'f_lineno') Test Plan: Before the fix:   got result:  After the fix:  Got Result:  ,2023-06-08T01:11:07Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/103227,"Hi, are we still going to land this?","> Hi, are we still going to land this? yes I can land it if  can stamp.",The committers listed above are authorized under a signed CLA.:white_check_mark: login: zhxchen17 / name: Zhengxu Chen  (a15b133b37227d10d0568b6a44677c1be807814b), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
564,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] Python 3.11: AttributeError: 'torch._C.dynamo.eval_frame._PyInterpreterFrame' object has no attribute 'f_lineno')ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When dynamo throws error from 3.11 in _export(), instead of UserError we ran into another AttributeError instead:  This is observed from exportdb CI test: https://osscirawjobstatus.s3.amazonaws.com/log/14083746115  Versions python 3.11 CI )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[dynamo] Python 3.11: AttributeError: 'torch._C.dynamo.eval_frame._PyInterpreterFrame' object has no attribute 'f_lineno'," ğŸ› Describe the bug When dynamo throws error from 3.11 in _export(), instead of UserError we ran into another AttributeError instead:  This is observed from exportdb CI test: https://osscirawjobstatus.s3.amazonaws.com/log/14083746115  Versions python 3.11 CI ",2023-06-07T22:16:37Z,triaged oncall: pt2 module: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/103210,Assigned myself from pt2 hackday:  CC([Placeholder] PyTorch 2.0 Dynamo/Inductor Hack{day/week}),.11 expert  ,"At the dynamopython level in 3.11, we don't actually have access to the internal CPython interpreter frame. Instead, we wrap the internal interpreter frame with a customdefined CPython object struct and give that to the dynamopython parts. I think you can add `f_lineno` to this struct here: https://github.com/pytorch/pytorch/blob/95fced4483af07767114a7674d9a3e124cdd5eba/torch/csrc/dynamo/eval_frame.cL63","So after running `pytest test/export/test_db.py` locally with py3.11, there is no error reproed. By looking at the stacktrace the error tracks to https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/convert_frame.pyL216 why we are running with suppress_errors=True for CI tests but not for local tests?","> At the dynamopython level in 3.11, we don't actually have access to the internal CPython interpreter frame. Instead, we wrap the internal interpreter frame with a customdefined CPython object struct and give that to the dynamopython parts. I think you can add `f_lineno` to this struct here: >  > https://github.com/pytorch/pytorch/blob/95fced4483af07767114a7674d9a3e124cdd5eba/torch/csrc/dynamo/eval_frame.cL63 yeah working on that."
1643,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯((fsdp - maybe a bug) SHARDED_STATE_DICT returns tensor with no data)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The huggingface `T5ForConditionalGeneration` model has the embedding layer named as `shared` at the top module, and passes a reference to it to the `T5Stack` ctor. In the `T5Stack` the passed embedding is stored as `embed_tokens`. So `t5model.shared` and `t5model.encoder.embed_tokens` point to the same `nn.Embedding()` layer. Here's the two code references: 1. `model.shared` 1. `model.encoder.embed_tokens` and an excerpt of the code:  Wrapping this T5 model with FSDP and inspecting the details of embedding layer in a transformer (T5 model) model wrapped in FSDP with `ShardingStrategy.FULL_SHARD` and `auto_wrap_policy=transformer_auto_wrap_policy`, I'm observing that with `StateDictType.SHARDED_STATE_DICT`: 1.  `model.state_dict()[""shared.weight""]`  is a `ShardedTensor` (as expected) 2. `model.state_dict()[""encoder.embed_tokens.weight""]` is a Tensor with no data (see repro script's output below) I'm guessing this has to do with this warning in the FSDP docs page ): !image But I'm wrapping the `T5Block` (see my repro script below) not `T5Stack` in an FSDP unit so the outer module (`T5ForConditionalGeneration`) and `T5Block` belongs to the same (root) FSDP unit.  Is this an expected behavior (I'm misunderstanding something) or is it a bug or should the documentation be updated? Repro Script  To run:  Save this as `repro_script.py`  Output     Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,(fsdp - maybe a bug) SHARDED_STATE_DICT returns tensor with no data," ğŸ› Describe the bug The huggingface `T5ForConditionalGeneration` model has the embedding layer named as `shared` at the top module, and passes a reference to it to the `T5Stack` ctor. In the `T5Stack` the passed embedding is stored as `embed_tokens`. So `t5model.shared` and `t5model.encoder.embed_tokens` point to the same `nn.Embedding()` layer. Here's the two code references: 1. `model.shared` 1. `model.encoder.embed_tokens` and an excerpt of the code:  Wrapping this T5 model with FSDP and inspecting the details of embedding layer in a transformer (T5 model) model wrapped in FSDP with `ShardingStrategy.FULL_SHARD` and `auto_wrap_policy=transformer_auto_wrap_policy`, I'm observing that with `StateDictType.SHARDED_STATE_DICT`: 1.  `model.state_dict()[""shared.weight""]`  is a `ShardedTensor` (as expected) 2. `model.state_dict()[""encoder.embed_tokens.weight""]` is a Tensor with no data (see repro script's output below) I'm guessing this has to do with this warning in the FSDP docs page ): !image But I'm wrapping the `T5Block` (see my repro script below) not `T5Stack` in an FSDP unit so the outer module (`T5ForConditionalGeneration`) and `T5Block` belongs to the same (root) FSDP unit.  Is this an expected behavior (I'm misunderstanding something) or is it a bug or should the documentation be updated? Repro Script  To run:  Save this as `repro_script.py`  Output     Versions  ",2023-06-07T19:34:51Z,triaged module: fsdp,closed,0,1,https://github.com/pytorch/pytorch/issues/103189,cc:  
415,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Preserve CreationMeta when metafying views.)ï¼Œ å†…å®¹æ˜¯ (  CC(Preserve CreationMeta when metafying views.) This helps us avoid erroring / generate more accurate error messages in Dynamo when doing mutations on views. Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Preserve CreationMeta when metafying views.,  CC(Preserve CreationMeta when metafying views.) This helps us avoid erroring / generate more accurate error messages in Dynamo when doing mutations on views. Signedoffby: Edward Z. Yang ,2023-06-07T14:41:00Z,Merged ciflow/trunk topic: bug fixes ciflow/inductor release notes: dynamo,closed,0,10,https://github.com/pytorch/pytorch/issues/103152, merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.9clang7asan / test (default, 5, 6, linux.4xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""asan timeout seems unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
2021,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile(model.generate) cannot run under torch.inference_mode() with dynamic input shape)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `torch.compile(model.generate)` cannot run under `torch.inference_mode()` with dynamic input shape, but it can run if I change `torch.inference_mode()` to `torch.no_grad()`. Users use `torch.inference_mode()` in most cases, would you please help to check it? Thx!  The error is as follows   Versions PyTorch version: 2.1.0.dev20230604+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 11.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.28 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.16.0rc8intelnext01534g53cb5f883cf7x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              224 Online CPU(s) list: 0223 Thread(s) per core:  2 Core(s) per socket:  56 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU family:          6 Model:               143 Model name:          Genuine Intel(R) CPU 0000%@ Stepping:            3 CPU MHz:             1900.000 CPU max MHz:         1900.0000 CPU min MHz:         800.0000 BogoMIPS:            3800.00 Virtualization:      VTx L1d cache:           48K L1i cache:           32K L2 cache:            2048K L3 cache:            107520K NUMA node0 CPU(s):   055,112167 NUMA node1 CPU(s):   56111,168223 Flags:              )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile(model.generate) cannot run under torch.inference_mode() with dynamic input shape," ğŸ› Describe the bug `torch.compile(model.generate)` cannot run under `torch.inference_mode()` with dynamic input shape, but it can run if I change `torch.inference_mode()` to `torch.no_grad()`. Users use `torch.inference_mode()` in most cases, would you please help to check it? Thx!  The error is as follows   Versions PyTorch version: 2.1.0.dev20230604+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 11.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.28 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.16.0rc8intelnext01534g53cb5f883cf7x86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              224 Online CPU(s) list: 0223 Thread(s) per core:  2 Core(s) per socket:  56 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU family:          6 Model:               143 Model name:          Genuine Intel(R) CPU 0000%@ Stepping:            3 CPU MHz:             1900.000 CPU max MHz:         1900.0000 CPU min MHz:         800.0000 BogoMIPS:            3800.00 Virtualization:      VTx L1d cache:           48K L1i cache:           32K L2 cache:            2048K L3 cache:            107520K NUMA node0 CPU(s):   055,112167 NUMA node1 CPU(s):   56111,168223 Flags:              ",2023-06-07T02:19:12Z,triaged inference mode oncall: pt2 module: dynamic shapes,closed,0,1,https://github.com/pytorch/pytorch/issues/103132,"Yep, looks like inference mode is still partially broken. I can repro locally  from running with `TORCH_SHOW_CPP_STACKTRACES=1`, the size call is coming from `at::matmul`: https://github.com/pytorch/pytorch/blob/3c896a5adbec24954e5bb77469911142fe5ba8b1/aten/src/ATen/native/LinearAlgebra.cppL1945 That shouldn't be happening, because we have a python decomp for matmul that's supposed to run when the python dispatcher is enabled."
289,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Reland] Add sym_size/stride/numel/storage_offset to native_function.yaml)ï¼Œ å†…å®¹æ˜¯ (Differential Revision: D46459100)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Reland] Add sym_size/stride/numel/storage_offset to native_function.yaml,Differential Revision: D46459100,2023-06-06T20:41:39Z,fb-exported Merged ciflow/trunk release notes: jit,closed,0,15,https://github.com/pytorch/pytorch/issues/103107,This pull request was **exported** from Phabricator. Differential Revision: D46459100,"   :x: The email address for the commit (aa235157264fe37e86caef5ef7db74a433a47c29) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D46459100,This pull request was **exported** from Phabricator. Differential Revision: D46459100,"   :x: The email address for the commit (fd4ed5b1f2252590b6838baccc42caf945586df7) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.","   :x: The email address for the commit (1b2cd79b35c3c5e9e22fc43c948514830569cf1c) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D46459100,"   :x: The email address for the commit (44061044b98ee9b22397c2328eaf151bcce5ef5f) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D46459100,"   :x: The email address for the commit (6ad875ddc0d3993e95eda96225e2b3f96711d207) is not linked to the GitHub account, preventing the EasyCLA check. Consult this Help Article and GitHub Help to resolve. (To view the commit's email address, add .patch at the end of this PR page's URL.) For further assistance with EasyCLA, please submit a support request ticket."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D46459100,This pull request was **exported** from Phabricator. Differential Revision: D46459100, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([benchmarks] Torchbench llama is not suitable for training)ï¼Œ å†…å®¹æ˜¯ (  CC([benchmarks] Torchbench llama is not suitable for training) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[benchmarks] Torchbench llama is not suitable for training,  CC([benchmarks] Torchbench llama is not suitable for training) ,2023-06-06T18:20:13Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/103094, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1207,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([OOM] Unable to convert 30B model to ONNX, using 4x A100's)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Unable to convert 30B model to ONNX. I am using 4x A100's , 500GB RAM, 2.5TB Memory, still running out of memory.  Here's the repro: I believe this is reproable in any container, but here's the container setup step: 1) Create a container on Runpod from winglian/axolotlrunpod:mainpy3.9cu1182.0.0  Runpod.io > My Templates > New Template > winglian/axolotlrunpod:mainpy3.9cu1182.0.0  Then deploy 4x A100 in Secure cloud, search for the Template just created:  2) Once it loads, start the terminal and:  3) Paste the following inference file using vim:  Paste this:  To exit vim, Esc > Shift + Z > Shift + Z  4) Now, run the conversion: python fp16_to_onnx.py WizardLM30BUncensored This will take about 45 minutes, which already sounds a bit wrong as it should take 5m. gpt2 takes 30 seconds to convert. Then , it will fail with this:  Can you please help unblock? I have been trying to convert this to ONNX for days already Many thanks  Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[OOM] Unable to convert 30B model to ONNX, using 4x A100's"," ğŸ› Describe the bug Unable to convert 30B model to ONNX. I am using 4x A100's , 500GB RAM, 2.5TB Memory, still running out of memory.  Here's the repro: I believe this is reproable in any container, but here's the container setup step: 1) Create a container on Runpod from winglian/axolotlrunpod:mainpy3.9cu1182.0.0  Runpod.io > My Templates > New Template > winglian/axolotlrunpod:mainpy3.9cu1182.0.0  Then deploy 4x A100 in Secure cloud, search for the Template just created:  2) Once it loads, start the terminal and:  3) Paste the following inference file using vim:  Paste this:  To exit vim, Esc > Shift + Z > Shift + Z  4) Now, run the conversion: python fp16_to_onnx.py WizardLM30BUncensored This will take about 45 minutes, which already sounds a bit wrong as it should take 5m. gpt2 takes 30 seconds to convert. Then , it will fail with this:  Can you please help unblock? I have been trying to convert this to ONNX for days already Many thanks  Versions ",2023-06-06T15:39:17Z,module: onnx triaged,open,0,1,https://github.com/pytorch/pytorch/issues/103089,"Hey  ,  Is this still an issue you're interested in solving? If so, you could try using the `torch.onnx.dynamo_export()` api to attempt to export, and if that doesn't work, you could also try fake mode. Let me know if you have any success! Will otherwise close as stale in 23 weeks :) "
636,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Teach Triton codegen to generate sqrt)ï¼Œ å†…å®¹æ˜¯ (  CC(Teach Triton codegen to generate sqrt)  CC(This extra message would have helped with Wav2Vec2 debugging.) Fixes  CC(NameError('math is not defined') in Triton codegen for llama with dynamic shapes) I know ngimel doesn't like this sort of fix because we shouldn't actually be computed sqrt at runtime, I'm open to some sort of perf warning saying that we're spending FLOPs weirdly. Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Teach Triton codegen to generate sqrt,"  CC(Teach Triton codegen to generate sqrt)  CC(This extra message would have helped with Wav2Vec2 debugging.) Fixes  CC(NameError('math is not defined') in Triton codegen for llama with dynamic shapes) I know ngimel doesn't like this sort of fix because we shouldn't actually be computed sqrt at runtime, I'm open to some sort of perf warning saying that we're spending FLOPs weirdly. Signedoffby: Edward Z. Yang  ",2023-06-06T13:49:15Z,Merged ciflow/trunk topic: bug fixes module: inductor ciflow/inductor release notes: inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/103084,producing a minified test case has... proven challenging lol,I'll wait for 's take, merge, As long as llama uses assume_static_by_default twill be fine. You only get hosed if you yolo dynamic=True.," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
245,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(increase clang-tidy coverage in torch/csrc)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,increase clang-tidy coverage in torch/csrc,Fixes ISSUE_NUMBER ,2023-06-06T03:35:25Z,module: cpu triaged open source module: amp (automated mixed precision) Merged NNC Reverted ciflow/trunk release notes: quantization topic: not user facing ciflow/periodic module: dynamo ciflow/inductor ciflow/slow,closed,0,32,https://github.com/pytorch/pytorch/issues/103058,"The header filter reflex in .clangtidy needs to be updated, otherwise none of these checks will run on any of the headers in the new files. Without the headers, many of the checks are implicitly disabled.", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/103058/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/5703110515,Need to allocate more time to lintrunner,?, It sounds like you might just need to add a timeoutminutes key to here: https://github.com/pytorch/pytorch/blob/f9988691600adf3f19beb1b13eaa33484e80b57e/.github/workflows/lint.ymlL20,Maybe it would be best to start by enabling it only on a subset of torch/csrc given the number of errors / changes needed to fix it?, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC(increase clangtidy coverage in torch/csrc), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Tried to rebase and push PR CC(increase clangtidy coverage in torch/csrc), but it was already up to date. Try rebasing against main by issuing: ` rebase b main`", label ciflow/periodic, label ciflow/slow, label ciflow/trunk, label ciflow/inductor, label ciflow/periodic, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `clang_tidy_coverage4` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout clang_tidy_coverage4 && git pull rebase`)","You have done a lot of great work, you may to consider splitting this PR into several parts to get it merged faster.", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `clang_tidy_coverage4` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout clang_tidy_coverage4 && git pull rebase`)", rebase,"> Nice, finally passes lintrunner, this is impressive expansion of clangtidy coverage. We may want to invest a clangtidy cache (ctcache) sometime in the future as this quickly becoming the heaviest lintrunner check by far. I will add aten/src/ATen/core torch/autograd and other dirs to clangtidy since they are so important, so it will quickly become extremely slow.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," This looks like break lint, will need to be reveted https://github.com/pytorch/pytorch/actions/runs/6123377163/job/16621614641 "
856,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(lit-llama lora fine tuning NetworkXUnbounded: Infinite capacity path, flow unbounded above)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug litllama version 8aa65ba33e844c283c0a84b9758445fe0c6aab2d Enable torch.compile with  Work around unrelated bug with  Patch in Triton fix https://github.com/openai/triton/pull/1741 Use standard setup instructions, `python finetune/lora.py` Fails with  The graph that was generated: https://gist.github.com/ezyang/71460c07dfc86d297090888e077bd88e I think this is the corresponding joint graph https://gist.github.com/ezyang/65e55a66c9cee4120c1ba242bbaef358 This also affects finetune/adapter.py, so it's probably an issue in the llama model itself    Versions main)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,"lit-llama lora fine tuning NetworkXUnbounded: Infinite capacity path, flow unbounded above"," ğŸ› Describe the bug litllama version 8aa65ba33e844c283c0a84b9758445fe0c6aab2d Enable torch.compile with  Work around unrelated bug with  Patch in Triton fix https://github.com/openai/triton/pull/1741 Use standard setup instructions, `python finetune/lora.py` Fails with  The graph that was generated: https://gist.github.com/ezyang/71460c07dfc86d297090888e077bd88e I think this is the corresponding joint graph https://gist.github.com/ezyang/65e55a66c9cee4120c1ba242bbaef358 This also affects finetune/adapter.py, so it's probably an issue in the llama model itself    Versions main",2023-06-06T03:10:21Z,triaged oncall: pt2 module: dynamic shapes,closed,0,6,https://github.com/pytorch/pytorch/issues/103055,Tentatively marking hipri due to the model being important,"Seems like a hipri to me, but defer it to  ", to comment and close out," has a better llama implementation that will be open sourced soon and you should use instead of litllama, if you need this",https://github.com/pytorchlabs/gptfast (private right now),"Closing, since the gptfast repo is public now"
436,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Package `torch/*.pyi` type hints)ï¼Œ å†…å®¹æ˜¯ (Including `torch._VF` and `torch.return_types` These are generated by: https://github.com/pytorch/pytorch/blob/4003e96ca1e54df58772870eb779c66e4b6e51fb/tools/pyi/gen_pyi.pyL1139L1155 Ref CC(Package `torch.fx` type hints))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Package `torch/*.pyi` type hints,Including `torch._VF` and `torch.return_types` These are generated by: https://github.com/pytorch/pytorch/blob/4003e96ca1e54df58772870eb779c66e4b6e51fb/tools/pyi/gen_pyi.pyL1139L1155 Ref CC(Package `torch.fx` type hints),2023-06-05T20:14:27Z,open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/103016,"> Wait, I couldn't find the .pyi this is referring to in the git repo, are they autogenerated? Updated comments in first floor.",Just to note: I took a look at the artifacts. The two `torch/*.pyi` files are packed as expected., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
413,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add undocumented functions for querying creation meta on autograd)ï¼Œ å†…å®¹æ˜¯ (  CC(Add undocumented functions for querying creation meta on autograd)  CC(Preserve leafness and requires_gradness in minified repros) Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add undocumented functions for querying creation meta on autograd,  CC(Add undocumented functions for querying creation meta on autograd)  CC(Preserve leafness and requires_gradness in minified repros) Signedoffby: Edward Z. Yang ,2023-06-05T15:06:21Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/102972,I'm going to roll this into an updated PR that also fixes a bug along the way
1373,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support for efficiently processing categorical distributions with varying dimensions)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Say I have a list of probability distribution tensors of varying size. It would be nice if `Categorical` could accept a flattened tensor `probs_flat` containing all the distributions, along with a pointer `ptr` that indicates the starting index of each distribution. Standard functions can efficiently be computed, e.g. entropy can be computed as `segment(probs_flat.log() * probs_flat, ptr)`, where `segment` is something like `segment_add_csr` from `torch_scatter` (I'm not sure if torch provides this function out of the box). I'm not too sure of the best way to integrate this with the existing code. Just an idea!  Alternatives Let `probs_list` be a list of probability distribution tensors, and `xs` be a list of samples from their respective distributions. There are currently two options to e.g. evaluate the logprobabilities of the samples: 1. (evaluate for each)  2. (pad, stack, evaluate)   Additional context This can be useful in RL, where the number of possible actions varies between environment steps, as a less wasteful alternative to action masking. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Support for efficiently processing categorical distributions with varying dimensions," ğŸš€ The feature, motivation and pitch Say I have a list of probability distribution tensors of varying size. It would be nice if `Categorical` could accept a flattened tensor `probs_flat` containing all the distributions, along with a pointer `ptr` that indicates the starting index of each distribution. Standard functions can efficiently be computed, e.g. entropy can be computed as `segment(probs_flat.log() * probs_flat, ptr)`, where `segment` is something like `segment_add_csr` from `torch_scatter` (I'm not sure if torch provides this function out of the box). I'm not too sure of the best way to integrate this with the existing code. Just an idea!  Alternatives Let `probs_list` be a list of probability distribution tensors, and `xs` be a list of samples from their respective distributions. There are currently two options to e.g. evaluate the logprobabilities of the samples: 1. (evaluate for each)  2. (pad, stack, evaluate)   Additional context This can be useful in RL, where the number of possible actions varies between environment steps, as a less wasteful alternative to action masking. ",2023-06-04T20:50:26Z,module: distributions feature triaged needs research,open,0,0,https://github.com/pytorch/pytorch/issues/102938
228,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(increase clang-tidy coverage to c10/test/*pp)ï¼Œ å†…å®¹æ˜¯ ()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,increase clang-tidy coverage to c10/test/*pp,,2023-06-04T07:11:38Z,open source topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/102924," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," label ""topic: not user facing""",  rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `clang_tidy_coverage2` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout clang_tidy_coverage2 && git pull rebase`)"
2008,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(PackedSequences on MPS accelerator yields `grad_y` missing or crashes the kernel.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  PackedSequences on MPS accelerator yields `grad_y` missing or crashes the kernel. This issue is related to [Issue 96416][loss error on m1] ([Loss.backward() error when using MPS on M1 CC(Loss.backward() error when using MPS on M1)][loss error on m1]), [Issue 94691][gru nan] ([Nan is output by GRU on mps CC(Nan is output by GRU on mps)][gru nan]), [Issue 97552][packed sequence failure] ([PackedSequence failure with MPS CC(PackedSequence failure with MPS)][packed sequence failure]), and [PR 96601][grad_y missing fix] ([[MPS] LSTM grad_y missing fix CC([MPS] LSTM grad_y missing fix)][grad_y missing fix]). To faithfully reproduce the error I have provided a self contained [Github Gist][Github Gist]. In addition, I already pasted some [results][Results] showing that this does consistently yield `Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) 'grad_y'` or crash a Jupyter Notebook kernel. A `env.yml` file is provided in the [Github Gist][Github Gist] and using `collect_env.py`:  Of note  has started to fix the error ([[MPS] LSTM grad_y missing fix CC([MPS] LSTM grad_y missing fix)][grad_y missing fix]), but it may not work with PackedSequences. Quote: > What worries me is that the example uses packed sequences. Not sure, but this may be the problem as MPS LSTM does not support it. >  > Concerning GRU, it simply does not work on MPS ;). No need to even test it. [packed sequence failure]:  CC(PackedSequence failure with MPS) [loss error on m1]:  CC(Loss.backward() error when using MPS on M1) [grad_y missing fix]: https://github.com/pytorch/pytorch/pull/96601 [gru nan]:  CC(Nan is output by GRU on mps) [Github Gist]: https://gist.github.com/dsm72/1cea06011)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,PackedSequences on MPS accelerator yields `grad_y` missing or crashes the kernel.," ğŸ› Describe the bug  PackedSequences on MPS accelerator yields `grad_y` missing or crashes the kernel. This issue is related to [Issue 96416][loss error on m1] ([Loss.backward() error when using MPS on M1 CC(Loss.backward() error when using MPS on M1)][loss error on m1]), [Issue 94691][gru nan] ([Nan is output by GRU on mps CC(Nan is output by GRU on mps)][gru nan]), [Issue 97552][packed sequence failure] ([PackedSequence failure with MPS CC(PackedSequence failure with MPS)][packed sequence failure]), and [PR 96601][grad_y missing fix] ([[MPS] LSTM grad_y missing fix CC([MPS] LSTM grad_y missing fix)][grad_y missing fix]). To faithfully reproduce the error I have provided a self contained [Github Gist][Github Gist]. In addition, I already pasted some [results][Results] showing that this does consistently yield `Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) 'grad_y'` or crash a Jupyter Notebook kernel. A `env.yml` file is provided in the [Github Gist][Github Gist] and using `collect_env.py`:  Of note  has started to fix the error ([[MPS] LSTM grad_y missing fix CC([MPS] LSTM grad_y missing fix)][grad_y missing fix]), but it may not work with PackedSequences. Quote: > What worries me is that the example uses packed sequences. Not sure, but this may be the problem as MPS LSTM does not support it. >  > Concerning GRU, it simply does not work on MPS ;). No need to even test it. [packed sequence failure]:  CC(PackedSequence failure with MPS) [loss error on m1]:  CC(Loss.backward() error when using MPS on M1) [grad_y missing fix]: https://github.com/pytorch/pytorch/pull/96601 [gru nan]:  CC(Nan is output by GRU on mps) [Github Gist]: https://gist.github.com/dsm72/1cea06011",2023-06-03T18:27:22Z,triaged module: mps,open,1,7,https://github.com/pytorch/pytorch/issues/102911, people (aside from the original poster ) are noticing it. The issue is just split across other issues and a PR:    CC(PackedSequence failure with MPS)issuecomment1531998792   CC(PackedSequence failure with MPS)   CC(Loss.backward() error when using MPS on M1)  https://github.com/pytorch/pytorch/pull/96601,"Thank you, it's good to know there is some progress","We can deduce if packed sequences are the reason of the failure. You can try to cause `missing grad_y` by using only output's states and not using LSTM output as described in this comment  CC(Loss.backward() error when using MPS on M1)issuecomment1464679133:  This way, there will be no gradient for y (`grad_y`) The fix I proposed previously (https://github.com/pytorch/pytorch/pull/96601) correctly works without packed sequences â€” such tests are even included in repo.  Btw, I have my finals now, and as I'm only a contributor and not an employee, I won't be able to dig into this issue for some time. But all finding concerning the bug will be appreciated! If you want to explore the code yourself, you can look into `RnnOps.mm` file. As I'm probably one of the few who understands what's going on in these 800 lines of code, I'll be happy to help with it ;)"," best of luck with your finals. Thank you for all of your assistance. Since we have the Github Gist example, it shouldn't be that hard to make the change and check.", I ran the changed example and am pasting results below.  TL;DR not using packed sequences (by using `torch.zeros`):  no longer crashes the kernel  sometimes causes exploding gradient by sending `L1` to `inf` (might be a bug but haven't checked extensively)  the four original `Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) 'grad_y'` remain  new `SeqEncoder`   results  new results  subset ,"Hey, 72! I'm done with my finals I've just built fresh `main` pytorch (`2.1.0a0+gitdb1ac4e`), and the issue is not reproduced. Though, I have no idea what modifications could have affected the reproducibility. Here's a run of the example you posted above. No tests are skipped  GRU still does not work, which is bad but expected","I had left a test case hereissuecomment1512044817) which demonstrates a crash when using `PackedSequence`.  That crash still occurs:  It is also incredibly slow, no doubt because of operations not implemented yet on MPS:  Although I will say that it is now about 25% faster than CPU for our LSTM use case (until it crashes in `PackedSequence`)"
719,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use 32bit for indexing in `multi_tensor_apply` as possible)ï¼Œ å†…å®¹æ˜¯ (I do think there's a better way to propagate what type to use for indexing, but I found an anonymous argument convenient to some extent as I was writing. Also, I can imagine the availability check of 32bit is a bit too optimistic. Rel:  https://github.com/pytorch/pytorch/pull/101760  Used optimizer with huggingface/transformers gpt2 setting as it doesn't need int64_t for indexing. The second row is to update `state_step` tensors of 1 element. **This PR**  script  command  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Use 32bit for indexing in `multi_tensor_apply` as possible,"I do think there's a better way to propagate what type to use for indexing, but I found an anonymous argument convenient to some extent as I was writing. Also, I can imagine the availability check of 32bit is a bit too optimistic. Rel:  https://github.com/pytorch/pytorch/pull/101760  Used optimizer with huggingface/transformers gpt2 setting as it doesn't need int64_t for indexing. The second row is to update `state_step` tensors of 1 element. **This PR**  script  command  ",2023-06-02T07:51:26Z,triaged open source module: mta release notes: foreach_frontend,closed,0,13,https://github.com/pytorch/pytorch/issues/102831,"Is the intended impact of this PR perf only? If so, I agree having benchmarking results would be great", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `mta/use_32bit_as_possible` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout mta/use_32bit_as_possible && git pull rebase`)",added numbers to the description, rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `mta/use_32bit_as_possible` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout mta/use_32bit_as_possible && git pull rebase`)","Hey  thanks for the benchmarks! I'm curious about these numbers. Is there any caching happening in these kernels? I'm surprised to see the min ~1us and the max being ~500us? Or are there multiple indepent calls here that get globed while they shouldn't?  Also to get a sense on the perf improvement, should we compare the two mean runtimes? Did we expect to see a bigger improvement from this or is this the expected behavior?", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `mta/use_32bit_as_possible` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout mta/use_32bit_as_possible && git pull rebase`)",closing for now as I haven't been able to see a convincing performance gain on A100 & some huggingface/transformers examples
448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo.export] `AssertionError: Dynamo attempts to add additional input during export` for Huggingface Bart)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Model source code of root cause is   Versions From source b02f48b18152ddfcf5fcbefb68f6b66a6c44b37f transformers                  4.29.2 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[dynamo.export] `AssertionError: Dynamo attempts to add additional input during export` for Huggingface Bart, ğŸ› Describe the bug Model source code of root cause is   Versions From source b02f48b18152ddfcf5fcbefb68f6b66a6c44b37f transformers                  4.29.2 ,2023-06-02T00:42:51Z,triaged oncall: pt2 module: dynamo,closed,0,7,https://github.com/pytorch/pytorch/issues/102794,This is expected behavior since export doesn't support python control flow. You could workaround this with something like:  You could also map it to torch.cond.,"Hi , what about python random package, `dropout_probability = random.uniform(0, 1)`? This triggers the `AssertionError` in title. Is it also by design not supported by export and should be worked around?",It probably is feasible to trace this into tensor operations if you're willing to change the RNG source; we're doing similar things for numpy,"you can replace `random.uniform(0, 1)` with `torch.rand([])`","I see, it will be something like what `torch_np` does that enables tracing `random.uniform(0, 1)` as `torch.rand([])`. What is dynamo's take on RNG source? Would it be considered sound to enable tracing as tensor operations by default?","Automatic would need to be optin, since the randomness would change and things like seeds would not be respected.  I'd suggest just changing the model if you want to export it.  `random.random()` should work fine with `torch.compile()` (with a graph break around it)/",Closing as workaround solution has been discussed in this thread. And that there is no immediate intention to support tracing this.
618,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo.export] `AssertionError: whole graph export entails exactly one guard export` for Huggingface T5)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug As title, it isn't immediately obvious what caused the break, or if there is any graph break at all. Any suggestions or hints to investigate this issue is greatly appreciated! Repro script below:  Error logs:   Versions From source b02f48b18152ddfcf5fcbefb68f6b66a6c44b37f transformers                  4.25.1 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[dynamo.export] `AssertionError: whole graph export entails exactly one guard export` for Huggingface T5," ğŸ› Describe the bug As title, it isn't immediately obvious what caused the break, or if there is any graph break at all. Any suggestions or hints to investigate this issue is greatly appreciated! Repro script below:  Error logs:   Versions From source b02f48b18152ddfcf5fcbefb68f6b66a6c44b37f transformers                  4.25.1 ",2023-06-01T23:39:08Z,triaged module: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/102788,Have you tried `torch._dynamo.explain()`? It's pretty great to debug this kind of issue,"Hi  , i did! Checking `explanation`, `break_reasons` and `explanation_verbose` from `explain` didn't raise anything alarming. The other three outputs `guards`, `graph` and `ops_per_graph` are pretty verbose, I'm not sure how to go about them.   ",Closing as seems to be resolved. Can no longer repro.
523,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Extremely slow will_fusion_create_cycle on nanogpt_generate)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug nanogpt_generate in the benchmark suite compiles extremely slowly. When looking at pyspy top, it looks like we spend a lot of time in will_fusion_create_cycle. I noticed the model is 40k nodes, which is probably triggering some bad asymptotics.  Versions main )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Extremely slow will_fusion_create_cycle on nanogpt_generate," ğŸ› Describe the bug nanogpt_generate in the benchmark suite compiles extremely slowly. When looking at pyspy top, it looks like we spend a lot of time in will_fusion_create_cycle. I noticed the model is 40k nodes, which is probably triggering some bad asymptotics.  Versions main ",2023-05-31T16:23:21Z,triaged bug oncall: pt2 module: inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/102622,Here is an example subgraph from the model: https://gist.github.com/ezyang/ba2050f844856235d0789a454f0cef3a  ,,"I think there's not just a performance issue, but a potential small bug in there, introduced by https://github.com/pytorch/pytorch/pull/102770 Looking at https://github.com/pytorch/pytorch/blob/c178257b40df935646dccbd75b0e8acde80aa621/torch/_inductor/scheduler.pyL1090L1100  the new ""shortcut"" returns cond0 which is always False if it reaches that point. I think the intention was to return True if a fused node is among it's own predecessors as that would constitute a cycle."," I think you've misread the code. `combined_predecessors` is the names of predecessors of the hypothetical fused node, and `node` is one of those predecessors which we're checking for cycles. So it's name is expected to be in the list of `combined_predecessors` and does not indicate a cycle. The optimization is just exploiting the fact that if all of the names in `node` are already in `combined_predecessors`, then we know the predecessors of those nodes are also in that list so they will be checked later anyway.", makes sense,Fixed by https://github.com/pytorch/pytorch/pull/102770
259,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(nanogpt_generate with dynamic shapes IMA)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Versions main)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,nanogpt_generate with dynamic shapes IMA, ğŸ› Describe the bug   Versions main,2023-05-31T15:00:02Z,module: dynamic shapes,closed,0,2,https://github.com/pytorch/pytorch/issues/102616, ,"dupe of  CC([Inductor] nanogpt_generate failed due to RuntimeError: probability tensor contains either `inf`, `nan` or element < 0)"
393,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Back out ""Add PyObject preservation for UntypedStorage (#97470)"")ï¼Œ å†…å®¹æ˜¯ (Summary: Original commit changeset: c24708d18ccb Original Phabricator Diff: D46159983 Test Plan: SL tests and CI Differential Revision: D46284986)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Back out ""Add PyObject preservation for UntypedStorage (#97470)""",Summary: Original commit changeset: c24708d18ccb Original Phabricator Diff: D46159983 Test Plan: SL tests and CI Differential Revision: D46284986,2023-05-30T17:54:29Z,fb-exported Merged Stale,closed,0,25,https://github.com/pytorch/pytorch/issues/102553," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.",This pull request was **exported** from Phabricator. Differential Revision: D46284986,This pull request was **exported** from Phabricator. Differential Revision: D46284986," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: PR CC(Back out ""Add PyObject preservation for UntypedStorage (97470)"") has not been reviewed yet Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge f 'Landed internally'," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",What was the reason for reverting this? I'd like to try to fix it as soon as possible  ,"The PR was merged, not reverted.",This is a revert PR,"I meant, what was the reason for reverting CC(Add PyObject preservation for UntypedStorage) through merging this PR?",The diff D46284986  was merged internally.,"I'm not a Meta employee, so I don't have access to D46284986. Presumably, my PR CC(Add PyObject preservation for UntypedStorage) caused some Meta internal failure, so it was reverted, is that right? I would like to help fix it if I can. I'd maybe need some way to reproduce the error",Hey ! Could you please clarify what is the problem with the changes.,"Edward shared a backtrace with me, so I'll start looking into it. But any extra info would no doubt be helpful. Evidently, the issue happens on CUDA. When I was working on making CC(Add PyObject preservation for UntypedStorage) compatible with MultiPy, I had only figured out how to get my local pytorch and multipy builds running together on CPU, and I didn't get CUDA configured properly. And I'm not sure, but it's possible that the PyTorch CI only runs MultiPy on CPU. So getting CUDA configured locally is a clear first step for me","Could someone provide a minimal reproducer? The cuda tests in multipy don't exercise the failing function `c10::newStorageImplFromRefcountedDataPtr`, and I haven't been able to reproduce the failure just from the information in the trace ","This is not going to be easy, because it looks like the person who bisected the failure did it via a big endtoend model run. However, there is an unrelated separate problem that was also pinned to this PR, looks like this:  Not sure if this is enough to work out the other problem...","Also, I looked at the SEV report for this problem, and actually it is not a failure per se; it is an OOM. So it is not surprising that it doesn't directly repro. The author tracked it down to  For some reason, the change you made caused Caffe2(!) CUDA tensor memory allocation to start going to cudaMalloc, which is what caused the OOM. Does that give a clue? The other thing we can do, is I can get you a Workplace guest account and you can directly ping the engineers who helped track down the problem.","It seems possible that the `FLAGS_caffe2_cuda_memory_pool` setting is different between the main thread and the new threads in multipy. https://github.com/pytorch/pytorch/blob/def1b57151687abd585e3000dd10907b8be01266/caffe2/core/context_gpu.cuL271 Hopefully should be simple enough for me to find out if it is different EDIT: Actually, I don't know how to check that. I guess I would need to transfer a PyTorch model to caffe2 and then somehow run it with multipy. I'm not sure how to do that","So it seems like this is what's happening: When the child threads create a new storage and allocate a CUDA buffer, they don't use a CUDA memory pool like the main thread does, for whatever reason. Potentially, the reason is that the gflag `caffe2_cuda_memory_pool` does not propagate to the child threadsand maybe it's not propagated on purpose, because maybe the memory pool doesn't work across multiple threads. (Here's where this flag is defined: https://github.com/pytorch/pytorch/blob/def1b57151687abd585e3000dd10907b8be01266/caffe2/core/context_gpu.cuL26L30). Then a cudaMalloc call on one of the child threads runs out of memory, potentially just because it's not as good at finding available memory compared to the memory pools. Regardless of the details of the CUDA memory pool flag, whether it could work across multiple threads, or why exactly the setting doesn't seem to be propagated to child threads, I think the fix might be rather simple, because the allocation that is causing the OOM error isn't even necessary. According to the trace that Edward sent me, this is the line where the OOM error is happening: https://github.com/pytorch/pytorch/pull/97470/filesdiff7a1737781f1ae70a347a8edc74e58f5cb8f91855554e5307e39f1c300467bcb1R70R74 I believe that's the only place where my PR added a new buffer allocation for child threads, and it's not necessary because the new data buffer gets replaced on the very next line with the old shared buffer that was allocated on the main thread. So I think it seems likely that I can fix the issue by just avoiding this allocation.",SGTM,"Internal note, the storage problem is reproed with ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", CC(Reland 2: Add PyObject preservation for UntypedStorage) relanded this
1072,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add mmap option to `torch.load`)ï¼Œ å†…å®¹æ˜¯ (Using `nanoGPT/model.py` run Click for script to save gpt2xlarge (1.5B params)    Click for script to load    `python test_load_gpt.py`  `python test_load_gpt.py mmap`  If we further use the `with torch.device('meta')` context manager and pull the changes from https://github.com/pytorch/pytorch/pull/102212 that allow the model to reuse tensors from the state_dict, we have  `python test_load_gpt.py mmap devicemeta`  \ \ Running the above in a docker container containing a build of PyTorch with RAM limited to 512mb by 1) running `make f docker.Makefile` from `pytorch/` directory 2) `docker run m 512m it  bash` 3) docker cp `gpt2xlarge.pth` and `test_load_gpt.py` into the image `python test_load_gpt.py` Docker will Kill the process due to OOM whereas `python test_load_gpt.py mmap devicemeta`  Stack from ghstack:  CC(Add mmap option to `torch.load`))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Add mmap option to `torch.load`,"Using `nanoGPT/model.py` run Click for script to save gpt2xlarge (1.5B params)    Click for script to load    `python test_load_gpt.py`  `python test_load_gpt.py mmap`  If we further use the `with torch.device('meta')` context manager and pull the changes from https://github.com/pytorch/pytorch/pull/102212 that allow the model to reuse tensors from the state_dict, we have  `python test_load_gpt.py mmap devicemeta`  \ \ Running the above in a docker container containing a build of PyTorch with RAM limited to 512mb by 1) running `make f docker.Makefile` from `pytorch/` directory 2) `docker run m 512m it  bash` 3) docker cp `gpt2xlarge.pth` and `test_load_gpt.py` into the image `python test_load_gpt.py` Docker will Kill the process due to OOM whereas `python test_load_gpt.py mmap devicemeta`  Stack from ghstack:  CC(Add mmap option to `torch.load`)",2023-05-30T17:44:41Z,Merged ciflow/trunk release notes: python_frontend topic: new features,closed,1,10,https://github.com/pytorch/pytorch/issues/102549, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / test (default, 3, 3, windows.4xlarge.nonephemeral) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"  This is great stuff. By looking the PR, it seems that the following piece is the actual responsible for doing the memorymap load?  Once `state_dict = torch.load(..., mmap=True, map_location=""cpu"")` returns, the returned `state_dict` will not be memorymapped, right? From memory consumption point of view, if `state_dict` must fit memory once `torch.load` returns, how using `mmap` during `torch.load` helps exactly? Is it because `f` will not occupy much memory, allowing more space for `state_dict` to use it all? That is, reducing the load memory footprint from 2x to only 1x? That makes sense to me, but I would like to confirm that is correct Would be conceptually conceivable to have `state_dict` as a memorymapped dict? I found this https://mmappickle.readthedocs.io/en/latest/ that is related","Another question is related to `torch.save`. Does it `torch.save(...,_use_new_zipfile_serialization=True)` imply memory map will also be leveraged internally? I ask that because when `torch.save(...,_use_new_zipfile_serialization=False)`, and I try to laod that state_dict in memory with `torch.load(...,mmap=True)`, I get an error like `mmap can only be used with files saved with `torch.save(_use_new_zipfile_serialization=True), please torch.save your checkpoint with this option in order to use mmap.`","What I am trying to assess is if I can do something like  such that memory memorymap is used to read and keep `f""/path/to/checkpoint_{i}.bin""` in virtual memory, never depending on the memory being big enough to fit all large state_dict in memory. The `mmap_file_writer` implements `mmap` under the hood and can properly serialize the contents into disk","   Comment 1 > Once state_dict = torch.load(..., mmap=True, map_location=""cpu"") returns, the returned state_dict will not be memorymapped, right? You are right that this line memory maps the entire checkpoint file  Observe that the `Unpickler` has an special case for unpickling storages (see `_load:persistent_load` which calls into `_load:load_tensor`)  What this means is that the **Storage** of the each tensor within the `state_dict` is memorymapped, even after `state_dict = torch.load(..., mmap=True, map_location=""cpu"")` returns.  Comment 2  > torch.save(...,_use_new_zipfile_serialization=True) This indicates that the checkpoint is written to file using the `PyTorchStreamWriter`, which provides the following guarantees https://github.com/pytorch/pytorch/blob/74e13624998f2a4de29bce73a949d7f0339ec04e/caffe2/serialize/inline_container.hL44L50  Comment 3 I did not quite follow your last comment, could you elaborate more on what you are trying to achieve please"," Thank you for the quick response! Replies below  Comment 1 > > Once state_dict = torch.load(..., mmap=True, map_location=""cpu"") returns, the returned state_dict will not be memorymapped, right? >  > You are right that this line memory maps the entire checkpoint file >  >  >  > Observe that the `Unpickler` has an special case for unpickling storages (see `_load:persistent_load` which calls into `_load:load_tensor`) >  >  >  > What this means is that the **Storage** of the each tensor within the `state_dict` is memorymapped, even after `state_dict = torch.load(..., mmap=True, map_location=""cpu"")` returns. This is exactly what I need. To have `state_dict` memorymapped :)  Comment 2 > > torch.save(...,_use_new_zipfile_serialization=True) >  > This indicates that the checkpoint is written to file using the `PyTorchStreamWriter`, which provides the following guarantees >  > https://github.com/pytorch/pytorch/blob/74e13624998f2a4de29bce73a949d7f0339ec04e/caffe2/serialize/inline_container.hL44L50 According to bullet 2. from `inline_container.h`, `f` argument from `torch.save(obj, f)` is ""compatible"" with memorymap, but it is up to the user to create a mmap'ed `f` and pass it to `torch.save`, right?  I've never seen such code, though.  Comment 3 > I did not quite follow your last comment, could you elaborate more on what you are trying to achieve please My TL;DR goal is to create a script that can converts a collection of pytorch checkpoints into another checkpoint file format, like ONNX, GGUF, etc using mmap from endtoend due to memory constraints. For this, I'd need to `torch.load` several files with mmap, with the assumption each of the files doesn't fit RAM memory individually. As per your reply, this is already possible with PR, so `state_dict` from  `state_dict = torch.load(path, mmap=True, map_location=""cpu"")` is not consuming all the actual size, but just a certain amount as per OS implementation. The next step is to iterate over each tensor element from the set of `state_dict` and save them in the new format. Given `state_dict` as returned from `torch.load(, mmap=True)` is using memory mapped storage, a better snippet might be  Does it make sense?"
666,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP enable users to seamlessly switch between DDP, ZeRO-1, ZeRO-2 and FSDP flavors of data parallelism)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch https://pytorch.org/blog/introducingpytorchfullyshardeddataparallelapi/ DeepSpeed is hard to use,  FSDP is easy to use for LLM, but FSDP do not support ZeRO1, ZeRO2,  we suggest FSDP enable users to seamlessly switch between DDP, ZeRO1, ZeRO2 and FSDP flavors of data parallelism  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"FSDP enable users to seamlessly switch between DDP, ZeRO-1, ZeRO-2 and FSDP flavors of data parallelism"," ğŸš€ The feature, motivation and pitch https://pytorch.org/blog/introducingpytorchfullyshardeddataparallelapi/ DeepSpeed is hard to use,  FSDP is easy to use for LLM, but FSDP do not support ZeRO1, ZeRO2,  we suggest FSDP enable users to seamlessly switch between DDP, ZeRO1, ZeRO2 and FSDP flavors of data parallelism  Alternatives _No response_  Additional context _No response_ ",2023-05-30T13:20:09Z,triaged module: fsdp,closed,0,10,https://github.com/pytorch/pytorch/issues/102532," ZeRO2 corresponds to `ShardingStrategy.SHARD_GRAD_OP`.  ZeRO1 corresponds to `SHARD_GRAD_OP` with gradient accumulation with `no_sync()`. Otherwise, there is no point to use ZeRO1 since ZeRO2 is strictly better.  FSDP's version of DDP is `ShardingStrategy.NO_SHARD`. These are all configured by passing to the `sharding_strategy` argument. Could you also explain more on why DeepSpeed is hard to use? We are thinking about API design lately, and I would love to get more feedback to understand the pain points.","> * ZeRO2 corresponds to `ShardingStrategy.SHARD_GRAD_OP`. > * ZeRO1 corresponds to `SHARD_GRAD_OP` with gradient accumulation with `no_sync()`. Otherwise, there is no point to use ZeRO1 since ZeRO2 is strictly better. > * FSDP's version of DDP is `ShardingStrategy.NO_SHARD`. >  > These are all configured by passing to the `sharding_strategy` argument. >  > Could you also explain more on why DeepSpeed is hard to use? We are thinking about API design lately, and I would love to get more feedback to understand the pain points. Will FSDP support ZeRO3 (parameterspartition)? ", ZeRO3 is the default for FSDP (`FULL_SHARD`) :),">  ZeRO3 is the default for FSDP (`FULL_SHARD`) :) lol, Thx!  ","> * ZeRO2 corresponds to `ShardingStrategy.SHARD_GRAD_OP`. > * ZeRO1 corresponds to `SHARD_GRAD_OP` with gradient accumulation with `no_sync()`. Otherwise, there is no point to use ZeRO1 since ZeRO2 is strictly better. > * FSDP's version of DDP is `ShardingStrategy.NO_SHARD`. >  > These are all configured by passing to the `sharding_strategy` argument. >  > Could you also explain more on why DeepSpeed is hard to use? We are thinking about API design lately, and I would love to get more feedback to understand the pain points. but zero2 is slower than zero1? ", I think it depends on what you consider ZeRO1. Could you explain why ZeRO2 is slower than ZeRO1?,>  I think it depends on what you consider ZeRO1. Could you explain why ZeRO2 is slower than ZeRO1? I think zero2 has the overhead of gradient shard merging,"I am not familiar with ""gradient shard merging"". Could you explain what that is?","> I am not familiar with ""gradient shard merging"". Could you explain what that is? I think what  means is ZeRO2 have to do gradient sync (reducescatter) after every microbatch when with gradient accumulation, but ZeRO1 only sync once."," Got it! For FSDP, there is a `no_sync` option that allows for ZeRO1 semantics when microbatching with gradient accumulation."
808,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(before the PR, running , dynamo will create a graph break when executing NNModuleVariable.call_method and raise unimplemented error for name=_conv_forward. see issue for full detail: https://github.com/pytorch/pytorch/issues/101155)ï¼Œ å†…å®¹æ˜¯ (  CC(before the PR, running , dynamo will create a graph break when executing NNModuleVariable.call_method and raise unimplemented error for name=_conv_forward. see issue for full detail:  CC([Dynamo] Graph break: call_method NNModuleVariable() _conv_forward)) after the PR, for torch.nn.conv module with function name _conv_forward, we inline the function with tx.inline_user_function_return )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,"before the PR, running , dynamo will create a graph break when executing NNModuleVariable.call_method and raise unimplemented error for name=_conv_forward. see issue for full detail: https://github.com/pytorch/pytorch/issues/101155","  CC(before the PR, running , dynamo will create a graph break when executing NNModuleVariable.call_method and raise unimplemented error for name=_conv_forward. see issue for full detail:  CC([Dynamo] Graph break: call_method NNModuleVariable() _conv_forward)) after the PR, for torch.nn.conv module with function name _conv_forward, we inline the function with tx.inline_user_function_return ",2023-05-30T03:06:41Z,module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/102499,"   :x: The commit (9c88b043a2ef9810d5b3a2f29bdc65b0a134e449). This user is missing the User's ID, preventing the EasyCLA check. Consult GitHub Help to resolve.For further assistance with EasyCLA, please submit a support request ticket.",close because of wrong CLA config. will create a new one with meta email address and correct diff title
1990,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(multi-processing DataLoader of 2.0.1 is much slower than 1.13.1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm pretraining GPT2 using the new 2.0.1 version. But I found that the speed of the DataLoder is much slower than previous version. My cpu is 8core16thread. When I run this code in 1.13.1, the CPU utility is very high and only cost 36 seconds. But when I switch to 2.0.1, it seems like only 2 cpu cores are in fully usage, and it took 3m48s to run.   Versions Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.070genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   43 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          16 Online CPU(s) list:             015 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen 7 3700X 8Core Processor CPU family:                      23 Model:                           113 Thread(s) per core:              2 Core(s) per socket:              8 Socket(s):                       1 Stepping:                  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,multi-processing DataLoader of 2.0.1 is much slower than 1.13.1," ğŸ› Describe the bug I'm pretraining GPT2 using the new 2.0.1 version. But I found that the speed of the DataLoder is much slower than previous version. My cpu is 8core16thread. When I run this code in 1.13.1, the CPU utility is very high and only cost 36 seconds. But when I switch to 2.0.1, it seems like only 2 cpu cores are in fully usage, and it took 3m48s to run.   Versions Collecting environment information... PyTorch version: 2.0.1 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04.1) 11.3.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.070genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   43 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          16 Online CPU(s) list:             015 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen 7 3700X 8Core Processor CPU family:                      23 Model:                           113 Thread(s) per core:              2 Core(s) per socket:              8 Socket(s):                       1 Stepping:                  ",2023-05-29T23:28:57Z,,closed,0,6,https://github.com/pytorch/pytorch/issues/102494,"Hi, since installing/downloading GPT model & datasets are expensive, would you try if this simple dummy dataloader can reproduce the issue on 2.0.1?  I did not reproduce the issue on my Intel CPU.",This seems like another case of  CC(Import of torch breaks standard multiprocessing) (my issue) and  CC(Unexpected modification to CPU affinity of Dataloader workers),"> Hi, since installing/downloading GPT model & datasets are expensive, would you try if this simple dummy dataloader can reproduce the issue on 2.0.1? >  >  >  > I did not reproduce the issue on my Intel CPU. Thanks for your reply! I find the reason is that Huggingface's  was imported before the . The codes below take 23.6 seconds with only 2 CPU cores fully used, even though I didn't really use the transformers.  And if I import the  before the , it only takes 5.4 seconds and the CPU utilization is high.  Then I did more tests, I found it was not PyTorch 2's problem. This issue exists in both 2.0.1 and 1.13.1. It's caused by the . The 4.26.1 doesn't have this issue. I think it's still strange for me but at least I got a solution for now. I will report this issue to Huggingface as well.",> This seems like another case of CC(Import of torch breaks standard multiprocessing) (my issue) and CC(Unexpected modification to CPU affinity of Dataloader workers) Thanks for your reply. I find the order of importing  changes the behavior of multiprocessing. Maybe you can try to import the torch at the beginning of the file.,"Hi  , as Stefan pointed out, from the previous discussions in CC(Unexpected modification to CPU affinity of Dataloader workers) , you can try to install Intel OpenMP and make it effective instead of llvmopenmp. I think your findings are also due that transformers have another openmp dependency and the new openmp lib flushed llvmopenmp invoked by torch.",>  I will try it. Thanks again!
349,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add storage dtype for custom device)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER 1ã€add `isinstance` check with dtyped storage for custom device 2ã€add `storage.type()` support for custom device)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,add storage dtype for custom device,Fixes ISSUE_NUMBER 1ã€add `isinstance` check with dtyped storage for custom device 2ã€add `storage.type()` support for custom device,2023-05-29T12:20:05Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/102481, merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `stroage_utils` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout stroage_utils && git pull rebase`)"," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,> Only small details about the test but sounds good otherwise! yeah I have fixed it
981,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP very slow on multi-node training)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I try to train model using torch.distributed.FullyShardedDataParallel, I found that : when training using singlenode multigpu (1x8A100), the training speed is normal. when training using multinode multigpu(2x8A100 or 4x8A100), the training speed is very slow. My FSDP code is as follows:  I print out the training speed, results as as follows  (three lines, first line is load data time, second is model inference and calculate loss time, last is backward() time): First is the speed using 4x8A100, the model inference is very slow.  Then is the speed using 1x8A100, the model inference is perfectly normal:  could someone tell me why this is happening? My test code:   Versions My versions:  My driver version: !ä¼ä¸šå¾®ä¿¡æˆªå›¾_16851761668393 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,FSDP very slow on multi-node training," ğŸ› Describe the bug When I try to train model using torch.distributed.FullyShardedDataParallel, I found that : when training using singlenode multigpu (1x8A100), the training speed is normal. when training using multinode multigpu(2x8A100 or 4x8A100), the training speed is very slow. My FSDP code is as follows:  I print out the training speed, results as as follows  (three lines, first line is load data time, second is model inference and calculate loss time, last is backward() time): First is the speed using 4x8A100, the model inference is very slow.  Then is the speed using 1x8A100, the model inference is perfectly normal:  could someone tell me why this is happening? My test code:   Versions My versions:  My driver version: !ä¼ä¸šå¾®ä¿¡æˆªå›¾_16851761668393 ",2023-05-27T08:38:30Z,triaged module: fsdp,closed,0,28,https://github.com/pytorch/pytorch/issues/102434,"Maybe your workload is communication bound, and when you go from singlenode to multinode, FSDP's communications (allgather / reducescatter) and heavily exposed on the critical path? I would recommend you collect profiler traces for both the singlenode and multinode cases. https://pytorch.org/docs/stable/profiler.html  By the way, regarding: > (three lines, first line is load data time, second is model inference and calculate loss time, last is backward() time): If I am understanding correctly, the second time is actually the forward _and_ backward time, and the third time is the optimizer step? (It would be strange if forward takes much longer than backward.)","> Maybe your workload is communication bound, and when you go from singlenode to multinode, FSDP's communications (allgather / reducescatter) and heavily exposed on the critical path? >  > I would recommend you collect profiler traces for both the singlenode and multinode cases. https://pytorch.org/docs/stable/profiler.html >  > By the way, regarding: >  > > (three lines, first line is load data time, second is model inference and calculate loss time, last is backward() time): >  > If I am understanding correctly, the second time is actually the forward _and_ backward time, and the third time is the optimizer step? (It would be strange if forward takes much longer than backward.) Yes, the second time is actually the forward and backward time. Can I ask what do  means? Can you be more specific? Thanks","> Maybe your workload is communication bound, and when you go from singlenode to multinode, FSDP's communications (allgather / reducescatter) and heavily exposed on the critical path? >  > I would recommend you collect profiler traces for both the singlenode and multinode cases. https://pytorch.org/docs/stable/profiler.html >  > By the way, regarding: >  > > (three lines, first line is load data time, second is model inference and calculate loss time, last is backward() time): >  > If I am understanding correctly, the second time is actually the forward _and_ backward time, and the third time is the optimizer step? (It would be strange if forward takes much longer than backward.) I further tested the forward time and backward time, results are as follows: multinode:  singlenode:  according to my observation is seems that both inference and backward are slowed down in multinode training.","FSDP uses collectives communications: allgather for parameters and reducescatter for gradient reduction. The forward pass only uses allgather, whereas the backward pass uses both allgather and reducescatter (meaning twice as much communication as forward). If communication is exposed on the critical path, then it is not overlapped with computation. For multinode, the communication may take longer due to using slower internode bandwidth, which may make communication more easily exposed. The times you are getting look unintuitive to me. As I mentioned before, I would recommend getting a profiler trace. Then, it will be clear what is going on. In addition, I would not recommend using `time.time()` to time your program since it may not accurately capture _GPU_ kernel execution. Instead, you can use CUDA events: https://auro227.medium.com/timingyourpytorchcodefragmentse1a556e81f2",">  OK, thank you. I will try to calculate time and update soon. Could I please ask about what does  means?","Critical path refers to the ops that actually affect your endtoend time. An op is not on the critical path if it is fully overlapped with other ops, which can happen since communication and computation can use separate GPU resources. For example, if FSDP can allgather the 'next' layer's parameters before finishing the 'current' layer's forward computation, then that allgather is not on the critical path because by the time we run the 'next' layer's forward computation, we already have the parameters materialized. On the other hand, if the 'next' allgather takes longer than the 'current' computation, then the part that is not overlapped is _exposed_ and delays the 'next' computation. I highly recommend looking at profiler traces to make these ideas concrete.","> Critical path refers to the ops that actually affect your endtoend time. An op is not on the critical path if it is fully overlapped with other ops, which can happen since communication and computation can use separate GPU resources. For example, if FSDP can allgather the 'next' layer's parameters before finishing the 'current' layer's forward computation, then that allgather is not on the critical path because by the time we run the 'next' layer's forward computation, we already have the parameters materialized. On the other hand, if the 'next' allgather takes longer than the 'current' computation, then the part that is not overlapped is _exposed_ and delays the 'next' computation. >  > I highly recommend looking at profiler traces to make these ideas concrete. I further tested the speed using following code:  Results are as follows: single node (8xA100)  multinode (16xA100):  It seems that in multinode training, cuda event time is 10x slower.  I also tested torch2.1+cu117, results are similiar. I will try torch.profiler later.","These are my distributed init code for possible bug checking  :  Besides, I want to ask whether the  in my FSDP init correct? I tested passing  and , which are both normal","These are results I gathered using , I warmup for 10 iterations and calculate for 10 active iterations. First is the  result  I don't quite understand all these metrics, but I can see that: 1. The cuda time consumes are most on  and , this is reasonable 2. from  and  it seems that twonode is 10x slower than singlenode for both gather and matmul, this is very weird. Something most be wrong, maybe I set FSDP wrong, but I can't find where.","Multinode:  Singlenode:  I do not see the matmuls taking more time (you can similarly check `aten::bmm` for batched matmul). I mainly see that the communications take 10x longer on multinode than singlenode. This may just be because of your internode network bandwidth being 10x slower than your intranode network bandwidth. I am not sure how you can check your internode connection type, but for example, if it is Ethernet (as opposed to Infiniband), the 10x slowdown is not unreasonable to me.","> Multinode: >  >  >  > Singlenode: >  >  >  > I do not see the matmuls taking more time (you can similarly check `aten::bmm` for batched matmul). I mainly see that the communications take 10x longer on multinode than singlenode. This may just be because of your internode network bandwidth being 10x slower than your intranode network bandwidth. I am not sure how you can check your internode connection type, but for example, if it is Ethernet (as opposed to Infiniband), the 10x slowdown is not unreasonable to me. Sorry, I see it wrong, the main time consume comes from , , , , are these all communication operations? The internode slowdown you mentioned may be the reason, but I train smaller model using DDP, there is no such a big gap. Is it possible that I configure FSDP wrong?","DDP only uses allreduce for communication and not allgather/reducescatter. Allreduce is more optimized in practice. At the same time, how much larger is your FSDP model than DDP model? This affects the communication volume.","> DDP only uses allreduce for communication and not allgather/reducescatter. Allreduce is more optimized in practice. At the same time, how much larger is your FSDP model than DDP model? This affects the communication volume. much smaller, the FSDP model is a LLAMA13B model plus some linear layers, the DDP only tunes linear layers","I am going to mark this as closed because I no evidence to suggest this is an issue with FSDP. From our discussion, it seems that you have a slow internode interconnect.","Finally, I set instead of  as https://pytorch.org/docs/stable/fsdp.htmltorch.distributed.fsdp.ShardingStrategy says, it turns out that multinode training is faster. As I observe, I use 4xnode training time of a single iteration is about 3x slower, but overall, training is faster.","> Finally, I set `ShardingStrategy=HYBRID_SHARD`instead of `FULL_SHARD` as https://pytorch.org/docs/stable/fsdp.htmltorch.distributed.fsdp.ShardingStrategy says, it turns out that multinode training is faster. As I observe, I use 4xnode training time of a single iteration is about 3x slower, but overall, training is faster. Hi, JulioZhao97, do you have your full code with training with FSDP in multinode? Can you share that with me? appreciate","> > Finally, I set `ShardingStrategy=HYBRID_SHARD`instead of `FULL_SHARD` as https://pytorch.org/docs/stable/fsdp.htmltorch.distributed.fsdp.ShardingStrategy says, it turns out that multinode training is faster. As I observe, I use 4xnode training time of a single iteration is about 3x slower, but overall, training is faster. >  > Hi, JulioZhao97, do you have your full code with training with FSDP in multinode? Can you share that with me? appreciate  wrap model is something like this, mainly adapted from  and .","> > > Finally, I set `ShardingStrategy=HYBRID_SHARD`instead of `FULL_SHARD` as https://pytorch.org/docs/stable/fsdp.htmltorch.distributed.fsdp.ShardingStrategy says, it turns out that multinode training is faster. As I observe, I use 4xnode training time of a single iteration is about 3x slower, but overall, training is faster. > >  > >  > > Hi, JulioZhao97, do you have your full code with training with FSDP in multinode? Can you share that with me? appreciate >  >  >  > wrap model is something like this, mainly adapted from `LLAVA` and `Huggingface`. Thanks! btw, are there any changes in the master file of the training? I used fsdp to run successfully on single node, but now I want to run on multinode, and I don't know how to write the code. ","As far as I concern the multinode training and singlenode training is basicly the same? If you are running in a slurm cluster, the srun parameters need to be carefully set. Besides, what is your error in multinode training?  ","> As far as I concern the multinode training and singlenode training is basicly the same? If you are running in a slurm cluster, the srun parameters need to be carefully set. Besides, what is your error in multinode training?  here are my script: `!/bin/bash SBATCH jobname=bloom SBATCH nodes=2 SBATCH ntaskspernode=4 SBATCH cpuspertask=8 SBATCH mem=64gb SBATCH gres=gpu:4 export MASTER_PORT=11343 export WORLD_SIZE=8 echo ""NODELIST=""${SLURM_NODELIST} master_addr=$(scontrol show hostnames ""$SLURM_JOB_NODELIST"" | head n 1) export MASTER_ADDR=$master_addr echo ""MASTER_ADDR=""$MASTER_ADDR srun train.py \ model_name_or_path news_bloom_7b1_qa \ data_path ./news_training.json \ bf16 True \ output_dir ./test_multinodetraining/ \ num_train_epochs 4 \ per_device_train_batch_size 2 \ per_device_eval_batch_size 2 \ gradient_accumulation_steps 4 \ evaluation_strategy ""no"" \ save_strategy ""steps"" \ save_steps 1000 \ save_total_limit 1 \ learning_rate 2e5 \ weight_decay 0. \ warmup_ratio 0.03 \ lr_scheduler_type ""cosine"" \ logging_steps 1 \ fsdp ""full_shard auto_wrap offload"" \ fsdp_transformer_layer_cls_to_wrap 'BloomBlock' \ tf32 True` and I applied for 2 nodes and 4 cards on the cluster, for a total of 8 cards. the error is `slurmstepd: error: execve(): train.py: Permission denied` .Can you help me see if there are any obvious errors in my sh script?"," Hi, the above problem is solved, but there is a new problem: ValueError: Using fsdp only works in distributed training. Can I see your setting in the main training file, please? My main reference is this link: https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904",">  Hi, the above problem is solved, but there is a new problem: ValueError: Using fsdp only works in distributed training. Can I see your setting in the main training file, please? My main reference is this link: https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904 How did you launch the job? You should launch job using  or  to run your script.", see this:https://github.com/huggingface/transformers/blob/7631db0fdcfbd95b1f21d8034a0b8df73b9380ff/src/transformers/trainer.pyL446,">  see this:https://github.com/huggingface/transformers/blob/7631db0fdcfbd95b1f21d8034a0b8df73b9380ff/src/transformers/trainer.pyL446   my runing script is : `!/bin/bash SBATCH jobname=bloom SBATCH nodes=2 SBATCH ntaskspernode=2 SBATCH cpuspertask=8 SBATCH mem=64gb SBATCH gres=gpu:2 SBATCH mailtype=ALL SBATCH mailuser=790567648.com export MASTER_PORT=11343 export WORLD_SIZE=4 echo ""NODELIST=""${SLURM_NODELIST} master_addr=$(scontrol show hostnames ""$SLURM_JOB_NODELIST"" | head n 1) export MASTER_ADDR=$master_addr echo ""MASTER_ADDR=""$MASTER_ADDR srun python train.py \ model_name_or_path bigscience/bloom7b1 \ data_path ./news_training.json \ bf16 True \ output_dir ./test_multinodetraining/ \ num_train_epochs 4 \ per_device_train_batch_size 1 \ per_device_eval_batch_size 1 \ gradient_accumulation_steps 4 \ evaluation_strategy ""no"" \ save_strategy ""steps"" \ save_steps 1000 \ save_total_limit 1 \ learning_rate 2e5 \ weight_decay 0. \ warmup_ratio 0.03 \ lr_scheduler_type ""cosine"" \ logging_steps 1 \ fsdp ""full_shard auto_wrap offload"" \ fsdp_transformer_layer_cls_to_wrap 'BloomBlock' \ tf32 True` Did you mean `srun torchrun train.py`?",Same problem for me: multinode training yields a linearly **increased** time........................ Can anyone help?  ,Thanks for your great discussions! I also meet the problem that multinode training is much slower than singlenode. The time for twonode training takes two times longer than singlenode's. My training doesn't use FSDP. Do you have some suggestions? Thank you very much!  ,"> Thanks for your great discussions! I also meet the problem that multinode training is much slower than singlenode. The time for twonode training takes two times longer than singlenode's. My training doesn't use FSDP. Do you have some suggestions? Thank you very much!  I suggest you check timing using  as discussed above, it is most likely that the communication between 2 nodes is very slow, you can check with your cluster manager.","Maybe try Deepspeed, it works perfectly well in my 8 nodes machine. FSDP just didn't work."
1876,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with ShufflerIterDataPipe in torch 1.13.1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I am trying to finetune a model in PyTorch using multiple GPUs. For this purpose I load a base model from `transformers` and I use the library `accelerate` to handle the multigpu setting. I have also created a new dataset class which inherits from `IterableDataset`. In that class, I define a shuffle function which make use of `ShufflerIterDataPipe`, and I use it to shuffle my dataset. From this dataset, I define a dataloader (using `DataLoader` from pytorch) and I go trough it with the help of a for loop. TL;DR : The code works fine with PyTorch `1.10.1` but fails with PyTorch `1.13.1` due to `ShufflerIterDataPipe` Here is my code  However when I use PyTorch `1.13.1`, I have an issue :  When I do not shuffle my dataset, which implies not using `ShuffleIterDataPipe`, my code works perfectly whether I use 1 GPU or 4 GPUs.  When I do shuffle my dataset, which implies using `ShufflerIterDataPipe`, my code works only when I use 1 GPU. And it does not work when I use more GPUs (typically 4 or 8). The error is `RuntimeError: Trying to create tensor with negative dimension 327818479245320786: [327818479245320786]` On the other hand, when I use PyTorch `1.10.1` there is no issue when using `ShuffleIterDataPipe`, whether I run my code with 1 or 4 GPUs. Can you give me some information about why it happens? Maybe there is a problem with `ShufflerIterDataPipe` when I use multiple GPUs. The update from torch `1.10.1` to torch `1.13.1` may have changed something. The whole trace in case of error  The expected output should be like    Versions Environment with torch `1.13.1`  Environment with torch `1.10.1`  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Issue with ShufflerIterDataPipe in torch 1.13.1," ğŸ› Describe the bug Hi, I am trying to finetune a model in PyTorch using multiple GPUs. For this purpose I load a base model from `transformers` and I use the library `accelerate` to handle the multigpu setting. I have also created a new dataset class which inherits from `IterableDataset`. In that class, I define a shuffle function which make use of `ShufflerIterDataPipe`, and I use it to shuffle my dataset. From this dataset, I define a dataloader (using `DataLoader` from pytorch) and I go trough it with the help of a for loop. TL;DR : The code works fine with PyTorch `1.10.1` but fails with PyTorch `1.13.1` due to `ShufflerIterDataPipe` Here is my code  However when I use PyTorch `1.13.1`, I have an issue :  When I do not shuffle my dataset, which implies not using `ShuffleIterDataPipe`, my code works perfectly whether I use 1 GPU or 4 GPUs.  When I do shuffle my dataset, which implies using `ShufflerIterDataPipe`, my code works only when I use 1 GPU. And it does not work when I use more GPUs (typically 4 or 8). The error is `RuntimeError: Trying to create tensor with negative dimension 327818479245320786: [327818479245320786]` On the other hand, when I use PyTorch `1.10.1` there is no issue when using `ShuffleIterDataPipe`, whether I run my code with 1 or 4 GPUs. Can you give me some information about why it happens? Maybe there is a problem with `ShufflerIterDataPipe` when I use multiple GPUs. The update from torch `1.10.1` to torch `1.13.1` may have changed something. The whole trace in case of error  The expected output should be like    Versions Environment with torch `1.13.1`  Environment with torch `1.10.1`  ",2023-05-26T12:43:58Z,module: dataloader triaged module: data,open,0,2,https://github.com/pytorch/pytorch/issues/102368,Does this issue persist on the latest version 2.0.1 as well?,"IIUC, The traceback shows the Error comes from `accelerate`."
1246,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensors that share same underlying storage to also share gradient storage)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Hello Pytorch Team!  I have a question concerning the way gradients are handled, in particular grad attribute. It's a bit hard to explain but in a very short manner I want ""views"" of a given parameter to have ""synced"" gradients with the original parameter (shared memory storage). In short the following script tries to mimick what I want to do.  In this example I'm using `view` but I'd like to support things like slices as well. So very concretely what I'm interested in is customising a grad getter. This would allow one to have a syncing mechanism that doesn't rely on the backward hook for syncing. Solution found as of now:   Creating a new property of a new class:    The only issue is that property is not something we can set on an instance, hence the need to define a new class. Maybe this solution can generalize by allowing for torch.Tensor to pass grad getter/setter/deleter    Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tensors that share same underlying storage to also share gradient storage," ğŸš€ The feature, motivation and pitch Hello Pytorch Team!  I have a question concerning the way gradients are handled, in particular grad attribute. It's a bit hard to explain but in a very short manner I want ""views"" of a given parameter to have ""synced"" gradients with the original parameter (shared memory storage). In short the following script tries to mimick what I want to do.  In this example I'm using `view` but I'd like to support things like slices as well. So very concretely what I'm interested in is customising a grad getter. This would allow one to have a syncing mechanism that doesn't rely on the backward hook for syncing. Solution found as of now:   Creating a new property of a new class:    The only issue is that property is not something we can set on an instance, hence the need to define a new class. Maybe this solution can generalize by allowing for torch.Tensor to pass grad getter/setter/deleter    Alternatives _No response_  Additional context _No response_",2023-05-26T05:08:05Z,module: autograd triaged needs research module: python frontend,open,1,12,https://github.com/pytorch/pytorch/issues/102337,Hi !  Tangential to this issue: I recall seeing that you were interested in this in relation to ZeRO. I was curious what the use case was and why existing solutions are insufficient.,"Right! The reason why I need this is I need to propagate `grad` to the parameters I'm passing to the optimizer. Basically it very roughly looks like that:  Typically `model.weight.grad` gets populated during backward pass, but it never goes ""up"" to `sliced_param`. There might be a way to hack the auto grad system, by first defining `sliced_param` and computing `model.weight` afterwards, but it really sounds like a system when writing a custom getter allows for syncing with no regards to backward pass (typically if for X reasons someone wants to play with `.grad` attribute.","Would something like the following work for you: We add a new `t.register_grad_hook(hook)` where `hook(self, original_grad) > new_grad` that will be called when the python `.grad` field is used. Note that we will need to update the warning in the default getter to make sure it doesn't fire spuriously when a hook is registered. In this case, you can have  ?","So this would work as a getter, so do we just disable setter/deleter? Btw yes, this would work for me.","If you want setter/deleter, we can do `register_grad_hook(getter_hooks, setter=setter_hook, deleter=deleter_hook)`. The problem is that the `.grad` field is not actually populated from python by the autograd engine. So the setter_hook might be a bit surprising. Also the engine can do `t.grad += new_grad` which wouldn't trigger any of these. To avoid these questions, I limited my proposal to just a getter. But if we can flesh this out for all 3 if you want.","Oh just a question, I'm fine with just a getter.",The subclass solution doesn't seem too bad for this use case actually.  do you run into any issues using your subclass? But I suppose .grad hooks would be easier to discover for the next person who wants something similar.,"One issue with your Tensor subclass is that whatever operations you do later on will also have to be done with Tensor subclasses, which can reduce perf in certain cases. A way to fix that is to use a torch dispatch wrapper subclass, which automatically unwraps into a plain old torch.Tensor when you do a nonview operation.    click to see example implementation   ","With the solution given, is it expected that `data_ptr` method doesn't work on the subclass?  I can always override it, but I was under the impression that `__torch_dispatch__` would magically unwrap my custom class into torch tensor anywhere required?",You will have to override this method if you want to. Otherwise data_ptr() is returning the actual storage pointer. Which is nullptr > 0 here.,Hum maybe I don't understand why it's not passing through the torch dispatch mechanism? Normal tensor subclassing would've worked normally. Also do you know which methods are impacted?,"I don't think we have a list no :/ The methods that are handled are all the native functions (as registered to the dispatcher) and special handling for metada (size, stride, storage offset, etc). Any other function that is pythononly (like numpy conversion for example) or too low level concepts (data_ptr) are not caught."
2046,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(transformer encoder-layer, the sample-Independent attn_mask(dim=3) has different behaviors when training and validating)ï¼Œ å†…å®¹æ˜¯ (in torch.nn.modules.transormer line 469  TransformerEncoderLayer.forward() https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.pyL500 it supports attn_mask with dim=3, whose first dim represent diff sample in batch,  however in this function(at line 543), it invokes the attention.merge_masks()  which only support the attn_mask with dim=2 and do nothing for the raw input attn_mask with dim=3. https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.pyL588 **encoderlayer.forward(): input args _src_mask_ was describe in Transformer.forward() which said:**  **the invoke point:**  **the defination:**  the code logic is so simple so there is no use case, please start with torch.nn.modules.transormer line 469 to line 543 I just simply check the latest version, Probably the bug still exists.  Versions Collecting environment information... PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.1 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.25.1 Libc version: N/A Python version: 3.9.16 (main, Mar  8 2023, 04:29:24)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==2.0.0 [pip3] torchaudio==2.0.0 [pip3] torchvision==0.15.0 [conda] numpy                     1.23.5           py39h1398885_0    https://)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"transformer encoder-layer, the sample-Independent attn_mask(dim=3) has different behaviors when training and validating","in torch.nn.modules.transormer line 469  TransformerEncoderLayer.forward() https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.pyL500 it supports attn_mask with dim=3, whose first dim represent diff sample in batch,  however in this function(at line 543), it invokes the attention.merge_masks()  which only support the attn_mask with dim=2 and do nothing for the raw input attn_mask with dim=3. https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.pyL588 **encoderlayer.forward(): input args _src_mask_ was describe in Transformer.forward() which said:**  **the invoke point:**  **the defination:**  the code logic is so simple so there is no use case, please start with torch.nn.modules.transormer line 469 to line 543 I just simply check the latest version, Probably the bug still exists.  Versions Collecting environment information... PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.1 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.25.1 Libc version: N/A Python version: 3.9.16 (main, Mar  8 2023, 04:29:24)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==2.0.0 [pip3] torchaudio==2.0.0 [pip3] torchvision==0.15.0 [conda] numpy                     1.23.5           py39h1398885_0    https://",2023-05-26T04:13:53Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/102333
501,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.nn.Transformer cannot be compiled)ï¼Œ å†…å®¹æ˜¯ ( Issue description I intended to use torch.nn.Transformer to build my model, and use torch.compile to compile it. But I found that the model is still eager mode even if I used torch.compile  Code example   PyTorch or Caffe2: Pytorch  PyTorch version: 2.0.0  Python version: 3.8 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.nn.Transformer cannot be compiled," Issue description I intended to use torch.nn.Transformer to build my model, and use torch.compile to compile it. But I found that the model is still eager mode even if I used torch.compile  Code example   PyTorch or Caffe2: Pytorch  PyTorch version: 2.0.0  Python version: 3.8 ",2023-05-26T03:10:20Z,triaged oncall: pt2,closed,0,7,https://github.com/pytorch/pytorch/issues/102331,"It's compiled successfully with your example, how can you say it's still eager mode?","> It's compiled successfully with your example, how can you say it's still eager mode? when I set torch._inductor.config.debug = True, It does not show any IR created.", ,`TORCH_LOGS=+inductor python transformer.py` shows expected debugging info,"Hello, I have the same problem.   Compare with Transformer, the other model is fine:  code: ", ,"> Hello, I have the same problem.  >  >  >  > Compare with Transformer, the other model is fine:  >  > code: >  >  I tried the nightly version `torchversion:  2.1.0.dev20230526+cpu`, it works for me. "
605,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DO NOT LAND] Trying to fix the missing symbol issue in torch_cpp_cpu)ï¼Œ å†…å®¹æ˜¯ (  CC([Executorch][codegen] Add ETKernelIndex for aggregating all kernels for kernel
keys and change codegen to take ETKernelIndex)  CC([DO NOT LAND] Trying to fix the missing symbol issue in torch_cpp_cpu) Differential Revision: D46196891 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[DO NOT LAND] Trying to fix the missing symbol issue in torch_cpp_cpu,"  CC([Executorch][codegen] Add ETKernelIndex for aggregating all kernels for kernel
keys and change codegen to take ETKernelIndex)  CC([DO NOT LAND] Trying to fix the missing symbol issue in torch_cpp_cpu) Differential Revision: D46196891 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!",2023-05-25T22:40:27Z,Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/102313," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
475,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([CUDAGraph Trees] Fix empty storages handling)ï¼Œ å†…å®¹æ˜¯ (  CC([CUDAGraph Trees] Fix empty storages handling) We don't need to handle managing their memory since they dont have any. Previously you would get error `RuntimeError: These storage data ptrs are not allocated in pool (0, 2) but should be {0}` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[CUDAGraph Trees] Fix empty storages handling,"  CC([CUDAGraph Trees] Fix empty storages handling) We don't need to handle managing their memory since they dont have any. Previously you would get error `RuntimeError: These storage data ptrs are not allocated in pool (0, 2) but should be {0}` ",2023-05-25T15:45:47Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/102273, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2042,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(HF GPT2 doesn't work with dynamic=True with inductor and aot_eager backends: `TypeError: unhashable type: 'SymInt'`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `model` (GPT2LMHeadModel) call fails with error in `inductor` and `aot_eager`  backends (`eager` is ok).    Error logs Traceback (most recent call last):   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/output_graph.py"", line 670, in call_user_compiler     compiled_fn = compiler_fn(gm, self.fake_example_inputs())   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/debug_utils.py"", line 1055, in debug_wrapper     compiled_gm = compiler_fn(gm, example_inputs)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/backends/common.py"", line 48, in compiler_fn     cg = aot_module_simplified(gm, example_inputs, **kwargs)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 2822, in aot_module_simplified     compiled_fn = create_aot_dispatcher_function(   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/utils.py"", line 163, in time_wrapper     r = func(*args, **kwargs)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 2515, in create_aot_dispatcher_function     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 1705, in aot_wrapper_dedupe     if a not in args_set: TypeError: unhashable type: 'SymInt' The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/experiments/check_torch_compile.py"", line 29, in      output_x = model(input_ids=inp_t)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,HF GPT2 doesn't work with dynamic=True with inductor and aot_eager backends: `TypeError: unhashable type: 'SymInt'`," ğŸ› Describe the bug `model` (GPT2LMHeadModel) call fails with error in `inductor` and `aot_eager`  backends (`eager` is ok).    Error logs Traceback (most recent call last):   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/output_graph.py"", line 670, in call_user_compiler     compiled_fn = compiler_fn(gm, self.fake_example_inputs())   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/debug_utils.py"", line 1055, in debug_wrapper     compiled_gm = compiler_fn(gm, example_inputs)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/backends/common.py"", line 48, in compiler_fn     cg = aot_module_simplified(gm, example_inputs, **kwargs)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 2822, in aot_module_simplified     compiled_fn = create_aot_dispatcher_function(   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_dynamo/utils.py"", line 163, in time_wrapper     r = func(*args, **kwargs)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 2515, in create_aot_dispatcher_function     compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 1705, in aot_wrapper_dedupe     if a not in args_set: TypeError: unhashable type: 'SymInt' The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/experiments/check_torch_compile.py"", line 29, in      output_x = model(input_ids=inp_t)   File ""/anaconda3/envs/cu118/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call",2023-05-24T09:59:55Z,triaged oncall: pt2 module: dynamic shapes,closed,0,2,https://github.com/pytorch/pytorch/issues/102161,"Try nightly, pretty sure we fixed this one ",Confirmed this runs without failing on main. Reopen if you disagree!
796,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(HF mpt1b model torch dynamo (inductor) compile error)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug This is an attempt to compile a HF model. Here is the code that I ran similar to torch dynamo tutorial:  `eager` and `aot_eager` passes but `inductor` fails. Here are the source files for this model which might given an idea about which layers/modules have been used.  Error logs If you set `config.low_precision_layernorm = False` during model initialization the following error pasted below happens (otherwise when `config.low_precision_layernorm = True` error happens 1 line above at > `a = self.ln_1(x)` ):   Minified repro   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,HF mpt1b model torch dynamo (inductor) compile error, ğŸ› Describe the bug This is an attempt to compile a HF model. Here is the code that I ran similar to torch dynamo tutorial:  `eager` and `aot_eager` passes but `inductor` fails. Here are the source files for this model which might given an idea about which layers/modules have been used.  Error logs If you set `config.low_precision_layernorm = False` during model initialization the following error pasted below happens (otherwise when `config.low_precision_layernorm = True` error happens 1 line above at > `a = self.ln_1(x)` ):   Minified repro   Versions  ,2023-05-24T06:54:58Z,high priority triaged oncall: pt2 module: inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/102152,"After fixing errors in the script https://gist.github.com/ezyang/fa58478d962ed8470b63b21522370913 I tested this script on inductor and reduceoverhead on main and none of them failed, so I'm guessing it was fixed in nightlies. Please reopen if upgrading doesn't help.","> After fixing errors in the script https://gist.github.com/ezyang/fa58478d962ed8470b63b21522370913 I tested this script on inductor and reduceoverhead on main and none of them failed, so I'm guessing it was fixed in nightlies. Please reopen if upgrading doesn't help. Would you be able to share more info about the runtime environment that you've run the code successfully, maybe similar to `Versions` subsection from this issue template. I am currently using the latest pytorch nvidia docker image 23.04py3. I feel it may be a bit difficult to make all the other dependencies inline without knowing the minimum requirement for torch inductor. I did upgrade the pytorch version inside the container to nightly cuda12.1 but now dealing with other dependency conflicts. I prefer using the container since it already has many optimized libraries preinstalled. Managed to fix the dependencies and run it with torch nightly, however compile function gives inconsistent timing:  Now it seems to work: ",So y'all good?
1585,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(On AMD paltform, Torch.profiler schedule(wait=x,warmup=y,active=z) option would count GPU kernel time (y+z) times rather than z times)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I use torch.profiler on AMD platform to profile GPT model inference, I found the raw data is unreasonable, the CPU operations ""self CUDA""  total time is far less than the GPU kernels ""self CUDA"" total time. After experiment and analyse the raw data, I found the GPU kernel ""self CUDA"" time is counted by the sum of warmup and active. Pthon script: `with torch.profiler.profile(         schedule=torch.profiler.schedule(wait=1,warmup=1,active=2),         on_trace_ready=torch.profiler.tensorboard_trace_handler(             dir_name='./logs'),         activities=[             torch.profiler.ProfilerActivity.CPU,             torch.profiler.ProfilerActivity.CUDA         ],         with_modules=True,         record_shapes=True,         profile_memory=True,         with_stack=True,     )as prof:         with torch.no_grad():              for i in range(4):              	logits = model.generate(**input_ids, do_sample=True, num_beams=1, min_length=12, max_new_tokens=12,pad_token_id=50256)              	prof.step()     print(prof.key_averages(group_by_input_shape=True).table(row_limit=1000000, sort_by='self_cuda_time_total'))  `  Versions model: GPTJ6B with FP16 inference H/W: MI100 * 8 S/W: PyTorch 1.13.0  ROCm:5.4  Transformers:4.29.2 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,"On AMD paltform, Torch.profiler schedule(wait=x,warmup=y,active=z) option would count GPU kernel time (y+z) times rather than z times"," ğŸ› Describe the bug When I use torch.profiler on AMD platform to profile GPT model inference, I found the raw data is unreasonable, the CPU operations ""self CUDA""  total time is far less than the GPU kernels ""self CUDA"" total time. After experiment and analyse the raw data, I found the GPU kernel ""self CUDA"" time is counted by the sum of warmup and active. Pthon script: `with torch.profiler.profile(         schedule=torch.profiler.schedule(wait=1,warmup=1,active=2),         on_trace_ready=torch.profiler.tensorboard_trace_handler(             dir_name='./logs'),         activities=[             torch.profiler.ProfilerActivity.CPU,             torch.profiler.ProfilerActivity.CUDA         ],         with_modules=True,         record_shapes=True,         profile_memory=True,         with_stack=True,     )as prof:         with torch.no_grad():              for i in range(4):              	logits = model.generate(**input_ids, do_sample=True, num_beams=1, min_length=12, max_new_tokens=12,pad_token_id=50256)              	prof.step()     print(prof.key_averages(group_by_input_shape=True).table(row_limit=1000000, sort_by='self_cuda_time_total'))  `  Versions model: GPTJ6B with FP16 inference H/W: MI100 * 8 S/W: PyTorch 1.13.0  ROCm:5.4  Transformers:4.29.2 ",2023-05-24T02:54:04Z,module: rocm oncall: profiler,closed,0,2,https://github.com/pytorch/pytorch/issues/102141,"  Good catch, you have it exactly right.  Fix is here: https://github.com/pytorch/kineto/pull/702 This was merged into kineto and it looks like Pytorch 2.0.0 updated the kineto submodule commit, so this is fixed in 2.0.","Since this issue was fixed by the time of the PT2 release, we ask that you use a newer pytorch version that has the fix.  We can't fix older releases.  Closing since this issue is known and fixed in the latest pytorch.  Please reopen if you still are having issues."
479,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] nanogpt_generate failed due to RuntimeError: probability tensor contains either `inf`, `nan` or element < 0)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Torchbench model **nanogpt_generate** failure tracked in  CC(TorchInductor CPU Performance Dashboard)issuecomment1558372237  SW information: SW	/  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,"[Inductor] nanogpt_generate failed due to RuntimeError: probability tensor contains either `inf`, `nan` or element < 0", ğŸ› Describe the bug Torchbench model **nanogpt_generate** failure tracked in  CC(TorchInductor CPU Performance Dashboard)issuecomment1558372237  SW information: SW	/  Versions  ,2023-05-23T07:01:38Z,high priority triaged module: NaNs and Infs oncall: pt2 module: inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/102064,This triggers on CUDA too,This also seems to fix the IMA,  can you confirm if this is still an issue?,"According to the latest report( CC(TorchInductor CPU Performance Dashboard)issuecomment1809031401), this issue no longer exists."
858,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inlined-inbuilt-nn-modules][dynamo][BE] Revisit call_method of NNModuleVariable)ï¼Œ å†…å®¹æ˜¯ (We have lots of specialization in NNModuleVariable `call_method` here  https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/variables/nn_module.pyL468L616 This task is to revisit this and cleanup. At the time of writing this code, there were large number of graph breaks and therefore we specialized for these methods. Today, it is possible that Dynamo can trace the innards of these methods and do the right thing! Steps * Take this commit https://github.com/pytorch/pytorch/commit/f6c97b2b54b44f118c8b671ad982fde27b6f17ca * Run `pytest test/dynamo/test_modules.py`. This is the start.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[inlined-inbuilt-nn-modules][dynamo][BE] Revisit call_method of NNModuleVariable,"We have lots of specialization in NNModuleVariable `call_method` here  https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/variables/nn_module.pyL468L616 This task is to revisit this and cleanup. At the time of writing this code, there were large number of graph breaks and therefore we specialized for these methods. Today, it is possible that Dynamo can trace the innards of these methods and do the right thing! Steps * Take this commit https://github.com/pytorch/pytorch/commit/f6c97b2b54b44f118c8b671ad982fde27b6f17ca * Run `pytest test/dynamo/test_modules.py`. This is the start.  ",2023-05-23T06:58:35Z,triaged enhancement oncall: pt2 module: dynamo dynamo-must-fix,open,0,1,https://github.com/pytorch/pytorch/issues/102063,sorry assigned myself accidentally during hackday. I will work on  CC([dynamo] Python 3.11: AttributeError: 'torch._C.dynamo.eval_frame._PyInterpreterFrame' object has no attribute 'f_lineno') instead.
508,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: Could not infer dtype of numpy.int64)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The following error occurs while running one Deep Graph Library testcase:  requirements.txt doesn't require any particular version of numpy, so numpy1.23.1 should be suitable.  Versions py39pytorch2.0.0 py39numpy1.23.1,1 py39dgl1.1.0 FreeBSD 13.2 STABLE )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,RuntimeError: Could not infer dtype of numpy.int64," ğŸ› Describe the bug The following error occurs while running one Deep Graph Library testcase:  requirements.txt doesn't require any particular version of numpy, so numpy1.23.1 should be suitable.  Versions py39pytorch2.0.0 py39numpy1.23.1,1 py39dgl1.1.0 FreeBSD 13.2 STABLE ",2023-05-22T23:52:40Z,triaged module: numpy,closed,0,7,https://github.com/pytorch/pytorch/issues/102026," this does not seem unexpected to me. A numpy dtype isn't a pytorch dtype, and I have no expectations about using a numpy dtype as an argument to a pytorch function will work. Is this a regression? If so, do you know what version of pytorch or numpy broke things for you?",I don't know if this is a regression. I didn't run with previous versions. Could it be a problem that PyTorch is built w/out Numpy support?,"Not impossible, but I have a regular PyTorch 1.9.0 install with Conda and can reproduce this:  I think this is a case of ""please don't do that"".",In case of DGL the error message is different (same as in the subject). I don't know what is going on. When DGL was released they probably tested it and it probably succeeded somehow.,"I had a look at the DGL code, it's at https://github.com/dmlc/dgl/blob/c83350dade3d0d2062c1a61a9b6a15d7db1f4016/python/dgl/utils/data.pyL168L186. It looks like a bug in DGL to me, the intent is to keep the `int64`/`int32` dtype, but not to mix dtype objects between libraries. I suggest you file an issue against DGL, and if they do think there's something in PyTorch (or NumPy) that broke a contract, a standalone reproducer would be helpful.",When PyTorch is built with Numpy support this error goes away.,Building PyTorch with Numpy fixes the problem.
392,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix fragile code in `torch.__init__.py` related to `torch._inductor` import)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Fragile code in `torch.__init__.py` related to `torch._inductor` import) For motivation of this change see the above issue. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix fragile code in `torch.__init__.py` related to `torch._inductor` import,Fixes CC(Fragile code in `torch.__init__.py` related to `torch._inductor` import) For motivation of this change see the above issue. ,2023-05-22T22:41:08Z,open source Merged ciflow/trunk topic: not user facing module: inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/102021," label ""topic: not user facing"" ""module: inductor""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cpupy3 / build Details for Dev Infra team Raised by workflow job "," merge f ""failure not related"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1623,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fragile code in `torch.__init__.py` related to `torch._inductor` import)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm using vscode+pylance with basic checking and on this line it reports no attribute of `_inductor`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/__init__.pyL1512 However, during runtime this does not raise an error.  I tried to trace the imports and found out why it could work. Before being used in L1643, there is an import of `torch._dynamo` at L1637. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/__init__.pyL1637L1643 And then in `torch/dynamo/__init__.py` it imports `eval_frame`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/_dynamo/__init__.pyL1 Next in `eval_frame.py` it imports `skipfiles`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/_dynamo/eval_frame.pyL52 Finally in `skipfiles.py`, this line implicitly imports `_inductor` as a submodule of `torch`, which also enables `torch._inductor.list_mode_options` defined in `torch/_inductor/__init__.py`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/_dynamo/skipfiles.pyL33  Clearly however, this is not a good coding practice. Instead, there should be an explicit import of `torch._inductor` before use.  Error logs _No response_  Minified repro _No response_  Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fragile code in `torch.__init__.py` related to `torch._inductor` import," ğŸ› Describe the bug I'm using vscode+pylance with basic checking and on this line it reports no attribute of `_inductor`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/__init__.pyL1512 However, during runtime this does not raise an error.  I tried to trace the imports and found out why it could work. Before being used in L1643, there is an import of `torch._dynamo` at L1637. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/__init__.pyL1637L1643 And then in `torch/dynamo/__init__.py` it imports `eval_frame`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/_dynamo/__init__.pyL1 Next in `eval_frame.py` it imports `skipfiles`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/_dynamo/eval_frame.pyL52 Finally in `skipfiles.py`, this line implicitly imports `_inductor` as a submodule of `torch`, which also enables `torch._inductor.list_mode_options` defined in `torch/_inductor/__init__.py`. https://github.com/pytorch/pytorch/blob/2ae87a1f870d46dba15e0f9e8dc32558273518ac/torch/_dynamo/skipfiles.pyL33  Clearly however, this is not a good coding practice. Instead, there should be an explicit import of `torch._inductor` before use.  Error logs _No response_  Minified repro _No response_  Versions master ",2023-05-22T22:40:36Z,module: typing triaged oncall: pt2,closed,0,0,https://github.com/pytorch/pytorch/issues/102020
279,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support resize on meta storage)ï¼Œ å†…å®¹æ˜¯ (  CC(Support resize on meta storage) Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Support resize on meta storage,  CC(Support resize on meta storage) Signedoffby: Edward Z. Yang ,2023-05-22T17:48:29Z,Merged Reverted ciflow/trunk release notes: composability topic: bug fixes,closed,0,12,https://github.com/pytorch/pytorch/issues/101988, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 2, 3, macosm112) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"This PR seems to be based atop https://github.com/pytorch/pytorch/pull/101949 and https://github.com/pytorch/pytorch/pull/101976 which were reverted. Since those will no longer be landed internally, this PR doesn't patch internally during difftrain import due to merge conflicts. In order to unblock importing the rest of the commits, I'll need to revert this change, and then this would need to be rebased and remerged. Apologies for the inconvenience."," revert m ""Need to revert and rebase this in order to unblock train import"" c weird", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,This PR didn't conflict at all on master so it's weird it had to be reverted
1199,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] Reshard frozen params in backward)ï¼Œ å†…å®¹æ˜¯ (  CC([FSDP][Easy] Remove redundant var def in test)  CC([FSDP] Reshard frozen params in backward) This PR makes a first attempt at improving FSDP's finetuning support by adding hooks to reshard frozen parameters in the backward pass.  Without this, frozen parameters involved in gradient computation are kept as unsharded through the entire backward pass.  The approach is to register a multigrad ~~post~~hook on the _input_ activations to the FSDP module, where the hook performs the resharding after all gradients for the FSDP module must have been computed (meaning that we are safe to reshard). ~~This PR relies on adding a ""multigrad posthook"" that differs from the existing ""multigrad hook"" from `register_multi_grad_hook()`. I find that with `register_multi_grad_hook()`, sometimes the unit test counting the number of times `_post_backward_reshard()` is called fails (due to it not being called).~~ This was resolved in https://github.com/pytorch/pytorch/pull/102859.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,[FSDP] Reshard frozen params in backward,"  CC([FSDP][Easy] Remove redundant var def in test)  CC([FSDP] Reshard frozen params in backward) This PR makes a first attempt at improving FSDP's finetuning support by adding hooks to reshard frozen parameters in the backward pass.  Without this, frozen parameters involved in gradient computation are kept as unsharded through the entire backward pass.  The approach is to register a multigrad ~~post~~hook on the _input_ activations to the FSDP module, where the hook performs the resharding after all gradients for the FSDP module must have been computed (meaning that we are safe to reshard). ~~This PR relies on adding a ""multigrad posthook"" that differs from the existing ""multigrad hook"" from `register_multi_grad_hook()`. I find that with `register_multi_grad_hook()`, sometimes the unit test counting the number of times `_post_backward_reshard()` is called fails (due to it not being called).~~ This was resolved in https://github.com/pytorch/pytorch/pull/102859.",2023-05-22T16:26:58Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: improvements,closed,0,10,https://github.com/pytorch/pytorch/issues/101982,"My understanding might be wrong here, but frozen parameters being resharded late (I'm assuming they're resharded in _catch_all_reshard) is similar to how we reshard ""unused parameters"" in the catch all reshard as well. Could we use this technique to completely eliminate the _catch_all_reshard?","varma The catchall reshard is still useful in the case that the very first input activation does not require gradient. Then, we have no activation on which we can reshard the root FSDP instance's parameters if they are frozen.", varma Is this MR ready for merge or not? What is the blocker?,  I was waiting for a fix on autograd side and have not had a chance to rebase it yet. I will do that soon., merge, Merge failed **Reason**: HTTP Error 403: rate limit exceeded Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," , thanks for the great work. but I just come across a problem that might be related. when I freeze some parameters (by setting `requires_grad = False`) I encounter the following error:  any idea? I am using the latest nightly version that I could find to support cuda117. That is `torch2.1.0.dev20230621+cu117cp38cp38linux_x86_64.whl`", Would it be possible to share a repro?
362,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([transformer benchmark] relax tolerance in sdp.py)ï¼Œ å†…å®¹æ˜¯ (Summary: Otherwise we get  Test Plan: buck run mode/devnosan //caffe2/benchmarks/transformer:sdp Differential Revision: D45843836)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[transformer benchmark] relax tolerance in sdp.py,Summary: Otherwise we get  Test Plan: buck run mode/devnosan //caffe2/benchmarks/transformer:sdp Differential Revision: D45843836,2023-05-22T06:10:04Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/101965,This pull request was **exported** from Phabricator. Differential Revision: D45843836,This pull request was **exported** from Phabricator. Differential Revision: D45843836," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,This pull request was **exported** from Phabricator. Differential Revision: D45843836, Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
760,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make _StorageBase.byteswap faster ( > 10000x))ï¼Œ å†…å®¹æ˜¯ (This PR addresses CC(_StrorageBase.byteswap() is too slow for large data). This PR implement faster data elements swap in `_StorageBase` using C++ rather than using Python. This PR helps such a situation that a large model saved on a littleendian machine will be loaded on a bigendian machine. TODO:  [x] Add test cases  [x] Add performance comparison before and after the PR  [ ] (Optional) Investigate further opportunities for performance improvements by SIMDization Fixes CC(_StrorageBase.byteswap() is too slow for large data))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Make _StorageBase.byteswap faster ( > 10000x),This PR addresses CC(_StrorageBase.byteswap() is too slow for large data). This PR implement faster data elements swap in `_StorageBase` using C++ rather than using Python. This PR helps such a situation that a large model saved on a littleendian machine will be loaded on a bigendian machine. TODO:  [x] Add test cases  [x] Add performance comparison before and after the PR  [ ] (Optional) Investigate further opportunities for performance improvements by SIMDization Fixes CC(_StrorageBase.byteswap() is too slow for large data),2023-05-20T06:01:04Z,open source Merged ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/101925,btw does torch.flip(uint8tensor) implement something similar? (if inner dim is super small like 2/4/8 uint8),"Thanks for your comment. IMHO, it looks similar, but a bit different in some cases.  This PR would like to do for an 1D array as follows:  Any comments?","Well, I mean that if you do `torch.tensor([0,1,2,3,4,5,6,7], dtype = torch.uint8).view(1, 2).flip(1).view(1)`, it should do the thing. But it's not inplace and probably isn't optimized for these small 2/3/4/8 dimensions. I mean that it might make sense to improve torch.flip by enabling inplace and making sure it's working fast for the usecase needed here. This could then be used for inplace BGRRGB flips for (*, H, W, 3) images. Supporting inplace arbitrary byte shuffle would be even better :)","Thanks, interesting. Since `flip()` does not support inplace version now, we would address it in another PR.",Benchmark is that 64KB float32 elements (=256KB) are swapped. Measured 10 times and got an average. Performance improvement is 176250x.  , merge," Merge failed **Reason**: This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Thank you very much for your valuable comments.
1116,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(  urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>  for  train_dataset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_train) in one random process while rest of processes have no problem moving forward)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I get urllib and socket error for DistributedDataParallel in PyTorch when I use Azure Cluster of 4 nodes each with 4 K80 GPUs. Please note this only happens in one out of the 16 GPUs (random process each time) and I do see that the line of code here is run with no problem in other nodes, data is being downloaded. So, I am unsure what's the cause or how to fix. Here's another of the 16 nodes showing progress: !Screenshot from 20230519 105802  `reuirements.txt`:  `Dockerfile`:  and `trainenv.yaml`:  Here's the training code:  also:  Please let me know if further information is needed.  Versions I had to use special setup. Check above. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"  urllib.error.URLError: <urlopen error [Errno 110] Connection timed out>  for  train_dataset = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_train) in one random process while rest of processes have no problem moving forward"," ğŸ› Describe the bug I get urllib and socket error for DistributedDataParallel in PyTorch when I use Azure Cluster of 4 nodes each with 4 K80 GPUs. Please note this only happens in one out of the 16 GPUs (random process each time) and I do see that the line of code here is run with no problem in other nodes, data is being downloaded. So, I am unsure what's the cause or how to fix. Here's another of the 16 nodes showing progress: !Screenshot from 20230519 105802  `reuirements.txt`:  `Dockerfile`:  and `trainenv.yaml`:  Here's the training code:  also:  Please let me know if further information is needed.  Versions I had to use special setup. Check above. ",2023-05-19T14:27:37Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/101873,"It's extremely strange. Rerunning the code with no changes at all, resulted in no error. What do you think causes this? !Screenshot from 20230519 113559","Sounds like an ephemeral connection issue with your environment. Closing for now as outside the scope of PyTorch proper issues, but please free to reopen if you identify information that leads you to believe otherwise."
2027,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile makes transformers model (llama) generating different outputs compared with the native)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug To run bf16 model generating, we found there are difference output sentence after using torch.compile compared with native: Native: Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun along the way. One day, the little girl decided to go on an adventure. She packed **up her backpack and set out on her journey.** torch.compile: Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun along the way. One day, the little girl decided to go on an adventure. She packed **her backpack and set out on her journey. She** Native code:  torch.compile code:    Versions Collecting environment information... PyTorch version: 2.1.0.dev20230518+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 11.2.1 20210728 (Red Hat 11.2.11) Clang version: 14.0.0 (Red Hat 14.0.01.module_el8.7.0+1142+5343df54) CMake version: version 3.22.1 Libc version: glibc2.28 Python version: 3.8.16 (default, Mar 2 2023, 03:21:46) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.18.0365.el8.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: transformers==4.28.1 [pip3] numpy==1.24.3 [pip3] torch==2.1.0.dev20230518+cpu [conda] blas 1.0 mkl [conda] mkl 2023.1.0 h6d00ec8_46342 [conda] mklinclude 2023.1.0 pypi_0 pypi [conda] mklser)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile makes transformers model (llama) generating different outputs compared with the native," ğŸ› Describe the bug To run bf16 model generating, we found there are difference output sentence after using torch.compile compared with native: Native: Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun along the way. One day, the little girl decided to go on an adventure. She packed **up her backpack and set out on her journey.** torch.compile: Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun along the way. One day, the little girl decided to go on an adventure. She packed **her backpack and set out on her journey. She** Native code:  torch.compile code:    Versions Collecting environment information... PyTorch version: 2.1.0.dev20230518+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream 8 (x86_64) GCC version: (GCC) 11.2.1 20210728 (Red Hat 11.2.11) Clang version: 14.0.0 (Red Hat 14.0.01.module_el8.7.0+1142+5343df54) CMake version: version 3.22.1 Libc version: glibc2.28 Python version: 3.8.16 (default, Mar 2 2023, 03:21:46) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.18.0365.el8.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: transformers==4.28.1 [pip3] numpy==1.24.3 [pip3] torch==2.1.0.dev20230518+cpu [conda] blas 1.0 mkl [conda] mkl 2023.1.0 h6d00ec8_46342 [conda] mklinclude 2023.1.0 pypi_0 pypi [conda] mklser",2023-05-19T09:14:49Z,triaged oncall: pt2 oncall: cpu inductor,closed,0,10,https://github.com/pytorch/pytorch/issues/101866,gu Can you share the fp32 result as a reference? Is the bf16 eager mode result the same as fp32?,"Sure: FP32 code:  Output: Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun along the way. One day, the little girl decided to go on an adventure. **She packed up her backpack and set out on her journey.** Output is the same as native bf16 eager  mode","Yeah, you don't need the autocast to trigger the problem.","There is also a divergence on CUDA, but it's not clear to me that there is actually a problem: the text in all of these cases is plausible and we don't guarantee that we will give bitwise identical results to eager mode when compiling.","That being said, when we run llama under torch.compile in our test suite with training, the results differ enough that it counts as an accuracy failure, so maybe there is something going on here.","we should be looking at perplexity, it's not fair to assume that the results would be identical since small numerical differences are expected. Instead the compiled model shouldnt find outputs from the uncompiled model and the uncompiled model shouldnt find outputs from the compiled model surprising I can add this to our test suite "," since I still haven't gotten around to it, would you mind checking the perplexity for this particular example?",Please help to confirm if it's still a valid issue gu , please help to verify with BF16.,"Verified FP32 and BF16 results of metallama/Llama27bhf, the output of torch.compile is the same as eager."
1973,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Observing negative number in PyTorch profiling)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug We found there are negative numbers in PyTorch profilings, which is inconvenient for users to get solid profiling for operators:                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                        aten::mm        72.77%        2.775s        72.77%        2.775s     385.375us          7200                    aten::linear        14.62%     557.318ms        96.82%        3.692s     383.617us          9624              aten::index_select         2.71%     103.334ms         2.73%     103.926ms      49.916us          2082                       aten::cat         2.36%      90.020ms         2.56%      97.640ms      20.144us          4847                       aten::bmm         2.29%      87.493ms         2.29%      87.493ms      42.721us          2048                       aten::mul         1.76%      67.101ms         1.96%      74.875ms       8.068us          9280                    aten::matmul         1.46%      55.748ms        77.53%        2.956s     292.904us         10093                       aten::add         1.18%      44.884ms         1.32%      50.220ms       6.822us          7362                     aten::copy_         1.07%      40.672ms         1.07%      40.672ms       2.349us         17317                      aten::silu         0.86%      32.886ms         0.86%      32.886ms      32.115us          1024                  aten::_to_copy         0.73%      27.821ms         1.63%      62.339ms       3.739us         16674                        aten::to         0.38%      14.327ms         1.82%      69.255ms       3.472us         19945                      aten::topk         0.37%      14.210ms         0.37%      14.210ms )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Observing negative number in PyTorch profiling," ğŸ› Describe the bug We found there are negative numbers in PyTorch profilings, which is inconvenient for users to get solid profiling for operators:                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                        aten::mm        72.77%        2.775s        72.77%        2.775s     385.375us          7200                    aten::linear        14.62%     557.318ms        96.82%        3.692s     383.617us          9624              aten::index_select         2.71%     103.334ms         2.73%     103.926ms      49.916us          2082                       aten::cat         2.36%      90.020ms         2.56%      97.640ms      20.144us          4847                       aten::bmm         2.29%      87.493ms         2.29%      87.493ms      42.721us          2048                       aten::mul         1.76%      67.101ms         1.96%      74.875ms       8.068us          9280                    aten::matmul         1.46%      55.748ms        77.53%        2.956s     292.904us         10093                       aten::add         1.18%      44.884ms         1.32%      50.220ms       6.822us          7362                     aten::copy_         1.07%      40.672ms         1.07%      40.672ms       2.349us         17317                      aten::silu         0.86%      32.886ms         0.86%      32.886ms      32.115us          1024                  aten::_to_copy         0.73%      27.821ms         1.63%      62.339ms       3.739us         16674                        aten::to         0.38%      14.327ms         1.82%      69.255ms       3.472us         19945                      aten::topk         0.37%      14.210ms         0.37%      14.210ms ",2023-05-19T08:03:04Z,triaged oncall: profiler,closed,1,3,https://github.com/pytorch/pytorch/issues/101861,I'll look into it.,"                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls               ProfilerStep        6.49%  247365.000us       100.00%        3.813s        3.813s             1 This issue should happen on very performant CPU only, that means, it's hard to reproduce on ordinary CPU. But, due to CPUrelated, it will happen on any accelerator + CPU systems, in PyTorch version earlier than or equal to 2.3. The rootcause should be in profiler postprocess in PyTorch, which truncates the timestamps during converting from ns to us, instead of rounding. Let's think about this case, we have 3 operators, grandparent_op, parent_op, and child_op. Obviously, the timeline between these 3 ops should be, `grandparent_op_start > parent_op_start > child_op_start > child_op_end > parent_op_end > grandparent_op_end`. And, grandparent_op_duration = grandparent_op_end  grandparent_op_start  parent_duration. In some case, PyTorch postprocess will disorder the relationship between these 3 ops. It will take the child_op as the child of grandparent_op, instead of parent_op, because the child_op_end is later than parent_op_end. https://github.com/pytorch/pytorch/blob/6181e65cd81725efc6bc5d64ef3be607b0aa3ca1/torch/autograd/profiler_util.pyL124L125 Then, the timeline will be, `grandparent_op_start > parent_op_start > child_op_start > parent_op_end > child_op_end > grandparent_op_end`. In result, grandparent_op_duration = grandparent_op_end  grandparent_op_start  parent_op_duration ** child_op_duration**, will become a negative value. Why the parent_op_end is earler than child_op_end? Because the end timestamp of operator is the result of twice computations, instead of from raw timestamp, that is, op_end = op_start + op_duration, while op_duration = raw_op_end  raw_op_start with truncation. https://github.com/pytorch/pytorch/blob/a4ef9cdd2807e7138e29d12ff03b48f60e1a5189/torch/csrc/autograd/profiler_kineto.cppL775L777 https://github.com/pytorch/pytorch/blob/a4ef9cdd2807e7138e29d12ff03b48f60e1a5189/torch/autograd/profiler.pyL470 For example, we have `parent_start_ns = 315877, parent_end_ns = 486764, child_start_ns = 319059, and child_end_ns = 486499`. Then, `parent_duration_us = (486764  315877) / 1000 = 170, child_duration_us = (486499  319059) / 1000 = 167`. And, `parent_start_us = 315877 / 1000 = 315, parent_end_us = parent_start_us + parent_duration_us = 485`. But, `child_start_us = 319059 / 1000 = 319, child_end_us = child_start_us + child_duration_us = 486`. Obviously, the child_end_us is later than parent_end_us. Fortunately, in latest PyTorch master, the profiler postprocess promoted the timestamp precision from us to ns, which bypassed this issue. https://github.com/pytorch/pytorch/pull/123650 But it's still a potential issue if improving the precision and adopting the truncation again. Therefore, we will submit a PR to use the raw_op_end to check the parent relationship, instead of computation result.",Associated fix PR: https://github.com/pytorch/pytorch/pull/129554
489,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([WIP] add private storage._from_file_offset that allows reading mmaped tensors from an offset in a file)ï¼Œ å†…å®¹æ˜¯ (Stack from ghstack:  CC([WIP] Let torch.load load memorymapped tensors (binary file attempt))  CC([WIP] add private storage._from_file_offset that allows reading mmaped tensors from an offset in a file))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[WIP] add private storage._from_file_offset that allows reading mmaped tensors from an offset in a file,Stack from ghstack:  CC([WIP] Let torch.load load memorymapped tensors (binary file attempt))  CC([WIP] add private storage._from_file_offset that allows reading mmaped tensors from an offset in a file),2023-05-19T04:01:19Z,topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/101853,Closing as this change is no longer needed
733,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Added stub of min gcc/clang reqirements to CUDA 12.1)ï¼Œ å†…å®¹æ˜¯ (when patched pytorchnightly with dirty workaround, adding in version.py materialized version of file (not template, but from sitepackages) setting cuda_version = 12.1 (previously: None) new error arised  above problem is described here in issue of pytorch:   CC(pytorch-nightly not have torch/version.py.tpl:cuda specified) It arised when using oobabooga/oneclickinstallers for Windows Short:  Long:  Related issue CC(pytorchnightly not have torch/version.py.tpl:cuda specified) (but not fixes it))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Added stub of min gcc/clang reqirements to CUDA 12.1,"when patched pytorchnightly with dirty workaround, adding in version.py materialized version of file (not template, but from sitepackages) setting cuda_version = 12.1 (previously: None) new error arised  above problem is described here in issue of pytorch:   CC(pytorch-nightly not have torch/version.py.tpl:cuda specified) It arised when using oobabooga/oneclickinstallers for Windows Short:  Long:  Related issue CC(pytorchnightly not have torch/version.py.tpl:cuda specified) (but not fixes it)",2023-05-18T17:10:29Z,triaged open source topic: not user facing,closed,0,14,https://github.com/pytorch/pytorch/issues/101806,The committers listed above are authorized under a signed CLA.:white_check_mark: login: JulianVolodia / name: Volodia  (749a2bb3409ecf8d577c0be3df4eb065a0956515),"Thanks  for review and suggestion. Also, for correcting my label and triage. I was just deciphering categorization guidelines as it is my first contribution. And, you are right  I updated the code. Actually I am progressing deeper and seen there is no 12.1 in torch/utils/cpp_extension.py  do you know if pytorchnightly should work with 12.1? here, I found it should be doable: https://pytorch.org/getstarted/locally/ !image",result now is sth like that: ,"in nightly branch, torch/utils/cpp_extension.py I also do not see clang nor gcc minimum version specified  I added them to check what would happen. Try to workaround nightly problem with this end with list index out of range in _get_cuda_arch_flags ","Deepest sorry for making some loud here, if you close it as unneeded for latest versions troubleshooting, I would completely understand.","made this draft  Looking forward for some domain knowledge discussion if CUDA 12.1 should be supported experimentally and underthetable only, or from nightly indeed; would love to make it happen with textgenerationwebui and maybe with other products depending on pytorch :))",I found in failing that linter warning blocks only linux job https://github.com/pytorch/pytorch/actions/runs/5017918767/jobs/8997840985?pr=101806step:10:1350,Looping in  about 12.1 changes in cpp_extensions. I'm not sure whether these changes should be made.,"> Looping in  about 12.1 changes in cpp_extensions. I'm not sure whether these changes should be made. Sorry for messing, I am pretty sure those shouldn't.","I added suggested change. Added extra spaces before the hash starting the python's comment. Fail was caused by linter's warning in latestage but only in one pipeline. Tell me if you would like to add some linter check, to fail early and not involve pipelines work until code is passing linter check. I could do it.","reverted pointless commits, and fixed git fork history after force push. I will reopen, and if I could join you working on testing new versions compatible to 12.1 I would be proud of doing so.","Thank you for the update. Two comments:   Please remove `not tested` comment, as it is tested by CI   Please update PR description and reference where it doc that specifies minimum and maximum supported clang versions (I.e. I don't see why cuda12 would not support clang15)", if you trying to fix this issue:   CC(pytorch-nightly not have torch/version.py.tpl:cuda specified) please take a look at this comment:  CC(pytorch-nightly not have torch/version.py.tpl:cuda specified)issuecomment1563545421 ,"Thanks  for comment here and in issue thread. Thanks   you are right. I do not have reference for this. It was (originally, now the commit message is different) ""dumb"" fix for errors I've seen when trying to integrate nightly with CUDA support, but turns out  I have been using wrong version. I will close this, as clearly that is not pytorch issue."
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-18T06:39:55Z,module: nn triaged module: flaky-tests skipped,open,0,0,https://github.com/pytorch/pytorch/issues/101787
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-18T06:39:52Z,module: nn triaged module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/101785,dup
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-18T03:39:40Z,module: nn triaged module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/101774,dup
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-18T03:39:40Z,module: nn triaged module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/101773,dup
476,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(UnsupportedOperatorError: Exporting the operator 'aten::broadcast_to' to ONNX opset version 14 is not supported. )ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Unable to export owlvitForObjectDetection from huggingface transformers library.  Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,UnsupportedOperatorError: Exporting the operator 'aten::broadcast_to' to ONNX opset version 14 is not supported. ," ğŸš€ The feature, motivation and pitch Unable to export owlvitForObjectDetection from huggingface transformers library.  Alternatives _No response_  Additional context _No response_",2023-05-18T03:28:24Z,module: onnx triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/101768,"torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::broadcast_to' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues. Python 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import torch >>> torch.__version__ '2.1.0a0+4136153' >>> "
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 11 workflow(s) with 11 failures and 11 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 11 workflow(s) with 11 failures and 11 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-18T00:57:35Z,module: nn triaged module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/101749,dup
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 11 workflow(s) with 11 failures and 11 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 11 workflow(s) with 11 failures and 11 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-18T00:57:32Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101748
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T21:40:50Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101737
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T21:40:47Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101735
2009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/master/test/test_transformers.py 200 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 1) headers: {""connection"":""keepalive"",""contentlength"":""112495"",""cachecontrol"":""maxage=300"",""contentsecuritypolicy"":""defaultsrc 'none'; stylesrc 'unsafeinline'; sandbox"",""contenttype"":""text/plain; charset=utf8"",""etag"":""\""21fe764a9e20ccb690418a8b6f04917d911a8170ae514cda7a42f22ba4e5c0b8\"""",""stricttransportsecurity"":""maxage=31536000"",""xcontenttypeoptions"":""nosniff"",""xframeoptions"":""deny"",""xxssprotection"":""1; mode=block"",""xgithubrequestid"":""FC2A:10B1:8C1EE8:A011AF:64651F5C"",""acceptranges"":""bytes"",""date"":""Wed, 17 May 2023 18:39:24 GMT"",""via"":""1.1 varnish"",""xservedby"":""cachesjc10032SJC"",""xcache"":""HIT"",""xcachehits"":""1"",""xtimer"":""S1684348764.178946,VS0,VE175"",""vary"":)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/master/test/test_transformers.py 200 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 1) headers: {""connection"":""keepalive"",""contentlength"":""112495"",""cachecontrol"":""maxage=300"",""contentsecuritypolicy"":""defaultsrc 'none'; stylesrc 'unsafeinline'; sandbox"",""contenttype"":""text/plain; charset=utf8"",""etag"":""\""21fe764a9e20ccb690418a8b6f04917d911a8170ae514cda7a42f22ba4e5c0b8\"""",""stricttransportsecurity"":""maxage=31536000"",""xcontenttypeoptions"":""nosniff"",""xframeoptions"":""deny"",""xxssprotection"":""1; mode=block"",""xgithubrequestid"":""FC2A:10B1:8C1EE8:A011AF:64651F5C"",""acceptranges"":""bytes"",""date"":""Wed, 17 May 2023 18:39:24 GMT"",""via"":""1.1 varnish"",""xservedby"":""cachesjc10032SJC"",""xcache"":""HIT"",""xcachehits"":""1"",""xtimer"":""S1684348764.178946,VS0,VE175"",""vary"":",2023-05-17T18:40:07Z,module: flaky-tests skipped module: unknown,closed,0,0,https://github.com/pytorch/pytorch/issues/101714
2009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/master/test/test_transformers.py 200 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 1) headers: {""connection"":""keepalive"",""contentlength"":""112495"",""cachecontrol"":""maxage=300"",""contentsecuritypolicy"":""defaultsrc 'none'; stylesrc 'unsafeinline'; sandbox"",""contenttype"":""text/plain; charset=utf8"",""etag"":""\""21fe764a9e20ccb690418a8b6f04917d911a8170ae514cda7a42f22ba4e5c0b8\"""",""stricttransportsecurity"":""maxage=31536000"",""xcontenttypeoptions"":""nosniff"",""xframeoptions"":""deny"",""xxssprotection"":""1; mode=block"",""xgithubrequestid"":""FC2A:10B1:8C1EE8:A011AF:64651F5C"",""acceptranges"":""bytes"",""date"":""Wed, 17 May 2023 18:39:24 GMT"",""via"":""1.1 varnish"",""xservedby"":""cachesjc10076SJC"",""xcache"":""MISS"",""xcachehits"":""0"",""xtimer"":""S1684348764.177125,VS0,VE176"",""vary"")è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/master/test/test_transformers.py 200 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 1) headers: {""connection"":""keepalive"",""contentlength"":""112495"",""cachecontrol"":""maxage=300"",""contentsecuritypolicy"":""defaultsrc 'none'; stylesrc 'unsafeinline'; sandbox"",""contenttype"":""text/plain; charset=utf8"",""etag"":""\""21fe764a9e20ccb690418a8b6f04917d911a8170ae514cda7a42f22ba4e5c0b8\"""",""stricttransportsecurity"":""maxage=31536000"",""xcontenttypeoptions"":""nosniff"",""xframeoptions"":""deny"",""xxssprotection"":""1; mode=block"",""xgithubrequestid"":""FC2A:10B1:8C1EE8:A011AF:64651F5C"",""acceptranges"":""bytes"",""date"":""Wed, 17 May 2023 18:39:24 GMT"",""via"":""1.1 varnish"",""xservedby"":""cachesjc10076SJC"",""xcache"":""MISS"",""xcachehits"":""0"",""xtimer"":""S1684348764.177125,VS0,VE176"",""vary""",2023-05-17T18:40:04Z,module: flaky-tests skipped module: unknown,closed,0,0,https://github.com/pytorch/pytorch/issues/101712
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T15:39:32Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101703
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 4 workflow(s) with 4 failures and 4 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T15:39:30Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101702
559,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(_StrorageBase.byteswap() is too slow for large data)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The `_StorageBase.byteswap()` method is implemented in Python. Therefore, it is too slow for large data. I hit that it was not finished within 5 minutes when I loaded 560M parameters of a DNN model in a pickle file. Possible solution is to write this method in C and to call it using pybind11.  Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,_StrorageBase.byteswap() is too slow for large data," ğŸ› Describe the bug The `_StorageBase.byteswap()` method is implemented in Python. Therefore, it is too slow for large data. I hit that it was not finished within 5 minutes when I loaded 560M parameters of a DNN model in a pickle file. Possible solution is to write this method in C and to call it using pybind11.  Versions ",2023-05-17T10:35:32Z,triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/101690,,"Thanks for the comment.  Do you already have a solution, which is ready for PR, at your end?","Hey , I would be happy to review a PR that implements a faster _StrorageBase.byteswap() as you suggested :)"," I have just created a draft of the PR at https://github.com/pytorch/pytorch/pull/101925. While I have some ToDos to make it ready, I would appreciate any comments to the initial implementation."
462,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Allow disabling bias for Transformer)ï¼Œ å†…å®¹æ˜¯ (As used by T5 and PaLM, citing ""increased training stability for large models"" (https://arxiv.org/abs/2204.02311). Depends on CC(Allow disabling bias for `LayerNorm`), which allows disabling bias for `LayerNorm`s. Marked as draft due to this.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Allow disabling bias for Transformer,"As used by T5 and PaLM, citing ""increased training stability for large models"" (https://arxiv.org/abs/2204.02311). Depends on CC(Allow disabling bias for `LayerNorm`), which allows disabling bias for `LayerNorm`s. Marked as draft due to this.",2023-05-17T10:09:10Z,triaged open source Merged ciflow/trunk release notes: nn topic: improvements,closed,0,10,https://github.com/pytorch/pytorch/issues/101687,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Since https://github.com/pytorch/pytorch/pull/101683 has been merged, this can now also be reviewed.  since you were involved in the previous LayerNorm without bias merge, would you be so kind to remove the `Stale` label here?", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `transformer_nobias` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout transformer_nobias && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Not merging any PRs at the moment because there is a merge blocking https://github.com/pytorch/pytorch/labels/ci:%20sev issue open at:   CC(6-7 hour queue times caused by GPU instances not being available ) Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
284,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix hpu storage serialization)ï¼Œ å†…å®¹æ˜¯ (ChangeId: Ia534400a0e8972590374eceba5b62a2525b796e5 Fixes ISSUE_NUMBER )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fix hpu storage serialization,ChangeId: Ia534400a0e8972590374eceba5b62a2525b796e5 Fixes ISSUE_NUMBER ,2023-05-17T07:34:51Z,module: serialization triaged open source Merged ciflow/trunk release notes: python_frontend topic: improvements,closed,0,6,https://github.com/pytorch/pytorch/issues/101680," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cuda11.8py3 / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T06:40:11Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101677
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T06:40:05Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101674
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T03:40:03Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101664
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 9 workflow(s) with 9 failures and 9 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T03:40:01Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101663
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 10 workflow(s) with 10 failures and 10 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 10 workflow(s) with 10 failures and 10 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T00:58:18Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101647
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 10 workflow(s) with 10 failures and 10 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 10 workflow(s) with 10 failures and 10 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-17T00:58:11Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101645
410,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use the symint version of computeStorageNbytes within get_nbytes.)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER according to comment. This change is needed to make sure calling tensor.sizes() will error if the tensor has dynamic dimension in pytorch/xla.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Use the symint version of computeStorageNbytes within get_nbytes.,Fixes ISSUE_NUMBER according to comment. This change is needed to make sure calling tensor.sizes() will error if the tensor has dynamic dimension in pytorch/xla.,2023-05-17T00:07:24Z,open source Merged ciflow/trunk topic: not user facing,closed,1,4,https://github.com/pytorch/pytorch/issues/101634,"hi  , I followed your suggestion and made the change in this PR. Do you know in which file I should add a test for this PR?", , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T18:40:29Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101601
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T18:40:26Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101600
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T15:39:53Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101581
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 3 failures and 3 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T15:39:50Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101580
266,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update llama to failing)ï¼Œ å†…å®¹æ˜¯ (  CC(Update llama to failing) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Update llama to failing,  CC(Update llama to failing) Signedoffby: Edward Z. Yang  ,2023-05-16T14:08:08Z,Merged topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/101565," merge f ""forward fixing trunk"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T12:48:39Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101562
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T12:48:39Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101561
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 12 workflow(s) with 12 failures and 12 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 12 workflow(s) with 12 failures and 12 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T09:40:11Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101550
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 12 workflow(s) with 12 failures and 12 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 12 workflow(s) with 12 failures and 12 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T09:40:08Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101549
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T06:40:24Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101526
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T06:40:21Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101524
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 13 workflow(s) with 13 failures and 13 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 13 workflow(s) with 13 failures and 13 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T03:41:21Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101501
1111,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 13 workflow(s) with 13 failures and 13 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 13 workflow(s) with 13 failures and 13 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-16T03:41:21Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101500
933,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Refactor causal mask generation and detection for nn.transformer)ï¼Œ å†…å®¹æ˜¯ (Summary: * Move causal mask detection into a method _detect_causal_mask * Add device and dtype parameters to generate_subsequent to avoid extra copies & conversions by passing directly to torch.full.  * Create a private globalscope function _generate_subsequent because static class attribute member functions not supported by TorchScript resulting in torchscripting errors. * Make TransformerEncoder and TransformerDecoder consistent w.r.t. is_causal handling by calling _detect_casual_mask Test Plan: sandcastle & github  Closing as the PR was reexported as CC(Refactor causal mask generation and detection for nn.transformer) due to a tooling issue. Differential Revision: D45827765)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Refactor causal mask generation and detection for nn.transformer,Summary: * Move causal mask detection into a method _detect_causal_mask * Add device and dtype parameters to generate_subsequent to avoid extra copies & conversions by passing directly to torch.full.  * Create a private globalscope function _generate_subsequent because static class attribute member functions not supported by TorchScript resulting in torchscripting errors. * Make TransformerEncoder and TransformerDecoder consistent w.r.t. is_causal handling by calling _detect_casual_mask Test Plan: sandcastle & github  Closing as the PR was reexported as CC(Refactor causal mask generation and detection for nn.transformer) due to a tooling issue. Differential Revision: D45827765,2023-05-16T01:39:40Z,fb-exported release notes: nn topic: not user facing suppress-bc-linter,closed,0,30,https://github.com/pytorch/pytorch/issues/101487,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,See also CC(Implement `is_causal` API for `Transformer`).,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,This pull request was **exported** from Phabricator. Differential Revision: D45827765,Closing as the PR was reexported as CC(Refactor causal mask generation and detection for nn.transformer) due to a tooling issue.
840,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: Triton Error [CUDA]: device-side assert triggered when trying torch.compile max-autotune on nanoGPT)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Tried to run `torch.compile(model, mode='maxautotune')` on nanoGPT, got some cuda errors such as `Triton Error [CUDA]: deviceside assert triggered` when doing inference.  I originally found this when experimenting with quantization, but it reproduces without quantization. Repro script (requires nanoGPT repo): https://gist.github.com/vkuzo/7352cf7b18fd2602668fb3d43ad9467a Stack trace: https://gist.github.com/vkuzo/57225adfacd7779309fd5444a5f80bcf  Versions https://gist.github.com/vkuzo/a5fbfc5905f9bd33732f4f73ec6ce0f4 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,RuntimeError: Triton Error [CUDA]: device-side assert triggered when trying torch.compile max-autotune on nanoGPT," ğŸ› Describe the bug Tried to run `torch.compile(model, mode='maxautotune')` on nanoGPT, got some cuda errors such as `Triton Error [CUDA]: deviceside assert triggered` when doing inference.  I originally found this when experimenting with quantization, but it reproduces without quantization. Repro script (requires nanoGPT repo): https://gist.github.com/vkuzo/7352cf7b18fd2602668fb3d43ad9467a Stack trace: https://gist.github.com/vkuzo/57225adfacd7779309fd5444a5f80bcf  Versions https://gist.github.com/vkuzo/a5fbfc5905f9bd33732f4f73ec6ce0f4 ",2023-05-15T20:30:07Z,triaged oncall: pt2 module: inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/101444, what was the resolution here?,ğŸ¤·â€â™‚ï¸,We run NanoGpt in dashboard without error. I'm going to assume this has since been fixed but  feel free to reopen if that's not the case.
638,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PyTorch] Add tests for empty tensors w/storage null data_ptr)ï¼Œ å†…å®¹æ˜¯ (  CC([PyTorch] Add tests for empty tensors w/storage null data_ptr) Further investigation seems to show that changing this behavior (making empty tensors sometimes have nonnull data_ptr) was the real problem with CC([PyTorch] Don't do extra numel() check in TensorImpl::data()) . Adding tests to lock down this behavior so we don't change it by accident again. Differential Revision: D45873002)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[PyTorch] Add tests for empty tensors w/storage null data_ptr,  CC([PyTorch] Add tests for empty tensors w/storage null data_ptr) Further investigation seems to show that changing this behavior (making empty tensors sometimes have nonnull data_ptr) was the real problem with CC([PyTorch] Don't do extra numel() check in TensorImpl::data()) . Adding tests to lock down this behavior so we don't change it by accident again. Differential Revision: D45873002,2023-05-15T17:51:27Z,Merged Stale ciflow/trunk topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/101426,"> This SGTM as a stopgap test for what is going on in the sev, but in the long term we probably want to think about if the existing behavior makes sense. IMO there isn't a lot of value in exposing a raw pointer to Python as an integer so I'd be curious to know why Tensor.data_ptr even exists. Given that it does exist, I don't see how we can change the behavior  it is what it is, the entire world could depend on it, and so it would be bcbreaking to change.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalrocm5.4.2py3.8 / test (default, 3, 3, linux.rocm.gpu) Details for Dev Infra team Raised by workflow job ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/swolchok/572/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/101426`)", merge, Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
638,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PyTorch] Add tests for empty tensors w/storage null data_ptr)ï¼Œ å†…å®¹æ˜¯ (  CC([PyTorch] Add tests for empty tensors w/storage null data_ptr) Further investigation seems to show that changing this behavior (making empty tensors sometimes have nonnull data_ptr) was the real problem with CC([PyTorch] Don't do extra numel() check in TensorImpl::data()) . Adding tests to lock down this behavior so we don't change it by accident again. Differential Revision: D45873002)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[PyTorch] Add tests for empty tensors w/storage null data_ptr,  CC([PyTorch] Add tests for empty tensors w/storage null data_ptr) Further investigation seems to show that changing this behavior (making empty tensors sometimes have nonnull data_ptr) was the real problem with CC([PyTorch] Don't do extra numel() check in TensorImpl::data()) . Adding tests to lock down this behavior so we don't change it by accident again. Differential Revision: D45873002,2023-05-15T17:51:27Z,Merged Stale ciflow/trunk topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/101426,"> This SGTM as a stopgap test for what is going on in the sev, but in the long term we probably want to think about if the existing behavior makes sense. IMO there isn't a lot of value in exposing a raw pointer to Python as an integer so I'd be curious to know why Tensor.data_ptr even exists. Given that it does exist, I don't see how we can change the behavior  it is what it is, the entire world could depend on it, and so it would be bcbreaking to change.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxfocalrocm5.4.2py3.8 / test (default, 3, 3, linux.rocm.gpu) Details for Dev Infra team Raised by workflow job ", rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `gh/swolchok/572/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/101426`)", merge, Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
496,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(support ConvBinaryInplace in Inductor cpp wrapper)ï¼Œ å†…å®¹æ˜¯ (  CC(support ConvBinaryInplace in Inductor cpp wrapper) This PR has changed the OP schema since `at::Tensor&` should be the FirstArg: https://github.com/pytorch/pytorch/blob/87f9160b6727ba554520bbd9f8905c7cf3c8654a/aten/src/ATen/core/boxing/impl/boxing.hL305L341 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,support ConvBinaryInplace in Inductor cpp wrapper,  CC(support ConvBinaryInplace in Inductor cpp wrapper) This PR has changed the OP schema since `at::Tensor&` should be the FirstArg: https://github.com/pytorch/pytorch/blob/87f9160b6727ba554520bbd9f8905c7cf3c8654a/aten/src/ATen/core/boxing/impl/boxing.hL305L341 ,2023-05-15T08:16:06Z,module: cpu open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/101394, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
606,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] [CPU] HF model OPTForCausalLM random failed in CI test caused by OOM)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug We found a random CI failure in Inductor CPU HF accuracy test, refer https://hud.pytorch.org/hud/pytorch/pytorch/main/1?per_page=50&name_filter=inductor_huggingface_cpu_accuracy. Which caused by the test node is `linux.4xlarge` with 32GB memory. And OPTForCausalLM model usage is close to the machine capacity.   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Inductor] [CPU] HF model OPTForCausalLM random failed in CI test caused by OOM," ğŸ› Describe the bug We found a random CI failure in Inductor CPU HF accuracy test, refer https://hud.pytorch.org/hud/pytorch/pytorch/main/1?per_page=50&name_filter=inductor_huggingface_cpu_accuracy. Which caused by the test node is `linux.4xlarge` with 32GB memory. And OPTForCausalLM model usage is close to the machine capacity.   Versions  ",2023-05-15T08:13:56Z,oncall: pt2,closed,0,0,https://github.com/pytorch/pytorch/issues/101390
401,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([torch.compile] AssertionError about `Pointwise` in `convolution_binary_inplace`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `torch.compile` raises AssertionError about `Pointwise` in `convolution_binary_inplace`   Versions  Click to expand  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[torch.compile] AssertionError about `Pointwise` in `convolution_binary_inplace`, ğŸ› Describe the bug `torch.compile` raises AssertionError about `Pointwise` in `convolution_binary_inplace`   Versions  Click to expand  ,2023-05-15T00:01:45Z,triaged inductor_pattern_match,closed,0,0,https://github.com/pytorch/pytorch/issues/101374
713,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(put third_party/ittapi/ in .bazelignore)ï¼Œ å†…å®¹æ˜¯ (put third_party/ittapi/ in .bazelignore Summary: Bazel fails when recursing into this directory because it has a symlink that infinitely recurses. We don't use this library in Bazel, so it's safe to just ignore its existence. Test Plan: Verified with `bazel query //...`  Stack created with Sapling. Best reviewed with ReviewStack.  CC(add program to diff generated files)  CC(update rules_python and let bazel install its own pip dependencies)  CC(put third_party/ittapi/ in .bazelignore) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,put third_party/ittapi/ in .bazelignore,"put third_party/ittapi/ in .bazelignore Summary: Bazel fails when recursing into this directory because it has a symlink that infinitely recurses. We don't use this library in Bazel, so it's safe to just ignore its existence. Test Plan: Verified with `bazel query //...`  Stack created with Sapling. Best reviewed with ReviewStack.  CC(add program to diff generated files)  CC(update rules_python and let bazel install its own pip dependencies)  CC(put third_party/ittapi/ in .bazelignore) ",2023-05-14T16:05:33Z,module: build open source Merged ciflow/trunk topic: not user facing module: bazel,closed,0,6,https://github.com/pytorch/pytorch/issues/101364, , merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,nice
374,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([transformer benchmark] sort by cuda time)ï¼Œ å†…å®¹æ˜¯ (Summary: The benchmark is running on CUDA Test Plan: buck run mode/opt //caffe2/benchmarks/transformer:sdp_backwards Differential Revision: D45843837)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[transformer benchmark] sort by cuda time,Summary: The benchmark is running on CUDA Test Plan: buck run mode/opt //caffe2/benchmarks/transformer:sdp_backwards Differential Revision: D45843837,2023-05-14T03:40:25Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/101349,This pull request was **exported** from Phabricator. Differential Revision: D45843837,This pull request was **exported** from Phabricator. Differential Revision: D45843837, merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1177,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:425)ï¼Œ å†…å®¹æ˜¯ (If you have a question or would like help and support, please ask at our forums. If you are submitting a feature request, please preface the title with [feature request]. If you are submitting a bug report, please fill in the following details.  Issue description Provide a short description.  Code example Please try to provide a minimal example to repro the bug. Error messages and stack traces are also helpful.  System Info Please copy and paste the output from our environment collection script (or fill out the checklist below manually). You can get the script and run it with:   PyTorch or Caffe2:  How you installed PyTorch (conda, pip, source):  Build command you used (if compiling from source):  OS:  PyTorch version:  Python version:  CUDA/cuDNN version:  GPU models and configuration:  GCC version (if compiling from source):  CMake version:  Versions of any other relevant libraries:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:425,"If you have a question or would like help and support, please ask at our forums. If you are submitting a feature request, please preface the title with [feature request]. If you are submitting a bug report, please fill in the following details.  Issue description Provide a short description.  Code example Please try to provide a minimal example to repro the bug. Error messages and stack traces are also helpful.  System Info Please copy and paste the output from our environment collection script (or fill out the checklist below manually). You can get the script and run it with:   PyTorch or Caffe2:  How you installed PyTorch (conda, pip, source):  Build command you used (if compiling from source):  OS:  PyTorch version:  Python version:  CUDA/cuDNN version:  GPU models and configuration:  GCC version (if compiling from source):  CMake version:  Versions of any other relevant libraries:",2023-05-14T03:15:42Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/101348,"Closing this issue, because we need more information to effectively triage it. When you provide the requested information, feel free to reopen the issue, to let us know that we can continue with assessing the issue. This open/close/open cycle is how we manage the flow of issues, so your reopening of the issue will constitute the signal that lets us know to take a look at the issue again. Please give us:  a script that reproduces the problem  the system info"
846,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Applying Weight Decay to certain parameters with FSDP)ï¼Œ å†…å®¹æ˜¯ (Hi all, I was wondering what the best way to apply weight decay to only certain parameters is when using PyTorch's FSDP. Does using `use_orig_params=True` have any negative impact on training with FSDP? Since FSDP flattens the parameters it would not return all of the original parameter names. I am using biasless layer norms and tie the embeddings so I need to make sure weight decay is not being applied to these. For example, I am separating the layers by which need to have weight decay applied:  Here are the named parameters and modules:  Any input would be very greatly appreciated. Thank you, Enrico )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Applying Weight Decay to certain parameters with FSDP,"Hi all, I was wondering what the best way to apply weight decay to only certain parameters is when using PyTorch's FSDP. Does using `use_orig_params=True` have any negative impact on training with FSDP? Since FSDP flattens the parameters it would not return all of the original parameter names. I am using biasless layer norms and tie the embeddings so I need to make sure weight decay is not being applied to these. For example, I am separating the layers by which need to have weight decay applied:  Here are the named parameters and modules:  Any input would be very greatly appreciated. Thank you, Enrico ",2023-05-13T21:48:56Z,triaged module: fsdp,closed,0,16,https://github.com/pytorch/pytorch/issues/101343,"Your approach looks good to me. By the way, is `to_logits.weight` meant to be trained? Regarding `use_orig_params=True`:  `use_orig_params=True` may have very slightly more CPU overhead during forward/backward just simply due to having to manage the original parameters instead of only managing the `FlatParameter`s. This CPU overhead is negligible unless your model is heavily CPU bound, which is not the case for most large models.   `use_orig_params=True` means that the optimizer runs peroriginalparameter instead of per`FlatParameter`, which generally involves issuing more GPU kernels. This is unavoidable for your case if you want to apply different weight decay, so I would not worry about this too much. The overhead here will not be different than normal local training or DDP training, which also run optimizer over each original parameter.  Some minor nits that might make the code more concise if you want:  ","> Your approach looks good to me. By the way, is `to_logits.weight` meant to be trained? >  > Regarding `use_orig_params=True`: >  > * `use_orig_params=True` may have very slightly more CPU overhead during forward/backward just simply due to having to manage the original parameters instead of only managing the `FlatParameter`s. This CPU overhead is negligible unless your model is heavily CPU bound, which is not the case for most large models. > * `use_orig_params=True` means that the optimizer runs peroriginalparameter instead of per`FlatParameter`, which generally involves issuing more GPU kernels. This is unavoidable for your case if you want to apply different weight decay, so I would not worry about this too much. The overhead here will not be different than normal local training or DDP training, which also run optimizer over each original parameter. >  > Some minor nits that might make the code more concise if you want: >  >  >  >  Hi  , Thank you for your help and insight. The `to_logits.weight` is tied to the `token_emb.weight`:  The `token_emb.weight` is added to the no decay parameters. Please let me know if you believe this needs to be adjusted to work with the above weight decay approach properly? I left the param dictionary as a loop mostly for user readability. I can make adjustments following the nitpicks. The CPU overhead should not be an issue as the nodes have plenty of cores. My largest concern was that there could potentially be issues when scaling across multiple nodes and the amount of overhead that could occur when using `use_orig_params=True`. Currently, I am using FSDP with 8 x 8 A100 nodes. I mostly wanted to ensure that this would not significantly harm distributed training as I needed to decouple the weight decay per param. Thank you, Enrico","Ah, my bad. If `to_logits.weight` is tied to `token_emb.weight`, you only need to be careful that the two parameters should be wrapped into the same FSDP instance. Otherwise, this should be fine since `nn.Module.parameters()` should deduplicate the shared parameter by default (since `remove_duplicate = True` by default). Overall, the setup looks good to me!","> Ah, my bad. If `to_logits.weight` is tied to `token_emb.weight`, you only need to be careful that the two parameters should be wrapped into the same FSDP instance. Otherwise, this should be fine since `nn.Module.parameters()` should deduplicate the shared parameter by default (since `remove_duplicate = True` by default). >  > Overall, the setup looks good to me! I greatly appreciate all your help. I do believe that it should be in the same instance. If you have free time and feel like doing a look over the FSDP portion of the training setup, the code is here: https://github.com/conceptofmind/PaLM/blob/main/train_distributed.py . I can be sure to acknowledge you. I can close out the issue if not. Thank you again, Enrico","Hi  , When trying to resume from a model checkpoint with `use_orig_params=True`:  I am receiving this error:  I am unsure where `shard_state` should be set to True as I do not see it in the FSDP documentation. Thank you, Enrico", How could `use_orig_params=True` dispatch into an `_optim_state_dict_to_load_impl()` that has `shard_state=False`? Is the issue that `use_orig_params=True` is not compatible with `FSDP.scatter_optim_state_dict()`?,I think this might be a matter of HuggingFace Accelerate needs to update its implementation to call into the new optim state dict API.,> I think this might be a matter of HuggingFace Accelerate needs to update its implementation to call into the new optim state dict API. That is what I was thinking it may be as well. I have been debating removing HF accelerate in total from the workflow but wanted to keep it since it is easier to understand for the end user. HF accelerate does not seem to work with other forms of parallelism either.," as  mentioned seems save method in HF accelerate for FSDP sharded state dict needs some updates, the sharded state dict save/load can be found here. This also extends to saving the optimizer states the sharded optimizer save/load can be found here. cc: ",">  as  mentioned seems save method in HF accelerate for FSDP sharded state dict needs some updates, >  > the correct code should be found here. >  > This also extends to saving the optimizer states >  > the correct code can be found here. >  > cc:  Hi  , I appreciate the information. I had opened an issue on the Huggingface Accelerate repository as well. Hopefully, they can make the necessary FSDP compatibility updates. Thank you, Enrico","  Hi! Thanks for your kindness reply which also benefits me a lot. I have a somewhat unrelated question (feels like asking here will get me answers faster): When enabling amp training with PyTorch native `autocast`, I noticed there seems to be obvious difference for `DDP` based model and `FSDP` based model. Here is a minimum example to reproduce the case:  run the script by  You'll find the `ddp_loss` is different form the `fsdp_loss` obviously, however, this will not happen when training with fp32. Such error caused my model to converge normally when using DDP, but encounter NaN losses when using FSDP. I've read the introduction of FSDP here. I find the discrepancy/error to be unusual since the gathered parameters supplied to operations such as `conv` and `linear` in FSDP should be identical to those in DDP. So, why is this error occurring?","I have not checked if this addresses the issue with your script, but I think you should use `ShardedGradScaler` with FSDP: https://github.com/pytorch/pytorch/blob/019c38624cdd079fbed04a561eebde45c6fa3b1f/torch/distributed/fsdp/sharded_grad_scaler.pyL35","> I have not checked if this addresses the issue with your script, but I think you should use `ShardedGradScaler` with FSDP: >  > https://github.com/pytorch/pytorch/blob/019c38624cdd079fbed04a561eebde45c6fa3b1f/torch/distributed/fsdp/sharded_grad_scaler.pyL35 Hi, I've replaced `ShardedGradScaler` with `GradScalar` for ddp_scaler, but the error still exists. Actually, the error exists even if calculate the gradient with `loss.backward` directly.  Although the error still exists, my model  can converge normally when using `ShardedGradScaler`","Hi . Thanks for trying out `ShardedGradScaler` too. Let me look into this then. If it is not too inconvenient, would it be possible to file a separate Github issue and tag me because it helps with organization (since it is not related to this current one of applying weight decay to certain parameters)? You can just copy in the same info you have here.","> Hi, I've replaced `ShardedGradScaler` with `GradScalar` for ddp_scaler, but the error still exists. Actually, the error exists even if calculate the gradient with `loss.backward` directly. >  > Although the error still exists, my model can converge normally when using `ShardedGradScaler` Hi, I've opened a new issue here)",I am marking this closed since the original issue is resolved.
406,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([transformer benchmark] fix in sdp_bwd for scaled_dot_product_attention return type)ï¼Œ å†…å®¹æ˜¯ (Summary: Otherwise we get  Test Plan: buck run mode/devnosan //caffe2/benchmarks/transformer:sdp_backwards Differential Revision: D45843838)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[transformer benchmark] fix in sdp_bwd for scaled_dot_product_attention return type,Summary: Otherwise we get  Test Plan: buck run mode/devnosan //caffe2/benchmarks/transformer:sdp_backwards Differential Revision: D45843838,2023-05-13T18:09:59Z,fb-exported Merged ciflow/trunk,closed,0,3,https://github.com/pytorch/pytorch/issues/101341,This pull request was **exported** from Phabricator. Differential Revision: D45843838, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-12T21:39:21Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101312
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-12T21:39:18Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101311
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_decoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-12T18:39:56Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101298
1108,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU))ï¼Œ å†…å®¹æ˜¯ (Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_encoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU),"Platforms: dynamo This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 6 workflow(s) with 6 failures and 6 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_encoder_padding_and_src_mask_bool_cpu` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_transformers.py` ",2023-05-12T18:39:56Z,module: nn triaged module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/101297
927,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`einsum` is about 40x slower on CUDA than manually multiplying and summing)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug A similar issue was reported in CC(`einsum` is ~20X slower than manually multiplying and summing). Unfortunately, it doesn't look like it was actually fully fixed at the time. Comparing  where `a` and `b` are tensors with a large `N` dimension and a relatively small `c` dimension  So on the CPU both the ""manual"" and the `einsum` versions are about the same (though `numpy` is faster). But on the GPU, the `einsum` version is about 40 times slower (the exact slowdown can vary by machine, I also got a 10x slowdown on a slightly more modern workstation). Here's the script that I used to get these results:  bench_einsum.py    Versions  Versions   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`einsum` is about 40x slower on CUDA than manually multiplying and summing," ğŸ› Describe the bug A similar issue was reported in CC(`einsum` is ~20X slower than manually multiplying and summing). Unfortunately, it doesn't look like it was actually fully fixed at the time. Comparing  where `a` and `b` are tensors with a large `N` dimension and a relatively small `c` dimension  So on the CPU both the ""manual"" and the `einsum` versions are about the same (though `numpy` is faster). But on the GPU, the `einsum` version is about 40 times slower (the exact slowdown can vary by machine, I also got a 10x slowdown on a slightly more modern workstation). Here's the script that I used to get these results:  bench_einsum.py    Versions  Versions   ",2023-05-12T02:59:52Z,module: performance triaged module: linear algebra,open,3,9,https://github.com/pytorch/pytorch/issues/101249, ,"I believe we're dispatching these to `torch.mm` and `torch.mm` is not optimized to do reductions over dimensions such small size (in this case 2).  If you want it to go even faster, you unroll the sum yourself. `x=a*b; x[:,0]+x[:,1]` gives me a `x4` speedup on CPU over calling `sum(1)`. On CUDA, we're calling cublas, so that's going to be slower. If we want to tweak the heuristics, we should do it at a `torch.mm/bmm` level.","Also, `torch.compile` does automatic loop unrolling for small summations, but we currently don't have a decomposition for `torch.einsum` :(. It'd be nice to have one. With `torch.compile` this example would go even faster.",> but we currently don't have a decomposition for torch.einsum Why not? This is already a composite implicit implementation so it should decompose automatically into mms ?,You are 200% right.,"In that case, I'd expect to see a speedup on CUDA as we don't need to prepare the inputs to be sent to cuBLAS (which incurs in a copy in this case). ","We wouldn't codegen `mm`/`bmm` that results from einsum decomposition, and neither would we have a chance to avoid materializing inputs because we need materialized inputs to send to `mm`/`bmm`.","> I believe we're dispatching these to `torch.mm` and `torch.mm` is not optimized to do reductions over dimensions such small size (in this case 2). >  > If you want it to go even faster, you unroll the sum yourself. `x=a*b; x[:,0]+x[:,1]` gives me a `x4` speedup on CPU over calling `sum(1)`. >  > On CUDA, we're calling cublas, so that's going to be slower. If we want to tweak the heuristics, we should do it at a `torch.mm/bmm` level. also, might be good to have some shortcuts for such small dimension sizes for `torch.sum` and `torch.mm` (e.g. if some 2d or 3d point ops or geometric transforms?)",Any update on this?
506,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tool for identifying where in eager model an operation is nondeterministic)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Let's say you have a model code and when you run it twice you get bitwise different results. Where did it diverge? We can use TorchFunctionMode/TorchDispatchMode to localize where the first divergence occurred.  Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tool for identifying where in eager model an operation is nondeterministic, ğŸ› Describe the bug Let's say you have a model code and when you run it twice you get bitwise different results. Where did it diverge? We can use TorchFunctionMode/TorchDispatchMode to localize where the first divergence occurred.  Versions master ,2023-05-12T02:50:04Z,triaged module: determinism,open,0,2,https://github.com/pytorch/pytorch/issues/101246,Note that gradcheck is checking this for backward passes.,"Note that gradcheck doesn't need to ""see into"" the backwards, since you're typically expecting to only run it on a single op"
1983,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I followed the example to finetune HuggingFace's wav2vec 2.0 for speech recognition, using `torch.compile`, aiming to get faster training. However, I ran into an issue as outlined in the error logs. I suspect that HuggingFace's wav2vec 2.0 is not yet supported in PyTorch 2.0 and needs some modification to ensure compatibility when running `torch.compile`. It's mostly related to creating mask tensors for SpecAugment. This issue seems to also be related: fairseq Hubert with torch.compile). Same issue also raised in HuggingFace.  Error logs ``` ***** Running training *****   Num examples = 3,478   Num Epochs = 15   Instantaneous batch size per device = 4   Total train batch size (w. parallel, distributed & accumulation) = 4   Gradient Accumulation steps = 1   Total optimization steps = 13,050   Number of trainable parameters = 311,270,569   0% (default, Nov 26 2020, 07:57:39)  [GCC 9.3.0] (64bit runtime) Python platform: Linux4.19.023cloudamd64x86_64withglibc2.28 Is CUDA available: True CUDA runtime version: 12.0.76 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB Nvidia driver version: 525.60.13 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian Address sizes:       46 bits physical, 48 bits virtual CPU(s):              12 Online CPU(s) list: 011 Thread(s) per core:  2 Core(s) per socket:  6 Socket(s):           1 NUMA node(s):        1 Vendor ID:           GenuineIntel CPU family:          6 Model:               85 Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz Stepping:            7 CPU MH)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`," ğŸ› Describe the bug I followed the example to finetune HuggingFace's wav2vec 2.0 for speech recognition, using `torch.compile`, aiming to get faster training. However, I ran into an issue as outlined in the error logs. I suspect that HuggingFace's wav2vec 2.0 is not yet supported in PyTorch 2.0 and needs some modification to ensure compatibility when running `torch.compile`. It's mostly related to creating mask tensors for SpecAugment. This issue seems to also be related: fairseq Hubert with torch.compile). Same issue also raised in HuggingFace.  Error logs ``` ***** Running training *****   Num examples = 3,478   Num Epochs = 15   Instantaneous batch size per device = 4   Total train batch size (w. parallel, distributed & accumulation) = 4   Gradient Accumulation steps = 1   Total optimization steps = 13,050   Number of trainable parameters = 311,270,569   0% (default, Nov 26 2020, 07:57:39)  [GCC 9.3.0] (64bit runtime) Python platform: Linux4.19.023cloudamd64x86_64withglibc2.28 Is CUDA available: True CUDA runtime version: 12.0.76 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB Nvidia driver version: 525.60.13 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian Address sizes:       46 bits physical, 48 bits virtual CPU(s):              12 Online CPU(s) list: 011 Thread(s) per core:  2 Core(s) per socket:  6 Socket(s):           1 NUMA node(s):        1 Vendor ID:           GenuineIntel CPU family:          6 Model:               85 Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz Stepping:            7 CPU MH",2023-05-11T06:33:41Z,oncall: distributed module: crash triaged module: nccl ezyang's list bug oncall: pt2 module: dynamic shapes pt2d-triage-nov2024,open,0,15,https://github.com/pytorch/pytorch/issues/101160,Try dynamic=True?,"This one would require unbacked symints, it's breaking on x[mask]=tensor","> Try dynamic=True? Setting `dynamic=True` doesn't work either. > This one would require unbacked symints, it's breaking on x[mask]=tensor I think it's breaking on the creation of `mask`, i.e. this line  which is right before ",I can look later in the week. ,When I run this repro it hangs indefinitely on tensor formatting in an error message.,"I can't successfully repro, after patching through  CC([inline-inbuilt-nn-modules][Dynamo] Can't inline functions under torch.nn.parallel ) the training script segfaults (bad! probably our fault!) It's worth noting that this training script is using DataParallel.","I reattempted the repro on a clean build environment and I still trigger a segfault, so I am bumping priority for resolving the segfault.","The segfault is here:  Looking over other threads, this doesn't appear to be a race; the other threads are waiting on locks. With optimizations disabled, I have a slightly better stack trace:  `me>topFrame.below` is indeed NULL at the segfault.","When I disable use of DataParallel, I get a different error:   ","I can get a repro, which doesn't actually repro, but suggests there's some boolean masking going on: https://gist.github.com/ezyang/ff789e19c0e42581dcf078c2f71dfff3","Re  CC(Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile`)issuecomment1570376548 this is the thing where sometimes AOTAutograd will produce a view created in no_grad mode, making subsequent mutations incorrect. Repro extraction doesn't work here because we don't faithfully generate an accurate enough input to trigger the same issue.","This issue reproduces when I add wav2vec2 to our benchmark suite, which is nice.",  is this still an issue?," can I now natively torch compile wav2vec2 with just torch.compile() with dynamic=True since we have a random mask generated every step, which changes the sequence_length every step?",I don't know offhand. Try it and see? dynamic=True not needed.
1980,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: Unrecognized CachingAllocator option: C)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I have installed pytorch 2.0.0+cu117,torchaudio 2.0.1+cu117 for my project. Previously it was running perfect and using the gpu as well. but suddenly , it has started giving an error ""Traceback (most recent call last):   File ""C:\Users\hp\PycharmProjects\Roberta_sindhiPost_processing\main.py"", line 21, in      device_name = torch.cuda.get_device_name(i)   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\cuda\__init__.py"", line 365, in get_device_name     return get_device_properties(device).name   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\cuda\__init__.py"", line 395, in get_device_properties     _lazy_init()   will define _get_device_properties   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\cuda\__init__.py"", line 247, in _lazy_init     torch._C._cuda_init() RuntimeError: Unrecognized CachingAllocator option: C"" below is the code snippet: import torch import pandas as pd from transformers import RobertaForMaskedLM, RobertaTokenizer,AutoConfig from sklearn.model_selection import train_test_split from torch.utils.data import DataLoader, Dataset from sklearn.preprocessing import LabelEncoder import numpy as np import time import torchmetrics.functional as metrics import torch print(torch.cuda.is_available()) if torch.cuda.is_available():      Get the number of available GPUs     device_count = torch.cuda.device_count()     print(f""Number of available GPUs: {device_count}"")     Iterate through each GPU and print its name     for i in range(device_count):         device_name = torch.cuda.get_device_name(i)         print(f""Device {i}: {device_name}"") else:     print(""CUDA is not available"")  Rea)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,RuntimeError: Unrecognized CachingAllocator option: C," ğŸ› Describe the bug I have installed pytorch 2.0.0+cu117,torchaudio 2.0.1+cu117 for my project. Previously it was running perfect and using the gpu as well. but suddenly , it has started giving an error ""Traceback (most recent call last):   File ""C:\Users\hp\PycharmProjects\Roberta_sindhiPost_processing\main.py"", line 21, in      device_name = torch.cuda.get_device_name(i)   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\cuda\__init__.py"", line 365, in get_device_name     return get_device_properties(device).name   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\cuda\__init__.py"", line 395, in get_device_properties     _lazy_init()   will define _get_device_properties   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\cuda\__init__.py"", line 247, in _lazy_init     torch._C._cuda_init() RuntimeError: Unrecognized CachingAllocator option: C"" below is the code snippet: import torch import pandas as pd from transformers import RobertaForMaskedLM, RobertaTokenizer,AutoConfig from sklearn.model_selection import train_test_split from torch.utils.data import DataLoader, Dataset from sklearn.preprocessing import LabelEncoder import numpy as np import time import torchmetrics.functional as metrics import torch print(torch.cuda.is_available()) if torch.cuda.is_available():      Get the number of available GPUs     device_count = torch.cuda.device_count()     print(f""Number of available GPUs: {device_count}"")     Iterate through each GPU and print its name     for i in range(device_count):         device_name = torch.cuda.get_device_name(i)         print(f""Device {i}: {device_name}"") else:     print(""CUDA is not available"")  Rea",2023-05-11T05:52:09Z,module: windows module: cuda triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/101158," hello, I tried recreating the issue, but could not. Are you still dealing with it? Can you provide some more details about how you run into this issue? I would like to ask you if you tried running something else right before this code snippet, as my guess is pytorch reserved too much gpu memory and since there was no memory left, it resulted in this error.","Hey, thanks for reaching out! This issue has been resolved by invalidating caches(option available in file menu of pycharm IDE) On Mon, Sep 11, 2023 at 4:17â€¯PM StefanAlin Pahontu  wrote: >   hello, I tried recreating the > issue, but could not. Are you still dealing with it? Can you provide some > more details about how you run into this issue? I would like to ask you if > you tried running something else right before this code snippet, as my > guess is pytorch reserved too much gpu memory and since there was no memory > left, it resulted in this error. > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","Thank you for your reply , well noted. I believe we can now close the issue.","I meet the question ""RuntimeError: Unrecognized CachingAllocator option: 0"" because pytorch is not matched with Cuda's version."
307,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo] Graph break: call_method NNModuleVariable() _conv_forward)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Repro:  Graph breaks:   Versions N/A )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Dynamo] Graph break: call_method NNModuleVariable() _conv_forward, ğŸ› Describe the bug Repro:  Graph breaks:   Versions N/A ,2023-05-11T05:10:26Z,triaged oncall: pt2 module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/101155,"Please also consider and fix this pattern, which should be the similar: ",I am investigating on the issues right now. It repros smoothly
2019,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] Error ""Inference tensors do not track version counter"" in inference_mode w/ llama7b)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Transformers: 4.29.0 Repro:  Error log:   Versions PyTorch version: 2.1.0a0+gitb536c40 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.2 Libc version: glibc2.31 Python version: 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.071genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   52 bits physical, 57 bits virtual CPU(s):                          128 Online CPU(s) list:             0127 Thread(s) per core:              2 Core(s) per socket:              32 Socket(s):                       2 NUMA node(s):                    2 Vendor ID:                       GenuineIntel CPU family:                      6 Model:                           106 Model name:                      Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz Stepping:                        6 CPU MHz:                         2600.000 CPU max MHz:                     3400.0000 CPU min MHz:                     800.0000 BogoMIPS:                        5200.00 L1d cache:                       3 MiB L1i cache:                       2 MiB L2 cache:                        80 MiB L3 cache:                        96 MiB NU)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[dynamo] Error ""Inference tensors do not track version counter"" in inference_mode w/ llama7b"," ğŸ› Describe the bug Transformers: 4.29.0 Repro:  Error log:   Versions PyTorch version: 2.1.0a0+gitb536c40 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.2 Libc version: glibc2.31 Python version: 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.071genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   52 bits physical, 57 bits virtual CPU(s):                          128 Online CPU(s) list:             0127 Thread(s) per core:              2 Core(s) per socket:              32 Socket(s):                       2 NUMA node(s):                    2 Vendor ID:                       GenuineIntel CPU family:                      6 Model:                           106 Model name:                      Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz Stepping:                        6 CPU MHz:                         2600.000 CPU max MHz:                     3400.0000 CPU min MHz:                     800.0000 BogoMIPS:                        5200.00 L1d cache:                       3 MiB L1i cache:                       2 MiB L2 cache:                        80 MiB L3 cache:                        96 MiB NU",2023-05-11T02:52:13Z,bug oncall: pt2,closed,0,7,https://github.com/pytorch/pytorch/issues/101151,"Inference mode is known to be pretty broken with torch.compile today, and we should fix it. The easiest thing to do is to make inference mode a noop inside of torch.compile: inference_mode is supposed to make your hotpath code cheaper by not having to store autograd metadata at runtime  but this is something we should be able to always avoid when using torch.compile (since we traced autograd ahead of time). I think inference_mode today just breaks during compilation.","Hmm, I made a local repro that I was able to fix with this PR: https://github.com/pytorch/pytorch/pull/101219  can you try running your repro on top of that PR and see if it fixes the issue?","> Hmm, I made a local repro that I was able to fix with this PR: CC(fix inference_mode with torch.compile) >  >  can you try running your repro on top of that PR and see if it fixes the issue?  sorry, just saw the message. Someone else reported a similar issue:  CC(torch.compile(model.generate) cannot run under torch.inference_mode() with dynamic input shape). Will check if it is a duplicate too.","Hi  , I compiled pytorch with your PR branch with commit a46937342204097f8b85c0b4ffde8c8ba59489ba.Here is the output. `Loading model... Loading checkpoint shards: 100% 990M/990M [01:55     output_compile = model.generate(**inputs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/eval_frame.py"", line 287, in _fn     return fn(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/external_utils.py"", line 17, in inner     return fn(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/utils/_contextlib.py"", line 115, in decorate_context     return func(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/generation/utils.py"", line 1250, in generate     self._validate_model_class()   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/generation/utils.py"", line 1257, in      new_generation_config = GenerationConfig.from_model_config(self.config)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/generation/utils.py"", line 1522, in      return self.greedy_search(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/generation/utils.py"", line 2339, in greedy_search     outputs = self(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1502, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1511, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/eval_frame.py"", line 440, in catch_errors     return callback(frame, cache_size, hooks, frame_state)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/convert_frame.py"", line 527, in _convert_frame     result = inner_convert(frame, cache_size, hooks, frame_state)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/convert_frame.py"", line 127, in _fn     return fn(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/convert_frame.py"", line 360, in _convert_frame_assert     return _compile(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/utils.py"", line 180, in time_wrapper     r = func(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/convert_frame.py"", line 430, in _compile     out_code = transform_code_object(code, transform)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 1000, in transform_code_object     transformations(instructions, code_options)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/convert_frame.py"", line 415, in transform     tracer.run()   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2019, in run     super().run()   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/symbolic_convert.py"", line 703, in run     and self.step()   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/symbolic_convert.py"", line 663, in step     getattr(self, inst.opname)(inst)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/symbolic_convert.py"", line 2107, in RETURN_VALUE     self.output.compile_subgraph(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/output_graph.py"", line 755, in compile_subgraph     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/contextlib.py"", line 79, in inner     return func(*args, **kwds)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/output_graph.py"", line 832, in compile_and_call_fx_graph     compiled_fn = self.call_user_compiler(gm)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/utils.py"", line 180, in time_wrapper     r = func(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/output_graph.py"", line 891, in call_user_compiler     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/output_graph.py"", line 887, in call_user_compiler     compiled_fn = compiler_fn(gm, self.example_inputs())   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/repro/after_dynamo.py"", line 117, in debug_wrapper     compiled_gm = compiler_fn(gm, example_inputs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/__init__.py"", line 1543, in __call__     return compile_fx(model_, inputs_, config_patches=self.config)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_inductor/compile_fx.py"", line 610, in compile_fx     return compile_fx(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_inductor/compile_fx.py"", line 720, in compile_fx     return aot_autograd(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/backends/common.py"", line 55, in compiler_fn     cg = aot_module_simplified(gm, example_inputs, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 3700, in aot_module_simplified     compiled_fn = create_aot_dispatcher_function(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_dynamo/utils.py"", line 180, in time_wrapper     r = func(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 3176, in create_aot_dispatcher_function     fw_metadata = run_functionalized_fw_and_collect_metadata(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 722, in inner     flat_f_outs = f(*flat_f_args)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py"", line 3309, in functional_call     out = Interpreter(mod).run(*args[params_len:], **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/fx/interpreter.py"", line 138, in run     self.env[node] = self.run_node(node)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/fx/interpreter.py"", line 195, in run_node     return getattr(self, n.op)(n.target, args, kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/fx/interpreter.py"", line 267, in call_function     return target(*args, **kwargs) torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised: RuntimeError: Cannot call sizes() on tensor with symbolic sizes/strides While executing %matmul : [users=1] = call_functiontarget=torch.matmul, kwargs = {}) Original traceback:   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/models/t5/modeling_t5.py"", line 1720, in forward     decoder_outputs = self.decoder(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1511, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/models/t5/modeling_t5.py"", line 1090, in forward     layer_outputs = layer_module(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1511, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/models/t5/modeling_t5.py"", line 693, in forward     self_attention_outputs = self.layer0   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/models/t5/modeling_t5.py"", line 600, in forward     attention_output = self.SelfAttention(   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1511, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/zhaoqion/miniconda3/envs/103132/lib/python3.9/sitepackages/transformers/models/t5/modeling_t5.py"", line 530, in forward     scores = torch.matmul( You can suppress this exception and fall back to eager by setting:     import torch._dynamo     torch._dynamo.config.suppress_errors = True`",Thanks for the callout   I have another fix here that should fix this one (also linked in the new issue) https://github.com/pytorch/pytorch/pull/103275,I just recently faced an this issue in `inference_mode` with the error: ,mshah did you find a working solution ? 
334,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([fx] change from #users to num_users in graph printout)ï¼Œ å†…å®¹æ˜¯ (`users` means stuff in various chat apps, which makes it annoying to copypasta graphs into them.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,[fx] change from #users to num_users in graph printout,"`users` means stuff in various chat apps, which makes it annoying to copypasta graphs into them.",2023-05-10T23:17:59Z,Merged Reverted ciflow/trunk release notes: quantization topic: not user facing,closed,0,16,https://github.com/pytorch/pytorch/issues/101140, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `fix_users` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout fix_users && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""There are internal changes to this commit that are preventing landing, so I am reverting to unblock the diff train"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/101140/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/5325418726, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
240,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(update torchbench pin - llama training)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,update torchbench pin - llama training,Fixes ISSUE_NUMBER,2023-05-10T18:30:01Z,topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/101104, rebase, successfully started a rebase job. Check the current status here,"Tried to rebase and push PR CC(update torchbench pin  llama training), but it was already up to date",You probably want https://github.com/pytorch/pytorch/pull/101071 instead,"> You probably want CC(Update torchbench pin) instead Yayy, thanks for filling out llama lines in csvs :)"
447,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update doc strings to make description of is_causal consistent for nn.Transformer and nn.MHA)ï¼Œ å†…å®¹æ˜¯ (Summary: Update doc strings to make description of is_causal consistent for nn.Transformer and nn.MHA Test Plan: sandcastle & github CI/CD Differential Revision: D45737197)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Update doc strings to make description of is_causal consistent for nn.Transformer and nn.MHA,Summary: Update doc strings to make description of is_causal consistent for nn.Transformer and nn.MHA Test Plan: sandcastle & github CI/CD Differential Revision: D45737197,2023-05-10T16:11:46Z,fb-exported Merged ciflow/trunk release notes: nn topic: not user facing bug suppress-bc-linter,closed,0,18,https://github.com/pytorch/pytorch/issues/101089,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197,This pull request was **exported** from Phabricator. Differential Revision: D45737197, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,This pull request was **exported** from Phabricator. Differential Revision: D45737197, Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Disable conv cache emptying)ï¼Œ å†…å®¹æ˜¯ (  CC([Tmp] [testing] enable conv benchmark)  CC(Disable conv cache emptying) We warmup cudagraph trees in the cudagraph memory pool so that if we are part of the way through your run, and a large majority of memory is already allocated to cudagraphs, we dont try to allocate again to eager which would split memory pool in half. However this means this is causing us to fail the following assert due to the `emptyCache` call in CUDNN benchmarking: https://github.com/pytorch/pytorch/blob/main/c10/cuda/CUDACachingAllocator.cppL2959.  Disable the empty cache call during cudagraph warmup to fix error. Disabling did not have a significant affect on memory: !image )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Disable conv cache emptying,"  CC([Tmp] [testing] enable conv benchmark)  CC(Disable conv cache emptying) We warmup cudagraph trees in the cudagraph memory pool so that if we are part of the way through your run, and a large majority of memory is already allocated to cudagraphs, we dont try to allocate again to eager which would split memory pool in half. However this means this is causing us to fail the following assert due to the `emptyCache` call in CUDNN benchmarking: https://github.com/pytorch/pytorch/blob/main/c10/cuda/CUDACachingAllocator.cppL2959.  Disable the empty cache call during cudagraph warmup to fix error. Disabling did not have a significant affect on memory: !image ",2023-05-10T01:20:34Z,module: cpu Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/101038, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1983,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Placeholder] PyTorch 2.0 Dynamo/Inductor Hack{day/week})ï¼Œ å†…å®¹æ˜¯ (We will have a list of graph breaks with the minimized repro for the hack{day/week}. (Assigned task to  temporarily before we start the hackday)  Dynamo Graph Breaks  * [x] [Easy]  CC([Dynamo] TB hf_Reformer graph breaks) * [x] [Easy]  CC([Dynamo] Graph break: call_method NNModuleVariable() _conv_forward) * [x] [Easy]  CC(Dynamo does not support methods defined on NamedTuples) * [x] [Medium]  CC([Dynamo] symbolic_convert returns ValueError: Cell is empty) * [x] [Easy]  CC([Dynamo] Unsupported: comparison NNModuleVariable() <built-in function eq> NNModuleVariable()) * [x] [Easy]  CC([Dynamo] Support several missing torch.* ops) * [x] [Easy]  CC([dynamo] Dynamo struggles with in-line dicts where keys are types) * [x] [Easy]  CC([easy] Graph break on module.buffers) * [x] [Easy]  CC([dynamo] Diffusers - Graph break on OrderedDict ) * [x] [Easy]  CC([dynamo] Graph break on itertools.chain) * [x] [Easy]  CC(graph break caused by funcs defined in `torch._C` in dynamo)  Other Dynamo Issues  * [x] [Easy]  CC([torch.compile] Add `explain` as a backend) * [x] [Easy]  CC(`torch.compile` raises DataDependentOutputException with `torch.autograd.set_detect_anomaly`) * [x] [Medium]  CC(Sufficiently deep nesting + inlining + closure fails with KeyError) * [x] [Medium]  CC([torch.compile] doesn't support `permute(*tensor)`) * [x] [Medium]  CC([torch.compile] reduces `nn.Parameter` incorrectly to make the execution of model fail)   + [x] [Medium] ^^ followup to the above issue, which was workedaround with a graph break but underlying AOT Autograd + nn.Parameter issue still exists * [x] [Easy]  CC([dynamo] `call_function BuiltinVariable` and `op returned non-Tensor` cause graph breaks) * [x] [Easy]  CC(Dynamo doesn't support user de)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Placeholder] PyTorch 2.0 Dynamo/Inductor Hack{day/week},"We will have a list of graph breaks with the minimized repro for the hack{day/week}. (Assigned task to  temporarily before we start the hackday)  Dynamo Graph Breaks  * [x] [Easy]  CC([Dynamo] TB hf_Reformer graph breaks) * [x] [Easy]  CC([Dynamo] Graph break: call_method NNModuleVariable() _conv_forward) * [x] [Easy]  CC(Dynamo does not support methods defined on NamedTuples) * [x] [Medium]  CC([Dynamo] symbolic_convert returns ValueError: Cell is empty) * [x] [Easy]  CC([Dynamo] Unsupported: comparison NNModuleVariable() <built-in function eq> NNModuleVariable()) * [x] [Easy]  CC([Dynamo] Support several missing torch.* ops) * [x] [Easy]  CC([dynamo] Dynamo struggles with in-line dicts where keys are types) * [x] [Easy]  CC([easy] Graph break on module.buffers) * [x] [Easy]  CC([dynamo] Diffusers - Graph break on OrderedDict ) * [x] [Easy]  CC([dynamo] Graph break on itertools.chain) * [x] [Easy]  CC(graph break caused by funcs defined in `torch._C` in dynamo)  Other Dynamo Issues  * [x] [Easy]  CC([torch.compile] Add `explain` as a backend) * [x] [Easy]  CC(`torch.compile` raises DataDependentOutputException with `torch.autograd.set_detect_anomaly`) * [x] [Medium]  CC(Sufficiently deep nesting + inlining + closure fails with KeyError) * [x] [Medium]  CC([torch.compile] doesn't support `permute(*tensor)`) * [x] [Medium]  CC([torch.compile] reduces `nn.Parameter` incorrectly to make the execution of model fail)   + [x] [Medium] ^^ followup to the above issue, which was workedaround with a graph break but underlying AOT Autograd + nn.Parameter issue still exists * [x] [Easy]  CC([dynamo] `call_function BuiltinVariable` and `op returned non-Tensor` cause graph breaks) * [x] [Easy]  CC(Dynamo doesn't support user de",2023-05-09T20:54:36Z,triaged oncall: pt2 module: dynamo,closed,5,0,https://github.com/pytorch/pytorch/issues/101011
409,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NameError('math is not defined') in Triton codegen for llama with dynamic shapes)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Repro:  Gives:  The full generated code is https://gist.github.com/ezyang/fd38597cbc2c61cee4710a8a0d871b2c    Versions master)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,NameError('math is not defined') in Triton codegen for llama with dynamic shapes, ğŸ› Describe the bug Repro:  Gives:  The full generated code is https://gist.github.com/ezyang/fd38597cbc2c61cee4710a8a0d871b2c    Versions master,2023-05-09T13:41:35Z,triaged oncall: pt2 module: dynamic shapes module: inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/100972,"I have run into this exact ""math is not defined"" error in the wild before. And I am getting what seems like a similar error in another llama repo using `dynamic=true`: https://github.com/qwopqwop200/GPTQforLLaMa/blob/triton/quant/custom_autotune.pyL66 llama_error.txt Which also seems similar to  CC([torch.compile] Name 'Ne' is not defined (Stable Diffusion)) Or maybe is all unrelated :shrug:","from : > sqrt is coming from here https://github.com/pytorch/benchmark/blob/main/torchbenchmark/models/llama/model.pyL138, self.head_dim is defined here https://github.com/pytorch/benchmark/blob/main/torchbenchmark/models/llama/model.pyL76, it should be specialized as it comes from integer module constructor arguments, but instead it is sent as an input to fx graph. How can we make it specialized?","Model doesn't fail if you force assumestaticbydefault:  The problem is that llama modifies nn modules:  in `torchbenchmark/models/llama/model.py` This causes the module to get unspecialized, which means we will attempt to compile accesses to its parameters with symbolic shapes if assume static by default is False (which it is). This policy... doesn't seem obviously wrong to me. In reality you would fix it by using assume static by default.  Maybe we should still fix the codegen though."
292,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AssertionError: slice.Tensor is not supported with cpp wrapper (llama))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,AssertionError: slice.Tensor is not supported with cpp wrapper (llama), ğŸ› Describe the bug   Versions master ,2023-05-09T13:02:38Z,triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/100968," , the latest error is  There is `test_view_as_complex` in `test_torchinductor.py`. You can add the test to `test_cpp_wrapper.py` to reproduce the problem and then debug from there."
2091,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(when i try to use FullyShardedDataParallel to finetune a LLM with huggingface peft library, pytorch raise error ValueError: `FlatParameter` requires uniform `requires_grad`)ï¼Œ å†…å®¹æ˜¯ ( Issue description:           when i try to use FullyShardedDataParallel to finetune a LLM with huggingface peft library, pytorch raise error ValueError:  `FlatParameter` requires uniform `requires_grad`. i also notice this issue: CC([RFC][FSDP] Rework `FlatParameter`, `FlattenParamsWrapper`). base on  the description of issue 76501, peft will not support in current pip installed pytorch 2.0.0 ?  Code example    model = ChatGLMForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=BASE_DIR,                                                             config=config)     print(""model loaded"")     peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,                              inference_mode=False,                              r=12,                              lora_alpha=32,                              lora_dropout=0.1)     model = get_peft_model(model, peft_config=peft_config)     print(""peft model loaded"")     model.print_trainable_parameters()     torch.cuda.set_device(rank)     glm_auto_wrap_policy = functools.partial(         transformer_auto_wrap_policy,         transformer_layer_cls={             GLMBlock,         },     )     model = FullyShardedDataParallel(model,                                      auto_wrap_policy=glm_auto_wrap_policy,                                      cpu_offload=CPUOffload(offload_params=True),                                      device_id=torch.cuda.current_device(),                                      sharding_strategy=ShardingStrategy.FULL_SHARD)     ...     normal train code  PyTorch  How you installed PyTorch: pip  OS: CentOS  PyTorch version:2.0.0  Python version:3.9.7  CUDA/cuDNN version: CUDA Version: 11.8  GPU models and configuration:NVIDIA T4 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",peft,"when i try to use FullyShardedDataParallel to finetune a LLM with huggingface peft library, pytorch raise error ValueError: `FlatParameter` requires uniform `requires_grad`"," Issue description:           when i try to use FullyShardedDataParallel to finetune a LLM with huggingface peft library, pytorch raise error ValueError:  `FlatParameter` requires uniform `requires_grad`. i also notice this issue: CC([RFC][FSDP] Rework `FlatParameter`, `FlattenParamsWrapper`). base on  the description of issue 76501, peft will not support in current pip installed pytorch 2.0.0 ?  Code example    model = ChatGLMForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=BASE_DIR,                                                             config=config)     print(""model loaded"")     peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,                              inference_mode=False,                              r=12,                              lora_alpha=32,                              lora_dropout=0.1)     model = get_peft_model(model, peft_config=peft_config)     print(""peft model loaded"")     model.print_trainable_parameters()     torch.cuda.set_device(rank)     glm_auto_wrap_policy = functools.partial(         transformer_auto_wrap_policy,         transformer_layer_cls={             GLMBlock,         },     )     model = FullyShardedDataParallel(model,                                      auto_wrap_policy=glm_auto_wrap_policy,                                      cpu_offload=CPUOffload(offload_params=True),                                      device_id=torch.cuda.current_device(),                                      sharding_strategy=ShardingStrategy.FULL_SHARD)     ...     normal train code  PyTorch  How you installed PyTorch: pip  OS: CentOS  PyTorch version:2.0.0  Python version:3.9.7  CUDA/cuDNN version: CUDA Version: 11.8  GPU models and configuration:NVIDIA T4 ",2023-05-09T04:18:43Z,triaged module: fsdp,closed,3,6,https://github.com/pytorch/pytorch/issues/100945,"This is a known limitation for FSDP. Every parameter in a module wrapped by `FullyShardedDataParallel` must have the same `requires_grad` for `use_orig_params=False`. You can try `use_orig_params=True` to avoid the error (using a PyTorch nightly build), but you may find that the memory usage is still high. Reducing that will require some fundamental design changes.","ok,thx,i will try deepspeed instead","I am encountering this problem. Can someone explain further what the answer above means? What exactly is ""use_orig_params"" and how does setting it to true/false change things?",I ran into this issue when using fixed sincos embeddings in a ViT model which has requires_grad=False. Is there a way to use sincos positional embeddings in a vision transformer model with FSDP?,Do you have plans to fix this?,"I am not up to date on the latest in HF PEFT, but if you use FSDP2 ( CC([RFC] Per-Parameter-Sharding FSDP), https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md), then you can mix frozen and nonfrozen parameters however you want. torchtune has examples, including for LoRA: https://github.com/pytorch/torchtune/blob/main/recipes/lora_finetune_distributed.py"
818,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Update timm version used in timm benchmarking suite and torchbench)ï¼Œ å†…å®¹æ˜¯ (Currently more than half a year old version is used for timm suite and 0.8.11.dev0 is used in torchbench. We should update to 0.8.21dev0 and keep torchbench and pt2 suites in sync.  This version enabled using fast scaled dot product attention on vision transformer models, and we should benchmark it.  Note: when updating timm suite, I expect that our speedups will meaningfully change (because of sdpa), we should make sure that absolute times improve, and provide a way to compare pre/post numbers. We should also make sure that fast path is actually taken in sdpa. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Update timm version used in timm benchmarking suite and torchbench,"Currently more than half a year old version is used for timm suite and 0.8.11.dev0 is used in torchbench. We should update to 0.8.21dev0 and keep torchbench and pt2 suites in sync.  This version enabled using fast scaled dot product attention on vision transformer models, and we should benchmark it.  Note: when updating timm suite, I expect that our speedups will meaningfully change (because of sdpa), we should make sure that absolute times improve, and provide a way to compare pre/post numbers. We should also make sure that fast path is actually taken in sdpa. ",2023-05-08T20:51:24Z,oncall: pt2,closed,0,0,https://github.com/pytorch/pytorch/issues/100903
268,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Reenable llama benchmark)ï¼Œ å†…å®¹æ˜¯ (  CC(Reenable llama benchmark) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Reenable llama benchmark,  CC(Reenable llama benchmark) Signedoffby: Edward Z. Yang  ,2023-05-08T13:44:10Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/100877," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
545,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve the functionality of untyped storage for privateuse1.)ï¼Œ å†…å®¹æ˜¯ (Complete the implementation of the interface  is_pinned() of untyped storage class for privateuse1. And refactor the implementation in typed storage by   untyped_storage.is_pinned(). Hi,     This is another improvement of untyped storage for privateuse1, can you  take a moment to review it?  Thanks.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Improve the functionality of untyped storage for privateuse1.,"Complete the implementation of the interface  is_pinned() of untyped storage class for privateuse1. And refactor the implementation in typed storage by   untyped_storage.is_pinned(). Hi,     This is another improvement of untyped storage for privateuse1, can you  take a moment to review it?  Thanks.",2023-05-08T12:02:30Z,triaged open source Merged ciflow/trunk release notes: python_frontend,closed,0,22,https://github.com/pytorch/pytorch/issues/100868, do you think you can take this one?,"Hi,  since the TypedStorage API will be deprecated in the future, I'm trying to implement is_pined()  in untyped storage to support other backends. The implementation of  is_pined()  in typed storage is only to invoke the interface in untyped storage. The default value of the â€˜deviceâ€™ parameter is â€˜cudaâ€™.  Therefore, users of the CUDA backend do not need to modify the usage of this interface. This parameter only needs to be set when other backends support the construction of pin_memory tensors. I sincerely look forward to your discussion and comments,    ",I think we should probably add a property function like `Tensor/UntypedStorage.pinned_device`. That can be in a different PR though,"> I think we should probably add a property function like `Tensor/UntypedStorage.pinned_device`. That can be in a different PR though Hi,       I am trying to understand this idea. Do you mean that we need to add an attribute to record the pinned device and a new function to obtain this attribute, like   ?","> Do you mean that we need to add an attribute to record the pinned device and a new function to obtain this attribute, like   ? Yeah that's what I'm suggesting. Although we could make it a readonly property to avoid needing the parentheses, so that it's semantically similar to `Tensor.device'","Yeah, in this latest PR, both  and  types are supported as input of  and . And in the implementation of , we always create the tensor without any conditions as you suggested before. , please review this change.","Hi,     Dose this change be finished as expected?  Would you please take a moment to review this pr, thanks a lot.",  merge," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," Merge failed **Reason**: Approval needed from one of the following: EscapeZero, 842974287, miguelmartin75, kauterry, priyaramani, ... Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," label ""release notes: python_frontend""","Hi,       I'm so sorry to bother you. I found that kurtamohler  has approved these changes, but I'm not sure if that means this pr can be appoveed?  If so, what else do I need to do to merge in this PR? Thanks a lot.","I'm not on the list of people whose approval allows a PR to be merged. When Edward has time, his approval will make it possible to merge", merge r, started a rebase job onto refs/remotes/origin/viable/strict. Check the current status here,"Successfully rebased `Masterstorageprivateuse1` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout Masterstorageprivateuse1 && git pull rebase`)"," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," Merge failed **Reason**: Approval needed from one of the following: laurencer, miguelmartin75, serhaty, jubinchheda, qxy11, ... Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," This PR needs a `release notes:` label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," label ""release notes: python_frontend""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1071,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to install standalone torch dynamo with pytorch1.x)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug For many reasons, the environment is not compatible with pytorch2.0. For example, MegatronLM compiles its transformer operators written in C++, which confine it to the limit of torch 1.x c++ extension, otherwise many compile errors. For another example, DeepSpeed implements their distributed trainer whose components depends on triton 1 but not triton 2 to build.   Error logs _No response_  Minified repro _No response_  Versions Therefore, could you be so kind to guide me how to install torchdynamo  independently without having a torch2.0? Or, are there other ways for compilation in torch1.0? I heard of torch.jit, but someone told me that it could not speed up training.  I would appreciate if there is any methods that work to speedup torch 1.x 's code with regard to fast Large Language Model training.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,How to install standalone torch dynamo with pytorch1.x," ğŸ› Describe the bug For many reasons, the environment is not compatible with pytorch2.0. For example, MegatronLM compiles its transformer operators written in C++, which confine it to the limit of torch 1.x c++ extension, otherwise many compile errors. For another example, DeepSpeed implements their distributed trainer whose components depends on triton 1 but not triton 2 to build.   Error logs _No response_  Minified repro _No response_  Versions Therefore, could you be so kind to guide me how to install torchdynamo  independently without having a torch2.0? Or, are there other ways for compilation in torch1.0? I heard of torch.jit, but someone told me that it could not speed up training.  I would appreciate if there is any methods that work to speedup torch 1.x 's code with regard to fast Large Language Model training.  ",2023-05-07T09:55:43Z,dependency issue oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/100827,Unfortunately this is not supported and will not be supported. Your best bet might be to figure out how to port megatronlm to pt2. I don't think we made that many c++ changes so it shouldn't be too bad. 
619,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(implement a function to materialize a copy-on-write storage)ï¼Œ å†…å®¹æ˜¯ (implement a function to materialize a copyonwrite storage Summary: This is triggered if the storage is copyonwrite whenever mutable access to a DataPtr is given. Test Plan: 100% code coverage  Stack created with Sapling. Best reviewed with ReviewStack.  CC(add new private operator copyonwrite torch._lazy_clone)  CC(implement a function to materialize a copyonwrite storage) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,implement a function to materialize a copy-on-write storage,implement a function to materialize a copyonwrite storage Summary: This is triggered if the storage is copyonwrite whenever mutable access to a DataPtr is given. Test Plan: 100% code coverage  Stack created with Sapling. Best reviewed with ReviewStack.  CC(add new private operator copyonwrite torch._lazy_clone)  CC(implement a function to materialize a copyonwrite storage) ,2023-05-07T01:05:41Z,module: internals triaged open source ciflow/trunk topic: not user facing ciflow/mps,closed,0,5,https://github.com/pytorch/pytorch/issues/100820,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","I'm continuing to work on COW storages, and the next step is to get this PR merged or submit a new PR to replace it. One issue is, as Edward pointed out already, we need the copy to work for any device. But it would be nice if we could just reuse the deviceagnostic copy that we already have, `at::native::copy_impl`. However, this PR puts the materialize function in `c10`, so we don't have access to `at` functions. There are at least three potential options to consider: 1. Reimplement deviceagnostic copy in `c10`, like dagitses decided to do. If this turns out to be a good solution, then great. But it seems worth considering alternatives that would avoid adding a second implementation of a deviceagnostic copy. This PR doesn't support all the devices that `at::native::copy_impl` supports yet, so if we go this route, the missing ones will need to be added. 2. Move the underlying implementation of deviceagnostic copy to `c10`. `at::native::copy_impl` would call into the `c10` implementation. However, there is at least one problem with this idea that probably makes it infeasible or impractical. `copy_impl` uses TensorIterator to enable broadcasting two tensors that aren't the same shape. The `c10` deviceagnostic copy would just be a contiguous data copy. I don't think it would really make sense to use a `c10` contiguous data copy to implement the broadcasting copy. 3. Move the materialize function to `at` so that it can use `at::native::copy_impl`. Obviously we would no longer be able to call the materialize function from `c10::StorageImpl::mutable_data` or any other `c10` context in this case. We'd also probably have to throw an error if `StorageImpl::mutable_data` is called on a COW storage. Any `at` operation that writes to a COW storage would have to call the materialize function at some point before the write happens. I don't know exactly how many call sites this would actually amount to, but it might not be a lot. I'm not 100% sure if this solution will be feasible or notif there are any calls to `StorageImpl::mutable_data` from within `c10` for which we cannot move the materialize function call up to `at` or another context that has access to `at`, then it won't be feasible After thinking through these options a little bit, I think option 1 is the best after all. Option 3 involves more unknowns and might end up being more complicated even though it avoids implementing a second deviceagnostic copy.  , I'm curious if you agree or have any other suggestions",(1) is fine by me. We have some precedent for device specific dispatch on DeviceGuardImpl and adding a copy doesn't seem like the end of the world to me.,Replaced by CC(Add function to materialize COW storages)
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(implement a function to convert a storage to copy-on-write)ï¼Œ å†…å®¹æ˜¯ (implement a function to convert a storage to copyonwrite Summary: This will be used in the _lazy_clone() operator as well as reshape(). Test Plan: 100% coverage of reachable lines.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(add new private operator copyonwrite torch._lazy_clone)  CC(implement a function to materialize a copyonwrite storage)  CC(implement a function to convert a storage to copyonwrite) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,implement a function to convert a storage to copy-on-write,implement a function to convert a storage to copyonwrite Summary: This will be used in the _lazy_clone() operator as well as reshape(). Test Plan: 100% coverage of reachable lines.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(add new private operator copyonwrite torch._lazy_clone)  CC(implement a function to materialize a copyonwrite storage)  CC(implement a function to convert a storage to copyonwrite) ,2023-05-07T01:05:40Z,module: internals triaged open source Merged Reverted ciflow/trunk topic: not user facing ciflow/mps,closed,0,9,https://github.com/pytorch/pytorch/issues/100819, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""added tests are breaking internal builds"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,">  revert m ""added tests are breaking internal builds"" c nosignal Hey Jean, sorry for the trouble. What was the nature of the failure?","> >  revert m ""added tests are breaking internal builds"" c nosignal >  > Hey Jean, sorry for the trouble. What was the nature of the failure?  ", Do you think I should just try landing this again without unit tests :P?,"Now that you're on the other side, I think you should try not to add new test files haha"
615,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Wrong type for `get_lr` inside lr_scheduler.pyi)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `lr_scheduler.pyi` describes `def get_lr(self) > float: ...` but the implementation returns `List[float]` https://github.com/pytorch/pytorch/blob/31f311a816c026bbfca622d6121d6a7fab44260d/torch/optim/lr_scheduler.pyiL19 https://github.com/pytorch/pytorch/blob/31f311a816c026bbfca622d6121d6a7fab44260d/torch/optim/lr_scheduler.pyL394  Versions Current master branch )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Wrong type for `get_lr` inside lr_scheduler.pyi, ğŸ› Describe the bug `lr_scheduler.pyi` describes `def get_lr(self) > float: ...` but the implementation returns `List[float]` https://github.com/pytorch/pytorch/blob/31f311a816c026bbfca622d6121d6a7fab44260d/torch/optim/lr_scheduler.pyiL19 https://github.com/pytorch/pytorch/blob/31f311a816c026bbfca622d6121d6a7fab44260d/torch/optim/lr_scheduler.pyL394  Versions Current master branch ,2023-05-06T16:15:24Z,module: optimizer module: typing triaged actionable module: LrScheduler,closed,0,4,https://github.com/pytorch/pytorch/issues/100804,"A brief glance at the lr schedulers does incriminate our get_lr() to be a lie in terms of type but also intention>it looks like get_lr() really gets the lr's per param group, but promises just one value. We would accept a preliminary fix that would:  [ ] Document correctly the intention of get_lr(). It looks like a bunch of the impls already redirect to get_last_lr already.  [ ] Rectify the type in the .pyi file (hopefully this is not BC breaking but I have unfortunate sneaking suspicions) The above can be multiple PRs. A better definitely bc breaking fix would:  [ ] Figure out the right APIs to offer in LR scheduler  [ ] Implement and do BC breaking things to rectify the API. There's no current maintainer for lr schedulers, so this option will not be easily committed to.","If we look at the  lr_scheduler.pyi , we can see that the type hint annotations for 2 methods are incorrect. As discussed above get_lr method is declared as returning a float when in fact it returns a list of floats However get_last_lr , returns a float, yet the type hint annotation reports a list of floats `   def get_last_lr(self) > List[float]: ...     def get_lr(self) > float: ...`   I traced the call stack for get_last_lr and the code does expect a single float value, and the implementation return a single float. I also trace the call stack and implementation of get_lr and this code also expects a list of floats. so the obvious solution seems to be that these type hint annotation have been switched in error and simply flipping the annotations  back seems to address the problem. As the changes are trivial I've opened a PR to try and close this issue.",Still facing this issue now. Would be happy to put up a PR but see that CC(fix Type Hinting Annotations) was not merged in. Is there any reason? ," lr_scheduler.pyi should have been merged into the same file as lr_scheduler.py now, so this issue should be fixed. Please reopen if something is still not corrected. And for posterity, the reason that PR wasn't merged was because get_last_lr also returns a list and not a float."
1973,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Quant][pt2e] Failed to run pt2e flow on LLaMA)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When try to enable the pt2e flow on transformer models, taking `LLaMA` as example. We meet the issue that these models can't generate the FX graph by `dynamo.export` API. To reproduce this issue: Please download the models from here. Rename the downloaded folder from `llama7bhf` to `llama7b`. Then test with below script.  For details of the error message, please refer to this gist .  Versions Collecting environment information... PyTorch version: 2.1.0a0+git3362c1d Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.12) Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.17 Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.19.51.el7.elrepo.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:          x86_64 CPU opmode(s):        32bit, 64bit Byte Order:            Little Endian CPU(s):                56 Online CPU(s) list:   055 Thread(s) per core:    1 Core(s) per socket:    28 Socket(s):             2 NUMA node(s):          2 Vendor ID:             GenuineIntel CPU family:            6 Model:                 85 Model name:            Intel Genuine CPU Stepping:              10 CPU MHz:               2900.698 CPU max MHz:           2900.0000 CPU min MHz:           1200.0000 BogoMIPS:              5800.00 Virtualization:        VTx L1d cache:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,[Quant][pt2e] Failed to run pt2e flow on LLaMA," ğŸ› Describe the bug When try to enable the pt2e flow on transformer models, taking `LLaMA` as example. We meet the issue that these models can't generate the FX graph by `dynamo.export` API. To reproduce this issue: Please download the models from here. Rename the downloaded folder from `llama7bhf` to `llama7b`. Then test with below script.  For details of the error message, please refer to this gist .  Versions Collecting environment information... PyTorch version: 2.1.0a0+git3362c1d Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.12) Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.17 Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.19.51.el7.elrepo.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:          x86_64 CPU opmode(s):        32bit, 64bit Byte Order:            Little Endian CPU(s):                56 Online CPU(s) list:   055 Thread(s) per core:    1 Core(s) per socket:    28 Socket(s):             2 NUMA node(s):          2 Vendor ID:             GenuineIntel CPU family:            6 Model:                 85 Model name:            Intel Genuine CPU Stepping:              10 CPU MHz:               2900.698 CPU max MHz:           2900.0000 CPU min MHz:           1200.0000 BogoMIPS:              5800.00 Virtualization:        VTx L1d cache:  ",2023-05-06T08:28:03Z,oncall: quantization triaged oncall: pt2,open,0,20,https://github.com/pytorch/pytorch/issues/100796,"Hi , we find `dynamo.export` failed to work on several huggingface models (not count the overall passrate among hugging face workloads yet). Appreciate of any suggestions for this kind of issue. Also . ",Are you applying quantization to this model? This looks like a torch compile or fx issue to me.,"Hi , thanks for the comment. Current quantization 2.0 flow depends on API of `dynamo export` to generate the whole graph. But `dynamo export` failed on LLaMA as I tested above. Did you mean `dynamo export` should work on LLaMA? And current status is a issue of `dynamo export`?",yeah this is a torchdynamo issue I think,"I guess it is because there are multiple graph breaks in the `model.generate` call. Even though, there is no graph break in the forward of the decoder module but there are breaks in the input preprocessing and postprocessing (e.g., beam search) inside the decoder loop. One of the primary causes is the usage of user defined data types like `LogitsProcessorList`, `StoppingCriteriaList` etc. Also, I'm not sure how the export path would support such a decoder loop with dynamic exit criteria even if there is no graph break in the body. A question: does the quantization flow have to rely on the export path? Have you considered to do it in the justintime manner? For example, we do the prepare/convert for each subgraph captured by dynamo.","> A question: does the quantization flow have to rely on the export path? Have you considered to do it in the justintime manner? For example, we do the prepare/convert for each subgraph captured by dynamo. We have considered this at length.  There are a couple of challenges: 1. teams doing production inference are using whole program capture, so even if we had quantization working with partial graphs this isn't something those teams could use 2. calibration/finetuning is not well defined/hasn't been extensively studied over arbitrary control flow  even if the APIs work, we still need to get good accuracy over all program paths for the output to be useful. It's probably solvable, but we haven't prioritized it because of the uncertainty of (1) and high eng cost of (2).","> It's probably solvable, but we haven't prioritized it because of the uncertainty of (1) and high eng cost of (2). Thanks for the info. Are you open for having this online quantization flow as an alternative to the export path? If so, fangintel and I can probably think further for a proposal."," , just curious, do you have a strong use case which the online quantization flow without whole graph program capture would solve?",">  , just curious, do you have a strong use case which the online quantization flow without whole graph program capture would solve? I guess the main motivation of this is to get the models quantized easily without the need of intrusively modifying them. LLAMA is one of such cases where whole graph capture is not easy while we still want to quantize the subgraphs, e.g., the decoder forward graph, to get the performance benefit. Does that make sense to you?","> I guess the main motivation of this is to get the models quantized easily without the need of intrusively modifying them. LLAMA is one of such cases where whole graph capture is not easy while we still want to quantize the subgraphs, e.g., the decoder forward graph, to get the performance benefit. Does that make sense to you? Yeah, of course that makes sense.  This is a hard problem and I haven't seen a good solution yet. Doing quantization over dynamo subgraphs would be straightforward if the following conditions are met: a. the subgraphs and graph breaks do not change once we called the quantization APIs b. we do not need to reason across subgraph boundaries (i.e. all of the subgraphs take in and return floating point tensors) c. there is either no control flow, or the user finetunes/calibrates the model carefully in such a way that all of the branches are hit enough times d. the end customer would need to be open to doing inference without a full program graph, knowing they are leaving performance on the table. I'd be curious on your thoughts on these.","> Yeah, of course that makes sense. This is a hard problem and I haven't seen a good solution yet. Agreed that it is a hard problem. Thanks for raising the following aspects of the problems. I think I need to think about it deeply to explore complete solutions. I just provide some rough thoughts below. Welcome to any feedbacks. > a. the subgraphs and graph breaks do not change once we called the quantization APIs Not sure if it is a must but if it is, we can consider to leverage the runonly mode of dynamo for that. The flow in my mind could be like this. Similar to the prepare/convert API, there could be two modes: prepare mode and convert mode. We run dynamo with the ""prepare mode"" first. The captured subgraphs are rewritten to prepared subgraphs while the model is fed with calibration data on PTQ or training data on QAT. Then, we switch to the ""convert mode"" and the cached prepared subgraphs are converted to the quantized subgraphs before the first execution. > b. we do not need to reason across subgraph boundaries (i.e. all of the subgraphs take in and return floating point tensors) Yes. We should make sure that the input and output are all floating point tensors. It would be part of the quantization recipe that decides what tensors are quantized and what are not. > c. there is either no control flow, or the user finetunes/calibrates the model carefully in such a way that all of the branches are hit enough times I think we can allow users to be aware of it and feed enough data to exercise the branches if possible?  > d. the end customer would need to be open to doing inference without a full program graph, knowing they are leaving performance on the table. Yes, end users need to be aware of it. On the other hand, they still have choices to intrusively modify the model for quantization if they believe better perf gain can be achieved.","Yeah, if you have a big enough slice of cases where such a system would be valuable, it could make sense.  It would be nice to have this in the ecosystem. Some other things to keep in mind: * for singlemodule/single function (weight only or dynamic quant), you can do it much simpler with module swaps/tensor subclass/parametrizations * for 100% of the model in static quant, you still need full graph * you can use existing tools to quantize smaller chunks of the model if the entire model is not quantizeable * the export ecosystem is going to improve over time, improving model coverage We have considered what you are proposing at length last year and ultimately decided against it, we are happy to share any additional learnings that would be helpful if you are interested in trying something like this.   was deeply involved in this as well.","> Even though, there is no graph break in the forward of the decoder module but there are breaks in the input preprocessing and postprocessing (e.g., beam search) inside the decoder loop. One of the primary causes is the usage of user defined data types like LogitsProcessorList, StoppingCriteriaList etc. Also, I'm not sure how the export path would support such a decoder loop with dynamic exit criteria even if there is no graph break in the body. given this evidence, would a simple fix here would be to just quantize the decoder, and not the entire model?","> We have considered what you are proposing at length last year and ultimately decided against it, we are happy to share any additional learnings that would be helpful if you are interested in trying something like this.  was deeply involved in this as well. Thanks for listing the alternatives and would be interested to learn more about your learnings. BTW, we can definitively do dynamic/weightonly quant on these LLMs by model swapping while static quantization usually brings best performance but is harder to support for these models. > given this evidence, would a simple fix here would be to just quantize the decoder, and not the entire model? I think this is doable but there are two aspects that would impact the UX: 1. One might need to modify the original transformers model code to use the quantized module. There might be ways to patch the model to avoid intrusive modification though, need to look deep. But anyway, users have to know how the model is implemented to rewrite/wrap the model code, and need to have knowledge what parts can be exported and what not, sounds like a high bar. 2. The model export needs one to construct model inputs. It is harder to construct this for a submodule of the model.","Hi  , considering of the complexity in doing static quant to models with graph break. How about we add the `pt2e dynamic quantization` flow as a backend of `torch.compile`. Some pseudocode might be:  BTW: Looks like the dynamic quant flow is not supported in `qnnpack_quantizer` yet.","agreed, restricting it to dynamic quant would get around a lot of the challenges of not having a full graph","> The flow in my mind could be like this. Similar to the prepare/convert API, there could be two modes: prepare mode and convert mode. We run dynamo with the ""prepare mode"" first. The captured subgraphs are rewritten to prepared subgraphs while the model is fed with calibration data on PTQ or training data on QAT. Then, we switch to the ""convert mode"" and the cached prepared subgraphs are converted to the quantized subgraphs before the first execution. >  here is a prototype I tested when dynamo just came out: https://github.com/pytorch/torchdynamo/pull/256, we didn't go with this immediately because the reasons that  mentioned: (1) all our customers are doing full program capture anyways (2) we are asked to work with aten IR (output of export), instead of torch IR (intermediate IR during dynamo tracing) but we are open to explore this further if there is a need I think","> Hi  , considering of the complexity in doing static quant to models with graph break. How about we add the `pt2e dynamic quantization` flow as a backend of `torch.compile`. Some pseudocode might be: >  >  >  > BTW: Looks like the dynamic quant flow is not supported in `qnnpack_quantizer` yet. I see, this might work today already I think, dynamic quant support is on the way, should be supported in ~2 weeks","keep in mind that for dynamic quant of linears, you could implement it with module swap or tensor subclass or weight parametrization with like 10% of the complexity of doing it over arbitrary dynamo subraphs","> here is a prototype I tested when dynamo just came out: https://github.com/pytorch/torchdynamo/pull/256,  Thanks for the pointer. Didn't realize you already have a PoC on it. That's great! :) > keep in mind that for dynamic quant of linears, you could implement it with module swap or tensor subclass or weight parametrization with like 10% of the complexity of doing it over arbitrary dynamo subraphs Agreed that doing this from the PyTorch frontend could be one of the options. In order to leverage the compiler backend, I guess the flow would be: 1) swap the module with a ref dynamic/weightonly quant module; 2) do torch.compile on the swapped module?"
416,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([IValue] Better handle sparseTensors in extractStorages)ï¼Œ å†…å®¹æ˜¯ (  CC([IValue] Better handle sparseTensors in extractStorages) Sparse tensors don't seem to be handled when we have tensors instead of pyobjects. Differential Revision: D45632427)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[IValue] Better handle sparseTensors in extractStorages,  CC([IValue] Better handle sparseTensors in extractStorages) Sparse tensors don't seem to be handled when we have tensors instead of pyobjects. Differential Revision: D45632427,2023-05-06T02:49:54Z,Merged release notes: jit,closed,0,2,https://github.com/pytorch/pytorch/issues/100783," merge f ""CI done"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
552,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Refactor is_causal detection into subroutine and clarify doc strings that is_causal is a hint)ï¼Œ å†…å®¹æ˜¯ (Summary: * Clarify documentation that is_causal is a hint * Refactor by putting is_causal detection in a separate utility function Split from janEbert's ""Implement is_causal API for Transformer CC(Implement `is_causal` API for `Transformer`)"" Differential Revision: D45628308)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Refactor is_causal detection into subroutine and clarify doc strings that is_causal is a hint,"Summary: * Clarify documentation that is_causal is a hint * Refactor by putting is_causal detection in a separate utility function Split from janEbert's ""Implement is_causal API for Transformer CC(Implement `is_causal` API for `Transformer`)"" Differential Revision: D45628308",2023-05-06T00:00:35Z,fb-exported Stale suppress-bc-linter,closed,0,21,https://github.com/pytorch/pytorch/issues/100774,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,This pull request was **exported** from Phabricator. Differential Revision: D45628308,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
527,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Unary ops yield wrong results if striding is different)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Consider the following code  it will print  but should have printed  I guess error stems from this check: https://github.com/pytorch/pytorch/blob/c9593bc0e15b661dc90286eeaf503f31a3c9df78/aten/src/ATen/native/mps/operations/UnaryOps.mmL84L88  Versions 2.0/nightly )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[MPS] Unary ops yield wrong results if striding is different, ğŸ› Describe the bug Consider the following code  it will print  but should have printed  I guess error stems from this check: https://github.com/pytorch/pytorch/blob/c9593bc0e15b661dc90286eeaf503f31a3c9df78/aten/src/ATen/native/mps/operations/UnaryOps.mmL84L88  Versions 2.0/nightly ,2023-05-05T22:43:09Z,high priority triaged module: regression module: correctness (silent) module: mps,closed,1,2,https://github.com/pytorch/pytorch/issues/100764,"I've made a PR fixing this two months ago... Thanks for the repro, added to the test.",We should check if this still reproduces on nightly.
795,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Reland] Add sym_size/stride/numel/storage_offset to native_function.â€¦)ï¼Œ å†…å®¹æ˜¯ (â€¦yaml ( CC(rename FullConv*d to ConvTranspose*d)â€¦ ( CC(Add sym_size/stride/numel/storage_offset to native_function.yaml)) Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/91919 Approved by: https://github.com/ezyang Fixes ISSUE_NUMBER Pull Request resolved: https://github.com/pytorch/pytorch/pull/92402 Reviewed By: ezyang Differential Revision: D42565586 Pulled By: SherlockNoMad fbshipitsourceid: 1c2986e45307e076d239836a1b45441a9fa3c9d9 ghstacksourceid: 969f4928486e04c57aaf98e20e3c3ca946c51613 Fixes ISSUE_NUMBER )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Reland] Add sym_size/stride/numel/storage_offset to native_function.â€¦,â€¦yaml ( CC(rename FullConv*d to ConvTranspose*d)â€¦ ( CC(Add sym_size/stride/numel/storage_offset to native_function.yaml)) Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/91919 Approved by: https://github.com/ezyang Fixes ISSUE_NUMBER Pull Request resolved: https://github.com/pytorch/pytorch/pull/92402 Reviewed By: ezyang Differential Revision: D42565586 Pulled By: SherlockNoMad fbshipitsourceid: 1c2986e45307e076d239836a1b45441a9fa3c9d9 ghstacksourceid: 969f4928486e04c57aaf98e20e3c3ca946c51613 Fixes ISSUE_NUMBER ,2023-05-05T20:29:26Z,Merged Reverted ciflow/trunk release notes: jit topic: not user facing module: dynamo ciflow/inductor,closed,0,29,https://github.com/pytorch/pytorch/issues/100749," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","> I'm curious why you need these to be native function? They need to be properly aten op, so that they can be included in the Core Aten IR.  Also, folks are expecting aten.sym_size to be an OpOverload  CC(aten::sym_size is not using torch._ops.OpOverload in FX graph) Also, we need schema for them. "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",But all of these would work with a registration from python right?,"> But all of these would work with a registration from python right? Sorry, I missed one requirement.  Sigmoid will need to run this op via JIT kernel, and need to access its schema from C++.  So the registration need to be persistent in libtorch. "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",Do you mean a libtorchonly build will need to access it? The usual dispatcher API to get a kernel even if it wasn't defined in c++. We do that to get our pythonregistered triton kernel here for example: https://github.com/pytorch/pytorch/blob/8769fb854d816d223cfb513979e97e23a93bddee/aten/src/ATen/native/sparse/SparseBlasImpl.cppLL97C10L97C14,  rebase s , successfully started a rebase job. Check the current status here,"Successfully rebased `bahuang/sym_size_reland` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout bahuang/sym_size_reland && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",> Do you mean a libtorchonly build will need to access it?  yeah, rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `bahuang/sym_size_reland` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout bahuang/sym_size_reland && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `bahuang/sym_size_reland` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout bahuang/sym_size_reland && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""It makes ExecuTorch sad, please land via codev and have better description"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team,Can't revert PR that was landed via phabricator as D42565586.  Please revert by going to the internal diff and clicking Unland.,Manually pushed the revert in https://github.com/pytorch/pytorch/commit/20cf42de2c4ea44f9f4652e986daa1a5f81ba06e,FYI the correct linked diff for this PR is D45620400,This pull request was **exported** from Phabricator. Differential Revision: D45620400
514,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([feature] Support TypedStorage type() on custom device.)ï¼Œ å†…å®¹æ˜¯ (Support TypedStorage type() on custom device, this pr wants to deal with the following case. When type() is used for a cudastorage, ""torch.cuda.xxxStorage"" is returned. Therefore, for a custom device storage, the return value is expected to be ""torch.privateuse1.xxxStorage"".)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[feature] Support TypedStorage type() on custom device.,"Support TypedStorage type() on custom device, this pr wants to deal with the following case. When type() is used for a cudastorage, ""torch.cuda.xxxStorage"" is returned. Therefore, for a custom device storage, the return value is expected to be ""torch.privateuse1.xxxStorage"".",2023-05-05T08:55:48Z,triaged open source,closed,0,2,https://github.com/pytorch/pytorch/issues/100699,"Hi,    This is another modification to implement the functionality of custom device, would you please take a moment to review it, thanks.","> No, we're deprecating the TypedStorage API, we're not going to add private use support for it. Use UntypedStorage instead. >  > If there are downstream APIs that require TypedStorage, I will help review PRs that make them use UntypedStorage. >  > , I get it, thanks"
353,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add unit test for nested_tensor input to nn.TransformerEncoder)ï¼Œ å†…å®¹æ˜¯ (Summary: Add unit test for nested_tensor input & fix Test Plan: sandcastle Differential Revision: D45580393)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add unit test for nested_tensor input to nn.TransformerEncoder,Summary: Add unit test for nested_tensor input & fix Test Plan: sandcastle Differential Revision: D45580393,2023-05-04T19:54:55Z,better-engineering fb-exported Merged ciflow/trunk topic: not user facing bug,closed,0,28,https://github.com/pytorch/pytorch/issues/100650,This pull request was **exported** from Phabricator. Differential Revision: D45580393,> Why not just move the `output.size()` call here: >  > https://github.com/pytorch/pytorch/blob/8994d9e6109c541a1d581c383e4de9ed68205d91/torch/nn/modules/transformer.pyL318L319 >  > `output_size` isn't used anywhere else AFAICT. Good point!  One consideration  do we benefit from not having src live until the end of the TransformerEncoder stack?,> That is correct. What change are you suggesting? Maybe something like this: ,"> > That is correct. What change are you suggesting? >  > Maybe something like this: >  >  Yes, this makes sense  great point and simplifies code! Would increasing the defineduse range for src across TransformerEncoder be a concern?  We concluded it does not, because the caller would still have a reference preventing GC",This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 2, 3, macosm112) Details for Dev Infra team Raised by workflow job ",This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393,This pull request was **exported** from Phabricator. Differential Revision: D45580393, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2002,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Synchronization issue when combining DPP and RPC - ""Parameter marked twice"")ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm using DDP in conjunction with RPC to create a masterworker setup to train a DQN agent using multiple GPUs in parallel. The master process manages the game and memory buffer and fits one of the partitions of the data, while the workers only fit their respective partitions of the data. I'm training on 2 NVIDIA 3090 GPUs within a single machine. The minimal code to reproduce the error is shown below. The problem appears to be that the master process triggers a second call to the worker's `_train_on_batch` before the first call is completed. As a result, the parameters of the worker get marked twice, which throws the error. This seems like a race condition since the error isn't reproduced 100% of the time. I added some prints to the code (not shown in the example for clarity purposes) which confirm this is the issue:  In the logs above, `rank` is the process rank (`0` for master, `1` for worker), `local_iter` is the `_train_on_batch` call number (i.e `0` for the first call, `1` for the second call), and `master_iter` is the number of `_train_on_batch` calls the master process has completed (iterations numbers are incremented just before exiting `_train_on_batch`). `backwd done` and `optim done` means the process has returned from the `loss.backward` and `optimizer.step` calls, respectively. We see that the master process has finished it's first training iteration when it signals the worker to start its second iteration, while the latter hasn't finished its first iteration yet. The error is fixed adding a `dist.barrier` before exiting `_train_on_batch` method, which points towards this being a process synchronization issue. However, the DDP documentation states that  > Constructor, forward metho)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,"Synchronization issue when combining DPP and RPC - ""Parameter marked twice"""," ğŸ› Describe the bug I'm using DDP in conjunction with RPC to create a masterworker setup to train a DQN agent using multiple GPUs in parallel. The master process manages the game and memory buffer and fits one of the partitions of the data, while the workers only fit their respective partitions of the data. I'm training on 2 NVIDIA 3090 GPUs within a single machine. The minimal code to reproduce the error is shown below. The problem appears to be that the master process triggers a second call to the worker's `_train_on_batch` before the first call is completed. As a result, the parameters of the worker get marked twice, which throws the error. This seems like a race condition since the error isn't reproduced 100% of the time. I added some prints to the code (not shown in the example for clarity purposes) which confirm this is the issue:  In the logs above, `rank` is the process rank (`0` for master, `1` for worker), `local_iter` is the `_train_on_batch` call number (i.e `0` for the first call, `1` for the second call), and `master_iter` is the number of `_train_on_batch` calls the master process has completed (iterations numbers are incremented just before exiting `_train_on_batch`). `backwd done` and `optim done` means the process has returned from the `loss.backward` and `optimizer.step` calls, respectively. We see that the master process has finished it's first training iteration when it signals the worker to start its second iteration, while the latter hasn't finished its first iteration yet. The error is fixed adding a `dist.barrier` before exiting `_train_on_batch` method, which points towards this being a process synchronization issue. However, the DDP documentation states that  > Constructor, forward metho",2023-05-03T21:29:21Z,oncall: distributed,open,0,3,https://github.com/pytorch/pytorch/issues/100582,"Synchronization happens in relation to the distributed backend used which, in your case is NCCL. This means that after a train step, the *GPUs* on all ranks will be in sync, but not their CPUs. This is what causes this order confusion. `dist.barrier()` be default synchronizes CPU as well, which is why it fixes your issue. One way to address this is to have the workers run the train loop by themselves and you use RPC to ship the batch only.","thanks for your reply   I didn't know that only the GPUs would be in sync. Regarding your last comment, do you have any examples on how this would look like codewise? Maybe using RPC to set an attribute in the worker containing the batch, and having the workers check when they have a received a new batch, calling the train loop by themselves? ","> thanks for your reply   I didn't know that only the GPUs would be in sync. Regarding your last comment, do you have any examples on how this would look like codewise? Maybe using RPC to set an attribute in the worker containing the batch, and having the workers check when they have a received a new batch, calling the train loop by themselves? What you can do is do an rpc call that adds the tensors to a queue and have the clients wait trying to pop from such queue."
366,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove speech_transformer workaround, torchbench handles it correctly now)ï¼Œ å†…å®¹æ˜¯ (  CC(Remove speech_transformer workaround, torchbench handles it correctly now) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Remove speech_transformer workaround, torchbench handles it correctly now","  CC(Remove speech_transformer workaround, torchbench handles it correctly now) Signedoffby: Edward Z. Yang  ",2023-05-03T16:40:14Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/100558," merge f ""ci failure looks unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
568,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(bsr_dense_mm(): better test coverage)ï¼Œ å†…å®¹æ˜¯ (This PR improves test coverage for `bsr_dense_mm` by:  ~~enabling correctness tests for `float32`~~.  extending and testing input correctness checks.   CC(bsr_dense_bmm(): enable more precise float32 support with float64 accumulators)  CC(bsr_dense_bmm(): remove sparse_rowspace kernel and some dead code)  CC(bsr_dense_mm(): better test coverage) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,bsr_dense_mm(): better test coverage,This PR improves test coverage for `bsr_dense_mm` by:  ~~enabling correctness tests for `float32`~~.  extending and testing input correctness checks.   CC(bsr_dense_bmm(): enable more precise float32 support with float64 accumulators)  CC(bsr_dense_bmm(): remove sparse_rowspace kernel and some dead code)  CC(bsr_dense_mm(): better test coverage) ,2023-05-03T14:35:30Z,module: sparse open source Merged ciflow/trunk release notes: sparse,closed,0,2,https://github.com/pytorch/pytorch/issues/100543, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1270,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add load_storage)ï¼Œ å†…å®¹æ˜¯ (  CC(Add offloadtodisk support to minifier)  CC(Add load_storage) This adds a new operator debugprims::load_storage which does the unusual thing of loading a tensor from disk (via ContentStoreReader). This will be used in a later PR to implement delta debugging in the minifier, even when the repro is too big to fit into memory. The way it works is that you specify a name of the tensor you want to load, as well as enough metadata to reconstruct the tensor, if the store isn't available. If there is an active content store, we read and return the tensor from that store; otherwise we use `rand_strided` to create it. I needed some infra improvements to do this: * `custom_op` now supports factory functions. Factory functions have to be registered specially via `impl_factory` * I modified `clone_input` to also support dtype conversion, which I use to change the dtype of a loaded tensor if necessary. * ContentStore needs to work with a device argument, so we torch.load directly to the correct device. This is for fake tensor support. Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add load_storage,"  CC(Add offloadtodisk support to minifier)  CC(Add load_storage) This adds a new operator debugprims::load_storage which does the unusual thing of loading a tensor from disk (via ContentStoreReader). This will be used in a later PR to implement delta debugging in the minifier, even when the repro is too big to fit into memory. The way it works is that you specify a name of the tensor you want to load, as well as enough metadata to reconstruct the tensor, if the store isn't available. If there is an active content store, we read and return the tensor from that store; otherwise we use `rand_strided` to create it. I needed some infra improvements to do this: * `custom_op` now supports factory functions. Factory functions have to be registered specially via `impl_factory` * I modified `clone_input` to also support dtype conversion, which I use to change the dtype of a loaded tensor if necessary. * ContentStore needs to work with a device argument, so we torch.load directly to the correct device. This is for fake tensor support. Signedoffby: Edward Z. Yang  ",2023-05-03T03:36:53Z,Merged ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/100519
627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Improve minifier printing to be more chatty when it makes sense)ï¼Œ å†…å®¹æ˜¯ (  CC(Add offloadtodisk support to minifier)  CC(Add load_storage)  CC(Improve minifier printing to be more chatty when it makes sense)  CC(Misc accuracy improvements on minifier)  CC(Relax after_aot restriction on no buffers, serialize small constants)  CC(Refactor minifier tests to be more compact)  CC(Make hash_storage work with size 0/1 storage) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,Improve minifier printing to be more chatty when it makes sense,"  CC(Add offloadtodisk support to minifier)  CC(Add load_storage)  CC(Improve minifier printing to be more chatty when it makes sense)  CC(Misc accuracy improvements on minifier)  CC(Relax after_aot restriction on no buffers, serialize small constants)  CC(Refactor minifier tests to be more compact)  CC(Make hash_storage work with size 0/1 storage) Signedoffby: Edward Z. Yang  ",2023-05-02T19:24:31Z,Merged ciflow/trunk release notes: fx module: inductor module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/100486," merge f ""spurious master failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
2014,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TransformerEncoderLayer always warns when using src_key_padding_mask in inference mode.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Gives the warning:  This is because the layer is converting it to a float mask regardless of what the user passes, and then ATen warns about requiring a bool mask. This warning appears on every call when doing inference with libtorch.  Versions Collecting environment information... PyTorch version: 2.0.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (GCC) 12.2.1 20230201 Clang version: 15.0.7 CMake version: version 3.25.0 Libc version: glibc2.37 Python version: 3.10.10 (main, Mar  5 2023, 22:26:53) [GCC 12.2.1 20230201] (64bit runtime) Python platform: Linux6.2.13arch11x86_64withglibc2.37 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 530.41.03 cuDNN version: Probably one of the following: /usr/lib/libcudnn.so.8.8.0 /usr/lib/libcudnn_adv_infer.so.8.8.0 /usr/lib/libcudnn_adv_train.so.8.8.0 /usr/lib/libcudnn_cnn_infer.so.8.8.0 /usr/lib/libcudnn_cnn_train.so.8.8.0 /usr/lib/libcudnn_ops_infer.so.8.8.0 /usr/lib/libcudnn_ops_train.so.8.8.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   43 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          32 Online CPU(s) list:             031 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen Threadripper 1950X 16Core Processor CPU family:                      23 Model:                           1 Thread(s) per)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,TransformerEncoderLayer always warns when using src_key_padding_mask in inference mode.," ğŸ› Describe the bug  Gives the warning:  This is because the layer is converting it to a float mask regardless of what the user passes, and then ATen warns about requiring a bool mask. This warning appears on every call when doing inference with libtorch.  Versions Collecting environment information... PyTorch version: 2.0.0+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (GCC) 12.2.1 20230201 Clang version: 15.0.7 CMake version: version 3.25.0 Libc version: glibc2.37 Python version: 3.10.10 (main, Mar  5 2023, 22:26:53) [GCC 12.2.1 20230201] (64bit runtime) Python platform: Linux6.2.13arch11x86_64withglibc2.37 Is CUDA available: True CUDA runtime version: 12.1.105 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 530.41.03 cuDNN version: Probably one of the following: /usr/lib/libcudnn.so.8.8.0 /usr/lib/libcudnn_adv_infer.so.8.8.0 /usr/lib/libcudnn_adv_train.so.8.8.0 /usr/lib/libcudnn_cnn_infer.so.8.8.0 /usr/lib/libcudnn_cnn_train.so.8.8.0 /usr/lib/libcudnn_ops_infer.so.8.8.0 /usr/lib/libcudnn_ops_train.so.8.8.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   43 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          32 Online CPU(s) list:             031 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen Threadripper 1950X 16Core Processor CPU family:                      23 Model:                           1 Thread(s) per",2023-05-02T14:06:59Z,module: cpp module: nn needs research,closed,1,6,https://github.com/pytorch/pytorch/issues/100469,"It does look like https://github.com/pytorch/pytorch/blob/843ead134ca6f168df98912efaeea72a0b380909/torch/nn/functional.pyL5042 this line always converts a mask to float.  are you familiar with why this is the case? Regardless, I do agree this is a confusing warning message that should be fixed.",Hm the behavior in that line seems aligned with the documentation for key_padding_mask here  This warning seems to come from masked_softmax. Perhaps a bool mask is needed in some paths (e.g. need_weights) and not others  ,"nn.MHA says that float and Boolean are supported, so we should not warn when there's a legal type passed => https://github.com/pytorch/pytorch/blob/843ead134ca6f168df98912efaeea72a0b380909/torch/nn/modules/activation.pyL1065L1069 Almost all scenarios use a float mask adding infinity to mask out elements today. It might be most expedient to just remove the warning.  (We might think how we change the mask or the mask usage for inference, depending what the best representation is. This might be expedient to include at the time when we refactor the current C++ implementation.)","The warning is printed only once and can easily be filtered out by `warnings.filterwarnings(""ignore"")` unless the model is compiled. Otherwise they get printed for every single layer of the transformer, on every inference run, which is extremely distractive and slows down the process. Any clues how to disable them for a cpp backend without recompiling entire project?  Other things that **do not** disable warnings:  adding `W ignore` as python parameter  Suppressing entire stdout and stderr streams     `export PYTHONWARNINGS=ignore`","At a minimum we should just remove the warning from the CPP execution path.    said: > The warning is printed only once and can easily be filtered out by warnings.filterwarnings(""ignore"") unless the model is compiled. Otherwise they get printed for every single layer of the transformer, on every inference run, which is extremely distractive and slows down the process. What layer and overall model structure are you using?  src_key_padding_mask should be folded into a Nested Tensor in TransformerEncoder, so that downstream layers can execute with variable length inputs .   This is happening here in transformer.py => https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.pyL315 Why are you calling the layer separately in this repro? Does this also happen when you call TransformerEncoder rather than just TransformerEncoderLayer?  If there are several layers of T*EncoderLayer, it'll be advantageous to use code similar to what TransformerEncoder does to create Nested Tensors.   That being said, I agree that the error message is a nuisance and we should fix it.","> Hm the behavior in that line seems aligned with the documentation for key_padding_mask here >  > This warning seems to come from masked_softmax. Perhaps a bool mask is needed in some paths (e.g. need_weights) and not others >  > !  I think step 1 is removing the symptom, not the cause.  In principle we could check whether we're going into native_* cpp code (same conditions and make producing canonical masks produce Bool canonical masks if we're on the trajectory to call native* cpp code.  Since we've discussing some refactoring of those paths in the future, that might be the right time to fix this holistically, and right now we can just fix the nuisance of a warning that the user can't plausibly fix."
542,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make hash_storage work with size 0/1 storage)ï¼Œ å†…å®¹æ˜¯ (  CC(Improve minifier printing to be more chatty when it makes sense)  CC(Misc accuracy improvements on minifier)  CC(Relax after_aot restriction on no buffers, serialize small constants)  CC(Refactor minifier tests to be more compact)  CC(Make hash_storage work with size 0/1 storage) Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Make hash_storage work with size 0/1 storage,"  CC(Improve minifier printing to be more chatty when it makes sense)  CC(Misc accuracy improvements on minifier)  CC(Relax after_aot restriction on no buffers, serialize small constants)  CC(Refactor minifier tests to be more compact)  CC(Make hash_storage work with size 0/1 storage) Signedoffby: Edward Z. Yang ",2023-05-02T13:46:52Z,Merged ciflow/trunk release notes: fx topic: bug fixes release notes: dynamo,closed,0,0,https://github.com/pytorch/pytorch/issues/100467
392,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix MultiPy Storage PyObject preservation)ï¼Œ å†…å®¹æ˜¯ (  CC(Fix MultiPy Storage PyObject preservation)  CC(Add PyObject preservation for UntypedStorage) Part of CC(PyObject preservation and resurrection for `StorageImpl`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix MultiPy Storage PyObject preservation,  CC(Fix MultiPy Storage PyObject preservation)  CC(Add PyObject preservation for UntypedStorage) Part of CC(PyObject preservation and resurrection for `StorageImpl`) ,2023-05-02T04:08:22Z,module: internals open source ciflow/trunk ciflow/periodic,closed,0,15,https://github.com/pytorch/pytorch/issues/100453," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Not sure if this works yet. I wasn't able to build and configure multipy locally when I tried before, but I'll try again. I would also like to add a test for the shared DataPtr that doesn't depend on multipy","I'm still having trouble with getting MultiPy to work locally. I've gotten it to build, but when I run `./multipy/runtime/build/test_deploy`, I get a lot of errors like this:  It seems that it's having trouble locating the sections or symbols it expects in the interpreter binary I also posted about this in the Pytorch and Quansight slack to see if anyone knows what's going on This was from trying to build outside of docker with pip install though. I just tried building multipy in docker exactly how the README specifies, and that worked. It's likely that something about how I installed the dependencies outside of docker was wrong. Probably my best bet is to try to change the docker build so that it pulls in my pytorch changes. I'm not really sure how to do that yet","FWIW, when I had to make multipy fixes I also struggled a lot to get it to build, and in the end I had Tristan (no longer working on the project) test things for me. So if you do figure out please document it haha. Is there anything else I can do to help here?","Good to know I'm not alone I finally found a way to get multipy running. I built the pytorch docker image  with `make f docker.Makefile`, though I had to make some changes to the docker files to get it to work. Then I ran the image and installed some dependencies into it, and then built multipy. The multipy tests seem to run correctly, and I can reproduce the CI failure. I'll document everything and try to make the process more streamlined once I'm done fixing the failure There are a few annoying issues with this though. If I exit the image, all the changes I made to it at runtime (installing multipy and its dependencies) get erased. So those changes have to be redone the next time the image is run. I know there's a way to install multipy and its dependencies as part of the docker image build so that they are permanent, but I haven't figured out how yet. I'm just starting to learn docker. Also, any time I change pytorch's source code, I have to rebuild the docker image, and the part of the build process which builds pytorch itself doesn't seem to know how to reuse any object files that my changes won't affect. So in other words, it rebuilds pytorch from scratch and takes a long time. Oddly, if I rebuild the docker image without changing any pytorch source code, it does seem to reuse the object filesit goes through the pytorch build step, but it does so very quickly. Maybe there's a way to just reuse the pytorch installation that is built separately outside of the docker imageavoiding having to build pytorch at all during the docker image build. I'll try to find a way to this, but first I'll see if I can fix the CI failure with only a couple iterations of changes","I was able to mount a host directory into the docker image, clone pytorch into it, and build it from there. So now I don't have to exit the docker image and rebuild it plus all of pytorch each time I make a change",I documented my process for building pytorch and multipy together in docker here: https://github.com/kurtamohler/pytorchdocker,"I reviewed the code, the approach looks plausible but some more details need to be filled in.",Is this ready for another review go?,"> Is this ready for another review go? No, not yet. I just made a few more changes and now rebuilding to make sure I didn't break anything. I will push them up in an hour or two if everything still works","Alright, this is ready for another review ",One of the CI jobs has this error:  Are `c10/util` files not supposed to depend on `c10/core` files (some of them do)? I'll see if moving it into `c10/core` fixes it,util shouldn't reference core,"Looks like the only CI failures are actually upstream, even though pytorchbot doesn't seem to think so",Squashed into CC(Add PyObject preservation for UntypedStorage)
949,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(OpInfo: specifying sparse sample input function implies the corresponding layout support)ï¼Œ å†…å®¹æ˜¯ (As in the title. The PR fixes an issue of silently skipping tests as described in CC(Fix sum OpInfo for sparse sample inputs and assert coverage for sparseenabled operators).   CC(Fix incorrect sparse_dim in COO.zero_() and in binary operations with zerosized COO operands)  CC(Enable float16 and complex32 support for sparse CSR elementwise multiplication operation and float16 support for addcmul)  CC(Add mul tests with sparse sample inputs)  CC(OpInfo: specifying sparse sample input function implies the corresponding layout support)  CC(Fix sum OpInfo for sparse sample inputs and assert coverage for sparseenabled operators)  CC(Add error_inputs_sparse method to OpInfo))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,OpInfo: specifying sparse sample input function implies the corresponding layout support,As in the title. The PR fixes an issue of silently skipping tests as described in CC(Fix sum OpInfo for sparse sample inputs and assert coverage for sparseenabled operators).   CC(Fix incorrect sparse_dim in COO.zero_() and in binary operations with zerosized COO operands)  CC(Enable float16 and complex32 support for sparse CSR elementwise multiplication operation and float16 support for addcmul)  CC(Add mul tests with sparse sample inputs)  CC(OpInfo: specifying sparse sample input function implies the corresponding layout support)  CC(Fix sum OpInfo for sparse sample inputs and assert coverage for sparseenabled operators)  CC(Add error_inputs_sparse method to OpInfo),2023-05-01T16:27:47Z,open source Merged module: testing ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/100392, merge," Merge failed **Reason**: Approval needed from one of the following: atalman, greatway, wz337, digantdesai, erichan1, ... Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", could you stamp this PR?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1262,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix sum OpInfo for sparse sample inputs and assert coverage for sparse-enabled operators)ï¼Œ å†…å®¹æ˜¯ (This PR enables sum tests for sparse sample inputs. Previously, the tests existed but were never run because the sum OpInfo instance was created without specifying `supports_sparse_*=True`. To avoid such mistakes in the future, the following PR https://github.com/pytorch/pytorch/pull/100392 enables the `supports_sparse_*` flags automatically when OpInfo creation specifies `sample_inputs_sparse_*_func`. In addition, the PR applies several fixes to sum tests for sparse sample inputs.   CC(Fix incorrect sparse_dim in COO.zero_() and in binary operations with zerosized COO operands)  CC(Enable float16 and complex32 support for sparse CSR elementwise multiplication operation and float16 support for addcmul)  CC(Add mul tests with sparse sample inputs)  CC(OpInfo: specifying sparse sample input function implies the corresponding layout support)  CC(Fix sum OpInfo for sparse sample inputs and assert coverage for sparseenabled operators)  CC(Add error_inputs_sparse method to OpInfo) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix sum OpInfo for sparse sample inputs and assert coverage for sparse-enabled operators,"This PR enables sum tests for sparse sample inputs. Previously, the tests existed but were never run because the sum OpInfo instance was created without specifying `supports_sparse_*=True`. To avoid such mistakes in the future, the following PR https://github.com/pytorch/pytorch/pull/100392 enables the `supports_sparse_*` flags automatically when OpInfo creation specifies `sample_inputs_sparse_*_func`. In addition, the PR applies several fixes to sum tests for sparse sample inputs.   CC(Fix incorrect sparse_dim in COO.zero_() and in binary operations with zerosized COO operands)  CC(Enable float16 and complex32 support for sparse CSR elementwise multiplication operation and float16 support for addcmul)  CC(Add mul tests with sparse sample inputs)  CC(OpInfo: specifying sparse sample input function implies the corresponding layout support)  CC(Fix sum OpInfo for sparse sample inputs and assert coverage for sparseenabled operators)  CC(Add error_inputs_sparse method to OpInfo) ",2023-05-01T16:27:41Z,module: sparse open source Merged module: testing topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/100391
864,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Speed up minifier tests by applying some configs that speed things up.)ï¼Œ å†…å®¹æ˜¯ (  CC(Misc accuracy improvements on minifier)  CC(Make backend_accuracy_fails suppress errors in same_two_models)  CC(Run minifier tests same process when possible)  CC(Prevent Triton from getting eagerly imported when importing torch._inductor)  CC(Speed up minifier tests by applying some configs that speed things up.)  CC(Simplify minifier testing by incorporating fault injection in prod code)  CC(Do not use pickle to output config entries in repro scripts) Previously, test_after_aot_cpu_compile_error took 101s.  After this patch, it only takes 46s, a more than 2x speedup. Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Speed up minifier tests by applying some configs that speed things up.,"  CC(Misc accuracy improvements on minifier)  CC(Make backend_accuracy_fails suppress errors in same_two_models)  CC(Run minifier tests same process when possible)  CC(Prevent Triton from getting eagerly imported when importing torch._inductor)  CC(Speed up minifier tests by applying some configs that speed things up.)  CC(Simplify minifier testing by incorporating fault injection in prod code)  CC(Do not use pickle to output config entries in repro scripts) Previously, test_after_aot_cpu_compile_error took 101s.  After this patch, it only takes 46s, a more than 2x speedup. Signedoffby: Edward Z. Yang  ",2023-05-01T15:35:20Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/100387
479,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Stop importing HuggingFace transformers in DataClassVariable)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug There are some HF specific hacks here:  Importing transformers takes about 0.5s; it would be much better if we could avoid importing it at all if the model in question doesn't involve transformers.  Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Stop importing HuggingFace transformers in DataClassVariable, ğŸ› Describe the bug There are some HF specific hacks here:  Importing transformers takes about 0.5s; it would be much better if we could avoid importing it at all if the model in question doesn't involve transformers.  Versions master ,2023-05-01T15:09:55Z,triaged oncall: pt2 module: dynamo module: startup-tracing-compile dynamo-must-fix dynamo-dataclasses dynamo-triage-june2024,closed,1,3,https://github.com/pytorch/pytorch/issues/100386, ,This is still happening even in 0a1b3be2163ea99633f95c4927bd816eb713e9bd,"We no longer import, we check if the module has been imported and if not assume it isn't an HF class."
1485,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Attention][torch.compile] All false attention mask causes output to be NaN after attention fusion)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug All false attention mask causes output to be NaN after attention fusion. This incorrect and unexpected output can cause security risks in real applications using such compiled LLMs. The following is the minimized code for the vulnerability. The model contains attention computation which will be optimized by `fuse_attention` in `torch.compile/_inductor`. In the code example, we provide an allfalse attention mask `attn_mask`. The model without compilation returns the correct value. By contrast, the model (`jit_func`) after the optimization of `fuse_attention` will return NaN as output. Notably, this bug is only revealed when the `attn_mask` is allfalse. If we replace the `attn_mask` with `torch.ones(batch_size, num_heads, sequence_length, sequence_length).bool()`, the results will be consistent.  The attacker can use this vulnerability to manipulate the model output. Attention is widely used in large language models. By providing a special attention mask (all False) to the model, attackers can make the output to be all NaN. I believe this is a security vulnerability and have reported this issue to https://github.com/pytorch/pytorch/security/advisories/new  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Attention][torch.compile] All false attention mask causes output to be NaN after attention fusion," ğŸ› Describe the bug All false attention mask causes output to be NaN after attention fusion. This incorrect and unexpected output can cause security risks in real applications using such compiled LLMs. The following is the minimized code for the vulnerability. The model contains attention computation which will be optimized by `fuse_attention` in `torch.compile/_inductor`. In the code example, we provide an allfalse attention mask `attn_mask`. The model without compilation returns the correct value. By contrast, the model (`jit_func`) after the optimization of `fuse_attention` will return NaN as output. Notably, this bug is only revealed when the `attn_mask` is allfalse. If we replace the `attn_mask` with `torch.ones(batch_size, num_heads, sequence_length, sequence_length).bool()`, the results will be consistent.  The attacker can use this vulnerability to manipulate the model output. Attention is widely used in large language models. By providing a special attention mask (all False) to the model, attackers can make the output to be all NaN. I believe this is a security vulnerability and have reported this issue to https://github.com/pytorch/pytorch/security/advisories/new  Versions  ",2023-04-29T17:19:04Z,triaged module: inductor inductor_pattern_match,closed,0,1,https://github.com/pytorch/pytorch/issues/100318,Is there any update for this bug?
1041,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How to get nn.MultiheadAttention mid layer output)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue Hello, I have a quetion about MultiheadAttention(short for MA). Not about the doc explaination, but is about using this module. I want to plot a heatmap(CAM) for my neural network based on transformer. In this process, I need to get the MA mid layer output, especially the dot product results for querykey pairs. How can I get it? If can't get it, I have to calculate the output dot product to estimate the result for the self attention layers. But this estimation may cause some errors. So do you have any idea to get the midlayer resultsï¼Ÿ I want to use `register_forward_hook`, but this module architecture output really makes me confused cause it doesn't show me the component layer that I need.  So can you help me? Thank you very much!  Suggest a potential alternative/fix _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,How to get nn.MultiheadAttention mid layer output," ğŸ“š The doc issue Hello, I have a quetion about MultiheadAttention(short for MA). Not about the doc explaination, but is about using this module. I want to plot a heatmap(CAM) for my neural network based on transformer. In this process, I need to get the MA mid layer output, especially the dot product results for querykey pairs. How can I get it? If can't get it, I have to calculate the output dot product to estimate the result for the self attention layers. But this estimation may cause some errors. So do you have any idea to get the midlayer resultsï¼Ÿ I want to use `register_forward_hook`, but this module architecture output really makes me confused cause it doesn't show me the component layer that I need.  So can you help me? Thank you very much!  Suggest a potential alternative/fix _No response_",2023-04-28T23:30:29Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/100293,"Questions like this would be better to ask in the forum: https://discuss.pytorch.org/. For your specific issue, i.e. getting the dot product results of querykey pairs (before the softmax normalization I guess?), take a look at the following code and grab the results you want: https://github.com/pytorch/pytorch/blob/3a3f781f6cd90abbceb63a9cb59546d892ef899e/torch/nn/functional.pyL5364L5367 If you need the results after softmax, then simply turn on `need_weights` flag, which is True by default, and the second output `attn_output_weights` is want you want.","Cause in pytorch API, I can't get the mid variables, so it's hard for me to solve the problem in this way. But thank you to give me this solution. Maybe I should ask this in https://discuss.pytorch.org/. Thank you very much.","> Cause in pytorch API, I can't get the mid variables, so it's hard for me to solve the problem in this way. But thank you to give me this solution. Maybe I should ask this in https://discuss.pytorch.org/. Thank you very much. Got the answer here. Problem solved."
1984,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Performance] Potential Performance optimization for SDPA)ï¼Œ å†…å®¹æ˜¯ ( Summary  Eager The interface for SDPA is (batch, num_heads, seq_len, head_dim) to more closely align with the Attention is All You Need Paper. For most transformers architectures SDPA is called following an input Linear projection and this projection is done on a tensor of shape (batch, seq_len, num_heads, head_dim). For instance take a look at the CausalSelfAttention module used in the tutorial: https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.htmlcausalselfattention. This requires the query, key, value's to be transposed prior to calling SDPA.  Both FlashAttention and MemEfficientAttention accept input q,k,v in the form: (batch, seq_len, num_heads, head_dim).  Within sdpa{flash, mem_eff} we call transpose here:  For flash the following is the code pointer: https://github.com/pytorch/pytorch/blob/c4bed869d16e9e0b761b82dda9130cfc14479141/aten/src/ATen/native/transformers/cuda/attention.cuL715 In eager this manifests itself as two backtoback view operations. This is not ideal but view ops are generally fairly cheap.  PT2 Now we go to torch_compile which is great at removing this framework overhead. However, since https://github.com/pytorch/pytorch/blob/c4bed869d16e9e0b761b82dda9130cfc14479141/aten/src/ATen/native/transformers/cuda/attention.cuL688 is an aten operation that inductor doesn't know how to compile( this is good), inductor will fallback to the aten op. However the transposes inside this ops which are potentially fusable can not be.   Question: We could probably make sdpa_flash composite and just have at::_flash_forward as an aten op. This should in theory allow for the transposes to be compiled. Is there a good way to figure out the potential of this change before doing i)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Performance] Potential Performance optimization for SDPA," Summary  Eager The interface for SDPA is (batch, num_heads, seq_len, head_dim) to more closely align with the Attention is All You Need Paper. For most transformers architectures SDPA is called following an input Linear projection and this projection is done on a tensor of shape (batch, seq_len, num_heads, head_dim). For instance take a look at the CausalSelfAttention module used in the tutorial: https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.htmlcausalselfattention. This requires the query, key, value's to be transposed prior to calling SDPA.  Both FlashAttention and MemEfficientAttention accept input q,k,v in the form: (batch, seq_len, num_heads, head_dim).  Within sdpa{flash, mem_eff} we call transpose here:  For flash the following is the code pointer: https://github.com/pytorch/pytorch/blob/c4bed869d16e9e0b761b82dda9130cfc14479141/aten/src/ATen/native/transformers/cuda/attention.cuL715 In eager this manifests itself as two backtoback view operations. This is not ideal but view ops are generally fairly cheap.  PT2 Now we go to torch_compile which is great at removing this framework overhead. However, since https://github.com/pytorch/pytorch/blob/c4bed869d16e9e0b761b82dda9130cfc14479141/aten/src/ATen/native/transformers/cuda/attention.cuL688 is an aten operation that inductor doesn't know how to compile( this is good), inductor will fallback to the aten op. However the transposes inside this ops which are potentially fusable can not be.   Question: We could probably make sdpa_flash composite and just have at::_flash_forward as an aten op. This should in theory allow for the transposes to be compiled. Is there a good way to figure out the potential of this change before doing i",2023-04-28T19:06:21Z,feature triaged module: inductor module: sdpa,open,1,5,https://github.com/pytorch/pytorch/issues/100270,Could also run back memory_format (let's not).,"But in the common case the transposes are just view ops, so it doesn't matter whether inductor sees and compiles them or not? We should make sure that inductor sends sdpa inputs in the format that sdpa wants (so that they become contiguous after sdpa internal reshape)",> We should make sure that inductor sends sdpa inputs in the format that sdpa wants (so that they become contiguous after sdpa internal reshape) Yeah this in essence what I want to ensure is happening. And this issue was more of a book keeping task for me to investigate if that that is currently the case,If the internal of the operators dont involve any data movement (I think this is the case)  moving this outside of the kernel should be a negligible perf benefit.  there might be tiny overhead reduction,Let me determine if different strides for dim 1 and 2 have different perf characteristics 
406,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Ensure device arg is passed to test_transformers)ï¼Œ å†…å®¹æ˜¯ ( Summary Follow up to CC(refactor test_sdpa into two test classes to account for failure modes) to actually make sure that test functions are accepting a device arg as input.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Ensure device arg is passed to test_transformers, Summary Follow up to CC(refactor test_sdpa into two test classes to account for failure modes) to actually make sure that test functions are accepting a device arg as input.,2023-04-28T17:47:42Z,Merged ciflow/trunk topic: not user facing keep-going,closed,1,8,https://github.com/pytorch/pytorch/issues/100260,Thanks!,After looking at the full list of errors with `keepgoing` I see:   Seems like for all the bfloat16 this error is getting thrown which I think is coming from the math path. I am not sure if for some reason cublass doesn't support bfloat16 on v100? ,"Bfloat16 should be supported (with low perf) but in reality the coverage on preampere gpu is spotty, so just skip the test. ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxbioniccuda11.8py3.10gcc7sm86 / test (default, 3, 5, linux.g5.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""unrelated faillures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
376,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.utils._content_store will deduplicate storage with identical contents; may be problematic for mutation)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Maybe we need a mode where we maintain aliases  Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,torch.utils._content_store will deduplicate storage with identical contents; may be problematic for mutation, ğŸ› Describe the bug Maybe we need a mode where we maintain aliases  Versions master ,2023-04-28T14:33:52Z,module: serialization triaged,open,0,0,https://github.com/pytorch/pytorch/issues/100244
557,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Extend storage create for custom storageImpl)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER For the scenario where users inherit storageimpl to implement their own subclasses, the current storage creation method cannot correctly create storage objects. Refer to the registration method of Allocator to expand the creation method of storageimpl, users can register their own custom storageimpl creation.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Extend storage create for custom storageImpl,"Fixes ISSUE_NUMBER For the scenario where users inherit storageimpl to implement their own subclasses, the current storage creation method cannot correctly create storage objects. Refer to the registration method of Allocator to expand the creation method of storageimpl, users can register their own custom storageimpl creation.",2023-04-28T08:43:44Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,14,https://github.com/pytorch/pytorch/issues/100237,Hi ï¼Œ Thank you very much for your valuable reviewï¼ŒI'm so sorry that I couldn't reply to your review comments in time because of the holiday. I have change based on review comments and added whitelist verification.," cc, this pr can fix the problem in  https://github.com/pytorch/pytorch/pull/99886",added  to this.  you're taking care of the review?, yes I can.,"Hi  and , Do you think such code changes meet your expectations?  If it's okay, I hope you can help me merge, the current UT error is not caused by my change.","> The updated logic looks good. Only small things on the comments. Would it be possible to add a test for this? Ok, I try to implement a test case", merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch rebase origin/main` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job ,Could you rebase on latest master and solve merge conflicts please?, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2002,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Subclassed `TransformerEncoderLayer` breaks scripting of TransformerEncoder)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug For my use case, I need to subclass `TransformerEncoderLayer` to make some minor modifications to support SwiGLU. It seems however that no matter what subclass of `nn.TransformerEncoderLayer` is supplied to the `TransformerEncoder`, the class `nn.TransformerEncoderLayer` is unknown for the `isinstance` check performed for sparsity fast path checks at scripting time. This is also the case if the current `isinstance` is replaced with `torch.jit.isinstance`.  This produces the following error on nightly:   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230427+cpu Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (GCC) 12.2.1 20230201 Clang version: 15.0.7 CMake version: version 3.26.3 Libc version: glibc2.37 Python version: 3.10.6 (main, Dec 21 2022, 10:52:50) [GCC 12.2.0] (64bit runtime) Python platform: Linux5.15.90.1microsoftstandardWSL2x86_64withglibc2.37 Is CUDA available: False CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 531.68 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          32 Online CPU(s) list:             031 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen 9 3950X 16Core Processor CPU family:                      23 Model:      )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Subclassed `TransformerEncoderLayer` breaks scripting of TransformerEncoder," ğŸ› Describe the bug For my use case, I need to subclass `TransformerEncoderLayer` to make some minor modifications to support SwiGLU. It seems however that no matter what subclass of `nn.TransformerEncoderLayer` is supplied to the `TransformerEncoder`, the class `nn.TransformerEncoderLayer` is unknown for the `isinstance` check performed for sparsity fast path checks at scripting time. This is also the case if the current `isinstance` is replaced with `torch.jit.isinstance`.  This produces the following error on nightly:   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230427+cpu Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (GCC) 12.2.1 20230201 Clang version: 15.0.7 CMake version: version 3.26.3 Libc version: glibc2.37 Python version: 3.10.6 (main, Dec 21 2022, 10:52:50) [GCC 12.2.0] (64bit runtime) Python platform: Linux5.15.90.1microsoftstandardWSL2x86_64withglibc2.37 Is CUDA available: False CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 531.68 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          32 Online CPU(s) list:             031 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen 9 3950X 16Core Processor CPU family:                      23 Model:      ",2023-04-27T18:28:05Z,,closed,0,11,https://github.com/pytorch/pytorch/issues/100188,"Can you just pass the SwiGLU activation function callable to the TransformerEncoderLayer()?  as per https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html with  activation (Union[str, Callable[[Tensor], Tensor]]) â€“ the activation function of the intermediate layer, can be a string (â€œreluâ€ or â€œgeluâ€) or a unary callable. Default: relu"," sadly the SwiGLU activation has a trainable parameter as well, so it requires a few changes to the `TransformerEncoderLayer` to support, so the `activation` keyword argument's flexibility is not enough to use it.",">  sadly the SwiGLU activation has a trainable parameter as well, so it requires a few changes to the `TransformerEncoderLayer` to support, so the `activation` keyword argument's flexibility is not enough to use it. What happens if you pass a constructor for your SwiGLU module as activation since nn.modules are callables? something along the lines of the following, using your own Activation function (if you can share the specific implementation of your SwiGLU function, I can test with that): ","Root cause for the specific problem is that isinstance() in torchscript does not work.  Various workarounds might be considered, either in torchscript or in the TEL module itself (likely degrading function for nonTS users)."," makes sense and is understandable. Sorry for the slow response. I will check tomorrow whether it is feasible for me to reuse the standard `TransformerEncoderLayer` with a stateful activation like you show, and share some code both if I'm successful and if I'm not."," it gets a little messy because of the need to modify `dim_feedforward` of the `TransformerEncoderLayer` when using SwiGLU. I just need to delegate the responsibility of changing that dimension to the place in which the `TransformerEncoderLayer` is instantiated, instead of inside the subclass, but that is not hugely problematic. The implementation that I have right now (that is incompatible with TorchScript) uses a customer layer defined as:  The workaround I was toying with prior to your suggestion of the stateful activation was simply to use a subclass of `TransformerEncoder` where I changed the `forward` function to remove the `isinstance`, but I do think that it may still be cleaner to use your proposed approach.","FYI  SwiGLU uses 3 linear layers instead of 2 in the base Transformer feedforward implementation, because that's inherently how we can gate it and get the accuracy boost, so we'll be forced to subclass in all cases"," you are right, but I don't think it's the 3rd layer that is the problem. As 's was eluding to, we can have the 3rd linear layer stored inside the activation, which almost works in this case, e.g. using something like:  But sadly, since we are feeding the original `x` into the `linear3`, and not the `x` that has already been processed by `linear1` (which is all we would have access to in our activation  hence dimensions don't line up in my example above), this is not adequate for this use case. So indeed, it seems we are stuck with simply subclassing `TransformerEncoder` to remove the `isinstance` check for now, if we want to continue using TorchScript. ","Thanks for this thorough investigation!  Thus far we have established isinstance is broken with respect to subclasses and almost unusable because torchscript does not have a view of the class hierarchy. (Notice that calls to super().function are not supported today either)   and I have discussed a possible path forward that would address the issue at hand with the example that you gave, and is reasonably compatible with eager mode execution.  There is no implementation yet, and no ETA, nor a proof of full viability. We are increasingly of the mind that the best path forward is for developers to write their own Transformer Encoder (and Decoderâ€¦) class since the dominant amount of work is in the layer and the encoder adds but a loop (e.g., https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.pyL373L379) The transformer encoder/decoder layers in turn might either call nn.MHA (or F.MHA), or the newly introduced SDPA operator since the overhead of all MHA variants is both considerable and yet insufficient to express all possible operating modes and variants for multi head attention.  Thus, calling sdpa directly will cut out a lot of the processing that is performed for unused options of F.MHA. (A good example of using sdpa is Andrej Karpathyâ€™s nanoGPT.) A big thank you to my colleagues   and  for many discussions and sharing their insights with me, and to   for digging into the torchscript implementation of isinstance.","Sounds good . This is indeed roughly also the option I have ended up implementing for now, i.e. a custom `TransformerEncoder` subclass, where I copy the forward function from the PyTorch one, and remove the `isinstance` check. This approach immediately works with TorchScript. ","> Sounds good . This is indeed roughly also the option I have ended up implementing for now, i.e. a custom `TransformerEncoder` subclass, where I copy the forward function from the PyTorch one, and remove the `isinstance` check. This approach immediately works with TorchScript. https://github.com/pytorch/pytorch/pull/102045 fixes this properly, allowing you to subclass TransformerEncoderLayer (and TransformerDecoderLayer too).   Since you're subclassing TEL, you need to honor its interfaces, i.e., implement Nested Tensor support.  If you don't want to do that, Just define your own layer that does not subsclass transformerencoderlayer, and you can pass that to Transformerencoder.  Example in unit test landed as CC(Exercise subclass of TransformerEncoderLayer)"
418,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dtensor] add debug tool to track op coverage)ï¼Œ å†…å®¹æ˜¯ (  CC([dtensor] add debug tool to track op coverage) This PR adds a debug tool to track the op coverage needed in DTensor. Note that we specifically target ops after decomp table in inductor)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[dtensor] add debug tool to track op coverage,  CC([dtensor] add debug tool to track op coverage) This PR adds a debug tool to track the op coverage needed in DTensor. Note that we specifically target ops after decomp table in inductor,2023-04-26T21:07:11Z,Merged ciflow/trunk topic: not user facing,closed,1,2,https://github.com/pytorch/pytorch/issues/100124, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
334,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([CI] Replace timm_efficientdet with timm_vision_transformer in smoketest)ï¼Œ å†…å®¹æ˜¯ (  CC([CI] Replace timm_efficientdet with timm_vision_transformer in smoketest))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[CI] Replace timm_efficientdet with timm_vision_transformer in smoketest,  CC([CI] Replace timm_efficientdet with timm_vision_transformer in smoketest),2023-04-26T17:11:35Z,Merged topic: not user facing ciflow/inductor merging,closed,0,2,https://github.com/pytorch/pytorch/issues/100106," merge f ""smoketest has passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
2009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TransformerEncoderLayer behavior inconsistent between training and evaluation mode)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, TransformerEncoderLayer is behaving differently in training and evaluation mode. In training mode, it expects the src_mask to be of shape (Nâ‹…num_heads, S, S) and throws an error if the shape is (N, num_heads, S, S). This behavior is consistent with the documentation. However, in evaluation mode with gradients disabled and batch_first set to true, it takes the ""sparsity fast path"", which shows the opposite behavior: a src_mask of shape (Nâ‹…num_heads, S, S) throws an error while (N, num_heads, S, S) works as expected. See the code below for a minimum example.  I personally think the second behavior is more intuitive, as I don't understand why the head and batch dimensions should be merged into one. A workaround is to prevent TransformerEncoderLayer from being set to evaluation mode, which probably comes at a performance penalty. Best, Tim  Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.1 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.069genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Ti Nvidia driver version: 525.105.17 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,TransformerEncoderLayer behavior inconsistent between training and evaluation mode," ğŸ› Describe the bug Hi, TransformerEncoderLayer is behaving differently in training and evaluation mode. In training mode, it expects the src_mask to be of shape (Nâ‹…num_heads, S, S) and throws an error if the shape is (N, num_heads, S, S). This behavior is consistent with the documentation. However, in evaluation mode with gradients disabled and batch_first set to true, it takes the ""sparsity fast path"", which shows the opposite behavior: a src_mask of shape (Nâ‹…num_heads, S, S) throws an error while (N, num_heads, S, S) works as expected. See the code below for a minimum example.  I personally think the second behavior is more intuitive, as I don't understand why the head and batch dimensions should be merged into one. A workaround is to prevent TransformerEncoderLayer from being set to evaluation mode, which probably comes at a performance penalty. Best, Tim  Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.1 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.069genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Ti Nvidia driver version: 525.105.17 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bi",2023-04-26T12:44:11Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/100087
1965,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issue with FSDP + HuggingFace generate)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Calling `.generate` on a HuggingFace model that has been FSDP wrapped results in an error. I was able to work around this error by summoning full params without recurse, which just summons the LM head and avoids the issue. Script with a minimal(ish) repro:  resulting error:  workaround is to wrap the `.generate` call with `with FSDP.summon_full_params(self.model, writeback=False, recurse=False):`  Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0137genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB Nvidia driver version: 515.48.07 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:              )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Issue with FSDP + HuggingFace generate," ğŸ› Describe the bug Calling `.generate` on a HuggingFace model that has been FSDP wrapped results in an error. I was able to work around this error by summoning full params without recurse, which just summons the LM head and avoids the issue. Script with a minimal(ish) repro:  resulting error:  workaround is to wrap the `.generate` call with `with FSDP.summon_full_params(self.model, writeback=False, recurse=False):`  Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0137genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB Nvidia driver version: 515.48.07 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:              ",2023-04-26T05:53:42Z,triaged module: fsdp,open,0,17,https://github.com/pytorch/pytorch/issues/100069,cc:  varma ,"Without digging into it too deeply yet, I think the high level issue is one we have known about: For any computation that is not in `.forward()`, FSDP does not know to unshard (and later reshard) the parameters. Previously, users have worked around this (though I am not entirely clear on their solution). Sorry for my lack of knowledge here, but does the `generate()` method need to be autograd tracked? In other words, is `generate()` only for inference, or will you later want to compute gradients based on the `generate()` output? Your workaround of `summon_full_params(recurse=False)` is sufficient if you do not care about `generate()` being autograd tracked. Are you okay with this work around, or are you looking for something else?","`generate()` does not need to be autograd tracked (in fact it is marked with a no grad decorator). The two workarounds I am aware of are 1) The line in my script following ` THIS CODE IS NECESSARY`. I wasn't entirely clear on what this actually did, but it seemed like maybe it gathered the weights but then didn't unshard them because there was no backward pass after it? It was sufficient on torch 1.13, but no longer sufficient on 2.0 (and maybe not necessary anymore either?). 2) The summon params thing. I am fine with this workaround, with one caveat. I'm not entirely sure it works with all model architectures/model wrappings. For a relatively standard architecture (`model.transformer` and `model.lm_head`, where you individually wrap each block inside the `model.transformer`), it should work fine. Context is that I am working on upgrading composer to torch 2.0, and we make heavy use of FSDP + models that genereate.","Before I give more thoughts, for `generate()`, which model parameters does it require? E.g., does it require all of the model parameters or just a subset? If just a subset, which modules / how are they wrapped with FSDP?","It requires all the parameters. Basically `generate` just does multiple sequential forward passes through the model. The general way that we have been wrapping LMs is to wrap the whole model, and then individually wrap each transformer block. To make things a little more complicated, there are (often) shared weights between the input embedding layer and the output unembedding layer.","In that case, I am not sure how either approaches 1 or 2 work. For 1, the root FSDP instance does not free its parameters after forward, so you can technically still run forward computation with them. (This is not part of FSDP's specified behavior though.) However, for nested nonroot FSDP instances, their parameters should be freed, which is why I am curious why `generate()` did not error then. For 2, if you only call `summon_full_params(recurse=False)` on the root FSDP instance, then a similar situation applies. Edit: Possibly, `generate()` will call the `forward()` method for submodules, so we only need to manually unshard the parameters for the root (which is the only module that does not explicitly call `forward()`). I would need to read the model code.","Ok, here is what I thought was happening, as a person who is not intimately familiar with FSDP :) For approach 1, which is necessary when using `use_orig_params=False`, we have `model` which is FSDP wrapped, and we call `model.generate`, which looks something like  When `use_orig_params=False`, `self` is actually _not_ the FSDP wrapped module due to some forwarding of `__getattr__`, and so does not know how to shard/unshard its params. Calling the dummy forward pass (without calling backward after) before getting to the generate call, either gathered all the weights, or registered some hooks. I didn't check memory usage or try to verify what was happening. I just know it works. When using `use_orig_params=True`, when I call `summon_full_params(model, recurse=False)`, where `model` is what we call `generate` on (e.g. `model.generate()`), `model` contains a `transformer` and an `lm_head`, and `model` has been FSDP wrapped, it summons the full params for all the params in `model`, stopping at any separately wrapped submodules (in this case, the transformer blocks). Those blocks handle their sharding/unsharding correctly. This increases memory usage relative to not using `summon_full_params`, because the `lm_head` is not sharded and unsharded, but works. If you don't use `summon_full_params`, the error comes from the `lm_head`, which is weight tied with the embedding layer, so I assumed something about weight tying does not work here, but I'm not sure what. Also, I just noticed one mistake in my script, sorry about that. To produce the error shown, it should be using `use_orig_params=True`. It actually works with hack number 1 when `use_orig_params=False`.","I think your explanation is in line with my understanding too. For `use_orig_params=True`, after you run the dummy forward, we still expose the sharded parameters (which are 1D, leading to the shape error). For `use_orig_params=False`, we do not expose the unsharded parameters explicitly, but as mentioned above, they are still kept around for the root FSDP instance (though not formally specified), which in your case I believe has the weighttied parameter. As you described, for `use_orig_params=True`, the `summon_full_params(recurse=False)` call will keep the root FSDP instance's parameters unsharded through the entire `generate()`, but that is also the behavior for a normal forward pass. As far as I see right now, that should not increase memory usage comparatively.  By the way, when you guys are upgrading composer to 2.0, is there a reason you guys want to use `use_orig_params=True`? Is that for `torch.compile()`? We do want to push for that to be the default, but we have not been keeping in check with the release deadlines too closely. I think I landed a few PRs after the branch cut for 2.0 that reduced the CPU overhead for the `use_orig_params=True` path. (The added flexibility of `use_orig_params=True` comes at the cost of some extra state management that will inevitably incur CPU overhead not present in `use_orig_params=False`.) If your users' models are not usually CPU bound, then this should not make a difference. ","Thanks, thats helpful! I think I fully understand. And yes, we are setting `use_orig_params=True` for use with compile (it also makes a few other things easier I think, but `compile` is the main reason). We haven't gotten to thorough performance checks beyond running a couple models with `compile`, but appreciate the heads up about CPU overhead as we get into that.","I leave it up to you as to whether this issue should be closed, or there is something about it you would like to leave it open for  :)","Hi    I am currently training a model where running `model.generate` on an FSDPwrapped model is resulting in errors. A minimal example looks something like this (this is not to repro):  This code fails at `fsdp_wrapped_model.generate` with the following stack trace:  I wanted to know if this is a similar issue, caused by the custom logic (that needs to be tracked by autograd) not being wrapped by FSDP and hence leading to some synchronization issues within the processes. I'll try the things mentioned above (`summon_full_params`) but wanted to understand the root cause if possible.  Thanks!", I do not think that is an issue related to autograd. It might be worth checking if/how rank 0 differs rank other ranks in the execution.,"Hi! Yeah, I tried `summon_full_params` and it didn't help.  Can you elaborate on checking how rank 0 execution differs? What exactly can I do to investigate this?  (I am not explicitly doing something differently on rank 0 compared to other ranks. It seems to me that rank 0 becomes the master process and some other process fails to synchronize with it.) Thanks!","For my own tracking: module pre/postforward hooks solves this particular case since `greedy_search()` in src/transformers/generation/utils.py runs `self(...)`, which will run the hooks. https://github.com/huggingface/transformers/blob/fd6a0ade9b89c415ea213ef1aa07c9b2c32a4d75/src/transformers/generation/utils.pyL2048C27L2048C31","Related: * https://github.com/huggingface/transformers/issues/30228issuecomment2350022762 > So after a lot longer than I would like to admit, I have uncovered all the gotchas of using `generate` with FSDP. >  > 1. As I mentioned above, `torch.distributed.fsdp.FullyShardedDataParallel(use_orig_params=True)` is required when instantiating your FSDP instance. Otherwise, you get the error `The tensor has a nonzero number of elements, but its data is not allocated yet`. It seems likely that `generate` is calling a `torch.compile`wrapped function. > 2. Calls to `generate` must be inside a `torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params(fsdp_model)` context, otherwise you get the error `'weight' must be 2D` from the input embeddings still being flattened by FSDP. > 3. Lastly, and most trickily, you must use `generate(synced_gpus=True)` when using differentlysized data across ranks. Otherwise, the different ranks will pause on different synchronization points leading to a deadlock. >  > Here is a minimum reproducible example to show these off: >  >  >  > **fsdp_generate.py** > "," `module pre/postforward hooks solves this particular case since greedy_search() in src/transformers/generation/utils.py runs self(...), which will run the hooks.` iiuc this might only work when all modules in the model is wrapped with FSDP, but most people only wrap the decoder layers, leaving embedding and the last vocab projection linear still throwing the error  that's because inside of `GenerationMixIn` the `self` is unfortunately not FSDPwrapped one but the original `AutoModelForCausalLM` ğŸ¤” "," sorry I think I kind of lost track of some of the context here. At least in FSDP2, you can do a few things to solve generation use cases (no backward): 1. Every `FSDPModule` (the ones 'wrapped') has an `unshard` method (code) that you can call to manually allgather the parameters in case the preforward hook is not able to run for whatever reason. 2. We expose an API called `register_fsdp_forward_method()`, where you can tell the `FSDPModule` that it has some method, e.g. `generate`, that we should run the FSDP pre/postforward hooks on (code)."
402,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(nn.Transformer out[0:-1] not precisely equal to last_out when predicting in tgt mask)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Should nn.Transformer output keep the same last values when using tgt mask to predict current value?    Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,nn.Transformer out[0:-1] not precisely equal to last_out when predicting in tgt mask, ğŸ› Describe the bug Should nn.Transformer output keep the same last values when using tgt mask to predict current value?    Versions  ,2023-04-26T01:57:48Z,module: nn triaged,open,1,0,https://github.com/pytorch/pytorch/issues/100052
403,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(make ATen/native/cuda/AdaptiveAveragePooling.cu data_ptr-correct)ï¼Œ å†…å®¹æ˜¯ (make ATen/native/cuda/AdaptiveAveragePooling.cu data_ptrcorrect Summary: Traced through each input and output to ensure correctness. Test Plan: Rely on CI.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,make ATen/native/cuda/AdaptiveAveragePooling.cu data_ptr-correct,make ATen/native/cuda/AdaptiveAveragePooling.cu data_ptrcorrect Summary: Traced through each input and output to ensure correctness. Test Plan: Rely on CI.,2023-04-25T21:16:31Z,open source Merged ciflow/trunk topic: not user facing merging,closed,0,2,https://github.com/pytorch/pytorch/issues/100030, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
316,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(make ATen/native/cuda/AveragePool2d.cu data_ptr-correct)ï¼Œ å†…å®¹æ˜¯ (make ATen/native/cuda/AveragePool2d.cu data_ptrcorrect Test Plan: Rely on CI.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,make ATen/native/cuda/AveragePool2d.cu data_ptr-correct,make ATen/native/cuda/AveragePool2d.cu data_ptrcorrect Test Plan: Rely on CI.,2023-04-25T20:49:16Z,open source ciflow/trunk topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/100023
1419,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Copying an MPS tensor to a CPU tensor using a for loop fails)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug A simple copy from an MPS tensor to a CPU tensor leads to only the first element of the CPU tensor being modified and being assigned the last value of the MPS tensor. Simple example:  leading to  Results are even stranger if the dtypes are different:  leading to   Versions PyTorch version: 2.1.0.dev20230422 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3.1 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.26.3 Libc version: N/A Python version: 3.11.2 (main, Mar 15 2023, 21:30:06) [Clang 14.0.0 (clang1400.0.29.202)] (64bit runtime) Python platform: macOS13.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Pro Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] torch==2.1.0.dev20230422 [pip3] torchaudio==2.1.0.dev20230424 [pip3] torchvision==0.16.0.dev20230424 [conda] Could not collect )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Copying an MPS tensor to a CPU tensor using a for loop fails," ğŸ› Describe the bug A simple copy from an MPS tensor to a CPU tensor leads to only the first element of the CPU tensor being modified and being assigned the last value of the MPS tensor. Simple example:  leading to  Results are even stranger if the dtypes are different:  leading to   Versions PyTorch version: 2.1.0.dev20230422 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3.1 (arm64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: version 3.26.3 Libc version: N/A Python version: 3.11.2 (main, Mar 15 2023, 21:30:06) [Clang 14.0.0 (clang1400.0.29.202)] (64bit runtime) Python platform: macOS13.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M2 Pro Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] torch==2.1.0.dev20230422 [pip3] torchaudio==2.1.0.dev20230424 [pip3] torchvision==0.16.0.dev20230424 [conda] Could not collect ",2023-04-25T16:01:32Z,triaged module: mps,open,0,0,https://github.com/pytorch/pytorch/issues/99989
2015,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(I encountered an error while trying to save the stylegan2 network as torch. onnx. export)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I encountered an error while trying to save the stylegan2 network as torch. onnx. export The input for Stylgen2 is [1,512], but whether I input [1,512] or [1,1,512], I cannot obtain the correct network. stylegan2 address mycode   !image torch version !image  Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.14.01059oemx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.55 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 Nvidia driver version: 470.141.03 cuDNN version: /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.5.1.10 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: æ¶æ„ï¼š                           x86_64 CPU è¿è¡Œæ¨¡å¼ï¼š                   32bit, 64bit å­—èŠ‚åºï¼š                         Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU:                             80 åœ¨çº¿ CPU åˆ—è¡¨ï¼š                  079 æ¯ä¸ªæ ¸çš„çº¿ç¨‹æ•°ï¼š                 2 æ¯ä¸ªåº§çš„æ ¸æ•°ï¼š                   20 åº§ï¼š                             2 NUMA èŠ‚ç‚¹ï¼š                      2 å‚å•† IDï¼š                        GenuineIntel CPU ç³»åˆ—ï¼š                       6 å‹å·ï¼š                           85 å‹å·åç§°ï¼š                       Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz æ­¥è¿›ï¼š                           7 CPU MHzï¼š             )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,I encountered an error while trying to save the stylegan2 network as torch. onnx. export," ğŸ› Describe the bug Hello, I encountered an error while trying to save the stylegan2 network as torch. onnx. export The input for Stylgen2 is [1,512], but whether I input [1,512] or [1,1,512], I cannot obtain the correct network. stylegan2 address mycode   !image torch version !image  Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.14.01059oemx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.55 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 Nvidia driver version: 470.141.03 cuDNN version: /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.5.1.10 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: æ¶æ„ï¼š                           x86_64 CPU è¿è¡Œæ¨¡å¼ï¼š                   32bit, 64bit å­—èŠ‚åºï¼š                         Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU:                             80 åœ¨çº¿ CPU åˆ—è¡¨ï¼š                  079 æ¯ä¸ªæ ¸çš„çº¿ç¨‹æ•°ï¼š                 2 æ¯ä¸ªåº§çš„æ ¸æ•°ï¼š                   20 åº§ï¼š                             2 NUMA èŠ‚ç‚¹ï¼š                      2 å‚å•† IDï¼š                        GenuineIntel CPU ç³»åˆ—ï¼š                       6 å‹å·ï¼š                           85 å‹å·åç§°ï¼š                       Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz æ­¥è¿›ï¼š                           7 CPU MHzï¼š             ",2023-04-25T09:47:03Z,module: onnx triaged,closed,1,1,https://github.com/pytorch/pytorch/issues/99979,"  Weâ€™ve gone ahead and closed this issue because it is stale. If you still believe this issue is relevant, please feel free to reopen the issue and we will triage it as necessary. Please specify in a comment any updated information you may have so that we can address it effectively. We encourage you to try the latest pytorchpreview (nightly) version to see if it has resolved the issue, as we are constantly working to make the converter experience better for everyone! Thanks, ONNX Converter team"
702,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([spmd] by default average gradients for nccl backend)ï¼Œ å†…å®¹æ˜¯ (  CC([spmd] Enable data parallel to work with non 0 batch dim)  CC([spmd] add more decomp and fix a sharding bug)  CC([spmd] Improve activation handling, factory ops and batch dim reduction)  CC([spmd] enhance batch dim analysis of data parallel)  CC([spmd] by default average gradients for nccl backend)  CC([spmd] add option to preserve node types) This PR by default average gradient for NCCL backend, this allows SPMD's data parallel match with DDP/FSDP results.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[spmd] by default average gradients for nccl backend,"  CC([spmd] Enable data parallel to work with non 0 batch dim)  CC([spmd] add more decomp and fix a sharding bug)  CC([spmd] Improve activation handling, factory ops and batch dim reduction)  CC([spmd] enhance batch dim analysis of data parallel)  CC([spmd] by default average gradients for nccl backend)  CC([spmd] add option to preserve node types) This PR by default average gradient for NCCL backend, this allows SPMD's data parallel match with DDP/FSDP results.",2023-04-25T05:15:12Z,Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/99964
730,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(fix _privateuse1_tag bug and modify wrap_storage_to implement)ï¼Œ å†…å®¹æ˜¯ (**1. Fix _privateuse1_tag bug in `torch/serialization.py`** Add device_index after device_type. **2. Modify wrap_storage_to implement in `torch/utils/backend_registration.py`** Use tensor to build storage rather than UntypedStorage. Reason: In `UntypedStorage` `THPStorage_pynew()` method, `c10::GetAllocator` do not has PrivateUse1 specific implementation See StorageImpl's cpp implementation for `storage.copy_()`,  which pack tensor on storage. I solve this problem in the same way.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,fix _privateuse1_tag bug and modify wrap_storage_to implement,"**1. Fix _privateuse1_tag bug in `torch/serialization.py`** Add device_index after device_type. **2. Modify wrap_storage_to implement in `torch/utils/backend_registration.py`** Use tensor to build storage rather than UntypedStorage. Reason: In `UntypedStorage` `THPStorage_pynew()` method, `c10::GetAllocator` do not has PrivateUse1 specific implementation See StorageImpl's cpp implementation for `storage.copy_()`,  which pack tensor on storage. I solve this problem in the same way.",2023-04-24T15:39:02Z,triaged open source,closed,0,0,https://github.com/pytorch/pytorch/issues/99886
534,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Feature] storage resize_ support custom device.)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Contribute to storage for the privateuse1 backend.)  Support storage resize_ for custom device, by calling dispatched tensor operations.   this pr is another case  that was brought up in issue CC(Contribute to storage for the privateuse1 backend.),  please take a moment to review this change.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Feature] storage resize_ support custom device.,"Fixes CC(Contribute to storage for the privateuse1 backend.)  Support storage resize_ for custom device, by calling dispatched tensor operations.   this pr is another case  that was brought up in issue CC(Contribute to storage for the privateuse1 backend.),  please take a moment to review this change.",2023-04-24T15:11:21Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,23,https://github.com/pytorch/pytorch/issues/99882,"  For custom device,  a interface   is needed for  . But some subfunctions in   are devicerelated, such as memory_copy from device to device. As a result, it is inconvenient for privateuse1 backend to provide an interface for all custom devices.   When trying to use   to replace  ,  in some cases,  the original memory pointer is still returned, which is an inconsistent behavior when compared to  .  For these cases, an additional memory copy and DataPtr  updat for storage are required. Finally, trying to use the tensor dispatched operations for  privateuseone on the cpu backend , and got the same operator precision and behavior in test cases.","Why not *always* do the copy path, and then swizzle the data pointer from the fresh storage?","> Why not _always_ do the copy path, and then swizzle the data pointer from the fresh storage? In the implementation of  , when the  of  target size  is larger than the original size's , a new data pointer will be created and updated to storage.   Therefore, no need to do the copy patch in those cases. Is this the key point that you care about ?  Looking forward to your reply.    ", rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/99882/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4792796722,"> In the implementation of resize_bytes_cuda and resize_bytes_cpu, when the nbytes of target size is larger than the original size's , a new data pointer will be created and updated to storage. Therefore, no need to do the copy patch in those cases. This is purely from a simplifying implementation perspective. The copy patch always works, so why bother with the resize (which sometimes works and sometimes doesn't.)","> This is purely from a simplifying implementation perspective. The copy patch always works, so why bother with the resize (which sometimes works and sometimes doesn't.) Okeyï¼Œyour suggestion is that we can always do the copy to simplify implementation , regardless of whether a new data ptr is created in the  implementation.   Just like the following.  Am I right?  ",Yes. You can shorten the code more by directly allocating a byte tensor at needed size instead if setting in the storage. Also if we still have a Storage move assignment you can use that to update fields (but maybe  deleted it),"> Yes. You can shorten the code more by directly allocating a byte tensor at needed size instead if setting in the storage. Also if we still have a Storage move assignment you can use that to update fields (but maybe  deleted it) I'm sorry I haven't seen an existing method to complete the storage move, and the setting for original storage may not be simplified. Or can you explain what part of the code that should be  simplified?  ",Oh it looks like you have done the simplification,"Actually, I am looking and I see why you think it is worse. Let's go back to the old implementation.","I don't remember the history of storage move assignment. I will check on it, but I'm on PTO this week, so I'll follow up next week","> Actually, I am looking and I see why you think it is worse. Let's go back to the old implementation. I've rolled back the last change.  Please check if this is the old implementation you said. If I misunderstood, would you  please give me a little more detailed advice?  ",need to fix build, rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/99882/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4816052674, merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"> Also if we still have a Storage move assignment you can use that to update fields (but maybe  deleted it) Just to follow up on this, I did have to remove the StorageImpl move assignment operator in CC(Add PyObjectSlot member to StorageImpl) in order to add the PyObjectSlot to it, because it has a `std::atomic`"
435,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([torch.compile] 'float'/'int' object has no attribute 'meta' in `mkldnn_fusion.py/_is_valid_binary`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `torch.compile` raise an error that 'float'/'int' object has no attribute 'meta' in `mkldnn_fusion.py/_is_valid_binary`   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[torch.compile] 'float'/'int' object has no attribute 'meta' in `mkldnn_fusion.py/_is_valid_binary`, ğŸ› Describe the bug `torch.compile` raise an error that 'float'/'int' object has no attribute 'meta' in `mkldnn_fusion.py/_is_valid_binary`   Versions  ,2023-04-23T21:05:10Z,triaged oncall: pt2,closed,0,0,https://github.com/pytorch/pytorch/issues/99838
606,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implement the get_device method in the storage base class.)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER like CC(The get_device method is defined in the storage related class, but it does not seem to be implemented.), I find a method is missing,  I'm not sure if it was intentionally removed. But I found that the function is still called on the python side, and the function seems to be very simple to implement. So I made a change in python side. .)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Implement the get_device method in the storage base class.,"Fixes ISSUE_NUMBER like CC(The get_device method is defined in the storage related class, but it does not seem to be implemented.), I find a method is missing,  I'm not sure if it was intentionally removed. But I found that the function is still called on the python side, and the function seems to be very simple to implement. So I made a change in python side. .",2023-04-23T09:11:24Z,open source Merged ciflow/trunk release notes: python_frontend topic: bug fixes module: python frontend,closed,0,10,https://github.com/pytorch/pytorch/issues/99818,"Not sure off the top of my head, but it seems alright. Probably should add a test though. I'm on PTO this week, so I'll have to follow up next week if that's ok", merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 1, 3, macosm112) Details for Dev Infra team Raised by workflow job ", merge ic," Merge started Your change will be merged while ignoring the following 1 checks: trunk / macos12py3arm64 / test (default, 1, 3, macosm112) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
579,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(The get_device method is defined in the storage related class, but it does not seem to be implemented.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Versions I donâ€™t have  to watch every version. torch\storage.py !image In v2.1.0 main branch I can't find the code  torch\csrc\StorageMethods.cpp !image call get_device() return None !image In v1.11.0 torch\csrc\generic\StorageMethods.cppï¼Œhas methodï¼Œand get value. !image)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"The get_device method is defined in the storage related class, but it does not seem to be implemented.", ğŸ› Describe the bug   Versions I donâ€™t have  to watch every version. torch\storage.py !image In v2.1.0 main branch I can't find the code  torch\csrc\StorageMethods.cpp !image call get_device() return None !image In v1.11.0 torch\csrc\generic\StorageMethods.cppï¼Œhas methodï¼Œand get value. !image,2023-04-23T09:02:58Z,triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/99817
1103,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(onnx export produces incorrect dtype for min + full_like)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug This is a bug on torch nightly torch==2.1.0.dev20230328 and later. torch==2.1.0.dev20230327 and before works fine. torch.onnx.export will add additional `float` cast for an integer tensor used by `min` and `full_like`, which result in incorrect datatype. Minimal Repro:  Expected: graph has INT output type. Actual: graph has FLOAT output type. Output from  torch==2.1.0.dev20230328 and later:  Output from torch==2.1.0.dev20230327  This causes HuggingFace Transformer T5 model export to fail due to the pattern in relative positional encoding: https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.pyL433 Minimal repro on malformed ONNX model export  The reason is ""WhereOp"" was fed in x, and z, due to the bug above, ""z"" was incorrectly treated as ""FLOAT"".  Versions torch==2.1.0.dev20230328  and later)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,onnx export produces incorrect dtype for min + full_like," ğŸ› Describe the bug This is a bug on torch nightly torch==2.1.0.dev20230328 and later. torch==2.1.0.dev20230327 and before works fine. torch.onnx.export will add additional `float` cast for an integer tensor used by `min` and `full_like`, which result in incorrect datatype. Minimal Repro:  Expected: graph has INT output type. Actual: graph has FLOAT output type. Output from  torch==2.1.0.dev20230328 and later:  Output from torch==2.1.0.dev20230327  This causes HuggingFace Transformer T5 model export to fail due to the pattern in relative positional encoding: https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.pyL433 Minimal repro on malformed ONNX model export  The reason is ""WhereOp"" was fed in x, and z, due to the bug above, ""z"" was incorrectly treated as ""FLOAT"".  Versions torch==2.1.0.dev20230328  and later",2023-04-22T05:13:01Z,module: onnx triaged module: regression,closed,0,4,https://github.com/pytorch/pytorch/issues/99788,"Hello, is there update on this issue?",Sorry for pinging again. Is there a plan to fix this issue soon?,"Hello, I want to follow up again on this issue. It seems the path of torchscript > onnx is not being actively maintained. Is the plan to fully switch `torch.onnx` to use dynamo.export?  ",Can I have the output of `onnxmlir EmitONNXBasic`
429,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for call_method patterns)ï¼Œ å†…å®¹æ˜¯ (Summary: This add support for CallMethod patterns in pattern_matcher. Also extends split_cat transforms to normalize tensor.split() type nodes Test Plan: Unit tests (fb + OSS) Differential Revision: D45195548 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Add support for call_method patterns,Summary: This add support for CallMethod patterns in pattern_matcher. Also extends split_cat transforms to normalize tensor.split() type nodes Test Plan: Unit tests (fb + OSS) Differential Revision: D45195548 ,2023-04-22T01:23:50Z,fb-exported Merged ciflow/trunk module: inductor ciflow/inductor merging,closed,0,3,https://github.com/pytorch/pytorch/issues/99782,This pull request was **exported** from Phabricator. Differential Revision: D45195548, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
263,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Skip timm_vision_transformer in Inductor torchbench smoketest)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Skip timm_vision_transformer in Inductor torchbench smoketest,Fixes ISSUE_NUMBER,2023-04-21T21:42:56Z,Merged ciflow/trunk topic: not user facing ciflow/inductor merging,closed,0,7,https://github.com/pytorch/pytorch/issues/99766, has been skipping in benchmark trainning run: https://github.com/pytorch/pytorch/blob/db46d9dc49bbc91ed9260422d0f5a443be8083bb/benchmarks/dynamo/torchbench.pyL95L96 Shall we skip it as well or replace it with ?,">  has been skipping in benchmark trainning run: >  > https://github.com/pytorch/pytorch/blob/db46d9dc49bbc91ed9260422d0f5a443be8083bb/benchmarks/dynamo/torchbench.pyL95L96 >  > Shall we skip it as well or replace it with ? But is the CI failure a recent recession? If yes, we should bisect it.","It's not a recent regression, it was added almost two months ago. https://github.com/pytorch/pytorch/pull/95685","Actually  training passed locally, I started CC([CI Testing] Reenable timm_efficientdet training) to reenable it, let's see what CI will say.",I think the problem is from the timm_vision_transformer test after timm_efficientdet. I am ok with skipping it for now as it runs ok on the dashboard., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
551,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Feature] storage pin memory support custom device.)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Contribute to storage for the privateuse1 backend.)  Support storage pin_memory and is_pinned for custom device, by calling dispatched tensor operations.   this pr is what we have discussed in issue CC(Contribute to storage for the privateuse1 backend.), would you please take a moment to review it, thanks.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Feature] storage pin memory support custom device.,"Fixes CC(Contribute to storage for the privateuse1 backend.)  Support storage pin_memory and is_pinned for custom device, by calling dispatched tensor operations.   this pr is what we have discussed in issue CC(Contribute to storage for the privateuse1 backend.), would you please take a moment to review it, thanks.",2023-04-21T09:55:27Z,open source Merged ciflow/trunk release notes: python_frontend,closed,0,4,https://github.com/pytorch/pytorch/issues/99712, merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
551,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Feature] storage pin memory support custom device.)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Contribute to storage for the privateuse1 backend.)  Support storage pin_memory and is_pinned for custom device, by calling dispatched tensor operations.   this pr is what we have discussed in issue CC(Contribute to storage for the privateuse1 backend.), would you please take a moment to review it, thanks.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Feature] storage pin memory support custom device.,"Fixes CC(Contribute to storage for the privateuse1 backend.)  Support storage pin_memory and is_pinned for custom device, by calling dispatched tensor operations.   this pr is what we have discussed in issue CC(Contribute to storage for the privateuse1 backend.), would you please take a moment to review it, thanks.",2023-04-21T08:51:41Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/99707
750,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Creating a fast path for compare the tensors sharing the same storage)ï¼Œ å†…å®¹æ˜¯ (Summary: For 423557497_225, most of the time is spent on matching whether some large tensors are equal. Actually, they are indeed equal, and these tensors share the same storage. So we create a fast path for such cases. With the optimization, we reduced the model loading time from 9m11s to 2m37s. Test Plan: buck run mode/optsplitdwarf scripts/lufang:load_pt_model  model_file_path=/data/local/models/423557497/225/423557497_225.predictor TODO: add more unittest. Differential Revision: D45174912)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Creating a fast path for compare the tensors sharing the same storage,"Summary: For 423557497_225, most of the time is spent on matching whether some large tensors are equal. Actually, they are indeed equal, and these tensors share the same storage. So we create a fast path for such cases. With the optimization, we reduced the model loading time from 9m11s to 2m37s. Test Plan: buck run mode/optsplitdwarf scripts/lufang:load_pt_model  model_file_path=/data/local/models/423557497/225/423557497_225.predictor TODO: add more unittest. Differential Revision: D45174912",2023-04-21T06:46:18Z,fb-exported release notes: jit,closed,0,2,https://github.com/pytorch/pytorch/issues/99703,This pull request was **exported** from Phabricator. Differential Revision: D45174912,This pull request was **exported** from Phabricator. Differential Revision: D45174912
628,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(In torchelastic support running worker rank 0 on agent rank 0 consistently)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Currently, when launching distributed jobs using torchrun, the Rank 0 worker can land on any arbitrary node. This ask is to add a new rendezvous implementation for which worker rank 0 always runs on agent rank 0.  Additional context This will improve observability of the distributed job by easily locating logs for the rank0 worker )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,In torchelastic support running worker rank 0 on agent rank 0 consistently," ğŸš€ The feature, motivation and pitch Currently, when launching distributed jobs using torchrun, the Rank 0 worker can land on any arbitrary node. This ask is to add a new rendezvous implementation for which worker rank 0 always runs on agent rank 0.  Additional context This will improve observability of the distributed job by easily locating logs for the rank0 worker ",2023-04-21T02:08:22Z,oncall: distributed triaged module: elastic,open,1,0,https://github.com/pytorch/pytorch/issues/99684
452,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix nnmodule call_method)ï¼Œ å†…å®¹æ˜¯ (  CC(Fix nnmodule call_method) Redirecting from ""_call_impl"" to .call_function was wrong Only redirecting from ""__call__"" to .call_function is right. It's subtle, but if someone changed the impl of torch.nn.Module._call_impl, then this matters )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,Fix nnmodule call_method,"  CC(Fix nnmodule call_method) Redirecting from ""_call_impl"" to .call_function was wrong Only redirecting from ""__call__"" to .call_function is right. It's subtle, but if someone changed the impl of torch.nn.Module._call_impl, then this matters ",2023-04-20T23:54:08Z,ciflow/trunk module: dynamo ciflow/inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/99678," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," maybe this helps with your recursion issue.    I think I'm right that the previous code is subtly wrong, but I also think its wrong to have so many special cases here.  I am not gonna try to rush to land this, will take a fresh look tmrw.  Also curious what CI will have to say..",Interesting I'm getting a correctness issue with this patch Repro here https://gist.github.com/msaroufim/e3c19a6485f2d6abd5dbc388a4349c48 ,> Interesting I'm getting a correctness issue with this patch You mean on trunk? or in addition to some of your changes for model.compile()?,"On trunk yeah, lmk if you're not seeing the error","Per my understanding, this change is not correct for the following case, though I think this is rare case: Look at this example:  Look at the logs:  Especially this line:  That means it's trying to inline  and it will go to . In main branch, we redirect it to  which would trigger both  and . However, after this change, it will only handle  function but ignoring , which is not expected."
1600,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Support aten::scaled_dot_product_attention in torchscript exporter)ï¼Œ å†…å®¹æ˜¯ (  CC([ONNX] Support aten::scaled_dot_product_attention in torchscript exporter) Fixes CC(UnsupportedOperatorError: Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 14 is not supported.)    ğŸ¤– Generated by Copilot at d06d195  Summary ğŸ†•ğŸš€ğŸ“  This pull request adds ONNX opset 14 support for the `nn.functional.scaled_dot_product_attention` operator, which is used for selfattention in transformer models. It does so by adding tests and annotations in `test/onnx/test_op_consistency.py`, and by adding a symbolic function in `torch/onnx/symbolic_opset14.py` that reuses an existing implementation. > _To export `scaled_dot_product_attention`_ > _To ONNX opset 14, we need some extension_ > _We import some modules and types_ > _And add a symbolic that pipes_ > _The existing code with some annotation_  Walkthrough *  Implement the `nn.functional.scaled_dot_product_attention` operator for ONNX opset 14 (link) *  Add imports for modules and types needed for the operator implementation (link) *  Add a command to run the pytest module for testing the operator consistency (link) *  Add the operator to the list of operators tested for consistency (link) *  Add annotations to indicate the operator's limitations and issues (link, link) *  Remove an empty line at the end of `test/onnx/test_op_consistency.py` (link))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Support aten::scaled_dot_product_attention in torchscript exporter,"  CC([ONNX] Support aten::scaled_dot_product_attention in torchscript exporter) Fixes CC(UnsupportedOperatorError: Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 14 is not supported.)    ğŸ¤– Generated by Copilot at d06d195  Summary ğŸ†•ğŸš€ğŸ“  This pull request adds ONNX opset 14 support for the `nn.functional.scaled_dot_product_attention` operator, which is used for selfattention in transformer models. It does so by adding tests and annotations in `test/onnx/test_op_consistency.py`, and by adding a symbolic function in `torch/onnx/symbolic_opset14.py` that reuses an existing implementation. > _To export `scaled_dot_product_attention`_ > _To ONNX opset 14, we need some extension_ > _We import some modules and types_ > _And add a symbolic that pipes_ > _The existing code with some annotation_  Walkthrough *  Implement the `nn.functional.scaled_dot_product_attention` operator for ONNX opset 14 (link) *  Add imports for modules and types needed for the operator implementation (link) *  Add a command to run the pytest module for testing the operator consistency (link) *  Add the operator to the list of operators tested for consistency (link) *  Add annotations to indicate the operator's limitations and issues (link, link) *  Remove an empty line at the end of `test/onnx/test_op_consistency.py` (link)",2023-04-20T20:09:12Z,module: onnx open source Merged ciflow/trunk release notes: onnx topic: new features,closed,1,8,https://github.com/pytorch/pytorch/issues/99658,"I would try to enable to test again after the adjustments to see if it passes. There may be quirks about onnx function that failed the test with ort in the onnxscript version, which we may be able to avoid here.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64 / test (default, 1, 3, macosm112) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,Has this issue been fixed?,The operator is supported since pytorch 2.1. Please open an issue if you see any errors. Thanks!
558,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ROCm runner 'worker-rocm-amd-108' is out of storage space)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Multiple ROCm jobs are failing with the error: `Error: No space left on device : '/home/pytorchci/actionsrunner/_diag/Worker_20230420194436utc.log'` These all occur on the runner  `workerrocmamd108` Example: https://github.com/pytorch/pytorch/actions/runs/4758073637/jobs/8456080762  Versions n/a )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,ROCm runner 'worker-rocm-amd-108' is out of storage space, ğŸ› Describe the bug Multiple ROCm jobs are failing with the error: `Error: No space left on device : '/home/pytorchci/actionsrunner/_diag/Worker_20230420194436utc.log'` These all occur on the runner  `workerrocmamd108` Example: https://github.com/pytorch/pytorch/actions/runs/4758073637/jobs/8456080762  Versions n/a ,2023-04-20T19:55:25Z,module: rocm triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/99656,This is a continuation of  CC(ROCm jobs are failing with No space left on device ),Diskspace issues should be resolved with 108 now: 
582,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failing debug assert:  RuntimeError: result.storage().use_count() == 1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Multiple tests fail with the same error when running them in debug mode.  To see the full logs, look at the jobs labeled ""debug"" over here:  https://hud.pytorch.org/pytorch/pytorch/commit/5d5e152fddd25259412155331bc0088a36753c51  Discovered during https://github.com/pytorch/pytorch/pull/92707  Versions n/a )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Failing debug assert:  RuntimeError: result.storage().use_count() == 1," ğŸ› Describe the bug Multiple tests fail with the same error when running them in debug mode.  To see the full logs, look at the jobs labeled ""debug"" over here:  https://hud.pytorch.org/pytorch/pytorch/commit/5d5e152fddd25259412155331bc0088a36753c51  Discovered during https://github.com/pytorch/pytorch/pull/92707  Versions n/a ",2023-04-20T19:39:01Z,module: sparse module: autograd module: nn triaged actionable,closed,0,9,https://github.com/pytorch/pytorch/issues/99655,"Thanks for the report. ? , I guess we can skip it? ?","Hmm the autograd one might be an issue with the codegen, since the _test_autograd_multiple_dispatch_view returns a view of the input it shouldn't emit the check that storage use count is 1.   ",> ? I think we just want channel shuffle to return an output with a shape that matches the input when the input is empty to avoid a hard crash ( CC(`nn.ChannelShuffle` will crash with empty input tensor)). Likely returning `self` to accomplish this was never the right way to go.,We should do `self.alias()` if we don't care and just need something that is similar to the input.,  Can you take a look at this?, for channel shuffle fix (don't return self),"Apparently, https://github.com/pytorch/pytorch/pull/92022 fixed this issue for `to_sparse` and `test_unsupported_backend_error_message` tests.","With the current main branch, only `test/test_autograd.py::TestAutogradMultipleDispatchCUDA::test_view_copy_cuda` fails with the reported exception. Here follows a simple reproducer (torch must be built with env. variable `DEBUG=1`):  A fix is provided in https://github.com/pytorch/pytorch/pull/104149",Closing as the PR ( CC(Make _test_autograd_multiple_dispatch_view a view operation)) fixing the last test case has landed.
827,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(pyi missing for `torch.fx` in wheel builds)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm getting typechecking errors with `torch.fx`. After looking into the problem, I found that everything should be correctly typed in https://github.com/pytorch/pytorch/blob/main/torch/fx/__init__.pyi, but this file does not exist in wheel distributions. The version I installed is https://download.pytorch.org/whl/cu117/torch2.0.0%2Bcu117cp38cp38linux_x86_64.whl. I also randomly checked some newest stable and nightly builds and the problem persists (didn't exhaustively check but I assume all the same).  Versions torch2.0.0+cu117cp38cp38linux_x86_64 Also in nightly (20230419) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,pyi missing for `torch.fx` in wheel builds," ğŸ› Describe the bug I'm getting typechecking errors with `torch.fx`. After looking into the problem, I found that everything should be correctly typed in https://github.com/pytorch/pytorch/blob/main/torch/fx/__init__.pyi, but this file does not exist in wheel distributions. The version I installed is https://download.pytorch.org/whl/cu117/torch2.0.0%2Bcu117cp38cp38linux_x86_64.whl. I also randomly checked some newest stable and nightly builds and the problem persists (didn't exhaustively check but I assume all the same).  Versions torch2.0.0+cu117cp38cp38linux_x86_64 Also in nightly (20230419) ",2023-04-19T14:27:02Z,module: typing triaged module: fx,closed,0,1,https://github.com/pytorch/pytorch/issues/99530,Thank you very much for reporting. There is no harm in adding to release
471,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯('Illegal instruction (core dumped)' for gpt-j bf16 generation task using greedy search )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug GPTJ model bf16 inference crashes for generation task with greedy search.   **Error info:** `Illegal instruction (core dumped)`   Versions torch              2.1.0.dev20230418+cpu )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,'Illegal instruction (core dumped)' for gpt-j bf16 generation task using greedy search , ğŸ› Describe the bug GPTJ model bf16 inference crashes for generation task with greedy search.   **Error info:** `Illegal instruction (core dumped)`   Versions torch              2.1.0.dev20230418+cpu ,2023-04-19T04:24:07Z,module: crash module: cpu triaged module: intel,open,0,15,https://github.com/pytorch/pytorch/issues/99509,"Experiments show that the crash occurs in the second bf16 matmul of Attention with AMX instruction,   it works when disable AMX instruction.  DNNL_VERBOSE=1 ONEDNN_MAX_CPU_ISA=AVX512_CORE_BF16 python test_greedy.py", can you please run `python3 mtorch.utils.collect_env` and post output here?,And also please post output from `DNNL_VERBOSE=1 ONEDNN_MAX_CPU_ISA=AVX512_CORE_BF16 python test_greedy.py` so that we can look if there are issues with oneDNN implementation for the case.,"> python3 mtorch.utils.collect_env Collecting environment information... PyTorch version: 2.1.0.dev20230418+cpucxx11abi Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: CentOS Stream release 8 (x86_64) GCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.018) Clang version: Could not collect CMake version: version 3.20.2 Libc version: glibc2.28 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.19.0rc6.0712.intel_next.1.x86_64+serverx86_64withglibc2.28 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              224 Online CPU(s) list: 0223 Thread(s) per core:  2 Core(s) per socket:  56 Socket(s):           2 NUMA node(s):        2 Vendor ID:           GenuineIntel CPU family:          6 Model:               143 Model name:          Genuine Intel(R) CPU 0000%@ Stepping:            8 CPU MHz:             3500.000 CPU max MHz:         3500.0000 CPU min MHz:         800.0000 BogoMIPS:            3800.00 Virtualization:      VTx L1d cache:           48K L1i cache:           32K L2 cache:            2048K L3 cache:            115200K NUMA node0 CPU(s):   055,112167 NUMA node1 CPU(s):   56111,168223 Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 shstk gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities Versions of relevant libraries: [pip3] numpy==1.24.3 [pip3] torch==2.1.0.dev20230418+cpu.cxx11.abi [conda] mkl                       2023.1.0         h6d00ec8_46342 [conda] mklinclude               2023.1.0         h06a4308_46342 [conda] numpy                     1.24.3                   pypi_0    pypi [conda] torch                     2.1.0.dev20230418+cpu.cxx11.abi          pypi_0    pypi",> DNNL_VERBOSE=1 ONEDNN_MAX_CPU_ISA=AVX512_CORE_BF16 ,same issue found in llama7b beam1 case !image after  ONEDNN_MAX_CPU_ISA=AVX512_CORE_BF16ï¼Œcoredump disappear, Does the issue still exist with oneDNN 3.x?,>  Does the issue still exist with oneDNN 3.x? there is no this issue for oneDNN 3.x,I do not see any issues with these shapes in oneDNN v2.7.3 or v3.1. ,"I managed to reproduce the issue on my side and root caused it to oneDNN matmul implementation for Intel AMX. In some cases oneDNN fails to reconfigure Intel AMX tiles between different shapes resulting in 'Illegal Instruction' error. With this model specifically the issue reproduces only with 112 threads. The issue is fixed in oneDNN v2.7.4 and all future versions. , , please help with updating oneDNN.","> The issue is fixed in oneDNN v2.7.4 and all future versions. >  > , , please help with updating oneDNN. Thanks for the quick update. Will bump oneDNN to 3.x soon.",", I would assume that v3.x goes into Pytorch main branch. It may be beneficial to update Pytorch 2.0 branch to oneDNN v2.7.4 to intercept Pytorch 2.0.1. ?", I'm afraid window for 2.0.1 patches closed last Thursday.,"This is unfortunate. Anyway, my point is that it may make sense to include the fix into one of the patch releases before Pytorch v2.1.",Facing same issue at 'import torch' with pytorch 2.0.1 rocm 5.4.2 on a pentium g4400 (no avx)
705,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([spmd] skip gradient copying for fused adam)ï¼Œ å†…å®¹æ˜¯ (  CC([spmd] simplify data parallel tests)  CC([spmd] Add a few more loss ops to the reduction op list)  CC([spmd] expose input_batch_dim to DataParallelMode)  CC([spmd] enable fully_shard fused_adam test)  CC([spmd] Use TupleStrategy and enable replicate fused_adam)  CC([spmd] skip gradient copying for fused adam)  CC(minor fix to fused adam meta registration)  CC(Improve ProxyTensor tensor_tree list/tuple handling) gradients does not need to be copy back as it's not useful)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[spmd] skip gradient copying for fused adam,  CC([spmd] simplify data parallel tests)  CC([spmd] Add a few more loss ops to the reduction op list)  CC([spmd] expose input_batch_dim to DataParallelMode)  CC([spmd] enable fully_shard fused_adam test)  CC([spmd] Use TupleStrategy and enable replicate fused_adam)  CC([spmd] skip gradient copying for fused adam)  CC(minor fix to fused adam meta registration)  CC(Improve ProxyTensor tensor_tree list/tuple handling) gradients does not need to be copy back as it's not useful,2023-04-19T01:06:49Z,Merged ciflow/trunk topic: not user facing merging,closed,0,2,https://github.com/pytorch/pytorch/issues/99489, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
495,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] make_fallback(aten.cumprod))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug While I was running inductor with LLaMA, I hit the following warning:  And not sure what it means. Will need sometime to prepare a reduced case.  suggests that this warning and potential workaround may be obvious to the inductor team.  Versions Nightly )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[Inductor] make_fallback(aten.cumprod)," ğŸ› Describe the bug While I was running inductor with LLaMA, I hit the following warning:  And not sure what it means. Will need sometime to prepare a reduced case.  suggests that this warning and potential workaround may be obvious to the inductor team.  Versions Nightly ",2023-04-18T22:58:36Z,oncall: pt2 module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/99474,This is a benign warning and we removed it already. ,"Thanks, ."
2012,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `parallelize_module` failed with `nn.Transformer` and the `PairwiseParallel` plan, which is unexpected according to the doc of `PairwiseParallel`.  Errors: test_tp_transformer.log  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230417 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.27 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01030awsx86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Tesla T4 GPU 1: Tesla T4 GPU 2: Tesla T4 GPU 3: Tesla T4 Nvidia driver version: 515.65.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              48 Online CPU(s) list: 047 Thread(s) per core:  2 Core(s) per socket:  24 Socket(s):           1 NUMA node(s):        1 Vendor ID:           GenuineIntel CPU family:          6 Model:               85 Model name:          Intel(R) Xeon(R) Platinum 825)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan," ğŸ› Describe the bug `parallelize_module` failed with `nn.Transformer` and the `PairwiseParallel` plan, which is unexpected according to the doc of `PairwiseParallel`.  Errors: test_tp_transformer.log  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230417 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.27 Python version: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01030awsx86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: Tesla T4 GPU 1: Tesla T4 GPU 2: Tesla T4 GPU 3: Tesla T4 Nvidia driver version: 515.65.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:        x86_64 CPU opmode(s):      32bit, 64bit Byte Order:          Little Endian CPU(s):              48 Online CPU(s) list: 047 Thread(s) per core:  2 Core(s) per socket:  24 Socket(s):           1 NUMA node(s):        1 Vendor ID:           GenuineIntel CPU family:          6 Model:               85 Model name:          Intel(R) Xeon(R) Platinum 825",2023-04-18T17:39:17Z,oncall: distributed triaged,open,0,1,https://github.com/pytorch/pytorch/issues/99432,"Hmm we need to change our comment, looks like we don't support `nn.Transformer`yet. You need to pass in the module path and parallel style to the API. Sorry for the late reply."
386,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([CUDA] Do accumulation for Adaptive Average Pooling in `opmath_t`)ï¼Œ å†…å®¹æ˜¯ (Fix for an issue surfaced from the discuss forum: https://discuss.pytorch.org/t/adaptiveavgpool2dcausessomedatatocontaininf/177420 CC    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[CUDA] Do accumulation for Adaptive Average Pooling in `opmath_t`,Fix for an issue surfaced from the discuss forum: https://discuss.pytorch.org/t/adaptiveavgpool2dcausessomedatatocontaininf/177420 CC    ,2023-04-17T22:36:16Z,module: cuda triaged open source Merged module: pooling ciflow/trunk topic: not user facing merging,closed,0,2,https://github.com/pytorch/pytorch/issues/99378, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
316,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(make ATen/native/cuda/AveragePool2d.cu data_ptr-correct)ï¼Œ å†…å®¹æ˜¯ (make ATen/native/cuda/AveragePool2d.cu data_ptrcorrect Test Plan: Rely on CI.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,make ATen/native/cuda/AveragePool2d.cu data_ptr-correct,make ATen/native/cuda/AveragePool2d.cu data_ptrcorrect Test Plan: Rely on CI.,2023-04-17T16:15:44Z,open source Merged ciflow/trunk topic: not user facing merging,closed,0,21,https://github.com/pytorch/pytorch/issues/99336, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: windowsbinaryconda / condapy3_10cuda11_8upload, windowsbinaryconda / condapy3_11cuda11_7upload Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: windowsbinaryconda / condapy3_10cuda11_8upload, windowsbinaryconda / condapy3_11cuda11_7upload Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 jobs have failed, first few of them are: windowsbinaryconda / condapy3_10cuda11_8upload, windowsbinaryconda / condapy3_11cuda11_7upload Details for Dev Infra team Raised by workflow job ", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `pr99336` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout pr99336 && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. You can rebase and merge by leaving the following comment on this PR: ` merge r` Or just rebase by leaving ` rebase` comment Details for Dev Infra team Raised by workflow job , rebase, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `pr99336` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout pr99336 && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1167,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Contribute to storage for the privateuse1 backend.)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Hi,   is mainly used for the CUDA backend and CPU backend.  Currently  I want to contribute to storage for the privateuse1 backend.  The content to be modified consists of two parts:  part a: python API, just like   part b: C++ API, just like  I don't know if it's better to add a function branch for privateuse1 backend or extract it into a function with distribution capabilities by dispatch.  Alternatives For  func    to support privateuse1 backend,  an backendrelated allocator is need . The possible modification is as follows(example).    In privateuse1.memory:    On the other hand,   I want to wrap the cpu storage to a cpu tensor, and use this parameter the param  as dispatch key for different backbend, such as . just like the fellowing example.  By this way, no need more function branch for new backend.  Additional context Would you please give me some advice? Thanks a lot.       )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Contribute to storage for the privateuse1 backend.," ğŸš€ The feature, motivation and pitch Hi,   is mainly used for the CUDA backend and CPU backend.  Currently  I want to contribute to storage for the privateuse1 backend.  The content to be modified consists of two parts:  part a: python API, just like   part b: C++ API, just like  I don't know if it's better to add a function branch for privateuse1 backend or extract it into a function with distribution capabilities by dispatch.  Alternatives For  func    to support privateuse1 backend,  an backendrelated allocator is need . The possible modification is as follows(example).    In privateuse1.memory:    On the other hand,   I want to wrap the cpu storage to a cpu tensor, and use this parameter the param  as dispatch key for different backbend, such as . just like the fellowing example.  By this way, no need more function branch for new backend.  Additional context Would you please give me some advice? Thanks a lot.       ",2023-04-17T15:00:15Z,triaged module: backend,closed,0,4,https://github.com/pytorch/pytorch/issues/99326,Triage review for label,"Wrapping and using the tensor operation to implement storage is something we have been doing and I would support here, if it works. Also  ","There is another case in .  In this case, perhaps trying to wrap storage as a tensor and call  instead of registering a backend specialized   is a better approach?  ","I generally encourage you to call dispatched tensor operations, which you can then override."
338,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(make ATen/native/cuda/AdaptiveAveragePooling3d.cu data_ptr-correct)ï¼Œ å†…å®¹æ˜¯ (make ATen/native/cuda/AdaptiveAveragePooling3d.cu data_ptrcorrect Test Plan: Rely on CI.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,make ATen/native/cuda/AdaptiveAveragePooling3d.cu data_ptr-correct,make ATen/native/cuda/AdaptiveAveragePooling3d.cu data_ptrcorrect Test Plan: Rely on CI.,2023-04-17T14:49:05Z,Merged ciflow/binaries ciflow/trunk topic: not user facing merging,closed,0,2,https://github.com/pytorch/pytorch/issues/99324, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1029,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Need TransformerEncoder to output attention map)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I need to observe the strength of attention for each element in the sequence with all the elements in the same sequence. This is called the attention map in the language of the transformer and is one of the important metrics for visualization. I want the output to be of shape num_layers x [batch_size, sequence_length, sequence_length].  The MultiheadAttention layer can return the attention map when `need_weights` is provided in the forward call, however, it is always set to `false` from the forward call of layer `TransformerEncoder`. It would be better if we could specify `need_weights` in `TransformerEncoder`'s forward call.  Proposed working changes: File: torch/nn/modules/transformer.py  Before:   After:   Additional context Pytorch version: 2.0.0 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Need TransformerEncoder to output attention map," ğŸš€ The feature, motivation and pitch I need to observe the strength of attention for each element in the sequence with all the elements in the same sequence. This is called the attention map in the language of the transformer and is one of the important metrics for visualization. I want the output to be of shape num_layers x [batch_size, sequence_length, sequence_length].  The MultiheadAttention layer can return the attention map when `need_weights` is provided in the forward call, however, it is always set to `false` from the forward call of layer `TransformerEncoder`. It would be better if we could specify `need_weights` in `TransformerEncoder`'s forward call.  Proposed working changes: File: torch/nn/modules/transformer.py  Before:   After:   Additional context Pytorch version: 2.0.0 ",2023-04-17T10:19:30Z,module: nn triaged topic: new features,open,9,6,https://github.com/pytorch/pytorch/issues/99304,Yes please! Looks like `need_weights` was set to false here https://github.com/pytorch/pytorch/pull/61692discussion_r670634743 which unfortunately also prevents getting the attentions by setting a `register_forward_hook` on the last layer. Does anyone know a good workaround for this without writing a transformer from scratch?,I also added it to decoder layer. ,"Arriving to this issue from trying to get attention maps of vision transformers (vits), actually, encoders don't even need to return the attention map. If only the attention block (sa, or in the case of vision transformers, scaled_dot_product_attention[1]) would return the attention map, we could hook this module to get attention during the forward pass without modifying encoder blocks. In the case of vits, the utility of the official implementation from pytorch is so low that even Meta's own repositories use vision transformer from timm [2] or their own implementation [3] instead of using pytorch vits. [1] https://github.com/pytorch/pytorch/blob/876161983d353f7553755a58b927762f09c86966/torch/nn/functional.pyLL5417C1L5417C93 [2] https://github.com/facebookresearch/dino/blob/main/vision_transformer.py [3] https://github.com/facebookresearch/mocov3/blob/main/vits.py","Here is easy solution! 1. copy&paste TransformerEncoderLayer to your .py file 2. switch False > True      ex)             def _sa_block(self, x: Tensor,                               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) > Tensor:                     x = self.self_attn(x, x, x,                                        attn_mask=attn_mask,                                        key_padding_mask=key_padding_mask,                                        **need_weights**=False)[0]   **fix this code > **need_weights=True**                     return self.dropout1(x) 4. with forward hook you can get the attention score! ","If the user does visualization, performance may not be of the utmost importance, but I want to highlight that any solution that sets needs_weights is necessarily slow due to the cost of additional member bandwidth, but also because it prevents the use of the fastest attention implementations such as flash attention","> If the user does visualization, performance may not be of the utmost importance, but I want to highlight that any solution that sets needs_weights is necessarily slow due to the cost of additional member bandwidth, but also because it prevents the use of the fastest attention implementations such as flash attention How abount this solution? If I want to pretrain total 10 epoch, disabling attention weight generation (just use the default mode) during the first 9 epochs and enabling it (modify custom nn.TransformerEncoderLayer) only for the final epoch."
981,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PT2] `isinstance` check failure with dynamic shape (gpt-j-6B))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Use dynamic shape to run gptj6B from HF transformers (`pip install transformers==4.26.1`). The `isinstance(num_beams, int)` check fails since `num_beams` is symbolic. Repro:  The error message:  Related code: ```python         if not isinstance(num_beams, int) or num_beams  [conda] mkl                       2023.0.0                 pypi_0    pypi [conda] mkldevel                 2023.0.0                 pypi_0    pypi [conda] mklinclude               2023.0.0                 pypi_0    pypi [conda] mklstatic                2023.0.0                 pypi_0    pypi [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.1.0a0+gitefc3887           dev_0     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[PT2] `isinstance` check failure with dynamic shape (gpt-j-6B)," ğŸ› Describe the bug Use dynamic shape to run gptj6B from HF transformers (`pip install transformers==4.26.1`). The `isinstance(num_beams, int)` check fails since `num_beams` is symbolic. Repro:  The error message:  Related code: ```python         if not isinstance(num_beams, int) or num_beams  [conda] mkl                       2023.0.0                 pypi_0    pypi [conda] mkldevel                 2023.0.0                 pypi_0    pypi [conda] mklinclude               2023.0.0                 pypi_0    pypi [conda] mklstatic                2023.0.0                 pypi_0    pypi [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.1.0a0+gitefc3887           dev_0     ",2023-04-17T07:14:07Z,triaged oncall: pt2 module: dynamic shapes module: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/99291,Confirmed this reproduces, it looks like isinstance is implemented incorrectly,fix  just needs a test
262,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] `AssertionError: Mixing fake modes NYI` from hf_Bart)ï¼Œ å†…å®¹æ˜¯ (Repro  Output  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[inductor] `AssertionError: Mixing fake modes NYI` from hf_Bart,Repro  Output  ,2023-04-17T04:33:59Z,triaged oncall: pt2 module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/99286,with the changes from my pr ,Observed also in  CC(TorchInductor CPU Performance Dashboard)issuecomment1512357199 (20230416 nightly)
657,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`requires_grad` does not respect parameters: Only Tensors of floating point and complex dtype can require gradients)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug While loading an `int8` quantized model, pytorch seemed not to respect the `requires_grad=False` parameter the model passed in. To reproduce:  Error log:  Note that in L359 of the model's `quantization.py` script it sets `requires_grad=False` but seems to be lost, so that `torch/nn/parameter.py` just reads the default value.  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",glm,`requires_grad` does not respect parameters: Only Tensors of floating point and complex dtype can require gradients," ğŸ› Describe the bug While loading an `int8` quantized model, pytorch seemed not to respect the `requires_grad=False` parameter the model passed in. To reproduce:  Error log:  Note that in L359 of the model's `quantization.py` script it sets `requires_grad=False` but seems to be lost, so that `torch/nn/parameter.py` just reads the default value.  Versions  ",2023-04-16T12:30:27Z,oncall: quantization,closed,0,1,https://github.com/pytorch/pytorch/issues/99264,"i think you'd need to look at the specifics of what that function is doing. You can't actually save/load quantized models (https://pytorch.org/docs/stable/quantization.htmlsavingandloadingquantizedmodels) you have to recreate the model and load the state dict or save the nonquantized model and quantize it after loading. In either case, it looks like its an issue with whatever this library is doing and you may be better off asking there."
2046,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (non-zero sized) parameter shards)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (nonzero sized) parameter shards)  Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (nonzero sized) parameter shards  The issue Now that ``use_orig_params=True`` allows nonuniform ``requires_grad`` (:tada: :rocket: thanks !!!) with [ CC([FSDP] Allow nonuniform `requires_grad` for `use_orig_params=True`)](https://github.com/pytorch/pytorch/pull/98221), there will be circumstances wherein some ranks have no (nonzero sized) local shards of the original parameters (and hence no associated gradients).  Use Cases For a simple Transformer case, imagine a user wraps all encoder layers in separate FSDP instances but allows the classifier head to be wrapped in the same FSDP instance as the relatively large embeddings layers. While this is a suboptimal wrapping strategy for most usecases, I believe it is expected to be supported (full precision training works in that context). I originally encountered this issue while extending a package I maintain, leveraging the relaxed ``requires_grad`` contstraint to simplify multiphase scheduled finetuning FSDP configuration, so a concrete example is there.  Reproduction and Remediation Currently, ``ShardedGradScaler`` does not accommodate these situations, failing to initialize ``optimizer_state[""found_inf_per_device""]`` when ``unscale_`` is called. In this PR, I extend the existing ``ShardedGradScaler`` tests with an ``use_orig_params=True`` dimension added to the parameterization and test scenarios wherein one rank possesses no (nonzero sized) parameter shards. The relevant issue can be reproduced with the tests I'm adding in this PR. The current (prePR))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (non-zero sized) parameter shards,"Fixes CC(Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (nonzero sized) parameter shards)  Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (nonzero sized) parameter shards  The issue Now that ``use_orig_params=True`` allows nonuniform ``requires_grad`` (:tada: :rocket: thanks !!!) with [ CC([FSDP] Allow nonuniform `requires_grad` for `use_orig_params=True`)](https://github.com/pytorch/pytorch/pull/98221), there will be circumstances wherein some ranks have no (nonzero sized) local shards of the original parameters (and hence no associated gradients).  Use Cases For a simple Transformer case, imagine a user wraps all encoder layers in separate FSDP instances but allows the classifier head to be wrapped in the same FSDP instance as the relatively large embeddings layers. While this is a suboptimal wrapping strategy for most usecases, I believe it is expected to be supported (full precision training works in that context). I originally encountered this issue while extending a package I maintain, leveraging the relaxed ``requires_grad`` contstraint to simplify multiphase scheduled finetuning FSDP configuration, so a concrete example is there.  Reproduction and Remediation Currently, ``ShardedGradScaler`` does not accommodate these situations, failing to initialize ``optimizer_state[""found_inf_per_device""]`` when ``unscale_`` is called. In this PR, I extend the existing ``ShardedGradScaler`` tests with an ``use_orig_params=True`` dimension added to the parameterization and test scenarios wherein one rank possesses no (nonzero sized) parameter shards. The relevant issue can be reproduced with the tests I'm adding in this PR. The current (prePR)",2023-04-14T17:40:27Z,triaged open source Merged ciflow/trunk release notes: distributed (fsdp) topic: bug fixes merging,closed,0,3,https://github.com/pytorch/pytorch/issues/99175, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"  I think this change is failing multigpu test in trunk, for example https://hud.pytorch.org/pytorch/pytorch/commit/05809c7d3b79f47b1d86b0c622a81ac92d1b340e.  Could you help take a look?  I have disabled the test in the meantime  CC(DISABLED test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_mixed_precision_use_orig_params (__main__.TestShardedGradScalerParityWithDDP))"
2046,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (non-zero sized) parameter shards)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  The issue Now that ``use_orig_params=True`` allows nonuniform ``requires_grad`` (:tada: :rocket: thanks !!!) with [ CC([FSDP] Allow nonuniform `requires_grad` for `use_orig_params=True`)](https://github.com/pytorch/pytorch/pull/98221), there will be circumstances wherein some ranks have no (nonzero sized) local shards of the original parameters (and hence no associated gradients).  Use Cases For a simple Transformer case, imagine a user wraps all encoder layers in separate FSDP instances but allows the classifier head to be wrapped in the same FSDP instance as the relatively large embeddings layers. While this is a suboptimal wrapping strategy for most usecases, I believe it is expected to be supported (full precision training works in that context). I originally encountered this issue while extending a package I maintain, leveraging the relaxed ``requires_grad`` contstraint to simplify multiphase scheduled finetuning FSDP configuration, so a concrete example is there.  Reproduction and Remediation Currently, ``ShardedGradScaler`` does not accommodate these situations, failing to initialize ``optimizer_state[""found_inf_per_device""]`` when ``unscale_`` is called. In the PR I'm submitting shortly, I extend the existing ``ShardedGradScaler`` tests with an ``use_orig_params=True`` dimension added to the parameterization and test scenarios wherein one rank possesses no (nonzero sized) parameter shards. The relevant issue can be reproduced with the tests I'm adding in the PR that will be attached to this issue shortly. The current (prePR) execution of those tests fail in ``use_orig_params=True`` mode with this error:  Thanks as always to the PyTorch distributed team for your astonishingly impressive)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (non-zero sized) parameter shards," ğŸ› Describe the bug  The issue Now that ``use_orig_params=True`` allows nonuniform ``requires_grad`` (:tada: :rocket: thanks !!!) with [ CC([FSDP] Allow nonuniform `requires_grad` for `use_orig_params=True`)](https://github.com/pytorch/pytorch/pull/98221), there will be circumstances wherein some ranks have no (nonzero sized) local shards of the original parameters (and hence no associated gradients).  Use Cases For a simple Transformer case, imagine a user wraps all encoder layers in separate FSDP instances but allows the classifier head to be wrapped in the same FSDP instance as the relatively large embeddings layers. While this is a suboptimal wrapping strategy for most usecases, I believe it is expected to be supported (full precision training works in that context). I originally encountered this issue while extending a package I maintain, leveraging the relaxed ``requires_grad`` contstraint to simplify multiphase scheduled finetuning FSDP configuration, so a concrete example is there.  Reproduction and Remediation Currently, ``ShardedGradScaler`` does not accommodate these situations, failing to initialize ``optimizer_state[""found_inf_per_device""]`` when ``unscale_`` is called. In the PR I'm submitting shortly, I extend the existing ``ShardedGradScaler`` tests with an ``use_orig_params=True`` dimension added to the parameterization and test scenarios wherein one rank possesses no (nonzero sized) parameter shards. The relevant issue can be reproduced with the tests I'm adding in the PR that will be attached to this issue shortly. The current (prePR) execution of those tests fail in ``use_orig_params=True`` mode with this error:  Thanks as always to the PyTorch distributed team for your astonishingly impressive",2023-04-14T17:35:55Z,oncall: distributed triaged module: fsdp,closed,1,2,https://github.com/pytorch/pytorch/issues/99174,"Thanks  for filing this detailed issue. Feel free to move forward with your PR. I will be out next week, but I can review it once I am back.","> Thanks  for filing this detailed issue. Feel free to move forward with your PR. I will be out next week, but I can review it once I am back. Sounds good, thanks !  The PR is now ready for review with all checks passed. Enjoy your week off!"
2043,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Run ChatRWKV on MBP(intel CPU)+eGPU[rx6800 16G], returna a very big number -9223372036854775808, looks like overflow)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Run ChatRWKV using 'mps', returna a very big number, looks like overflow. MBP(intel CPU, not M1/M2), with eGPU[rx6800 16G] pytorch==2.0.0 It can  load model, but when calculate the 1st token, it gets a very big number `9223372036854775808` and return error. error log:  I tried to rollback to the previous version of pytorch(1.12.1/1.13.1), it will send a warning msg and fallback to cpu, but it can  return a correct number:  https://github.com/BlinkDL/ChatRWKV/issues/93  Versions (ChatRWKV) pptPro ChatRWKV % python ./collect_env.py  Collecting environment information... PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.2.1 (x86_64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.10 (main, Mar 21 2023, 13:41:39) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Intel(R) Core(TM) i51038NG7 CPU @ 2.00GHz Versions of relevant libraries: [pip3] numpy==1.24.2 [pip3] torch==2.0.0 [pip3] torchaudio==2.0.1 [pip3] torchvision==0.15.1 [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.0.0                    pypi_0    pypi [conda] torchaudio                2.0.1                    pypi_0    pypi [conda] torchvision               0.15.1                   pypi_0    pypi (ChatRWKV) pptPro ChatRWKV %  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,"Run ChatRWKV on MBP(intel CPU)+eGPU[rx6800 16G], returna a very big number -9223372036854775808, looks like overflow"," ğŸ› Describe the bug Run ChatRWKV using 'mps', returna a very big number, looks like overflow. MBP(intel CPU, not M1/M2), with eGPU[rx6800 16G] pytorch==2.0.0 It can  load model, but when calculate the 1st token, it gets a very big number `9223372036854775808` and return error. error log:  I tried to rollback to the previous version of pytorch(1.12.1/1.13.1), it will send a warning msg and fallback to cpu, but it can  return a correct number:  https://github.com/BlinkDL/ChatRWKV/issues/93  Versions (ChatRWKV) pptPro ChatRWKV % python ./collect_env.py  Collecting environment information... PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.2.1 (x86_64) GCC version: Could not collect Clang version: 14.0.3 (clang1403.0.22.14.1) CMake version: Could not collect Libc version: N/A Python version: 3.10.10 (main, Mar 21 2023, 13:41:39) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Intel(R) Core(TM) i51038NG7 CPU @ 2.00GHz Versions of relevant libraries: [pip3] numpy==1.24.2 [pip3] torch==2.0.0 [pip3] torchaudio==2.0.1 [pip3] torchvision==0.15.1 [conda] numpy                     1.24.2                   pypi_0    pypi [conda] torch                     2.0.0                    pypi_0    pypi [conda] torchaudio                2.0.1                    pypi_0    pypi [conda] torchvision               0.15.1                   pypi_0    pypi (ChatRWKV) pptPro ChatRWKV %  ",2023-04-14T15:49:46Z,triaged module: mps,open,0,1,https://github.com/pytorch/pytorch/issues/99160,Related:  CC('mps' and argmax())  CC(torch.argmax only returns -9223372036854775808 on mps ) CC(torch.tensor.argmax will get a error output when device='mps')
621,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([EdgeML] Remove dependency on all_mobile_model_configs.yaml from pt_operator_library BUCK rule)ï¼Œ å†…å®¹æ˜¯ (Summary: Removes the dependency on the unified YAML file Test Plan: Smoke test via some caffe2 tests.  Build a major FoA app that uses model tracing  and confirm it still works.  CI/CD for the rest.  If operator tracing / bundling was broken, I'd hope in the 1000+ tests spawned by this change should catch it. Differential Revision: D44946368)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[EdgeML] Remove dependency on all_mobile_model_configs.yaml from pt_operator_library BUCK rule,"Summary: Removes the dependency on the unified YAML file Test Plan: Smoke test via some caffe2 tests.  Build a major FoA app that uses model tracing  and confirm it still works.  CI/CD for the rest.  If operator tracing / bundling was broken, I'd hope in the 1000+ tests spawned by this change should catch it. Differential Revision: D44946368",2023-04-14T02:38:44Z,fb-exported Merged ciflow/trunk merging,closed,0,7,https://github.com/pytorch/pytorch/issues/99122,This pull request was **exported** from Phabricator. Differential Revision: D44946368,This pull request was **exported** from Phabricator. Differential Revision: D44946368,This pull request was **exported** from Phabricator. Differential Revision: D44946368,This pull request was **exported** from Phabricator. Differential Revision: D44946368,This pull request was **exported** from Phabricator. Differential Revision: D44946368, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
594,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Look into test coverage for `UntypedStorage`)ï¼Œ å†…å®¹æ˜¯ (In CC(Fix loading data on different encoding), there was a mypy error when trying to use `_StorageBase.__setitem__`. Since we don't currently have this mypy error in the main branch, this suggests that `UntypedStorage.__setitem__` is not getting tested anywhere, and a test should be added for it. There might be other tests that `UntypedStorage` is lacking as well )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Look into test coverage for `UntypedStorage`,"In CC(Fix loading data on different encoding), there was a mypy error when trying to use `_StorageBase.__setitem__`. Since we don't currently have this mypy error in the main branch, this suggests that `UntypedStorage.__setitem__` is not getting tested anywhere, and a test should be added for it. There might be other tests that `UntypedStorage` is lacking as well ",2023-04-13T21:02:55Z,module: typing module: tests triaged,open,0,0,https://github.com/pytorch/pytorch/issues/99082
971,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PT2E][Quant] Support specifying None for obs_or_fq_ctr in target_dtype_info)ï¼Œ å†…å®¹æ˜¯ (  CC([PT2E][Quant] Support for embedding op quantization via
ExecuTorchNativeQuantizer)  CC([PT2E][Quant] Support specifying None for obs_or_fq_ctr in target_dtype_info)  CC([PT2E][Quant][BE] Refactor observer code)  CC([PT2E][Quant][BE] Split short term and long term tests in different files)  CC([PT2E][Quant][BE] Move pt2e quantization test to separate folder)  CC([PT2E][Quantization] Refactor Quantizer and QNNPACKQuantizer) It is cleaner for quantizer to say what does not need observation instead of putting fp32 observers. This diff add support for that by checking if target_dtype_info contains none for specific observers and if so skip inserting observers for those. Differential Revision: D44971357)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[PT2E][Quant] Support specifying None for obs_or_fq_ctr in target_dtype_info,"  CC([PT2E][Quant] Support for embedding op quantization via
ExecuTorchNativeQuantizer)  CC([PT2E][Quant] Support specifying None for obs_or_fq_ctr in target_dtype_info)  CC([PT2E][Quant][BE] Refactor observer code)  CC([PT2E][Quant][BE] Split short term and long term tests in different files)  CC([PT2E][Quant][BE] Move pt2e quantization test to separate folder)  CC([PT2E][Quantization] Refactor Quantizer and QNNPACKQuantizer) It is cleaner for quantizer to say what does not need observation instead of putting fp32 observers. This diff add support for that by checking if target_dtype_info contains none for specific observers and if so skip inserting observers for those. Differential Revision: D44971357",2023-04-13T18:57:42Z,Merged ciflow/trunk release notes: quantization release notes: AO frontend merging,closed,0,2,https://github.com/pytorch/pytorch/issues/99071, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
496,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RFC] Allow elastic agent to fail fast)ï¼Œ å†…å®¹æ˜¯ (Summary: Today, on a segfault on a single trainer , we end up keeping the gpu on all ranks blocked for 5 minutes due to elastic agents barrier timeouts Test Plan: Rely on existing test to validate . Looking to get some feedback on adding UTs Differential Revision: D44929488)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,[RFC] Allow elastic agent to fail fast,"Summary: Today, on a segfault on a single trainer , we end up keeping the gpu on all ranks blocked for 5 minutes due to elastic agents barrier timeouts Test Plan: Rely on existing test to validate . Looking to get some feedback on adding UTs Differential Revision: D44929488",2023-04-13T17:03:53Z,fb-exported Merged ciflow/trunk release notes: distributed (miscellaneous),closed,0,40,https://github.com/pytorch/pytorch/issues/99051,The committers listed above are authorized under a signed CLA.:white_check_mark: login: shrikantnagori / name: Shrikant Nagori  (f90d7b1e160c31371bc34aa4e4823dca4ce8db77):white_check_mark: login: facebookgithubbot / name: Facebook Community Bot  (f90d7b1e160c31371bc34aa4e4823dca4ce8db77),This pull request was **exported** from Phabricator. Differential Revision: D44929488,"thanks for the PR, could you elaborate on the failure scenarios (would help if you are specific about num workers and nodes and whether the node fails/exits on a signal, exception, or exits successfully) and the desired outcome behavior?",">> thanks for the PR, could you elaborate on the failure scenarios (would help if you are specific about num workers and nodes and whether the node fails/exits on a signal, exception, or exits successfully) and the desired outcome behavior? 1. unsuccessful (non zero) exit code 2. hardware/host crashes are the two common cases. Most often we run 8 gpu on a single Trainer Worker group(host) , so if training is using say 64 gpus. We will have 8 Trainer Worker groups, so if one fails, it waits 5 minutes for the other worker groups to finish for the above 2 failures. ","> > > thanks for the PR, could you elaborate on the failure scenarios (would help if you are specific about num workers and nodes and whether the node fails/exits on a signal, exception, or exits successfully) and the desired outcome behavior? >  > 1. unsuccessful (non zero) exit code > 2. hardware/host crashes >  > are the two common cases. Most often we run 8 gpu on a single Trainer Worker group(host) , so if training is using say 64 gpus. We will have 8 Trainer Worker groups, so if one fails, it waits 5 minutes for the other worker groups to finish for the above 2 failures. I see, so in terms of what behavior torchelastic currently exhibits, the worker failures can be bucketed into two categories: 1. Worker process exists with a nonzero exit code, either due to an catchable exception or a signal  2. Something outside the worker process exits with a nonzero exit code (e.g. torchelastic agent failure (doesn't happen all that often), host failure) For 2) the exit barrier is irrelevant since the failure will be caught at the scheduler level (not torchelastic). For 1) I see that the agent waits on the exit barrier (see code) on worker failures. And I wonder whether that is even needed... ( ,    do you guys remember why we added an exit barrier for worker failures?) On partial worker failures, the surviving workers on the other nodes will most likely get stuck until PTD times them out after `process_group.timeout` seconds or the node with the failed worker exits (after waiting on the exit barrier), whichever is shorter. In either case, this is a catastrophic failure with no reliable way to recover other than to restart all the workers, so there really isn't a great reason to wait on an exit barrier, but we might have added it purposefully based on an ops event in the past.", I don't think I was involved with this before but if you restart one worker before all workers are restarting there's higher risk of getting bad overlapping rendezvous behavior where some workers are trying to rerendezvous but others aren't and you may just hit a timeout in rdzv instead.,I think removing the barrier is great as long as we can ensure there is something else that will mark the other workers as failed. Today timeout serves that purpose. So my take was to keep a small timeout to avoid changing the behavior on failure on other workers. ,">  I don't think I was involved with this before but if you restart one worker before all workers are restarting there's higher risk of getting bad overlapping rendezvous behavior where some workers are trying to rerendezvous but others aren't and you may just hit a timeout in rdzv instead. Gotcha, FWIW in torchelastic, overlapping rdzv is avoided in this case (even without the exit barrier) since the elastic agent that observed the first worker failure would ""reannounce"" itself (as if it was readded to the rdzv group). The monitoring loop on the other agents (runs every 30sec by default) will detect that there's a node waiting to be admitted and will SIGTERM all their respective workers and rerdzv. > I think removing the barrier is great as long as we can ensure there is something else that will mark the other workers as failed.  Today we rely on PTD (the pg watchdog) to raise a `TimeoutError` naturally since if there are partial worker failures, then the collective op will get stuck, then eventually timeout. > Today timeout serves that purpose. So my take was to keep a small timeout to avoid changing the behavior on failure on other workers. If you are referring to the proces group `timeout`, then yes. If by `timeout` you were referring to the `exit_barrier()`, then I'm not 100% sure this was by design. The following is what is ""supposed"" to happen when there is a worker failure: 1. Assume `j 2x4` (2 nodes, 4 workers on each node) 2. Assume rank 1 on node 0 failed 3. The elastic agent on node 0 will kill ranks 0, 1, 2, 3 4. If `max_restarts == 0`, then the elastic agent on node 0 will exit with a nonzero status, and hence the scheduler will deem the job as failed (it could restart the job according to the scheduler's job definition policy) 5. If `max_restarts > 0`, then the elastic agent on node 0 will ""reannounce"" itself to the rdzv group. The other workers in the group will notice that there's an agent waiting to ""join"" the group and will then proceed to kill all their local workers, and rerdzv. Now IIRC, this exit barrier was added due to the way PyPER was using torch.rpc and therefore had ranks that finish (either successfully or unsuccessfully) at different times."," i was not referring to the process group timeout. I was referring to the exit barrier , can you point me to the process group timeout that you are referring to. It looks like the exit barrier served no purpose other than rpc.shutdown in PyPer, but for that we have a timeout in the path added these days of 60 seconds, so that shall serve the same purpose. I can take the diff in a few paths from here 1\ Remove the barrier from non restart case when agent fails as the pg timeout 2\ Remove the barrier from restart and non restart case when agent fails What do you recommend ?","> can you point me to the process group timeout that you are referring to. Take a look at https://pytorch.org/docs/stable/distributed.htmltorch.distributed.init_process_group !image > I can take the diff in a few paths from here I think we should:  1. Remove the `exit_barrier()` for the failure case (see code). 2. Add a unittest testing for this behavior (run 2 agents, 2 workers each, with `max_retries=1` fail rank 1 the first time by design then make sure that the exit barrier is not hit and that the workers can rerdzv fine without overlapping rdzv versions). >  It looks like the exit barrier served no purpose other than rpc.shutdown in PyPer, but for that we have a timeout in the path added these days of 60 seconds, so that shall serve the same purpose. If this is to get pyper jobs to failfast and pyper does not need the exit barrier anymore (since its added 60sec torch.rpc timeouts), can't we run those jobs with `exit_barrier_timeout=0`?",This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","  you are listed as code owners, will you be able to help with merging?",This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488," , please take a look ",This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488,This pull request was **exported** from Phabricator. Differential Revision: D44929488
1420,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_transformer_no_grad_mixed_precision_True (__main__.TestNoGrad))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_transformer_no_grad_mixed_precision_True` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `distributed/fsdp/test_fsdp_core.py` or `distributed/fsdp/test_fsdp_core.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/master/test/distributed/fsdp/test_fsdp_core.py 1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0) headers: {} )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_transformer_no_grad_mixed_precision_True (__main__.TestNoGrad),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_transformer_no_grad_mixed_precision_True` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `distributed/fsdp/test_fsdp_core.py` or `distributed/fsdp/test_fsdp_core.py` ResponseTimeoutError: Response timeout for 5000ms, GET https://raw.githubusercontent.com/pytorch/pytorch/master/test/distributed/fsdp/test_fsdp_core.py 1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0) headers: {} ",2023-04-13T03:39:47Z,oncall: distributed triaged module: flaky-tests skipped module: fsdp,closed,0,7,https://github.com/pytorch/pytorch/issues/99011,"The failure is reported from within the fsdp test files, so this seems more related to distributed than transformer/mha. But maybe I'm misreading this?","It looks like the test is either hitting SIGSEGV or SIGABRT:    varma   Interestingly, this is `use_orig_params=False` code path. I do not think anything notable changed recently for that. Is it possible this is just some hardware flakiness?",I was able to repro this after many runs. We need to investigate this.,"This repros on our AWS cluster with high failure rate using:  There may be a synchronization issue when running  followed by more forward passes (e.g. `ref_output = fsdp_model(*input)`). I say this because adding a `torch.cuda.synchronize()` after the `self._train_for_several_steps()` seems to avoid the error (100 consecutive passes); however, maybe that is blocking CPU execution enough to hide the race.  I will update with any new findings.","The issue is not unique to this unit test only. I can repro the same error on `test_pre_backward_hook_registration_cuda_first_True`, and this is with `CUDA_LAUNCH_BLOCKING=1`:   Full stack trace    Notably, the main CPU thread is able to proceed past the `dist.destroy_process_group()` before the final `sys.exit(0)` before the error: https://github.com/pytorch/pytorch/blob/05809c7d3b79f47b1d86b0c622a81ac92d1b340e/torch/testing/_internal/common_fsdp.pyL910", ,"I am folding this into  CC([Distributed] Destruction order fiasco in ProcessGroupNCCL workCleanupLoop()) because the issue does not repro with `NCCL_DESYNC_DEBUG=0` (which disables the NCCL `workCleanupLoop`). I ran `test_transformer_no_grad` 100 times without the error reproing, whereas with `NCCL_DESYNC_DEBUG=1`, it repros usually after only a few runs."
550,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(InternalTorchDynamoError when using torch.compile with Huggingface Llama model and PEFT)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When `torch.compile` is applied to the Huggingface model `LlamaForCausalLM` wrapped with PEFT (https://github.com/huggingface/peft), the forward compilation failed with `torch._dynamo.exc.InternalTorchDynamoError`.  Minimal Example   Error Trace:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,InternalTorchDynamoError when using torch.compile with Huggingface Llama model and PEFT," ğŸ› Describe the bug When `torch.compile` is applied to the Huggingface model `LlamaForCausalLM` wrapped with PEFT (https://github.com/huggingface/peft), the forward compilation failed with `torch._dynamo.exc.InternalTorchDynamoError`.  Minimal Example   Error Trace:   Versions  ",2023-04-12T23:47:08Z,needs reproduction triaged module: dynamo,closed,4,10,https://github.com/pytorch/pytorch/issues/98993,"I meet the same problem, my env is:  python3.9, a100  here is my error: ",Any fix to the issue? I am facing the same issue as well.,Any update for the issue,"Using the Ludwig framework, I just got the same issue while trying the compilation option while finetuning a llama model (it also stops at the forward pass). WSL2 Debian Torch Version: 2.0.1+cu118","This seems to work on `main` branch. hwang  Can you check again? If it does not work, can you please provide a repro script.","The offending line in PEFT apparently was this one: https://github.com/huggingface/peft/blob/139624750a6a431b9958be5c9485aec5571d64c1/src/peft/peft_model.pyL933 It doesn't matter what comes in the ifblock, simply accessing `self.base_model.config` caused the error. With PyTorch v2.1.0, it is working though, so next release should fix it.","Ok, I will close the issue then. Please feel free to reopen if you still see the issue.",But PyTorch v2.1.0 hasnt been released yet. SO is there any work around for this?,i installed 2.1.0 still facing the error,I have something similar to this error
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AOTAutograd: fix 'Trying to backward through the graph a second time' error)ï¼Œ å†…å®¹æ˜¯ (Fixes  CC(Using 'aot_eager' to compile model makes an error about `backward()` and `detach()`). See discussion and comment in the PR for more details.   CC(fix scalartensor issue with CrossEntropyLoss label smoothing, symintify)  CC(change torch._dynamo.export(aten_graph=...) to allow pre_autograd tracing)  CC(fix perdispatchkeymode caching bug)  CC(AOTAutograd: fix 'Trying to backward through the graph a second time' error) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AOTAutograd: fix 'Trying to backward through the graph a second time' error,"Fixes  CC(Using 'aot_eager' to compile model makes an error about `backward()` and `detach()`). See discussion and comment in the PR for more details.   CC(fix scalartensor issue with CrossEntropyLoss label smoothing, symintify)  CC(change torch._dynamo.export(aten_graph=...) to allow pre_autograd tracing)  CC(fix perdispatchkeymode caching bug)  CC(AOTAutograd: fix 'Trying to backward through the graph a second time' error) ",2023-04-12T19:19:35Z,Merged ciflow/trunk release notes: composability topic: bug fixes module: aotdispatch module: inductor module: dynamo merging,closed,0,18,https://github.com/pytorch/pytorch/issues/98960, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , merge,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 mandatory check(s) failed.  The first few are:  pull / linuxbionicpy3.11clang9 / test (default, 2, 3, linux.2xlarge)  pull / linuxbionicpy3.11clang9 / test (default, 3, 3, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","Looked at the failures, it looks like this PR regresses this case:  Digging into it more. also fixing the lint","So, it looks like there are some cases where we legitimately assign a None gradient for a few ops.. e.g. `norm()` ops, here: https://github.com/pytorch/pytorch/blob/c650d7b67fdc7594c7c400721f481914aac51e6e/torch/csrc/autograd/FunctionsManual.cppL265 So in eager mode, the above repro prints the following:   (and would it be reasonable to diverge from eager here and raise an error with torch.compile)? I'm not sure that we can differentiate between this case and the detach() case, since in both cases our compiled bw graph will produce `None` grad_inputs.","some offline discussion:  pointed out that in theory we could update these derivative formulas to return zero tensors instead of None, and use EfficientZeroTensor to avoid the perf penalty.  pointed out that if the zero tensor needs to get populated in the grad field, then we're forced to materialize the tensor of zeros (expensive) For now in this PR, I'm xfail'ing the tests above and living with the (slight) difference in eager mode. To be clear, this error only shows up if we have a graph such that **every** tensor input that requires gradients would have gotten a None grad in eager mode.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxbioniccuda11.8py3.10gcc7sm86 / test (default, 5, 5, linux.g5.4xlarge.nvidia.gpu) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ","`linalg.det` also has the same issue (was failing an inductor test)  I skipped the test in the same way. We might want to consider fixing `linalg.det` to not return `None` gradients though (in this case it's cheap, since it only appears to happen when you input is size `(0, 0)`  That was the only remaining test failure on CI (minus 3 failing tests related to distributed, that I confirmed were failing on master but are now passing). Fingers crossed for the next merge.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch rebase origin/master` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
653,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add storage serialization methods for privateuse1)ï¼Œ å†…å®¹æ˜¯ (add entry for privateuse1 storage serialization register_package in _register_device_module. 1. User only need to implement `privateuse1_tag` and `privateuse1_deserialize` in the device module of open device. When registering device module, the methods are registered with _package_registry in storage serialization. 2. Provides a fixed sequence number 30 for privateuse1 in storage serialization _package_registry list.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,add storage serialization methods for privateuse1,"add entry for privateuse1 storage serialization register_package in _register_device_module. 1. User only need to implement `privateuse1_tag` and `privateuse1_deserialize` in the device module of open device. When registering device module, the methods are registered with _package_registry in storage serialization. 2. Provides a fixed sequence number 30 for privateuse1 in storage serialization _package_registry list.",2023-04-12T07:49:04Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/98920, drci,"While I understand the motivation, I am a bit skeptical, because the whole point of privateuse1 is to be localized a single process with a particular device loaded; if you can serialize privateuse1... well, now it's not very private, is it?","Yes, we want privateuse1 to support more comprehensive features, like cuda; The serialization of storage is also one of the features. In this way, the open device is easier to adapt. Back to the current issue. It might be a better way to register the _tag and _deserialize methods for privateuse1 at initialization. Add privateuse1 registration to torch/serialization.py  The preceding implementation depends on the generation of the privateuse1 method in storage, which is implemented in 98478, such as `storage.foo()` or `storage.is_foo` What's your idea?  ","Yes, I think I like your proposed new strategy better. The basic idea is to universally install a privateuse1 backend that delegates to the open device module, IIUC. That seems good.",æ‚¨çš„é‚®ä»¶å·²æ”¶åˆ°ï¼Œæˆ‘ä¼šå°½å¿«å¤„ç†ï¼.,The committers listed above are authorized under a signed CLA.:white_check_mark: login: XDaoHong  (b4f23feddc81303b6a90bcced689675459493241),"This looks broadly ok, need to fix ci", merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job "," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
397,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Gloo][BE] Print stacktrace on collectFullMesh)ï¼Œ å†…å®¹æ˜¯ (  CC([Gloo][BE] Print stacktrace on collectFullMesh) Catch error and torch_check it so full C++ stacktrace is printed for better debug Differential Revision: D44860626)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Gloo][BE] Print stacktrace on collectFullMesh,  CC([Gloo][BE] Print stacktrace on collectFullMesh) Catch error and torch_check it so full C++ stacktrace is printed for better debug Differential Revision: D44860626,2023-04-11T03:41:28Z,Merged release notes: distributed (c10d) merging,closed,0,2,https://github.com/pytorch/pytorch/issues/98810," merge f ""CI finished"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1682,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(simplify indexing expression before trying to determine strides)ï¼Œ å†…å®¹æ˜¯ (This fixes a few failing cases where we fail to compute stride_hint for an indexing expression with ModularIndexing When can size_hint error out? It shouldn't happen when we are getting regular size hints for expressions where free vars are in ShapeEnv. But this is not the case when we try to recover strides from indexing expressions (which is what stride_hint is for). Suppose you have an indexing expression that looks like   and want to understand its stride wrt to variable `d1`. Let's ignore for a moment that stride for ModularIndexing is not well defined, it'll become negative around modulo divisor value, but even without that, the way we usually compute stride is we substitute `0` and `1` for `d1` and compute difference in indexing expression with those substitutions  this is our stride. But for the expression above, the difference would result in an expression that still has free variable `d2` that we don't have a substitution for.  The fix that this PR makes is it expands stride computation to substitute not only `0` and `1` for the variable we are computing a stride for, but also `0` for other variables in the indexing expression (`support_vars`).  Note that computing strides in `stride_hints` is a performance optimization that we use to reorder dimensions or make split decisions for split reduction. If it fails, it's not a hard error  we may incorrectly apply reordering by it won't affect correctness.   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,simplify indexing expression before trying to determine strides,"This fixes a few failing cases where we fail to compute stride_hint for an indexing expression with ModularIndexing When can size_hint error out? It shouldn't happen when we are getting regular size hints for expressions where free vars are in ShapeEnv. But this is not the case when we try to recover strides from indexing expressions (which is what stride_hint is for). Suppose you have an indexing expression that looks like   and want to understand its stride wrt to variable `d1`. Let's ignore for a moment that stride for ModularIndexing is not well defined, it'll become negative around modulo divisor value, but even without that, the way we usually compute stride is we substitute `0` and `1` for `d1` and compute difference in indexing expression with those substitutions  this is our stride. But for the expression above, the difference would result in an expression that still has free variable `d2` that we don't have a substitution for.  The fix that this PR makes is it expands stride computation to substitute not only `0` and `1` for the variable we are computing a stride for, but also `0` for other variables in the indexing expression (`support_vars`).  Note that computing strides in `stride_hints` is a performance optimization that we use to reorder dimensions or make split decisions for split reduction. If it fails, it's not a hard error  we may incorrectly apply reordering by it won't affect correctness.   ",2023-04-10T21:18:35Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor merging,closed,0,2,https://github.com/pytorch/pytorch/issues/98783, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
735,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([release 2.0.1] Warn once for TypedStorage deprecation)ï¼Œ å†…å®¹æ˜¯ (Release 2.0.0 added a warning for TypedStorage deprecation. However, that warning is emitted multiple times, rather than just once, because the warning message contains the line of user code that caused the warning, so the `warnings` module filter doesn't filter them out. PR CC(Only warn once for TypedStorage deprecation) fixed the warning to only happen once PR CC(Specify file encoding in test_torch.py) fixed an error introduced by a test from CC(Only warn once for TypedStorage deprecation) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[release 2.0.1] Warn once for TypedStorage deprecation,"Release 2.0.0 added a warning for TypedStorage deprecation. However, that warning is emitted multiple times, rather than just once, because the warning message contains the line of user code that caused the warning, so the `warnings` module filter doesn't filter them out. PR CC(Only warn once for TypedStorage deprecation) fixed the warning to only happen once PR CC(Specify file encoding in test_torch.py) fixed an error introduced by a test from CC(Only warn once for TypedStorage deprecation) ",2023-04-10T19:43:35Z,open source module: python frontend,closed,0,0,https://github.com/pytorch/pytorch/issues/98777
608,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Fix type casting copy with storage offset (#95573))ï¼Œ å†…å®¹æ˜¯ (This PR handles the case where the `dst` tensor of type casting has a storage offset by creating a temporary buffer to store results and then copy them back to the dst with the offset added. Fixes CC(tensor __setitem__ incorrect on mps when dtypes mismatch) Pull Request resolved: https://github.com/pytorch/pytorch/pull/95573 Approved by: https://github.com/kulinseth)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MPS] Fix type casting copy with storage offset (#95573),This PR handles the case where the `dst` tensor of type casting has a storage offset by creating a temporary buffer to store results and then copy them back to the dst with the offset added. Fixes CC(tensor __setitem__ incorrect on mps when dtypes mismatch) Pull Request resolved: https://github.com/pytorch/pytorch/pull/95573 Approved by: https://github.com/kulinseth,2023-04-10T18:48:49Z,open source release notes: mps ciflow/mps,closed,0,2,https://github.com/pytorch/pytorch/issues/98770," thanks for the fix in the original PR. I have couple of points here for Copy kernels with different data types.  1. First one there is a Performance hit and increase memory footprint where we are using the copy_cast to write to a temporary MTLBuffer and the doing a Blit. This can be combined in a single Dispatch, using a single kernel. If you are up for it, please propose that as a PR in master branch.  2. Second I think we still need to test the scenarios where we have strided copy (for view ops) with different types. I am not sure all the cases are covered as we are just looking at Destination offset.  I would recommend we fix this properly in the following release.","Hi , sorry I'm not actually the one who worked on the fix in the original PR, it's  instead  I was just the one who created the issue. I just opened this PR since the original issue seemed forgotten and I was hoping the fix could be incorporated into a new release."
300,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add storage share memory support for more device)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER add storage share memory support for more device.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,add storage share memory support for more device,Fixes ISSUE_NUMBER add storage share memory support for more device.,2023-04-10T09:02:29Z,open source Stale,closed,0,1,https://github.com/pytorch/pytorch/issues/98731,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
324,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cherry-pick of #97214)ï¼Œ å†…å®¹æ˜¯ (Cherrypick of CC([Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Cherry-pick of #97214,Cherrypick of CC([Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA),2023-04-10T03:19:19Z,release notes: nn,closed,0,0,https://github.com/pytorch/pytorch/issues/98718
419,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add parameter for pin memory of storage to support other devices.)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER Add parameter for pin memory of storage to support other devices. In the future, other backends will provide their own allocators to create pin memory.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add parameter for pin memory of storage to support other devices.,"Fixes ISSUE_NUMBER Add parameter for pin memory of storage to support other devices. In the future, other backends will provide their own allocators to create pin memory.",2023-04-09T09:21:00Z,triaged open source Merged ciflow/trunk release notes: python_frontend topic: new features merging,closed,0,8,https://github.com/pytorch/pytorch/issues/98692,The committers listed above are authorized under a signed CLA.:white_check_mark: login: wbigat  (07606c5b9dfbddfbc306fed7fe1112d011567d0a), merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: windowsbinarylibtorchdebug / libtorchcpusharedwithdepsdebugbuild Details for Dev Infra team Raised by workflow job "," merge f ""looks like infra failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
383,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.library.Library.impl: add missing param in docstring example)ï¼Œ å†…å®¹æ˜¯ (Stack from ghstack:  CC(torch.library.Library.impl: add missing param in docstring example) previously this was missing the callable )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,torch.library.Library.impl: add missing param in docstring example,Stack from ghstack:  CC(torch.library.Library.impl: add missing param in docstring example) previously this was missing the callable ,2023-04-07T19:46:47Z,Merged ciflow/trunk topic: not user facing ciflow/nightly merging,closed,0,2,https://github.com/pytorch/pytorch/issues/98619, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
275,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix off-by-1 error in dynamo coverage stats)ï¼Œ å†…å®¹æ˜¯ (  CC(Fix offby1 error in dynamo coverage stats) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix off-by-1 error in dynamo coverage stats,  CC(Fix offby1 error in dynamo coverage stats) ,2023-04-06T23:46:30Z,Merged ciflow/trunk topic: bug fixes topic: not user facing module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/98558, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3arm64mps / test (default, 1, 1) Details for Dev Infra team Raised by workflow job ", merge ic," Merge started Your change will be merged while ignoring the following 1 checks: trunk / macos12py3arm64mps / test (default, 1, 1) Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
482,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(always on shadow storage instrumentation)ï¼Œ å†…å®¹æ˜¯ (always on shadow storage instrumentation Summary: Test Plan: Reviewers: Subscribers: Tasks: Tags:  Stack created with Sapling. Best reviewed with ReviewStack.  CC(always on shadow storage instrumentation)  CC(warn on future reshape alias mutation violations))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,always on shadow storage instrumentation,always on shadow storage instrumentation Summary: Test Plan: Reviewers: Subscribers: Tasks: Tags:  Stack created with Sapling. Best reviewed with ReviewStack.  CC(always on shadow storage instrumentation)  CC(warn on future reshape alias mutation violations),2023-04-06T20:22:20Z,open source Stale release notes: mps ciflow/mps,closed,0,1,https://github.com/pytorch/pytorch/issues/98536,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
2016,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([FSDP] `use_orig_params=True` with CPU offloading and Gradient Accumulation: RuntimeError)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I use HugginFace/Accelerate for FSDP training of RoBERTabased models, e.g. `xlmrobertabase` and others, on multiple GPUs [2..8].   Preconditions:  FSDP is initialized with `use_orig_params=True` via Accelerate's `FullyShardedDataParallelPlugin`  CPU offloading is **ON**  Gradient Accumulation is **ON**, with steps > 1. Accelerate uses `no_sync` context for gradient accumulation, as far as I understand correctly  `torch.compile` is **NOT used**  Train in full precision using `tf32` or `fp32`  Outcome: While first iteration in the training loop the error appears:  Error does **NOT** happen, when:  `use_orig_params=False`, or  CPU Offloading is **OFF**, or   Gradient Accumulation Steps = 1, or  FSDP with 1 GPU I am not sure whether this issue is somehow caused by Add Gradient Accumulation Outside no_sync() Compatibility with CPU Offloading` Compatibility with CPU Offloading)) CC([FSDP] Add Gradient Accumulation Outside `no_sync()` Compatibility with CPU Offloading)  Expected Behavior: Training loop should run and output loss values.  Steps to reproduce: 1. Clone repo with reproduction code   `git clone https://github.com/agrizzli/test_use_orig_params` 2. Install and activate:  `virtualenv venv`  `source venv/bin/activate`  `pip install r requirements.txt` 3. Run one of scripts for MaskedLM or SequenceClassification:  `accelerate launch config_file accelerate.cfg train_sentiment.py` or  `accelerate launch config_file accelerate.cfg train_mlm.py`  Deps:  Use torch `2.0.0` or latest nightly `2.1.0.dev20230406`  (and corresponding `torchtriton` dep)  Use accelerate `0.18.0` or latest from `main` branch on `20230404`  Use transformers `4.27.4`  Links:  Repo for reproduction of error  Env: PyTorch versi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[FSDP] `use_orig_params=True` with CPU offloading and Gradient Accumulation: RuntimeError," ğŸ› Describe the bug I use HugginFace/Accelerate for FSDP training of RoBERTabased models, e.g. `xlmrobertabase` and others, on multiple GPUs [2..8].   Preconditions:  FSDP is initialized with `use_orig_params=True` via Accelerate's `FullyShardedDataParallelPlugin`  CPU offloading is **ON**  Gradient Accumulation is **ON**, with steps > 1. Accelerate uses `no_sync` context for gradient accumulation, as far as I understand correctly  `torch.compile` is **NOT used**  Train in full precision using `tf32` or `fp32`  Outcome: While first iteration in the training loop the error appears:  Error does **NOT** happen, when:  `use_orig_params=False`, or  CPU Offloading is **OFF**, or   Gradient Accumulation Steps = 1, or  FSDP with 1 GPU I am not sure whether this issue is somehow caused by Add Gradient Accumulation Outside no_sync() Compatibility with CPU Offloading` Compatibility with CPU Offloading)) CC([FSDP] Add Gradient Accumulation Outside `no_sync()` Compatibility with CPU Offloading)  Expected Behavior: Training loop should run and output loss values.  Steps to reproduce: 1. Clone repo with reproduction code   `git clone https://github.com/agrizzli/test_use_orig_params` 2. Install and activate:  `virtualenv venv`  `source venv/bin/activate`  `pip install r requirements.txt` 3. Run one of scripts for MaskedLM or SequenceClassification:  `accelerate launch config_file accelerate.cfg train_sentiment.py` or  `accelerate launch config_file accelerate.cfg train_mlm.py`  Deps:  Use torch `2.0.0` or latest nightly `2.1.0.dev20230406`  (and corresponding `torchtriton` dep)  Use accelerate `0.18.0` or latest from `main` branch on `20230404`  Use transformers `4.27.4`  Links:  Repo for reproduction of error  Env: PyTorch versi",2023-04-06T11:18:47Z,oncall: distributed triaged module: fsdp,closed,1,0,https://github.com/pytorch/pytorch/issues/98494
629,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add the DistributedDataParallel types that were lost when distributed.pyi was removed)ï¼Œ å†…å®¹æ˜¯ (The file `torch/nn/parallel/distributed.pyi` was removed in https://github.com/pytorch/pytorch/pull/88701. I am not sure about the reason for the removal, but some effort was invested in adding type annotations to the `DistributedDataParallel` class and it is unfortunate that these were lost. This pull request adds them to `torch/nn/parallel/distributed.py`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add the DistributedDataParallel types that were lost when distributed.pyi was removed,"The file `torch/nn/parallel/distributed.pyi` was removed in https://github.com/pytorch/pytorch/pull/88701. I am not sure about the reason for the removal, but some effort was invested in adding type annotations to the `DistributedDataParallel` class and it is unfortunate that these were lost. This pull request adds them to `torch/nn/parallel/distributed.py`.",2023-04-06T06:20:14Z,triaged open source Stale,closed,0,4,https://github.com/pytorch/pytorch/issues/98482,"> Looks like the linter isn't happy, could you please check that? I will. Sorry, should have created this as draft.",varma seems this has been forgotten. Can you please review?,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",Is there any particular reason this pull request is not being reviewed?
849,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add custom backend case for storage and automatically generate storage attributes.)ï¼Œ å†…å®¹æ˜¯ (Currently storage only considers partial backend. We want storage to create on custom backend by key PrivateUse1. It also provides an easy automatic generation of storagerelated attributes. When the user registers a new backend, the corresponding methods and attributes can be automatically generated. Do this code. `torch.utils.rename_privateuse1_backend('foo')` `torch.utils.generate_storage_for_privateuse1_backend()` Then, get the following methods and attributes. `torch.TypedStorage.is_foo` `torch.TypedStorage.foo()` `torch.UntypedStorage.is_foo` `torch.UntypedStorage.foo()` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add custom backend case for storage and automatically generate storage attributes.,"Currently storage only considers partial backend. We want storage to create on custom backend by key PrivateUse1. It also provides an easy automatic generation of storagerelated attributes. When the user registers a new backend, the corresponding methods and attributes can be automatically generated. Do this code. `torch.utils.rename_privateuse1_backend('foo')` `torch.utils.generate_storage_for_privateuse1_backend()` Then, get the following methods and attributes. `torch.TypedStorage.is_foo` `torch.TypedStorage.foo()` `torch.UntypedStorage.is_foo` `torch.UntypedStorage.foo()` ",2023-04-06T03:14:16Z,module: internals triaged open source Merged ciflow/trunk topic: not user facing merging,closed,0,12,https://github.com/pytorch/pytorch/issues/98478, do you have more context behind what you're trying to do?,Related:  CC(Add methods for StorageImpl to obtain its subclass information.)? There was also some discussion on autogenerated methods in this PR: https://github.com/pytorch/pytorch/pull/98066. I don't think we want to work extra to autogenerate some methods like `.is_foo()` (see the discussion in that PR for details). But a backend implementor can choose to monkey patch those methods themselves!,">  do you have more context behind what you're trying to do? Currently extending the custom backend requires a lot of workï¼ŒI hope to extract some common logic or methods on the framework side to reduce the work of extension to a certain extent. Like bdhirsh point of view, the `is_foo`  and `foo` methods currently provided to the storage and tensor classes in this pr and CC(Automatically generate attributes and methods for custom backends.) can also be implemented through the customside monkey patch.  But I think it is fine to provide a method like the one here that registers them if the backend writer does want them. What I want to do next is related to CC(Request custom backend device memory Allocator.)ï¼Œprovide a general and basic device memory management mechanism, users only need to register the actual malloc and free functions. Of course, if they need it, they can develop a memory pool that is more suitable for their backend",> Related: CC(Add methods for StorageImpl to obtain its subclass information.)? nothing to do with it.   Hi ï¼š I think albanD made a certain affirmation of the meaning of my workï¼Œwould you willing to help me merge my extension after I have modified my function annotations in detail., , merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: windowsbinarylibtorchrelease / libtorchcpusharedwithdepsreleasetest Details for Dev Infra team Raised by workflow job ", merge ic," Merge started Your change will be merged while ignoring the following 2 checks: windowsbinarylibtorchrelease / libtorchcpusharedwithdepsreleasetest, trunk / macos12py3arm64 / build Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
368,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Add] storage support for custom backend.)ï¼Œ å†…å®¹æ˜¯ (Currently storage only considers partial backend. We want storage to create on custom backend by key PrivateUse1.  Could you review my changes?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Add] storage support for custom backend.,Currently storage only considers partial backend. We want storage to create on custom backend by key PrivateUse1.  Could you review my changes?,2023-04-06T01:09:51Z,open source Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/98469," label ""topic: not user facing""", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. You can rebase and merge by leaving the following comment on this PR: ` merge r` Or just rebase by leaving ` rebase` comment Details for Dev Infra team Raised by workflow job , merge r, successfully started a rebase job. Check the current status here,"Successfully rebased `dev_storage` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout dev_storage && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
831,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([torch.library] Add ability to create library fragments)ï¼Œ å†…å®¹æ˜¯ (Stack from ghstack:  CC(Simple Custom Operator API, V0)  CC([torch.library] Add ability to create library fragments)  CC(Fix test_python_dispatch under debug mode)  CC([autograd_function_db] Add NumpyTake as OpInfo) In C++ we have TORCH_LIBRARY_FRAGMENT. This PR adds the same functionality to the Python torch.library API. The motivation for this is: for the simple custom op API, we don't want users to need to deal with Library objects. One way to hide this from users is to create library fragments. Test Plan:  tests that you can create multiple fragments and def+impl operators on each.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[torch.library] Add ability to create library fragments,"Stack from ghstack:  CC(Simple Custom Operator API, V0)  CC([torch.library] Add ability to create library fragments)  CC(Fix test_python_dispatch under debug mode)  CC([autograd_function_db] Add NumpyTake as OpInfo) In C++ we have TORCH_LIBRARY_FRAGMENT. This PR adds the same functionality to the Python torch.library API. The motivation for this is: for the simple custom op API, we don't want users to need to deal with Library objects. One way to hide this from users is to create library fragments. Test Plan:  tests that you can create multiple fragments and def+impl operators on each.",2023-04-05T19:31:19Z,Merged ciflow/trunk release notes: composability merging,closed,0,2,https://github.com/pytorch/pytorch/issues/98439, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
821,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add lint rule for std::round and replace with std::nearbyint)ï¼Œ å†…å®¹æ˜¯ (Avoid inconsistencies introduced by `std::round` such as that discovered in CC(Make the Index Rounding Mode Consistent Between the 2D and 3D GridSample Nearest Neighbor Interpolations). `torch.round` has round to nearest even behavior, which is different from `std::round` but consistent with `std::nearbyint` with `FE_TONEAREST`. There are a few comments throughout the codebase saying we don't use `std::round` but we should lint for this. Added a lint rule for `aten/src/ATen/native/**`   Stack from ghstack:  CC(Add lint rule for std::round and replace with std::nearbyint))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add lint rule for std::round and replace with std::nearbyint,"Avoid inconsistencies introduced by `std::round` such as that discovered in CC(Make the Index Rounding Mode Consistent Between the 2D and 3D GridSample Nearest Neighbor Interpolations). `torch.round` has round to nearest even behavior, which is different from `std::round` but consistent with `std::nearbyint` with `FE_TONEAREST`. There are a few comments throughout the codebase saying we don't use `std::round` but we should lint for this. Added a lint rule for `aten/src/ATen/native/**`   Stack from ghstack:  CC(Add lint rule for std::round and replace with std::nearbyint)",2023-04-05T19:17:17Z,Stale topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/98435,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",Should we merge this? :D 
676,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(assert callable(unaltered_fn))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug This is a bug generated from  CC([compile] TypeError: __init__() missing 1 required positional argument: 'parent_module') To reproduce, check out transformers and patch (I tested on a515d0a77c769954ac2f0151a2a99c04d8d6cf95)  Then, run  It fails with  It's possible that HF is misusing the torch.compile API (there's some sort of repeated wrapping going on, it seems like), but even if that's true, it shouldn't assert error.    Versions master)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,assert callable(unaltered_fn)," ğŸ› Describe the bug This is a bug generated from  CC([compile] TypeError: __init__() missing 1 required positional argument: 'parent_module') To reproduce, check out transformers and patch (I tested on a515d0a77c769954ac2f0151a2a99c04d8d6cf95)  Then, run  It fails with  It's possible that HF is misusing the torch.compile API (there's some sort of repeated wrapping going on, it seems like), but even if that's true, it shouldn't assert error.    Versions master",2023-04-05T18:52:55Z,high priority triaged oncall: pt2,closed,0,9,https://github.com/pytorch/pytorch/issues/98434,", the model wrapping is just to deal with with DDP or Deepspeed or whatever other framework drives the multigpu training. I'm not sure where you see a repeated wrapping. The wrapping code is here: https://github.com/huggingface/transformers/blob/15641892985b1d77acc74c9065c332cd7c3f7d7f/src/transformers/trainer.pyL1386 And if we aren't doing `compile` in the right way, we are all ears to correct it. Thank you.",Maybe fixed last week? We should try to rerun this, can you confirm?,I'm unable to repro this  it doesn't look like HF calls torch.compile in their trainer wrapper code anymore. I grepped the repo too and couldn't find where the torch_compile trainer args are used. Perhaps they're now favoring the user calling torch.compile on the model directly?,Close since Lazos cannot repro.  Feel free to reopen w/ repro instruction if still fails for you.,", I investigated this  `torch.compile` functionality was delegated to Accelerate in this PR: https://github.com/huggingface/transformers/commit/03db59104714f60b591ef48073840b288ee4cdc0 so it's still there in the trainer, but is now done by Accelerate if `torch_compile` flag is passed.", thanks!! I will take another look at this then,Looks like it is passing  I patched this line in accelerate  https://github.com/huggingface/accelerate/blob/eafcea07f639a5476385854ea9bccdbba467db9d/src/accelerate/accelerator.pyL1409,"I was able to update everything and I confirm that this particular issue is no longer there. I still can't use `torch.compile`  even after turning off DDP `torch._dynamo.config.optimize_ddp=False` it spins the CPU  looking at the stack trace it's deep inside recursive `percolate_tags` calls  fluctuating between 100150 frames deep, I gave up after 10min. This was just llama2 model from HF transformers. Well, will try again in 6 months."
1319,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support backward hook optimizers in FSDP)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I'm currently optimizing the Lightning reference implementation of LLaMA (7B), although the following will be generally applicable to any LLM with high memory pressure. The default configuration (24GB model sharded across 4x40GB A100s) is just on the cusp of being able to run (Between weights, 2 AdamW states, and gradients for the shard **logical** GPU memory caps out around 27GB, although I don't think that captures comms buffers) and the profile shows clear signs of allocator thrashing. After a bit of hacking I came up with this monstrosity:    More importantly, that's enough to get out of the high contention regime and decreases the step time close to an order of magnitude. But given how much FSDP internal state I had to crack open to get things running (and I'm sure I missed plenty...) it's really only suitable as a PoC. CC varma    Alternatives I know there's been more general discussion of creating optimizers on the fly so there might be a better alternative to the big list of single Tensor optimizers.  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,Support backward hook optimizers in FSDP," ğŸš€ The feature, motivation and pitch I'm currently optimizing the Lightning reference implementation of LLaMA (7B), although the following will be generally applicable to any LLM with high memory pressure. The default configuration (24GB model sharded across 4x40GB A100s) is just on the cusp of being able to run (Between weights, 2 AdamW states, and gradients for the shard **logical** GPU memory caps out around 27GB, although I don't think that captures comms buffers) and the profile shows clear signs of allocator thrashing. After a bit of hacking I came up with this monstrosity:    More importantly, that's enough to get out of the high contention regime and decreases the step time close to an order of magnitude. But given how much FSDP internal state I had to crack open to get things running (and I'm sure I missed plenty...) it's really only suitable as a PoC. CC varma    Alternatives I know there's been more general discussion of creating optimizers on the fly so there might be a better alternative to the big list of single Tensor optimizers.  Additional context _No response_ ",2023-04-05T15:59:27Z,oncall: distributed triaged module: fsdp,open,2,5,https://github.com/pytorch/pytorch/issues/98419,"From checking, I see there are already 3 implementations of this in torch.distributed:  torch.distributed.algorithms.ddp_comm_hooks.  torch.distributed.algorithms._optimizer_overlap  torch.distributed.optim.apply_optimizer_in_backward Maybe one of these work with FSDP?",I think varma had plans for adding this support in FSDP. Maybe we just need to resync on the timeline.,"Hi  , sorry for the late follow up here!  https://github.com/pytorch/pytorch/pull/98667 prototypes the FSDP overlapped optimizer by making it work with `apply_optimizer_in_backward` API.  It uses many similar ideas as in 's prototype:  Runs optimizer step in post backward  set grad to None after optimizer step call However, had a q on your implementation:  Since we use `register_hook` API and FSDP uses this API as well separately to do gradient communication in backward, how do we ensure these are appropriately ordered? Overall, we'd like FSDP optimizer overlap to support the following:  Usable through apply_optimizer_in_backward API  Setting grad to None after step() provides the expected memory saving  Ideally, in CPU offload case, we can run optimizer step on the GPU before offloading params and grads, but this needs some more design discussion.  , agreed the current situation with hooks is confusing. Here is a small clarification:  ddp_comm_hooks: are supposed to be general hooks for gradient communication (i.e. bf16 allreduce)  _optimizer_overlap: was our first prototype of DDP optimizer overlap, but not the API we are planning to publicly release. We should kill this and remove the code  apply_optimizer_in_backward is our new optimizer overlap API, which can work for general use cases but is being implemented for DDP / FSDP","> Since we use `register_hook` API and FSDP uses this API as well separately to do gradient communication in backward, how do we ensure these are appropriately ordered? On the host side I rely on the existence of `_post_backward_hook_state` to know that FSDP has gotten a chance to register its hooks. (It *seems* like backward hooks respect registration order...) And of course I had to use the comm stream to make sure device kernels were properly ordered. I'm very happy that you're thinking of how to pair this with CPU offload. In this case I don't know that you can get much out of parameter offloading since they not only need to be present in the forward, but also the backward due to activation checkpointing. OTOH if you kept the optimizer state on host, sent grads to the host, and sent parameter updates back to device you could save 2 bytes of device memory for every byte roundtrip'd. (My experiments indicate that you don't have enough PCI bandwidth to do this for all state, but even offloading a fraction would do wonders.) But yeah, very complex and interesting space so some design work is needed.","Hi, it seems as though this discussion has stopped for some reason. I'm also interested in how this can be used with an optimizer hook (as I want to do gradient clipping of some sort)."
498,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([reland] remove typed StorageImpl::data() and StorageImpl::unsafe_data())ï¼Œ å†…å®¹æ˜¯ (  CC([reland] remove typed StorageImpl::data() and StorageImpl::unsafe_data())  CC(rewrite at::vec::*::convert_to_int_of_same_size) Original commit changeset: a466b3cb6a0a Original Phabricator Diff: D44629941 Differential Revision: D44709004 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[reland] remove typed StorageImpl::data() and StorageImpl::unsafe_data(),  CC([reland] remove typed StorageImpl::data() and StorageImpl::unsafe_data())  CC(rewrite at::vec::*::convert_to_int_of_same_size) Original commit changeset: a466b3cb6a0a Original Phabricator Diff: D44629941 Differential Revision: D44709004 ,2023-04-05T13:48:00Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/98411, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1092,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add methods for StorageImpl to obtain its subclass information.)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch We found that the Storageimpl class no longer has the final keyword. So thirdparty extensions sometimes inherit StorageImpl to record hardwarerelated information, just like `received_cuda_`, but Storage is a class visible on the python side. Sometimes we want to directly obtain subclass information from Storage, but We only have the parent class pointer `c10::intrusive_ptr storage_impl_;` in Storage class. So one idea I had was to add a method to StoageImpl and Storage, something like  But this has the disadvantage that subclasses must put all their extra information in only one class for recovery from void* pointer.I don't know if this is reasonable, or if there is any other better implementation. Please give your valuable suggestions.  Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add methods for StorageImpl to obtain its subclass information.," ğŸš€ The feature, motivation and pitch We found that the Storageimpl class no longer has the final keyword. So thirdparty extensions sometimes inherit StorageImpl to record hardwarerelated information, just like `received_cuda_`, but Storage is a class visible on the python side. Sometimes we want to directly obtain subclass information from Storage, but We only have the parent class pointer `c10::intrusive_ptr storage_impl_;` in Storage class. So one idea I had was to add a method to StoageImpl and Storage, something like  But this has the disadvantage that subclasses must put all their extra information in only one class for recovery from void* pointer.I don't know if this is reasonable, or if there is any other better implementation. Please give your valuable suggestions.  Alternatives _No response_  Additional context _No response_",2023-04-05T13:45:56Z,,closed,0,6,https://github.com/pytorch/pytorch/issues/98410,One question that I have is: what sort of information do you want to stash away in a StorageImpl subclass that is not suitable for you to put directly on your `TensorImpl` subclass instead?," Just like FunctionalStorageImpl, since storageimpl is inheritable, there must be some scenarios where subclasses will add their own additional information. For me, I want to add some description to describe what format this data is stored in. These formats may be strongly hardware dependent. Like C4HWC4 in MNN.But this information is actually strongly related to memory layout, so it is more suitable to be placed in storeageimpl instead of Tensorimpl.","And we can directly call `tensor.set_(storage)` on the python side, if placed in tensorimpl, we cannot get the data storage information mentioned above.", ,"I am not really a fan of increasing API surface area on storage. Instead of adding more methods on storage, make a function that gets the Storage, casts it to your subclass, and then does the access.",Got it.
240,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add HPU to the storage tensor backends)ï¼Œ å†…å®¹æ˜¯ (Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add HPU to the storage tensor backends,Fixes ISSUE_NUMBER,2023-04-05T10:57:17Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/98404, merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
418,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PTD][Checkpoint] Upstream fsspec storage read/write to PT)ï¼Œ å†…å®¹æ˜¯ (Remove sync_files. Remove single_file_per_rank and will add it back once we resolve the issue.  CC([PTD][Checkpoint] Enable single_file_per_rank for fsspec storage read/write) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[PTD][Checkpoint] Upstream fsspec storage read/write to PT,Remove sync_files. Remove single_file_per_rank and will add it back once we resolve the issue.  CC([PTD][Checkpoint] Enable single_file_per_rank for fsspec storage read/write) ,2023-04-05T05:57:27Z,Merged ciflow/trunk release notes: distributed (checkpoint),closed,0,12,https://github.com/pytorch/pytorch/issues/98387,Is something like this better located in torchsnapshot?  , rebase,> Is something like this better located in torchsnapshot? >  > 'd like to enable them to use fsspec., successfully started a rebase job. Check the current status here,"Successfully rebased `upstream_fsspec` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout upstream_fsspec && git pull rebase`)", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `upstream_fsspec` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout upstream_fsspec && git pull rebase`)", merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
647,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PTD][Checkpoint] Enable single_file_per_rank for fsspec storage read/write)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch With our current setup, single_file_per_rank is not supported for fsspec StorageWriter and StorageReader. This means we can only write single file per tensor/blob, which will significantly affect our performance.  We need to support single file per rank in fsspec and add the option back.  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[PTD][Checkpoint] Enable single_file_per_rank for fsspec storage read/write," ğŸš€ The feature, motivation and pitch With our current setup, single_file_per_rank is not supported for fsspec StorageWriter and StorageReader. This means we can only write single file per tensor/blob, which will significantly affect our performance.  We need to support single file per rank in fsspec and add the option back.  Alternatives _No response_  Additional context _No response_ ",2023-04-05T05:56:34Z,oncall: distributed triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/98386,Closing as this has been enabled. https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/_fsspec_filesystem.pyL320
305,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( [Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA )ï¼Œ å†…å®¹æ˜¯ (2.0.1 cherry pick )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer, [Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA ,2.0.1 cherry pick ,2023-04-05T02:02:58Z,module: cpu module: mkldnn module: amp (automated mixed precision) NNC release notes: quantization release notes: releng ciflow/mps module: inductor module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/98377
703,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Change daily aggregates upload job to use sum and occurence counter instead of averages)ï¼Œ å†…å®¹æ˜¯ (  CC(Change daily aggregates upload job to use sum and occurence counter instead of averages) We used to keep track of the average of stats, however, when we munge the data to find interesting insights this makes things difficult (ie. finding total test time for an oncall). The pin is updated such that we keep track of the sum instead as well as an ""occurrences"" field such that the average can be rederived from sum/occurrences. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Change daily aggregates upload job to use sum and occurence counter instead of averages,"  CC(Change daily aggregates upload job to use sum and occurence counter instead of averages) We used to keep track of the average of stats, however, when we munge the data to find interesting insights this makes things difficult (ie. finding total test time for an oncall). The pin is updated such that we keep track of the sum instead as well as an ""occurrences"" field such that the average can be rederived from sum/occurrences. ",2023-04-04T22:42:24Z,Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/98359, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
481,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Implement `is_causal` API for `Transformer`)ï¼Œ å†…å®¹æ˜¯ (Adjust documentation according to CC([Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA) and implement `is_causal` API for `Transformer`. As discussed in CC(Add `is_causal` API for `TransformerDecoder`).)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Implement `is_causal` API for `Transformer`,Adjust documentation according to CC([Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA) and implement `is_causal` API for `Transformer`. As discussed in CC(Add `is_causal` API for `TransformerDecoder`).,2023-04-04T17:18:46Z,triaged open source Stale,closed,0,13,https://github.com/pytorch/pytorch/issues/98327,"Nice!  Should we add detection of causal mask similar to https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.pyL302 in TransformerEncoder  to TransformerDecoder and Transformer as well? Also, should we replace https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.pyL305 with a call to Transformer. generate_square_subsequent_mask (and maybe give that a dtype argument that defaults to torch.float, too?"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," to fix the build fails, please apply this diff  should remove the text commented out, which I did to highlight the change.  Notice that this may change when adding the is_causal mask compute check for nn.Transformer* as discussed above. ",Rebased on `main` and applied your suggestions. I took the liberty to include some other minor changes like minor refactoring and documentation adjustments (some arguments were in the wrong order). ~~I don't manage to build the current head for testing; do I still need to apply your patch even though I implemented the causal mask detection?~~ I did not apply your patch and instead also added a failure case for `TransformerDecoder`. Hope this is correct.,"Actually nevermind, it seems you somehow ended up on a wrong link to the commits or something. I have no idea. Anyway, this link with the same commit hash works: `https://github.com/pytorch/pytorch/pull/98327/commits/ed37254f86558585f3df2dd8a217435dfbf72272` (GitHub shortens this to the incorrect link if I don't escape it.)","Looks great.  The fails seem to be here, in line 3792 of test_nn. You can presumably make it pass by switching raises=False.  But wiould also be interesting to know why did it change?  One possibility might be that if we are passing a causal mask in, then the flag is_causal=True can preempt the mask from being used, and hence not be diagnosed of being of the wrong size?  (The description is very clear  the is_causal flag can preempt inspection of the attention mask.  We lose the performance benefit that the is_causal boolean promises if we're bound to using the attention mask regardless! So, if the is_causal preempts the recognizing of the mask being wrong then in  that case, this is the ""expected"" behavior, in that is_causal can preempt inspection/use of the mask itself.)","> One possibility might be that if we are passing a causal mask in, then the flag is_causal=True can preempt the mask from being used, and hence not be diagnosed of being of the wrong size? (The description is very clear  the is_causal flag can preempt inspection of the attention mask. We lose the performance benefit that the is_causal boolean promises if we're bound to using the attention mask regardless! So, if the is_causal preempts the recognizing of the mask being wrong then in that case, this is the ""expected"" behavior, in that is_causal can preempt inspection/use of the mask itself.) Ahh really great point, that seems very likely. So should I rather hardcode `is_causal=False` for these test instead of changing it to `raises=True` and hoping that the current behavior of the mask shape being ignored stays?","(1) Yeah, letâ€™s try that.  ie.e, is_causal = False (2) Another way to fix this ay be to disable all but the sdpa_math() kernel for this test using the sdpa backend context manager since the sdpa_math() kernel uses the attention mask and igniresthe other operators.   (3) If not, Iâ€™m not above just fixing up the test to not raise an exception â€” although that noexception behavior really depends on the hardware this runs, since it requires xformers or flash which both only run on more recent hardware. In effect (1) and (2) both force the use od sdpa_math  by setting is_causal=False (today sda_flash and mem_efficient onoy support is_causal=True), or by disabling those kernels outright using the sdpa backend contex mgr.","I think this is good now, PTAL. :)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","> I think this is good now, PTAL. :) Thanks.  This look good.  I've started to split the PR into several pieces, to make review and landing easier as we have a lot of changes going on in this part of PyTorch.  The doc updates are about to land with https://github.com/pytorch/pytorch/pull/101089 and we'll have additional pieces landing in the future! Thanks for putting these changes together, they make the code much more readable, consistent and better structured!","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", should I rebase this onto `main` to remove the refactoring part of this? Or do you already have other pieces of this ready so a rebase from my side would be redundant?
456,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove no-op Storage::use_byte_size_t)ï¼Œ å†…å®¹æ˜¯ (  CC(remove noop Storage::use_byte_size_t)  CC(remove noop StorageImpl::use_byte_size_t) Differential Revision: D44667578 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove no-op Storage::use_byte_size_t,"  CC(remove noop Storage::use_byte_size_t)  CC(remove noop StorageImpl::use_byte_size_t) Differential Revision: D44667578 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!",2023-04-04T13:40:52Z,open source Stale release notes: quantization topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/98302,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
347,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove no-op StorageImpl::use_byte_size_t)ï¼Œ å†…å®¹æ˜¯ (  CC(remove noop Storage::use_byte_size_t)  CC(remove noop StorageImpl::use_byte_size_t) Differential Revision: D44665575 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove no-op StorageImpl::use_byte_size_t,  CC(remove noop Storage::use_byte_size_t)  CC(remove noop StorageImpl::use_byte_size_t) Differential Revision: D44665575 ,2023-04-04T13:40:47Z,module: cpu module: mkldnn open source Stale release notes: quantization topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/98301,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove no-op StorageImpl::use_byte_size_t)ï¼Œ å†…å®¹æ˜¯ (  CC(remove noop StorageImpl::use_byte_size_t)  CC(remove unused StorageImpl::use_byte_size_t) Differential Revision: D44667578 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove no-op StorageImpl::use_byte_size_t,"  CC(remove noop StorageImpl::use_byte_size_t)  CC(remove unused StorageImpl::use_byte_size_t) Differential Revision: D44667578 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!",2023-04-04T13:37:12Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/98298
354,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove unused StorageImpl::use_byte_size_t)ï¼Œ å†…å®¹æ˜¯ (  CC(remove noop StorageImpl::use_byte_size_t)  CC(remove unused StorageImpl::use_byte_size_t) Differential Revision: D44665575 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove unused StorageImpl::use_byte_size_t,  CC(remove noop StorageImpl::use_byte_size_t)  CC(remove unused StorageImpl::use_byte_size_t) Differential Revision: D44665575 ,2023-04-04T12:01:00Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/98293
1430,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make BetterTransformer implementation non-blocking)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch I am using `optimum` integration for `BetterTransformer` with AMP. Here is what I get without `BetterTransformer`: !Screenshot from 20230404 124646 !Screenshot from 20230404 124400 The key point here is `_process_doc_contents` is a CPUintensive preprocessing function and `repad` is GPUblocking. Notice how we seemingly spend no time in `transformers` code. Now after I introduce `BetterTransformer` I get the following picture: !Screenshot from 20230404 124650 !Screenshot from 20230404 125158 Suddenly, we start spending a lot of time in `transformers` code, which tells me some operation is GPUblocking there. Furthermore, my guess is confirmed by the drop in GPU saturation and increase in total time by ~30s (the time needed to preprocess the inputs). Why is this issue not in `transformers` repo? As far as I can tell from the profiler's output, the execution was blocked by this function: !image **Software:** Ubuntu 18.04.6 LTS  **Hardware:** NVIDIA GeForce RTX 2060  Sorry for not providing a reproducible example. I will work on it later since for now I am not sure how to implement it.  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Make BetterTransformer implementation non-blocking," ğŸš€ The feature, motivation and pitch I am using `optimum` integration for `BetterTransformer` with AMP. Here is what I get without `BetterTransformer`: !Screenshot from 20230404 124646 !Screenshot from 20230404 124400 The key point here is `_process_doc_contents` is a CPUintensive preprocessing function and `repad` is GPUblocking. Notice how we seemingly spend no time in `transformers` code. Now after I introduce `BetterTransformer` I get the following picture: !Screenshot from 20230404 124650 !Screenshot from 20230404 125158 Suddenly, we start spending a lot of time in `transformers` code, which tells me some operation is GPUblocking there. Furthermore, my guess is confirmed by the drop in GPU saturation and increase in total time by ~30s (the time needed to preprocess the inputs). Why is this issue not in `transformers` repo? As far as I can tell from the profiler's output, the execution was blocked by this function: !image **Software:** Ubuntu 18.04.6 LTS  **Hardware:** NVIDIA GeForce RTX 2060  Sorry for not providing a reproducible example. I will work on it later since for now I am not sure how to implement it.  Alternatives _No response_  Additional context _No response_ ",2023-04-04T10:39:13Z,module: nn triaged,open,1,0,https://github.com/pytorch/pytorch/issues/98291
1990,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.angle and torch.atan2 are producing unexpected gradients.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug ``torch.angle`` can produce NaN gradients for inputs that are close to (0, 0). ``torch.atan2`` produces NaN gradient when the input is exactly (0, 0)  In contrast, ``torch.atan2`` is able to produce the correct gradient.  However, when the input is exactly (0, 0), ``torch.atan2`` can produce NaN gradients (because of divided by zero issue in the backward function)  I found this piece of function in the source code,  https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/csrc/autograd/FunctionsManual.cppL493 it turns out that when a complex number is divided by a very small real number, the result will become inf. This is likely a numerical precision issue which causes the NaN gradient of ``torch.angle``.  On the other hand, the backward of ``torch.atan2`` computes the gradient of the real and imag part separately, and there seems to be no numerical precision issue https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/csrc/autograd/FunctionsManual.cppL2960 I'm not sure if this should be considered a bug, but maybe ``torch.angle`` and ``torch.atan2`` should have consistent behaviors. Also, since both ``torch.angle`` and ``torch.atan2`` produce correct forward result, the NaN gradient issue should at least be documented so that users know about this.  Versions PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platfor)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,torch.angle and torch.atan2 are producing unexpected gradients.," ğŸ› Describe the bug ``torch.angle`` can produce NaN gradients for inputs that are close to (0, 0). ``torch.atan2`` produces NaN gradient when the input is exactly (0, 0)  In contrast, ``torch.atan2`` is able to produce the correct gradient.  However, when the input is exactly (0, 0), ``torch.atan2`` can produce NaN gradients (because of divided by zero issue in the backward function)  I found this piece of function in the source code,  https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/csrc/autograd/FunctionsManual.cppL493 it turns out that when a complex number is divided by a very small real number, the result will become inf. This is likely a numerical precision issue which causes the NaN gradient of ``torch.angle``.  On the other hand, the backward of ``torch.atan2`` computes the gradient of the real and imag part separately, and there seems to be no numerical precision issue https://github.com/pytorch/pytorch/blob/c263bd43e8e8502d4726643bc6fd046f0130ac0e/torch/csrc/autograd/FunctionsManual.cppL2960 I'm not sure if this should be considered a bug, but maybe ``torch.angle`` and ``torch.atan2`` should have consistent behaviors. Also, since both ``torch.angle`` and ``torch.atan2`` produce correct forward result, the NaN gradient issue should at least be documented so that users know about this.  Versions PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platfor",2023-04-04T03:48:09Z,module: autograd triaged module: complex module: NaNs and Infs,closed,0,3,https://github.com/pytorch/pytorch/issues/98276,The torch.angle() issue has been fixed in latest version. Would you update to nightly build and confirm?,Yes. The nightly build seems to fix the ``torch.angle`` issue.,"`arctan2(0,0) = arctan(0 / 0)`, which is undefined. As such, its gradients are also undefined. See point number 5 in https://pytorch.org/docs/stable/notes/autograd.htmlgradientsfornondifferentiablefunctions. Closing as both issues seem either fixed or expected. Feel free to reopen if you think otherwise"
665,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] [CPU] Huggingface model BartForCausalLM & MBartForCausalLM & OPTForCausalLM & PLBartForCausalLM performance regression > 10% on 2023-04-02 nightly release)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Compare with the 20230329, there is a performance regression on huggingface model**BartForCausalLM & MBartForCausalLM & OPTForCausalLM & PLBartForCausalLM** on TorchInductor CPU Performance Dashboardissuecomment1495275117) on 20230402 as bellow: / Graph dump by cosim:  Versions Minified repro:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[Inductor] [CPU] Huggingface model BartForCausalLM & MBartForCausalLM & OPTForCausalLM & PLBartForCausalLM performance regression > 10% on 2023-04-02 nightly release," ğŸ› Describe the bug Compare with the 20230329, there is a performance regression on huggingface model**BartForCausalLM & MBartForCausalLM & OPTForCausalLM & PLBartForCausalLM** on TorchInductor CPU Performance Dashboardissuecomment1495275117) on 20230402 as bellow: / Graph dump by cosim:  Versions Minified repro:  ",2023-04-04T03:36:53Z,triaged oncall: pt2 module: inductor oncall: cpu inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/98273,"fangintel double checked with latest reports, those regression issue still exist.",Thanks .   Could you help to narrow down the PR which cause the regression and check for how to fix?,These four models have the same suspected guilty commit: https://github.com/pytorch/pytorch/commit/6b319d152525c002d8cf87a2a1845c0599c1530a huggingfaceBartForCausalLMinferencefloat32staticdefaultperformancesingledrop_guilty_commit.log,"The guilty commit(https://github.com/pytorch/pytorch/commit/6b319d152525c002d8cf87a2a1845c0599c1530a) fixed a type of the frequent graph breaks in HF models, resulting in a significant reduction in the number of graph breaks in HF models, but the codegen kernels generated by Inductor based on the more complete FX graph are not well vectorized, which leads to performance regression. But the latest PyTorch no longer has this problem, and there is no obvious regression anymore. The following are the benchmark results on 2024_03_04. Compare with the benchmark results on 20230329, the performance regression on the four huggingface models (BartForCausalLM & MBartForCausalLM & OPTForCausalLM & PLBartForCausalLM) are both less than 10%. model   1.23434455484095",Close this issue as it's close to the data before regression.
417,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NotImplementedError: UserDefinedObjectVariable(_UnionGenericAlias) is not a constant (CM3Leon))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The CM3Leon team has the following model:     When run as is, it fails with:  This should not happen.  Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,NotImplementedError: UserDefinedObjectVariable(_UnionGenericAlias) is not a constant (CM3Leon)," ğŸ› Describe the bug The CM3Leon team has the following model:     When run as is, it fails with:  This should not happen.  Versions master ",2023-04-04T02:14:54Z,oncall: pt2 module: dynamo,closed,0,1,https://github.com/pytorch/pytorch/issues/98265,I just sent CC([Dynamo] Support typing.Union and typing.Optional) and verified it can fix this.
1578,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Better Handling of Storage Cache)ï¼Œ å†…å®¹æ˜¯ (  CC(Flip Switch Redux)  CC(Better Handling of Storage Cache)  CC(Make sure we dealloc on recording, not just replay)  CC(Account for forwards which whose corresponding backwards are not invoked)  CC(Increment pending forwards after invocation)  CC(Deduplicate pointers to manually free) Because we do not persist output memory of cudagraphs, we need to reconstruct tensors at their correct memory locations after we've done a run. We were using a storage cache for that but it had a couple of issues:  If the a data ptr existed in the cache, we should only reuse the corresponding storage if the storage hadn't died  didnt work across separate nodes. While you wouldn't think this would be an issue, it was in testing HF.  StorageWeakRef maintains whether the Storage C++ object remains allocated, not whether the corresponding memory has been deallocated. In order to use them to track memory deallocations we must maintain a single StorageWeakRef for all Storages that reference that memory (even if we are constructing Storages that do not have a deallocator function).  This PR  a singlestorage_cache as we execute any tree path. When we retrieve a storage from the cache we check that it is still alive, and we hash based on both observed recording data ptr and storageimpl weak ref.  Update to use a single storage cache across all executions in a path.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Better Handling of Storage Cache,"  CC(Flip Switch Redux)  CC(Better Handling of Storage Cache)  CC(Make sure we dealloc on recording, not just replay)  CC(Account for forwards which whose corresponding backwards are not invoked)  CC(Increment pending forwards after invocation)  CC(Deduplicate pointers to manually free) Because we do not persist output memory of cudagraphs, we need to reconstruct tensors at their correct memory locations after we've done a run. We were using a storage cache for that but it had a couple of issues:  If the a data ptr existed in the cache, we should only reuse the corresponding storage if the storage hadn't died  didnt work across separate nodes. While you wouldn't think this would be an issue, it was in testing HF.  StorageWeakRef maintains whether the Storage C++ object remains allocated, not whether the corresponding memory has been deallocated. In order to use them to track memory deallocations we must maintain a single StorageWeakRef for all Storages that reference that memory (even if we are constructing Storages that do not have a deallocator function).  This PR  a singlestorage_cache as we execute any tree path. When we retrieve a storage from the cache we check that it is still alive, and we hash based on both observed recording data ptr and storageimpl weak ref.  Update to use a single storage cache across all executions in a path.  ",2023-04-03T22:29:32Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/98254, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
446,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove typed StorageImpl::data() and StorageImpl::unsafe_data())ï¼Œ å†…å®¹æ˜¯ (  CC(remove typed StorageImpl::data() and StorageImpl::unsafe_data())  CC(remove typed StorageImpl::unsafe_data()) Typed data will now only be a tensor level concept. Differential Revision: D44629941)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove typed StorageImpl::data() and StorageImpl::unsafe_data(),  CC(remove typed StorageImpl::data() and StorageImpl::unsafe_data())  CC(remove typed StorageImpl::unsafe_data()) Typed data will now only be a tensor level concept. Differential Revision: D44629941,2023-04-03T15:11:30Z,Merged Reverted ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/98219, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m=""Diff reverted internally"" c=""ghfirst"" This Pull Request has been reverted by a revert inside Meta. To reland this change, please open another pull request, assign the same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).)", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
422,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove typed StorageImpl::unsafe_data())ï¼Œ å†…å®¹æ˜¯ (  CC(remove typed StorageImpl::data() and StorageImpl::unsafe_data())  CC(remove typed StorageImpl::unsafe_data()) Typed data will now only be a tensor level concept. Differential Revision: D44629939)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove typed StorageImpl::unsafe_data(),  CC(remove typed StorageImpl::data() and StorageImpl::unsafe_data())  CC(remove typed StorageImpl::unsafe_data()) Typed data will now only be a tensor level concept. Differential Revision: D44629939,2023-04-03T15:11:24Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/98218, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1996,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AssertionError: was expecting embedding dimension of 22, but got 1320)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am new to pytorch on Colab. I used Transformer encoder as feature extractor to build DQN. The input shape of DQN is (60,22), and the output is one of the three numbers of 0,1,2. model.learn completed successfully. But model.predict is wrong. How can I change it? code: class TransformerFeaturesExtractor(BaseFeaturesExtractor):     def __init__(self, observation_space, features_dim=128):         super(TransformerFeaturesExtractor, self).__init__(observation_space, features_dim)         self.transformer_encoder = nn.TransformerEncoder(             nn.TransformerEncoderLayer(d_model=features_dim, nhead=2), num_layers=6         )         self.flatten = nn.Flatten()     def forward(self, observations):         x = self.flatten(observations)         x = self.transformer_encoder(x.unsqueeze(0))         return x.squeeze(0) policy_kwargs = dict(     features_extractor_class=TransformerFeaturesExtractor,     features_extractor_kwargs=dict(features_dim=22), , action_space=env.action_space), )  Create an instance of the DQN agent model = DQN(""MlpPolicy"", env,policy_kwargs=policy_kwargs, verbose=1) model = DQN('MlpPolicy', stock_trade_env, verbose=1) model = PPO('MlpPolicy', env, verbose=1)  Train the agent  pdb.set_trace() model.learn(total_timesteps=10_000, progress_bar=True) obs = env.reset() for i in range(100):     action, _states = model.predict(obs)     action = env.action_space.sample()     obs, rewards, dones, info = env.step(action)     if dones:         break     env.render()  Versions  AssertionError                            Traceback (most recent call last)  in ()     365 obs = env.reset()     366 for i in range(100): > 367     action, _states = model.predict(obs)     368     action = env.ac)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"AssertionError: was expecting embedding dimension of 22, but got 1320"," ğŸ› Describe the bug I am new to pytorch on Colab. I used Transformer encoder as feature extractor to build DQN. The input shape of DQN is (60,22), and the output is one of the three numbers of 0,1,2. model.learn completed successfully. But model.predict is wrong. How can I change it? code: class TransformerFeaturesExtractor(BaseFeaturesExtractor):     def __init__(self, observation_space, features_dim=128):         super(TransformerFeaturesExtractor, self).__init__(observation_space, features_dim)         self.transformer_encoder = nn.TransformerEncoder(             nn.TransformerEncoderLayer(d_model=features_dim, nhead=2), num_layers=6         )         self.flatten = nn.Flatten()     def forward(self, observations):         x = self.flatten(observations)         x = self.transformer_encoder(x.unsqueeze(0))         return x.squeeze(0) policy_kwargs = dict(     features_extractor_class=TransformerFeaturesExtractor,     features_extractor_kwargs=dict(features_dim=22), , action_space=env.action_space), )  Create an instance of the DQN agent model = DQN(""MlpPolicy"", env,policy_kwargs=policy_kwargs, verbose=1) model = DQN('MlpPolicy', stock_trade_env, verbose=1) model = PPO('MlpPolicy', env, verbose=1)  Train the agent  pdb.set_trace() model.learn(total_timesteps=10_000, progress_bar=True) obs = env.reset() for i in range(100):     action, _states = model.predict(obs)     action = env.action_space.sample()     obs, rewards, dones, info = env.step(action)     if dones:         break     env.render()  Versions  AssertionError                            Traceback (most recent call last)  in ()     365 obs = env.reset()     366 for i in range(100): > 367     action, _states = model.predict(obs)     368     action = env.ac",2023-04-03T08:48:21Z,module: nn triaged,open,0,3,https://github.com/pytorch/pytorch/issues/98203,"This could be because the `features_dim` parameter passed to the `TransformerFeaturesExtractor` class is set to 128, which is used as the `d_model` parameter for the `nn.TransformerEncoderLayer`. This means that each input observation is projected into a 128dimensional space before being fed into the transformer encoder. To resolve this issue, you may need to update the `features_dim` parameter to match the expected input shape of the DQN, which is (60, 22). One possible solution could be to set `features_dim` to 60*22=1320, so that the transformer encoder outputs a tensor with shape (60, 22, 128).","Thanks a lot.  I modified the code:  However, I still got the same problem. line 55 produced the error. model.learn(total_timesteps=10_000, progress_bar=True) could not run completely according the progess bar. !image Line 55 did always not produced the error, otherwise, model.learn could not run. Assertion Error indicated the error made by Line 367 which run after model.learn finished.  AssertionError                            Traceback (most recent call last) [](https://localhost:8080/) in ()     365 obs = env.reset()     366 for i in range(100): > 367     action, _states = model.predict(obs)     368     action = env.action_space.sample()     369     obs, rewards, dones, info = env.step(action) 16 frames /usr/local/lib/python3.9/distpackages/torch/nn/functional.py in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)    5166         attn_mask = None    5167  > 5168     assert embed_dim == embed_dim_to_check, \    5169         f""was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}""    5170     if isinstance(embed_dim, torch.Tensor): AssertionError: was expecting embedding dimension of 22, but got 1320","Thank you very much for your help. The input sequence has 60 elements, with each element having 22 dimensions. Shouldn't the features_dim parameter passed to the TransformerFeaturesExtractor class be set to 22? I made the change, but the same error still occurs. Line 55 in the code causes this error, but not consistently. This is because the forward function is also used during model.learn when training. Another strange thing is that model.learn did not complete 10,000 time steps. Then it proceeded to model.predict, which triggered the error. Why is this happening? [image: image.png] [image: image.png] On Tue, Apr 4, 2023 at 1:53â€¯PM Durgesh Patel ***@***.***> wrote: > This could be because the features_dim parameter passed to the > TransformerFeaturesExtractor class is set to 128, which is used as the > d_model parameter for the nn.TransformerEncoderLayer. This means that > each input observation is projected into a 128dimensional space before > being fed into the transformer encoder. > > To resolve this issue, you may need to update the features_dim parameter > to match the expected input shape of the DQN, which is (60, 22). One > possible solution could be to set features_dim to 60*22=1320, so that the > transformer encoder outputs a tensor with shape (60, 22, 128). > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >"
417,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add new() method identifier to _StorageBase)ï¼Œ å†…å®¹æ˜¯ (The method torch.UntypedStorage.new is not detailed in API docs. Adding a method identifier may make it easier to know that new() method is only implemented by cpp, like copy_() or nbytes().)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,add new() method identifier to _StorageBase,"The method torch.UntypedStorage.new is not detailed in API docs. Adding a method identifier may make it easier to know that new() method is only implemented by cpp, like copy_() or nbytes().",2023-04-03T08:42:20Z,open source Merged ciflow/trunk release notes: python_frontend topic: docs,closed,0,12,https://github.com/pytorch/pytorch/issues/98201, merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxbioniccuda11.8py3.10gcc7 / test (nogpu_AVX512, 1, 1, linux.2xlarge) Details for Dev Infra team Raised by workflow job ", merge r, successfully started a rebase job. Check the current status here,"Successfully rebased `master` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout master && git pull rebase`)", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
416,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add new method identifier to _StorageBase)ï¼Œ å†…å®¹æ˜¯ (The method torch.UntypedStorage.new is not detailed in API docs. Adding a method identifier may make it easier to know that new() method is only implemented by cpp, like copy_()  or nbytes().)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,add new method identifier to _StorageBase,"The method torch.UntypedStorage.new is not detailed in API docs. Adding a method identifier may make it easier to know that new() method is only implemented by cpp, like copy_()  or nbytes().",2023-04-03T07:48:33Z,open source,closed,0,1,https://github.com/pytorch/pytorch/issues/98197," :x:  login:  . The commit (1ac2548168c551bb180076eb44ef3926232afa71) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."
548,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TorchInductor failure with: `Unsupported: meta converter nyi with fake tensor propagation`)ï¼Œ å†…å®¹æ˜¯ (This test:  started failing after landing this PR https://github.com/pytorch/pytorch/pull/98155 I believe before that PR this error was hidden due to `suppress_errors` logic. I suspect we are handling `._neg_view()` incorrectly, possibly in functionalization. Repro with:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TorchInductor failure with: `Unsupported: meta converter nyi with fake tensor propagation`,"This test:  started failing after landing this PR https://github.com/pytorch/pytorch/pull/98155 I believe before that PR this error was hidden due to `suppress_errors` logic. I suspect we are handling `._neg_view()` incorrectly, possibly in functionalization. Repro with:   ",2023-04-02T20:25:50Z,triaged oncall: pt2 module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/98175,This test failure also related:  CC(DISABLED test_equal (__main__.TestTorch)),fails for other reasons now
815,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of Storage::unsafe_data)ï¼Œ å†…å®¹æ˜¯ (  CC(introduce TensorIterator::unsafe_replace_input())  CC(introduce TensorBase::mutable_data_ptr())  CC(introduce TensorBase::mutable_data_ptr)  CC(distinguish mutability of TensorImpl::data)  CC(distinguish mutability of TensorImpl::data())  CC(make TensorImpl::data_ptr_impl() nonconst and have mutable in the name)  CC(make TensorImpl::unsafe_data() return a const pointer)  CC(add mutable to name of nonconst Storage::data_ptr)  CC(distinguish mutability of untyped Storage::data)  CC(distinguish mutability of Storage::unsafe_data) See D44409928. Differential Revision: D44426197)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of Storage::unsafe_data,  CC(introduce TensorIterator::unsafe_replace_input())  CC(introduce TensorBase::mutable_data_ptr())  CC(introduce TensorBase::mutable_data_ptr)  CC(distinguish mutability of TensorImpl::data)  CC(distinguish mutability of TensorImpl::data())  CC(make TensorImpl::data_ptr_impl() nonconst and have mutable in the name)  CC(make TensorImpl::unsafe_data() return a const pointer)  CC(add mutable to name of nonconst Storage::data_ptr)  CC(distinguish mutability of untyped Storage::data)  CC(distinguish mutability of Storage::unsafe_data) See D44409928. Differential Revision: D44426197,2023-04-01T21:40:23Z,topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/98152,"> It seems to me that more accessors on Storage should be added first. >  > I also think the ""unsafe"" nomenclature on the storage data ptr accessor should go. There is no way to actually check that the dtypes match; let's not force people to say unsafe when there isn't a safe alternative Yeah I agree it's not ideal. But it looks like, at least for the nonconst pointers, it's only ever used with uint8 or char as the types. char is definitely safe, uint8_t is not legal, but close enough and could probably be converted to std::byte. So what I would propose:  1) drop the unsafe variants  2) eschew the template in favor of accessors `bytes() > const std::byte*` and `mutable_bytes() > std::byte*`.  3) eventually merge the `data()` and `bytes()` accessors. I don't think we really need both. And then only have the safe/unsafe variants at the Tensor level, where we actually have enough information to distinguish them.",>  I take it back. Storage and StorageImpl should only have `data() > const char*` and `mutable_data() > char*`. There's no need to integrate interpreting them as any type into the API.,other changes are done and merged. abandoning this.
330,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of Storage::unsafe_data)ï¼Œ å†…å®¹æ˜¯ (  CC(distinguish mutability of Storage::unsafe_data) See D44409928. Differential Revision: D44426197)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of Storage::unsafe_data,  CC(distinguish mutability of Storage::unsafe_data) See D44409928. Differential Revision: D44426197,2023-04-01T21:15:14Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/98151
1971,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(T5 model taking too long with torch compile.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I have been trying to use the T5 model with the torch compile feature in PyTorch, but it seems to be taking an unreasonably long time to compile compared to using the eager mode. how to reproduce using the transformers examples:  go to this folder : transformers/examples/pytorch/translation/  **to run eager mode:**  `python run_translation.py     model_name_or_path t5base     do_train do_eval    source_lang en     target_lang de     source_prefix ""translate English to German: ""     dataset_name stas/wmt14endepreprocessed     output_dir /tmp/tsttranslation    num_train_epochs 1 per_device_train_batch_size=1     max_train_samples 1000     overwrite_output_dir     seed 1137     per_device_eval_batch_size 1     max_eval_samples 1000   fp16  predict_with_generate ` **to run torch compile:** `python run_translation.py     model_name_or_path t5base     do_train do_eval    source_lang en     target_lang de     source_prefix ""translate English to German: ""     dataset_name stas/wmt14endepreprocessed     output_dir /tmp/tsttranslation    num_train_epochs 1 per_device_train_batch_size=1     max_train_samples 1000     overwrite_output_dir     seed 1137     per_device_eval_batch_size 1     max_eval_samples 1000   fp16  predict_with_generate torch_compile  ` Results :   you can see that it took over 5hrs to run compared to only 2 mins for eager mode. The problem happens in all T5 versions, as well as other models we've tested so far like jeanbaptistecamembertner and Summarization/sshleifer/distilbartcnn126Â   Versions Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.24.1 [pip3] pytorchtriton==2.1.0+2c32f43999 [pip3] torch==2.1.0.dev20230313+cu117 [pip3] torchort==1.14.0 [pip3] torc)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,T5 model taking too long with torch compile.," ğŸ› Describe the bug I have been trying to use the T5 model with the torch compile feature in PyTorch, but it seems to be taking an unreasonably long time to compile compared to using the eager mode. how to reproduce using the transformers examples:  go to this folder : transformers/examples/pytorch/translation/  **to run eager mode:**  `python run_translation.py     model_name_or_path t5base     do_train do_eval    source_lang en     target_lang de     source_prefix ""translate English to German: ""     dataset_name stas/wmt14endepreprocessed     output_dir /tmp/tsttranslation    num_train_epochs 1 per_device_train_batch_size=1     max_train_samples 1000     overwrite_output_dir     seed 1137     per_device_eval_batch_size 1     max_eval_samples 1000   fp16  predict_with_generate ` **to run torch compile:** `python run_translation.py     model_name_or_path t5base     do_train do_eval    source_lang en     target_lang de     source_prefix ""translate English to German: ""     dataset_name stas/wmt14endepreprocessed     output_dir /tmp/tsttranslation    num_train_epochs 1 per_device_train_batch_size=1     max_train_samples 1000     overwrite_output_dir     seed 1137     per_device_eval_batch_size 1     max_eval_samples 1000   fp16  predict_with_generate torch_compile  ` Results :   you can see that it took over 5hrs to run compared to only 2 mins for eager mode. The problem happens in all T5 versions, as well as other models we've tested so far like jeanbaptistecamembertner and Summarization/sshleifer/distilbartcnn126Â   Versions Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.24.1 [pip3] pytorchtriton==2.1.0+2c32f43999 [pip3] torch==2.1.0.dev20230313+cu117 [pip3] torchort==1.14.0 [pip3] torc",2023-03-31T22:49:16Z,triaged oncall: pt2 module: dynamic shapes module: inductor module: dynamo,closed,0,30,https://github.com/pytorch/pytorch/issues/98102,Can you print and share ?,"At a guess, you need to use dynamic=True.","> torch._dynamo.utils.compilation_metrics that's the output :  OrderedDict([('_compile', [28.510122776031494, 0.006930112838745117, 0.002085447311401367, 0.0017774105072021484]), ('OutputGraph.call_user_compiler', [23.453813552856445]), ('create_aot_dispatcher_function', [20.985525846481323]), ('compile_fx..fw_compiler', [6.880029678344727]), ('GraphLowering.run', [1.3998785018920898, 1.8204922676086426]), ('GraphLowering.compile_to_module', [4.990485668182373, 5.290128231048584]), ('Scheduler.__init__', [2.3708276748657227, 2.292379140853882]), ('Scheduler.codegen', [1.463493824005127, 1.810530424118042]), ('WrapperCodeGen.generate', [0.19363665580749512, 0.2001209259033203]), ('compile_fx..bw_compiler', [7.222400903701782])]) > At a guess, you need to use dynamic=True. I modified the trainer.py in transformers torch.compile and added Dynamic = True as a parameter, but it will fail with a different error,  ","> that's the output : OrderedDict([('_compile', [28.510122776031494, 0.006930112838745117, 0.002085447311401367, 0.0017774105072021484]), ('OutputGraph.call_user_compiler', [23.453813552856445]), ('create_aot_dispatcher_function', [20.985525846481323]), ('compile_fx..fw_compiler', [6.880029678344727]), ('GraphLowering.run', [1.3998785018920898, 1.8204922676086426]), ('GraphLowering.compile_to_module', [4.990485668182373, 5.290128231048584]), ('Scheduler.init', [2.3708276748657227, 2.292379140853882]), ('Scheduler.codegen', [1.463493824005127, 1.810530424118042]), ('WrapperCodeGen.generate', [0.19363665580749512, 0.2001209259033203]), ('compile_fx..bw_compiler', [7.222400903701782])]) Nothing looks suspicious and indicates where takes 5 hours. Can you print out  and check if there is abnormal recompilation?"," has also reported that the real world T5 models in HF do not work (e.g., they need dynamic shapes and recompile a lot.) I think someone just needs to sit down and actually try torch.compile on one of the real examples","I patched this in:  and then ran the example. It doesn't appear to error, but it does not seem to have improved perf:  I don't have loss as it failed due to some other problem "," Looks like it got `self.model` to be wrapped in `DataParallel` which doesn't know anything about the model's real attributes. I think somewhere some wrong assumption is made, could you please rerun with `CUDA_VISIBLE_DEVICES=0` which would not trigger `DataParallel` wrapping. edit: I can reproduce the bug  `CUDA_VISIBLE_DEVICES=0` does overcome this, but then it fails again in eval. I meanwhile filed a bug report https://github.com/huggingface/transformers/issues/22571 Until this is sorted out, it's probably good enough to just run the training and will deal with eval later. This I tested to work: ","Once I apply 's change to make it stop using DataParallel, I also see that the script now takes a very long time to run, even with dynamic=True."," can you confirm that a more recent nightly + dynamic=True advances you past the ""NYI"" error, and gets you slow compilation again? I think this makes sense to me, that we fixed the NYI.",Metrics from the slow run: ,", sorry, I changed the cmd line args while I was testing so it was quick to run and forgot to change those back to the original  I reduced `max_train_samples 10`, I think to match the original you need `max_train_samples 1000`, so: ",", your 2 entries to compare in the OP aren't matching on their common parts: to run eager mode: python run_translation.py     model_name_or_path t5base     do_train do_eval    source_lang en     target_lang de     source_prefix ""translate English to German: ""     dataset_name stas/wmt14endepreprocessed     output_dir /tmp/tsttranslation    num_train_epochs 1 per_device_train_batch_size=1     max_train_samples 1000     overwrite_output_dir     seed 1137     per_device_eval_batch_size 1     max_train_samples 1000     predict_with_generate fp16   to run torch compile: python run_translation.py     model_name_or_path t5base     do_train do_eval    source_lang en     target_lang de     source_prefix ""translate English to German: ""     dataset_name stas/wmt14endepreprocessed     output_dir /tmp/tsttranslation    num_train_epochs 1 per_device_train_batch_size=1     max_train_samples 1000     overwrite_output_dir     seed 1137     per_device_eval_batch_size 1     max_eval_samples 1000   fp16  predict_with_generate torch_compile   in the first one you mistakengly used `max_train_samples 1000` twice. The second time it probably had to be `max_eval_samples 1000`  so that apples to apples are compared.",">  Hey, I ran it using the latest nightly version, and these are the results ***** train metrics *****   epoch                    =        1.0   train_loss               =     0.7757   train_runtime            = 0:29:06.93   train_samples            =         10   train_samples_per_second =      0.006   train_steps_per_second   =      0.006 note that you have to add torch_compile in your example.",">  correct, its supposed to be eval steps. ","> correct, its supposed to be eval steps. Then it might be a good idea to fix the OP, no? Others won't be able to repro your report w/o it.","With a few local changes (need https://github.com/pytorch/pytorch/pull/98347 and https://github.com/pytorch/pytorch/pull/98223 and https://github.com/pytorch/pytorch/pull/98350), here is a log from dynamic=True: https://gist.github.com/ezyang/25be1fc14312c34fdfcddad895225877 Repro with  Among other interesting things,  `L['input_ids'].size()[1]` keeps getting new guards issued for it:  ","> > correct, its supposed to be eval steps. >  > Then it might be a good idea to fix the OP, no? Others won't be able to repro your report w/o it.   I just did thank you,  how long did it took to finish? did it resolve the perf issue? ","   It is better, but still not great. I think the problem now is that Inductor is specializing too aggressively and we need to make it generate more generic kernels. The first thing to do is kill the modulus of 16 specialization ()","tbh, I don't think we can kill modulus 16 specialization without serious perf consequences. ","The guards above look like persistent reduction guards, so probably we should try disabling persistent reduction (there's a config for that)?","Disabling persistent reduction helps, but now we're playing pinochle with the modulus 16 guards: Run one, we have `Eq(Mod(L['decoder_input_ids'].size()[1], 16), 0)` and `Eq(Mod(L['input_ids'].size()[1], 16), 0)` Run two, we drop the decoder_input_ids guard. Run three, we instead drop the input_ids guard. Run four, we duck size `L['decoder_input_ids'].size()[1] == L['input_ids'].size()[1]` Run five, we drop both guards","Well, that helped a lot!  This is with  It's still not faster though... EDIT: After compilation, actually, it's probably a little faster; on eager I see 78 iter/s, whereas I was getting 11 iter/s on the compiled model, but it takes so long to compile that we're unable to amortize the compilation cost overall.",we need padding to multiple of 16,"With 's updated trainer, here are the avg of 2nd half results with persistent reductions and divisible by 16 annotations turned off: With torch.compile: Avg of 2nd half: 50.84254 ms Without torch.compile: Avg of 2nd half: 96.06191 ms So that's now a nearly 2x speedup.","Interesting negative result: turning divisible by 16 back on quadruples compile time, and makes perf worse: Avg of 2nd half: 57.60326 ms","> Interesting negative result: turning divisible by 16 back on quadruples compile time, and makes perf worse: Avg of 2nd half: 57.60326 ms how can I reproduce this? ","I finished landing all the fixes to master just now. If you take a master build f98c1809a42f49d64849f66ea0abc939377b96f7 or later, you should be able to run your experiment as is (using your modified trainer script, and with dynamic=True on torch.compile) without any other changes. I am running  on this version and I will double check that this reproduces. To reproduce the negative result, revert bdb79a8f5254bdbe4dcc08d74548d6b8e9926405 and run the same command as before. EDIT: You probably also want `skip_nnmodule_hook_guards = True` EDIT 2: Master gives me `Avg of 2nd half: 54.24770 ms`. I notice there are now a few irrelevant modulus guards which may account for some of the regression: "," noob question, where do I change skip_nnmodule_hook_guards = True ?",`torch._dynamo.config.skip_nnmodule_hook_guards=True` Note: it landed as defaulting to true as of last friday.,Closing due to inactivity. Please reopen if issue still exists.
513,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] hf_Reformer's graph break has increased)ï¼Œ å†…å®¹æ˜¯ (https://github.com/pytorch/pytorch/pull/98003 updated the torchbench version that CI uses. While it does pick up a transformers upstream change which fixes graph break in Bert, it increase hf_Reformer's graph break from 5 to 26 for inference and from 51 to 67 for training.  cc:   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[dynamo] hf_Reformer's graph break has increased,"https://github.com/pytorch/pytorch/pull/98003 updated the torchbench version that CI uses. While it does pick up a transformers upstream change which fixes graph break in Bert, it increase hf_Reformer's graph break from 5 to 26 for inference and from 51 to 67 for training.  cc:   ",2023-03-31T17:12:00Z,low priority triaged oncall: pt2 module: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/98087,I can take this. ,"Turning this low priority because hf_Reformer has manual_seed sprinkled all over the model. I also think this will be well covered in AOT export workstream. So, making this low priority.",Close since the stats has improved
354,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo] Graph break in Huggingface model Vilt)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Logs   Versions torch at 2f86c9bc0b0281074400548283461e241cac1a35 transformers                  4.25.1 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Dynamo] Graph break in Huggingface model Vilt, ğŸ› Describe the bug  Logs   Versions torch at 2f86c9bc0b0281074400548283461e241cac1a35 transformers                  4.25.1 ,2023-03-31T00:47:41Z,high priority triaged oncall: pt2 module: dynamo,closed,0,6,https://github.com/pytorch/pytorch/issues/98045,"I found several similar issues in 14k github models, it's probably because we don't handle the meta functions correctly for a few ops.", CC(python_arg_parser to allow fake tensor element in symint_list when in dynamo mode  95424) probably can fix this.,I had another repro:  Error: ," thanks for quick follow up! I tried https://github.com/pytorch/pytorch/pull/97508 locally, the issue persists though. I guess it probably does unblock the `tuple of FakeTensor` use case. What's left blocking is probably the last argument `NoneType` ","Yes, then I think this function may don't have a correct meta registration. Let me take a closer look.","The original issue is fixed. The new issue is this, which given the complexity should be handled in  a separate issue. ~~~ torch._dynamo.exc.UserError: Tried to use datadependent value in the subsequent computation. This can happen when we encounter unbounded dynamic value that is unknown during tracing time.You will need to explicitly give hint to the compiler. Please take a look at constrain_as_value OR constrain_as_size APIs ~~~ I am closing this issue for now. Please feel free to reopen if needed."
692,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([CI] Bump up torchbench version to fix dynamo graph breaks in transformers)ï¼Œ å†…å®¹æ˜¯ (  CC([CI] Bump up torchbench version to fix dynamo graph breaks in transformers) Summary: When we bump up the torchbench version pin last time, we found there were new graph breaks introduced with the trasformers version upgrade, see https://github.com/pytorch/pytorch/pull/96782. Turns out they are already fixed upstream, see https://github.com/huggingface/transformers/pull/21648 and https://github.com/pytorch/benchmark/pull/1511 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[CI] Bump up torchbench version to fix dynamo graph breaks in transformers,"  CC([CI] Bump up torchbench version to fix dynamo graph breaks in transformers) Summary: When we bump up the torchbench version pin last time, we found there were new graph breaks introduced with the trasformers version upgrade, see https://github.com/pytorch/pytorch/pull/96782. Turns out they are already fixed upstream, see https://github.com/huggingface/transformers/pull/21648 and https://github.com/pytorch/benchmark/pull/1511 ",2023-03-30T19:06:35Z,Merged ciflow/trunk release notes: releng ciflow/periodic module: dynamo ciflow/inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/98003," , a few new models failed with `ImportError: 'NeighborSampler' requires either 'pyglib' or 'torchsparse'`. Should those lib dependency be added to torchbench?",">  , a few new models failed with `ImportError: 'NeighborSampler' requires either 'pyglib' or 'torchsparse'`. Should those lib dependency be added to torchbench?   Yes, they are the new models added by Intel in https://github.com/pytorch/benchmark/pull/1422. They require users to install the pyglib and torchsparse packages located at https://data.pyg.org/whl/torch2.0.0+cpu.html, see: https://github.com/yanbingj/torchbench/blob/2d2ff6ebbb0f9c9a7b6c47e1b666b96685b23f9b/torchbenchmark/models/sage/install.py", merge g, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: inductor / cuda11.8py3.10gcc7sm86 / test (inductor_torchbench, 1, 1, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ",">  So basically `gat`, `gcn` and `sage` don't support CUDA yet."," merge f ""All the torchbench tests have passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
805,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile not compatible with multiprocessing pool)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Attempting to call `torch.compile()` within a multiprocessing pool results in:  I am using `transformers` `ViTModel` to encode images and cache them to disk inside my DataLoader transform. To improve performance, I was hoping to use `torch.compile()` since I perform this preprocessing transform on CPU. However, compiling in my transform's `__init__` runs into pickling issues when the DataLoader forks the workers. As for compiling after the workers have forked, I run into the issue above.  Error logs   Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile not compatible with multiprocessing pool," ğŸ› Describe the bug Attempting to call `torch.compile()` within a multiprocessing pool results in:  I am using `transformers` `ViTModel` to encode images and cache them to disk inside my DataLoader transform. To improve performance, I was hoping to use `torch.compile()` since I perform this preprocessing transform on CPU. However, compiling in my transform's `__init__` runs into pickling issues when the DataLoader forks the workers. As for compiling after the workers have forked, I run into the issue above.  Error logs   Minified repro _No response_  Versions  ",2023-03-30T16:45:08Z,triaged bug oncall: pt2 module: inductor,open,0,9,https://github.com/pytorch/pytorch/issues/97992,"you can try compiling your script before creating the multiprocessing pool, and then passing the compiled script as a parameter to your transform's init() method. This should avoid the pickling issues you are experiencing when the DataLoader forks the workers.","I think this will be resolved when we switching to a threading based parallel compile approach, which should be soon hopefully. For, can you see if `TORCHINDUCTOR_COMPILE_THREADS=1` fixes this issue ?",  any updates?,"I'm also having this issue. My use case is that I want to run a Celery app, and load and compile my pytorch model when a task worker is created. When I `torch.compile()` inside a task's `__init__()` method, I get the same error `AssertionError: daemonic processes are not allowed to have children`. Setting `TORCHINDUCTOR_COMPILE_THREADS=1` didn't fix the issue for me. If I change Celery to use a threadbased worker pool, then the error goes away, but the python GIL causes the workers to block each other, so I would much rather not do this.  is there any update on how to `torch.compile()` in a multiprocessing pool?", this should be fixed on master. The default parallel impl now is a process pool instead of fork. Can you try this again on master?,"I can test this today for you. Thanks! ________________________________ From: eellison ***@***.***> Sent: Tuesday, June 18, 2024 3:39:13 AM To: pytorch/pytorch ***@***.***> Cc: Aaron Snoswell ***@***.***>; Mention ***@***.***> Subject: Re: [pytorch/pytorch] torch.compile not compatible with multiprocessing pool (Issue CC(torch.compile not compatible with multiprocessing pool))  this should be fixed on master. The default parallel impl now is a process pool instead of fork. Can you try this again on master? â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>","> Setting TORCHINDUCTOR_COMPILE_THREADS=1 didn't fix the issue for me. Hmmm,  if this didn't fix then I'm not optimistic that the new parallel impl will make a difference since the process pool isn't even active in this case.", the error in the stack trace would only occur if we had multiple compile threads.   `TORCHINDUCTOR_COMPILE_THREADS=1` ?,"Hello! I am working on testing this, haven't quite got around to it yet sorry. Will get back to you next week. ________________________________ From: eellison ***@***.***> Sent: Friday, June 28, 2024 2:50 AM To: pytorch/pytorch ***@***.***> Cc: Aaron Snoswell ***@***.***>; Mention ***@***.***> Subject: Re: [pytorch/pytorch] torch.compile not compatible with multiprocessing pool (Issue CC(torch.compile not compatible with multiprocessing pool))  the error in the stack trace would only occur if we had multiple compile threads.  can you post the new error with TORCHINDUCTOR_COMPILE_THREADS=1 ? â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>"
538,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(llocated memory try setting max_split_size_mb to avoid fragmentation)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug i using the webui in stable diffusion and i have 8 gb ram  and i got this error and i want how to solve it by steps  llocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON  Versions ?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,llocated memory try setting max_split_size_mb to avoid fragmentation, ğŸ› Describe the bug i using the webui in stable diffusion and i have 8 gb ram  and i got this error and i want how to solve it by steps  llocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON  Versions ?,2023-03-29T23:14:16Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/97925,"Sounds like you're running out of CUDA memory. Here is a link to the referenced docs. I suggest asking questions like this on the PyTorch forums, as you're more likely to get help there. The issues here are reserved for problems within PyTorch itself. Closing as expected behavior."
374,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(do not need to check if element in dict input is Tensor.)ï¼Œ å†…å®¹æ˜¯ (sometimes it's a tuple with tensor element such as past value key in text generation case Fixes  CC(Jit trace failed with dict inputs))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",,do not need to check if element in dict input is Tensor.,sometimes it's a tuple with tensor element such as past value key in text generation case Fixes  CC(Jit trace failed with dict inputs),2023-03-29T07:37:17Z,open source Merged ciflow/trunk release notes: jit,closed,0,4,https://github.com/pytorch/pytorch/issues/97866,  matrix. please help review,test case added, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
286,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Simplify by using yield from in torch/utils/data)ï¼Œ å†…å®¹æ˜¯ (Also see https://github.com/pytorch/pytorch/pull/97831)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Simplify by using yield from in torch/utils/data,Also see https://github.com/pytorch/pytorch/pull/97831,2023-03-29T00:15:41Z,Merged ciflow/trunk release notes: dataloader topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/97839, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
334,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Simplify by using yield from)ï¼Œ å†…å®¹æ˜¯ (The issues were found by SIM104 flake8simplify in a local run. I'll take a look on adding the check to the CI separately. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Simplify by using yield from,The issues were found by SIM104 flake8simplify in a local run. I'll take a look on adding the check to the CI separately. ,2023-03-28T22:23:01Z,better-engineering Merged ciflow/trunk release notes: distributed (ddp) module: inductor ciflow/inductor,closed,0,8,https://github.com/pytorch/pytorch/issues/97831,"Hmm, I did a sweep with ruff a few weeks ago using it's pyupgrade rules. Surprised it missed so many of them.", Was planning on adding this check to the CI as well along with SIM101.,">  Was planning on adding this check to the CI as well along with SIM101. Feel free to go ahead, mention me on the PRs.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: GraphQL query  fragment PRReviews on PullRequestReviewConnection {   nodes {     author {       login     }     state   }   pageInfo {     startCursor     hasPreviousPage   } } fragment PRCheckSuites on CheckSuiteConnection {   edges {     node {       app {         name         databaseId       }       workflowRun {         workflow {           name         }         url       }       checkRuns(first: 50) {         nodes {           name           conclusion           detailsUrl           databaseId         }         pageInfo {           endCursor           hasNextPage         }       }       conclusion     }     cursor   }   pageInfo {     hasNextPage   } } fragment CommitAuthors on PullRequestCommitConnection {   nodes {     commit {       author {         user {           login         }         email         name       }       oid     }   }   pageInfo {     endCursor     hasNextPage   } } query ($owner: String!, $name: String!, $number: Int!) {   repository(owner: $owner, name: $name) {     pullRequest(number: $number) {       closed       isCrossRepository       author {         login       }       title       body       headRefName       headRepository {         nameWithOwner       }       baseRefName       baseRepository {         nameWithOwner         isPrivate         defaultBranchRef {           name         }       }       mergeCommit {         oid       }       commits_with_authors: commits(first: 100) {         ...CommitAuthors         totalCount       }       commits(last: 1) {         nodes {           commit {             checkSuites(first: 10) {               ...PRCheckSuites             }             status {               contexts {                 context                 state                 targetUrl               }             }             pushedDate             oid           }         }       }       changedFiles       files(first: 100) {         nodes {           path         }         pageInfo {           endCursor           hasNextPage         }       }       reviews(last: 100) {         ...PRReviews       }       comments(last: 5) {         nodes {           bodyText           createdAt           author {             login           }           authorAssociation           editor {             login           }           databaseId         }         pageInfo {           startCursor           hasPreviousPage         }       }       labels(first: 100) {         edges {           node {             name           }         }       }     }   } } , args {'name': 'pytorch', 'owner': 'pytorch', 'number': 97831} failed: [{'message': 'Something went wrong while executing your query. Please include `0402:863C:2A5062:56A08B:64246301` when reporting this issue.'}] Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`dynamo.utils.clone_inputs` failed with `BatchEncoding` from huggingface tokenizer)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Versions PyTorch version: 2.1.0a0+git7c52582 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.0 Libc version: glibc2.31 Python version: 3.8.0 (default, Nov  6 2019, 21:49:08)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 510.108.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.6.0 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_train.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`dynamo.utils.clone_inputs` failed with `BatchEncoding` from huggingface tokenizer," ğŸ› Describe the bug   Versions PyTorch version: 2.1.0a0+git7c52582 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.0 Libc version: glibc2.31 Python version: 3.8.0 (default, Nov  6 2019, 21:49:08)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 510.108.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.6.0 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_train.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                     ",2023-03-28T01:21:15Z,triaged oncall: pt2 module: dynamo,closed,0,0,https://github.com/pytorch/pytorch/issues/97724
565,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Foreach optimizers use higher memory due to intermediates)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Use https://github.com/pytorch/examples/tree/main/word_language_model, and apply the following changes to `main.py`, since the original script does not use any optimizer.  And then training with   By training with PyTorch1.13.1 and PyTorch2.0.0, we can find the latter one uses higher memory.  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Foreach optimizers use higher memory due to intermediates," ğŸ› Describe the bug Use https://github.com/pytorch/examples/tree/main/word_language_model, and apply the following changes to `main.py`, since the original script does not use any optimizer.  And then training with   By training with PyTorch1.13.1 and PyTorch2.0.0, we can find the latter one uses higher memory.  Versions  ",2023-03-27T22:00:59Z,high priority module: optimizer module: memory usage triaged module: mta,closed,0,15,https://github.com/pytorch/pytorch/issues/97712, , Can you add some numbers on how much higher is the memory usage in this scenario?,"For the example above, `nvidiasmi` shows memory usage difference: `1.13.1+cu117`: 5515MiB `2.0.0+cu118`: 6802MiB", can you please compare `1.13.1+cu117` and `2.0.0+cu117` version? (`cu118` might have a higher memory footprint),I am having the same issue. Pytorch1.13.1+cu117 uses 5.4gb VRAM Pytorch2.0.0+cu117 uses 7gb VRAM,"i just found out that setting the optimizer parameter fused:False, fixes this issue for me.","One major difference between 1.13 and 2.0 is that we now default to the faster foreach implementation (setting foreach=True) for most optimizers, including Adam. The default switching is disabled when one passes in certain kwargs, like fused=False.  Thus, based on your comment, it seems like there is a memory disparity between the foreach and the forloop implementation. Iâ€™ll take a look into why later today.","Aha. I have discovered the reason why peak memory differs, and it very much does have to do with the tradeoff between perf vs memory for the foreach and forloop implementations. The slow OG implementation of most optimizers is the forloop implementation, which is the straightforward go param by param update. The foreach implementation recognizes that it would save lots of time if we did the update all at once by globbing the params together into one big multitensor. However! As this issue points out, the peak memory usage with the globbed computation is much higher, _because the intermediates are also now huge globbed tensorlists_. To better illustrate with code, this is the adam.py forloop weight decay update: https://github.com/pytorch/pytorch/blob/234df29901a5fca1e23594a338a2f2201a28032f/torch/optim/adam.pyL335 Note that this is NOT an inplace update, so grad would point to an intermediate Tensor that is not the same as the grad of the param, using up extra memory for one intermediate Tensor. The foreach equivalent has a whole tensorlists of grads as an intermediate, using up len(params) * Tensor amount of memory: https://github.com/pytorch/pytorch/blob/234df29901a5fca1e23594a338a2f2201a28032f/torch/optim/adam.pyL442 This is why the peak memory usage differs so greatly. I am going to rename the issue title accordingly and prioritize using more inplace ops to minimize our use of intermediates.","I agree with your assessment that this is due to converting many small allocations and frees into a small number of monster allocations and frees. I put together this gist to profile a simple stack of linear layers:  If we pull apart the events to get python calls around the memory peak (in the gist) we get:  L448 (`if weight_decay != 0: device_grads = ...`) was identified above, but there are two other large blocks. (And unlike weight decay, they occur with default hyperparameters.) https://github.com/pytorch/pytorch/blob/fa893f3f58d4fbbf498132586e6e9c335d800d2e/torch/optim/adam.pyL510L512 As much as I love the idea of foreach optimizers, IMHO the default should be rolled back until the memory problem has been solved. It is a serious and hard to diagnose regression for anyone training parameter heavy models and only delivers a modest overhead reduction for a comparatively modest range of models. CC ,  ","Alright, let me prioritize this within the next week","An optimizer that globs all the parameters together is naturally going to run after the backward is done (and activations are freed). So one possibility here is that memory usage doesn't actually need to be higher, it is just the case that those big allocations cannot fit in the segments for the smaller tensors and end up being allocated separately. If the process was actually under memory pressure (e.g. memory got entirely filled), then those small segments would get freed, the big ones would be allocated in their place, and the next pass would likely be able to run by using the big allocated memory.   If the above is what is happening the expandable_segments:True allocator option would prevent extra memory use but I don't think that is a general solution.","Due to planning and several other things that have popped up this week, I won't be able to get to opening a fix by the end of this week :(. However, it is clear what needs to be done: we need to use more in place operators and fewer intermediates in each of the foreach optimizer implementations. I am hoping to do this in the coming weeks for each optimizer, but if you are blocked, please submit a PR and I will be happy to review.",  I've starting minimizing memory as much as I could in the Adam family. There is a minimum budget of 1 intermediate (with size equal to the sum of the sizes of the parameters) as we cannot inplace modify saved state. Could y'all give the latest nightly a whirl to verify the memory usage has decreased? It also looks like runtime should have gotten slightly faster as well based on my dash: ,"All current foreach optimizers have been evaluated for peak memory usage and the PRs hto minimize intermediate usage have all landed as of https://github.com/pytorch/pytorch/pull/105193. With this, I am closing this issue.",This was a great discussion and analysis. Stumbled on this when looking for information on what `for_each` does. Very informative.  Thanks everyone. 
655,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add mutable to name of non-const Storage::data_ptr)ï¼Œ å†…å®¹æ˜¯ (  CC(introduce TensorIterator::unsafe_replace_input())  CC(introduce TensorBase::mutable_data_ptr())  CC(introduce TensorBase::mutable_data_ptr)  CC(distinguish mutability of TensorImpl::data)  CC(distinguish mutability of TensorImpl::data())  CC(make TensorImpl::data_ptr_impl() nonconst and have mutable in the name)  CC(add mutable to name of nonconst Storage::data_ptr) See D44409928. Differential Revision: D44432585)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,add mutable to name of non-const Storage::data_ptr,  CC(introduce TensorIterator::unsafe_replace_input())  CC(introduce TensorBase::mutable_data_ptr())  CC(introduce TensorBase::mutable_data_ptr)  CC(distinguish mutability of TensorImpl::data)  CC(distinguish mutability of TensorImpl::data())  CC(make TensorImpl::data_ptr_impl() nonconst and have mutable in the name)  CC(add mutable to name of nonconst Storage::data_ptr) See D44409928. Differential Revision: D44432585,2023-03-27T19:24:24Z,Merged ciflow/trunk topic: not user facing merging,closed,0,3,https://github.com/pytorch/pytorch/issues/97694, rebase help, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
513,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of untyped Storage::data)ï¼Œ å†…å®¹æ˜¯ (  CC(distinguish mutability of TensorImpl::data())  CC(make TensorImpl::data_ptr_impl() nonconst and have mutable in the name)  CC(add mutable to name of nonconst Storage::data_ptr)  CC(distinguish mutability of untyped Storage::data) See D44409928. Differential Revision: D44429769)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of untyped Storage::data,  CC(distinguish mutability of TensorImpl::data())  CC(make TensorImpl::data_ptr_impl() nonconst and have mutable in the name)  CC(add mutable to name of nonconst Storage::data_ptr)  CC(distinguish mutability of untyped Storage::data) See D44409928. Differential Revision: D44429769,2023-03-27T18:37:54Z,Merged topic: not user facing merging,closed,0,4,https://github.com/pytorch/pytorch/issues/97690, help, drci," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
330,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of Storage::unsafe_data)ï¼Œ å†…å®¹æ˜¯ (  CC(distinguish mutability of Storage::unsafe_data) See D44409928. Differential Revision: D44426197)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of Storage::unsafe_data,  CC(distinguish mutability of Storage::unsafe_data) See D44409928. Differential Revision: D44426197,2023-03-27T17:34:21Z,topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/97685, rebase help, rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/dagitses/27/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4585053095
650,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pytorch 2 compile + fsdp + transformers crash)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hello, I am trying to combine both pytorch 2.0 compile + fsdp on TPU but it doesn't work. What does work on TPU: base training. base training + PyTorch compile. base training + FSDP. But it doesn't work when I combine both FSDP + PyTorch compile. I have created an example here to reproduce the problem: https://colab.research.google.com/drive/1RmarhGBIjeWHIngO7fAp239eqt5Za8bZ?usp=sharing  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Pytorch 2 compile + fsdp + transformers crash," ğŸ› Describe the bug Hello, I am trying to combine both pytorch 2.0 compile + fsdp on TPU but it doesn't work. What does work on TPU: base training. base training + PyTorch compile. base training + FSDP. But it doesn't work when I combine both FSDP + PyTorch compile. I have created an example here to reproduce the problem: https://colab.research.google.com/drive/1RmarhGBIjeWHIngO7fAp239eqt5Za8bZ?usp=sharing  Versions  ",2023-03-27T15:46:56Z,triaged module: xla,open,0,3,https://github.com/pytorch/pytorch/issues/97676,"Quick clarification: This is `XlaFullyShardedDataParallel`, not PyTorch's native `FullyShardedDataParallel`.",> XlaFullyShardedDataParallel Should I open this issue at XLA rather than here?,Yes. You might get help faster than way.
1996,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How do I get the original object wrapped by the torch.fx.proxy classï¼Ÿ)ï¼Œ å†…å®¹æ˜¯ (I want quant the swin transformers. This model need get some shape of tensors in the forward function, but the args is a proxy object, How do I get the original object(It is the window tensor below) wrapped by the Proxy classï¼ŸThanks.  Issue description Provide a short description.  Code example !image !image  Traceback **the windows is a tensor wrapped by the proxy, Is there some method to get the original tensor or its shape?** !image  System Info Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.12.0+cu116 [pip3] torchaudio==0.12.0+cu116 [pip3] torchvision==0.13.0+cu116 [conda] blas                      1.0                         mkl    defaults [conda] cudatoolkit               11.3.1               h2bc3f7f_2    defaults [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640    defaults [conda] mklservice               2.4.0            py38h7f8727e_0    defaults [conda] mkl_fft                   1.3.1            py38hd3c417c_0    defaults [conda] mkl_random                1.2.2            py38h51133e4_0    defaults [conda] numpy                     1.23.3           py38h14f4228_0    defaults [conda] numpybase                1.23.3           py38h31eccc5_0    defaults [conda] pytorch                   1.13.0              py3.8_cpu_0    pytorch [conda] pytorchmutex             1.0                         cpu    pytorch [conda] torch                     1.12.0+cu116               [conda] torchaudio                0.12.0+cu116               [conda] torchaudio                0.13.0                 py38_cpu    pytorch [conda] torchvision               0.14.0                 py38_cpu    pytorch [conda] torchvisi)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,How do I get the original object wrapped by the torch.fx.proxy classï¼Ÿ,"I want quant the swin transformers. This model need get some shape of tensors in the forward function, but the args is a proxy object, How do I get the original object(It is the window tensor below) wrapped by the Proxy classï¼ŸThanks.  Issue description Provide a short description.  Code example !image !image  Traceback **the windows is a tensor wrapped by the proxy, Is there some method to get the original tensor or its shape?** !image  System Info Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.12.0+cu116 [pip3] torchaudio==0.12.0+cu116 [pip3] torchvision==0.13.0+cu116 [conda] blas                      1.0                         mkl    defaults [conda] cudatoolkit               11.3.1               h2bc3f7f_2    defaults [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640    defaults [conda] mklservice               2.4.0            py38h7f8727e_0    defaults [conda] mkl_fft                   1.3.1            py38hd3c417c_0    defaults [conda] mkl_random                1.2.2            py38h51133e4_0    defaults [conda] numpy                     1.23.3           py38h14f4228_0    defaults [conda] numpybase                1.23.3           py38h31eccc5_0    defaults [conda] pytorch                   1.13.0              py3.8_cpu_0    pytorch [conda] pytorchmutex             1.0                         cpu    pytorch [conda] torch                     1.12.0+cu116               [conda] torchaudio                0.12.0+cu116               [conda] torchaudio                0.13.0                 py38_cpu    pytorch [conda] torchvision               0.14.0                 py38_cpu    pytorch [conda] torchvisi",2023-03-27T10:46:30Z,triaged oncall: fx,open,0,2,https://github.com/pytorch/pytorch/issues/97659,You can't. Use FakeTensorProp to regenerate shape info or use make_fx,> You can't. Use FakeTensorProp to regenerate shape info or use make_fx Can you provide a demo that demonstrates the FakeTensorProp or make_fx to get size? I'm not very familiar with these interfaces. Thanks.
290,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove redundant trailing semicolons in StorageImpl.h)ï¼Œ å†…å®¹æ˜¯ (remove redundant trailing semicolons in StorageImpl.h)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove redundant trailing semicolons in StorageImpl.h,remove redundant trailing semicolons in StorageImpl.h,2023-03-27T10:28:31Z,open source Merged ciflow/trunk topic: not user facing merging,closed,0,14,https://github.com/pytorch/pytorch/issues/97658, merge r, successfully started a rebase job. Check the current status here,"Successfully rebased `pr97658` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout pr97658 && git pull rebase`)",The committers listed above are authorized under a signed CLA.:white_check_mark: login: dagitses  (729573b7f9dc1a8d6cb23ebaafb9a7df4fcfb967), Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  EasyCLA Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  EasyCLA Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , do you mind changing your email to personal one for this PR?,>  do you mind changing your email to personal one for this PR?  I can't push to pytorch. But I have updated a branch in my fork. Are you able to change the head ref to the corresponding branch in my fork? https://github.com/dagitses/pytorch/tree/pr97658," sent you an invite. If PR has one thing, that should be immutable, is PR head. But you should be able to push the update."," merge f ""Lint is green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",>  do you mind changing your email to personal one for this PR? Wrong git user  ?  is me and I'm not involved in this PR.,"> Wrong git user  ?  is me and I'm not involved in this PR. Sorry about the confusion. GitHub sometimes converts first name to the user name, but sometimes fails to do so."
1585,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(How do I get the original object wrapped by the Proxy objectï¼Ÿ)ï¼Œ å†…å®¹æ˜¯ ( Issue description I want quant the swin transformers. This model need get some shape of tensors in the forward function, but the args is a proxy object, How do I get the original object wrapped by the Proxy objectï¼ŸThanks.  Code example def window_reverse(windows, window_size, H, W):     """"""     Args:         windows: (num_windows*B, window_size, window_size, C)         window_size (int): Window size         H (int): Height of image         W (int): Width of image     Returns:         x: (B, H, W, C)     """"""     B = int(windows.shape[0] / (H * W / window_size / window_size))     x = windows.view(B, H // window_size, W // window_size, window_size, window_size, 1)     x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, 1)     return x Please try to provide a minimal example to repro the bug. Error messages and stack traces are also helpful.  System Info Please copy and paste the output from our environment collection script (or fill out the checklist below manually). You can get the script and run it with:   PyTorch or Caffe2:  How you installed PyTorch (conda, pip, source):  Build command you used (if compiling from source):  OS:  PyTorch version:  Python version:  CUDA/cuDNN version:  GPU models and configuration:  GCC version (if compiling from source):  CMake version:  Versions of any other relevant libraries:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,How do I get the original object wrapped by the Proxy objectï¼Ÿ," Issue description I want quant the swin transformers. This model need get some shape of tensors in the forward function, but the args is a proxy object, How do I get the original object wrapped by the Proxy objectï¼ŸThanks.  Code example def window_reverse(windows, window_size, H, W):     """"""     Args:         windows: (num_windows*B, window_size, window_size, C)         window_size (int): Window size         H (int): Height of image         W (int): Width of image     Returns:         x: (B, H, W, C)     """"""     B = int(windows.shape[0] / (H * W / window_size / window_size))     x = windows.view(B, H // window_size, W // window_size, window_size, window_size, 1)     x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, 1)     return x Please try to provide a minimal example to repro the bug. Error messages and stack traces are also helpful.  System Info Please copy and paste the output from our environment collection script (or fill out the checklist below manually). You can get the script and run it with:   PyTorch or Caffe2:  How you installed PyTorch (conda, pip, source):  Build command you used (if compiling from source):  OS:  PyTorch version:  Python version:  CUDA/cuDNN version:  GPU models and configuration:  GCC version (if compiling from source):  CMake version:  Versions of any other relevant libraries:",2023-03-27T09:51:14Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/97657
377,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(where is the  engine_layer_visualize.py,isn't removed?)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug where is the  engine_layer_visualize.py,isn't removed?  Versions where is the  engine_layer_visualize.py,isn't removed?)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"where is the  engine_layer_visualize.py,isn't removed?"," ğŸ› Describe the bug where is the  engine_layer_visualize.py,isn't removed?  Versions where is the  engine_layer_visualize.py,isn't removed?",2023-03-27T08:45:04Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/97654,I am closing this for lack of detail.
405,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of StorageImpl::data_ptr() member)ï¼Œ å†…å®¹æ˜¯ (  CC(remove redundant typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::data_ptr() member) See D44409928. Differential Revision: D44410323)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of StorageImpl::data_ptr() member,  CC(remove redundant typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::data_ptr() member) See D44409928. Differential Revision: D44410323,2023-03-27T07:17:03Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/97651, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
658,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(remove redundant typed StorageImpl::data() member)ï¼Œ å†…å®¹æ˜¯ (  CC(remove redundant typed StorageImpl::data() member)  CC(add StorageImpl::mutable_unsafe_data) This has the same implementation as the unsafe variants and the unsafe variants match the original semantics of the code, given that they don't check that the type matches. Given that we're updating callsites anyways to address the mutability aspect, we might as well just drop this method now. Differential Revision: D44410210)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,remove redundant typed StorageImpl::data() member,"  CC(remove redundant typed StorageImpl::data() member)  CC(add StorageImpl::mutable_unsafe_data) This has the same implementation as the unsafe variants and the unsafe variants match the original semantics of the code, given that they don't check that the type matches. Given that we're updating callsites anyways to address the mutability aspect, we might as well just drop this method now. Differential Revision: D44410210",2023-03-27T07:10:21Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/97650, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
365,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(add StorageImpl::mutable_unsafe_data)ï¼Œ å†…å®¹æ˜¯ (  CC(remove redundant typed StorageImpl::data() member)  CC(add StorageImpl::mutable_unsafe_data) See D44409928. Differential Revision: D44409945)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,add StorageImpl::mutable_unsafe_data,  CC(remove redundant typed StorageImpl::data() member)  CC(add StorageImpl::mutable_unsafe_data) See D44409928. Differential Revision: D44409945,2023-03-27T06:36:35Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/97648, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1205,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of untyped StorageImpl::data() member)ï¼Œ å†…å®¹æ˜¯ (  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(remove redundant typed StorageImpl::data() member)  CC(add StorageImpl::mutable_unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member) To implement the warning when transitioning reshape to copyonwrite storage, we want to be able to detect a write to one view family following by a read or a write to another one that shares the same copyonwrite storage. Because we have historically not been strict about the mutability of our data pointers, any warning we have would likely be far too aggressive. Therefore, this is the first PR in a long series to ensure a strict distinction between mutable and const data accessors in TensorBase, TensorImpl, Storage, and StorageImpl. The rough plan is to give the mutable accessor a new name that is explicit about mutation, this will also force us to rewrite any code that really needs a mutation. Differential Revision: D44409928)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of untyped StorageImpl::data() member,"  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(remove redundant typed StorageImpl::data() member)  CC(add StorageImpl::mutable_unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member) To implement the warning when transitioning reshape to copyonwrite storage, we want to be able to detect a write to one view family following by a read or a write to another one that shares the same copyonwrite storage. Because we have historically not been strict about the mutability of our data pointers, any warning we have would likely be far too aggressive. Therefore, this is the first PR in a long series to ensure a strict distinction between mutable and const data accessors in TensorBase, TensorImpl, Storage, and StorageImpl. The rough plan is to give the mutable accessor a new name that is explicit about mutation, this will also force us to rewrite any code that really needs a mutation. Differential Revision: D44409928",2023-03-27T06:31:28Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/97647, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
690,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_typed_storage_deprecation_warning (__main__.TestTorch))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing on master (recent examples). After https://github.com/pytorch/pytorch/pull/97379, the test starts to fail with an unicode decoding issue:  I'm not quite sure why the error only starts to surface after https://hud.pytorch.org/pytorch/pytorch/commit/f09347a9f11ac751023d3598ab8a04a10ce22a59, but not the original commit. So marking it as flaky to disable it while investigation.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,DISABLED test_typed_storage_deprecation_warning (__main__.TestTorch),"Platforms: linux This test was disabled because it is failing on master (recent examples). After https://github.com/pytorch/pytorch/pull/97379, the test starts to fail with an unicode decoding issue:  I'm not quite sure why the error only starts to surface after https://hud.pytorch.org/pytorch/pytorch/commit/f09347a9f11ac751023d3598ab8a04a10ce22a59, but not the original commit. So marking it as flaky to disable it while investigation.",2023-03-26T16:55:11Z,skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/97630,This is fixed by https://github.com/pytorch/pytorch/pull/97628
2020,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MultiHeadAttention remove the constraint that the input dimension equals the output dimension)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch The `MultiHeadAttention` accepts a single parameter `embed_dim` which dictates both the number of input features as well as the number of output features. I propose to split this parameter into two separate parameters, namely: `in_features` and `out_features`. My concern is that all `q_proj_weight`, `k_proj_weight` and `v_proj_weight` project to the same space as the input space `embed_dim` (here). I propose instead to have these matrices project to the space `out_features`:  In addition the `out_proj_weight` matrix should now be of shape `(out_features, out_features)` (here). I believe this change would allow the `MultiHeadAttention` layer to be applied to a more broad variety of tasks, and not only to the specific use case of the Transofmer encoder and decoder, where the number of input features match then number output of output features. And even in the transformer model, I don't see a reason why this would be the case? In the paper the authors even go as far as to propose the $W_V$ projection matrix to project to a different space $d_v$ and the `out_proj_weight` to be of shape `(d_v, out_features)`.  Alternatives As a workaround I add an additional embedding matrix prior to the `MultiHeadAttention` layer. Let's say I have the following input on which I want to apply the multihead attention:  Then if I simply apply the layer I get the following:  But if my input features are to few (e.g. `D=16, 32`) or if they are too many (e.g. `D=10000`), then I want the multihead attention layer to work with a different embedding space. Thus, I need to apply an embedding matrix prior to that:  In my opinion it doesn't make much sense to be learning both the embedding matrix $W_E$ and the)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,MultiHeadAttention remove the constraint that the input dimension equals the output dimension," ğŸš€ The feature, motivation and pitch The `MultiHeadAttention` accepts a single parameter `embed_dim` which dictates both the number of input features as well as the number of output features. I propose to split this parameter into two separate parameters, namely: `in_features` and `out_features`. My concern is that all `q_proj_weight`, `k_proj_weight` and `v_proj_weight` project to the same space as the input space `embed_dim` (here). I propose instead to have these matrices project to the space `out_features`:  In addition the `out_proj_weight` matrix should now be of shape `(out_features, out_features)` (here). I believe this change would allow the `MultiHeadAttention` layer to be applied to a more broad variety of tasks, and not only to the specific use case of the Transofmer encoder and decoder, where the number of input features match then number output of output features. And even in the transformer model, I don't see a reason why this would be the case? In the paper the authors even go as far as to propose the $W_V$ projection matrix to project to a different space $d_v$ and the `out_proj_weight` to be of shape `(d_v, out_features)`.  Alternatives As a workaround I add an additional embedding matrix prior to the `MultiHeadAttention` layer. Let's say I have the following input on which I want to apply the multihead attention:  Then if I simply apply the layer I get the following:  But if my input features are to few (e.g. `D=16, 32`) or if they are too many (e.g. `D=10000`), then I want the multihead attention layer to work with a different embedding space. Thus, I need to apply an embedding matrix prior to that:  In my opinion it doesn't make much sense to be learning both the embedding matrix $W_E$ and the",2023-03-25T19:44:43Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/97607,We are not currently looking to extend the API surface of torch.nn.MultiheadAttention. You could take a look at scaled_dot_product_attention which can be used to create more customizable Attention setup. For Instance here is an implementation of CausalAttention using SDPA: https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.htmlcausalselfattention
1023,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Multi-output derivative formulas can save unnecessary tensors)ï¼Œ å†…å®¹æ˜¯ (Usually when you have a derivative formula, e.g. `mm`:  The structure informs autograd codegen what tensors need to be saved to compute which gradients, i.e. if only `self` requires grad, I only need to save `mat2`, and if only `mat2` requires grad, I only need to save `self`.  In VariableType, the following logic is generated for `mm`:  However, when you have a single derivative formula that produces multiple outputs, autograd codegen no longer has visibility into what tensors need to be saved in order to compute which gradients.   This yields the following unoptimized logic in autograd kernel:  One common case where this can matter is if I'm doing some finetuning, e.g. I want to requires_grad=False for the parameters of a bunch of layers in the middle of a network. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,Multi-output derivative formulas can save unnecessary tensors,"Usually when you have a derivative formula, e.g. `mm`:  The structure informs autograd codegen what tensors need to be saved to compute which gradients, i.e. if only `self` requires grad, I only need to save `mat2`, and if only `mat2` requires grad, I only need to save `self`.  In VariableType, the following logic is generated for `mm`:  However, when you have a single derivative formula that produces multiple outputs, autograd codegen no longer has visibility into what tensors need to be saved in order to compute which gradients.   This yields the following unoptimized logic in autograd kernel:  One common case where this can matter is if I'm doing some finetuning, e.g. I want to requires_grad=False for the parameters of a bunch of layers in the middle of a network. ",2023-03-24T23:42:24Z,module: autograd triaged module: nestedtensor actionable,open,1,8,https://github.com/pytorch/pytorch/issues/97575,How would you fix this? By splitting single derivative formulas into multiple?,"I don't think there is a way for us to do anything here: if you can split up the formula, you can do so. But if computing each gradient separately is more expensive, then we could imagine a custom rule where you can provide both separate and merged formulas. But that would be quite a bit of work and I don't think we see a significant benefit for it? Also in the great new world of torch.compile, the unused computations will be DCEd away :D ","I haven't checked too many other ones, but matmul autograd explicit kernel (specific to nested) seems splittable","Sometimes, the issue with splitting them is that, if you have to implement the second derivatives, you need to implement the cross derivatives twice (and these will be recomputed if the user asks for these) which is a bit annoying.","of course, this is not the case of matmul",Another crappy fix is to do something like: ,"Yeah that's what I hinted at above, but we should only do that if we have cases with clear benefit. And given the new compile world, I don't think it will happen.", has a fix for this now in https://github.com/pytorch/pytorch/pull/103750 which introduces a macro to hint autograd codegen at what to save. What's left to do is update existing formulas to use this.
688,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Suppress exceptions for timeouts in torch elastic agent barrier)ï¼Œ å†…å®¹æ˜¯ (Summary: If the final barrier in elastic agent times out, it emits an exception message to the log file. This makes it difficult to debug failed jobs since I need to skip over these spurious error before finding the actual exception that caught the job to fail. Test Plan: * Verified all Sandcastle tests passed * Ran an e2e job with the patched agent file (I will update with test result once it completes). Differential Revision: D44375414)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Suppress exceptions for timeouts in torch elastic agent barrier,"Summary: If the final barrier in elastic agent times out, it emits an exception message to the log file. This makes it difficult to debug failed jobs since I need to skip over these spurious error before finding the actual exception that caught the job to fail. Test Plan: * Verified all Sandcastle tests passed * Ran an e2e job with the patched agent file (I will update with test result once it completes). Differential Revision: D44375414",2023-03-24T23:02:00Z,fb-exported Stale,closed,0,8,https://github.com/pytorch/pytorch/issues/97572,The committers listed above are authorized under a signed CLA.:white_check_mark: login: drdarshan / name: Sudarshan Raghunathan  (5af084a245201a710d401c1ca08e5a54cdb622fc),This pull request was **exported** from Phabricator. Differential Revision: D44375414,"To resolve the CLA issue, make sure to add the email address of the commit to your GitHub profile  (to see the actual email address, add `.patch` at the end of this PR page's URL).","> To resolve the CLA issue, make sure to add the email address of the commit to your GitHub profile (to see the actual email address, add `.patch` at the end of this PR page's URL).   thank you for the suggestion. I added the email to my profile. Please let me know if I need to to rerun anything for the CLA error to go away? Thanks again! ",/easycla," Now that you've added the email, you need to actually sign the CLA. Please follow the link in the linuxfoundationeasycla's comment.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.", do we still want to merge this? Conflicts need to be resolved.
858,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor] Complex multiplication fails in inductor with KeyError: torch.complex64)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The sequence of operations,   breaks under compilation, see the manual reproduction example below. This is sequence of operations is used, for example, to implement relative positional embeddings, as in LLAMA (see https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.pyL63). P.S: The minifier actually also breaks when minifying the reproduction example given below, it misses the `view_as_real` part and only constructs  which is not runnable, because it ends on the complex number.  Error logs   Minified repro   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llama,[Inductor] Complex multiplication fails in inductor with KeyError: torch.complex64," ğŸ› Describe the bug The sequence of operations,   breaks under compilation, see the manual reproduction example below. This is sequence of operations is used, for example, to implement relative positional embeddings, as in LLAMA (see https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/llama/model.pyL63). P.S: The minifier actually also breaks when minifying the reproduction example given below, it misses the `view_as_real` part and only constructs  which is not runnable, because it ends on the complex number.  Error logs   Minified repro   Versions  ",2023-03-24T21:17:54Z,high priority triaged module: complex ezyang's list oncall: pt2 module: inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/97562,,"Repro from above passes on newer nightlies, here is a slightly more realistic version:  which fails with error  ",Need to add conj support to meta converter: ,WIP PR here  will update,Works on master
1992,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TransformerEncoder fast path raises incorrect mask dtype warning )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The following snippet shouldn't result in a warning, but does:    Versions Collecting environment information... PyTorch version: 2.1.0.dev20230324+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.067genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.85.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          8 Online CPU(s) list:             07 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Core(TM) i79700 CPU @ 3.00GHz CPU family:                      6 Model:                        )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,TransformerEncoder fast path raises incorrect mask dtype warning ," ğŸ› Describe the bug The following snippet shouldn't result in a warning, but does:    Versions Collecting environment information... PyTorch version: 2.1.0.dev20230324+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.067genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.85.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          8 Online CPU(s) list:             07 Vendor ID:                       GenuineIntel Model name:                      Intel(R) Core(TM) i79700 CPU @ 3.00GHz CPU family:                      6 Model:                        ",2023-03-24T14:47:03Z,module: cuda triaged,closed,10,3,https://github.com/pytorch/pytorch/issues/97532,I'm still seeing this issue with the latest nightly (`2.1.0.dev20230503`).,duplicate  CC(TransformerEncoderLayer always warns when using src_key_padding_mask in inference mode.),"This seems to be fixed in `2.1.0`, thanks all!"
920,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add a warning and setting weights_only=True to encourage safer torch.load() practices)ï¼Œ å†…å®¹æ˜¯ (Helps with CC(Third party PyTorch models may execute arbitrary code during deserialization) This pull request aims to encourage safer loading practices in PyTorch, especially in an environment that can be encouraging to run models from less reputable users/orgs. This change ensures that only tensor data is loaded by default, reducing the risk of arbitrary code execution during unpickling. A warning message is displayed to inform users of the potential risks associated with loading data with `weights_only=False`. This educates users about the potential risks and encourages them to use safer loading practices when dealing with untrusted sources.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add a warning and setting weights_only=True to encourage safer torch.load() practices,"Helps with CC(Third party PyTorch models may execute arbitrary code during deserialization) This pull request aims to encourage safer loading practices in PyTorch, especially in an environment that can be encouraging to run models from less reputable users/orgs. This change ensures that only tensor data is loaded by default, reducing the risk of arbitrary code execution during unpickling. A warning message is displayed to inform users of the potential risks associated with loading data with `weights_only=False`. This educates users about the potential risks and encourages them to use safer loading practices when dealing with untrusted sources.",2023-03-24T02:00:19Z,triage review triaged open source Stale release notes: quantization topic: bc breaking topic: security,closed,0,3,https://github.com/pytorch/pytorch/issues/97495,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: tensorneko / name: YubiHunter  (47c7b51c537c05435e571bac448b6e0631a8a79d, 5de35927939a35a10b722defb2ff8372b838f8ab)","Might require some discussion but I recently chatted with  about this change and I feel like it makes a lot of sense, it would break BC but in this case it feels worth it","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
1491,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Refactor op level debugging)ï¼Œ å†…å®¹æ˜¯ (  CC([ONNX] Fix scalar elements in op.Concat)  CC([ONNX] Refactor op level debugging) Fixes CC([ONNX] `fx_name_to_onnxscipt_value` is annotated the wrong type)  Fixes CC(DISABLED test_mutation (__main__.TestFxToOnnxWithOnnxRuntime))  Fixes https://github.com/microsoft/onnxscript/issues/393 Provide op_level_debug in exporter which creates randomnied torch.Tensor based on FakeTensorProp real shape as inputs of both torch ops and ONNX symbolic function. The PR leverages on Transformer class to create a new fx.Graph, but shares the same Module with the original one to save memory. The test is different from op_correctness_test.py as op_level_debug generating real tensors based on the fake tensors in the model. Limitation: 1. Some of the trace_only function is not supported due to lack of param_schema which leads to arg/kwargs wronly split and ndarray wrapping. (WARNINGS in SARIF) 2. The ops with dim/indices (INT64) is not supported that they need the information(shape) from other input args.  (WARNINGS in SARIF) 3. sym_size and builtin ops are not supported.  4. op_level_debug only labels results in SARIF. It doesn't stop exporter. 5. Introduce ONNX owning FakeTensorProp supports int/float/bool 6. parametrized op_level_debug and dynamic_shapes into FX tests)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[ONNX] Refactor op level debugging,"  CC([ONNX] Fix scalar elements in op.Concat)  CC([ONNX] Refactor op level debugging) Fixes CC([ONNX] `fx_name_to_onnxscipt_value` is annotated the wrong type)  Fixes CC(DISABLED test_mutation (__main__.TestFxToOnnxWithOnnxRuntime))  Fixes https://github.com/microsoft/onnxscript/issues/393 Provide op_level_debug in exporter which creates randomnied torch.Tensor based on FakeTensorProp real shape as inputs of both torch ops and ONNX symbolic function. The PR leverages on Transformer class to create a new fx.Graph, but shares the same Module with the original one to save memory. The test is different from op_correctness_test.py as op_level_debug generating real tensors based on the fake tensors in the model. Limitation: 1. Some of the trace_only function is not supported due to lack of param_schema which leads to arg/kwargs wronly split and ndarray wrapping. (WARNINGS in SARIF) 2. The ops with dim/indices (INT64) is not supported that they need the information(shape) from other input args.  (WARNINGS in SARIF) 3. sym_size and builtin ops are not supported.  4. op_level_debug only labels results in SARIF. It doesn't stop exporter. 5. Introduce ONNX owning FakeTensorProp supports int/float/bool 6. parametrized op_level_debug and dynamic_shapes into FX tests",2023-03-24T01:28:52Z,module: onnx open source Merged ciflow/trunk release notes: onnx topic: bug fixes merging,closed,0,7,https://github.com/pytorch/pytorch/issues/97494,Looks like FakeTensorProp has behavior change after https://github.com/pytorch/pytorch/pull/97204. Now that sym_size shape can't be propagated. We might need an alternative way if it's intended.,"> Looks like FakeTensorProp has behavior change after CC(Add Config to Skip Cpp Codegen, Enable in FBCode). Now that sym_size shape can't be propagated. We might need an alternative way if it's intended. Added our own pass for FakeTensorProp which supports int/float/bool", PTAL,Had CI fail on: `worker 'gw1' crashed while running 'test/onnx/test_fx_to_onnx_with_onnxruntime.py::TestFxToOnnxWithOnnxRuntime_op_level_debug_False_dynamic_shapes_True::test_flatten_dynamic_axes'` Not sure it's on the machine or the test...,Most likely because of ORT. This may be useful: https://github.com/microsoft/onnxscript/pull/602, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
744,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of StorageImpl::data_ptr() member)ï¼Œ å†…å®¹æ˜¯ (distinguish mutability of StorageImpl::data_ptr() member Summary: See CC(distinguish mutability of untyped StorageImpl::data() member). Test Plan: Rely on CI. Reviewers: Subscribers: Tasks: Tags:  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of StorageImpl::data_ptr() member,distinguish mutability of StorageImpl::data_ptr() member Summary: See CC(distinguish mutability of untyped StorageImpl::data() member). Test Plan: Rely on CI. Reviewers: Subscribers: Tasks: Tags:  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member),2023-03-24T01:02:10Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/97492
711,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of typed StorageImpl::data() member)ï¼Œ å†…å®¹æ˜¯ (distinguish mutability of typed StorageImpl::data() member Summary: See CC(distinguish mutability of untyped StorageImpl::data() member). Test Plan: Rely on CI.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of typed StorageImpl::data() member,distinguish mutability of typed StorageImpl::data() member Summary: See CC(distinguish mutability of untyped StorageImpl::data() member). Test Plan: Rely on CI.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member),2023-03-24T00:15:44Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/97489
672,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([eazy][inductor] fix typo in mm max-autotune log)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] correctly setup constant in the wrapper)  CC([eazy][inductor] fix typo in mm maxautotune log)  CC([inductor] use torch.profiler in the triton wrapper) max autotune log like  is confusion since the sum of runtime of all the kernels is larger than the total time used for tuning. In fact, `triton.testing.do_bench` return milliseconds scale time rather than seconds scale. Fix the typo in the log message to make that clear.  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[eazy][inductor] fix typo in mm max-autotune log,"  CC([inductor] correctly setup constant in the wrapper)  CC([eazy][inductor] fix typo in mm maxautotune log)  CC([inductor] use torch.profiler in the triton wrapper) max autotune log like  is confusion since the sum of runtime of all the kernels is larger than the total time used for tuning. In fact, `triton.testing.do_bench` return milliseconds scale time rather than seconds scale. Fix the typo in the log message to make that clear.  ",2023-03-23T23:38:26Z,Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/97486, merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge f 'The test failure does not seem relevant'," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1052,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Adding Backward Support for NestedTensors and FlashAttention)ï¼Œ å†…å®¹æ˜¯ ( Summary   ğŸ¤– Generated by Copilot at 318764f This pull request implements the CUDA backend of the SDPA kernel for nested tensors, which enables efficient transformer models with variablelength sequences. It adds a new dispatch key, a backward function, a unit test, and some helper functions for the kernel. It modifies `test/test_transformers.py`, `aten/src/ATen/native/native_functions.yaml`, `aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctionsBackward.cpp`, and `aten/src/ATen/native/nested/cuda/NestedTensorTransformerUtils.h`.   ğŸ¤– Generated by Copilot at ed4a773 > _Fused kernels of doom, unleash the flash attention_ > _Nested tensors on fire, reshape and pad with caution_ > _Backward pass of power, dispatch the CUDA key_ > _Test the gradients of hell, warn the user if they disagree_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Adding Backward Support for NestedTensors and FlashAttention," Summary   ğŸ¤– Generated by Copilot at 318764f This pull request implements the CUDA backend of the SDPA kernel for nested tensors, which enables efficient transformer models with variablelength sequences. It adds a new dispatch key, a backward function, a unit test, and some helper functions for the kernel. It modifies `test/test_transformers.py`, `aten/src/ATen/native/native_functions.yaml`, `aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctionsBackward.cpp`, and `aten/src/ATen/native/nested/cuda/NestedTensorTransformerUtils.h`.   ğŸ¤– Generated by Copilot at ed4a773 > _Fused kernels of doom, unleash the flash attention_ > _Nested tensors on fire, reshape and pad with caution_ > _Backward pass of power, dispatch the CUDA key_ > _Test the gradients of hell, warn the user if they disagree_ ",2023-03-23T23:36:03Z,module: nestedtensor Merged Reverted ciflow/trunk ciflow/periodic release notes: nested tensor,closed,1,14,https://github.com/pytorch/pytorch/issues/97485," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",Why this is stale?," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m 'Sorry for reverting you change, but one of the tests test_fused_kernels_nested_broadcasting_requires_grad_failure_cuda is failing on Windows CUDA https://hud.pytorch.org/pytorch/pytorch/commit/f7ba3e85e26927ab5296737186ec32ef06c69a69' c nosignal As the PR is still in diff train and not yet landed in fbcode, I think it would be easier to revert and reland this change.  I'll add `ciflow/periodic` to run the Windows CUDA jobs.", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,Sounds good thank you  , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
695,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of StorageImpl::unsafe_data)ï¼Œ å†…å®¹æ˜¯ (distinguish mutability of StorageImpl::unsafe_data Summary: See CC(distinguish mutability of untyped StorageImpl::data() member). Test Plan: Rely on CI.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of StorageImpl::unsafe_data,distinguish mutability of StorageImpl::unsafe_data Summary: See CC(distinguish mutability of untyped StorageImpl::data() member). Test Plan: Rely on CI.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member),2023-03-23T22:22:39Z,ciflow/trunk topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/97477
348,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add PyObject preservation for UntypedStorage)ï¼Œ å†…å®¹æ˜¯ (  CC(Add PyObject preservation for UntypedStorage) Part of CC(PyObject preservation and resurrection for `StorageImpl`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add PyObject preservation for UntypedStorage,  CC(Add PyObject preservation for UntypedStorage) Part of CC(PyObject preservation and resurrection for `StorageImpl`) ,2023-03-23T21:25:28Z,module: internals open source Merged Reverted ciflow/trunk release notes: python_frontend ciflow/periodic,closed,1,16,https://github.com/pytorch/pytorch/issues/97470,"Hey , there are still a couple CI failures, I just kicked off periodic and trunk CI, and there are a couple more tests I would like to write, but I think whenever you have time to review this would be helpful",", thanks for the comments! I agree with the idea of trying to avoid duplication. I will do that as a follow up because I think there are multiple different ways to do it that might need to be discussed, and just getting preservation working in the first place was complicated enough on its own","I'm having trouble figuring out what to do about the failure in linuxbioniccuda11.7py3.10gcc7 / test (deploy, 1, 1, linux.4xlarge.nvidia.gpu) The error says:  I'll need to change that error message in `c10/core/impl/PyObjectSlot.h:113`, because it can also happen for storages, and it seems like it must actually be a storage in this case, since I didn't change anything with respect to tensor's PyObjectSlot I'm not exactly sure what multipy does, but based on its description, it seems that storages are getting used by a different interpreter than the one that created them. Since the interpreter is different from what the storage's PyObjectSlot expected, it throws this error. So potentially, we'll need some way to tell the PyObjectSlot that another interpreter might end up accessing it. Or maybe there's a better solution, I'm not sure I'm also having trouble installing and running multipy to reproduce the errorfollowing its instructions leads to various build errors for me  ","Ugh, the multipy problem is legitimately troublesome. The backstory here is that multipy lets you have multiple Python interpreters in the same process, and they may share Tensor and Storage between each other. This is problematic because PyObject preservation assumes that there is only one PyObject for a tensor, but if you share it between interpreters well... now there's two. Our compromise for this was to say that any given Tensor can only have one PyObject, and therefore you can't share them between interpreters. However, a legitimate use case for multipy is to load up weights once, and then share them across several Python interpreters. So the way we did this was that we had a single shared Storage, and then have a Tensor per interpreter. But we can't do this anymore, since we have Storage PyObject preservation now! One possible resolution is to create separate Storages per multipy interpreter. This is troublesome though because we need a nonstandard data ptr now that can do an extra level of reference counting on the data pointer itself. Blegh. I can't think of any other possible solutions.",", I listened to the podcast about torchdeploy/MultiPy to get more context. I wrote up a few notes about this problem and a few of the questions I have. I'd like to share to make sure I understand things correctly. Let me know if I'm misunderstanding anything and if you can shed some light on some of my questions  Click to expand The reason we don't have this problem with tensors is that a given tensor object itself is not actually shared across different interpreters in MultiPyeach interpreter gets its own `c10::TensorImpl` instance with its own associated PyObject that is associated with the appropriate interpreter. But each corresponding instance of a tensor points to a shared `c10::StorageImpl` instance. MultiPy is now failing to do this, because after adding PyObject preservation to UntypedStorage/StorageImpl, the PyObjectSlot in a StorageImpl assumes there is only one PyObject for a `StorageImpl`, and thus only one interpreter. Just the one interpreter is allowed to handle preservation and resurrection of a given PyObject. If a different interpreter tries to do this, PyObjectSlot throws an error. Passing Tensors and other objects between different interpreters in MultiPy is handled by IValue. IValue is a class that lets an interpreter box up a PyObject, send it to a different interpreter, and then unbox it to create another PyObject that can be used by the other interpreter. IValues are not just used for MultiPy, they are also used for serialization/deserialization in TorchScript interpretation (JIT, for example). IValue is implemented in PyTorch itself, not MultiPy. So changing how storages are shared in MultiPy will mainly require making changes in PyTorch's IValue implementation (or minimal changes in MultiPy, if any). But the question is, what should that modification do?  Edward suggests the possibility of making storages act similarly to tensors when they're shared between interpreters. We could create separate StorageImpls that each have their own different PyObjects for each interpreter, but the StorageImpls would all point to the same data. He mentions that this would require a different kind of data pointer than StorageImpl currently uses, which will track an extra level of reference counting across the different interpreters. Some questions I have/had: Could the current reference counting in DataPtr already be enough? Are we already able to create two StorageImpls that have the same DataPtr and get the correct reference counting and deallocation behavior?  * Answer: No, `DataPtr` (defined in `c10/core/Allocator.h`) uses a `UniqueVoidPtr` (defined in `c10/util/UniqueVoidPtr.h`), which is a wrapper for `std::unique_ptr`. We would need to create a nonunique DataPtr. I guess we'd probably use an `intrusive_ptr`. Edward mentions that creating such a nonunique DataPtr would be troublesome. What exactly is troublesome about it? Are there other things in PyTorch that assume that each StorageImpl has a unique DataPtr which would break if we changed that? To help me get some more context about MultiPy, what currently happens if two different interpreters inplace modify tensors that point to the same storage? Is this even allowed? If not, how is it detected? Is there some readonly flag on a tensor (I suspect not)? If it is allowed, is there any mechanism that somehow handles the race automatically (I suspect not), or is the user expected to manage or avoid races themselves (I suspect so)? * Partial answer: I see that  CC(Immutable (read-only) tensors) suggests adding readonly tensors, but looks like that has not been done yet. If making a nonunique DataPtr is overly troublesome or difficult, is there any merit in completely avoiding sharing storage data between MultiPy interpreters? Instead, we could just create copies. I realize that would give worse performance thoughmuch worse in some cases. Also, do we actually rely on the ability to share DataPtrs between interpreters, other than just for performance reasons? ","> Edward mentions that creating such a nonunique DataPtr would be troublesome. What exactly is troublesome about it? Are there other things in PyTorch that assume that each StorageImpl has a unique DataPtr which would break if we changed that? So, you can always simulate a shared pointer in a unique pointer by making a custom deleter. The custom deleter reduces the reference count (stored somewhere else) and then deallocates the data when you're done. You need to make sure you retain whatever the original deleter was. It's relatively straightforward to do, but assuming the Storage is coming from some random place you have to inplace swizzle the DataPtr, and there's probably not any APIs to do it. > To help me get some more context about MultiPy, what currently happens if two different interpreters inplace modify tensors that point to the same storage? Is this even allowed? If not, how is it detected? Is there some readonly flag on a tensor (I suspect not)? If it is allowed, is there any mechanism that somehow handles the race automatically (I suspect not), or is the user expected to manage or avoid races themselves (I suspect so)? This is ""allowed"" but not the intended use case. There's no synchronization but since you're just doing writes to float data, you'll just clobber each other hog wild style and... it will probably work out. > If making a nonunique DataPtr is overly troublesome or difficult, is there any merit in completely avoiding sharing storage data between MultiPy interpreters? Instead, we could just create copies. I realize that would give worse performance thoughmuch worse in some cases. Also, do we actually rely on the ability to share DataPtrs between interpreters, other than just for performance reasons? You don't want to copy the storage; the point of sharing is so you only need to hold one copy of the weights in memory. I think we should go with the separate storage with shared data ptr strategy.",", I think I sort of understand what to do, but let me know if any of this makes sense or not. I assume the custom deleter you're talking about is `DeleterFnPtr`, which is in the `c10::DataPtr` constructor:   So to make multiple `StorageImpl`s point to the same data buffer, the `DataPtr`s of each of those `StorageImpl`s will have to share the same special `DeleterFnPtr` instance that handles refcounting for that particular group of `StorageImpl`s that share data. The reason why each shared group of `DataPtr`s needs to have its own instance of `DeleterFnPtr` is that it needs to handle the refcount of the particular data buffer for that group. It also needs to have access to the correct deallocation function (which is itself also a `DeleterFnPtr`) for whatever device the data is allocated on. So let's say we start with a normal `StorageImpl`, ""normal"" meaning that it has its own unique data buffer and its `DataPtr` doesn't yet have this special refcounting `DeleterFnPtr`it just has the base `DeleterFnPtr` for whatever device it's on. If we want to create another `StorageImpl` that shares the same data, we'll need to first create the new refcounting `DeleterFnPtr` instance, initialize its refcount to 1 (or immediately go to 2 if we want to skip incrementing it later), give it access to the `DeleterFnPtr` that the original `StorageImpl`'s `DataPtr` was using, and reassign the original `StorageImpl`'s `DataPtr`'s `DeleterFnPtr` to this new one (APIs for this reassignment probably need to be added, and I assume this is what you meant by ""swizzling"" the `DataPtr`). Then when we create the second `StorageImpl`, we'll set its `DataPtr` to point to the same data, give it the same refcounting `DeleterFnPtr` instance that the original `StorageImpl`'s `DataPtr` now has, and increment `DeleterFnPtr` instance's refcount. I'm not fully sure what the code for this will look like yet. I'm working on a small standalone example that sort of does what I've described (https://github.com/kurtamohler/notes/blob/main/cpp/refcounted_unique_ptr/main0.cpp). I think we might need to change `DeleterFnPtr` to be a shared pointer to a functional (`std::shared_ptr>`), or a shared pointer to our own kind of functional (`std::shared_ptr`), or something like that, not just the basic function pointer (`void (*)(void*)`). That would allow the deleter for a group of shared `StorageImpl`s to have an internal state (like the refcount and the device's `DeleterFnPtr`), and the deleter functional would delete itself once the refcount goes to zero as well. Or is there a more straightforward way to do this? EDIT: Maybe it's better to do something like this, so that the shared ptr automatically handles refcounts, rather than having to explicitly handle them ```cpp using DeleterFnPtr = std::shared_ptr; class DeleterFnExample : public DeleterFn {  public:   DeleterFnExample(void* ptr, DeleterFnPtr concrete_deleter_ptr)     : ptr_(ptr),       concrete_deleter_ptr_(concrete_deleter_ptr) {}   ~DeleterFnExample() {     (*concrete_deleter_fn)(ptr_);   } };","I realized there's another way to implement this refcounted deleter function which doesn't require modifying how `UniqueVoidPtr` works at all, and we can continue using just a normal function pointer. I'm not sure that's necessarily a good thing, but it's one way to do it: https://github.com/kurtamohler/notes/blob/main/cpp/refcounted_unique_ptr/main3.cpp But I wonder if we should just modify `DataPtr` to use a shared pointer instead of making a unique pointer act like a shared pointer","> I realized there's another way to implement this refcounted deleter function which doesn't require modifying how `UniqueVoidPtr` works at all, and we can continue using just a normal function pointer. I'm not sure that's necessarily a good thing, but it's one way to do it: https://github.com/kurtamohler/notes/blob/main/cpp/refcounted_unique_ptr/main3.cpp >  > But I wonder if we should just modify `DataPtr` to use a shared pointer instead of making a unique pointer act like a shared pointer This is what I had in mind.",Note that you need to make the refcount atomic.,Ok cool! I'll go ahead with that then, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," whoops, yes we do. But probably best to just introduce StorageWeakRef directly that handles everything for users","This pull request has been **reverted** by 685505353af1c2ddc37c26b49ba2e733b7cdd704. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).","I'm making progress on fixing the issue that caused this to be reverted by CC(Back out ""Add PyObject preservation for UntypedStorage (97470)"") I got CUDA working with my local pytorch and multipy builds. I was hoping that just running `multipy/multipy/runtime/build/test_deploy_gpu` might reproduce a failure, but it did not. However the test coverage in there is pretty sparsethere's only one test that really does very much, and none of them exercises the function `c10::newStorageImplFromRefcountedDataPtr` where the Meta internal failure occurs (shown in a backtrace that Edward sent me). So I will add a test that does exercise it, and hopefully it will fail similarly. I'll try copying `TorchpyTest.ThreadedSimpleModel` (which does exercise the failing function) from https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/test_deploy.cppL163 but make it use cuda instead of cpu (BTW, I also have an idea of how to fix this just based on the trace. The error is happening when the new storage is being created and a new cuda buffer is being allocated. I'm not exactly sure why that allocation is failing, but just to guess, maybe different threads must use different cuda allocators? But either way, that allocation is not even necessary, since that new data pointer is going to be immediately replaced by the shared data pointer. However, I wonder, if it is true that the allocator cannot be used by other threads, will it be a problem when other threads try to use the deleter, when the last instance of a shared data pointer is destroyed? Hopefully not)"
1348,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(distinguish mutability of untyped StorageImpl::data() member)ï¼Œ å†…å®¹æ˜¯ (distinguish mutability of untyped StorageImpl::data() member Summary: To implement the warning when transitioning reshape to copyonwrite storage, we want to be able to detect a write to one view family following by a read or a write to another one that shares the same copyonwrite storage. Because we have historically not been strict about the mutability of our data pointers, any warning we have would likely be far too aggressive. Therefore, this is the first PR in a long series to ensure a strict distinction between mutable and const data accessors in TensorBase, TensorImpl, Storage, and StorageImpl. The rough plan is to give the mutable accessor a new name that is explicit about mutation, this will also force us to rewrite any code that really needs a mutation. Test Plan: Rely on CI.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,distinguish mutability of untyped StorageImpl::data() member,"distinguish mutability of untyped StorageImpl::data() member Summary: To implement the warning when transitioning reshape to copyonwrite storage, we want to be able to detect a write to one view family following by a read or a write to another one that shares the same copyonwrite storage. Because we have historically not been strict about the mutability of our data pointers, any warning we have would likely be far too aggressive. Therefore, this is the first PR in a long series to ensure a strict distinction between mutable and const data accessors in TensorBase, TensorImpl, Storage, and StorageImpl. The rough plan is to give the mutable accessor a new name that is explicit about mutation, this will also force us to rewrite any code that really needs a mutation. Test Plan: Rely on CI.  Stack created with Sapling. Best reviewed with ReviewStack.  CC(distinguish mutability of StorageImpl::data_ptr() member)  CC(distinguish mutability of typed StorageImpl::data() member)  CC(distinguish mutability of StorageImpl::unsafe_data)  CC(distinguish mutability of untyped StorageImpl::data() member)",2023-03-23T20:14:14Z,ciflow/trunk topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/97461
295,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Only warn once for TypedStorage deprecation)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(repeated warning `UserWarning: TypedStorage is deprecated`))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Only warn once for TypedStorage deprecation,Fixes CC(repeated warning `UserWarning: TypedStorage is deprecated`),2023-03-22T21:44:12Z,open source Merged ciflow/trunk release notes: python_frontend,closed,0,6,https://github.com/pytorch/pytorch/issues/97379, merge," Merge failed **Reason**: This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. To add a label, you can comment to pytorchbot, for example ` label ""topic: not user facing""` For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork. Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," I think the fix has a flaky unicodedecoding error https://hud.pytorch.org/failure/test_torch.py%3A%3ATestTorch%3A%3Atest_typed_storage_deprecation_warning, i.e.  I have temporarily disabled the test in  CC(DISABLED test_typed_storage_deprecation_warning (__main__.TestTorch)).  Could you help take a look at the error?","It seems that https://github.com/pytorch/pytorch/pull/97628 has fixed the issue, so I have resolved the ticket to let the test run."
2028,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA)ï¼Œ å†…å®¹æ˜¯ (Summary: This fixes an issue raised in is_causal parameter in torch.nn.TransformerEncoderLayer.forward does not work CC(`is_causal` parameter in torch.nn.TransformerEncoderLayer.forward does not work)) where results computed with is_causal do not properly reflect causal masking. In PyTorch 2.0, Accelerated PT Transformers added the is_causal parameter to legacy nn.Transformer* and nn.MHA APIs aligned with and intended to engage the is_causal parameter of the new scaled_dot_product_attention (SDPA) operator. At present is_causal works differently for Transformer* modules, compared to the nn.MHA and F.MHA: * The nn.Transformer* modules treat is_causal as an optional indicator about the format of attn_mask. This is because some layers (such as the CLIP layer use the attention mask in the layer, and thus the attn_mask was a required feature.) * Initially, nn.MHA and F.MHA were defined to align with F.SDPA in behavior: a user may specify either the attention mask, or is_causal, but not both.  It seemed to make sense at the time to align SDPA and MHA, esp since there was a larger overlap of parameters which have since changed, e.g., with the removal of need_weights from SDPA. (See below for why this makes sense.) Unfortunately, this does not work because of how MHA was changed to handle the need_weights parameter.  When need_weights is present, we do not (any more) call SDPA because support for need_weights was removed from SDPA before the release.  The rationale is that need_weights defeats all optimization at the foundation of SDPA performance.  Having the flag might thus mislead users into thinking they get good performance and have them disappointed when they enable a legacy feature of MHA which massively degrades p)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Better Transformer] make is_causal a hint and force attn_mask to be set on `is_causal=True` in F.MHA,"Summary: This fixes an issue raised in is_causal parameter in torch.nn.TransformerEncoderLayer.forward does not work CC(`is_causal` parameter in torch.nn.TransformerEncoderLayer.forward does not work)) where results computed with is_causal do not properly reflect causal masking. In PyTorch 2.0, Accelerated PT Transformers added the is_causal parameter to legacy nn.Transformer* and nn.MHA APIs aligned with and intended to engage the is_causal parameter of the new scaled_dot_product_attention (SDPA) operator. At present is_causal works differently for Transformer* modules, compared to the nn.MHA and F.MHA: * The nn.Transformer* modules treat is_causal as an optional indicator about the format of attn_mask. This is because some layers (such as the CLIP layer use the attention mask in the layer, and thus the attn_mask was a required feature.) * Initially, nn.MHA and F.MHA were defined to align with F.SDPA in behavior: a user may specify either the attention mask, or is_causal, but not both.  It seemed to make sense at the time to align SDPA and MHA, esp since there was a larger overlap of parameters which have since changed, e.g., with the removal of need_weights from SDPA. (See below for why this makes sense.) Unfortunately, this does not work because of how MHA was changed to handle the need_weights parameter.  When need_weights is present, we do not (any more) call SDPA because support for need_weights was removed from SDPA before the release.  The rationale is that need_weights defeats all optimization at the foundation of SDPA performance.  Having the flag might thus mislead users into thinking they get good performance and have them disappointed when they enable a legacy feature of MHA which massively degrades p",2023-03-21T04:42:37Z,fb-exported Merged ciflow/trunk release notes: nn,closed,1,32,https://github.com/pytorch/pytorch/issues/97214,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,I'm a bit confused by this change.  Could you add a more detailed description on what is the motivation to do this?  Isn't this a BCbreaking change? We should follow our bc policy.  Also there are quite a few unrelated comments left? We should make sure to keep the code clean and follow high level design.,"> I'm a bit confused by this change. >  > * Could you add a more detailed description on what is the motivation to do this? > * Isn't this a BCbreaking change? We should follow our bc policy. This is still exploratory / experimental, and one possible way to resolve Issue CC(`is_causal` parameter in torch.nn.TransformerEncoderLayer.forward does not work). A change here is not backward breaking because the combination of parameters addressed here currently doesn't work in the API that is being changed here, as per the issue.  ",> This is still exploratory / experimental Aren't `nn.Transformer` and associate stable features?,"> > This is still exploratory / experimental >  > Aren't `nn.Transformer` and associate stable features? The reference was to this PR, not to the API.  I wanted to establish that we can pass the full set of tests before writing the rather lenghty summary of analysis and solutions.",This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725,This pull request was **exported** from Phabricator. Differential Revision: D44245725, merge,This pull request was **exported** from Phabricator. Differential Revision: D44245725
827,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(repeated warning `UserWarning: TypedStorage is deprecated`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I noticed that the warning `UserWarning: TypedStorage is deprecated` that results from calling `Tensor.storage()` is issued repeatedly, even though the documentation about `torch.set_warn_always` says that by default pytorch warnings should only be issued once per process. While modifying my code to call `untyped_storage()` instead of `storage()` could get rid of the excessive warnings, I would prefer not to since I want to keep my code compatible with pre2.0.0 versions for now, and `untyped_storage()` is only available >=2.0.0. Example:  Output:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,repeated warning `UserWarning: TypedStorage is deprecated`," ğŸ› Describe the bug I noticed that the warning `UserWarning: TypedStorage is deprecated` that results from calling `Tensor.storage()` is issued repeatedly, even though the documentation about `torch.set_warn_always` says that by default pytorch warnings should only be issued once per process. While modifying my code to call `untyped_storage()` instead of `storage()` could get rid of the excessive warnings, I would prefer not to since I want to keep my code compatible with pre2.0.0 versions for now, and `untyped_storage()` is only available >=2.0.0. Example:  Output:   Versions  ",2023-03-21T02:00:25Z,triaged module: python frontend,closed,4,7,https://github.com/pytorch/pytorch/issues/97207,"? As a temporary workaround, you can silent this warning with regular python warning filter: `python W ignore::UserWarning: foo.py`","The complication is that the warning message contains the line of code that triggered the warning, with `warnings.warn(..., stacklevel=...)`: https://github.com/pytorch/pytorch/blob/0b094ca37f6ac0d3c1899653ea069dbaaf33176b/torch/storage.pyL365L372 So one option is to remove the `stacklevel` arg. But having the stack in the message is helpful for finding where the warning came from (https://github.com/pytorch/pytorch/pull/89867) Another option is to only emit the warning the first time `_warn_typed_storage_removal()` gets called. For testing purposes, we could have a flag to make it always warn. Does that seem reasonable?  ","Yeah, let's just have this warn only once and never again. If you're trying to scrub these, you can comment it out","Hi! Was this issue already solved? I tried to figure it out based on the PRs, but it seems none of them have been merged / released. Is there any suggested way of silencing it? Thanks!",", CC(Only warn once for TypedStorage deprecation) has been merged to fix this issue, but it hasn't been included in a new release yet Until it's released, you could use the workaround mentioned here:  CC(repeated warning `UserWarning: TypedStorage is deprecated`)issuecomment1478692625 But that will silence all `UserWarning`s. If you only want to silence the TypedStorage deprecation warnings, you'd have to do something like this: ","Hi there, I was wondering about the implications/measures to take. Is it normal behavior, is pytorch gracefuly reverting to the new UntypedStorage class or not?  WHile running a streamlit web server locally I am good, but that's one of my currently possible reason for crash when deploying the app on streamlit.io. Got torch==2.0.0 from pip... Let me know if more info are relevant, tks:",", it sounds like you're experiencing a crash related to UntypedStorage/TypedStorage. Could you please open a new issue and ? https://github.com/pytorch/pytorch/issues/new?assignees=&labels=&template=bugreport.yml"
1874,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] coordinate descent tuning upon max-autotune)ï¼Œ å†…å®¹æ˜¯ (  CC([inductor] add the ability to do heavier search for coordinate descent tuning)  CC([inductor] coordinate descent tuning upon maxautotune)  CC([inductor][easy] create a wrap for triton do_bench function) Command to run max autotune baseline:  Command to do coordinate descent autotuning:   Explanation of the envvars show up on the command:  Here are my experiments results for around 40 torchbench models: https://docs.google.com/spreadsheets/d/1G7i2whIf8YuHhN_WovNxwcEiFDSAw4x3NK4uL4XhI/editgid=0  Some highlights  We improve 2.2% further upon maxautotune on average (geomean)  timm_resnest benefits most from coordinate descent tuning. There is 1.07x speedup  We have descent speedup on transformer models    BERT_pytorch:  1.056x    timm_vision_transformer: 1.04x    hf_Bert: 1.030x  For resnet models, it looks like we have less gain as model get larger. My guess is larger model spend more time on mm/conv, so our tuning for pointwise/reduction helps less    resnet18: 1.021x    resnet50: 1.014x    resnet152: 1.005x This kind of coordinate descent autotuning can give us 'upper bound' of the gain we can get for tuning configs for pointwise/reduction. On the other hand, by spot checking, we roughly double the compilation time compared to maxautotune. Next steps can be  we disable persistent reduction in coordinate descent autotune (it's still enabled in baseline) so we can tune RBLOCK for reduction. We can also try to use autotune to pick persistent reduction or not.  pick good config without benchmarking (e.g. Natalia mentioned checking register spill)  try the idea on matmul so we know what's the potential there. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[inductor] coordinate descent tuning upon max-autotune,"  CC([inductor] add the ability to do heavier search for coordinate descent tuning)  CC([inductor] coordinate descent tuning upon maxautotune)  CC([inductor][easy] create a wrap for triton do_bench function) Command to run max autotune baseline:  Command to do coordinate descent autotuning:   Explanation of the envvars show up on the command:  Here are my experiments results for around 40 torchbench models: https://docs.google.com/spreadsheets/d/1G7i2whIf8YuHhN_WovNxwcEiFDSAw4x3NK4uL4XhI/editgid=0  Some highlights  We improve 2.2% further upon maxautotune on average (geomean)  timm_resnest benefits most from coordinate descent tuning. There is 1.07x speedup  We have descent speedup on transformer models    BERT_pytorch:  1.056x    timm_vision_transformer: 1.04x    hf_Bert: 1.030x  For resnet models, it looks like we have less gain as model get larger. My guess is larger model spend more time on mm/conv, so our tuning for pointwise/reduction helps less    resnet18: 1.021x    resnet50: 1.014x    resnet152: 1.005x This kind of coordinate descent autotuning can give us 'upper bound' of the gain we can get for tuning configs for pointwise/reduction. On the other hand, by spot checking, we roughly double the compilation time compared to maxautotune. Next steps can be  we disable persistent reduction in coordinate descent autotune (it's still enabled in baseline) so we can tune RBLOCK for reduction. We can also try to use autotune to pick persistent reduction or not.  pick good config without benchmarking (e.g. Natalia mentioned checking register spill)  try the idea on matmul so we know what's the potential there. ",2023-03-21T01:10:04Z,Merged Reverted ciflow/trunk topic: not user facing module: inductor ciflow/inductor merging,closed,0,17,https://github.com/pytorch/pytorch/issues/97203,"Can you try on current master? Some alignment errors have been fixed in triton, and I've landed new triton pin today ","> Can you try on current master? Some alignment errors have been fixed in triton, and I've landed new triton pin today After rebasing, I seem to pass the place where it fails previously for hf_Bert training. But now I get   Maybe some config we tried is too crazy. I'll try to print what config it is. But before that, I think I need add the best choice found by coordinate descent into cache so we don't need start over every time. That can make us iterate much faster.",I think it would be ideal if we could just capture the kernels out separately so we don't need to run things model by model. But up to you.,"> I think it would be ideal if we could just capture the kernels out separately so we don't need to run things model by model. But up to you. yea, before trying this on a real model, I actually tried it on the hf_Bert kernel you showed me. The algorithm actually found the same best config we found manually.","> I think it would be ideal if we could just capture the kernels out separately so we don't need to run things model by model. But up to you. On the other hand, this algorithm assumes the system as a block box. But it would be nice that we capture the kernels that is speed up most and verify how the config change affect the perf. I've see quite a few kernels being optimized more than 5x in hf_Bert training before the crash happens.    EDIT: the above number is misleading since this is a persistent_reduction kernel and we should not tune RBLOCK for it. Thanks   for pointing out.","Can you describe what happens when  1) both max_autotune and coordinate_descent are on 2) max_autotune is off, but coordinate_descent is on","yep. > 1. both max_autotune and coordinate_descent are on > 2. max_autotune is off, but coordinate_descent is on for the above 2 scenarios, the only difference is the starting config for coordinate_descent tuning. E.g., assuming regular autotune only get one config C1; while maxautotune get 4 configs C1, C2, C3, C4 and maxautotune figure out C3 is the best. Then in scenario 1 you mentioned. coordinate descent tuning will start from C1; while in scenario 2, coordinate descent tuning will start from C3","Great, can you leave this comment in `cached_autotune`? ","> The way PR is structured it's a bit hard to make sure that this doesn't change behavior and caching when coordinate descent is not enabled. Can you double check that caching for noautotuning/no coordinate descent still works as before? Testing on a specific kernel is much easier compared to testing on a whole model. So I tested on this kernel: https://gist.github.com/shunting314/80432018df55aab48444429ad971e176  (named k.py)  out of box behavior run `python k.py`. Cache is skipped since this kernel only has 1 config by default.  max autotune run `TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py`, the cache metadata file contains:  It contains a config_hash for all the configs considered and the metadata for the picked config. It also tells this best config is not found by coordinate descent search.  run coordinate descent tuning upon maxautotune run `TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py`, the cache metadata file contains:  We can see that `configs_hash` is the same as before since we are based on the same maxautotune configs. We see found_by_coordesc is true since we have run coordinate descent tuning. We see coordinate descent tuning find a better config (1.127x faster according to log)  run maxautotune without coordinate descent tuning again The cache metadata recovers to   Note that the config found by coordinate descent tuning previously is dropped since it does not match any of the candidate config that maxautotune check. In my tests, I usually use a different inductor cache directory when testing coordinate descent tuning to avoid the pingpong effect:  running coordinate descent tuning overwrite the warmed up maxautotune cache  running maxautotune overwrite the warmed up coordinate descent tuning cache"," merge f ""accuracy check failures are unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m 'Sorry for reverting your PR, but it breaks MacOS test in trunk' c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,  any idea why I don't see the breakage of MacOS test on github CI? I only see 2 test failures due to accuracy check which are unrelated.,Is the MacOS tests triggered after I submit the merge request? I can see the failure now but I didn't see it before I submit merge request.,"This PR also broke Windows tests, see https://github.com/pytorch/pytorch/actions/runs/4738152877/jobs/8414630039 "
440,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(eliminate typed data in Storage and StorageImpl)ï¼Œ å†…å®¹æ˜¯ (eliminate typed data in Storage and StorageImpl Summary: Storage doesn't need to expose this, we can just move the cast up to the tensor impl. Test Plan: Rely on CI. Reviewers: ezyang Subscribers: Tasks: Tags:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,eliminate typed data in Storage and StorageImpl,"eliminate typed data in Storage and StorageImpl Summary: Storage doesn't need to expose this, we can just move the cast up to the tensor impl. Test Plan: Rely on CI. Reviewers: ezyang Subscribers: Tasks: Tags:",2023-03-20T22:31:36Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/97192," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
337,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `is_causal` API for `TransformerDecoder`)ï¼Œ å†…å®¹æ˜¯ (The same API is implemented for `TransformerEncoder`, where this argument is passed through to the sublayers.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add `is_causal` API for `TransformerDecoder`,"The same API is implemented for `TransformerEncoder`, where this argument is passed through to the sublayers.",2023-03-20T18:23:32Z,triaged open source Merged,closed,0,10,https://github.com/pytorch/pytorch/issues/97166," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", should this also cover `class Decoder`?,"Sorry, I think my message was unclear. I meant that the API is already implemented for `TransformerEncoder`; this just implements the same thing for `TransformerDecoder`. Did I understand you correctly?",Thereâ€™s a top level transformer module as well. My question referred to that module. (`class Transformer(Module):` starting at L17 of trnasformer.py) (This can be a separate PR if you want to submit that.)," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," merge f ""updated diff"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," can you please submit a PR that updates the definition of *_is_causal in docstrings, based on the change in https://github.com/pytorch/pytorch/pull/97214 which replaces the mutual exclusivity and makes *_is_causal a hint.  (Bad things happen when the hint incorrectly asserts the attention mask is causal, when it is not.  The other direction works fine when is_causal is False but the matrix actually contains a causal mask)"," Thanks for merging already. :) I'll gladly do both things you mentioned, updating the docstring and implementing the API for `Transformer` as well. Cheers!","ğŸ‘ Get Outlook for iOS ________________________________ From: janEbert ***@***.***> Sent: Friday, March 24, 2023 11:28:33 PM To: pytorch/pytorch ***@***.***> Cc: Michael Gschwind ***@***.***>; Mention ***@***.***> Subject: Re: [pytorch/pytorch] Add `is_causal` API for `TransformerDecoder` (PR CC(Add `is_causal` API for `TransformerDecoder`))  Thanks for merging already. :) I'll gladly do both things you mentioned, updating the docstring and implementing the API for Transformer as well. Cheers! â€” Reply to this email directly, view it on GitHub, or unsubscribe.â€ŠYou ZjQcmQRYFpfptBannerStart This Message Is From an External Sender ZjQcmQRYFpfptBannerEnd  Thanks for merging already. :) I'll gladly do both things you mentioned, updating the docstring and implementing the API for Transformer as well. Cheers! â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>"
466,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Problem with Hugging Face model that is not in training loop)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I use the hugging face transformers and the DistilBert specifically. However, torch.compile fails because of it. The code associated with the bug:   Error logs   Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Problem with Hugging Face model that is not in training loop," ğŸ› Describe the bug I use the hugging face transformers and the DistilBert specifically. However, torch.compile fails because of it. The code associated with the bug:   Error logs   Minified repro _No response_  Versions  ",2023-03-20T11:35:12Z,triaged bug oncall: pt2 oncall: cpu inductor,closed,0,7,https://github.com/pytorch/pytorch/issues/97146,This looks like an issue with the CPU compiler stack. Does something simple like  work?,"This does work, and after a restart which updated my GPU driver , the error changes... Here is the relevant code:  Here is the error log:  This code snipped transforms some elements of the input. Maybe i shouldn't do this as torch compile aims to the creation of a static graph?",The original error (`Python.h: No such file or directory`) seems like a duplicate of  CC( PyTorch 2.0 torch.compile works on CPU but not GPU under WSL 2) nik your last message perhaps only includes part of the log? Could you verify there's no `The above exception was the direct cause of the following exception` line right above the cut?,Indeed this is the beginning of my error log ,"So just as in  CC( PyTorch 2.0 torch.compile works on CPU but not GPU under WSL 2), [`sysconfig.get_paths()[""include""]`](https://github.com/openai/triton/blob/bd5c2117f62c73a9e922d5e93353a39ab3ac269b/python/triton/compiler.pyL1387) resolves into `/usr/include/python3.10`, but the headers (`Python.h`) are not present there (have you got any ""dev"" package for python installed?) Thus, I think, we should track this issue in `openai/triton` together with  CC(torch.compile model fails to run on CPU and CUDA)issuecomment1480538003",Is this still an issue? Can I close?,"Looks like a env issue, feel free to reopen if any further issue."
767,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: NYI: Named tensors are not supported with the tracer)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Bug While running the following script, I got the error: ""RuntimeError: NYI: Named tensors are not supported with the tracer""  refer the onnxmodel mnist1 To Reproduce  Expected behavior No error Environment PyTorch Version (e.g., 1.0): 1.10 OS (e.g., Linux): Ubuntu 18.04 How you installed PyTorch (conda, pip, source): conda Build command you used (if compiling from source): Python version: 3.7.13 CUDA/cuDNN version: 11.2 GPU models and configuration: Any other relevant information:  Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,RuntimeError: NYI: Named tensors are not supported with the tracer," ğŸ› Describe the bug Bug While running the following script, I got the error: ""RuntimeError: NYI: Named tensors are not supported with the tracer""  refer the onnxmodel mnist1 To Reproduce  Expected behavior No error Environment PyTorch Version (e.g., 1.0): 1.10 OS (e.g., Linux): Ubuntu 18.04 How you installed PyTorch (conda, pip, source): conda Build command you used (if compiling from source): Python version: 3.7.13 CUDA/cuDNN version: 11.2 GPU models and configuration: Any other relevant information:  Versions ",2023-03-20T08:03:04Z,module: onnx triaged,closed,1,2,https://github.com/pytorch/pytorch/issues/97138, how to solve the problem?,`torch.onnx.export` is in maintenance mode and we don't plan to add new operators/features or fix complex issues. Please try the new ONNX exporter and reopen this issue with a full repro if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial
1981,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Errors using torch.compile() on Megatron-LM GPT model )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug enable torch2 on megatronlm get model !image  training script like:  Got such error:   Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.0 Libc version: glibc2.31 Python version: 3.8.13  (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64bit runtime) Python platform: Linux5.4.0113genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB Nvidia driver version: 470.129.06 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   43 bits physical, 48 bits virtual CPU(s):                          256 Online CPU(s) list:             0255 Thread(s) per core:              2 Core(s) per socket:              64 Socket(s):                       2 NUMA node(s):                    8 Ve)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Errors using torch.compile() on Megatron-LM GPT model ," ğŸ› Describe the bug enable torch2 on megatronlm get model !image  training script like:  Got such error:   Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.26.0 Libc version: glibc2.31 Python version: 3.8.13  (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64bit runtime) Python platform: Linux5.4.0113genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB Nvidia driver version: 470.129.06 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   43 bits physical, 48 bits virtual CPU(s):                          256 Online CPU(s) list:             0255 Thread(s) per core:              2 Core(s) per socket:              64 Socket(s):                       2 NUMA node(s):                    8 Ve",2023-03-20T07:36:43Z,triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/97137,Can you confirm the model runs without error with eager? Seems like some sort of broadcasting problem.,Should be fixed by CC([Inductor] index_put  unsqueeze indices[0] if self and indices[0] are not broadcastable) I hope,"Closing, as this should be fixed by https://github.com/pytorch/pytorch/pull/97105  reopen if otherwise."
2011,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inconsistent results when mutating a tensor with shared storage in a nested function)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi! I found the next snippet will generate different results before/after compilation. It exists for both CPU and CUDA backend. I'm not sure but it might be related to CC(Inconsitent results before/after compilation for squeeze + tensor mutation + if statement) and CC([Dynamo] symbolic_convert returns ValueError: Cell is empty).   Error logs _No response_  Minified repro _No response_  Versions  PyTorch version: 2.1.0a0+gitfe05266 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.25.2 Libc version: glibc2.31 Python version: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0] (64bit runtime) Python platform: Linux5.15.067genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.112 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070 Nvidia driver version: 510.108.03 cuDNN version: Probably one of the following: /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_adv_train.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_ops_train.so.8.2.4 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn.so.8.6.0 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.6.0 /usr/local/cuda11.6/targets/x86_64lin)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Inconsistent results when mutating a tensor with shared storage in a nested function," ğŸ› Describe the bug Hi! I found the next snippet will generate different results before/after compilation. It exists for both CPU and CUDA backend. I'm not sure but it might be related to CC(Inconsitent results before/after compilation for squeeze + tensor mutation + if statement) and CC([Dynamo] symbolic_convert returns ValueError: Cell is empty).   Error logs _No response_  Minified repro _No response_  Versions  PyTorch version: 2.1.0a0+gitfe05266 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.25.2 Libc version: glibc2.31 Python version: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0] (64bit runtime) Python platform: Linux5.15.067genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.112 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070 Nvidia driver version: 510.108.03 cuDNN version: Probably one of the following: /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_adv_train.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.2.4 /usr/local/cuda11.3/targets/x86_64linux/lib/libcudnn_ops_train.so.8.2.4 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn.so.8.6.0 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.6.0 /usr/local/cuda11.6/targets/x86_64lin",2023-03-20T06:25:37Z,triaged oncall: pt2 module: inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/97130,", looks like mutation is not copied back?",Workaround is to replace reshape with view. So it is definitely reshape shenanigans. .,Note that this code still works under CoW reshape because a is never referenced after mutation.,"Actually, the script as is works on master 85112b0dccb2e2d2f3cd568ceffc2dda850d5756 .  can you check a more recent nightly?","  Oops, I made a mistake copying the code :(. It should be  I tested on the newest master branch and the problem can be reproduced. Apologize for the mistake...",Also repros with view() now.,I see a `copy_()` in the graph that aot_autograd sends to the backend. I also confirmed that this passes with `backend='aot_eager'`.,"Looks like inductor isn't handling the `copy_()` well. Here's the aot_autograd FX graph, and the output inductor code: FX graph:  inductor code: ",Most recent repro is no longer giving incorrect result  both function calls give the same output.
2007,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Certain configurations of `MultiHeadAttention` fail during inference, because of issues in the fast execution path. These need to be added to the fast path, or else handled gracefully (i.e. revert to slow path).  The following configurations break: 1. `batch_first=True`, `bias=False` 2. `batch_first=True`, `num_heads` is odd, and `key_padding_mask` provided  Example 1  Yields,   Example 2  Yields,   Versions PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01029gcpx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB Nvidia driver version: 495.29.05 cuDNN version: Probably one of the following: /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_adv_train.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_ops_train.so.8.3.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CP)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads"," ğŸ› Describe the bug Certain configurations of `MultiHeadAttention` fail during inference, because of issues in the fast execution path. These need to be added to the fast path, or else handled gracefully (i.e. revert to slow path).  The following configurations break: 1. `batch_first=True`, `bias=False` 2. `batch_first=True`, `num_heads` is odd, and `key_padding_mask` provided  Example 1  Yields,   Example 2  Yields,   Versions PyTorch version: 2.0.0 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01029gcpx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB Nvidia driver version: 495.29.05 cuDNN version: Probably one of the following: /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_adv_train.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.3.1 /usr/local/cuda11.5/targets/x86_64linux/lib/libcudnn_ops_train.so.8.3.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CP",2023-03-20T06:01:36Z,module: nn triaged,open,2,6,https://github.com/pytorch/pytorch/issues/97128, ,fix candidate D45161767,"What is the status update on this issue? Using a single attention head fails during inference as of September 28, 2023.","> What is the status update on this issue? Using a single attention head fails during inference as of September 28, 2023. Did you use today's nightlies?  (We don't update back level versions, if a numbered version is broken, it will only be fixed in the next release. Can you pass the test from CC(Deselect odd numbered heads from nn.MHA fastpath) ? if that test fails, you have the wrong version. if tha tyest passes, and you have another issue provide a repro that fails? ","Having an uneven number of heads was giving me the same error on torch==2.0, but worked after upgrading to torch==2.1","> Having an uneven number of heads was giving me the same error on torch==2.0, but worked after upgrading to torch==2.1 same here, using uneven number of heads and it's giving me `RuntimeError: Only support when num_heads is even in transformer` and I use release/2.0 branch. The latest release/2.0 branch does not have this fix, after I patch 2.0 with CC(Deselect odd numbered heads from nn.MHA fastpath) then it works. The latest release/2.1 branch and main branch have the fix and work. "
1993,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo+XLA] lowering error with Bert model and multi-batch inputs)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  I tested `torch.compile` with `aot_torchxla_trace_once` backend and met the following error with `Bert` model and input `(2,512)`. I can run same input size with eager mode and inductor backend.  reproduce script:   Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01031awsx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          48 Online CPU(s) list:             047 Thread(s) per core:              2 Core(s) per socket:              24 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       AuthenticAMD CPU family:                      23 Model:                           49 Model name:                      AMD EPYC 7R32 Stepping:                        0 CPU MHz:                         2800.000 BogoMIPS:                        56)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[dynamo+XLA] lowering error with Bert model and multi-batch inputs," ğŸ› Describe the bug  I tested `torch.compile` with `aot_torchxla_trace_once` backend and met the following error with `Bert` model and input `(2,512)`. I can run same input size with eager mode and inductor backend.  reproduce script:   Versions Collecting environment information... PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01031awsx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          48 Online CPU(s) list:             047 Thread(s) per core:              2 Core(s) per socket:              24 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       AuthenticAMD CPU family:                      23 Model:                           49 Model name:                      AMD EPYC 7R32 Stepping:                        0 CPU MHz:                         2800.000 BogoMIPS:                        56",2023-03-20T02:25:46Z,triaged module: xla oncall: pt2,closed,0,9,https://github.com/pytorch/pytorch/issues/97121, Do you know if anybody is maintaining this backend?," /   which backend name is used for the dynamo XLA bridge? (Is it this one, `aot_torchxla_trace_once`?)",i think aot_ is the training backend that is WIP and there is another one that is stable for inference.  I think we can assign this to the torchxla team unless  has a line of sight on that as_strided backtrace and wants to keep it.,"No immediate ideas just from that backtrace  it looks like as_strided is present in the graph, and XLA doesn't support that flavor of arguments to as_strided. Assigning to XLA sgtm",jian It looks like you are doing inference. Can you try 'torchxla_trace_once' backend to see if it works? Here is the dynamo user guide for XLA: https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md,using backend `torchxla_trace_once` gives following error: ,Sorry for not getting back to the issue on time. jian you have it working somehow?,jian I'm also getting that `assert len(args) > 0   can not handle no args case for now` error. Did you figure that out?,"following up on this issue  has this problem been resolved on your end, jian?"
863,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Original post.  **THIS ONLY HAPPENS WHEN `enable_nested_tensor=True`.** When `src_key_padding_mask` ([N, K]) has one column (k) all with value `True`, `TransformerEncoder` will remove this column in the output, causing inconsistency issues.  Minimal example: Class init  Forward  Is this an expected behavior by design?  If so, I don't think this is a good practice because oftentimes we want aggregation on the sequence level, and this kind of removal would make the use of scatter aggregation functions, e.g. `scatter_mean` difficult.  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch," ğŸ› Describe the bug Original post.  **THIS ONLY HAPPENS WHEN `enable_nested_tensor=True`.** When `src_key_padding_mask` ([N, K]) has one column (k) all with value `True`, `TransformerEncoder` will remove this column in the output, causing inconsistency issues.  Minimal example: Class init  Forward  Is this an expected behavior by design?  If so, I don't think this is a good practice because oftentimes we want aggregation on the sequence level, and this kind of removal would make the use of scatter aggregation functions, e.g. `scatter_mean` difficult.  Versions  ",2023-03-19T16:18:44Z,module: nn triaged module: nestedtensor,open,0,11,https://github.com/pytorch/pytorch/issues/97111, ,  please review D45167874 as possible fix ,I tried to run the script you added:  And I get the following error:  Are you sure that the repro script is correct?,Just to confirm that this issue still persists in torch 2.0.1 when using `eval()` and `no_grad()`: ,"This should be fixed in nightlies. Please run this repro against current nightlies? Get Outlook for iOS ________________________________ From: Martijn Schuemie ***@***.***> Sent: Friday, August 25, 2023 9:51:01 PM To: pytorch/pytorch ***@***.***> Cc: Michael Gschwind ***@***.***>; Mention ***@***.***> Subject: Re: [pytorch/pytorch] TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch (Issue CC(TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch)) Just to confirm that this issue still persists in torch 2.â€Š0.â€Š1 when using eval() and no_grad(): from torch import nn d_model = 32 nhead = 4 dim_feedforward = 48 num_layers = 2 batch_size = 3 seq_length = 5 layer = nn.â€ŠTransformerEncoderLayer(d_model=32, ZjQcmQRYFpfptBannerStart This Message Is From an External Sender ZjQcmQRYFpfptBannerEnd Just to confirm that this issue still persists in torch 2.0.1 when using eval() and no_grad(): from torch import nn d_model = 32 nhead = 4 dim_feedforward = 48 num_layers = 2 batch_size = 3 seq_length = 5 layer = nn.TransformerEncoderLayer(d_model=32, nhead=4, batch_first=True, dim_feedforward=dim_feedforward) xencoder = nn.TransformerEncoder(layer, num_layers=num_layers) x = torch.rand(batch_size, seq_length, d_model) mask = torch.zeros(batch_size, seq_length, dtype=bool) xencoder.train() print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 5, 32]) xencoder.eval() print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 5, 32]) xencoder.train() with torch.no_grad():     print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 5, 32]) xencoder.eval() with torch.no_grad():     print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 5, 32])  Set last mask to True for all batches: for i in range(batch_size):     mask[i, 1] = True xencoder.train() print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 5, 32]) xencoder.eval() print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 5, 32]) xencoder.train() with torch.no_grad():     print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 5, 32]) xencoder.eval() with torch.no_grad():     print(xencoder(x, src_key_padding_mask=mask).shape)  torch.Size([3, 4, 32])  Last token is truncated â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>","This issue does not occur with today's nightly, so it appears to be fixed. Thank you!","Hello, could you share with commit fixed this issue? thank you.","Hello, This issue still happens to me, is it solved? Thanks!","Bug still persists with torch 2.0.1, can be solved temporarily as mentioned by setting `enable_nested_tensor=False`","> I tried to run the script you added: >  >  >  > And I get the following error: >  >  >  > Are you sure that the repro script is correct? Sorry for getting back to you late  should be `x = torch.randn(6, 2000, 256)` instead of `x = torch.randn(2000, 6, 256)`. As mentioned before `batch_first=True` is ok.","Please retest with PyTorch 2.1. Get Outlook for iOS ________________________________ From: Yepeng ***@***.***> Sent: Tuesday, October 3, 2023 11:42:34 AM To: pytorch/pytorch ***@***.***> Cc: Michael Gschwind ***@***.***>; Mention ***@***.***> Subject: Re: [pytorch/pytorch] TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch (Issue CC(TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch)) I tried to run the script you added: import torch import torch.â€Šnn as nn encoder_layer = nn.â€ŠTransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False) transformer_encoder = nn.â€ŠTransformerEncoder(encoder_layer, ZjQcmQRYFpfptBannerStart This Message Is From an External Sender ZjQcmQRYFpfptBannerEnd I tried to run the script you added: import torch import torch.nn as nn encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False) transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True) x = torch.randn(2000, 6, 256) mask = torch.ones(2000, 6) mask[:, 0] = 0   here I masked 5 columns instead of just one mask = mask.bool() out = transformer_encoder(src=x, src_key_padding_mask=mask) assert out.shape[1] == 6   Error, the actual dim is only 1 And I get the following error: Traceback (most recent call last):   File ""/scratch/drisspg/work/pytorch/../scripts/junk_pile/rando.py"", line 12, in      out = transformer_encoder(src = x, src_key_padding_mask = mask)   File ""/scratch/drisspg/work/pytorch/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/scratch/drisspg/work/pytorch/torch/nn/modules/transformer.py"", line 315, in forward     output = mod(output, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)   File ""/scratch/drisspg/work/pytorch/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/scratch/drisspg/work/pytorch/torch/nn/modules/transformer.py"", line 598, in forward     x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))   File ""/scratch/drisspg/work/pytorch/torch/nn/modules/transformer.py"", line 606, in _sa_block     x = self.self_attn(x, x, x,   File ""/scratch/drisspg/work/pytorch/torch/nn/modules/module.py"", line 1501, in _call_impl     return forward_call(*args, **kwargs)   File ""/scratch/drisspg/work/pytorch/torch/nn/modules/activation.py"", line 1227, in forward     attn_output, attn_output_weights = F.multi_head_attention_forward(   File ""/scratch/drisspg/work/pytorch/torch/nn/functional.py"", line 5341, in multi_head_attention_forward     assert key_padding_mask.shape == (bsz, src_len), \ AssertionError: expecting key_padding_mask shape of (6, 2000), but got torch.Size([2000, 6]) Are you sure that the repro script is correct? Sorry for getting back to you late  should be x = torch.randn(6, 2000, 256) instead of x = torch.randn(2000, 6, 256). As mentioned before batch_first=True is ok. â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned.Message ID: ***@***.***>"
766,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Memory access fault by GPU node-1 when Training NanoGPT with ROCm)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm currently running  `python train.py config/train_shakespeare_char.py` in Andrej Karpathy's nanoGPT repo, to no avail. When running on my Cuda or on the CPU, the script works just fine. But when executing the all of the new Pytorch 2 features like fused AdamW, flash attention, compile, etc I get this error:  I can't figure out when this is happening, even after doing `export HSA_OVERRIDE_GFX_VERSION=10.3.0` and turning off compile. Any thoughts for why it isn't working on ROCm?  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,Memory access fault by GPU node-1 when Training NanoGPT with ROCm," ğŸ› Describe the bug I'm currently running  `python train.py config/train_shakespeare_char.py` in Andrej Karpathy's nanoGPT repo, to no avail. When running on my Cuda or on the CPU, the script works just fine. But when executing the all of the new Pytorch 2 features like fused AdamW, flash attention, compile, etc I get this error:  I can't figure out when this is happening, even after doing `export HSA_OVERRIDE_GFX_VERSION=10.3.0` and turning off compile. Any thoughts for why it isn't working on ROCm?  Versions  ",2023-03-18T04:58:01Z,module: rocm triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/97074,Is this related to  CC(NanoGPT training crashes on ROCm)?," Yes, seems to be related. I tried the docker container, and it works well. However, I'm unable to build the pull request from source on my system, so I can't test it right now. Hopefully it gets merge soon.",it should be fixed now (for ROCm/no triton) in `main`.... maybe this issue can be closed? ,This issue was confirmed to be a duplicate of CC(NanoGPT training crashes on ROCm) which was closed automatically when CC(Fix null pointer dereference on ROCm) merged with https://github.com/pytorch/pytorch/commit/933166e5c0bfe599bcfa9dc3470af05864c7a05e.  Closing this as well.
315,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(is_causal transformer encoder)ï¼Œ å†…å®¹æ˜¯ (Summary: is_causal transfromer encoder Test Plan: sandcastle / github Differential Revision: D44152507)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,is_causal transformer encoder,Summary: is_causal transfromer encoder Test Plan: sandcastle / github Differential Revision: D44152507,2023-03-17T22:00:15Z,fb-exported topic: not user facing bug,closed,0,18,https://github.com/pytorch/pytorch/issues/97050,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507,This pull request was **exported** from Phabricator. Differential Revision: D44152507
626,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorExpr eval: fix copying variables from pointers on big endian systems)ï¼Œ å†…å®¹æ˜¯ (When copying data from pointers, only lowest bytes are copied. On little endian systems they are located at the beginning of pointer. On big endian systems they are located at the end of pointer. This change fixes TestTensorExprPyBind::test_dynamic_shape and TestTensorExprPyBind::test_dynamic_shape_2d tests from test/test_tensorexpr_pybind.py on big endian systems. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TensorExpr eval: fix copying variables from pointers on big endian systems,"When copying data from pointers, only lowest bytes are copied. On little endian systems they are located at the beginning of pointer. On big endian systems they are located at the end of pointer. This change fixes TestTensorExprPyBind::test_dynamic_shape and TestTensorExprPyBind::test_dynamic_shape_2d tests from test/test_tensorexpr_pybind.py on big endian systems. ",2023-03-16T13:19:14Z,triaged open source Merged NNC ciflow/trunk release notes: jit,closed,0,25,https://github.com/pytorch/pytorch/issues/96951, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  Lint / lintrunner / linuxjob Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ,need lintfix,"There's one more issue:  It looks like static assert fails on one of platforms for ""double"" type. As far as I'm aware, only ""int"" type is saved as pointer now, but I might be incorrect: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tensorexpr/tensorexpr_init.cppL833 Maybe, ""double"" type should be excluded here? Or only int types should remain?", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3clang7androidndkr19cgradlecustombuildsinglefulljit / buildandtest (default, 1, 1, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", do you know?,"> Maybe, ""double"" type should be excluded here? Or only int types should remain? TE supports most torch datatypes including `double`, and `SimpleIREvaluator` is a codegen of TE. Hence, we should not just support `int` types.","> https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tensorexpr/tensorexpr_init.cppL833  , This is a python interface that intends to be used from python directly. The initial idea was to enable the user could use Python to develop the TE program. Something like this  https://github.com/pytorch/pytorch/blob/master/test/test_tensorexpr_pybind.py. But the L833 code does not cover all the cases that have been supported by `SimpleIREvaluator`. `SimpleIREvaluator` could support most torch data types, including `double`.","It theoretically could support data types other than tensors and ints in ""call"" and ""call_raw"", but only code pieces and cases I see are actually only those 2 listed above. I don't see any implementation for basic types other than int, and current approach wouldn't work for types bigger than pointer anyway. https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tensorexpr/tensorexpr_init.cppL833 Currently, this is only piece of code I could find which processes types other than tensors in related code. So, here we have ""int"" converted to ""int64_t"" and stored into pointer (void*). Let's call it pointer 1. https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tensorexpr/eval.cppL1280L1292 Here's the code reading previously stored value. ""data"" is pointer (void*), let's call it pointer 2. pointer 2 points to pointer 1, which contains ""int64_t"", our intended data, which was originally ""int"". On 64bit systems pointer 2 is usually 8 bytes, pointer 1 also 8 bytes, int64_t is 8 bytes and int is usually 4 bytes. Everything good. On 32bit systems pointers are usually 4 bytes, int64_t is 8 bytes and int isusually 4 bytes. While int64_t is truncated to 4 bytes when copying to pointer, considering originally it was 4 bytes int, everything ends well. Now, let's for example take ""double"" (int64_t would work same) as value instead of original ""int"", on 32bit systems. ""double"" is usually 8 bytes, even on 32bit systems. So, pointer 2 (4 bytes) points to pointer 1 (4 bytes), but originally pointer 1 should have contained ""double"", which is 8 bytes, which it cannot do. What I'm trying to say, is that for values longer than sizeof(void*) current approach won't work. But currently I can find only code using ""int"" types, and luckly for those types it usually works. I propose to fix ""int"" and smaller integer types usecases, and to have current values binding system reworked when longer ints (int64_t) or floatingpoint types would actually be used.","> I propose to fix ""int"" and smaller integer types usecases, and to have current values binding system reworked when longer ints (int64_t) or floatingpoint types would actually be used. It is an issue of TE to support `double`/`int64_t` scalar on 32bit system. But we still need to support other data types, not only the integer.  https://github.com/pytorch/pytorch/blob/master/test/test_tensorexpr.pyL1184L1199 Let's separate the two things:  Enable TE to support bigendian  Fix the issue that TE does not support `double`/`int64_t` on a 32bit system  , How about let's only focus on the second thing for this PR to meet your requirement now? Regarding the first one, We need to sort out a good design to fix it but not this PR.","I've tested how  https://github.com/pytorch/pytorch/blob/master/test/test_tensorexpr.pyL1184L1199 works. It looks like it uses different mechanism, and not affected by this patch at all. It goes through `ScriptFunction.__call__`: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/python/script_init.cppL1385L1401 and I guess it always makes buffers, and check `!bufArg.isVar()` is always true. https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tensorexpr/eval.cppL1274L1278 I see there are multiple checks failed. Any of them I should take a look in regards to this patch?  I agree with separation of two topics, big endian support and `double`/`int64_t` on 32bit systems support. But from what I can see, there are no places where anything except for `int` is used in TE through this mechanism and it's actually a `var`, not a `buf`. If such places are in tests, they should fail for this pull request. And they'd have been failing on big endian systems right now, but they're passing. Please let me know if I missed new failing tests for this pull request too. So, this pull request fixes TE big endian support, and while it disables `double`/`int64_t` support for some use cases, such use cases are most likely currently not covered by tests.","Then how about let's add something like follows to ensure the input is neither `Double` nor `Long` for big endian system?  With the guard, your code does not need to add `static_assert`.  Again, I need to highlight that Python float and int are translated to double and int64_t respectively.","A few lines below it's already caught in default case, and exception would be thrown: https://github.com/pytorch/pytorch/pull/96951/filesdiff33783d984927670883fec7121b94a5142e54bedf159d7b85af6800818e513d09R1311R1312 Should it still be added?","> A few lines below it's already caught in default case, and exception would be thrown: >  > https://github.com/pytorch/pytorch/pull/96951/filesdiff33783d984927670883fec7121b94a5142e54bedf159d7b85af6800818e513d09R1311R1312 >  > Should it still be added? It means that we still need to support double and int64_t because the Python float and int are translated to 64bit as I mentioned before. "," , I submitted a PR to fix the 32bit issue  https://github.com/pytorch/pytorch/pull/97669."," my PR https://github.com/pytorch/pytorch/pull/97669 has been merged, please rebase this PR.","Thanks, with your change the fix can be simplified.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1997,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(TensorStorage error when deepcopying FX graph_module of Adam optimizer)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When there is Adam optimizer inside torch.compile'd function, it is not possible to do deepcopy of the module. !image My assumption is that after AOT, FX graph module is just a program description that at most contains FakeTensors which do not have storage and depending on backend needs, we should be able to deepcopy such module. It's not the case now it seems. Attaching very small reproducer (need to change .txt to .py): repro.txt Reproducer uses Adam optimizer, I did not see such issue with SGD.  Error logs !error  Minified repro _No response_  Versions PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.9.16 (main, Dec  7 2022, 01:11:51)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.147+x86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 525.85.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.7.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Orde)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,TensorStorage error when deepcopying FX graph_module of Adam optimizer," ğŸ› Describe the bug When there is Adam optimizer inside torch.compile'd function, it is not possible to do deepcopy of the module. !image My assumption is that after AOT, FX graph module is just a program description that at most contains FakeTensors which do not have storage and depending on backend needs, we should be able to deepcopy such module. It's not the case now it seems. Attaching very small reproducer (need to change .txt to .py): repro.txt Reproducer uses Adam optimizer, I did not see such issue with SGD.  Error logs !error  Minified repro _No response_  Versions PyTorch version: 2.0.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.9.16 (main, Dec  7 2022, 01:11:51)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.147+x86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 525.85.12 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.7.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Orde",2023-03-16T10:42:04Z,module: optimizer triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/96949,No longer able to repro  closing as fixed.
1125,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_nn_TransformerEncoderLayer_relu_activation (__main__.TestJitGeneratedModule))ï¼Œ å†…å®¹æ˜¯ (Platforms: win, windows This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_nn_TransformerEncoderLayer_relu_activation` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_jit_legacy.py` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,DISABLED test_nn_TransformerEncoderLayer_relu_activation (__main__.TestJitGeneratedModule),"Platforms: win, windows This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_nn_TransformerEncoderLayer_relu_activation` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_jit_legacy.py` ",2023-03-16T09:39:33Z,oncall: jit module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/96945
395,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`is_causal` parameter in torch.nn.TransformerEncoderLayer.forward does not work)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug As the title says, `is_causal` parameter in torch.nn.TransformerEncoderLayer.forward does not work.   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`is_causal` parameter in torch.nn.TransformerEncoderLayer.forward does not work," ğŸ› Describe the bug As the title says, `is_causal` parameter in torch.nn.TransformerEncoderLayer.forward does not work.   Versions  ",2023-03-16T07:55:58Z,,closed,0,5,https://github.com/pytorch/pytorch/issues/96941, ,"https://github.com/pytorch/pytorch/pull/97214 clarifies that is_causal is a hint that can be optionally provided.   When the hint is provided, it may override use of the attention mask.  Undefined behavior occurs when is_causal is set and the mask is not a causal mask.","So basically, if I get you right, the way to make a causal model (or what some people might call a GPT/decoder only model) is this:  Though I'm still a bit confused about the purpose of setting is_causal, when you also use a causal mask. And for some reason the code seems to run the same, even if the `seq` argument to `generate_square_subsequent_mask` is replaced with another value, like 0...","> So basically, if I get you right, the way to make a causal model (or what some people might call a GPT/decoder only model) is this: >  >  >  > Though I'm still a bit confused about the purpose of setting is_causal, when you also use a causal mask. And for some reason the code seems to run the same, even if the `seq` argument to `generate_square_subsequent_mask` is replaced with another value, like 0... is_causal is a hint that the mask is causal, and we don't actually have to use the mask if we have logic for hardwired causal mask, such as in Flash Attention  when we process, we have the liberty to ignore the mask and just assume it's the causal mask.   You don't need to supply is_causal, and evrything will work just fine (with small overhead for checking whether the mask is causal). Bad results will happen if is_causal is set when the supplied mask is not a causal mask.  It might work sometimes (when we look at is_causal), not at other times (when we look at the mask).","The docs on this are super ambiguous: ""is_causal: If specified, applies a causal mask as attention mask, and ignores attn_mask"".  Yet it requires that you pass in a mask.  And what should you do if you have a causal mask that is not strictly causal but also ignores padding?  It is causal but it can't be ignored... so I guess you have to set causal to False?  This API really needs some attention : )"
1136,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_variant_consistency_jit_cholesky_inverse_cuda_complex64 (__main__.TestJitCUDA))ï¼Œ å†…å®¹æ˜¯ (Platforms: win, windows This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_variant_consistency_jit_cholesky_inverse_cuda_complex64` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_ops_jit.py`)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DISABLED test_variant_consistency_jit_cholesky_inverse_cuda_complex64 (__main__.TestJitCUDA),"Platforms: win, windows This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 2 failures and 2 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_variant_consistency_jit_cholesky_inverse_cuda_complex64` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. Test file path: `test_ops_jit.py`",2023-03-15T12:50:53Z,triaged module: flaky-tests skipped module: unknown,closed,0,0,https://github.com/pytorch/pytorch/issues/96823
397,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Change `THPStorage::cdata` to be a `MaybeOwned<Storage>`, add unpack func)ï¼Œ å†…å®¹æ˜¯ (  CC(Change `THPStorage::cdata` to be a `MaybeOwned`, add unpack func) Part of CC(PyObject preservation and resurrection for `StorageImpl`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Change `THPStorage::cdata` to be a `MaybeOwned<Storage>`, add unpack func","  CC(Change `THPStorage::cdata` to be a `MaybeOwned`, add unpack func) Part of CC(PyObject preservation and resurrection for `StorageImpl`) ",2023-03-15T04:04:47Z,module: internals open source Merged ciflow/trunk topic: not user facing ciflow/periodic,closed,0,4,https://github.com/pytorch/pytorch/issues/96801,do you want this reviewed now or later,", I think it's ready for a review", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
735,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cache the transformers model used in ONNX test)ï¼Œ å†…å®¹æ˜¯ (Also updating merge_rule to allow ONNX exporter team to update the Docker script by themselves.  By default, the model is cached at ~/.cache/huggingface/hub/ under CI jenkins user. The model is cached so that we don't need to redownload it every time in CI, which causes flaky CI failures%3A%20Read%20timed%20out.%20(read%20timeout%3D10.0)). This is the second part after https://github.com/pytorch/pytorch/pull/96590  Testing Confirm that the model is cached in the Docker image before running the test: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Cache the transformers model used in ONNX test,"Also updating merge_rule to allow ONNX exporter team to update the Docker script by themselves.  By default, the model is cached at ~/.cache/huggingface/hub/ under CI jenkins user. The model is cached so that we don't need to redownload it every time in CI, which causes flaky CI failures%3A%20Read%20timed%20out.%20(read%20timeout%3D10.0)). This is the second part after https://github.com/pytorch/pytorch/pull/96590  Testing Confirm that the model is cached in the Docker image before running the test: ",2023-03-15T01:40:26Z,Merged ciflow/trunk topic: not user facing test-config/default,closed,0,6,https://github.com/pytorch/pytorch/issues/96793,Thanks :), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxbionicpy3.8clang9 / build Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2001,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯( Segmentation fault (core dumped) during Torch finetuning (at random step))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Iâ€™m encountering a problem while running a finetune script for my FlanT5(large) model. The script crashes with the message Segmentation fault (core dumped) after a few (thousand) steps. The error seems to occur randomly, sometimes within a few minutes and sometimes it takes around 40 minutes. I initially attempted to use the stable version of PyTorch, but based on advice from , I decided to upgrade to the nightly version. Unfortunately, neither option resolved my issue, but the nightly version did provide a more detailed stack trace, which is shown below. My imports:  Some snippets from my training loop:  With the following forward function:  Let me know if the provided information is insufficient. In that case I can try to create a simple application.  ` Originates from https://discuss.pytorch.org/t/segmentationfaultcoredumpedduringtorchfinetuningonnewsetup/174570  Versions ollecting environment information... PyTorch version: 2.1.0.dev20230313+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.19.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 Nvidia driver version: 530.30.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer, Segmentation fault (core dumped) during Torch finetuning (at random step)," ğŸ› Describe the bug Iâ€™m encountering a problem while running a finetune script for my FlanT5(large) model. The script crashes with the message Segmentation fault (core dumped) after a few (thousand) steps. The error seems to occur randomly, sometimes within a few minutes and sometimes it takes around 40 minutes. I initially attempted to use the stable version of PyTorch, but based on advice from , I decided to upgrade to the nightly version. Unfortunately, neither option resolved my issue, but the nightly version did provide a more detailed stack trace, which is shown below. My imports:  Some snippets from my training loop:  With the following forward function:  Let me know if the provided information is insufficient. In that case I can try to create a simple application.  ` Originates from https://discuss.pytorch.org/t/segmentationfaultcoredumpedduringtorchfinetuningonnewsetup/174570  Versions ollecting environment information... PyTorch version: 2.1.0.dev20230313+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.19.035genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 Nvidia driver version: 530.30.02 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                  ",2023-03-14T21:36:12Z,needs reproduction module: crash triaged,open,0,0,https://github.com/pytorch/pytorch/issues/96779
598,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix calls to sizes to enable dynamic shapes with sdpa)ï¼Œ å†…å®¹æ˜¯ (Fixes part of CC(torch.compile(dynamic=True) does not work with a simple Transformer + embedding at inference) Replaces any calls to sizes, with sym_sizes. Still seeing an error with the repro script:  still trying to track down this empty call from the looks of it, might be coming from at::layer_norm? the BT from lldb is 221 frames however, so lots of noise )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Fix calls to sizes to enable dynamic shapes with sdpa,"Fixes part of CC(torch.compile(dynamic=True) does not work with a simple Transformer + embedding at inference) Replaces any calls to sizes, with sym_sizes. Still seeing an error with the repro script:  still trying to track down this empty call from the looks of it, might be coming from at::layer_norm? the BT from lldb is 221 frames however, so lots of noise ",2023-03-13T19:20:36Z,Merged ciflow/trunk topic: not user facing module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/96674, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1146,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Request for adding Warning/Error feature when dropout set to 1.0 in Transformer layer)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch The model is not &quot;turned off during training&quot;. With dropout=1.0, for dropout layers you& CC(Error messages to improve);ll get all zero at train and, apparently, identity at test. I don& CC(Error messages to improve);t think pytorch should have allowed dropout=1.0. It should be ValueError, not sure I get the reasoning there.&mdash; Andrej Karpathy () March 12, 2023  As suggested by Andrej Karpathy, it makes sense to raise an error or atleast a warning when the user sets dropout in the transformer layer.  So it would be great if someone can decide whether it should raise an error or warning , and which warning or error to raise in the above case. P.S :  I would like to be **assigned** the task for **implementation** of this issue.  Alternatives _No response_  Additional context Could you look into this?    _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Request for adding Warning/Error feature when dropout set to 1.0 in Transformer layer," ğŸš€ The feature, motivation and pitch The model is not &quot;turned off during training&quot;. With dropout=1.0, for dropout layers you& CC(Error messages to improve);ll get all zero at train and, apparently, identity at test. I don& CC(Error messages to improve);t think pytorch should have allowed dropout=1.0. It should be ValueError, not sure I get the reasoning there.&mdash; Andrej Karpathy () March 12, 2023  As suggested by Andrej Karpathy, it makes sense to raise an error or atleast a warning when the user sets dropout in the transformer layer.  So it would be great if someone can decide whether it should raise an error or warning , and which warning or error to raise in the above case. P.S :  I would like to be **assigned** the task for **implementation** of this issue.  Alternatives _No response_  Additional context Could you look into this?    _No response_ ",2023-03-13T03:43:44Z,module: nn triaged enhancement,open,0,0,https://github.com/pytorch/pytorch/issues/96633
2045,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(MPS cumsum issue - RuntimeError: MPS does not support cumsum op with int64 input. Support has been added in macOS 13.3)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm on a Macbook Pro M1 Pro and I've upgraded to 13.3 Beta 3  I am running into the `cumsum` issue. I've created 2 new conda environment and installed the nightly version on 3/11/2023 at 12PM PST using pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu and the other using conda install pytorch torchvision torchaudio c pytorchnightly but I'm still getting `cumsum` errors. It looks like https://github.com/pytorch/pytorch/pull/96512 may have broken the previous patch, unless I'm missing something? I'm new to this so any help is greatly appreciated! Code:   Traceback:   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230311 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: version 3.25.2 Libc version: N/A Python version: 3.9.16  (main, Feb  1 2023, 21:38:11)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.24.2 [pip3] torch==2.1.0.dev20230311 [pip3] torchaudio==2.0.0.dev20230311 [pip3] torchvision==0.15.0.dev20230311 [conda] numpy                     1.24.2           py39hff61c6a_0    condaforge [conda] pytorch                   2.1.0.dev20230311         py3.9_0    pytorchnightly [conda] torchaudio                2.0.0.dev20230311     )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,MPS cumsum issue - RuntimeError: MPS does not support cumsum op with int64 input. Support has been added in macOS 13.3," ğŸ› Describe the bug I'm on a Macbook Pro M1 Pro and I've upgraded to 13.3 Beta 3  I am running into the `cumsum` issue. I've created 2 new conda environment and installed the nightly version on 3/11/2023 at 12PM PST using pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu and the other using conda install pytorch torchvision torchaudio c pytorchnightly but I'm still getting `cumsum` errors. It looks like https://github.com/pytorch/pytorch/pull/96512 may have broken the previous patch, unless I'm missing something? I'm new to this so any help is greatly appreciated! Code:   Traceback:   Versions Collecting environment information... PyTorch version: 2.1.0.dev20230311 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.3 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: version 3.25.2 Libc version: N/A Python version: 3.9.16  (main, Feb  1 2023, 21:38:11)  [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] numpy==1.24.2 [pip3] torch==2.1.0.dev20230311 [pip3] torchaudio==2.0.0.dev20230311 [pip3] torchvision==0.15.0.dev20230311 [conda] numpy                     1.24.2           py39hff61c6a_0    condaforge [conda] pytorch                   2.1.0.dev20230311         py3.9_0    pytorchnightly [conda] torchaudio                2.0.0.dev20230311     ",2023-03-11T21:39:55Z,triaged module: mps,closed,8,44,https://github.com/pytorch/pytorch/issues/96610," , I tried the attached script on latest nightly and it didn't show me the error message. Below was the print out.  Here is the pip list: ","Hey  , I've started a new Conda environment and installed all packages to those same versions and I still have the same issue. After asking others on M1's, it seems to be the same result. Here is one example: https://github.com/microsoft/visualchatgpt/issues/37issuecomment1465149007 I am on macOS 13.3 Beta 3. Any ideas as to how to solve this issue?","> Hey  , >  > I've started a new Conda environment and installed all packages to those same versions and I still have the same issue. After asking others on M1's, it seems to be the same result. Here is one example: microsoft/visualchatgpt CC(Add more functions to autograd) (comment) >  > I am on macOS 13.3 Beta 3. Any ideas as to how to solve this issue? You are indeed right, the MacOS selector check was broken with Beta 1 onwards. With this PR, it should be fixed. You can give this a try if you would like, otherwise I will update here when it lands in Nightly.",Thanks for bringing this up  ,"> Thanks for bringing this up  Seems to be working with this PR! I appreciate you fixing the issue. Let me know when it's in the latest nightly, thanks!",> Thanks for bringing this up  Any ETA on a land in nightly? Really appreciate it!,It should be in nightly . Can you please check ,Updated to macOS13.3 today. I am still getting this error.,Updated to macOS 13.3 today. The error is gone. It works on my M1 as well as my M2 Pro.,"Getting similar error, but it doesn't mention the OS ""RuntimeError: MPS does not support cumsum op with int64 input"" I am on macOS 13.3 and using pytorch nightly. What may be going wrong ?","Guys make sure you are not using the stable Pytorch build but rather Preview (nightly) build and you have MacOS 13.3 to fix this issue, I updated to macOS 13.3 today and installed preview nightly build of Pytorch and the error is gone. To Install preview nightly build run the command on your terminal: (pip version) pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu source: https://pytorch.org/getstarted/locally/",M2 Max: Updated to macOS 13.3 Ran pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu Fixed this issue for me as well. ,Issue fixed.,"> Updated to macOS13.3 today. I am still getting this error. > Getting similar error, but it doesn't mention the OS ""RuntimeError: MPS does not support cumsum op with int64 input"" >  > I am on macOS 13.3 and using pytorch nightly. What may be going wrong ?   Try making sure you are installing the packages with pip using the same python version as the one you are running the script with. In my case, pip3 pointed to pip3.10 while python3 pointed to python3.11. vittoAir vittonlpscripts % python3 version Python 3.11.3 vittoAir vittonlpscripts % pip3 version pip 23.1 from /opt/homebrew/lib/python3.10/sitepackages/pip (python 3.10) Rerunning the pip torch nightly command with the correct version of pip the unsupported cumsum issue.",Hello  and  Could your please share the exact reason why macos 13.3 required and by which software?,"$ which python /usr/local/anaconda3/bin/python $ python version Python 3.10.9 $ pip3 version pip 23.1.2 from /usr/local/anaconda3/lib/python3.10/sitepackages/pip (python 3.10) Intel CPU  MacOS 13.4   Ran pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu Also ran python m pip install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu The issue still exists, any ideas? Any help is very appreciated, Thank you very much! > Guys make sure you are not using the stable Pytorch build but rather Preview (nightly) build and you have MacOS 13.3 to fix this issue, I updated to macOS 13.3 today and installed preview nightly build of Pytorch and the error is gone. >  > To Install preview nightly build run the command on your terminal: (pip version) pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu >  > source: https://pytorch.org/getstarted/locally/", have you tried creating a new `venv` and running `pip install` again? I had the same problem as above and fixed it with the nightly on my M1 mbp.,"> OS 13.4  , can you try PyTorch nightly and see if it works?  as  suggested.","I am using miniforge on macOS 13.4 (M2) and I'm getting the same error. ~~Installing PyTorch nightly as suggested has not fixed it for me.~~ Scrub that, my mistake  pip3 was my Homebrew installed Python. Changed to pip and it works fine now, many thanks!",I also have the same problem. I have macOS 13.4 (M2).  I used Python 3.8. Here the installed packages via pip  and here the error  is there anything I can do? Thank you very much for your support!, have you tried with PyTorch nightly as  suggested?,"Yes, I used  ``pip install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu`` Is it correct?",I'm facing the same problem. ,Does it make sense to replace position_ids = attention_mask.long().cumsum(1)  1 with  position_ids = attention_mask.int().cumsum(1)  1 ?, For me the 's code runs on a Apple M2 Pro in a virtual environment: ," ,  confirmed, run on my m2 pro 16gb using   but lot of disk swap"," do you know why pytorch use a lot of memory, for example in stable diffusion, it uses lot of memory even when using medvram (little out of context)",Just to save someone the trouble of looking it up  If you want to update your existing environment that already has an older nightly build you can do: `pip3 install upgrade nodeps forcereinstall pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu`," Thats very useful, thanks", It is now running. Thanks!
525,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Shape Error when training HF deberta-base with Inductor)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When using HuggingFace's Trainer API I noticed that PyTorch eager mode succeeds as expected but inductor fails with a shape mismatch error:  This only happens with the debertabase model  Error logs   Minified repro Minifier was unable to repro the error   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Shape Error when training HF deberta-base with Inductor, ğŸ› Describe the bug When using HuggingFace's Trainer API I noticed that PyTorch eager mode succeeds as expected but inductor fails with a shape mismatch error:  This only happens with the debertabase model  Error logs   Minified repro Minifier was unable to repro the error   Versions  ,2023-03-09T22:08:48Z,good first issue triaged module: meta tensors oncall: pt2 module: pt2-dispatcher,closed,0,13,https://github.com/pytorch/pytorch/issues/96456,Looks like a stride propagation error. ,"managed to get this more minimal repro, haven't looked much at it yet. (note  if you're trying to repro the original transformers issue, you need to run with a single gpu or else you'll run into some other faketensor issue) ",Hmm neither `CrossRefFakeMode` nor DebugInterpreter catch this.,"Even aot_eager fails here.  ~~~ import torch x = torch.rand((1, 12, 256*64), requires_grad=True) def transpose_for_scores(x):     new_x_shape = x.size()[:1] + (256, 1)     x = x.view(new_x_shape)     return x.permute(0, 2, 1, 3) def fn(x):     scale_factor = 0.5     x = x.relu()     x = transpose_for_scores(x)     x /= torch.sqrt(torch.tensor(x.size(1), dtype=torch.float) * scale_factor)     return x.transpose(1, 2) fn(x) torch.compile(fn, backend=""aot_eager"")(x) ~~~",.,"THe minimum repros throw different error (""one of the variables needed for gradient computation has been modified by an inplace operation""). THe original view error is probably due to `copy_` decomposition producing wrong strides,  has a fix for this that is blocked by cpp codegen in fbcode",That looks like something that should be fixed by this PR  CC(Shape Error when training HF deberta-base with Inductor)issuecomment1562284376. I can't test it at the moment (allocation was nuked) but I can try to confirm later.,"Unfortunately even with the copy() decomp fix in inductor, the repro now gives this error for me: ","Actually, I realized that the small repro above is broken (that error also shows up if you run in eager, and actually call `.backward()`).","I tried running the HuggingFace repro. On my 40gb machine, I get an OOM  it would be great if someone can patch this PR locally and try to repro!  CC(Shape Error when training HF deberta-base with Inductor).",'s repro still fails for me in AOTAutograd  CC(Shape Error when training HF deberta-base with Inductor)issuecomment1467355129 ,I get a different error when I try to run 's repro today: ," can you reopen if you're still seeing an issue? David's smaller repro above no longer fails with the original error, as Yanbo pointed out. The new error is actually because the minimized repro isn't quite valid  even in eager mode, that code will fail if you call `out.sum().backward()`, because the repro code is mutating the output of `relu()`, which was saved for backward."
735,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.compile(dynamic=True) does not work with a simple Transformer + embedding at inference)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Using yesterday's CI build (possibly with previous builds too), I am getting the following error while trying to run `torch.compile` in dynamic mode for a simple TransformerEncoder with an embedding layer. The compilation works fine with the embedding layer only, the encoder only, but not with the combination of both. The compilation also works fine if `dynamic=False`.  Throws the following error in inductor compilation:     Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.compile(dynamic=True) does not work with a simple Transformer + embedding at inference," ğŸ› Describe the bug Using yesterday's CI build (possibly with previous builds too), I am getting the following error while trying to run `torch.compile` in dynamic mode for a simple TransformerEncoder with an embedding layer. The compilation works fine with the embedding layer only, the encoder only, but not with the combination of both. The compilation also works fine if `dynamic=False`.  Throws the following error in inductor compilation:     Versions  ",2023-03-09T11:54:02Z,triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/96414,Sdpa related coverage regression . Should be easy to fix, ,"I see that  marked this as being part of the nested tensor module, but the error is still happening when I set `enable_nested_tensor=False` in the TransformerEncoder.",I'll take a look at this today,"Thanks all, the issue seems fixed on today's nightly. It didn't work right away after Driss's fix, but a few more days of nightlies seems to have fixed the issue."
762,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(views created in __torch_dispatch__ share storage but not version_counter)ï¼Œ å†…å®¹æ˜¯ (Usually the ADInplaceOrView kernel is responsible for handling this, but since we're operating under autograd, the version counter information is not correctly propagated.  Previously this was probably intentional for inference mode, but it may be useful to support version counter propagation for `__torch_dispatch__` use cases to prevent common silently correctness issues. If this were a Tensor subclass, we could probably enable_reentrant_dispatch, but that may not work for modes (can we fix this?). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,views created in __torch_dispatch__ share storage but not version_counter,"Usually the ADInplaceOrView kernel is responsible for handling this, but since we're operating under autograd, the version counter information is not correctly propagated.  Previously this was probably intentional for inference mode, but it may be useful to support version counter propagation for `__torch_dispatch__` use cases to prevent common silently correctness issues. If this were a Tensor subclass, we could probably enable_reentrant_dispatch, but that may not work for modes (can we fix this?). ",2023-03-08T18:44:43Z,module: autograd module: molly-guard triaged needs design module: __torch_dispatch__,open,0,11,https://github.com/pytorch/pytorch/issues/96319,We don't track views at all right? `._is_view()` will be `False` as well? That sounds ok to me: you are under autograd so you can't do anything autograd related. And escaping objects into global state from torch_dispatch level is very shady anyways. Curious what  thinks though!,Smuggling tensor out of the mode is very naughty and I don't really see how we can even make it work in principle.,"The context is that smuggling tensor out of a mode is how xformers currently implements selective checkpoint  it uses TorchDispatchModes to implement a cache under autograd. The issue is that if you cache the output of some operation, and then perform inplace afterwards, thats like mutating a tensor saved for backwards except you'll get silently incorrect results this time because there's no version counter to protect you. We want to produce an error for this case. ~If we're okay with implementing selective checkpoint as part of checkpoint itself, we can avoid doing naughty things entirely. This might be a good thing for other reasons  I can see selective checkpoint working better with other checkpoint features like early stopping if implemented this way.~ ~If we want to keep the selective checkpoint separate,~ I don't see a good way around tensor smuggling. In that case the workaround for the inplace issue is to for all inplace operations to check if we're modifying something in the cache.  ","So, just to confirm, you want to have selective checkpoint check the VC on the smuggled tensors so that it can check if mutation occurred. The root cause of this problem is that the VC mechanism should live ""lower"" than autograd, but it currently lives at autograd for efficiency reasons. One possibility is to provide a lower level version counter service that is not tied to autograd. Another is to use 's upcoming mode at any dispatch key stuff to get xformers thing to live after autograd but before ADInplaceOrView. Yet another is to refactor where version counting is done so that nonautograd mechanisms can hook into it (but this might be quite involved / involve slowing down regular code.) Yet another is to have another variant of version counting that's stored on storage ('s storage PyObject preservation will help) which then would always be stable no matter where the VC setup is done.","> So, just to confirm, you want to have selective checkpoint check the VC on the smuggled tensors so that it can check if mutation occurred. Yeah Another solution (shortterm): just propagate the version counter instead of passing 0 (see below)? This solution won't decouple the version counter mechanism entirely from autograd (incrementing version counter on inplace is still handled by autograd, but maybe that is we want, for efficiency purposes?), but does seem to fix this particular issue.  (","I'm cool for this, esp if we can avoid the overwrite when it's not necessary","Since this has come up again with integrating subclasses within PT2 and fp8 integration, I think we want to have a full solution. One proposal I would have would be:  Add a new `mark_as_view(t, base, is_bw_differentiable=True, is_fw_differentiable=True, view_fn=None, creation_meta=CM.Default)` function. This is the equivalent of our `VariableTypeUtils.h` `as_view` function (it will always work in inplace with the right asserts to ensure there is no preexisting autograd metadata on the input). It can be used to add autograd view metadata to any Tensor from Python code.  This function can be used explicitly within TorchDispatch Class/Mode to properly mark some Tensors as views as each other when working below autograd.  This function can be used to faithfully implement ADInplaceOrView kernel for userdefined custom ops. It is important to keep in mind that for regular use of dispatch class/mode, the ADInplaceOrView layer that we provide does add the autograd metadata properly. There is a separate question about making the storage from these view alias properly. In this case, I think that we should:  Use `.set_()` to make storage aliasing when working below autograd. This can be added to the `mark_as_view()` method above for convenience.  [Maybe] Add an automatic postprocessing on the Python key where, based on the schema, we properly fixup the storages of any wrapper Tensor we encounter (and raise errors for real Tensors that have storage but don't properly alias). Also .","Another option might be to have users reenable ADInplaceOrView manually:  Note that _UnexcludeDispatchKeyGuard doesn't actually exist yet, but it should be trivial to write if we want it. Edit: I think exposing `as_view` to python is more useful for the custom op use case where I'd like to register a new view op and want to define its ADInplaceOrView kernel, but if I already have an existing view op, it might be better to reuse the existing ADInplaceOrView kernel. A downside of offering a context manager though is that it could lead to ADInplaceOrView still being active when you reach the backend if you dispatch into a nonview or noninplace inside the context manager, leading to silently worse performance. Perhaps we can offer the context manager as a part of a wrapper function instead, e.g. `out = with_tracking(torch.view)(inp)`. A question that applies to both proposals is what happens if I dispatch into torch dispatch for an existing view op, lets say `torch.ops.aten.view.default`, and our `__torch_dispatch__` function returned a tensor that already has view metadata,  what should the existing ADInplaceOrView kernel do?","> There is a separate question about making the storage from these view alias properly. Views performed in Python key do already do result in storages that are aliased, if this is what you mean ","> Views performed in Python key do already do result in storages that are aliased, if this is what you mean Well, did you check that data_ptr() value, it is `0` for every Tensor created with make_wrapper_subclass IIRC.","> A question that applies to both proposals is what happens if I dispatch into torch dispatch for an existing view op, lets say torch.ops.aten.view.default, and our __torch_dispatch__ function returned a tensor that already has view metadata, what should the existing ADInplaceOrView kernel do? I think a good first draft for this API would be:  Provide the same tool as what we have in c++ so that users can implement the exact same thing with torch.library  Assert that a wrapper subclass implements a backend and the user does not mock around with autograd metadata (error on already set metadata). If the user explicitly reenabled autograd, they should know exactly what they're doing wrt what has what  autograd metadata.  If some of these asserts are too restrictive in practice (calls we do internally or usual patterns trip them), we can look case by case to relax them / add the appropriate API to not trip them anymore."
671,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(extract out TransformFallback and UnaryInvolutionFallback types)ï¼Œ å†…å®¹æ˜¯ (Summary: TransformFallback is the generic interface for implementation a tensor transformation using a dispatch key. This has been done twice before with the negation and conjugate unary operators. We intend to do it again with infallible view and compositional as_strided, which are somewhat more complex than the unary involution views. Test Plan: Should be a noop, rely on existing tests. Differential Revision: D43908030)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,extract out TransformFallback and UnaryInvolutionFallback types,"Summary: TransformFallback is the generic interface for implementation a tensor transformation using a dispatch key. This has been done twice before with the negation and conjugate unary operators. We intend to do it again with infallible view and compositional as_strided, which are somewhat more complex than the unary involution views. Test Plan: Should be a noop, rely on existing tests. Differential Revision: D43908030",2023-03-08T17:32:46Z,triaged open source fb-exported,closed,0,3,https://github.com/pytorch/pytorch/issues/96312,This pull request was **exported** from Phabricator. Differential Revision: D43908030,This pull request was **exported** from Phabricator. Differential Revision: D43908030,This pull request was **exported** from Phabricator. Differential Revision: D43908030
872,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Better error message when trying to run fp16 weights on CPU)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Hey :wave: from the Hugging Face OpenSource team, We're seeing the following issue over and over again across libraries  or:  E.g.: https://github.com/runwayml/stablediffusion/issues/23 The problem here is that a PyTorch model has been converted to fp16 and the user tried to run it on CPU, e.g. the following:  yields:  Could we maybe catch such errors in the forward of https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.htmlModule and return a simpler error message that just says ""Float16 cannot be run on CPU""?  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Better error message when trying to run fp16 weights on CPU," ğŸš€ The feature, motivation and pitch Hey :wave: from the Hugging Face OpenSource team, We're seeing the following issue over and over again across libraries  or:  E.g.: https://github.com/runwayml/stablediffusion/issues/23 The problem here is that a PyTorch model has been converted to fp16 and the user tried to run it on CPU, e.g. the following:  yields:  Could we maybe catch such errors in the forward of https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.htmlModule and return a simpler error message that just says ""Float16 cannot be run on CPU""?  Alternatives _No response_  Additional context _No response_ ",2023-03-08T10:57:00Z,good first issue module: error checking triaged,open,0,7,https://github.com/pytorch/pytorch/issues/96292,"Yeah I think this is a good idea. We would be happy to accept a PR doing this, and I would open to review it",Cool! Will open a PR for it ,"  Can we make it a warning instead of an error? We are in progress adding FP16 support on CPU. The work would improve the functional coverage of FP16 ops (which can address the problems you raise), improve the performance of CPU FP16 ops (expecting performance speedup with non conv/gemm ops, and conv/gemm will be sped up on next generation Xeon processors), add FP16 autocast (check RFC)) and add FP16 support in TorchInductor.  ",Ah ok in this case maybe not worth it to introduce a new error/warning if Fp16 will work on CPU soon? ,"In general, these error messages could be improved to use standardized dtype names: torch.float16 instead of Half. Same for thing called dispatch code, for sparse tensors the name is quite cryptic SparseCpu when it means torch.sparse_coo_tensor :( ",> Ah ok in this case maybe not worth it to introduce a new error/warning if Fp16 will work on CPU soon? Agreed. We will prioritize the ops to cover according to the popular model families. Models from HuggingFace will definitively be at the top. Thanks for the patience.,"file location: aten/src/ATen/native/LinearAlgebra.cpp Why addmm_impl_cpu_ can support bfloat16 but dont't support half?   // Apply BLAS routine   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(**kBFloat16,**       result.scalar_type(), ""addmm_impl_cpu_"",       [&]{         using opmath_t = at::opmath_type;         at::native::cpublas::gemm(             transpose_a ? a.is_conj() ? TransposeType::ConjTranspose : TransposeType::Transpose : TransposeType::NoTranspose,             transpose_b ? b.is_conj() ? TransposeType::ConjTranspose : TransposeType::Transpose : TransposeType::NoTranspose,             m, n, k,             alpha.to(),             a.data_ptr(), lda,             b.data_ptr(), ldb,             beta.to(),             c.data_ptr(), ldc);       });"
1988,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pytorch 2.0 Segmentation error / IMA on model compile on GPT2)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Driver and CUDA details (RTX 3060 laptop GPU)  Torch isntalled via   Installation okay though got some pip dependency error  Code snippet used   Segmentation fault (no core files were generated on path)   Versions $ python3 collect_env.py  Collecting environment information... PyTorch version: 2.0.0.dev20230202+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Pop!_OS 22.04 LTS (x86_64) GCC version: (Ubuntu 11.2.019ubuntu1) 11.2.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.18.1076051810genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.6.55 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU Nvidia driver version: 510.73.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          16 Online CPU(s) list:             015 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen 7 5800H with Radeon Graphics CPU family:                      25 Model:                           80 Thread(s) per core:              2 Core(s) per socket:              8 Socket(s):                       1 Stepping:                        0 Frequency boost:                 enabled CPU max MHz:                     4462.5000 CPU min MHz: )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Pytorch 2.0 Segmentation error / IMA on model compile on GPT2," ğŸ› Describe the bug Driver and CUDA details (RTX 3060 laptop GPU)  Torch isntalled via   Installation okay though got some pip dependency error  Code snippet used   Segmentation fault (no core files were generated on path)   Versions $ python3 collect_env.py  Collecting environment information... PyTorch version: 2.0.0.dev20230202+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Pop!_OS 22.04 LTS (x86_64) GCC version: (Ubuntu 11.2.019ubuntu1) 11.2.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.18.1076051810genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.6.55 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU Nvidia driver version: 510.73.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   48 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          16 Online CPU(s) list:             015 Vendor ID:                       AuthenticAMD Model name:                      AMD Ryzen 7 5800H with Radeon Graphics CPU family:                      25 Model:                           80 Thread(s) per core:              2 Core(s) per socket:              8 Socket(s):                       1 Stepping:                        0 Frequency boost:                 enabled CPU max MHz:                     4462.5000 CPU min MHz: ",2023-03-08T07:54:23Z,high priority needs reproduction triaged oncall: pt2,closed,0,10,https://github.com/pytorch/pytorch/issues/96287," are you able to provide a repro (preferably a minimal repro, if possible)? You can try using the minifier mentioned here: https://pytorch.org/docs/master/dynamo/troubleshooting.html, although it's unlikely to work if this is a segfault.",Hi   Here is the repo https://github.com/alexcpn/tranformer_learn/tree/torch2.0_crash To reproduce  https://github.com/alexcpn/tranformer_learn/blob/torch2.0_crash/gpt2_train_model.py  Minifier output torch_compile_debug.zip ,And here is with `CUDA_LAUNCH_BLOCKING=1 python3 gpt2_train_model.py` torch_compile_debug_cude_launch_blocking.zip," thanks for the repro code. I'm not able to repro on the current nightly from a few days ago. Could you check to see whether updating your version of pytorch fixes this? If not  can you try running the minifier again? In addition to the steps you already took  can you also run the minifier_launcher.py that's placed in `torch_compile_debug/run_[date_time]/minifier/minifier.py`? If successful, this should generate a `repro.py`.","   I installed the latest nightly `2.0.0.dev20230202+cu116` Still getting error  `RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered` Also ran the minifier.py; results below  >can you also run the minifier_launcher.py.  If successful, this should generate a repro.py. Below is the output. Both run files are here  minifierout.zip Note  It has not generate a repro.py, but Class Repro is there Alternatively my repo is pretty small and easy to reproduce (not sure if Cuda version or particular Card is the problem)  ", `dev20230202` sounds like it's from feb 2. And I don't think current nightlies have cuda 11.6 versions available. Can you double check?, I see you installed via https://download.pytorch.org/whl/nightly/cu116  to get more recent nightlies you might need https://download.pytorch.org/whl/nightly/cu117 or https://download.pytorch.org/whl/nightly/cu118.,"  thanks; however updating to cuda 11.7 is not very easy for me now As per the doc (https://pytorch.org/getstarted/pytorch2.0/gettingstarted) , Torch 2.0 supports Cuda 11.6 . Since most users who have say one or two years old HW will be on cards and cuda version like this; will it be possible to have a solution to cuda 11.6.  As it is not easy to update Cuda . My RTX 3060 card supported (https://www.nvidia.com/enus/drivers/results/199656/)  driver max is only  515. With 515 my external display was not working, when I tried a week back; maybe they corrected it now. So I am stuck with 510 and Cuda 11.6 is the max supported.   I will give a shot at 515 assuming it supports 11.7 or 11.8  over the weekend and let you know, if I get it to work with my display",I updated to Cuda 12.1 as my laptop had problems and had to reinstall  However still getting errors  However this driver 530.30.02 has still problems with my external display  (very slow inputs when working on external display) and this seems a common problem. So I am downgrading as of now,Marking as closed since the issue appears to be an issue with old cuda drivers that are no longer supported by pytorch
378,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Transformer encoder compile fail)ï¼Œ å†…å®¹æ˜¯ (Summary: Transformer encoder compile fail  CC(compile() breaks TransformerEncoder mask dtype check) Test Plan: github & sadcastle Differential Revision: D43814772)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Transformer encoder compile fail,Summary: Transformer encoder compile fail  CC(compile() breaks TransformerEncoder mask dtype check) Test Plan: github & sadcastle Differential Revision: D43814772,2023-03-08T07:41:30Z,fb-exported Stale topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/96284,This pull request was **exported** from Phabricator. Differential Revision: D43814772,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
460,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Dynamo] HuggingFace transformers configuration_utils graph break workaround)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug HF worked around dynamo problem here https://github.com/huggingface/transformers/pull/21648/filesr1107482201 This should not be necessary, we should fix our code.  Versions master )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Dynamo] HuggingFace transformers configuration_utils graph break workaround," ğŸ› Describe the bug HF worked around dynamo problem here https://github.com/huggingface/transformers/pull/21648/filesr1107482201 This should not be necessary, we should fix our code.  Versions master ",2023-03-07T17:48:55Z,needs reproduction triaged oncall: pt2 module: dynamo dynamo-dataclasses,open,0,6,https://github.com/pytorch/pytorch/issues/96205,I wasn't able to find a way to repro; I asked the author of the transformers PR if they can send one over.,Was able to repro with the bug report at  CC(Runtime error of TorchDynamo compiled model (unflatten)),Prior art for only one config https://github.com/pytorch/torchdynamo/commit/e88bd663a0d6e38d4fa0fd3b737f8311e11585ecdifffee9e700f4a2e99d7072c1cb178d221517a0ac62aca2c39c24211a284cfec4c2R76 We should change dynamo to reflect all configs,Is this still broken?, Was just wondering if you had a chance to look at this issue?,I haven't
1588,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Use Storages for Cudagraph liveness)ï¼Œ å†…å®¹æ˜¯ (  CC(Flip Switch)  CC(Do warmup in cudagraph pool)  CC(Expose Stream Recording Apis in python)  CC(Use Storages for Cudagraph liveness)  CC(cudagraph trees)  CC(Return Live Data Pointers from Checkpoint, swap onto tensors)  CC(Handle additional live allocations not in checkpointed state)  CC(Checkpoint CUDA Allocator Private Pool State) In the previous PR we used tensors for liveness. This isn't actually correct as we should be treating a cudagraph output as dead only when its storage is dead, not the tensor impl output. The previous handling also had a particularly bad interaction with aot autograd detaching, since it would treat all outputs as dead.  I'm using `StorageWeakRef` from `reductions.py`. This is sufficient for checking if a storage has died, although it doesn't allow you to trigger an event immediately upon deallocation. That works fine for most of the handling of the storage weak ref. The one case it doesn't is if you deallocated a torch dynamo function and its cudagraph wrapper, but you still have a Tensor output of it that you need to wait on to deallocate the cudagraph memory pool.  We would need to trigger an event in order to do proper deferred deletion of the cudagraph. I'm deferring that till  CC(PyObject preservation and resurrection for `StorageImpl`) has landed, but that shouldn't be a particularly common case.  : D43999890)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Use Storages for Cudagraph liveness,"  CC(Flip Switch)  CC(Do warmup in cudagraph pool)  CC(Expose Stream Recording Apis in python)  CC(Use Storages for Cudagraph liveness)  CC(cudagraph trees)  CC(Return Live Data Pointers from Checkpoint, swap onto tensors)  CC(Handle additional live allocations not in checkpointed state)  CC(Checkpoint CUDA Allocator Private Pool State) In the previous PR we used tensors for liveness. This isn't actually correct as we should be treating a cudagraph output as dead only when its storage is dead, not the tensor impl output. The previous handling also had a particularly bad interaction with aot autograd detaching, since it would treat all outputs as dead.  I'm using `StorageWeakRef` from `reductions.py`. This is sufficient for checking if a storage has died, although it doesn't allow you to trigger an event immediately upon deallocation. That works fine for most of the handling of the storage weak ref. The one case it doesn't is if you deallocated a torch dynamo function and its cudagraph wrapper, but you still have a Tensor output of it that you need to wait on to deallocate the cudagraph memory pool.  We would need to trigger an event in order to do proper deferred deletion of the cudagraph. I'm deferring that till  CC(PyObject preservation and resurrection for `StorageImpl`) has landed, but that shouldn't be a particularly common case.  : D43999890",2023-03-07T00:10:25Z,topic: not user facing module: inductor ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/96154," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",Squashed into https://github.com/pytorch/pytorch/pull/89146
2011,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(_canonical_mask throws warning when bool masks passed as input to TransformerDecoder)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Similar to CC(_canonical_mask throws warning when bool masks passed as input to TransformerEncoder), but for `TransformerDecoder`  passing `bool` masks results in a warning being thrown about mismatched mask types, as `_canonical_masks` is called multiple times.   ()  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230306+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.067genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.85.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          8 Online CPU(s) l)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,_canonical_mask throws warning when bool masks passed as input to TransformerDecoder," ğŸ› Describe the bug Similar to CC(_canonical_mask throws warning when bool masks passed as input to TransformerEncoder), but for `TransformerDecoder`  passing `bool` masks results in a warning being thrown about mismatched mask types, as `_canonical_masks` is called multiple times.   ()  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230306+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.067genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.85.05 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          8 Online CPU(s) l",2023-03-06T15:31:53Z,,closed,0,6,https://github.com/pytorch/pytorch/issues/96103, this is probably fixed?,Still seeing this issue in the latest nightly build (2.1.0.dev20230307).,"Does the build. you're looking at contain this PR => https://github.com/pytorch/pytorch/pull/96009 This should also include your repro as a unit test, and it passes in CI test_padding_and_src_mask_bool in tests/test_transformer.py, so I'll appreciate if you can check this on your end?","I will double check when I can  but just to confirm that the issue I'm seeing now is with the Decoder, not Encoder, the previous issue I raised was for the Encoder only (which is now fixed). ","Running the code from CC(_canonical_mask throws warning when bool masks passed as input to TransformerEncoder) (testing `TransformerEncoder`, fixed in CC(Transformers: fix src and key padding mask bool regression)) with the latest nightly build (2.1.0.dev20230308+cu118) works fine, no warning is thrown. Running the snippet above (using `TransformerDecoder`) throws the warning from `_canonical_mask`, trace as below: ",Fixed for me with `2.1.0.dev20230313+cu118`  thanks!
2009,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add support for ALiBi/relative positional biases to the fast path for Transformers)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Recent work in transformer architectures such as T5 [1] and Attention with Linear Biases (ALiBi) [2] have shown that moving the positional encodings from the word embedding layer and directly into the selfattention computation improves the capacity of models to extrapolate to longer sentences while keeping lower perplexity scores. Both T5 and ALiBi add biases to the logits resulting from the QK multiplication, turning the softmax operation into something like this: weights = softmax(QK^T + B), where B can either be a constant predefined matrix, like in ALiBi, or a learnable bias, like in T5. In both cases, the values of B are dependent on the relative distance between tokens, with the farthest tokens usually having the highest penalties. Usually, this is implemented through modification of the attention mask to add these extra weights on top, changing it from a boolean yes/no attention to a weighted kind of model. Performancewise, our experiments are summarized in the graph below, with  explaining: ""Here we train 3 AliBi decoders (390M params) with different sequence lengths (2048, 1024, 512) but perform validation at sequence length 2048 all the time. This allows us to track the degree of overfitting to sequence length over time. We then do the same for 3 equivalent models using T5 relative positional bias (rpb) instead of AliBi. While val perplexity of both variants suffers greatly over time with diverging train/val sequence lengths, the degree of increase is worse for rpb, indicating that while generalization to longer sequence lengths is a difficult problem, AliBi is indeed better at it than rpb."" From the ALiBi paper we also know that both perform better than the tradition)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add support for ALiBi/relative positional biases to the fast path for Transformers," ğŸš€ The feature, motivation and pitch Recent work in transformer architectures such as T5 [1] and Attention with Linear Biases (ALiBi) [2] have shown that moving the positional encodings from the word embedding layer and directly into the selfattention computation improves the capacity of models to extrapolate to longer sentences while keeping lower perplexity scores. Both T5 and ALiBi add biases to the logits resulting from the QK multiplication, turning the softmax operation into something like this: weights = softmax(QK^T + B), where B can either be a constant predefined matrix, like in ALiBi, or a learnable bias, like in T5. In both cases, the values of B are dependent on the relative distance between tokens, with the farthest tokens usually having the highest penalties. Usually, this is implemented through modification of the attention mask to add these extra weights on top, changing it from a boolean yes/no attention to a weighted kind of model. Performancewise, our experiments are summarized in the graph below, with  explaining: ""Here we train 3 AliBi decoders (390M params) with different sequence lengths (2048, 1024, 512) but perform validation at sequence length 2048 all the time. This allows us to track the degree of overfitting to sequence length over time. We then do the same for 3 equivalent models using T5 relative positional bias (rpb) instead of AliBi. While val perplexity of both variants suffers greatly over time with diverging train/val sequence lengths, the degree of increase is worse for rpb, indicating that while generalization to longer sequence lengths is a difficult problem, AliBi is indeed better at it than rpb."" From the ALiBi paper we also know that both perform better than the tradition",2023-03-06T15:01:54Z,,closed,11,29,https://github.com/pytorch/pytorch/issues/96099, ,"We are planning on upstreaming the changes introduced in  Xformers for their memefficient kernel that allow for an arbitrary mask to be fused into SDPA. Causal is somewhat special since the mask has a well defined pattern and doesn't require the kernel to materialize it.  It would be kind of nice to create a mask ""zoo"" which could provide a nice abstraction layer the different attention mask familys. I think that some more thought would need to be taken for this but once the above is introduced some perf gains will still be had for the fused kernels with arbitrary additive attention masks.",Do you have an approximate timeline on when that is going to happen?,> Do you have an approximate timeline on when that is going to happen? Likely within a couple of weeks,Any progress? It seems  produces much more memory and time costs with relative pos (attn_mask) provided. Thank u.,> Any progress? It seems `scaled_dot_product_attention` produces much more memory and time costs with relative pos (attn_mask) provided. Thank u. No update yet but will post back here after the upstream,"Hello, According to the documentation, `F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None)` is equivalent to:  Thus, it suffices to feed into the model the bias tensor `B` by setting `attn_mask=B` to achieve what is requested in this issue (the only condition is that `B` is not a boolean tensor).  Am I missing something? Thanks for anh clarifications.","This is correct, but doing so will send the execution of the operator through a slow path instead of a fast one, nullifying any performance gains we could get by using this instead of our operator in the first place. This issue is about supporting arbitrary masks for the fast execution paths.",> > Any progress? It seems `scaled_dot_product_attention` produces much more memory and time costs with relative pos (attn_mask) provided. Thank u. >  > No update yet but will post back here after the upstream  Is this confirmed to be going into the 2.0.1 release (that is happening soon)? Or is the timeline longer?,"No this won't be going into the 2.0.1 release since that is for bug fixes and this is a new feature, not a bug fix. We are planning for the 2.1.0 release.","Just a quick updated here, we have been communicating with xformers and plan on upstreaming by end of april.",Hi ! Am I correct in thinking that this discussion references enabling custom masks just for the torch.backends.cuda.enable_mem_efficient_sdp() backend and not for the flash attention backend? Are there any plans to enable inputting custom masks in flash attention? Thanks!,> Hi ! >  >  >  > Am I correct in thinking that this discussion references enabling custom masks just for the torch.backends.cuda.enable_mem_efficient_sdp() >  > backend and not for the flash attention backend? >  >  >  > Are there any plans to enable inputting custom masks in flash attention? >  >  >  > Thanks! Correct this discussion is only around mem efficient attention. Currently FlashAttention does not support attention bias but you can see that it is on their roadmap posted on their readme: https://github.com/HazyResearch/flashattention,Quick update: https://github.com/pytorch/pytorch/pull/100583 is all green. I will be adding dropout support and generic attention bias in follow up once this PR has landed,: I wanted to follow up on this: > Correct this discussion is only around mem efficient attention. Currently FlashAttention does not support attention bias but you can see that it is on their roadmap posted on their readme: https://github.com/HazyResearch/flashattention This commit: https://github.com/HazyResearch/flashattention/commit/40a25c8ee7465cf547b929cfa2937034e37bfce9 means that they're not going to support custom masks/arbitrary attention bias.  Does that change anything for pytorch? ,Support for arbitrary attention bias is coming from the xformers memoryefficient attention kernel and not from the flash attention implementation," After landing CC(Upstream xformers code), can we already use SDPA with biases and get the fast path, or is there something else that needs to land?",One more PR is needed and that should be up imminetly ,Please make a note or a comment or some other indication that the PR was merged?,The PR has not been merged ,"Apologies, I might have phrased it wrongly; please make some kind of note here once it is?",Do you have plan to add flash_attention 2 ?,  Please see  CC(support FlashAttention-2),"I am going to close this because CC(Meff Attn Bias ) has been landed. This should allow users to create an alibi attn_mask and pass into sdpa. The mem efficient kernel will be called, which in my testing was ~ 2x faster than the math implementation","  I've tested out the new function, and it's impressive! When used during inference, it's significantly faster than the standard alibi attention implementation. However, I encountered a problem when integrating it into the training phase. Specifically, my loss value exploded, even though my training code was previously stable before I swapped the attention mechanism with this new version. Here's the snippet of my code, how I use the new implementation:  Is it possible that the new implementation currently only supports the forward pass and doesn't yet support the backward propagation with alibi?","ericzhou571 So the numerics for the fused kernel are slightly different than that of the unfused math path. It is hard to say how much of a deviation can impact a model's training a priori, could you send some example inputs, sizes for the sdpa components. Otherwise it might be helpful to investigate, any local loss spikes that exist with the fused kernel and don't exist with unfused kernel",Does fast path now support all types of attention masks? e.g: swin transformer https://github.com/cszn/SCUNet https://github.com/ofsoundof/GRLImageRestoration,"Hi Still confused on whether it is possible for rotary and relative positional embeddings to be integrated with the fast kernels in pytorch sdpa, allowing for faster training/inference? If so what would be a blue print of how to merge previous architecture that use both to the new sdpa one ? ","also, it seems that recently flash_attention started to natively support ALiBi via the `alibi_slopes` argument"
2038,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When attempting to run below code snippet,   which makes use of both `torch.compile` as well as `torch.func.jvp`, I get a  > RuntimeError: Cannot access data pointer of Tensor that doesn't have storage When commenting out to the `torch.compile` decorator, however, the code runs seamlessly.  Error logs [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py Traceback (most recent call last):   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 547, in preserve_rng_state     yield   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 925, in wrap_fx_proxy_cls     example_value = wrap_to_fake_tensor_and_record(   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 1062, in wrap_to_fake_tensor_and_record     fake_e = wrap_fake_exception(   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile`," ğŸ› Describe the bug When attempting to run below code snippet,   which makes use of both `torch.compile` as well as `torch.func.jvp`, I get a  > RuntimeError: Cannot access data pointer of Tensor that doesn't have storage When commenting out to the `torch.compile` decorator, however, the code runs seamlessly.  Error logs [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/contextlib.py [20230304 13:47:45,071] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py Traceback (most recent call last):   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/utils.py"", line 547, in preserve_rng_state     yield   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 925, in wrap_fx_proxy_cls     example_value = wrap_to_fake_tensor_and_record(   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python3.10/sitepackages/torch/_dynamo/variables/builder.py"", line 1062, in wrap_to_fake_tensor_and_record     fake_e = wrap_fake_exception(   File ""/space/ahmedk/anaconda3/envs/simple_updated/lib/python",2023-03-04T21:50:31Z,triaged actionable oncall: pt2 module: functorch module: pt2-dispatcher,closed,2,14,https://github.com/pytorch/pytorch/issues/96041, assigned this to you since it's functorch related,"Also CC .  I don't think this is expected to work, but it would be nice if we had a nicer error message.","Yeah, this is not expected to work yet. A nicer error message would be good",I think this can work with `allow_in_graph`.,I'm having the same issue when using torch.compile and torch.func.grad. Is there any workaround?,Do we have a nicer error message or a recommended workaround yet?, I'm pt2_stack_for_oss oncall this week so trying to do some maintenance. Did you remove the triaged label because this issue should go back to triage review?, has a PR out for this.,https://github.com/pytorch/pytorch/pull/119926 should fix this,I also encountered a similar problem when using torch.func.vmap with c++/cuda extensionï¼Œ **Pytorch version: 2.2.1**.  ***Cannot access data pointer of Tensor that doesn't have storage*** ,", which version of PyTorch are you using?","> , which version of PyTorch are you using? Pytorch version: 2.2.1","Oh, just realized you're not using `torch.compile`. This is a different issue, then. , could you create a new issue for the problem you're facing? I'm happy to investigate it further.","> Oh, just realized you're not using `torch.compile`. This is a different issue, then. , could you create a new issue for the problem you're facing? I'm happy to investigate it further.  new issue  CC(Cannot access data pointer of Tensor that doesn't have storage when using torch.func.vmap with c++/cuda extension)"
1039,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Encourage dynamo.export users to assume static by default if they call nonzero / unbacked SymInt)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  has observed that on latest master you can end up with a bunch of ""cannot guard on unbacked SymInt"" failures that look something like `Ne(i0*((s2//2  1)//8)**2 + 2*i0*((s2//2  1)//8) + i0, ((s2//2  1)//8)**2 + 2*((s2//2  1)//8) + 1)`. These look complicated but they actually are quite trivial if you can substitute in s2 (in this case, the substitued guard is `Ne(36*i0, 36)`). This could be a legitimate problem if s2 was actually dynamic, but it's pretty common for it to actually be static (and we're just trying to export a ""maximally"" dynamic model). In this case, upfront marking it as static would solve the problem. We should encourage export users (perhaps by defaulting this way) to assume inputs are static.    Versions master)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Encourage dynamo.export users to assume static by default if they call nonzero / unbacked SymInt," ğŸ› Describe the bug  has observed that on latest master you can end up with a bunch of ""cannot guard on unbacked SymInt"" failures that look something like `Ne(i0*((s2//2  1)//8)**2 + 2*i0*((s2//2  1)//8) + i0, ((s2//2  1)//8)**2 + 2*((s2//2  1)//8) + 1)`. These look complicated but they actually are quite trivial if you can substitute in s2 (in this case, the substitued guard is `Ne(36*i0, 36)`). This could be a legitimate problem if s2 was actually dynamic, but it's pretty common for it to actually be static (and we're just trying to export a ""maximally"" dynamic model). In this case, upfront marking it as static would solve the problem. We should encourage export users (perhaps by defaulting this way) to assume inputs are static.    Versions master",2023-03-04T14:22:39Z,triaged oncall: pt2 module: dynamic shapes oncall: export,closed,0,2,https://github.com/pytorch/pytorch/issues/96033,"> upfront marking it as static would solve the problem Curious, how to do this?",assume_static_by_default
839,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Transformers: fix src and key padding mask bool regression)ï¼Œ å†…å®¹æ˜¯ (Summary: fix src and pad mask bool regression This fixes a regression introduced previously with CC(Regularize mask handling for attn_mask and key_padding_mask). That PR unified testing of masks to remove Byte Tensors as permissible mask, introduced mask compatibility check, and mask conversion to FP mask.  The problem addressed in this PR was that after the first mask had been converted, a check for mask compatibility would fail.   Test Plan: sandcastle & github Differential Revision: D43782858 Fixes   CC(_canonical_mask throws warning when bool masks passed as input to TransformerEncoder))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Transformers: fix src and key padding mask bool regression,"Summary: fix src and pad mask bool regression This fixes a regression introduced previously with CC(Regularize mask handling for attn_mask and key_padding_mask). That PR unified testing of masks to remove Byte Tensors as permissible mask, introduced mask compatibility check, and mask conversion to FP mask.  The problem addressed in this PR was that after the first mask had been converted, a check for mask compatibility would fail.   Test Plan: sandcastle & github Differential Revision: D43782858 Fixes   CC(_canonical_mask throws warning when bool masks passed as input to TransformerEncoder)",2023-03-03T22:00:39Z,fb-exported Merged ciflow/trunk release notes: nn topic: bug fixes topic: not user facing,closed,0,14,https://github.com/pytorch/pytorch/issues/96009,This pull request was **exported** from Phabricator. Differential Revision: D43782858,This pull request was **exported** from Phabricator. Differential Revision: D43782858,This pull request was **exported** from Phabricator. Differential Revision: D43782858,This pull request was **exported** from Phabricator. Differential Revision: D43782858,This pull request was **exported** from Phabricator. Differential Revision: D43782858,Fix issue 95702,This pull request was **exported** from Phabricator. Differential Revision: D43782858,This pull request was **exported** from Phabricator. Differential Revision: D43782858, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3x8664liteinterpreter / build Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," : https://github.com/pytorch/pytorch/milestone/36?closed=1, if you want to see your fix included in this minor release. Please post it as a cherrypick into the [[v2.0.1] Release Tracker]( CC([v2.0.1] Release Tracker)).  **The deadline is April 14, 5PM PST.** **Only issues that have â€˜cherrypicksâ€™ will be considered for the release.** Common FAQs: Q1: Where can I find more information on the release process and terminology? A: pytorch/RELEASE.md at master Â· pytorch/pytorch Â· GitHub Q2: Am I guaranteed to be included in the cherrypick if I do above? A: No, it is not guaranteed, the Release Team will review all submissions against the listed criteria before making the final decision on what to include on 4/17. Q3: When is 2.1 going to be released? A: We do not have a formal date at this time but will update the community when we do. Our immediate focus is 2.0.1. Note that 1.12 was released on 6/28/22, 1.13 on 10/28/22 and 2.0 on 3/15/23. Q4: **I missed the 4/14 5PM PST deadline, is there any option to have an extension?** A: **No, in order to meet our 4/28 goal, we must hold 4/14 as our deadline and will not accept any requests after the fact. We are over communicating the timelines and process with the community to avoid such issues.** Q5: Where should I double check to see if my issue is in the cherry pick tracker? A: [[v2.0.1] Release Tracker Â· Issue CC([v2.0.1] Release Tracker) Â· pytorch/pytorch Â· GitHub]( CC([v2.0.1] Release Tracker)) Q6: Where can I find the Release Compatibility Matrix for PyTorch? A: pytorch/RELEASE.md at master Â· pytorch/pytorch Â· GitHub Please contact OSS Releng team members if you have any questions/comments. Again we appreciate everyoneâ€™s time and commitment to the community, PyTorch and 2.0 and 2.01 releases! Please refer to this post for more details: https://devdiscuss.pytorch.org/t/pytorchrelease201importantinformation/1176"
1970,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Remove mention of dynamo.optimize() in docs)ï¼Œ å†…å®¹æ˜¯ ( [ONNX] Add bloom ops ( CC([ONNX] Add bloom ops))  [MPS] Fixes for LSTM. ( CC([MPS] Fixes for LSTM.))  [MPS] Convert output back to ChannelsLast for MaxPool2D ( CC([MPS] Convert output back to ChannelsLast for MaxPool2D))  inductor(cpu): fix C++ compile error when sigmoid's post ops is a reduction op ( CC(inductor(cpu): fix C++ compile error when sigmoid's post ops is a reduction op))  Reenable a FXtoONNX kwargs Test ( CC(Reenable a FXtoONNX kwargs Test))  Take `CUDA_VISIBLE_DEVICES` into account for nvml calls ( CC(Take `CUDA_VISIBLE_DEVICES` into account for nvml calls))  [BE] Add flake8loggingformat linter ( CC([BE] Add flake8loggingformat linter))  Revert ""Reenable a FXtoONNX kwargs Test ( CC(Reenable a FXtoONNX kwargs Test))""  [pytorch] Add support for ""height"" and ""width"" dimension for the ""select"" operator on pytorch vulkan backend ( CC([pytorch] Add support for ""height"" and ""width"" dimension for the ""select"" operator on pytorch vulkan backend))  Clarify meaning of `pin_memory_device` argument ( CC(Clarify meaning of `pin_memory_device` argument))  [MPS] Fix the crash in elu_backward() ( CC([MPS] Fix the crash in elu_backward()))  try to fix OSS CI error ( CC(try to fix OSS CI error))  Stub all TensorImpl bools; do not go to Python if not hinted. ( CC(Stub all TensorImpl bools; do not go to Python if not hinted.))  Make the glue compute short circuit only if possible ( CC(Make the glue compute short circuit only if possible))  [Inductor] Added aten.normal_ decomp ( CC([Inductor] Added aten.normal_ decomp))  [tp] additional doc fixes ( CC([tp] additional doc fixes))  [MPS] Fix nn.functional.conv_transpose2d grad ( CC([MPS] Fix nn.functional.conv_transpose2d grad))  Fix XNNPACK OSS Buck build ( CC(Fix XNNPACK)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Remove mention of dynamo.optimize() in docs," [ONNX] Add bloom ops ( CC([ONNX] Add bloom ops))  [MPS] Fixes for LSTM. ( CC([MPS] Fixes for LSTM.))  [MPS] Convert output back to ChannelsLast for MaxPool2D ( CC([MPS] Convert output back to ChannelsLast for MaxPool2D))  inductor(cpu): fix C++ compile error when sigmoid's post ops is a reduction op ( CC(inductor(cpu): fix C++ compile error when sigmoid's post ops is a reduction op))  Reenable a FXtoONNX kwargs Test ( CC(Reenable a FXtoONNX kwargs Test))  Take `CUDA_VISIBLE_DEVICES` into account for nvml calls ( CC(Take `CUDA_VISIBLE_DEVICES` into account for nvml calls))  [BE] Add flake8loggingformat linter ( CC([BE] Add flake8loggingformat linter))  Revert ""Reenable a FXtoONNX kwargs Test ( CC(Reenable a FXtoONNX kwargs Test))""  [pytorch] Add support for ""height"" and ""width"" dimension for the ""select"" operator on pytorch vulkan backend ( CC([pytorch] Add support for ""height"" and ""width"" dimension for the ""select"" operator on pytorch vulkan backend))  Clarify meaning of `pin_memory_device` argument ( CC(Clarify meaning of `pin_memory_device` argument))  [MPS] Fix the crash in elu_backward() ( CC([MPS] Fix the crash in elu_backward()))  try to fix OSS CI error ( CC(try to fix OSS CI error))  Stub all TensorImpl bools; do not go to Python if not hinted. ( CC(Stub all TensorImpl bools; do not go to Python if not hinted.))  Make the glue compute short circuit only if possible ( CC(Make the glue compute short circuit only if possible))  [Inductor] Added aten.normal_ decomp ( CC([Inductor] Added aten.normal_ decomp))  [tp] additional doc fixes ( CC([tp] additional doc fixes))  [MPS] Fix nn.functional.conv_transpose2d grad ( CC([MPS] Fix nn.functional.conv_transpose2d grad))  Fix XNNPACK OSS Buck build ( CC(Fix XNNPACK",2023-03-03T21:34:15Z,module: cpu module: amp (automated mixed precision) NNC release notes: quantization release notes: releng ciflow/mps module: inductor module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/96002
392,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix unpicklable object in AveragedModel)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(SWA AveragedModel saving issue: avg_fn is unpickable) Don't store the callable `avg_fn`, instead test if `avg_fn` is None and call the default impl if it's not.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix unpicklable object in AveragedModel,"Fixes CC(SWA AveragedModel saving issue: avg_fn is unpickable) Don't store the callable `avg_fn`, instead test if `avg_fn` is None and call the default impl if it's not.",2023-03-03T17:10:21Z,triaged open source Merged ciflow/trunk release notes: nn,closed,0,5,https://github.com/pytorch/pytorch/issues/95979,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: DavidFM43 / name: DavidFM43  (0787633360797d3b2707b5cc0e732d78ebf2da76, 2a62f3c19b94c07a0865a5cd7591806e3e77ddf9)",Sure,ping  , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1979,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(compile() breaks TransformerEncoder mask dtype check)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Running `compile()` on a function that builds an attention mask and passes a tensor through a `TransformerEncoder` results in the dtype of the mask getting changed, triggering an exception:  Output:   Output for non compiled function:   Error logs _No response_  Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230302+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.78.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,compile() breaks TransformerEncoder mask dtype check," ğŸ› Describe the bug Running `compile()` on a function that builds an attention mask and passes a tensor through a `TransformerEncoder` results in the dtype of the mask getting changed, triggering an exception:  Output:   Output for non compiled function:   Error logs _No response_  Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.1.0.dev20230302+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.78.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   39 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                   ",2023-03-03T09:36:57Z,oncall: pt2,closed,0,0,https://github.com/pytorch/pytorch/issues/95958
1792,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Runtime error of TorchDynamo compiled model (unflatten))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Use torch dynamo to compiled a model (a bert followed with a TransformerEncoderLayer), compile successed but got an error in runtime. Simple code to reproduce:  and the details of config:  Error log:   Versions Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] pytorchtriton==2.0.0+b8b470bc59 [pip3] torch==2.0.0.dev20230228+cu117 [pip3] torchaudio==2.0.0.dev20230301+cu117 [pip3] torchvision==0.15.0.dev20230227+cu117 [conda] blas                      1.0                         mkl [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0           py310h7f8727e_0 [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0 [conda] mkl_random                1.2.2           py310h00e6091_0 [conda] numpy                     1.24.1                   pypi_0    pypi [conda] pytorchcuda              11.6                 h867d48c_1    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] pytorchtriton            2.0.0+b8b470bc59          pypi_0    pypi [conda] torch                     2.0.0.dev20230228+cu117          pypi_0    pypi [conda] torchaudio                2.0.0.dev20230301+cu117          pypi_0    pypi [conda] torchelastic              0.2.2                    pypi_0    pypi [conda] torchtext                 0.14.1                    py310    pytorch [conda] torchvision               0.15.0.dev20230227+cu117          pypi_0    pypi )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Runtime error of TorchDynamo compiled model (unflatten)," ğŸ› Describe the bug Use torch dynamo to compiled a model (a bert followed with a TransformerEncoderLayer), compile successed but got an error in runtime. Simple code to reproduce:  and the details of config:  Error log:   Versions Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] pytorchtriton==2.0.0+b8b470bc59 [pip3] torch==2.0.0.dev20230228+cu117 [pip3] torchaudio==2.0.0.dev20230301+cu117 [pip3] torchvision==0.15.0.dev20230227+cu117 [conda] blas                      1.0                         mkl [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0           py310h7f8727e_0 [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0 [conda] mkl_random                1.2.2           py310h00e6091_0 [conda] numpy                     1.24.1                   pypi_0    pypi [conda] pytorchcuda              11.6                 h867d48c_1    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] pytorchtriton            2.0.0+b8b470bc59          pypi_0    pypi [conda] torch                     2.0.0.dev20230228+cu117          pypi_0    pypi [conda] torchaudio                2.0.0.dev20230301+cu117          pypi_0    pypi [conda] torchelastic              0.2.2                    pypi_0    pypi [conda] torchtext                 0.14.1                    py310    pytorch [conda] torchvision               0.15.0.dev20230227+cu117          pypi_0    pypi ",2023-03-02T07:25:49Z,oncall: pt2 module: dynamic shapes,closed,0,12,https://github.com/pytorch/pytorch/issues/95868,   can you assign to someone who's worked on dynamic shapes?,Bypassing loading the pretrained model with https://gist.github.com/e011f6954632147523136b0102270c68 on master I get  which looks like straightforward coverage problem,yup it's unflatten ,https://github.com/pytorch/pytorch/pull/96100 should help,With the most recent version of 96100 the benchmark script runs to the end and prints 1. I'm not sure if we actually managed to make it dynamic though haha.,"Oh but I also am running with randomly initialized weights, not the pretrained model.","Unfortunately, it looks like the inputs all got specialized.   another one for guard lineage. .","> Oh but I also am running with randomly initialized weights, not the pretrained model. Well I think running with initialized weights has no difference to pretrained model lol","Once I fix the graph break in HF, it looks like we get a graph that doesn't overspecialize. Test script at https://gist.github.com/bdb46cddfe27ed441c508a3ecea12dcd","It looks like the overspecialization is because we are still specializing ints that are in lists. Here is how I figured it out: 1. I ran the model with `torch._dynamo.config.print_graph_breaks = True`. This let me see that there was a graph break in HF.  This reminded me of  CC(torch.compile fails when compiling a T5-style model with HF interfaces)issuecomment1457029109 where there is a known graph break in HF but if you fix it the graph break goes away. So I patched HF to remove the graph break and then confirmed with `torch._dynamo.mark_dynamic_dim` that without the graph break, everything was fine. I then posted  CC(Runtime error of TorchDynamo compiled model (unflatten))issuecomment1460746460 2. This reminded me of a problem I recently fixed at  CC(OpenNMT doesn't work with dynamic torch.compile) where overspecialization was due to a size variable passed across a graph break that then got specialized on. So I set `torch._dynamo.config.specialize_int = False`, but this did not fix the specialization. RIP. 3. I wanted to get a backtrace for where the specialization occurred. The raised error from `mark_dynamic_dim` happens at the very end of execution so that's not useful. I decided to patch `ShapeEnv.evaluate_expr` to print the guards we were adding. Interestingly, because I was using `torch._dynamo.config.assume_static_by_default = True` there actually aren't very many guards, and it was easy to find the naughty one. (, I take it back; if there are not too many guards, it is plausible to print the backtrace at every single one, and then just search the logs for the correct backtrace.) However, the backtrace was inside some broadcasting code in PrimTorch: not useful, because chances were that the broadcasting code was fine and the real problem was in one of the tensor inputs. 4. I turned on DEBUG logging for torchdynamo. I could easily find the spot in the logs where the specialization occurred. But dynamo logging by default doesn't print sizes of tensors on the stack, and the bytecode is hard to read anyway, so I really couldn't make heads or tails of it. 5. However, the DEBUG logging does print what line of user code you're in. This revealed that we were failing because of this addition:  6. I decided I wanted to just read BERT code until I could predict what the problem was. But which tensor was the one with the static shape? I decided to figure this out with a comptime print:  This revealed that `attention_mask` was the problem. But where did it come from?  I could get the user backtrace using:  This gave me:  I then walked up the stack until I found this code:  where `input_shape` was a list of ints, and I concluded that this must be what we overspecialized on.  ",Here is a minimal repro of the problem:  This fails as is but passes if you remove the graph break.,"Hi  could you please post this and other issues in milestones as cherry picks in release tracker ? This issue is in the  milestones : https://github.com/pytorch/pytorch/milestone/36?closed=1, if you want to see your fix included in this minor release. Please post it as a cherrypick into the [[v2.0.1] Release Tracker]( CC([v2.0.1] Release Tracker)).  **The deadline is April 14, 5PM PST.** **Only issues that have â€˜cherrypicksâ€™ will be considered for the release.** Common FAQs: Q1: Where can I find more information on the release process and terminology? A: pytorch/RELEASE.md at master Â· pytorch/pytorch Â· GitHub Q2: Am I guaranteed to be included in the cherrypick if I do above? A: No, it is not guaranteed, the Release Team will review all submissions against the listed criteria before making the final decision on what to include on 4/17. Q3: When is 2.1 going to be released? A: We do not have a formal date at this time but will update the community when we do. Our immediate focus is 2.0.1. Note that 1.12 was released on 6/28/22, 1.13 on 10/28/22 and 2.0 on 3/15/23. Q4: **I missed the 4/14 5PM PST deadline, is there any option to have an extension?** A: **No, in order to meet our 4/28 goal, we must hold 4/14 as our deadline and will not accept any requests after the fact. We are over communicating the timelines and process with the community to avoid such issues.** Q5: Where should I double check to see if my issue is in the cherry pick tracker? A: [[v2.0.1] Release Tracker Â· Issue CC([v2.0.1] Release Tracker) Â· pytorch/pytorch Â· GitHub]( CC([v2.0.1] Release Tracker)) Q6: Where can I find the Release Compatibility Matrix for PyTorch? A: pytorch/RELEASE.md at master Â· pytorch/pytorch Â· GitHub Please contact OSS Releng team members if you have any questions/comments. Again we appreciate everyoneâ€™s time and commitment to the community, PyTorch and 2.0 and 2.01 releases! Please refer to this post for more details: https://devdiscuss.pytorch.org/t/pytorchrelease201importantinformation/1176"
1993,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Sparse tensor conversion methods yield false results when chained )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Chaining `.to_sparse_{csr,csc,coo}()` methods changes the initial matrix. For instance, with CSR matrices, the following snippet  yields different matrices when we actually expect them to be the same:  I observe a similar bug with CSC matrices:   I think exploring the conversion steps one by one should yield insight on what needs to be fixed :slightly_smiling_face:   Versions PyTorch version: 1.13.1+cu117                                                                                                                                                                                       Is debug build: False                                                                                                                                                                                               CUDA used to build PyTorch: 11.7                                                                                                                                                                                    ROCM used to build PyTorch: N/A                                                                                                                                                                                     OS: elementary OS 7 Horus (x86_64)                                                                                                                                                                                  GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0                                                                                                                                                                  Clang version: Could not collect                                         )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Sparse tensor conversion methods yield false results when chained ," ğŸ› Describe the bug Chaining `.to_sparse_{csr,csc,coo}()` methods changes the initial matrix. For instance, with CSR matrices, the following snippet  yields different matrices when we actually expect them to be the same:  I observe a similar bug with CSC matrices:   I think exploring the conversion steps one by one should yield insight on what needs to be fixed :slightly_smiling_face:   Versions PyTorch version: 1.13.1+cu117                                                                                                                                                                                       Is debug build: False                                                                                                                                                                                               CUDA used to build PyTorch: 11.7                                                                                                                                                                                    ROCM used to build PyTorch: N/A                                                                                                                                                                                     OS: elementary OS 7 Horus (x86_64)                                                                                                                                                                                  GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0                                                                                                                                                                  Clang version: Could not collect                                         ",2023-03-01T20:16:30Z,module: sparse triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/95814,  can you take a look?,"Actually, I just tried to run this locally a on recent nightly and got    Maybe this is already fixed?","I can both reproduce the issue with stock PyTorch 1.13.1, and confirm that the snippet works as expected with a recent build.  Most probably it's fixed with my recent updates in the sparse conversions code.","Closing as this is confirmed fixed in the current version.  using the latest nightly will resolve your issues in the short term. Long term, the 2.0 release will be out next week and upgrading to that version will also provide resolution on a stable version.  "
2032,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP - Avoid loading all parameters to GPU before cpu offloading, which is necessary to load large models)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch  Context  We are working on large language models with FSDP. We turn on cpu offloading to explore larger models.  Issue The largest model we can load with cpu offloading seems upperbounded by the size of the whole model, instead of a Transformer block being wrapped by FSDP. This significantly limits the maximal size of large models we can explore.  Reason After diving deep, we find the limitation comes from `_move_module_to_device` in torch/distributed/fsdp/_init_utils.py   Obviously, the whole module is loaded to GPU before its submodules are offloaded back to cpu. In the FSDP initialization, a model and its submodules are recursively called and wrapped by FSDP. A very last step is to wrap the whole model, and causes the issue we observed above.  There is a `NOTE` in the above code, making us think it might be due to a design choice for some other good reasons. So, it seems suitable for us to submit this as a feature request, instead of reporting it as a bug. There is also a `TODO` in the above code; however it is not clear whether it is to exactly address the same issue we discussed here.   Workaround As a workaround, we define a function `_move_modules_not_under_fsdp_offload_to_device` to only load to GPU the modules that are not under another FSDP modules with cpu_offload. That is, we replace the line of  by   with the following def:   Feature Request We's like to request a feature that avoids loading all parameters to GPU before cpu offloading, because it is important to load large models.  Please review if the above workaround makes sense (or better off to combine it with the subsequence code of cpu offloading in `_move_module_to_device`). We are willing to submit a PR if)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"FSDP - Avoid loading all parameters to GPU before cpu offloading, which is necessary to load large models"," ğŸš€ The feature, motivation and pitch  Context  We are working on large language models with FSDP. We turn on cpu offloading to explore larger models.  Issue The largest model we can load with cpu offloading seems upperbounded by the size of the whole model, instead of a Transformer block being wrapped by FSDP. This significantly limits the maximal size of large models we can explore.  Reason After diving deep, we find the limitation comes from `_move_module_to_device` in torch/distributed/fsdp/_init_utils.py   Obviously, the whole module is loaded to GPU before its submodules are offloaded back to cpu. In the FSDP initialization, a model and its submodules are recursively called and wrapped by FSDP. A very last step is to wrap the whole model, and causes the issue we observed above.  There is a `NOTE` in the above code, making us think it might be due to a design choice for some other good reasons. So, it seems suitable for us to submit this as a feature request, instead of reporting it as a bug. There is also a `TODO` in the above code; however it is not clear whether it is to exactly address the same issue we discussed here.   Workaround As a workaround, we define a function `_move_modules_not_under_fsdp_offload_to_device` to only load to GPU the modules that are not under another FSDP modules with cpu_offload. That is, we replace the line of  by   with the following def:   Feature Request We's like to request a feature that avoids loading all parameters to GPU before cpu offloading, because it is important to load large models.  Please review if the above workaround makes sense (or better off to combine it with the subsequence code of cpu offloading in `_move_module_to_device`). We are willing to submit a PR if",2023-03-01T20:13:21Z,oncall: distributed module: fsdp,closed,2,3,https://github.com/pytorch/pytorch/issues/95813,"Thanks for this issue! This is a great investigation and point. Like the other backward resharding issue, let me discuss with the other FSDP developers, though this one should be easier to fix (especially since you already have one way to fix).","Related:  CC([FSDP] FSDP with CPU offload consumes `1.65X` more GPU memory when training models with most of the params frozen),  CC(Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading)"," What do you think about support for a `nn.Module.to()` method / `nn.Module._apply()` that does not apply recursively to all submodules but only the target module? For FSDP, we may want to only move some modules but not _all_ submodules to GPU."
1959,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(NanoGPT training crashes on ROCm)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When training nanoGPT on ROCm (MI250x) GPUs, I get a memory access fault:   Expected outcome Training continues  Versions Please see the nanoGPT repository for how to install the reproducer and dependencies. In a nutshell:  Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: SUSE Linux Enterprise Server 15 SP3 (x86_64) GCC version: (SUSE Linux) 7.5.0 Clang version: 14.0.2  (ecfd9ef4dfd5696cd449133c0da0293d503c2f21) CMake version: version 3.17.0 Libc version: glibc2.31 Python version: 3.8.16 (default, Jan 17 2023, 23:13:24)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.3.18150300.59.87_11.0.78cray_shasta_cx86_64withglibc2.17 Is CUDA available: N/A CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          128 Online CPU(s) list:             0127 Thread(s) per core:              2 Core(s) per socket:              64 Socket(s):                       1 NUMA node(s):                    4 Vendor ID:                       AuthenticAMD CPU family:                      25 Model:                           48 Model name:                      AMD EPYC 7A53 64Core Processor Stepping:                        1 Frequency boost:                 enabled CPU MHz:       )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,NanoGPT training crashes on ROCm," ğŸ› Describe the bug When training nanoGPT on ROCm (MI250x) GPUs, I get a memory access fault:   Expected outcome Training continues  Versions Please see the nanoGPT repository for how to install the reproducer and dependencies. In a nutshell:  Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: SUSE Linux Enterprise Server 15 SP3 (x86_64) GCC version: (SUSE Linux) 7.5.0 Clang version: 14.0.2  (ecfd9ef4dfd5696cd449133c0da0293d503c2f21) CMake version: version 3.17.0 Libc version: glibc2.31 Python version: 3.8.16 (default, Jan 17 2023, 23:13:24)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.3.18150300.59.87_11.0.78cray_shasta_cx86_64withglibc2.17 Is CUDA available: N/A CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          128 Online CPU(s) list:             0127 Thread(s) per core:              2 Core(s) per socket:              64 Socket(s):                       1 NUMA node(s):                    4 Vendor ID:                       AuthenticAMD CPU family:                      25 Model:                           48 Model name:                      AMD EPYC 7A53 64Core Processor Stepping:                        1 Frequency boost:                 enabled CPU MHz:       ",2023-03-01T19:43:19Z,module: rocm triaged,closed,0,7,https://github.com/pytorch/pytorch/issues/95808,"Hi  , which docker container did you use? I pulled the docker container `rocm/pytorch:latest`, and I'm able to use the following steps for training:  ","I am using a locally built pyorch, installed via pip. I did not try the docker container.",P.S.: Source commit  ,"> Hi  , which docker container did you use? I pulled the docker container `rocm/pytorch:latest`, and I'm able to use the following steps for training: >  >  I will also try your container","I spoke too soon. The system I am running on is an HPC system and does not currently support containers, unfortunately https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html On a side note, I noticed in your output that the line ""using fused AdamW: False""  differs (mine says `True`).  Just to make sure, would it be possible for you to run the example in `rocgdb`?","I was able to reproduce your reported issue with the nightly WHL, we'll take a deeper look there and update.. thanks for reporting the problem.  Also I can confirm the workload trains well with the stable release whl installed with the following instruction:  Or use the `rocm/pytorch:latest` container..",Thanks.  Please note that I provided a fix in this PR CC(Fix null pointer dereference on ROCm) 
2046,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(FSDP - Allow to reshard frozen parameters in the backward pass, for parameter-efficient trainings of much larger models)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch  Context  We are working on parameterefficient finetuning of large language models where 99%+ or 99.9%+ parameters are frozen. That is, we freeze all the Transformer parameters, while only tuning a small % of extra parameters (e.g., in LoRA by using Huggingface PEFT). We further turn on activation checkpointing and cpu offloading because the model is large.   Issue FSDP works in the forward pass (i.e., flat CUDA memory usage after the forward pass of each Transformer block), but fails in the backward pass because of continuous increase of CUDA memory after the backward pass of each block.   Reason After diving deep, we find it is because `_post_backward_hook` is not registered for FlatParameter with `requires_grad=False` in `_register_post_backward_hooks`. Related code torch/distributed/fsdp/_runtime_utils.py   As a result, the FlatParameter of each frozen Transformer block (`requires_grad=False`) is first unsharded before its backward calculation (in the `_pre_backward_hook`), but never got reshareded afterwards (i.e., NO `_post_backward_hook` is triggered). So the CUDA memory continuously increases after each block's backward calculation, which is different from the flat CUDA memory usage in the forward pass of each block (unshard, then reshard)  Workaround As a workaround, we set requires_grad=True for all Transformer blocks, but don't include them in the optimizer. This leads to unnecessary CUDA memory for grad, but not for optimizer states. This way, we observe transformer FlatParameter is reshared in the backward pass, and we can increase the model size to a certain extent.   Feature Request We's like to request a feature that allows to reshard frozen parameters (i.e., Fl)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"FSDP - Allow to reshard frozen parameters in the backward pass, for parameter-efficient trainings of much larger models"," ğŸš€ The feature, motivation and pitch  Context  We are working on parameterefficient finetuning of large language models where 99%+ or 99.9%+ parameters are frozen. That is, we freeze all the Transformer parameters, while only tuning a small % of extra parameters (e.g., in LoRA by using Huggingface PEFT). We further turn on activation checkpointing and cpu offloading because the model is large.   Issue FSDP works in the forward pass (i.e., flat CUDA memory usage after the forward pass of each Transformer block), but fails in the backward pass because of continuous increase of CUDA memory after the backward pass of each block.   Reason After diving deep, we find it is because `_post_backward_hook` is not registered for FlatParameter with `requires_grad=False` in `_register_post_backward_hooks`. Related code torch/distributed/fsdp/_runtime_utils.py   As a result, the FlatParameter of each frozen Transformer block (`requires_grad=False`) is first unsharded before its backward calculation (in the `_pre_backward_hook`), but never got reshareded afterwards (i.e., NO `_post_backward_hook` is triggered). So the CUDA memory continuously increases after each block's backward calculation, which is different from the flat CUDA memory usage in the forward pass of each block (unshard, then reshard)  Workaround As a workaround, we set requires_grad=True for all Transformer blocks, but don't include them in the optimizer. This leads to unnecessary CUDA memory for grad, but not for optimizer states. This way, we observe transformer FlatParameter is reshared in the backward pass, and we can increase the model size to a certain extent.   Feature Request We's like to request a feature that allows to reshard frozen parameters (i.e., Fl",2023-03-01T19:06:41Z,oncall: distributed module: fsdp,closed,9,5,https://github.com/pytorch/pytorch/issues/95805,"Thanks for the detailed issue! Let me discuss with the other FSDP developers as well. The forefront question in my mind is how we can tell when it is safe (with respect to timing) to reshard the frozen parameters. Normally, the postbackward hook running signifies that gradient computation involving the parameters has completed, so it is safe to reshard. I do not have suggestion for debugging backward pass either :/"," Thanks for the prompt response.  I am thinking about two situations:  All the parameters of a FSDP module are frozen (including the parameters of all its submodules, which might be also wrapped by FSDP  let's refer them as FSDP submodule below).   All the parameters of a FSDP module (except for some of its FSDP submodules) are frozen.  In the first situation, it might be more straightforward and safe to reshard. In the second situation, is it also safe to reshard the FSDP module's own frozen parameters in its postbackward hook (which is called after all its FSDP submodules have finished calculation including gradients)? One exception could be that a FSDP module with frozen parameters is called twice (for whatever reason) in the backward pass of the whole model. But in this case, the frozen parameters will be unsharded in the prebackward hook when the FSDP module is called again; so still safe, right? I have probably missed some edge cases, but will be glad to learn and get your help on this.","Apologies for the delay. > Even in the above workaround, we still observe some continuous CUDA memory increase after the backward pass of each Transformer block. I was wondering, could the increase just be from the gradients computed for the ""frozen"" `FlatParameter`s that you forced to use `requires_grad=True`? > In the first situation, it might be more straightforward and safe to reshard. In the second situation, is it also safe to reshard the FSDP module's own frozen parameters in its postbackward hook (which is called after all its FSDP submodules have finished calculation including gradients)? I may be misunderstanding your idea, so please feel free to correct me if I am wrong. My main concern is that there _cannot_ be a postbackward hook for a `FlatParameter` that does not require gradient. Normally, the postbackward hook is registered on the `FlatParameter'`s `AccumulateGrad` function object that runs when its gradient is finalized. However, if the `FlatParameter` does not require gradient, I do not think that `AccumulateGrad` object exists. Registering the hook on the `AccumulateGrad` object gives us the correct timing to reshard. Off the top of my mind, there may be an ""advanced"" technique that might address this. However, I am not sure when we will have the bandwidth to try this out. Here is what I am thinking:  https://github.com/pytorch/pytorch/pull/86260 added something called a multigrad hook that takes in a list of tensors and runs the hook only when all tensors in the list have their gradients computed.  For an FSDP module with a `FlatParameter` with `requires_grad=False`, we register a multigrad hook on the _input tensors_ of that FSDP module's forward that require gradient. This hook only performs resharding of the `FlatParameter` without any gradient reduction (i.e. reducescatter).  If I am understanding correctly, we can assume that the `FlatParameter`'s original parameters are only needed to compute the gradient with respect to the input tensors. In that case, the `FlatParameter` should only be resharded after it is no longer needed.  We need the multigrad hook because there may be multiple input tensors that require gradient, and we do not know which will be the last to compute its gradient. In short, FSDP normally uses the point when the `FlatParameter` gradient is finalized to reshard it, but since that does not exist for `requires_grad=False`, we can try to use the point when the input tensors' gradients are finalized.", Do you have any recommendation for a workload that I can test on (including any instructions to get it set up)? I want to test out a PR to see if it helps.,Marking this as closed due to https://github.com/pytorch/pytorch/pull/101982
1983,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(CUDA IMA on forward pass of BertForMaskedLM with PT2 RC )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When running the forward pass of a transformers `BertForMaskedLM` with `torch.compile`, an illegal memory CUDA error appears. The same code runs fine without `torch.compile`ing the model. Repro:  The script works fine when compiling with `""aot_eager""`.  Error logs   Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.0.0.dev20230301+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01027gcpx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB Nvidia driver version: 515.86.01 cuDNN version: Probably one of the following: /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_train.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_train.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_train.so.8 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:            )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,CUDA IMA on forward pass of BertForMaskedLM with PT2 RC ," ğŸ› Describe the bug When running the forward pass of a transformers `BertForMaskedLM` with `torch.compile`, an illegal memory CUDA error appears. The same code runs fine without `torch.compile`ing the model. Repro:  The script works fine when compiling with `""aot_eager""`.  Error logs   Minified repro _No response_  Versions Collecting environment information... PyTorch version: 2.0.0.dev20230301+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01027gcpx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM440GB Nvidia driver version: 515.86.01 cuDNN version: Probably one of the following: /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_train.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_train.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_infer.so.8 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_train.so.8 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:            ",2023-03-01T16:29:11Z,high priority oncall: pt2,closed,0,4,https://github.com/pytorch/pytorch/issues/95794,"To repro using Rc use following commands: ` if you see error similar to this:  do:  Confirmed with Python 3.9, CUDA 11.7",could it be related to following IMA failure :  CC(TorchInductor fails with memoy violations in `test_comprehensive_grid_sampler_2d_cuda_float16` and `test_reflection_pad2d_dynamic_shapes_cuda`) ?,I can reproduce the issue and the failing kernel is reported as:  Is there a way to get more information what `triton__0d1d2d34d` is or how it was built? ,"The problem is in nll_loss_forward decomposition that doesn't ignore ignore_index if it's negative.  for a quick workaround you can set `ignore_index` to some large positive number, I'll send a fix soon. "
739,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(extract out TransformFallback and UnaryInvolutionFallback types)ï¼Œ å†…å®¹æ˜¯ (extract out TransformFallback and UnaryInvolutionFallback types Summary: TransformFallback is the generic interface for implementation a tensor transformation using a dispatch key. This has been done twice before with the negation and conjugate unary operators. We intend to do it again with infallible view and compositional as_strided, which are somewhat more complex than the unary involution views. Test Plan: Should be a noop, rely on existing tests. Reviewers: Subscribers: Tasks: Tags:)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,extract out TransformFallback and UnaryInvolutionFallback types,"extract out TransformFallback and UnaryInvolutionFallback types Summary: TransformFallback is the generic interface for implementation a tensor transformation using a dispatch key. This has been done twice before with the negation and conjugate unary operators. We intend to do it again with infallible view and compositional as_strided, which are somewhat more complex than the unary involution views. Test Plan: Should be a noop, rely on existing tests. Reviewers: Subscribers: Tasks: Tags:",2023-03-01T15:55:00Z,open source Stale topic: not user facing ciflow/mps,closed,0,2,https://github.com/pytorch/pytorch/issues/95788," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
318,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Rearrange some transformer tests)ï¼Œ å†…å®¹æ˜¯ (This changes the test placement to be more inline with the class hierarchy in the test_transformers.py)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Rearrange some transformer tests,This changes the test placement to be more inline with the class hierarchy in the test_transformers.py,2023-03-01T00:15:33Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/95745, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
706,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: new(): expected key in DispatchKeySet(CPU, CUDA, HIP, XLA, MPS, IPU, XPU, HPU, Lazy, Meta) but got: PrivateUse1)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Microsoft directml custom backend for pytorch gpu acceleration in WSL receives error in huggingface transormer .generate method. directml_torch reference:  https://learn.microsoft.com/enus/windows/ai/directml/gpupytorchwindows huggingface transformer reference: https://github.com/huggingface/transformers/blob/v4.26.1/src/transformers/generation/utils.pyL2424 ` `  Versions ` )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"RuntimeError: new(): expected key in DispatchKeySet(CPU, CUDA, HIP, XLA, MPS, IPU, XPU, HPU, Lazy, Meta) but got: PrivateUse1", ğŸ› Describe the bug Microsoft directml custom backend for pytorch gpu acceleration in WSL receives error in huggingface transormer .generate method. directml_torch reference:  https://learn.microsoft.com/enus/windows/ai/directml/gpupytorchwindows huggingface transformer reference: https://github.com/huggingface/transformers/blob/v4.26.1/src/transformers/generation/utils.pyL2424 ` `  Versions ` ,2023-02-28T21:40:43Z,module: windows triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/95734,This should probably be filed against https://github.com/microsoft/DirectML cc:  aubrecht ,"Sure, I went ahead and logged here because CC(expected key in DispatchKeySet(CPU, CUDA, ....) but got: MPS) was the exact same issue but for a different backend.  Is the PrivateUse1 custom backend not intended to be supported in the same way for the tensor.new method? Linked directml issue: https://github.com/microsoft/DirectML/issues/400","So `torch.new()` is a legacy constructor (and you should probably prefer to use other factor functions like `torch.tensor(), torch.zeros(), torch.empty()`, etc). But I think we should still fix this, since the `PrivateUse1` dispatch key is meant to be used for any outoftree backends.",The commits was reverted and the branch was deleted.,This should be available in pytorch releases >= v2.1.0
1025,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Faketensor issue when using torch inductor as backend with Huggingface Trainer API)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I use the `torch_compile_backend=inductor` in a Huggingface training script based on Trainer API, I got error at the first step of training`torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.convolution.default(*(FakeTensor(FakeTensor(..., device='meta', size=(1, 3, 224, 224)), cuda:0), tensor([[[ 1.5585e02,  5.1153e02,  5.5507e02,  ...,  8.7095e02,...` in the forward pass. Trainers API uses [one single line wrap to enable torch.compile. Here is the minified version of the code. Note: the minified code does not produce the same error. To reproduce the original error use:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Faketensor issue when using torch inductor as backend with Huggingface Trainer API," ğŸ› Describe the bug When I use the `torch_compile_backend=inductor` in a Huggingface training script based on Trainer API, I got error at the first step of training`torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.convolution.default(*(FakeTensor(FakeTensor(..., device='meta', size=(1, 3, 224, 224)), cuda:0), tensor([[[ 1.5585e02,  5.1153e02,  5.5507e02,  ...,  8.7095e02,...` in the forward pass. Trainers API uses [one single line wrap to enable torch.compile. Here is the minified version of the code. Note: the minified code does not produce the same error. To reproduce the original error use:   Versions  ",2023-02-28T21:08:35Z,triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/95731,I think this is probably the same bug as  CC(TorchDynamo doesn't inline modified nn.Modules forward - Fails with Huggingface Accelerate),   I hit this issue outside of HuggingFace when trying a TorchServe example. I do not believe any hooks were involved in this case: ,Command no longer fails for me locally  closing as fixed.
2011,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(_canonical_mask throws warning when bool masks passed as input to TransformerEncoder)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Passing bool masks to `TransformerEncoder` causes warnings to be raised:  > ~/research/pt2/lib/python3.9/sitepackages/torch/nn/functional.py:5004: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead. > ~/research/pt2/lib/python3.9/sitepackages/torch/nn/functional.py:5004: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead. These warnings should not be raised, since the two masks are the same dtype. It looks like `F._canonical_mask` gets called 3 times, by `TransformerEncoder`, `TransformerEncoderLayer` and `MultiHeadAttention`: the first call converts a bool padding mask to float, which then doesn't match the src_mask on subsequent calls.  Versions Collecting environment information...                                                                                                                                         PyTorch version: 2.0.0.dev20230228+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.78.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,_canonical_mask throws warning when bool masks passed as input to TransformerEncoder," ğŸ› Describe the bug Passing bool masks to `TransformerEncoder` causes warnings to be raised:  > ~/research/pt2/lib/python3.9/sitepackages/torch/nn/functional.py:5004: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead. > ~/research/pt2/lib/python3.9/sitepackages/torch/nn/functional.py:5004: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead. These warnings should not be raised, since the two masks are the same dtype. It looks like `F._canonical_mask` gets called 3 times, by `TransformerEncoder`, `TransformerEncoderLayer` and `MultiHeadAttention`: the first call converts a bool padding mask to float, which then doesn't match the src_mask on subsequent calls.  Versions Collecting environment information...                                                                                                                                         PyTorch version: 2.0.0.dev20230228+cu118 Is debug build: False CUDA used to build PyTorch: 11.8 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.2 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 525.78.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.2 /usr/",2023-02-28T14:27:09Z,triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/95702,It looks like `_canonical_mask` was introduced in CC(Regularize mask handling for attn_mask and key_padding_mask)   could you take a look?," : https://github.com/pytorch/pytorch/milestone/36?closed=1, if you want to see your fix included in this minor release. Please post it as a cherrypick into the [[v2.0.1] Release Tracker]( CC([v2.0.1] Release Tracker)).  **The deadline is April 14, 5PM PST.** **Only issues that have â€˜cherrypicksâ€™ will be considered for the release.** Common FAQs: Q1: Where can I find more information on the release process and terminology? A: pytorch/RELEASE.md at master Â· pytorch/pytorch Â· GitHub Q2: Am I guaranteed to be included in the cherrypick if I do above? A: No, it is not guaranteed, the Release Team will review all submissions against the listed criteria before making the final decision on what to include on 4/17. Q3: When is 2.1 going to be released? A: We do not have a formal date at this time but will update the community when we do. Our immediate focus is 2.0.1. Note that 1.12 was released on 6/28/22, 1.13 on 10/28/22 and 2.0 on 3/15/23. Q4: **I missed the 4/14 5PM PST deadline, is there any option to have an extension?** A: **No, in order to meet our 4/28 goal, we must hold 4/14 as our deadline and will not accept any requests after the fact. We are over communicating the timelines and process with the community to avoid such issues.** Q5: Where should I double check to see if my issue is in the cherry pick tracker? A: [[v2.0.1] Release Tracker Â· Issue CC([v2.0.1] Release Tracker) Â· pytorch/pytorch Â· GitHub]( CC([v2.0.1] Release Tracker)) Q6: Where can I find the Release Compatibility Matrix for PyTorch? A: pytorch/RELEASE.md at master Â· pytorch/pytorch Â· GitHub Please contact OSS Releng team members if you have any questions/comments. Again we appreciate everyoneâ€™s time and commitment to the community, PyTorch and 2.0 and 2.01 releases! Please refer to this post for more details: https://devdiscuss.pytorch.org/t/pytorchrelease201importantinformation/1176"
1997,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inference error when trying Streaming ASR Mobile Dynamic Quantization )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug trying to apply quantization to this pytorch model code: https://github.com/pytorch/androiddemoapp/blob/master/StreamingASR/generate_ts.py  The demo use model from EMFORMER_RNNT_BASE_LIBRISPEECH quantization process success, but the inference fail Does anyone know if EMFORMER_RNNT not supported, or is there something missing during quantization process?  Generate model code  Quantization code  reference for inference code from :  https://github.com/pytorch/androiddemoapp/blob/master/StreamingASR/run_sasr.py this is the part that replaced;  during inference it produces error:   Versions Collecting environment information... PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0137genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 10.1.243 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce GTX 1080 Ti GPU 1: NVIDIA GeForce GTX 1080 Ti GPU 2: NVIDIA GeForce GTX 1080 Ti GPU 3: NVIDIA GeForce GTX 1080 Ti Nvidia driver version: 510.108.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   40 bits physical, 48 bits virtual CPU(s):                          8 Online CPU(s) list:       )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Inference error when trying Streaming ASR Mobile Dynamic Quantization ," ğŸ› Describe the bug trying to apply quantization to this pytorch model code: https://github.com/pytorch/androiddemoapp/blob/master/StreamingASR/generate_ts.py  The demo use model from EMFORMER_RNNT_BASE_LIBRISPEECH quantization process success, but the inference fail Does anyone know if EMFORMER_RNNT not supported, or is there something missing during quantization process?  Generate model code  Quantization code  reference for inference code from :  https://github.com/pytorch/androiddemoapp/blob/master/StreamingASR/run_sasr.py this is the part that replaced;  during inference it produces error:   Versions Collecting environment information... PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0137genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 10.1.243 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce GTX 1080 Ti GPU 1: NVIDIA GeForce GTX 1080 Ti GPU 2: NVIDIA GeForce GTX 1080 Ti GPU 3: NVIDIA GeForce GTX 1080 Ti Nvidia driver version: 510.108.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   40 bits physical, 48 bits virtual CPU(s):                          8 Online CPU(s) list:       ",2023-02-28T09:30:03Z,oncall: quantization,closed,0,1,https://github.com/pytorch/pytorch/issues/95694,"after further analyzing, the error is caused because the input tensor format is wrong. Since I was trying to replace pyaudio.PyAudio() streaming with audio file.  So the problem lies somewhere else."
1939,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Testing InvokeAI 2.3.1.post1, using mps, with PyTorch nightly dev20230226 yields RuntimeError cross-device copies are not allowed!))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Testing InvokeAI 2.3.1.post1 with PyTorch nightly dev20230226, using mps, yields `RuntimeError: Attempting to copy from device mps:0 to device meta, but crossdevice copies are not allowed!` Comment: Posted this as an issue in InvokeAI repo. Moderator deferred this to pytorch issue. To reproduce: 1. Install InvokeAI 2.3.1.post1 2. Install recommended models during configuration (option `r`.) 3. Run invoke.sh 4. Choose web interface (option 2: browserbased UI) 5. See console output. Traceback:   Versions Collecting environment information... PyTorch version: 2.0.0.dev20230226 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: Could not collect Libc version: N/A Python version: 3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS13.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] clipanytorch==2.5.0 [pip3] numpy==1.23.5 [pip3] pytorchlightning==1.7.7 [pip3] torch==2.0.0.dev20230226 [pip3] torchfidelity==0.3.0 [pip3] torchaudio==2.0.0.dev20230226 [pip3] torchdiffeq==0.2.3 [pip3] torchmetrics==0.11.1 [pip3] torchsde==0.2.5 [pip3] torchvision==0.15.0.dev20230226 [conda] No relevant packages )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Testing InvokeAI 2.3.1.post1, using mps, with PyTorch nightly dev20230226 yields RuntimeError cross-device copies are not allowed!)"," ğŸ› Describe the bug Testing InvokeAI 2.3.1.post1 with PyTorch nightly dev20230226, using mps, yields `RuntimeError: Attempting to copy from device mps:0 to device meta, but crossdevice copies are not allowed!` Comment: Posted this as an issue in InvokeAI repo. Moderator deferred this to pytorch issue. To reproduce: 1. Install InvokeAI 2.3.1.post1 2. Install recommended models during configuration (option `r`.) 3. Run invoke.sh 4. Choose web interface (option 2: browserbased UI) 5. See console output. Traceback:   Versions Collecting environment information... PyTorch version: 2.0.0.dev20230226 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: Could not collect Libc version: N/A Python version: 3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS13.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Apple M1 Pro Versions of relevant libraries: [pip3] clipanytorch==2.5.0 [pip3] numpy==1.23.5 [pip3] pytorchlightning==1.7.7 [pip3] torch==2.0.0.dev20230226 [pip3] torchfidelity==0.3.0 [pip3] torchaudio==2.0.0.dev20230226 [pip3] torchdiffeq==0.2.3 [pip3] torchmetrics==0.11.1 [pip3] torchsde==0.2.5 [pip3] torchvision==0.15.0.dev20230226 [conda] No relevant packages ",2023-02-27T17:00:48Z,triaged module: mps,open,0,5,https://github.com/pytorch/pytorch/issues/95622,I had this issue as well. The last working nightly was Feb 15., I just had a chance to check again with the current nightlies (torch 2.1.0.dev20230307 and torchvision 0.15.0.dev20230306) and it seems to be working again. Can you confirm?,"  I'm still seeing the error, though the latest nightly for me  includes `torchaudio2.0.0.dev20230307` and `torchvision0.15.0.dev20230307`. I'll try with `torchvision0.15.0.dev20230306`. UPDATE: Tried `torchvision0.15.0.dev20230306`. Same error. Could you run the `collect_env.py` script from the Bug Issue template here and post the results here, so we can compare our environments? Here's the section from that form if you'd rather just run it without starting another issue (and you trust me not to be faking it. :D ): `wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py` ` For security purposes, please check the contents of collect_env.py before running it.` `python collect_env.py` Cheers.","Hmm, interesting. I had rolled back to Invoke 2.2.5 due to the crashing issue at larger resolutions with 2.3.x and the newest torch nightlies are working fine there. But for some reason the collect_env.py script errored out, so I switched over to 2.3.1. The latest nightlies are still producing the device meta error with that version. Weird! Maybe it's some incompatibility with diffusers?","Correction: last I checked, the newest torch nightlies only work with Invoke 2.2.5 using two (or maybe three?) schedulers including DDIM (I don't remember the others). Most schedulers only produce noise with the newest nightlies. Also, Invoke 2.3.2 still generates the device meta error with the new nightlies."
773,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Reproducibility]replication_pad2d_backward_cuda does not have a deterministic implementation)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Hi, I am following the instruction from here to set torch.use_deterministic_algorithms(True) However, i encounter an error as shown in title.  Alternatives I have tried: torch.use_deterministic_algorithms(True, warn_only=True) and it works for me.  Additional context I not sure why its happening. But my implementation does involves padding. The main architecture contains convolution (Unetencoder), a transformer encoder, and a Unetdecoder with upsampling. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Reproducibility]replication_pad2d_backward_cuda does not have a deterministic implementation," ğŸš€ The feature, motivation and pitch Hi, I am following the instruction from here to set torch.use_deterministic_algorithms(True) However, i encounter an error as shown in title.  Alternatives I have tried: torch.use_deterministic_algorithms(True, warn_only=True) and it works for me.  Additional context I not sure why its happening. But my implementation does involves padding. The main architecture contains convolution (Unetencoder), a transformer encoder, and a Unetdecoder with upsampling. ",2023-02-26T20:35:36Z,triaged module: determinism module: padding oncall: pt2,closed,0,6,https://github.com/pytorch/pytorch/issues/95578,"Besides,  this one seems also lacking a deterministic implementaition. ""upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation,"" I'm using this version: os.environ[""CUBLAS_WORKSPACE_CONFIG""]="":16:8""","Unfortunately, the error message is pretty self explanatory. Maybe we can do a deterministic impl using pt2",We should apply the same playbook here as in https://github.com/pytorch/pytorch/pull/101115,I'll try to do this soon,"There doesn't seem to be a decomposition for any of the padding functions in `torch/_decomp/decompositions.py`, so it doesn't seem like we can do this yet","Have you solved the problem? And if set torch.use_deterministic_algorithms as False, could it be reproduced?"
488,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Fix type casting copy with storage offset)ï¼Œ å†…å®¹æ˜¯ (This PR handles the case where the `dst` tensor of type casting has a storage offset by creating a temporary buffer to store results and then copy them back to the dst with the offset added. Fixes CC(tensor __setitem__ incorrect on mps when dtypes mismatch))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MPS] Fix type casting copy with storage offset,This PR handles the case where the `dst` tensor of type casting has a storage offset by creating a temporary buffer to store results and then copy them back to the dst with the offset added. Fixes CC(tensor __setitem__ incorrect on mps when dtypes mismatch),2023-02-26T10:30:36Z,triaged open source Merged ciflow/trunk release notes: mps ciflow/mps,closed,0,13,https://github.com/pytorch/pytorch/issues/95573, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , merge, Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: windowsbinarylibtorchrelease / libtorchcpusharedwithdepsreleasetest Details for Dev Infra team Raised by workflow job ", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `fix_type_casting_offset_mm` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout fix_type_casting_offset_mm && git pull rebase`)"," merge f ""All tests are green."""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",this regression existed in 1.13.1 hence not qualified for 2.0.1 release," sorry if it's a dumb question since I don't really know how release cycles for pytorch works, but doesn't this fit under the category 2: 'Low risk critical fixes for silent correctness'?",If qualified I can send a PR against the release branch.
499,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Import parameters from jit)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Hi, it would be great to have the possibility to import parameters from a jit module to the same network implemented as Torch::Module. This will allow performing finetuning on the platform.  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,Import parameters from jit," ğŸš€ The feature, motivation and pitch Hi, it would be great to have the possibility to import parameters from a jit module to the same network implemented as Torch::Module. This will allow performing finetuning on the platform.  Alternatives _No response_  Additional context _No response_ ",2023-02-24T20:21:55Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/95497
461,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(hf_GPT2_large CPU inference shows random failure on CI)ï¼Œ å†…å®¹æ˜¯ (https://hud.pytorch.org/hud/pytorch/pytorch/master/1?per_page=50&name_filter=inductor_torchbench_cpu https://github.com/pytorch/pytorch/pull/95473 takes hf_GPT2_large off from the CI test, but it still needs investigation. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,hf_GPT2_large CPU inference shows random failure on CI,"https://hud.pytorch.org/hud/pytorch/pytorch/master/1?per_page=50&name_filter=inductor_torchbench_cpu https://github.com/pytorch/pytorch/pull/95473 takes hf_GPT2_large off from the CI test, but it still needs investigation. ",2023-02-24T15:22:51Z,triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/95474,I don't see hf_GPT2_large being skipped anywhere in the code except for accuracy checks since the model is too large. Closing as fixed.
608,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Inductor][CI] Remove hf_GPT2_large from CPU inference test)ï¼Œ å†…å®¹æ˜¯ (  CC([CI] Switch to pinned torchbench version)  CC([CI] Specify more torch.backends.cudnn options to reduce nondeterminism)  CC([Inductor][CI] Remove hf_GPT2_large from CPU inference test) Summary: hf_GPT2_large shows random failure on CI for the CPU inference. Created  CC(hf_GPT2_large CPU inference shows random failure on CI) for the Intel team to investigate. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[Inductor][CI] Remove hf_GPT2_large from CPU inference test,  CC([CI] Switch to pinned torchbench version)  CC([CI] Specify more torch.backends.cudnn options to reduce nondeterminism)  CC([Inductor][CI] Remove hf_GPT2_large from CPU inference test) Summary: hf_GPT2_large shows random failure on CI for the CPU inference. Created  CC(hf_GPT2_large CPU inference shows random failure on CI) for the Intel team to investigate. ,2023-02-24T15:20:24Z,Merged topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/95473," merge f ""inductor_torchbench_cpu_accuracy has already passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
413,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`add/add_` for CSC: errors when trying to access non-existent `crow_indices`.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug A simple repro:  All in all, add is broken in many ways because of missing checks/dispatch manipulations.  Versions Current master. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,`add/add_` for CSC: errors when trying to access non-existent `crow_indices`.," ğŸ› Describe the bug A simple repro:  All in all, add is broken in many ways because of missing checks/dispatch manipulations.  Versions Current master. ",2023-02-24T10:45:14Z,module: sparse triaged,open,0,1,https://github.com/pytorch/pytorch/issues/95463,  Thanks for discovering this. Please add it to the roadmap and work on a fix.
454,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Compile error in nanoGPT)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Use `pytorch==2.0.0dev20230213` or earlier, everything works fine. However after that (maybe due to the updated triton), the model in nanoGPT fails in the following code:   Error logs   Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Compile error in nanoGPT," ğŸ› Describe the bug Use `pytorch==2.0.0dev20230213` or earlier, everything works fine. However after that (maybe due to the updated triton), the model in nanoGPT fails in the following code:   Error logs   Minified repro _No response_  Versions  ",2023-02-24T03:39:35Z,oncall: pt2,closed,0,19,https://github.com/pytorch/pytorch/issues/95445,"Hi , thanks for filing these issue.  Could you please try the following version of torch? This will use a new version of triton. I have verified it works with your model.  pip install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/test/cu117  As of now, the above command will download 2.0.0rc2  Below is the output I got  ubuntu@:~$ python nanoGPT.py                         number of parameters: 123.69M                                                                 .../python3.10/sitepackages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplicati on available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.   warnings.warn( /lib/python3.10/sitepackages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a functi on that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137issu ecomment1211320670 for rationale.   warnings.warn( Hope this helps. ",Latest nightly also works:  pip install torch torchvision torchAudio indexurl https://download.pytorch.org/whl/nightly/cu117 you may need to manually install jinja2 (pip install jinja2). ,I also can't repro this with my most recent pytorch + torch audio version,"> Latest nightly also works: That's strange, because I already used the latest nightly build (please see `Versions of relevant libraries`). > you may need to manually install jinja2 (pip install jinja2). And the latest jinja2 has also been installed. Is it due to some dependencies, compile settings or environments?","> > Latest nightly also works: >  > That's strange, because I already used the latest nightly build (please see `Versions of relevant libraries`). >  > > you may need to manually install jinja2 (pip install jinja2). >  > And the latest jinja2 has also been installed. >  > Is it due to some dependencies, compile settings or environments? By ""the latest nightly"", we really mean ""the latest"", like today's version: torch2.0.0.dev20230226%2Bcu117cp310cp310linux_x86_64.whl if you uninstall your torch version and rerun this command:  pip install torch torchvision torchAudio indexurl https://download.pytorch.org/whl/nightly/cu117  It will install the latest nightly.  And after you do that, your model works without error messages.    Last week, we had resolved several issues related to triton. So 20230223 nightly must have been in broken status. ",I tried the latest nightly just now but it failed again :( The error is not changed. ,"Can you please create a new python environment and retry? Below is the output I got. You can see my environment is very clean.  (cleannightly20230226) weiwangmeta:~ $  python nanoGPT.py  number of parameters: 123.69M /home/weiwangmeta/.conda/envs/cleannightly20230226/lib/python3.9/sitepackages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.   warnings.warn( /home/weiwangmeta/.conda/envs/cleannightly20230226/lib/python3.9/sitepackages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137issuecomment1211320670 for rationale.   warnings.warn( (cleannightly20230226) weiwangmeta:~ $  pip list |grep torch pytorchtriton     2.0.0+b8b470bc59 torch              2.0.0.dev20230226+cu117 torchaudio         2.0.0.dev20230223+cu117 torchvision        0.15.0.dev20230226+cu117 One additional thing is: can you please try to clean the cache by ""rm /tmp/torchinductor_{username}"" ? ","Your suggestion is helpful! Use a more clean docker image instead of my company's custom image, everything works. Through step by step retries, I finally found that there was a `tensorflow==2.7.2` in the custom image and the issue would be solved if I upgraded it to `tensorflow==2.11.0`. Then I tried to install `tensorflow==2.7.2` back, found that the issue showed again. I have no idea why the old tensorflow mysteriously hurts the latest pytorch...","It runs now but is much slower than `pytorch==2.0.0dev20230213`, seems due to the flash attention (`torch.backends.cuda.enable_flash_sdp(False)` recovers the speed a lot).", you marked this issue as triaged but I don't see any other labels on it nor an assignment," Looks like we have two issues:  1) tensorflow and pytorch compatibility issues with tensorflow 2.7.2  2) performance regressions if not setting torch.backends.cuda.enable_flash_sdp(False).  I will take a look at No.1, can you please check if No.2 is expected?  ","For 1, most likely old tensorflow links against older llvm version and doesn't hide its symbols for 2,  ",There has only been two things effecting SDPA since 2/13: CC([SDPA] Fix bug in parsing scaled_dot_product_attention arguments ) CC([SDPA] Update dispatch logic to check for sm86 and head_size == 128 for flash attention )  I am not entirely sure how either of these would have effected your performance,"> Your suggestion is helpful! Use a more clean docker image instead of my company's custom image, everything works. Through step by step retries, I finally found that there was a `tensorflow==2.7.2` in the custom image and the issue would be solved if I upgraded it to `tensorflow==2.11.0`. Then I tried to install `tensorflow==2.7.2` back, found that the issue showed again. I have no idea why the old tensorflow mysteriously hurts the latest pytorch... Thanks   It seems the issue might be setup specific. I installed tensorflow==2.7.2 (and later tensorflowgpu==2.7.2), I could not reproduce this issue.  (cleannightly20230226) weiwangmeta:~ $ python nanoGPT.py   number of parameters: 123.69M /home/weiwangmeta/.conda/envs/cleannightly20230226/lib/python3.9/sitepackages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.   warnings.warn( (cleannightly20230226) weiwangmeta:~ $ pip list grep torch pytorchtriton               2.0.0+b8b470bc59 torch                        2.0.0.dev20230227+cu117 torchaudio                   2.0.0.dev20230223+cu117 torchvision                  0.15.0.dev20230227+cu117", can you reproduce perf difference with torch.backends.cuda.enable_flash_sdp(False)? Is enabling flash expected to regress perf?,>  can you reproduce perf difference with torch.backends.cuda.enable_flash_sdp(False)? Is enabling flash expected to regress perf? https://github.com/pytorch/pytorch/pull/92917issuecomment1411678461 This is the table of benchmarks and timings for nanogpt I gathered when enabling flash backward support. It boosted performance. ,Can you please check then why in the benchmark in this issue enabling it leads to perf degradation?,I just pulled nanogpt master. torch.compile + flashattention : 3858.03ms torch.compile + mem_efficient_attention: 4112.60ms,I'll close this issue because I cannot reproduce it on pytorch 2.0 stable version.
627,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(inductor: fix complier error when trying to vectorize logit_and and logit_or (#95361))ï¼Œ å†…å®¹æ˜¯ (Currently, `operator&& `  and `operator ` don't have vectorization implementation, disable them now for a quick fix for 2.0 release. Pull Request resolved: https://github.com/pytorch/pytorch/pull/95361 Approved by: https://github.com/ngimel, https://github.com/EikanWang Fixes  CC(Pytorch 2.0 [compile] backward call on logit causes graph compiler failure). )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,inductor: fix complier error when trying to vectorize logit_and and logit_or (#95361),"Currently, `operator&& `  and `operator ` don't have vectorization implementation, disable them now for a quick fix for 2.0 release. Pull Request resolved: https://github.com/pytorch/pytorch/pull/95361 Approved by: https://github.com/ngimel, https://github.com/EikanWang Fixes  CC(Pytorch 2.0 [compile] backward call on logit causes graph compiler failure). ",2023-02-24T02:56:12Z,open source module: inductor ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/95439
478,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Issues with torch.compile on A100 when finetuning DistilBERT)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug LLVM ERROR when using `torch.compile`, following up on a discussion with  (https://twitter.com/marksaroufim/status/1628833167481376768?s=20)  Attached the code files to reproduce this: pythonscripts.zip  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",finetuning,Issues with torch.compile on A100 when finetuning DistilBERT," ğŸ› Describe the bug LLVM ERROR when using `torch.compile`, following up on a discussion with  (https://twitter.com/marksaroufim/status/1628833167481376768?s=20)  Attached the code files to reproduce this: pythonscripts.zip  Versions  ",2023-02-23T20:13:12Z,oncall: releng triaged oncall: pt2,closed,0,5,https://github.com/pytorch/pytorch/issues/95407,"Does doing this solve things? `TRITON_PTXAS_PATH=/usr/bin/ptxas python your_script.py` ,    as FYI Seems like this is a known issue and is being tracked here https://github.com/pytorch/builder/issues/1318","This looks like the same issue as comments in  CC(Pytorch 2.0 [compile] Runtime error when compiling torchvision.resnet18) and https://github.com/pytorch/builder/issues/1318  In the meanwhile, we just fixed nightly branch push. Expecting new nightly binary and this issue would be fixed.  Alternatively, please try our 2.0.0RC2:  pip install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/test/cu117 ","In short, [pip3] pytorchtriton==2.0.0+c8bfe3f548 is broken.  Latest is pytorchtriton           2.0.0+d54c04abe2. You can get it through the above RC2 installation command. Or via the download links here:  https://download.pytorch.org/whl/test/pytorchtriton/index.html ","Thanks for the pointer, let me try this with the new nightly release then tomorrow and report back!",The new nightly fixed it! Thanks   !
308,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Failed to export CLIPProcessor.from_pretrained(""openai/clip-vit-large-patch14""))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug   Versions pytorch master)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Failed to export CLIPProcessor.from_pretrained(""openai/clip-vit-large-patch14"")", ğŸ› Describe the bug   Versions pytorch master,2023-02-23T18:57:59Z,module: onnx triaged onnx-triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/95393,"The model is exportable, but there is an issue on the `torch.onnx.export` call Below is the fix  The output is ",Closing as the model was exported and executed on ORT
1084,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.onnx.export raises `Constant folding in symbolic shape inference fails: Expected all tensors to be on the same device` although the model runs fine in pytorch eager)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, Related issues:  CC(Error in Aten/core when exporting to Yolov5 ONNX GPU-FP16 model?)  CC(export onnx error,Warning: Constant folding in symbolic shape inference fails: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select))   This code runs fine:  But this code:  raises  The issue exists both on `1.13.1` and `2.0.0.dev20230220+cu117`. This looks like a bug, but should I assume this won't be fixed in PyTorch? The suggestion of exporting on CPU is okay, but what if I want to export directly in half precision (instead of using later utilities that parse the ONNX)? Thank you!  Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.onnx.export raises `Constant folding in symbolic shape inference fails: Expected all tensors to be on the same device` although the model runs fine in pytorch eager," ğŸ› Describe the bug Hi, Related issues:  CC(Error in Aten/core when exporting to Yolov5 ONNX GPU-FP16 model?)  CC(export onnx error,Warning: Constant folding in symbolic shape inference fails: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select))   This code runs fine:  But this code:  raises  The issue exists both on `1.13.1` and `2.0.0.dev20230220+cu117`. This looks like a bug, but should I assume this won't be fixed in PyTorch? The suggestion of exporting on CPU is okay, but what if I want to export directly in half precision (instead of using later utilities that parse the ONNX)? Thank you!  Versions ",2023-02-23T15:03:02Z,module: onnx triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/95377,"If you turned off constant folding, does it work? Is there any error messages?","Logs with `do_constant_folding=True` (error is raised):  Log with `do_constant_folding=False` (error is not raised, only userwarning): ",I think thatâ€™s the best bet. Itâ€™s unlikely to be high priority but contributions are welcome ,"I see thanks. Feel free to close then, or to leave open as a known issue."
1979,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SWA AveragedModel saving issue: avg_fn is unpickable)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, I've encountered a  issue while using the , the error is raised at the call of  for model saving. here's a small notebook for reproducing the issue: https://colab.research.google.com/drive/1bs04ahdxVjoW2s3Kbmc5IA3S3i2UKSQD?usp=sharing PS: the same behavior was seen on a win7 PC with python 3.7.6 and torch 1.13 cpu versions.   Versions Collecting environment information... PyTorch version: 1.13.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.22.6 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.147+x86_64withglibc2.29 Is CUDA available: False CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          2 Online CPU(s) list:         )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,SWA AveragedModel saving issue: avg_fn is unpickable," ğŸ› Describe the bug Hi, I've encountered a  issue while using the , the error is raised at the call of  for model saving. here's a small notebook for reproducing the issue: https://colab.research.google.com/drive/1bs04ahdxVjoW2s3Kbmc5IA3S3i2UKSQD?usp=sharing PS: the same behavior was seen on a win7 PC with python 3.7.6 and torch 1.13 cpu versions.   Versions Collecting environment information... PyTorch version: 1.13.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.22.6 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.147+x86_64withglibc2.29 Is CUDA available: False CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          2 Online CPU(s) list:         ",2023-02-23T13:27:39Z,module: optimizer good first issue module: pickle triaged actionable,closed,0,5,https://github.com/pytorch/pytorch/issues/95376,"avg_fn is a function which is why it can't be pickled. Unfortunately, the API takes a callable, so in general we can't make this pickleable. But the default value can be made pickleable easily; don't store the callable, instead test if avg_fn is None and call the default impl if it is. I am happy to review.","Hey, I think the problem is not that you cannot pickle a function (which you should be able to in python 3.8.10) but the local definition of avg_fn inside the __init__ (https://github.com/pytorch/pytorch/blob/master/torch/optim/swa_utils.pyL110) This is also reflected in the error message: ""Can't pickle local object â€¦"" If so (and I may be mistaken), my proposed solution would be to define the default avg_fn outside the AveragedModel class (or somewhere else for that matter) and not locally.", happy to review a PR with your proposed solution (it looks to not at all be mutually exclusive from 's!) along with a test case showing that it would now be pickleable (similar to `test_cycle_lr_state_dict_picklable` in test_optim.py). ,"> Hey, I think the problem is not that you cannot pickle a function (which you should be able to in python 3.8.10) but the local definition of avg_fn inside the **init** (https://github.com/pytorch/pytorch/blob/master/torch/optim/swa_utils.pyL110) This is also reflected in the error message: ""Can't pickle local object â€¦"" >  > If so (and I may be mistaken), my proposed solution would be to define the default avg_fn outside the AveragedModel class (or somewhere else for that matter) and not locally. As a quick hack I'de taken the definition of the avg_fn outside the class. this fix I think breaks the API.",Putting the avg_fn function outside the class might fix the issue
2119,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(isGenericDict()INTERNAL ASSERT FAILED at \""C:\\\\w\\\\b\\\\windows\\\\pytorch\\\\aten\\\\src\\\\ATen/core/ivalue_inl.h\"":1392, please report a bug to PyTorch. Expected GenericDict but got None)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm trying to move a state_dict from python to c++ per the following code and pickle_load() fails with an error requesting that I report the bug. I've attached the zipped version of ScriptUntrained_LibTorchTest03Cmb.ptsd. Python ` torch.save(model.state_dict(), Folder + 'ScriptUntrained_LibTorchTest03Cmb.ptsd') ` C++  ScriptUntrained_LibTorchTest03Cmb.zip  Versions libtorchwinsharedwithdeps1.10.2+cu113 (base) F:\Dev\LibTorch\pytorch\torch\utils>python collect_env.py Collecting environment information... PyTorch version: 1.13.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Microsoft Windows Server 2019 Standard GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.17763SP0 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration: GPU 0: Tesla V100SPCIE32GB GPU 1: Tesla V100SPCIE32GB GPU 2: Tesla V100SPCIE32GB GPU 3: Tesla V100SPCIE32GB Nvidia driver version: 516.94 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin\cudnn_ops_train64_8.dll HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.1 [pip3] numpydoc==1.1.0 [pip3] torch==1.13.0 [pip3] torchaudio==0.13.0 [pip3] torchinfo==1.7.1 [pip3] torchvision==0.14.0 [conda] blas 1.0 mkl [conda] mkl 2021.2.0 haa95532_296 [conda] mklservice 2.3.0 py38h2bbff1b_1 [conda] mkl_fft 1.3.0 py38h277e83a_2 [conda] mkl_random 1.2.1 py38hf11a4ad_2 [conda] mypy_extensions 0.4.3 py38_0 [conda] numpy 1.20)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,"isGenericDict()INTERNAL ASSERT FAILED at \""C:\\\\w\\\\b\\\\windows\\\\pytorch\\\\aten\\\\src\\\\ATen/core/ivalue_inl.h\"":1392, please report a bug to PyTorch. Expected GenericDict but got None"," ğŸ› Describe the bug I'm trying to move a state_dict from python to c++ per the following code and pickle_load() fails with an error requesting that I report the bug. I've attached the zipped version of ScriptUntrained_LibTorchTest03Cmb.ptsd. Python ` torch.save(model.state_dict(), Folder + 'ScriptUntrained_LibTorchTest03Cmb.ptsd') ` C++  ScriptUntrained_LibTorchTest03Cmb.zip  Versions libtorchwinsharedwithdeps1.10.2+cu113 (base) F:\Dev\LibTorch\pytorch\torch\utils>python collect_env.py Collecting environment information... PyTorch version: 1.13.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Microsoft Windows Server 2019 Standard GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.17763SP0 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration: GPU 0: Tesla V100SPCIE32GB GPU 1: Tesla V100SPCIE32GB GPU 2: Tesla V100SPCIE32GB GPU 3: Tesla V100SPCIE32GB Nvidia driver version: 516.94 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin\cudnn_ops_train64_8.dll HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.1 [pip3] numpydoc==1.1.0 [pip3] torch==1.13.0 [pip3] torchaudio==0.13.0 [pip3] torchinfo==1.7.1 [pip3] torchvision==0.14.0 [conda] blas 1.0 mkl [conda] mkl 2021.2.0 haa95532_296 [conda] mklservice 2.3.0 py38h2bbff1b_1 [conda] mkl_fft 1.3.0 py38h277e83a_2 [conda] mkl_random 1.2.1 py38hf11a4ad_2 [conda] mypy_extensions 0.4.3 py38_0 [conda] numpy 1.20",2023-02-23T05:07:34Z,module: windows module: cpp module: serialization triaged module: assert failure,closed,0,5,https://github.com/pytorch/pytorch/issues/95362,"May be related to this bug:  CC(isTuple()INTERNAL ASSERT FAILED at \""C:\\\\w\\\\b\\\\windows\\\\pytorch\\\\aten\\\\src\\\\ATen/core/ivalue_inl.h\"":1400, please report a bug to PyTorch. Expected Tuple but got String)issue1538443073",Do you know if this happens on Linux?,"Sorry, I don't know. I don't have a linux installation for testing.",I just saw this and will try it.  CC(Add load_state_dict and state_dict() in C++)issuecomment1450163197,"This was indeed fixed by saving as a python dict, rather than a statedict: "
476,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(inductor: fix complier error when trying to vectorize logit_and and logit_or)ï¼Œ å†…å®¹æ˜¯ (  CC(inductor: fix complier error when trying to vectorize logit_and and logit_or) Currently, `operator&& `  and `operator ` don't have vectorization implementation, disable them now for a quick fix for 2.0 release. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,inductor: fix complier error when trying to vectorize logit_and and logit_or,"  CC(inductor: fix complier error when trying to vectorize logit_and and logit_or) Currently, `operator&& `  and `operator ` don't have vectorization implementation, disable them now for a quick fix for 2.0 release. ",2023-02-23T04:30:57Z,open source Merged ciflow/trunk module: inductor ciflow/inductor release notes: inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/95361, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8gcc7 / test (default, 1, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1018,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter)ï¼Œ å†…å®¹æ˜¯ (Changes:  CC([1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code) 1. Recognize `.py.in` and `.pyi.in` files as Python in VS Code for a better development experience. 2. Fix deep setting merge in `tools/vscode_settings.py`.  CC([2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`) 3. Use `Namedtuple` rather than `namedtuple + __annotations__` for `torch.nn.utils.rnn.PackedSequence_`:     `namedtuple + __annotations__`:          `Namedtuple`: Python 3.6+       => this PR: CC([3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter) 4. Sort import statements and remove unnecessary imports in `.pyi`, `.pyi.in` files. 5. Format `.pyi`, `.pyi.in` files and remove unnecessary ellipsis `...` in type stubs.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter,"Changes:  CC([1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code) 1. Recognize `.py.in` and `.pyi.in` files as Python in VS Code for a better development experience. 2. Fix deep setting merge in `tools/vscode_settings.py`.  CC([2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`) 3. Use `Namedtuple` rather than `namedtuple + __annotations__` for `torch.nn.utils.rnn.PackedSequence_`:     `namedtuple + __annotations__`:          `Namedtuple`: Python 3.6+       => this PR: CC([3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter) 4. Sort import statements and remove unnecessary imports in `.pyi`, `.pyi.in` files. 5. Format `.pyi`, `.pyi.in` files and remove unnecessary ellipsis `...` in type stubs.",2023-02-22T04:34:24Z,triaged open source Merged ciflow/trunk topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/95268,> Were these changes generated by a linter?  Yes. I manually removed some unnecessary ellipsis `...` and added/removed some commas. Then run `black` to format the stub files. All these modifications do not change the semantic.,/pytorchdevinfra and  to review regarding this specific lint change,No CI linters are enabled for `.pyi` files currently. But we can enable the lintrunner to run `'UFMT'` for `.pyi` files.,> No CI linters are enabled for `.pyi` files currently. But we enable the lintrunner to run `'UFMT'` for `.pyi` files. Should this change to cover `*.pyi` file be added into https://github.com/pytorch/pytorch/blob/master/.lintrunner.tomlL825 as part of this PR?,> Should this change to cover `*.pyi` file be added into https://github.com/pytorch/pytorch/blob/master/.lintrunner.tomlL825 as part of this PR? Enabled., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1047,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`)ï¼Œ å†…å®¹æ˜¯ (Changes:  CC([1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code) 1. Recognize `.py.in` and `.pyi.in` files as Python in VS Code for a better development experience. 2. Fix deep setting merge in `tools/vscode_settings.py`.  => this PR: CC([2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`) 3. Use `Namedtuple` rather than `namedtuple + __annotations__` for `torch.nn.utils.rnn.PackedSequence_`:     `namedtuple + __annotations__`:          `Namedtuple`: Python 3.6+       CC([3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter) 4. Sort import statements and remove unnecessary imports in `.pyi`, `.pyi.in` files. 5. Format `.pyi`, `.pyi.in` files and remove unnecessary ellipsis `...` in type stubs.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`,"Changes:  CC([1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code) 1. Recognize `.py.in` and `.pyi.in` files as Python in VS Code for a better development experience. 2. Fix deep setting merge in `tools/vscode_settings.py`.  => this PR: CC([2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`) 3. Use `Namedtuple` rather than `namedtuple + __annotations__` for `torch.nn.utils.rnn.PackedSequence_`:     `namedtuple + __annotations__`:          `Namedtuple`: Python 3.6+       CC([3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter) 4. Sort import statements and remove unnecessary imports in `.pyi`, `.pyi.in` files. 5. Format `.pyi`, `.pyi.in` files and remove unnecessary ellipsis `...` in type stubs.",2023-02-22T04:33:40Z,triaged open source Merged ciflow/trunk topic: not user facing release notes: AO frontend,closed,0,3,https://github.com/pytorch/pytorch/issues/95267,Feel free to land this PR whenever you are ready, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
855,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([vulkan] Pad channels when using texture storage instead of ""tight packing"")ï¼Œ å†…å®¹æ˜¯ (  CC([vulkan] Pad channels when using texture storage instead of ""tight packing"") Currently, in Vulkan 4D tensors are represented in GPU textures by simply combining the batch and channel dimensions into the depth axis. However, if the number of channels is not a multiple of 4, then data belonging to the same batch can cross texel boundaries. For instance, consider a tensor with `N=2`, `C=3`. The depth axis of the texture would contain the data  Differential Revision: D43068669 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[vulkan] Pad channels when using texture storage instead of ""tight packing""","  CC([vulkan] Pad channels when using texture storage instead of ""tight packing"") Currently, in Vulkan 4D tensors are represented in GPU textures by simply combining the batch and channel dimensions into the depth axis. However, if the number of channels is not a multiple of 4, then data belonging to the same batch can cross texel boundaries. For instance, consider a tensor with `N=2`, `C=3`. The depth axis of the texture would contain the data  Differential Revision: D43068669 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!",2023-02-22T00:01:56Z,Merged release notes: vulkan,closed,0,2,https://github.com/pytorch/pytorch/issues/95251," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
855,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([vulkan] Pad channels when using texture storage instead of ""tight packing"")ï¼Œ å†…å®¹æ˜¯ (  CC([vulkan] Pad channels when using texture storage instead of ""tight packing"") Currently, in Vulkan 4D tensors are represented in GPU textures by simply combining the batch and channel dimensions into the depth axis. However, if the number of channels is not a multiple of 4, then data belonging to the same batch can cross texel boundaries. For instance, consider a tensor with `N=2`, `C=3`. The depth axis of the texture would contain the data  Differential Revision: D43068669 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"[vulkan] Pad channels when using texture storage instead of ""tight packing""","  CC([vulkan] Pad channels when using texture storage instead of ""tight packing"") Currently, in Vulkan 4D tensors are represented in GPU textures by simply combining the batch and channel dimensions into the depth axis. However, if the number of channels is not a multiple of 4, then data belonging to the same batch can cross texel boundaries. For instance, consider a tensor with `N=2`, `C=3`. The depth axis of the texture would contain the data  Differential Revision: D43068669 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!",2023-02-22T00:01:56Z,Merged release notes: vulkan,closed,0,2,https://github.com/pytorch/pytorch/issues/95251," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
476,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Make fx.Transformer.get_attr call tracer to preserve node.meta)ï¼Œ å†…å®¹æ˜¯ (Currently, transformer creates proxy objects directly for get_attr method. node.meta is lost in this step. In order to keep it, we invoke tracer.create_proxy. Meta data is copied over in tracer.create_proxy and tracer.create_node.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Make fx.Transformer.get_attr call tracer to preserve node.meta,"Currently, transformer creates proxy objects directly for get_attr method. node.meta is lost in this step. In order to keep it, we invoke tracer.create_proxy. Meta data is copied over in tracer.create_proxy and tracer.create_node.",2023-02-21T22:14:19Z,Merged ciflow/trunk release notes: fx,closed,0,2,https://github.com/pytorch/pytorch/issues/95245, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
834,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([pt] move csrc shm logic to aten storage utils)ï¼Œ å†…å®¹æ˜¯ (Summary: This is part 1 of the effort to support `share_memory_()` in C++ aten library. This allows C++ code to in place replace the tensor storage to shm based. For now fd based shm is the only implementation supported to simplify memory management in general. This first part intentionally avoids public api changes (to `TensorBase`, see comments in `StorageUtil.h`) such that we can get the core features usable outside pt/csrc first. The API addition to `Tensor` or `TensorBase` would involve more distracting changes and make the change harder to review. Test Plan:  Differential Revision: D43467616)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[pt] move csrc shm logic to aten storage utils,"Summary: This is part 1 of the effort to support `share_memory_()` in C++ aten library. This allows C++ code to in place replace the tensor storage to shm based. For now fd based shm is the only implementation supported to simplify memory management in general. This first part intentionally avoids public api changes (to `TensorBase`, see comments in `StorageUtil.h`) such that we can get the core features usable outside pt/csrc first. The API addition to `Tensor` or `TensorBase` would involve more distracting changes and make the change harder to review. Test Plan:  Differential Revision: D43467616",2023-02-21T18:02:46Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/95228,The committers listed above are authorized under a signed CLA.:white_check_mark: login: xunnanxu / name: Shawn Xu  (72cc3b43fa411c5f174b4f7e585071fc67a09ff2),This pull request was **exported** from Phabricator. Differential Revision: D43467616,This pull request was **exported** from Phabricator. Differential Revision: D43467616,/easycla,/easycla,This pull request was **exported** from Phabricator. Differential Revision: D43467616,This pull request was **exported** from Phabricator. Differential Revision: D43467616," thanks for reviewing. Yes this PR is only focusing on moving the code. As mentioned in the summary, actually exposing `share_memory_` to `TensorBase` would probably involve more discussion around the proper API semantics around device handling etc. to mirror THP behavior + a few other logistics involved. Thus I intentionally separated them to make things easier for both ends.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
2032,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add parallel attention layers and Multi-Query Attention (MQA) from PaLM to the fast path for transformers)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Parallel attention layers and MQA are introduced in PaLM [1]. The â€œstandardâ€ encoder layer, as currently implemented in PyTorch, follows the following structure: Serial attention (with default `norm_first=False`): `z = LN(x + Attention(x)); y = LN(z + MLP(z))` A parallel attention layer from PaLM, on the other hand, is implemented as follows: Parallel attention (with default `norm_first=False`): `y = LN(x + Attention(x) + MLP(x))` Parallel attention (with `norm_first=True`): `y = x + Attention(LN(x)) + MLP(LN(x))` As for MQA, the description from [1] is pretty concise and explains the advantage as well: â€œThe standard Transformer formulation uses k attention heads, where the input vector for each timestep is linearly projected into â€œqueryâ€, â€œkeyâ€, and â€œvalueâ€ tensors of shape [k, h], where h is the attention head size. Here, the key/value projections are shared for each head, i.e. â€œkeyâ€ and â€œvalueâ€ are projected to [1, h], but â€œqueryâ€ is still projected to shape [k, h]. We have found that this has a neutral effect on model quality and training speed (Shazeer, 2019), but results in a significant cost savings at autoregressive decoding time. This is because standard multiheaded attention has low efficiency on accelerator hardware during autoregressive decoding, because the key/value tensors are not shared between examples, and only a single token is decoded at a time.â€ In our own experiments, we have seen a 23% speedup in training for T5large when using parallel attention, with the same convergence, due to the ability to fuse the attention and feedforward projection layers into a single matmul operation. Furthermore, adding MQA on top shaves off an extra 10% training time. See the)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add parallel attention layers and Multi-Query Attention (MQA) from PaLM to the fast path for transformers," ğŸš€ The feature, motivation and pitch Parallel attention layers and MQA are introduced in PaLM [1]. The â€œstandardâ€ encoder layer, as currently implemented in PyTorch, follows the following structure: Serial attention (with default `norm_first=False`): `z = LN(x + Attention(x)); y = LN(z + MLP(z))` A parallel attention layer from PaLM, on the other hand, is implemented as follows: Parallel attention (with default `norm_first=False`): `y = LN(x + Attention(x) + MLP(x))` Parallel attention (with `norm_first=True`): `y = x + Attention(LN(x)) + MLP(LN(x))` As for MQA, the description from [1] is pretty concise and explains the advantage as well: â€œThe standard Transformer formulation uses k attention heads, where the input vector for each timestep is linearly projected into â€œqueryâ€, â€œkeyâ€, and â€œvalueâ€ tensors of shape [k, h], where h is the attention head size. Here, the key/value projections are shared for each head, i.e. â€œkeyâ€ and â€œvalueâ€ are projected to [1, h], but â€œqueryâ€ is still projected to shape [k, h]. We have found that this has a neutral effect on model quality and training speed (Shazeer, 2019), but results in a significant cost savings at autoregressive decoding time. This is because standard multiheaded attention has low efficiency on accelerator hardware during autoregressive decoding, because the key/value tensors are not shared between examples, and only a single token is decoded at a time.â€ In our own experiments, we have seen a 23% speedup in training for T5large when using parallel attention, with the same convergence, due to the ability to fuse the attention and feedforward projection layers into a single matmul operation. Furthermore, adding MQA on top shaves off an extra 10% training time. See the",2023-02-21T14:21:28Z,module: nn triaged,open,6,16,https://github.com/pytorch/pytorch/issues/95210,Thank you for posting this ! We'll discuss it internally. It seems like a good application of scaled_dot_product_attention and torch.compile.  ,In particular for `y = x + Attention(LN(x)) + MLP(LN(x))` you should be able to already do quite a bit of horizontal fusion manually by using `chunk`. For example see how we pack three separate matmuls into one within sdp.py.,"SantaCoder was another model leveraging MultiQuery Attention: https://huggingface.co/bigcode/santacoder We have a pure PyTorch implementation, but would love an optimized version. We already use F.scaled_dot_product_attention() for training."," thanks for sharing, wonder if you have observed any speed up in your training using `F.scaled_dot_product_attention() `","Hello   we're seeing a ~5% increase in training speed when using F.scaled_dot_product_attention(), where both versions are using PT2 and we are compiling the models. It also use 14% less ram, however, it crashes when we increase the batchsize to take advantage of the additional ram :) not sure yet what to make of that. ",  Can you share more details on this crash? Is this on a most recent nightly?  ,"Hi   I just updated to the latest nightly (2.0.0.dev20230223+cu118).  There are no more crashes. Here are the stats (gradaccumulation disabled for all cases); 1. pytorch compiled + no flash + batch_size=32 2.18 it/s GPU RAM: 44646MiB / 49140MiB 2. pytorch compiled + flash enabled + batch_size=32 2.29 it/s GPU RAM: 38496MiB / 49140MiB 3, pytorch compiled + flash enabled + batch_size=40 1.85 it/s GPU RAM: 47656MiB / 49140MiB Both case 2 & 3 are processing the same amount of data (i.e. 2.29 * 32 ~ 1.85 * 40), so no crashes, but no real gain either. I am running this on an NVIDIA A6000 card (Ampere/48GB of GPU RAM)"," Depending on the exact details of your model architecture (for example it might be dominated by the feedforward network and not the attention calculation) and how much you're using existing, already optimized modules (like nn.MHA) the gains might not be significant. But I'm happy to see that you're getting some gains at least, including much decreased memory footprint and more it/s.  The slowdown you're seeing for batch_size=40 might be explained by the GPU memory allocator. To really dig into the last mile optimizations here we'll need to look into a GPU profile of the workload.","thanks for the response   What do you mean exactly by "" a GPU profile of the workload""? Is this something I could see specifically? I have been thinking about profiling the code, but I'm unsure where/how to start. Is there a good reference for profiling pytorch code? thanks!",  I recommend using https://developer.nvidia.com/nsightsystems and its `nsys` command line tool. This can capture GPU traces for any arbitrary CUDA workload and you can use the desktop application to visualize them. Otherwise a more accessible approach might to use the PyTorch profiler.," thanks a lot for sharing the details on the perf gains and the issue, lets open another ticket to follow up on that issue and keep these two issues separate.",How did you manage to get scaled_dot_product_attention to work with MQA? I get shape mismatch errors,"Hi, it appears PyTorch SDPA can not dispatch to memefficient attention or flashattention for the shapes: ","> Hi, it appears PyTorch SDPA can not dispatch to memefficient attention or flashattention for the shapes: >  >  >  >  I think you need to add an expand as to key and value to make the num_heads match",Yes that's what I did!,related  CC([Breaking change 2.1] Passing non-contiguous inputs to SDPA on CUDA device with the mem-efficient attention backend returns garbage)
1019,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code)ï¼Œ å†…å®¹æ˜¯ (Changes:  => this PR: CC([1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code) 1. Recognize `.py.in` and `.pyi.in` files as Python in VS Code for a better development experience. 2. Fix deep setting merge in `tools/vscode_settings.py`.  CC([2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`) 3. Use `Namedtuple` rather than `namedtuple + __annotations__` for `torch.nn.utils.rnn.PackedSequence_`:     `namedtuple + __annotations__`:          `Namedtuple`: Python 3.6+       CC([3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter) 4. Sort import statements and remove unnecessary imports in `.pyi`, `.pyi.in` files. 5. Format `.pyi`, `.pyi.in` files and remove unnecessary ellipsis `...` in type stubs.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code,"Changes:  => this PR: CC([1/3] Recognize `.py.in` and `.pyi.in` files as Python in VS Code) 1. Recognize `.py.in` and `.pyi.in` files as Python in VS Code for a better development experience. 2. Fix deep setting merge in `tools/vscode_settings.py`.  CC([2/3] Update `.pyi` Python stub files: Prettify `rnn.py` by using type annotated `NamedTuple`) 3. Use `Namedtuple` rather than `namedtuple + __annotations__` for `torch.nn.utils.rnn.PackedSequence_`:     `namedtuple + __annotations__`:          `Namedtuple`: Python 3.6+       CC([3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter) 4. Sort import statements and remove unnecessary imports in `.pyi`, `.pyi.in` files. 5. Format `.pyi`, `.pyi.in` files and remove unnecessary ellipsis `...` in type stubs.",2023-02-21T09:51:47Z,triaged open source Merged ciflow/trunk release notes: distributed (c10d) topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/95200,"Thanks for the PR! Could you split this up into formattingonly PRs and the rest? In particular, the PackedSequence_ change would need to be looked at carefully. Also I'm not sure what the vscode_settings.py file does, how to test these changes?","> Could you split this up into formattingonly PRs and the rest? > In particular, the PackedSequence_ change would need to be looked at carefully. I split this into 3 PRs. The `PackedSequence_` change is moved to a standalone PR. > Also I'm not sure what the vscode_settings.py file does, how to test these changes? You could run:  or  It will update the workspace VS Code settings `.vscode/settings.json` from `.vscode/settings_recommended.json`. The script will create a setting file if not exist. For example: current file (.vscode/settings.json):  after running `./tools/vscode_settings.py`: ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
590,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Pytorch 2.0 [compile] as_strided inplace causes out of bounds for storage )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Please use below code to reproduce error:  The same code works fine in eager mode. This issue only seen in compile mode. Error:   Versions Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] torch==2.0.0.dev20230209+cpu [pip3] torchaudio==2.0.0.dev20230209+cpu [pip3] torchvision==0.15.0.dev20230209+cpu )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Pytorch 2.0 [compile] as_strided inplace causes out of bounds for storage , ğŸ› Describe the bug Please use below code to reproduce error:  The same code works fine in eager mode. This issue only seen in compile mode. Error:   Versions Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] torch==2.0.0.dev20230209+cpu [pip3] torchaudio==2.0.0.dev20230209+cpu [pip3] torchvision==0.15.0.dev20230209+cpu ,2023-02-21T06:56:13Z,triaged oncall: pt2 module: dynamo,closed,0,2,https://github.com/pytorch/pytorch/issues/95186,"I think this falls under inplace metadata mutations, which is being tracked as part of  CC([Pytorch 2.0][compile] Squeeze_ torch._dynamo.exc.BackendCompilerFailed Dimension out of range)issuecomment1441944039",I see the same failure in eager mode too:  Closing issue for now.
884,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo] import torch._dynamo error ""module 'transformers' has no attribute 'configuration_utils'"")ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi, Here is a import error. I install the following env, while i got a error when importing torch._dynamo torch                  2.0.0.dev20230218+cpu torchaudio         2.0.0.dev20230218+cpu torchvision         0.15.0.dev20230218+cpu  It looks like the package name has been changed ? Any comments ? Thank you :)  Versions torch                  2.0.0.dev20230218+cpu torchaudio         2.0.0.dev20230218+cpu torchvision         0.15.0.dev20230218+cpu Ubuntu 20.04 install command: pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[dynamo] import torch._dynamo error ""module 'transformers' has no attribute 'configuration_utils'"""," ğŸ› Describe the bug Hi, Here is a import error. I install the following env, while i got a error when importing torch._dynamo torch                  2.0.0.dev20230218+cpu torchaudio         2.0.0.dev20230218+cpu torchvision         0.15.0.dev20230218+cpu  It looks like the package name has been changed ? Any comments ? Thank you :)  Versions torch                  2.0.0.dev20230218+cpu torchaudio         2.0.0.dev20230218+cpu torchvision         0.15.0.dev20230218+cpu Ubuntu 20.04 install command: pip3 install pre torch torchvision torchaudio indexurl https://download.pytorch.org/whl/nightly/cpu",2023-02-19T07:30:27Z,,closed,1,3,https://github.com/pytorch/pytorch/issues/95131,`pip install transformers==4.20.1`,"Hi chen, is this the appropriate fix for this issue? I don't see why installing `transformers` should be required in order to use `torch`, i.e. this seems like a bug. Unless my understanding of the `torch._dynamo` package is wrong and it for some reason relies on `transformers`.","> Hi chen, is this the appropriate fix for this issue? >  > I don't see why installing `transformers` should be required in order to use `torch`, i.e. this seems like a bug. >  > Unless my understanding of the `torch._dynamo` package is wrong and it for some reason relies on `transformers`. Maybe it's required just for `_dynamo`.  There is a comment in the code about it.  It's unfortunate, because `transformers` requires `torch`, making this a circular dependency. chen 's suggestion worked for me."
725,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Fix fill_ where input tensor has a storage offset)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(RuntimeError: length >= offset INTERNAL ASSERT FAILED at ""/Users/runner/work/_temp/anaconda/condabld/pytorch_1675757334591/work/aten/src/ATen/mps/MPSStream.mm"":122, please report a bug to PyTorch. ) Apart from fixing the issue above, this PR also fixes a bug that when an input tensor can be sliced, a sliced array view is created. This array view seems to be not writable or have a different storage from the original tensor, causing incorrect results with the inplace `fill`.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[MPS] Fix fill_ where input tensor has a storage offset,"Fixes CC(RuntimeError: length >= offset INTERNAL ASSERT FAILED at ""/Users/runner/work/_temp/anaconda/condabld/pytorch_1675757334591/work/aten/src/ATen/mps/MPSStream.mm"":122, please report a bug to PyTorch. ) Apart from fixing the issue above, this PR also fixes a bug that when an input tensor can be sliced, a sliced array view is created. This array view seems to be not writable or have a different storage from the original tensor, causing incorrect results with the inplace `fill`.",2023-02-18T11:07:16Z,open source Merged release notes: mps ciflow/mps,closed,0,6,https://github.com/pytorch/pytorch/issues/95113," label ""accept2run""",Looks good.," merge f ""mps tests are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: HTTP Error 500: Internal Server Error Details for Dev Infra team Raised by workflow job ,This pull request has been merged in pytorch/pytorch.
449,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Fix copy_cast_mps() on tensors with storage offset)ï¼Œ å†…å®¹æ˜¯ ( The copy_cast path requires storage_offset to be applied before casting  This should fix some correctness issues in transformer models Fixes CC(nonzero() output results in incorrect type conversion to float) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[MPS] Fix copy_cast_mps() on tensors with storage offset, The copy_cast path requires storage_offset to be applied before casting  This should fix some correctness issues in transformer models Fixes CC(nonzero() output results in incorrect type conversion to float) ,2023-02-17T21:16:05Z,open source Merged release notes: mps ciflow/mps,closed,0,2,https://github.com/pytorch/pytorch/issues/95093," merge f ""MPS tests are green."""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
980,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add torch.utils._sympy.interp)ï¼Œ å†…å®¹æ˜¯ (  CC(Introduce constrain_range; remove old expr_subs)  CC(Add torch.utils._sympy.interp)  CC(Add boolean/comparison operator support to ValueRanges)  CC(Add exhaustive testing to ValueRanges, fix bugs)  CC(Assert more invariants on ValueRanges) This utility allows us to conveniently abstract interpret Sympy expressions with respect to some alternative domain. I am particularly interested in using ValueRanges to do range analysis on expressions (not this PR). Some minor housekeeping: * ReferenceAnalysis got moved to its own file, sprouted a constant() implementation, and some uses of math.* got converted to sympy.* * ValueRangeAnalysis now understands mod * Test file gets moved from `test_value_ranges.py` to `test_sympy_utils.py` Signedoffby: Edward Z. Yang )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Add torch.utils._sympy.interp,"  CC(Introduce constrain_range; remove old expr_subs)  CC(Add torch.utils._sympy.interp)  CC(Add boolean/comparison operator support to ValueRanges)  CC(Add exhaustive testing to ValueRanges, fix bugs)  CC(Assert more invariants on ValueRanges) This utility allows us to conveniently abstract interpret Sympy expressions with respect to some alternative domain. I am particularly interested in using ValueRanges to do range analysis on expressions (not this PR). Some minor housekeeping: * ReferenceAnalysis got moved to its own file, sprouted a constant() implementation, and some uses of math.* got converted to sympy.* * ValueRangeAnalysis now understands mod * Test file gets moved from `test_value_ranges.py` to `test_sympy_utils.py` Signedoffby: Edward Z. Yang ",2023-02-16T15:48:46Z,Merged ciflow/trunk release notes: composability,closed,0,8,https://github.com/pytorch/pytorch/issues/94985, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8gcc7 / test (distributed, 2, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed.  The first few are:  pull / linuxfocalpy3.8gcc7 / test (distributed, 2, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job Failing merge rule: Core Maintainers "," merge f ""known flakiness"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
516,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Inductor cpp wrapper: support ConvBinaryInplace)ï¼Œ å†…å®¹æ˜¯ (  CC(Inductor cpp wrapper: support ConvBinaryInplace)  CC(Inductor cpp wrapper: support ConvUnary and ConvBinary)  CC(Inductor cpp wrapper: cache the wrapper) Due to constrain of `op.call`, `other (Tensor&)` should be at `input[0]`. TODO: Fix optional value based on previous ghstack. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Inductor cpp wrapper: support ConvBinaryInplace,"  CC(Inductor cpp wrapper: support ConvBinaryInplace)  CC(Inductor cpp wrapper: support ConvUnary and ConvBinary)  CC(Inductor cpp wrapper: cache the wrapper) Due to constrain of `op.call`, `other (Tensor&)` should be at `input[0]`. TODO: Fix optional value based on previous ghstack. ",2023-02-16T10:24:38Z,module: cpu open source topic: not user facing module: inductor ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/94977,Reopen in https://github.com/pytorch/pytorch/pull/101394
301,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([ONNX] Enable skipped gpt2 test)ï¼Œ å†…å®¹æ˜¯ (  CC([ONNX] Enable skipped gpt2 test) I think the skip is outdated. Test passed in CI.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,[ONNX] Enable skipped gpt2 test,  CC([ONNX] Enable skipped gpt2 test) I think the skip is outdated. Test passed in CI.,2023-02-15T20:38:16Z,module: onnx open source Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/94930,  merge g, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.sum does not return the sum on ROCm)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `torch.sum` always return 0 or first tensor element when running. This causes nlp sampling to be impossible. End to end example  Causes error  This is because of `torch.sum` working incorrectly. Demonstration:  Output:  This is simply the first item in the tensor. `torch.sum` will sometimes return number such as `4.203895392981743e45`.  Versions Collecting environment information... PyTorch version: 1.13.0a0+git941769a Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22801aaa1e3d8 OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 15.0.0 (https://github.com/RadeonOpenCompute/llvmproject roc5.4.0 22465 d6f0fe8b22e3d8ce0f2cbd657ea14b16043018a5) CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration: AMD Radeon PRO V620 Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22801 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          32 Online CPU(s) list:             031 Thread(s) per core:              1 Core(s) per socket:              32 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       AuthenticAMD CPU family:             )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,torch.sum does not return the sum on ROCm," ğŸ› Describe the bug `torch.sum` always return 0 or first tensor element when running. This causes nlp sampling to be impossible. End to end example  Causes error  This is because of `torch.sum` working incorrectly. Demonstration:  Output:  This is simply the first item in the tensor. `torch.sum` will sometimes return number such as `4.203895392981743e45`.  Versions Collecting environment information... PyTorch version: 1.13.0a0+git941769a Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22801aaa1e3d8 OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 15.0.0 (https://github.com/RadeonOpenCompute/llvmproject roc5.4.0 22465 d6f0fe8b22e3d8ce0f2cbd657ea14b16043018a5) CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.060genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration: AMD Radeon PRO V620 Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22801 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          32 Online CPU(s) list:             031 Thread(s) per core:              1 Core(s) per socket:              32 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       AuthenticAMD CPU family:             ",2023-02-15T06:53:11Z,module: rocm triaged module: random,closed,0,33,https://github.com/pytorch/pytorch/issues/94891,"Would you please share how you installed pytorch?  I see   We didn't push out pytorch 1.13 docker images to rocm/pytorch, and the stable 1.13 wheels on pytorch.org only support up to ROCm 5.3. I tried the following using an ubuntu 20.04 docker image on a AMD Radeon PRO V620 and your repro was passing for me. ","I have used local build of ROCmdocker image. I have posted below version report using different vm and your wheel.    new environment Collecting environment information... PyTorch version: 2.0.0.dev20230215+rocm5.4.2 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22803474e8620 OS: Fedora Linux 36 (Server Edition) (x86_64) GCC version: (GCC) 12.2.1 20221121 (Red Hat 12.2.14) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.10.9 (main, Dec  7 2022, 00:00:00) [GCC 12.2.1 20221121 (Red Hat 12.2.14)] (64bit runtime) Python platform: Linux6.1.11100.fc36.x86_64x86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Radeon Graphics Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22803 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Address sizes:                   40 bits physical, 48 bits virtual Byte Order:                      Little Endian CPU(s):                          16 Online CPU(s) list:             015 Vendor ID:                       AuthenticAMD Model name:                      AMD EPYC Processor CPU family:                      23 Model:                           1 Thread(s) per core:              1 Core(s) per socket:              16 Socket(s):                       1 Stepping:                        2 BogoMIPS:                        3999.99 Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 arat Hypervisor vendor:               KVM Virtualization type:             full L1d cache:                       512 KiB (16 instances) L1i cache:                       1 MiB (16 instances) L2 cache:                        8 MiB (16 instances) L3 cache:                        8 MiB (1 instance) NUMA node(s):                    1 NUMA node0 CPU(s):               015 Vulnerability Itlb multihit:     Not affected Vulnerability L1tf:              Not affected Vulnerability Mds:               Not affected Vulnerability Meltdown:          Not affected Vulnerability Mmio stale data:   Not affected Vulnerability Retbleed:          Mitigation; untrained return thunk; SMT disabled Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSBeIBRS Not affected Vulnerability Srbds:             Not affected Vulnerability Tsx async abort:   Not affected Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] torch==2.0.0.dev20230215+rocm5.4.2 [pip3] torchaudio==2.0.0.dev20230215+rocm5.4.2 [pip3] torchvision==0.15.0.dev20230215+rocm5.4.2 [conda] Could not collect  result  Is this amd driver problem? May be hardware defect? ","I observe change in result of torch.sum based on  of tensor items not item number.  1,2 tensor  1,4 tensor (same result of last commentissuecomment1432024762)) ",`torch.sum` appear to be returning  of size1 of tensor or n. of iteration by tensoriterator. Returned sum is `0xbebebebe` in 1st use of `torch.sum` during lifetime of tensor object `x` refer to detail 2  demo Additional test: [torch.sum(x) called multiple times]  output    detail 2  output (all 0xbebebebe)  ,"Hi  , can you kindly provide the raw logs using the following command: `uname a` `apt installed list  grep dkms WARNING: apt does not have a stable CLI interface. Use with caution in scripts. amdgpudkmsfirmware/jammy,now 1:5.18.2.22.40.503001483871.22.04 all [installed,automatic] amdgpudkms/jammy,now 1:5.18.2.22.40.503001483871.22.04 all [installed] dkms/jammy,now 2.8.72ubuntu2 all [installed,upgradable to: 2.8.72ubuntu2.1] ```", I can return to Ubuntu 22.04 system but that is how I found original problem.,"I was able to make `torch.sum` functional by using container on bare metal. `torch.sum` failed in 2 systems of 2 different GPUs (naples:v620, pinnacle ridge:mi100) if run in QEMU/KVM environment. `torch.sum` functional if environment **not** using PCIE passthrough.   environment   However standard developing environment here all VMs. I am not sure how to proceed.",I also have same problem.,`torch.mean` has the same problem,"same problem on physical machine (not VM or container) uname a Linux 5.15.072generic CC(Support ""device"" keyword argument)~20.04.1Ubuntu SMP Thu Apr 20 22:12:07 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux apt installed list | grep dkms WARNING: apt does not have a stable CLI interface. Use with caution in scripts. amdgpudkmsfirmware/focal,focal,now 1:5.18.13.504021528701.20.04 all amdgpudkms/focal,focal,now 1:5.18.13.504021528701.20.04 all dkms/focalupdates,focalupdates,now 2.8.15ubuntu2 all","I'm running into this, any updates? I'm running under ESXi 8, Ubuntu 20.04 uname a Linux ubuntuml 5.4.0148generic CC(indexing with numpy elements)Ubuntu SMP Tue Apr 18 08:53:12 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux apt installed list | grep dkms amdgpudkmsfirmware/focal,now 1:5.18.13.504001510348.20.04 all [installed,automatic] amdgpudkms/focal,now 1:5.18.13.504001510348.20.04 all [installed] dkms/focalupdates,now 2.8.15ubuntu2 all [installed,automatic]","Same issue (also physical machine, no VM) with ROCm 5.4.2. Also tried to install ROCm 5.6.0 and latest rocmpytorch 5.5 but the issue remains. uname a Linux ... 5.15.076generic CC([legacynn] losses need to return tensors and not numbers)Ubuntu SMP Thu Jun 15 19:16:32 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux apt installed list | grep dkms amdgpudkmsfirmware/jammy,now 1:6.1.5.506001609671.22.04 all [installed,automatic] amdgpudkms/jammy,now 1:6.1.5.506001609671.22.04 all [installed] dkms/now 2.8.72ubuntu2.1 all [installed,upgradable to: 2.8.72ubuntu2.2]",    are you all running on AMD Radeon PRO V620 same as the original reporter ?,> are you all running on AMD Radeon PRO V620 same as the original reporter ? I'm running Radeon Instinct MI25 cards.,  > are you all running on AMD Radeon PRO V620 same as the original reporter? AMD Radeon RX 6800 XT and 6600," > Same issue (also physical machine, no VM) with ROCm 5.4.2. Also tried to install ROCm 5.6.0 and latest rocmpytorch 5.5 but the issue remains. >  > uname a Linux ... 5.15.076generic CC([legacynn] losses need to return tensors and not numbers)Ubuntu SMP Thu Jun 15 19:16:32 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux >  > apt installed list | grep dkms amdgpudkmsfirmware/jammy,now 1:6.1.5.506001609671.22.04 all [installed,automatic] amdgpudkms/jammy,now 1:6.1.5.506001609671.22.04 all [installed] dkms/now 2.8.72ubuntu2.1 all [installed,upgradable to: 2.8.72ubuntu2.2] Is that possible you use docker container to see if the problem still exists? The reason I am asking is the  reported the container is good. So, we would like to check whether it is related to that.",We also would like to collect information. Can you run the following commands and send out output:  ," Here you can see the output of the 3 commands.  Update: When pulling the docker image, I unfortunately run into disk space issues. :( python m torch.utils.collect_env   rocminfo   rocmsmi ",">   Sorry about the disk space issue, and hope you can clean up some to move forward.  At the same time, can you do the following:   Then upload the onelinesum3.txt to gist.github.com and share the link. Thank you.","  python m torch.utils.collect_env Collecting environment information... PyTorch version: 2.0.1+rocm5.4.2 Is debug build: False CUDA used to build PyTorch: N/A ROCM used to build PyTorch: 5.4.22803474e8620 OS: Ubuntu 20.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.26.3 Libc version: glibc2.31 Python version: 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.076genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: AMD Radeon Graphics Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: 5.4.22803 MIOpen runtime version: 2.19.0 Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   48 bits physical, 48 bits virtual CPU(s):                          16 Online CPU(s) list:             015 Thread(s) per core:              2 Core(s) per socket:              8 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       AuthenticAMD CPU family:                      25 Model:                           80 Model name:                      AMD Ryzen 7 5825U with Radeon Graphics Stepping:                        0 Frequency boost:                 enabled CPU MHz:                         1600.000 CPU max MHz:                     2000.0000 CPU min MHz:                     1600.0000 BogoMIPS:                        3992.43 Virtualization:                  AMDV L1d cache:                       256 KiB L1i cache:                       256 KiB L2 cache:                        4 MiB L3 cache:                        16 MiB NUMA node0 CPU(s):               015 Vulnerability Itlb multihit:     Not affected Vulnerability L1tf:              Not affected Vulnerability Mds:               Not affected Vulnerability Meltdown:          Not affected Vulnerability Mmio stale data:   Not affected Vulnerability Retbleed:          Not affected Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP alwayson, RSB filling, PBRSBeIBRS Not affected Vulnerability Srbds:             Not affected Vulnerability Tsx async abort:   Not affected Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] pytorchtritonrocm==2.0.1 [pip3] torch==2.0.1+rocm5.4.2 [pip3] torchtbprofiler==0.4.1 [pip3] torchaudio==2.0.2+rocm5.4.2 [pip3] torchvision==0.15.2+rocm5.4.2 [conda] Could not collect  rocminfo ROCk module is loaded =====================     HSA System Attributes     =====================     Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE                               System Endianness:       LITTLE                              ==========                HSA Agents                ==========                *******                   Agent 1                   *******                     Name:                    AMD Ryzen 7 5825U with Radeon Graphics   Uuid:                    CPUXX                                Marketing Name:          AMD Ryzen 7 5825U with Radeon Graphics   Vendor Name:             CPU                                   Feature:                 None specified                        Profile:                 FULL_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        0(0x0)                                Queue Min Size:          0(0x0)                                Queue Max Size:          0(0x0)                                Queue Type:              MULTI                                 Node:                    0                                     Device Type:             CPU                                   Cache Info:                   L1:                      32768(0x8000) KB                      Chip ID:                 0(0x0)                                ASIC Revision:           0(0x0)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   2000                                  BDFID:                   0                                     Internal Node ID:        0                                     Compute Unit:            16                                    SIMDs per CU:            0                                     Shader Engines:          0                                     Shader Arrs. per Eng.:   0                                     WatchPts on Addr. Ranges:1                                     Features:                None   Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: FINE GRAINED               Size:                    15156180(0xe743d4) KB                     Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                    Pool 2                          Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED       Size:                    15156180(0xe743d4) KB                     Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                    Pool 3                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    15156180(0xe743d4) KB                     Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       TRUE                                  ISA Info:                 *******                   Agent 2                   *******                     Name:                    gfx900                                Uuid:                    GPUXX                                Marketing Name:                                                Vendor Name:             AMD                                   Feature:                 KERNEL_DISPATCH                       Profile:                 BASE_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        128(0x80)                             Queue Min Size:          64(0x40)                              Queue Max Size:          131072(0x20000)                       Queue Type:              MULTI                                 Node:                    1                                     Device Type:             GPU                                   Cache Info:                   L1:                      16(0x10) KB                             L2:                      1024(0x400) KB                        Chip ID:                 5607(0x15e7)                          ASIC Revision:           0(0x0)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   2000                                  BDFID:                   1024                                  Internal Node ID:        1                                     Compute Unit:            8                                     SIMDs per CU:            4                                     Shader Engines:          1                                     Shader Arrs. per Eng.:   1                                     WatchPts on Addr. Ranges:4                                     Features:                KERNEL_DISPATCH    Fast F16 Operation:      TRUE                                  Wavefront Size:          64(0x40)                              Workgroup Max Size:      1024(0x400)                           Workgroup Max Size per Dimension:     x                        1024(0x400)                             y                        1024(0x400)                             z                        1024(0x400)                           Max Waves Per CU:        40(0x28)                              Max Workitem Per CU:    2560(0xa00)                           Grid Max Size:           4294967295(0xffffffff)                Grid Max Size per Dimension:     x                        4294967295(0xffffffff)                  y                        4294967295(0xffffffff)                  z                        4294967295(0xffffffff)                Max fbarriers/Workgrp:   32                                    Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    1048576(0x100000) KB                      Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 2                          Segment:                 GROUP                                     Size:                    64(0x40) KB                               Allocatable:             FALSE                                     Alloc Granule:           0KB                                       Alloc Alignment:         0KB                                       Accessible by all:       FALSE                                 ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx900:xnack          Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *** Done ***               rocmsmi ======================= ROCm System Management Interface ======================= ================================= Concise Info ================================= ERROR: GPU[0]	: sclk clock is unsupported ================================================================================ GPU[0]		: Not supported on the given system GPU  Temp (DieEdge)  AvgPwr  SCLK  MCLK     Fan  Perf  PwrCap       VRAM%  GPU%   0    43.0c           0.003W  None  1200Mhz  0%   auto  Unsupported   44%   0%     ================================================================================ ============================= End of ROCm SMI Log ==============================  `python3 c ""import torch;print(torch.sum(torch.ones(1,1234).to('cuda:0')))""` onelinesum3.txt",">  Hi,  By inspecting the output, I found you have the failure of ""Pcie atomics not enabled, hostcall not supported"". The PCIe atomics is essential for the ROCm support. (please check this link https://github.com/RadeonOpenCompute/ROCm/issues/157 for more information)", here's my onelinesum3.txt,">  here's my onelinesum3.txt  Your log showed similar problem as  's :  ""Pcie atomics not enabled"" :  ","So it is essentially a CPU or motherboard problem? Is this something, that can be fixed with trivial reconfiguration or a chipset update? Otherwise we would have to abandon the AMD cards/ROCm for machine learning purposes altogether. Should the PCIE atomics support be tested at runtime by the driver, so it doesn't return a wrong result?","The driver should test that, but apparently they are not. I can run opencl apps without problem. On Wed, 26 Jul 2023 at 14:17, Arne Wellnitz ***@***.***> wrote: > So it is essentially a CPU or motherboard problem? Is this something, that > can be fixed with trivial reconfiguration or a chipset update? Otherwise we > would have to abandon the AMD cards/ROCm for machine learning purposes > altogether. > Should the PCIE atomics support be tested at runtime by the driver, so it > doesn't return a wrong result? > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","> So it is essentially a CPU or motherboard problem? Is this something, that can be fixed with trivial reconfiguration or a chipset update? Otherwise we would have to abandon the AMD cards/ROCm for machine learning purposes altogether. Should the PCIE atomics support be tested at runtime by the driver, so it doesn't return a wrong result? Thanks for the question and suggestion. We will get back to you with the answers. ","  After checking with our experts, here are some steps to follow up: (1) check whether PCIe atomics working on one of your GPUs. Test the torch.sum code by setting ROCR_VISIBLE_DEVICES=0. If that fails, try with ROCR_VISIBLE_DEVICES=1.  Please let us know whether either of GPUs works. (2) Please post your motherboard name so that we can look up the manual for the specific board. (3) please send us the output of below:  For more context, please check below https://github.com/RadeonOpenCompute/ROCm/issues/760issuecomment482372238 Thanks.","Motherboard info   lspci vvv   lspci tv  [1150460.239666] amdgpu 0000:0a:00.0: amdgpu: SMU driver if version not matched [1150460.239741] amdgpu 0000:0a:00.0: amdgpu: use vbios provided pptable [1150460.249265] amdgpu 0000:0a:00.0: amdgpu: SMU is resumed successfully! [1150460.301498] amdgpu 0000:0a:00.0: amdgpu: ring gfx_0.0.0 uses VM inv eng 0 on hub 0 [1150460.301500] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.0 uses VM inv eng 1 on hub 0 [1150460.301501] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.0 uses VM inv eng 4 on hub 0 [1150460.301502] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.0 uses VM inv eng 5 on hub 0 [1150460.301503] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.0 uses VM inv eng 6 on hub 0 [1150460.301503] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.1 uses VM inv eng 7 on hub 0 [1150460.301504] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.1 uses VM inv eng 8 on hub 0 [1150460.301505] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.1 uses VM inv eng 9 on hub 0 [1150460.301506] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.1 uses VM inv eng 10 on hub 0 [1150460.301507] amdgpu 0000:0a:00.0: amdgpu: ring kiq_2.1.0 uses VM inv eng 11 on hub 0 [1150460.301507] amdgpu 0000:0a:00.0: amdgpu: ring sdma0 uses VM inv eng 12 on hub 0 [1150460.301508] amdgpu 0000:0a:00.0: amdgpu: ring sdma1 uses VM inv eng 13 on hub 0 [1150460.301509] amdgpu 0000:0a:00.0: amdgpu: ring vcn_dec_0 uses VM inv eng 0 on hub 1 [1150460.301510] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.0 uses VM inv eng 1 on hub 1 [1150460.301511] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.1 uses VM inv eng 4 on hub 1 [1150460.301511] amdgpu 0000:0a:00.0: amdgpu: ring jpeg_dec uses VM inv eng 5 on hub 1 [1150460.305566] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150460.305571] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150466.669570] amdgpu 0000:0a:00.0: amdgpu: free PSP TMR buffer [1150488.358462] amdgpu 0000:0a:00.0: amdgpu: RAS: optional ras ta ucode is not available [1150488.375552] amdgpu 0000:0a:00.0: amdgpu: SECUREDISPLAY: securedisplay ta ucode is not available [1150488.375558] amdgpu 0000:0a:00.0: amdgpu: SMU is resuming... [1150488.375570] amdgpu 0000:0a:00.0: amdgpu: smu driver if version = 0x0000000f, smu fw if version = 0x00000013, smu fw program = 0, version = 0x003b2a00 (59.42.0) [1150488.375577] amdgpu 0000:0a:00.0: amdgpu: SMU driver if version not matched [1150488.375652] amdgpu 0000:0a:00.0: amdgpu: use vbios provided pptable [1150488.385539] amdgpu 0000:0a:00.0: amdgpu: SMU is resumed successfully! [1150488.437513] amdgpu 0000:0a:00.0: amdgpu: ring gfx_0.0.0 uses VM inv eng 0 on hub 0 [1150488.437515] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.0 uses VM inv eng 1 on hub 0 [1150488.437516] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.0 uses VM inv eng 4 on hub 0 [1150488.437517] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.0 uses VM inv eng 5 on hub 0 [1150488.437518] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.0 uses VM inv eng 6 on hub 0 [1150488.437518] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.1 uses VM inv eng 7 on hub 0 [1150488.437519] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.1 uses VM inv eng 8 on hub 0 [1150488.437520] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.1 uses VM inv eng 9 on hub 0 [1150488.437521] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.1 uses VM inv eng 10 on hub 0 [1150488.437522] amdgpu 0000:0a:00.0: amdgpu: ring kiq_2.1.0 uses VM inv eng 11 on hub 0 [1150488.437523] amdgpu 0000:0a:00.0: amdgpu: ring sdma0 uses VM inv eng 12 on hub 0 [1150488.437523] amdgpu 0000:0a:00.0: amdgpu: ring sdma1 uses VM inv eng 13 on hub 0 [1150488.437524] amdgpu 0000:0a:00.0: amdgpu: ring vcn_dec_0 uses VM inv eng 0 on hub 1 [1150488.437525] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.0 uses VM inv eng 1 on hub 1 [1150488.437526] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.1 uses VM inv eng 4 on hub 1 [1150488.437527] amdgpu 0000:0a:00.0: amdgpu: ring jpeg_dec uses VM inv eng 5 on hub 1 [1150488.442116] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150488.442134] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150494.828657] amdgpu 0000:0a:00.0: amdgpu: free PSP TMR buffer [1150503.687287] amdgpu 0000:0a:00.0: amdgpu: RAS: optional ras ta ucode is not available [1150503.704446] amdgpu 0000:0a:00.0: amdgpu: SECUREDISPLAY: securedisplay ta ucode is not available [1150503.704453] amdgpu 0000:0a:00.0: amdgpu: SMU is resuming... [1150503.704463] amdgpu 0000:0a:00.0: amdgpu: smu driver if version = 0x0000000f, smu fw if version = 0x00000013, smu fw program = 0, version = 0x003b2a00 (59.42.0) [1150503.704470] amdgpu 0000:0a:00.0: amdgpu: SMU driver if version not matched [1150503.704598] amdgpu 0000:0a:00.0: amdgpu: use vbios provided pptable [1150503.715186] amdgpu 0000:0a:00.0: amdgpu: SMU is resumed successfully! [1150503.767333] amdgpu 0000:0a:00.0: amdgpu: ring gfx_0.0.0 uses VM inv eng 0 on hub 0 [1150503.767334] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.0 uses VM inv eng 1 on hub 0 [1150503.767335] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.0 uses VM inv eng 4 on hub 0 [1150503.767337] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.0 uses VM inv eng 5 on hub 0 [1150503.767337] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.0 uses VM inv eng 6 on hub 0 [1150503.767338] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.1 uses VM inv eng 7 on hub 0 [1150503.767339] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.1 uses VM inv eng 8 on hub 0 [1150503.767340] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.1 uses VM inv eng 9 on hub 0 [1150503.767341] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.1 uses VM inv eng 10 on hub 0 [1150503.767341] amdgpu 0000:0a:00.0: amdgpu: ring kiq_2.1.0 uses VM inv eng 11 on hub 0 [1150503.767342] amdgpu 0000:0a:00.0: amdgpu: ring sdma0 uses VM inv eng 12 on hub 0 [1150503.767343] amdgpu 0000:0a:00.0: amdgpu: ring sdma1 uses VM inv eng 13 on hub 0 [1150503.767344] amdgpu 0000:0a:00.0: amdgpu: ring vcn_dec_0 uses VM inv eng 0 on hub 1 [1150503.767345] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.0 uses VM inv eng 1 on hub 1 [1150503.767345] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.1 uses VM inv eng 4 on hub 1 [1150503.767346] amdgpu 0000:0a:00.0: amdgpu: ring jpeg_dec uses VM inv eng 5 on hub 1 [1150503.771801] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150503.771805] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150510.141370] amdgpu 0000:0a:00.0: amdgpu: free PSP TMR buffer [1150526.414835] amdgpu 0000:0a:00.0: amdgpu: RAS: optional ras ta ucode is not available [1150526.431948] amdgpu 0000:0a:00.0: amdgpu: SECUREDISPLAY: securedisplay ta ucode is not available [1150526.431956] amdgpu 0000:0a:00.0: amdgpu: SMU is resuming... [1150526.431968] amdgpu 0000:0a:00.0: amdgpu: smu driver if version = 0x0000000f, smu fw if version = 0x00000013, smu fw program = 0, version = 0x003b2a00 (59.42.0) [1150526.431975] amdgpu 0000:0a:00.0: amdgpu: SMU driver if version not matched [1150526.432047] amdgpu 0000:0a:00.0: amdgpu: use vbios provided pptable [1150526.441780] amdgpu 0000:0a:00.0: amdgpu: SMU is resumed successfully! [1150526.493560] amdgpu 0000:0a:00.0: amdgpu: ring gfx_0.0.0 uses VM inv eng 0 on hub 0 [1150526.493561] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.0 uses VM inv eng 1 on hub 0 [1150526.493562] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.0 uses VM inv eng 4 on hub 0 [1150526.493563] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.0 uses VM inv eng 5 on hub 0 [1150526.493564] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.0 uses VM inv eng 6 on hub 0 [1150526.493565] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.1 uses VM inv eng 7 on hub 0 [1150526.493565] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.1 uses VM inv eng 8 on hub 0 [1150526.493566] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.1 uses VM inv eng 9 on hub 0 [1150526.493567] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.1 uses VM inv eng 10 on hub 0 [1150526.493568] amdgpu 0000:0a:00.0: amdgpu: ring kiq_2.1.0 uses VM inv eng 11 on hub 0 [1150526.493569] amdgpu 0000:0a:00.0: amdgpu: ring sdma0 uses VM inv eng 12 on hub 0 [1150526.493570] amdgpu 0000:0a:00.0: amdgpu: ring sdma1 uses VM inv eng 13 on hub 0 [1150526.493570] amdgpu 0000:0a:00.0: amdgpu: ring vcn_dec_0 uses VM inv eng 0 on hub 1 [1150526.493571] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.0 uses VM inv eng 1 on hub 1 [1150526.493572] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.1 uses VM inv eng 4 on hub 1 [1150526.493573] amdgpu 0000:0a:00.0: amdgpu: ring jpeg_dec uses VM inv eng 5 on hub 1 [1150526.497743] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150526.497747] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1150532.837123] amdgpu 0000:0a:00.0: amdgpu: free PSP TMR buffer [1685887.394386] amdgpu 0000:0a:00.0: amdgpu: RAS: optional ras ta ucode is not available [1685887.411460] amdgpu 0000:0a:00.0: amdgpu: SECUREDISPLAY: securedisplay ta ucode is not available [1685887.411467] amdgpu 0000:0a:00.0: amdgpu: SMU is resuming... [1685887.411477] amdgpu 0000:0a:00.0: amdgpu: smu driver if version = 0x0000000f, smu fw if version = 0x00000013, smu fw program = 0, version = 0x003b2a00 (59.42.0) [1685887.411484] amdgpu 0000:0a:00.0: amdgpu: SMU driver if version not matched [1685887.411557] amdgpu 0000:0a:00.0: amdgpu: use vbios provided pptable [1685887.421228] amdgpu 0000:0a:00.0: amdgpu: SMU is resumed successfully! [1685887.471974] amdgpu 0000:0a:00.0: amdgpu: ring gfx_0.0.0 uses VM inv eng 0 on hub 0 [1685887.471976] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.0 uses VM inv eng 1 on hub 0 [1685887.471977] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.0 uses VM inv eng 4 on hub 0 [1685887.471978] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.0 uses VM inv eng 5 on hub 0 [1685887.471979] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.0 uses VM inv eng 6 on hub 0 [1685887.471979] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.0.1 uses VM inv eng 7 on hub 0 [1685887.471980] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.1.1 uses VM inv eng 8 on hub 0 [1685887.471981] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.2.1 uses VM inv eng 9 on hub 0 [1685887.471982] amdgpu 0000:0a:00.0: amdgpu: ring comp_1.3.1 uses VM inv eng 10 on hub 0 [1685887.471983] amdgpu 0000:0a:00.0: amdgpu: ring kiq_2.1.0 uses VM inv eng 11 on hub 0 [1685887.471984] amdgpu 0000:0a:00.0: amdgpu: ring sdma0 uses VM inv eng 12 on hub 0 [1685887.471985] amdgpu 0000:0a:00.0: amdgpu: ring sdma1 uses VM inv eng 13 on hub 0 [1685887.471985] amdgpu 0000:0a:00.0: amdgpu: ring vcn_dec_0 uses VM inv eng 0 on hub 1 [1685887.471986] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.0 uses VM inv eng 1 on hub 1 [1685887.471987] amdgpu 0000:0a:00.0: amdgpu: ring vcn_enc_0.1 uses VM inv eng 4 on hub 1 [1685887.471988] amdgpu 0000:0a:00.0: amdgpu: ring jpeg_dec uses VM inv eng 5 on hub 1 [1685887.477162] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1685887.477166] amdgpu 0000:0a:00.0: [drm] Cannot find any crtc or sizes [1685895.441035] amdgpu 0000:0a:00.0: amdgpu: free PSP TMR buffer ``` ","> Motherboard info > lspci vvv > lspci tv > dmesg | grep amdgpu Thanks. How about item (1) regarding the ROCR_VISIBLE_DEVICES test on the lower level check? ALso, can you output dmesg without grep amdgpu? if it is too long, maybe you can put it to gist?  Thanks/","Sorry, I forgot. torch.sum() with either ROCR_VISIBLE_DEVICES = 0 or 1 leads to same wrong result. dmesg"
1597,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(RuntimeError: Expected is_sm80 to be true, but got false.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm running Karpathy's nanoGPT code on an NVIDIA A6000 system via Docker: `nvcr.io/nvidia/cuda:11.8.0develubuntu22.04` and the latest nightly pytorch 2.0 build (see below for exact version): `pip3 install pre torch torchvision torchaudio forcereinstall indexurl https://download.pytorch.org/whl/nightly/cu118` The easiest way to reproduce the error is to just download the model.py from github and run it directly: `wget https://raw.githubusercontent.com/karpathy/nanoGPT/master/model.py`  Note the nonstandard (yet very optimal!) GPT2 configuration. The model isn't compiled, but still leveraging the `F.scaled_dot_product_attention()` (fused) path. Note the model does run correctly when using the standard GPT2 config: `model = GPT(config=GPTConfig(n_layer=12, n_head=12, n_embd=768, dropout=0,  bias=False)).cuda()` This error is most likely due to the fact that the A6000 is an SM86 card and not an SM80... although one would think this should still pass the check (hmmm) As discussed here: https://discuss.pytorch.org/t/expectedissm80tobetruebutgotfalse/172572/5 Strangely enough the model was fine when we used: `pip3 install pre torchvision torch==2.0.0.dev20230201 forcereinstall indexurl https://download.pytorch.org/whl/nightly/cu117` but training broke after Feb 1st... thanks! see below for full error message:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,"RuntimeError: Expected is_sm80 to be true, but got false."," ğŸ› Describe the bug I'm running Karpathy's nanoGPT code on an NVIDIA A6000 system via Docker: `nvcr.io/nvidia/cuda:11.8.0develubuntu22.04` and the latest nightly pytorch 2.0 build (see below for exact version): `pip3 install pre torch torchvision torchaudio forcereinstall indexurl https://download.pytorch.org/whl/nightly/cu118` The easiest way to reproduce the error is to just download the model.py from github and run it directly: `wget https://raw.githubusercontent.com/karpathy/nanoGPT/master/model.py`  Note the nonstandard (yet very optimal!) GPT2 configuration. The model isn't compiled, but still leveraging the `F.scaled_dot_product_attention()` (fused) path. Note the model does run correctly when using the standard GPT2 config: `model = GPT(config=GPTConfig(n_layer=12, n_head=12, n_embd=768, dropout=0,  bias=False)).cuda()` This error is most likely due to the fact that the A6000 is an SM86 card and not an SM80... although one would think this should still pass the check (hmmm) As discussed here: https://discuss.pytorch.org/t/expectedissm80tobetruebutgotfalse/172572/5 Strangely enough the model was fine when we used: `pip3 install pre torchvision torch==2.0.0.dev20230201 forcereinstall indexurl https://download.pytorch.org/whl/nightly/cu117` but training broke after Feb 1st... thanks! see below for full error message:   Versions  ",2023-02-15T05:33:45Z,high priority triage review,closed,2,16,https://github.com/pytorch/pytorch/issues/94883,"As  pointed out, the check seems to be introduced in https://github.com/pytorch/pytorch/pull/91994 and would approx. fit the time frame: ""but training broke after Feb 1st...""",.0,I will update the dispatching logic to account for this. I do think we should fix this before 2.0  ,"  As a temp workaround, Can you try using the context manager described in the documentation to disable flash_attention: ", yes the context manager works.  please fix :) ,Pushed the fixing PR: https://github.com/pytorch/pytorch/pull/94921 Don't have easy access to sm86 but should be tested in CI,thanks   I don't have time to build the source :) so I will just test when it shows up in the nightlies.,"I'm observing same error with sm90, worked around by `with torch.backends.cuda.sdp_kernel(enable_flash=False) as disable` as suggested. Will there be a fix for sm90 too?   thx","> I'm observing same error with sm90, worked around by `with torch.backends.cuda.sdp_kernel(enable_flash=False) as disable` as suggested. >  > Will there be a fix for sm90 too?   thx This is actually quite surprising: https://github.com/pytorch/pytorch/blob/d6dd67a2488c7e17fbf010eee805f1cb2d64ba28/aten/src/ATen/native/transformers/cuda/sdp_utils.hL356 This line should have prevented you from being able to run flash_attention since this should fail on sm90. I don't have access to sm90 hardware, nor do we currently have it in CI/CD. This might be hard to reproduce for me. Can you put some more details up, or perhaps create a new issue. ","I got this error with A10, which is actually a sm80 device...","> I got this error with A10, which is actually a sm80 device... Note that A10 is sm86 rather than sm80, which is a different compute capability: CUDAEnabled Datacenter Products", Thanks a lot for the information! Is this flash attention only compatible with sm80? ,FlashAttention is compatible with compute capability > sm75. However https://github.com/pytorch/pytorch/blob/41280a0791b4e89b6e250b702704c04121deb070/aten/src/ATen/native/transformers/cuda/sdp_utils.hL541 is another constraint. This however does not seem to be effecting sm90 hardware see: https://github.com/pytorch/pytorch/pull/99776,why is this issue closed?  was it fixed?  in what release?,"In case it helps, this was solved on my machine after installing the nightly: ","When I first used the base imageï¼ˆpytorch/pytorch:2.0.0cuda11.7cudnn8develï¼‰, I got an error,  so I just needed to change it ï¼ˆpytorch/pytorch:2.0.1cuda11.7cudnn8develï¼‰"
1632,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Loss of accuracy when Longformer for SequenceClassification model is exported to ONNX)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug This is a crosspost to huggingface huggingface/optimum CC(Failed when RNN run with cudnn with `requires_grad` set to `False`)   Expected behavior I would expect a similar accuracy for both models: Accuracy onnx:  17 % Accuracy torch: 70 % on test data with 3800 samples. I would like to know what went wrong, how I can fix it, or who can help me. I'm clueless at the moment. I trained a small Longformer language model from scratch and finetuned it with custom data on a Sequence classification head. I used fp16 for training. The training run on a GPU.  Reproduction This model is trained on client data and I'm not allowed to share the data or the weights, which makes any reproduction of this issue much harder. Please let me know when you need more information. Here is the code snippet for the onnx conversion: I follow this tutorial, but I also tried your tutorial.  conversion:  Calculating the accuracy:  I also looked into the models' weights and the weights for the attention layer differ between torch and onnx. Here is an example:  For the layer longformer.encoder.layer.0.output.dense.weight, which aligns with onnx::MatMul_6692 in shape and position:  I get:   Versions Python version: 3.9.12 PyTorch version (GPU?): 1.13.0 (False) onnx: 1.13.0 onnxruntime: 1.13.1 transformers version: 4.26.1 Platform: macOS10.16x86_64i38664bit)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Loss of accuracy when Longformer for SequenceClassification model is exported to ONNX," ğŸ› Describe the bug This is a crosspost to huggingface huggingface/optimum CC(Failed when RNN run with cudnn with `requires_grad` set to `False`)   Expected behavior I would expect a similar accuracy for both models: Accuracy onnx:  17 % Accuracy torch: 70 % on test data with 3800 samples. I would like to know what went wrong, how I can fix it, or who can help me. I'm clueless at the moment. I trained a small Longformer language model from scratch and finetuned it with custom data on a Sequence classification head. I used fp16 for training. The training run on a GPU.  Reproduction This model is trained on client data and I'm not allowed to share the data or the weights, which makes any reproduction of this issue much harder. Please let me know when you need more information. Here is the code snippet for the onnx conversion: I follow this tutorial, but I also tried your tutorial.  conversion:  Calculating the accuracy:  I also looked into the models' weights and the weights for the attention layer differ between torch and onnx. Here is an example:  For the layer longformer.encoder.layer.0.output.dense.weight, which aligns with onnx::MatMul_6692 in shape and position:  I get:   Versions Python version: 3.9.12 PyTorch version (GPU?): 1.13.0 (False) onnx: 1.13.0 onnxruntime: 1.13.1 transformers version: 4.26.1 Platform: macOS10.16x86_64i38664bit",2023-02-14T10:36:31Z,module: onnx triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/94810,"Hi  , as noted in the linked issue, the bottom line issue is  CC(Export to ONNX of `as_strided()` hard codes stride in the graph, although it should be dynamic) .","Closing as this is a duplicate of CC(Export to ONNX of `as_strided()` hard codes stride in the graph, although it should be dynamic). Please continue discussion on that issue. Thanks!"
1513,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(EAGER backend raise `torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(RLock) acquire [] {}` error but training is continuing)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Refers to issue CC(`TypeError: super(type, obj): obj must be an instance or subtype of type` has no type error in training with EAGER backend), I manually add patch CC([Dynamo] Fix bug of calling super from class extended from metaclass) to `torch._dynamo/variables/misc.py` to fix bug 1 (`TypeError: super(type, obj) `) When I add `dynamo.config.log_level = logging.DEBUG` and `dynamo.config.suppress_errors = True` to my code,  `torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(RLock) acquire [] {}` occurs, but the training is continuing. If a remove `dynamo.config.log_level = logging.DEBUG` and `dynamo.config.suppress_errors = True`, no error will be printed on the screen.  I wonder whether this `torch._dynamo.exc.Unsupported` error will harm the training process, or should I just simply ignore this error?  Error logs The complete debug log: Google Drive Error Traceback of the debug log:   Minified repro 1. Install dependencied of OpenMMLab  3. Install MMEditing from source  4. Launch training   Versions Pytorch version: 2.0.0.dev20230202+cu116 (with Patch CC([Dynamo] Fix bug of calling super from class extended from metaclass)) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,EAGER backend raise `torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(RLock) acquire [] {}` error but training is continuing," ğŸ› Describe the bug Refers to issue CC(`TypeError: super(type, obj): obj must be an instance or subtype of type` has no type error in training with EAGER backend), I manually add patch CC([Dynamo] Fix bug of calling super from class extended from metaclass) to `torch._dynamo/variables/misc.py` to fix bug 1 (`TypeError: super(type, obj) `) When I add `dynamo.config.log_level = logging.DEBUG` and `dynamo.config.suppress_errors = True` to my code,  `torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(RLock) acquire [] {}` occurs, but the training is continuing. If a remove `dynamo.config.log_level = logging.DEBUG` and `dynamo.config.suppress_errors = True`, no error will be printed on the screen.  I wonder whether this `torch._dynamo.exc.Unsupported` error will harm the training process, or should I just simply ignore this error?  Error logs The complete debug log: Google Drive Error Traceback of the debug log:   Minified repro 1. Install dependencied of OpenMMLab  3. Install MMEditing from source  4. Launch training   Versions Pytorch version: 2.0.0.dev20230202+cu116 (with Patch CC([Dynamo] Fix bug of calling super from class extended from metaclass)) ",2023-02-14T03:45:56Z,oncall: pt2,closed,0,5,https://github.com/pytorch/pytorch/issues/94795,Can you show the full log? If there is a graph break (seems like it here due to Rlock) then this message is printed as the cause for the graph break. (i.e. you should not worry),"`Unsupported` means there is a graph break on `RLock.acquire()`, so part of the model is being run in eager mode rather than being compiled.","> Can you show the full log? If there is a graph break (seems like it here due to Rlock) then this message is printed as the cause for the graph break. >  > (i.e. you should not worry) chuang  and , thanks for your response! My complete debug log can be downloaded from this URL: Google Drive.","Indeed, it's a graph break:  Let's close the issue?",chuang. Thanks for your response! Sorry that I didn't read the technical documentation on graph break and dynamo before. Now I know what happens to my code.
2019,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Bootcamp] [AI Accelerators] Convert attention equation to attention call in torch.compile())ï¼Œ å†…å®¹æ˜¯ (Summary:  Goals Many attentionbased models directly implement scaled dot product attention used by transformer models using component pytorch operators.  Recognize these with FX rewrites and convert them to calls to sdpa(), such that the optimized implementation is chosen where applicable. Implementation Given that the usual scaled dot product attention equation references a shape variable ( because everything gets normalized by a dividing by sqrt(embedding_dim_size), the graph rewrite is not entirely trivial. It requires a new scale_factor parameter in torch.nn.functional.scaled_dot_product_attention ( not part of this PR, see  discussion https://github.com/pytorch/pytorch/pull/94729discussion_r1107490896 about earlier version of this PR ) This in turn will allow to rewrite any pattern like   torch.matmul(query, key.transpose(2, 1))         .div(scale_factor)         .softmax(dim=1)         .matmul(value) into a corresponding (potentially optimized) API call like   scaled_dot_product_attention(         query.contiguous(),         key.contiguous(),         value.contiguous(),         attn_mask=None,         dropout_p=0.0,         is_causal=False,         scale_factor=scale_factor,     ) without knowing the scale factor or query and key shapes at compile time.  Reviewer Notes: As discussed with drisspg, this code would be ready for integration into torch._inductor.overrides once the scale_factor parameter is available on torch.nn.functional.scaled_dot_product_attention. Once that is done, the corresponding sections in the unit tests may be activated again to verify not just the graph rewrite, but also numerical equivalence.  Followup Tasks: Once torch.nn.functional.scaled_dot_product_attention has a scale_factor pa)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Bootcamp] [AI Accelerators] Convert attention equation to attention call in torch.compile(),"Summary:  Goals Many attentionbased models directly implement scaled dot product attention used by transformer models using component pytorch operators.  Recognize these with FX rewrites and convert them to calls to sdpa(), such that the optimized implementation is chosen where applicable. Implementation Given that the usual scaled dot product attention equation references a shape variable ( because everything gets normalized by a dividing by sqrt(embedding_dim_size), the graph rewrite is not entirely trivial. It requires a new scale_factor parameter in torch.nn.functional.scaled_dot_product_attention ( not part of this PR, see  discussion https://github.com/pytorch/pytorch/pull/94729discussion_r1107490896 about earlier version of this PR ) This in turn will allow to rewrite any pattern like   torch.matmul(query, key.transpose(2, 1))         .div(scale_factor)         .softmax(dim=1)         .matmul(value) into a corresponding (potentially optimized) API call like   scaled_dot_product_attention(         query.contiguous(),         key.contiguous(),         value.contiguous(),         attn_mask=None,         dropout_p=0.0,         is_causal=False,         scale_factor=scale_factor,     ) without knowing the scale factor or query and key shapes at compile time.  Reviewer Notes: As discussed with drisspg, this code would be ready for integration into torch._inductor.overrides once the scale_factor parameter is available on torch.nn.functional.scaled_dot_product_attention. Once that is done, the corresponding sections in the unit tests may be activated again to verify not just the graph rewrite, but also numerical equivalence.  Followup Tasks: Once torch.nn.functional.scaled_dot_product_attention has a scale_factor pa",2023-02-13T13:30:10Z,fb-exported module: amp (automated mixed precision) Stale release notes: fx module: inductor ciflow/inductor,closed,0,17,https://github.com/pytorch/pytorch/issues/94729,The committers listed above are authorized under a signed CLA.:white_check_mark: login: kadeng / name: Kai Londenberg  (0c26d4ad4f1d7a7a74ba76165059d0f005acc357),This pull request was **exported** from Phabricator. Differential Revision: D43234635,CLA is signed now. I am a Meta employee in Bootcamp.,/easycla,"Update:  Addressed issues flagged by CI, made torch.nn.functional.scale_factor_dot_product_attention private ( now called torch.nn.functional._scale_factor_dot_product_attention  ) Had to do a forcecommit from the patch file after I was unable to merge via export from Metainternal diff.",This pull request was **exported** from Phabricator. Differential Revision: D43234635,This pull request was **exported** from Phabricator. Differential Revision: D43234635,This pull request was **exported** from Phabricator. Differential Revision: D43234635,"> It doesn't look like this is actually called from anywhere? The original PR called this, but I removed the corresponding code on drisspg's request and was waiting for drisspg's changes to find their way into master. I can now adapt this changeset, so that it will be called from within overrides.py > This should be done with pattern_matcher.py's patterns which are require O(1) passes over the graph rather than O(num_patterns). I was not aware of pattern_matcher.py so far. It looks like it is also capable replacing patterns like these, even more efficiently so. But I could not find any examples of comparable pattern complexity in there. Also, there is one advantage of the approach I chose over pattern_matcher.py as it is: The ease of pattern specification as simply parameterized python functions, which does not require an understanding of torch.fx.Graph internals at all (e.g. it is something that could maybe exposed as an API to Pytorch users at some point). I would like to expand the list of replacements further sometime in the future ( if I find time ).  But of course, replacements which are applied *by default* when calling torch.compile should be efficient enough to not slow compilation too much, even in worst case. I will first adapt the changeset to drisspg's changes and update this PR accordingly with a working version. Then maybe we can have a chat about how to turn these patterns into pattern_matcher.py patterns efficiently. ",Another update is that : https://github.com/pytorch/pytorch/pull/95259 has landed which enables the scale kwarg for sdpa,"> I was not aware of pattern_matcher.py so far. It looks like it is also capable replacing patterns like these, even more efficiently so. But I could not find any examples of comparable pattern complexity in there. Also, there is one advantage of the approach I chose over pattern_matcher.py as it is: The ease of pattern specification as simply parameterized python functions, which does not require an understanding of torch.fx.Graph internals at all (e.g. it is something that could maybe exposed as an API to Pytorch users at some point). I would like to expand the list of replacements further sometime in the future ( if I find time ). But of course, replacements which are applied _by default_ when calling torch.compile should be efficient enough to not slow compilation too much, even in worst case. All the patterns here seem pretty simple.  Should be easy to convert them over."," you should be able to generate patterns with something like:  The second one will apply all the decomps automatically, so you don't need to try to match a decomposed softmax.",This pull request was **exported** from Phabricator. Differential Revision: D43234635,">  you should be able to generate patterns with something like: Thanks, I just saw your reply here when I updated my diff. I will try to incorporate that into the most recent version. ",This pull request was **exported** from Phabricator. Differential Revision: D43234635,https://github.com/pytorch/pytorch/pull/97741 includes most of these patterns as a postgrad joint graph pass.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
653,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug  Code   Bug The GPU memory in the forward stage is normal, but the GPU memory overflows in the backward stage. According to the principle of fsdp, it is judged that the memory usage of the GPU should not overflow at this time. !image  Versions host with 4 A10 GPU, 236 CPU cores  and 974G memory torch==1.13.1+cu116 transformers==4.26.0 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading"," ğŸ› Describe the bug  Code   Bug The GPU memory in the forward stage is normal, but the GPU memory overflows in the backward stage. According to the principle of fsdp, it is judged that the memory usage of the GPU should not overflow at this time. !image  Versions host with 4 A10 GPU, 236 CPU cores  and 974G memory torch==1.13.1+cu116 transformers==4.26.0 ",2023-02-09T14:35:11Z,oncall: distributed module: fsdp,open,1,6,https://github.com/pytorch/pytorch/issues/94511,varma  maybe you can shine in on why this setup is not working?,We need to invest some time to investigate CPU offloading issues. See also  CC([FSDP] FSDP with CPU offload consumes `1.65X` more GPU memory when training models with most of the params frozen).  One curious thing though is why do you have the `CPUOffload` constructed like  instead of simply `CPUOffload(offload_params=True)`? Is that a typo only in this Github issue?,"> We need to invest some time to investigate CPU offloading issues. See also CC([FSDP] FSDP with CPU offload consumes `1.65X` more GPU memory when training models with most of the params frozen). >  > One curious thing though is why do you have the `CPUOffload` constructed like >  >  >  > instead of simply `CPUOffload(offload_params=True)`? Is that a typo only in this Github issue? Thank you for your reply. Hope you can investigate the CPU offloading issues soon. ""CPUOffload(CPUOffload(offload_params=True))"" was indeed a lowlevel mistake, but it does not affect the training. I corrected this code but still can only train opt13b model, not opt30b. ", I have some questions about activation checkpoint here. Is it right if I use the following code to checkpointï¼ŸI found that the GPU memory didn't change significantly after I added that code. I used torch.cuda.memory_allocated() to get gpu memory allocation. ,"chao You may be using an older version of checkpoint_wrapper, as we've removed the offload_to_cpu argument and instead offer an offload_wrapper argument.  Curious, if offload_to_cpu is removed and traditional activation checkpoint (involving recompute) is used, does that change GPU memory characteristics?","varma Do you mean that I need to use a new version of pytorch, in which I can use a new version of checkpoint? But the pytorch I am using is already version 1.13.1. How should I install a newer version of pytorch, do I need to build it from source? I want it to be stable."
500,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug `jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage`. By contrast, the `torch.autograd.jacobian` will return the gradient without any error   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"`jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage`"," ğŸ› Describe the bug `jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage`. By contrast, the `torch.autograd.jacobian` will return the gradient without any error   Versions  ",2023-02-08T23:05:47Z,module: autograd triaged actionable module: functorch,open,0,1,https://github.com/pytorch/pytorch/issues/94451,"This is expected, but maybe its worth printing a better error for this."
722,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Exporting the operator 'aten::_transformer_encoder_layer_fwd' to ONNX opset version 13 is not supported)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I just wanted to export to onnx torch.nn.TransformerEncoder, and got this type of error raise errors.UnsupportedOperatorError( torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::_transformer_encoder_layer_fwd' to ONNX opset version 13 is not supported.)  Versions numpy==1.24.1 pytorchlightning==1.9.0 torch==1.13.1 torchaudio==0.13.1 torchdata==0.5.1 torchmetrics==0.11.1 torchvision==0.14.1)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Exporting the operator 'aten::_transformer_encoder_layer_fwd' to ONNX opset version 13 is not supported," ğŸ› Describe the bug I just wanted to export to onnx torch.nn.TransformerEncoder, and got this type of error raise errors.UnsupportedOperatorError( torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::_transformer_encoder_layer_fwd' to ONNX opset version 13 is not supported.)  Versions numpy==1.24.1 pytorchlightning==1.9.0 torch==1.13.1 torchaudio==0.13.1 torchdata==0.5.1 torchmetrics==0.11.1 torchvision==0.14.1",2023-02-08T20:25:29Z,module: onnx low priority triaged onnx-needs-info,closed,1,9,https://github.com/pytorch/pytorch/issues/94434,Could you please provide a full repro?  ,"Would this be what are you referring to?  I am assuming this is not a valid repro, because another operator (aka `aten::unflatten`) is reported as unsupported. ","Nonetheless, https://pytorch.org/docs/stable/onnx_supported_aten_ops.html?highlight=_transformer_encoder_layer_fwd shows that `aten::_transformer_encoder_layer_fwd` is also unsupported at this point","Could you please provide a full repro?  Without a repro, we are going to close this task",I have also tried this and get the same error with opsets 14 and 17. Repro:  ,"Hi,  I am facing the same issue. Is there any plan to add this operation ?",My repo is https://github.com/bshall/hubert. Code:  Result: ,"`_transformer_encoder_layer_fwd` is not an op we have implemented. Please feel free to contribute to `torch/onnx/symbolic_function[,.]` or experiment with the new `torch.onnx.dynamo_export` which requires the latest torch nightly build","This issue (has/has not) been replicated on our end. We believe that it is a viable issue, but we do not plan to address it in the immediate future, due to other more prevalent priorities."
2023,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(nn.TransformerEncoderLayer fastpath (BetterTransformer) is much slower with src_key_padding_mask)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug TransformerEncoder runs much slower with src_key_padding_mask than without any padding. On v100, it takes ~8.8ms for bertbase batch size 1 seq 128 with mask set while only takes ~4.5ms without mask.   Versions ollecting environment information... PyTorch version: 1.13.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01031azurex86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla V100PCIE16GB Nvidia driver version: 510.108.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          6 Online CPU(s) list:             05 Thread(s) per core:              1 Core(s) per socket:              6 Socket(s):                       1 NUMA node(s)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,nn.TransformerEncoderLayer fastpath (BetterTransformer) is much slower with src_key_padding_mask," ğŸ› Describe the bug TransformerEncoder runs much slower with src_key_padding_mask than without any padding. On v100, it takes ~8.8ms for bertbase batch size 1 seq 128 with mask set while only takes ~4.5ms without mask.   Versions ollecting environment information... PyTorch version: 1.13.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.01031azurex86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla V100PCIE16GB Nvidia driver version: 510.108.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True CPU: Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          6 Online CPU(s) list:             05 Thread(s) per core:              1 Core(s) per socket:              6 Socket(s):                       1 NUMA node(s",2023-02-08T20:00:39Z,module: nn triaged,open,0,2,https://github.com/pytorch/pytorch/issues/94428,"Trying to reproduce the issue on cpu as well, I'll post updates.","> Trying to reproduce the issue on cpu as well, I'll post updates. unable to reproduce on cpu"
556,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BE]: Apply pyupgrade yield from and unit test alias upgrades)ï¼Œ å†…å®¹æ˜¯ (Applies some more harmless pyupgrades. This one gets rid of deprecated aliases in unit_tests and more upgrades yield for loops into yield from generators which are more performance and propagates more information / exceptions from original generator. This is the modern recommended way of forwarding generators.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[BE]: Apply pyupgrade yield from and unit test alias upgrades,Applies some more harmless pyupgrades. This one gets rid of deprecated aliases in unit_tests and more upgrades yield for loops into yield from generators which are more performance and propagates more information / exceptions from original generator. This is the modern recommended way of forwarding generators.,2023-02-07T15:56:32Z,open source better-engineering Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/94309, merge g, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
384,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Factor out SYMPY_INTERP)ï¼Œ å†…å®¹æ˜¯ (  CC(Switch is_contiguous to use guardless implementation)  CC(Track and record hint on SymNode and use when possible)  CC(Factor out SYMPY_INTERP) Signedoffby: Edward Z. Yang  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Factor out SYMPY_INTERP,  CC(Switch is_contiguous to use guardless implementation)  CC(Track and record hint on SymNode and use when possible)  CC(Factor out SYMPY_INTERP) Signedoffby: Edward Z. Yang  ,2023-02-07T15:22:24Z,Merged release notes: fx topic: not user facing module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/94307," merge f ""flaky master job"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",Oh PR is closed nvm
305,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Docs] Add pointer to FlashAttention paper)ï¼Œ å†…å®¹æ˜¯ (As discussed with , we're adding pointers to the docs for MHA and Transformers.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[Docs] Add pointer to FlashAttention paper,"As discussed with , we're adding pointers to the docs for MHA and Transformers.",2023-02-07T01:08:22Z,open source Merged ciflow/trunk,closed,0,3,https://github.com/pytorch/pytorch/issues/94253,The committers listed above are authorized under a signed CLA.:white_check_mark: login: tridao / name: Tri Dao  (0c1c8b99d4f3d1c74899c7a61a35ab43e8dc3e53), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
375,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix Storage destruction GC tracking)ï¼Œ å†…å®¹æ˜¯ (  CC(Test enabling full testing on 3.11 for linux)  CC(Fix testing now that random.sample() arg must be a sequence)  CC(Fix Storage destruction GC tracking))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix Storage destruction GC tracking,  CC(Test enabling full testing on 3.11 for linux)  CC(Fix testing now that random.sample() arg must be a sequence)  CC(Fix Storage destruction GC tracking),2023-02-03T17:01:56Z,Merged ciflow/trunk topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/94051, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: 1 mandatory check(s) failed (Rule `Core Reviewers`).  The first few are:  Lint / lintrunner Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job 
350,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([inductor] bug fix: use `create_symbolic_sizes_strides_storage_offset`)ï¼Œ å†…å®¹æ˜¯ (Stack from ghstack:  CC([inductor] bug fix: use `create_symbolic_sizes_strides_storage_offset`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[inductor] bug fix: use `create_symbolic_sizes_strides_storage_offset`,Stack from ghstack:  CC([inductor] bug fix: use `create_symbolic_sizes_strides_storage_offset`) ,2023-02-03T06:43:31Z,open source Merged ciflow/trunk topic: not user facing module: inductor ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/94031, merge, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
590,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ROCm runners hang during setup-rocm step)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Recently I have seen some `linuxfocalrocm5.4.2py3.8` hang in trunk during the setuprocm step, for example, https://github.com/pytorch/pytorch/actions/runs/4078634797/jobs/7029412580.  The log when it stops is below. AFAICT, most recent occurrences were all on `workerrocmamd74`, so may be a bad runner?   Versions ROCm CI runners /pytorchdevinfra)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",agent,ROCm runners hang during setup-rocm step," ğŸ› Describe the bug Recently I have seen some `linuxfocalrocm5.4.2py3.8` hang in trunk during the setuprocm step, for example, https://github.com/pytorch/pytorch/actions/runs/4078634797/jobs/7029412580.  The log when it stops is below. AFAICT, most recent occurrences were all on `workerrocmamd74`, so may be a bad runner?   Versions ROCm CI runners /pytorchdevinfra",2023-02-03T03:26:01Z,module: rocm module: ci triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/94026,Closing since this is no longer observed.
1081,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_average_parameters (__main__.TestDistBackendWithSpawn))ï¼Œ å†…å®¹æ˜¯ (Platforms: rocm `Running distributed tests for the gloo backend with env init_method` https://github.com/pytorch/pytorch/pull/92932 enabled ROCm distributed CI and had the above test passing: https://github.com/pytorch/pytorch/actions/runs/4069762385/jobs/7010060068step:9:14805 However, the periodic jobs on trunk started showing failures for this test: https://github.com/pytorch/pytorch/actions/runs/4071357852/jobs/7013502890step:9:5412 https://github.com/pytorch/pytorch/actions/runs/4072747817/jobs/7016182070step:9:4970 Failure reason seems to be `free(): double free detected in tcache 2`  [ ] Need to figure out why this failure didn't show up in the runs for PR CC([ROCm] Fix distributed tests failure and enable ROCm distributed CI)  [ ] Need to figure out if this test runs and passes in CUDA CI runs, and if so, why )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,DISABLED test_average_parameters (__main__.TestDistBackendWithSpawn),"Platforms: rocm `Running distributed tests for the gloo backend with env init_method` https://github.com/pytorch/pytorch/pull/92932 enabled ROCm distributed CI and had the above test passing: https://github.com/pytorch/pytorch/actions/runs/4069762385/jobs/7010060068step:9:14805 However, the periodic jobs on trunk started showing failures for this test: https://github.com/pytorch/pytorch/actions/runs/4071357852/jobs/7013502890step:9:5412 https://github.com/pytorch/pytorch/actions/runs/4072747817/jobs/7016182070step:9:4970 Failure reason seems to be `free(): double free detected in tcache 2`  [ ] Need to figure out why this failure didn't show up in the runs for PR CC([ROCm] Fix distributed tests failure and enable ROCm distributed CI)  [ ] Need to figure out if this test runs and passes in CUDA CI runs, and if so, why ",2023-02-02T12:27:48Z,oncall: distributed module: rocm triaged skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/93929,"? Please keep in mind that the test failure only seems to be occurring for the gloo backend with env init_method, which might be relevant for reproducing the failure."
453,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([BUG] jit.trace not working for torchvision ViT models)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug torch.jit.trace is not working for torchvision vision transformer models, i.e. vit_b_16, vit_b_32, and vit_l_16, vit_l_32. It made a TracingCheckError.  Versions pytorch 1.13.0 torchvision v0.14 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[BUG] jit.trace not working for torchvision ViT models," ğŸ› Describe the bug torch.jit.trace is not working for torchvision vision transformer models, i.e. vit_b_16, vit_b_32, and vit_l_16, vit_l_32. It made a TracingCheckError.  Versions pytorch 1.13.0 torchvision v0.14 ",2023-02-02T06:44:03Z,oncall: jit,open,0,1,https://github.com/pytorch/pytorch/issues/93913,"Hi, Any update on this issue? I still get `TracingCheckError: Tracing failed sanity checks!` when running torch.jit.trace on VIT models. **Versions:** pytorch: v2.1 torchvision: v0.16.0 "
1336,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([dynamo]: Unsupported: call_method ListVariable() copy [] {})ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Using nightlies (and triton from `HEAD`, though unlikely that matters here), I am unable to use `torch.compile(model, fullgraph=True, dynamic=False)` with SWinv2 provided by `torchvision` due to this line: https://github.com/pytorch/vision/blob/b094075cbc8834d63a9fa8ae08bcad3d72a43321/torchvision/models/swin_transformer.pyL156  It makes a local copy of a list of integers passed by reference to avoid mutations in the method affecting the original. Excerpt from traceback:  This could be fixed by rewriting the `.copy` as a list comprehension (`shift_size = [n for n in shift_size]`), which I have done locally to ensure that fixes the issue. (And so this would belong in Torchvision's issue tracker.) However, the `copy` method is commonplace and more concise than a list comprehension which does effectively the same thing. What would the level of effort be to add support for `copy` to Dynamo, even if only for primitive types? (I'm not sure what would be involved in supporting `copy` with arbitrary objects.)  Error logs     Minified repro _No response_  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",llm,[dynamo]: Unsupported: call_method ListVariable() copy [] {}," ğŸ› Describe the bug Using nightlies (and triton from `HEAD`, though unlikely that matters here), I am unable to use `torch.compile(model, fullgraph=True, dynamic=False)` with SWinv2 provided by `torchvision` due to this line: https://github.com/pytorch/vision/blob/b094075cbc8834d63a9fa8ae08bcad3d72a43321/torchvision/models/swin_transformer.pyL156  It makes a local copy of a list of integers passed by reference to avoid mutations in the method affecting the original. Excerpt from traceback:  This could be fixed by rewriting the `.copy` as a list comprehension (`shift_size = [n for n in shift_size]`), which I have done locally to ensure that fixes the issue. (And so this would belong in Torchvision's issue tracker.) However, the `copy` method is commonplace and more concise than a list comprehension which does effectively the same thing. What would the level of effort be to add support for `copy` to Dynamo, even if only for primitive types? (I'm not sure what would be involved in supporting `copy` with arbitrary objects.)  Error logs     Minified repro _No response_  Versions  ",2023-02-02T04:59:48Z,triaged oncall: pt2 module: dynamo,closed,0,4,https://github.com/pytorch/pytorch/issues/93906,"I think we can reuse  to implement it, I'll send a PR.","Hi  if you have not made progress on it, could I try this?  Just want to get more familiar with dynamo code.","Sure, let me assign this to you. Thanks for helping! BTW, there are a bunch of bugs listed at  CC(14k github models TorchDynamo + TorchInductor bugs umbrella task), feel free to pick and fix them. Please put your name after the failure to let others know you are working on it, in case duplicated work. "," passes today, closing as fixed."
698,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault when hipifying code.)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am trying to hipify code from flashattention, which has a dependency of cutlass. I adapt the setup.py and when hipifying cutlass, segmentation fault happens. I debugged and the main reason is that there are some .h files that include each other. For example, simd.h and simd_sm60.h. Because the preprocessor will try to process the files that it includes and itself was not yet hipified successfully. This results in infinite loop.  Versions torch 1.10 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Segmentation fault when hipifying code.," ğŸ› Describe the bug I am trying to hipify code from flashattention, which has a dependency of cutlass. I adapt the setup.py and when hipifying cutlass, segmentation fault happens. I debugged and the main reason is that there are some .h files that include each other. For example, simd.h and simd_sm60.h. Because the preprocessor will try to process the files that it includes and itself was not yet hipified successfully. This results in infinite loop.  Versions torch 1.10 ",2023-02-01T06:57:01Z,needs reproduction module: rocm triaged,closed,0,8,https://github.com/pytorch/pytorch/issues/93827," does it reproduce using PyTorch1.13.1. Also, issue can you please post some sort of logs (even partial) to get a better sense at what is going on?","  Sorry I don't have a rocm pytorch 1.13 env here.  Log `File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 792, in preprocessor     output_source = RE_QUOTE_HEADER.sub(mk_repl('include ""{0}""', True), output_source)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 785, in repl     clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 177, in preprocess_file_and_save_result     hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)   File ""/root/anaconda3/envs/py37/lib/python3.7/sitepackages/torch/utils/hipify/hipify_python.py"", line 722, in preprocessor     output_source = fin.read() RecursionError: maximum recursion depth exceeded while calling a Python object` and I add this to the code: ` sys.setrecursionlimit(1000000) ` And I got this: `   torch.__version__  = 1.10.0   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/cmake/nop.cu > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/cmake/nop.hip skipped   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/cmake/version.h.in > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/cmake/version.h.in ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/array.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/array.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/blas3.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/blas3.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/complex.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/complex_hip.h skipped   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/constants.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/constants.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/coord.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/coord.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/core_io.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/core_io_hip.h skipped   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/cutlass.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/cutlass.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/device_kernel_hip.h skipped   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/fast_math.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/fast_math.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/functional.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/functional.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/half.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/half_hip.h skipped   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/matrix.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/matrix.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/quaternion.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/quaternion.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/real.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/real.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/semaphore.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/semaphore_hip.h skipped   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/trace.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/trace.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/uint128.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/uint128.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/complex_hip.h > None ignored   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/core_io_hip.h > None ignored   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/device_kernel_hip.h > None ignored   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/half_hip.h > None ignored   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/semaphore_hip.h > None ignored   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60_hip.h skipped   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h ok   /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h > /home/thanks3q/dcu_pkg/flashattention/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h ok   Segmentation fault (core dumped) `","I can confirm this and get past the issue by skipping headers that trigger cyclic dependency:  output:  So `preprocessor` is not caching already visited files and is getting lost in the cyclic dependency for these headers on the `cutlass` project. Related issues: https://github.com/ROCmDeveloperTools/HIPIFY/issues/660 https://github.com/facebookresearch/xformers/issues/485 https://github.com/ROCmSoftwarePlatform/hipify_torch/issues/47 https://github.com/ROCmSoftwarePlatform/hipify_torch/issues/39 Not sure if this report should be here or at https://github.com/ROCmSoftwarePlatform/hipify_torch, probably there, but that project issues doesn't seem to be moving forward, which is very sad."," When you say you're able to get past the issue, does that mean you're able to compile?",", Where did you add these lines? somewhere in hipify_python.py? ", we fixed the circular recursion issue in an upstream PyTorch PR:  CC(Segmentation fault when hipifying code.) .... the latest PyTorch nightly wheels should contain this fix in hipify. Our internal testing with flash_attention repo seems to indicate this issue is resolved. Can you please verify on your end? NOTE: We will not be able to backport this fix into PyTorch stable wheels.,AMD has an fork of this repository where we reimplemented the CUTLASS portions in Composable Kernels.  The hipify issue should be addressed as indicated but take a look at this repo for Flash Attention on ROCm: https://github.com/ROCmSoftwarePlatform/flashattention/tree/flash_attention_for_rocm,Thank you all! Closing now.
1992,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensorboard SummaryWriter with cloud storage does not work on Mac)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I have been trying to use google cloud storage with the Tensorboard logger in pytorch lightning, but it seems to do incomplete writes. I created an example using SummaryWriter that produces the same results. I have been able to confirm this is working fine on linux. I am aware that underlying this may not be pytorch issue, but haven't been able to isolate the tensorboard logic, nor have I been able to setup tensorflow on mac to test this. I would appreciate some help with isolating the issue further if the issue is in tensorboard or another library. Dependencies: `pip install tensorboard torch gcsfs numpy`  Open tensorboard: `tensorboard logdir gs://testbucket124/summary_writer_test/` Result:  Expected:   Versions Collecting environment information... PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.0.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: Could not collect Libc version: N/A Python version: 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang1400.0.29.202)] (64bit runtime) Python platform: macOS13.0.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] torch==1.13.1 [conda] efficientnetpytorch      0.7.1                    pypi_0    pypi [conda] numpy                     1.22.4                   pypi_0    pypi [conda] numpybase                1.21.5           py39hadd41eb_3 [conda] )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Tensorboard SummaryWriter with cloud storage does not work on Mac," ğŸ› Describe the bug I have been trying to use google cloud storage with the Tensorboard logger in pytorch lightning, but it seems to do incomplete writes. I created an example using SummaryWriter that produces the same results. I have been able to confirm this is working fine on linux. I am aware that underlying this may not be pytorch issue, but haven't been able to isolate the tensorboard logic, nor have I been able to setup tensorflow on mac to test this. I would appreciate some help with isolating the issue further if the issue is in tensorboard or another library. Dependencies: `pip install tensorboard torch gcsfs numpy`  Open tensorboard: `tensorboard logdir gs://testbucket124/summary_writer_test/` Result:  Expected:   Versions Collecting environment information... PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.0.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: Could not collect Libc version: N/A Python version: 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang1400.0.29.202)] (64bit runtime) Python platform: macOS13.0.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.24.1 [pip3] torch==1.13.1 [conda] efficientnetpytorch      0.7.1                    pypi_0    pypi [conda] numpy                     1.22.4                   pypi_0    pypi [conda] numpybase                1.21.5           py39hadd41eb_3 [conda] ",2023-01-31T10:09:23Z,oncall: visualization,open,0,0,https://github.com/pytorch/pytorch/issues/93350
1966,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Quantized Transformer ONNX Export Fails)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am trying to export the torchnative PTQ model using the timm repo. I am working on exporting the deit_tiny_patch_16 model. The model conversion is however failing with runtime error:  I am not aware on how to debug this specific type of issue, tried changing the specific layer to suppress if the error is because of that layer, however, I start getting the error in some other layer. For reproducing, we just need to export the quantized timm network, timm can be directly cloned, and the torchnative quantization can be used from https://pytorch.org/blog/quantizationinpractice/ . Basically,   Versions Collecting environment information... PyTorch version: 1.12.1 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.13 (default, Oct 21 2022, 23:50:54)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0136genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: NVIDIA Graphics Device GPU 1: NVIDIA Graphics Device GPU 2: NVIDIA Graphics Device Nvidia driver version: 465.19.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.12.1 [pip3] torchaudio==0.12.1 [pip3] torchvision==0.13.1 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] ffmpeg                 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Quantized Transformer ONNX Export Fails," ğŸ› Describe the bug I am trying to export the torchnative PTQ model using the timm repo. I am working on exporting the deit_tiny_patch_16 model. The model conversion is however failing with runtime error:  I am not aware on how to debug this specific type of issue, tried changing the specific layer to suppress if the error is because of that layer, however, I start getting the error in some other layer. For reproducing, we just need to export the quantized timm network, timm can be directly cloned, and the torchnative quantization can be used from https://pytorch.org/blog/quantizationinpractice/ . Basically,   Versions Collecting environment information... PyTorch version: 1.12.1 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.13 (default, Oct 21 2022, 23:50:54)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0136genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: NVIDIA Graphics Device GPU 1: NVIDIA Graphics Device GPU 2: NVIDIA Graphics Device Nvidia driver version: 465.19.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.12.1 [pip3] torchaudio==0.12.1 [pip3] torchvision==0.13.1 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] ffmpeg                 ",2023-01-31T09:29:00Z,module: onnx oncall: quantization low priority triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/93346,"same question,transformer model (Vit)cannot be exported as onnx","> same question,transformer model (Vit)cannot be exported as onnx I am just facing this problem with the quantized model, however the original works fine.","The same issue , there are many problems when converting quantized ViT to ONNX. Are there have any solutions, or will torch 2.0 fix this issue?",`torch.onnx.export` is in maintenance mode and we don't plan to add new operators/features or fix complex issues. Please try the new ONNX exporter with the latest pt2 quantization API and reopen this issue with a full repro if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial
630,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add PyObjectSlot member to StorageImpl)ï¼Œ å†…å®¹æ˜¯ (  CC(Add PyObjectSlot member to StorageImpl) Part of CC(PyObject preservation and resurrection for `StorageImpl`) Also modifies how `StorageImpl`s are stored in JIT static runtime's `MemoryPlanner`, which used to `std::move` `StorageImpl`s into a vector. But `StorageImpl` can no longer be moved. Instead, `MemoryPlanner` now contains a malloced buffer to which we add new `StorageImpl`s using placement new )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add PyObjectSlot member to StorageImpl,"  CC(Add PyObjectSlot member to StorageImpl) Part of CC(PyObject preservation and resurrection for `StorageImpl`) Also modifies how `StorageImpl`s are stored in JIT static runtime's `MemoryPlanner`, which used to `std::move` `StorageImpl`s into a vector. But `StorageImpl` can no longer be moved. Instead, `MemoryPlanner` now contains a malloced buffer to which we add new `StorageImpl`s using placement new ",2023-01-31T06:35:50Z,module: internals open source Merged ciflow/trunk release notes: jit topic: not user facing ciflow/periodic,closed,0,18,https://github.com/pytorch/pytorch/issues/93342,"I'm trying to build the benchmarks in `benchmarks/static_runtime` to reproduce the CI errors. There don't seem to be any build instructions, and this doesn't work for me:  It generates some cmake files, but doesn't build any of the binaries","It doesn't look like you have to build it separately. It should be built in build/bin/static_runtime_test if you BUILD_TEST=1 when building PyTorch. But let's talk about static runtime for a moment. They seem to really want the StorageImpl to be laid out contiguously without requiring an extra indirection (read the comment above). This is going to be difficult to arrange for because I told you to make StorageImpl nonmovable. If they never resize the vector, we can reimplement this using a malloc'd buffer which we placementnew the StorageImpls into. Can you check if they do resize? If they don't, this may be cause to work out how to keep StorageImpl movable.",", the vector in the static runtime memory planner is never resized and it looks like a malloced buffer would work. So should we make StorageImpl movable? EDIT: Actually, maybe I should first try to replace the vector with a malloced buffer to make sure it works","I've confirmed that the `managed_tensor_storage_impls_` vector's capacity never changes after the first storage is put into it. I first tested this by keeping an expected capacity variable and asserting that the capacity of the vector is always equal to it after the `emplace_back` call. I ran the `static_runtime_test` and `static_runtime_bench`, and they didn't throw an error, so the vector's capacity must be staying unchanged. That was probably enough, but I also tried replacing the `vector>>` with two heapallocated arrays just to make sure, and that worked fine too. So should we go forward with enabling `StorageImpl` to be moved?","Nope, since the capacity never changes, you can do the mallocinplace initialize trick.","Ok I misunderstood. So are you suggesting to basically copy the `StorageImpl`s into a buffer in the `MemoryPlanner` using placement new? I don't think copying the `PyObjectSlot` would work after we have full PyObject preservation for storages because whichever `StorageImpl` gets destructed first will deallocate the `PyObject`. Maybe it's possible to make it work if we can keep track of how many `StorageImpl`s hold onto the same PyObjectpotentially doable just with another `Py_INCREF` call. But on the other hand, maybe the copies of the `StorageImpl`s in `MemoryPlanner` can just have an uninitialized `PyObjectSlot`. It seems like the static runtime will not need the PyObject, but I'm not completely sure","I wouldn't use the word ""copy"" since that has a specific technical meaning in C++, but yes, we can construct a ""vector"" of StorageImpls that are contiguous by mallocing a buffer and then placementnew'ing each slot. In the case of static runtime, I think the PyObject slots are always null (because this is a C++ only concept), so you don't have to worry about that interaction.","Thanks for the review, ! Let me know if there's anything else","Hey, this looks great ! Would you mind landing it? Edit: I guess this only does part of  CC(PyObject preservation and resurrection for `StorageImpl`), so nvm.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. You can rebase by leaving the following comment on this PR: ` rebase` Details for Dev Infra team Raised by workflow job , rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/kurtamohler/4/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4311551495,I'm on PTO from now to March 7. I'll fix the conflict and land when I'm back," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
764,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`torch.compile()` failed on Huggingface Flan-T5 `torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(forward) [] OrderedDict()`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Attempting to compile FlanT5 (and other `AutoModelForSeq2SeqLM` models) from Huggingface.  Possibly related to https://github.com/huggingface/transformers/issues/21013, and https://devdiscuss.pytorch.org/t/torchdynamoupdate3gpuinferenceedition/460/12?u=kastan Minimal reproduction:   Error logs   Minified repro Unable to produce due to same error as above `torch._dynamo.exc.InternalTorchDynamoError`.  Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`torch.compile()` failed on Huggingface Flan-T5 `torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(forward) [] OrderedDict()`," ğŸ› Describe the bug Attempting to compile FlanT5 (and other `AutoModelForSeq2SeqLM` models) from Huggingface.  Possibly related to https://github.com/huggingface/transformers/issues/21013, and https://devdiscuss.pytorch.org/t/torchdynamoupdate3gpuinferenceedition/460/12?u=kastan Minimal reproduction:   Error logs   Minified repro Unable to produce due to same error as above `torch._dynamo.exc.InternalTorchDynamoError`.  Versions  ",2023-01-30T21:48:59Z,triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/93307,"  I tried your repro but found it can't pass eager mode as well. Can you double check? Per , I think this is a regression that I'm fixing it via CC([Dynamo] Fix bug if module calls module with static forward function). After it passed eager mode, if it's still error from , feel free to let me know.","Has this problem been solved? I have met the error, too.",Repro doesn't fail on main  closing for now. Reopen if there's still an issue on main.
426,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([fx.GraphModule] Populate memo in deepcopy BEFORE copying children.)ï¼Œ å†…å®¹æ˜¯ (Summary: Apparently if not then at somepoint, we might lose fields if the submodules have circular reference Test Plan: Reviewers: Subscribers: Tasks: Tags: Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[fx.GraphModule] Populate memo in deepcopy BEFORE copying children.,"Summary: Apparently if not then at somepoint, we might lose fields if the submodules have circular reference Test Plan: Reviewers: Subscribers: Tasks: Tags: Fixes ISSUE_NUMBER",2023-01-30T20:00:23Z,Merged ciflow/trunk release notes: fx,closed,0,2,https://github.com/pytorch/pytorch/issues/93295, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1262,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`torch.set_num_threads` cannot accelerate the inference performance with increasing the number of cpu cores)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I want to increase the inference performance of pytorch model by increase the number of cpu cores. My machine has totally 72 CPU cores, but the model performance got worse when the cpu number increase by set `torch.set_num_threads`. I got the same result with 2 model, one is Restnet50, another is a NLP transformer model. * This is the performance curve with cpu cores: !cpuresnet * This is the performance curve with cpu cores: !cpunlp  Versions PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Versions of relevant libraries: [pip3] intelextensionforpytorch==1.11.0+cpu [pip3] numpy==1.22.4 [pip3] torch==1.11.0+cu113 [pip3] torchaudio==0.11.0+cu113 [pip3] torchvision==0.12.0+cu113 [conda] Could not collect)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,`torch.set_num_threads` cannot accelerate the inference performance with increasing the number of cpu cores," ğŸ› Describe the bug I want to increase the inference performance of pytorch model by increase the number of cpu cores. My machine has totally 72 CPU cores, but the model performance got worse when the cpu number increase by set `torch.set_num_threads`. I got the same result with 2 model, one is Restnet50, another is a NLP transformer model. * This is the performance curve with cpu cores: !cpuresnet * This is the performance curve with cpu cores: !cpunlp  Versions PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Versions of relevant libraries: [pip3] intelextensionforpytorch==1.11.0+cpu [pip3] numpy==1.22.4 [pip3] torch==1.11.0+cu113 [pip3] torchaudio==0.11.0+cu113 [pip3] torchvision==0.12.0+cu113 [conda] Could not collect",2023-01-30T03:05:30Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/93247,"Hi  , few things you may want to try to finding the optimal number of threads on a given system. Keep in mind that simply increasing the number of threads can cause oversubscription.  (1) if your system has multiple sockets (can check with `lscpu` command), try setting the thread affinity to a specific socket (e.g., socket 0) to avoid overhead from crosssocket thread migration, remote memory accesses, etc (2) if your system has hyperthreading enabled (can check with `lscpu` command), try setting the thread affinity to physical cores only. The operators in ResNet50 like GEMM, conv, are computeintensive ops, which generally benefit from using nonhyperthreading (the overhead from sharing the same core resource for these heavy ops is generally greater than the gain from running both threads at the same time).  While tuning for the optimal number of threads requires some experimentation, I've generally set the thread affinity and number of threads to the number of physical cores on a single socket as a good starting ground.  You may also be interested in this tutorial https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.htmltuningthenumberofthreads .",Please also take a reference to https://intel.github.io/intelextensionforpytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html We also have integrated a launch script to simplify the execution. Please take a reference to https://github.com/pytorch/pytorch/blob/master/torch/backends/xeon/run_cpu.py,"Hi, We use github issues only for bug reports or feature requests. Please use the forum to ask questions: https://discuss.pytorch.org/",>  Thank you very much. I will try your sugguestion.
1968,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(PT2 doesn't work well with inference mode)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Code  Error   Versions Collecting environment information... PyTorch version: 2.0.0.dev20230124 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.27 Python version: 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.14.301224.520.amzn2.x86_64x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==2.0.0.dev20230124 [pip3] torchaudio==2.0.0.dev20230124 [pip3] torchdata==0.6.0.dev20230124 [pip3] torchelastic==0.2.2 [pip3] torchtext==0.15.0.dev20230124 [pip3] torchvision==0.15.0.dev20230124 [conda] blas                      1.0                         mkl   [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0           py310h7f8727e_0   [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0   [conda] mkl_random                1.2.2           py310h00e6091_0   [conda] numpy                     1.23.5          py310hd5efca6_0   [conda] numpybase                1.23.5          py310h8e6c178_0   [conda] pytorch                   2.0.0.dev20230124 py3.10_cuda11.6_cudnn8.3.2_0    pytorchnightly [conda] pytorchcuda              11.6                 h867d48c_2    pytorchnightly [conda] pytorchmutex    )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,PT2 doesn't work well with inference mode," ğŸ› Describe the bug Code  Error   Versions Collecting environment information... PyTorch version: 2.0.0.dev20230124 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.27 Python version: 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0] (64bit runtime) Python platform: Linux4.14.301224.520.amzn2.x86_64x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==2.0.0.dev20230124 [pip3] torchaudio==2.0.0.dev20230124 [pip3] torchdata==0.6.0.dev20230124 [pip3] torchelastic==0.2.2 [pip3] torchtext==0.15.0.dev20230124 [pip3] torchvision==0.15.0.dev20230124 [conda] blas                      1.0                         mkl   [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0           py310h7f8727e_0   [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0   [conda] mkl_random                1.2.2           py310h00e6091_0   [conda] numpy                     1.23.5          py310hd5efca6_0   [conda] numpybase                1.23.5          py310h8e6c178_0   [conda] pytorch                   2.0.0.dev20230124 py3.10_cuda11.6_cudnn8.3.2_0    pytorchnightly [conda] pytorchcuda              11.6                 h867d48c_2    pytorchnightly [conda] pytorchmutex    ",2023-01-26T04:20:34Z,triaged inference mode oncall: pt2 module: dynamo,closed,0,7,https://github.com/pytorch/pytorch/issues/93042,"Thanks  , after some tinkering with `transformers` package versions, I found out that the above error is with `transformers==4.26.0`.  If I use `transformers==4.22.2`, it would work and the resulting optimized `generate2` function is much faster than vanilla pytorch without dynamo. Although during the compilation process, a bunch of logs get printed out as below. Hope this new piece of information is helpful. ","Able to reproduce the ""RuntimeError: Inference tensors do not track version counter."" error","The ""simple"" fix is to not use inference mode in your test script; inference mode isn't buying you much in the PT2 world as it is primarily designed to eliminate eager mode overhead (but you've got a compiler now)","My guess is that there are several other things that will be broken, but that specific error has a targeted fix. During functionalization when we create `FunctionalTensorWrapper` objects, if the inner tensor has no version counter (because it was created in inference mode), we need the wrapper to respect that: https://github.com/pytorch/pytorch/blob/2a31c3589bb62ebaac3b3ec30c455668743f1893/aten/src/ATen/FunctionalTensorWrapper.cppL21","Hey wish and PyTorch folks ğŸ‘‹ The exception in the original post is solved on the `transformers` side, in the PR linked above (this one). This means that we no longer need to downgrade `transformers` to compile `.generate()`.",Thanks  !,Thanks for the fix !
496,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([vulkan] Enable command buffer reuse and add keys to Tensor/StorageBuffer objects)ï¼Œ å†…å®¹æ˜¯ (  CC([vulkan] Add core graph components)  CC([vulkan] Enable command buffer reuse and add keys to Tensor/StorageBuffer objects)  CC([vulkan] Create separate BUCK target for command buffer recording) Differential Revision: D42614180)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[vulkan] Enable command buffer reuse and add keys to Tensor/StorageBuffer objects,  CC([vulkan] Add core graph components)  CC([vulkan] Enable command buffer reuse and add keys to Tensor/StorageBuffer objects)  CC([vulkan] Create separate BUCK target for command buffer recording) Differential Revision: D42614180,2023-01-25T18:52:27Z,Merged ciflow/trunk release notes: vulkan,closed,0,2,https://github.com/pytorch/pytorch/issues/92993, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
934,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Feature request: access to variable)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch In `torch.nn.functional.multi_head_attention_forward`, there is a variable, `use_separate_proj_weight` that can not be controled by the users of Transformers. I would have liked to use NONshared weights. In `torch.nn.modules.transformers.TransformerEncoderLayer.self_attn`, during the forward, use_separate_proj_weight is set to False only in the case where the dimensions for Q, K, V are not the same.  This is verified during initialisation of self_attn. But user can't control this if dimensions are the same. (same with the decoder) Could you add an option to control if weights are shared? Thank you very much!  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Feature request: access to variable," ğŸš€ The feature, motivation and pitch In `torch.nn.functional.multi_head_attention_forward`, there is a variable, `use_separate_proj_weight` that can not be controled by the users of Transformers. I would have liked to use NONshared weights. In `torch.nn.modules.transformers.TransformerEncoderLayer.self_attn`, during the forward, use_separate_proj_weight is set to False only in the case where the dimensions for Q, K, V are not the same.  This is verified during initialisation of self_attn. But user can't control this if dimensions are the same. (same with the decoder) Could you add an option to control if weights are shared? Thank you very much!  Alternatives _No response_  Additional context _No response_ ",2023-01-25T18:27:27Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/92990
1512,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault when replaying an uninitialized CUDA graph)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Running the code below causes segmentation fault. It would be great if a warning was emitted instead for easier debugging.   Versions PyTorch version: 2.0.0.dev20230119+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.9.16 (main, Jan 11 2023, 16:05:54)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01015awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB Nvidia driver version: 510.73.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.24.1 [pip3] pytorchtriton==2.0.0+0d7e753227 [pip3] torch==2.0.0.dev20230119+cu117 [conda] numpy                     1.24.1                   pypi_0    pypi [conda] pytorchtriton            2.0.0+0d7e753227          pypi_0    pypi [conda] torch                     2.0.0.dev20230119+cu117          pypi_0    pypi )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Segmentation fault when replaying an uninitialized CUDA graph," ğŸ› Describe the bug Running the code below causes segmentation fault. It would be great if a warning was emitted instead for easier debugging.   Versions PyTorch version: 2.0.0.dev20230119+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.9.16 (main, Jan 11 2023, 16:05:54)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.01015awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB Nvidia driver version: 510.73.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.24.1 [pip3] pytorchtriton==2.0.0+0d7e753227 [pip3] torch==2.0.0.dev20230119+cu117 [conda] numpy                     1.24.1                   pypi_0    pypi [conda] pytorchtriton            2.0.0+0d7e753227          pypi_0    pypi [conda] torch                     2.0.0.dev20230119+cu117          pypi_0    pypi ",2023-01-25T13:58:45Z,triaged module: cuda graphs,closed,0,1,https://github.com/pytorch/pytorch/issues/92978,I believe this should be fixed in CC(make sure that our error handling runs with the GIL enabled). verified locally that the error should be 
2006,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Running Attention with batch size and sequence length of one is very slow (CPU))ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The model is a standard transformer. I wanted to implement caching in the decoder so that autoregressive decoding in inference time becomes faster. Inference is always run with batch size of 1. I noticed that considering just the last token for decoding is approx 10 times slower than using the full sequence. This entire experiment was done on the CPU. Here is a trimmed down version of the code that is representative of the slowdown:  Here are the average execution times (across 50 runs of the above script) for difference sequence lengths (`seqlen`).  As you can see from the above results, sequence length of 1 (for the query tensor) is more than 10 times slower than the same run for longer sequence lengths. Not sure why this is the case. Any insight into this would be much appreciated.  Versions PyTorch version: 1.12.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0135genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: NVIDIA A100PCIE40GB GPU 1: NVIDIA A100PCIE40GB GPU 2: NVIDIA A100PCIE40GB GPU 3: NVIDIA A100PCIE40GB Nvidia driver version: 515.86.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] pytorchwpe==0.0.1 [pip3] torch==1.12.1+cu116 [pip3] torchcomplex)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Running Attention with batch size and sequence length of one is very slow (CPU)," ğŸ› Describe the bug The model is a standard transformer. I wanted to implement caching in the decoder so that autoregressive decoding in inference time becomes faster. Inference is always run with batch size of 1. I noticed that considering just the last token for decoding is approx 10 times slower than using the full sequence. This entire experiment was done on the CPU. Here is a trimmed down version of the code that is representative of the slowdown:  Here are the average execution times (across 50 runs of the above script) for difference sequence lengths (`seqlen`).  As you can see from the above results, sequence length of 1 (for the query tensor) is more than 10 times slower than the same run for longer sequence lengths. Not sure why this is the case. Any insight into this would be much appreciated.  Versions PyTorch version: 1.12.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0135genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: NVIDIA A100PCIE40GB GPU 1: NVIDIA A100PCIE40GB GPU 2: NVIDIA A100PCIE40GB GPU 3: NVIDIA A100PCIE40GB Nvidia driver version: 515.86.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] pytorchwpe==0.0.1 [pip3] torch==1.12.1+cu116 [pip3] torchcomplex",2023-01-24T05:29:33Z,,closed,1,2,https://github.com/pytorch/pytorch/issues/92892,Ran the profiler for various seqlen's for 100 iterations. Profiler data below shows aten:bmm call is considerably slower for seqlen 1. Need to inspect torch.bmm() in torch/nn/functional.py: multi_head_attention_forward() Seqlen : 1                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls          profile_attention         3.20%     358.399ms       100.00%       11.200s       11.200s             1                  aten::bmm        88.78%        9.943s        88.78%        9.943s       4.143ms          2400               aten::linear         0.34%      38.616ms         5.08%     569.020ms     158.061us          3600                aten::addmm         4.12%     461.575ms         4.38%     489.996ms     136.110us          3600 Seqlen :2                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls          profile_attention        23.15%     466.351ms       100.00%        2.014s        2.014s             1                  aten::bmm        36.09%     727.082ms        36.09%     727.091ms     302.955us          2400               aten::linear         2.54%      51.241ms        16.62%     334.860ms      93.017us          3600                aten::addmm         7.80%     157.087ms         9.91%     199.673ms      55.465us          3600 Seqlen : 10                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls          profile_attention        15.61%     282.043ms       100.00%        1.807s        1.807s             1                  aten::bmm        57.63%        1.042s        57.63%        1.042s     434.008us          2400               aten::linear         1.58%      28.585ms        11.33%     204.691ms      56.859us          3600                aten::addmm         6.39%     115.423ms         7.66%     138.372ms      38.437us          3600," thanks for the findings! There are a couple of issues in this issue: 1. `bmm` on the CPU path has two paths: given two tensors of shape [B, M, K] and [B, K, N], `baddbmm_cpu_kernel` is used when MNK = 400. So your tests will use both paths. 2. the grain size in `baddbmm_cpu_kernel` is not correct, this will lead to parallel implementation even for very small input shape. (usually we would want to go sequential implementation for small input shape, since it is not worth paralleling it) 3. your benchmark script has too few iterations, need to add more iterations in the benchmark to get reasonable result. (I updates the script with 1000 iters) 4. since the input shape is not very big ( which means the workload would be latency bound), using more CPU cores would not get better results. I have fixed the 2. with this https://github.com/pytorch/pytorch/pull/98297, and the following is results on my machine: (unit: ms per iteration) seq length  4.708 You can see that using 20 cores would have even worse performance. One more thing I want to emphasize is the environment setting, you need to set NUMA on Xeon later than Skylake, for example: "
1990,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RFC] Make more operations inplace (GELU, BatchNorm, LayerNorm))ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch  Make more operations inplace (GELU, BatchNorm, LayerNorm)  **Summary** Hi PyTorch team, We would like to enable users to make the following operations inplace: LayerNorm, BatchNorm and GELU.  **Motivation** Inplace operations can lower the amount of memory used by operators in specific cases with minimal overhead compared to other methods of reducing the memory footprint of models such as activation checkpointing, this can then lead to increased performance in the form of throughput gains through batch size improvements. Inplace GELU and Inplace LayerNorm combined for example, leads to an activation memory footprint savings of over 10% for Huggingface BERT at a sequence length of 512. These benefits and prior art are described in the papers listed in the prior arts. Currently, in the github repositories associated with these papers these Inplace versions of these operators are implemented as  PyTorch modules that must be imported. The value add of this feature would be allowing users to take advantage of these operators directly from PyTorch.  This would have no impact to other users of the library.  **Proposed Implementation** Our proposed implementation is to add an inplace flag to existing modules, which can then be set to True to enable  the inplace version, such as with dropout for example. `torch.nn.Dropout1d(p=0.5, inplace=False)` The existing LayerNorm module, for example, would change from `torch.nn.LayerNorm(normalized_shape, eps=1e05, elementwise_affine=True, device=None, dtype=None)` to  `torch.nn.LayerNorm(normalized_shape, eps=1e05, elementwise_affine=True, inplace=False, device=None, dtype=None)`. Internally, this can then be handled as other inplace operators w)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"[RFC] Make more operations inplace (GELU, BatchNorm, LayerNorm)"," ğŸš€ The feature, motivation and pitch  Make more operations inplace (GELU, BatchNorm, LayerNorm)  **Summary** Hi PyTorch team, We would like to enable users to make the following operations inplace: LayerNorm, BatchNorm and GELU.  **Motivation** Inplace operations can lower the amount of memory used by operators in specific cases with minimal overhead compared to other methods of reducing the memory footprint of models such as activation checkpointing, this can then lead to increased performance in the form of throughput gains through batch size improvements. Inplace GELU and Inplace LayerNorm combined for example, leads to an activation memory footprint savings of over 10% for Huggingface BERT at a sequence length of 512. These benefits and prior art are described in the papers listed in the prior arts. Currently, in the github repositories associated with these papers these Inplace versions of these operators are implemented as  PyTorch modules that must be imported. The value add of this feature would be allowing users to take advantage of these operators directly from PyTorch.  This would have no impact to other users of the library.  **Proposed Implementation** Our proposed implementation is to add an inplace flag to existing modules, which can then be set to True to enable  the inplace version, such as with dropout for example. `torch.nn.Dropout1d(p=0.5, inplace=False)` The existing LayerNorm module, for example, would change from `torch.nn.LayerNorm(normalized_shape, eps=1e05, elementwise_affine=True, device=None, dtype=None)` to  `torch.nn.LayerNorm(normalized_shape, eps=1e05, elementwise_affine=True, inplace=False, device=None, dtype=None)`. Internally, this can then be handled as other inplace operators w",2023-01-24T01:42:21Z,module: nn triaged enhancement actionable,open,1,12,https://github.com/pytorch/pytorch/issues/92884,Related:    CC(Inplace and out arguments for BatchNorm (and other norm layers: InstanceNorm / LayerNorm / GroupNorm ...))   CC([feature request] Core API for invertible/inplace and flow-like ops + memory-saving (hookless?) reversible sequential container for RevNets to allow for much larger batch-sizes in academic setting),Hey! I think one of the reason for not having these is that these functions are used during training and we do need the input value to be able to compute the backward. So making them inplace is not very interesting because we would still need to make a copy of the input so that the backward can run properly.,"Hi  , just to clarify the RFC is in fact *for* training! While you are correct that normally we need the input for the backward, the contribution that is made in the above papers and the point of our RFC is that you donâ€™t actually need to stash the input, you can do the backward using the output alone. So, we can discard the input and save memory during training!"," as you know, these are still useful for implementing inplace reversible nets that are very memoryefficient and allow larger batch sizes :) Also, with checkpointing/recomputation (and sometimes even without it e.g. as in  CC(Inplace fused (leaky)relu+(leaky)dropout for memory savings (I think, can be made fully allocation-less if never fully allocating random mask in FlashAttention style and recover the mask from the output))), these inplace reversible ops can even be fused in forward path.","Sure, but in both cases, you will need a higher level system that is able to figure out that these are ok and use them. In any case, if you need those, we would be happy to review a PR adding them (including properly sharing code with existing impl, doing proper autograd impl) but I don't think anyone on our end will work on this."," that sounds good, thanks! Just a couple of questions/comments before implementing.  I donâ€™t think the higher level system is necessary, thereâ€™s no situation where these layers wouldnâ€™t be ok to use and the user can choose whether to use them or not based on if itâ€™s beneficial to their use case.  Second, which of the implementation alternatives above would you suggest? Do you recommend the implementation making these layers completely separate, (i.e. add a new Module called `InPlaceLayerNorm`) or add them as flags to the existing layers (i.e. add an `inplace=True` flag)?","> I donâ€™t think the higher level system is necessary, thereâ€™s no situation where these layers wouldnâ€™t be ok to use and the user can choose whether to use them or not based on if itâ€™s beneficial to their use case. yes agreed. > Second, which of the implementation alternatives above would you suggest? Do you recommend the implementation making these layers completely separate, (i.e. add a new Module called InPlaceLayerNorm) or add them as flags to the existing layers (i.e. add an inplace=True flag)? The flag `inplace=True` is good for the nn.Module level. For the nn.functional API called by the Module, an inplace: bool flag is good as well. For the torch. API level that is called after that, this one comes directly from the c++ impl (defined in native_functions.yaml). There we need two functions: `layer_norm` (out of place) and `layer_norm_` (inplace). Each should get its own autograd formula, and the impl should route to the same thing in the end.","For reference, here're two related PRs that were not completed:  https://github.com/pytorch/pytorch/pull/61821  https://github.com/pytorch/pytorch/pull/76149",  with torch.compile we're able to fuse many pointwise and reduction operations. Wouldn't this address your needs as well?," we discussed it recently in  CC(Inplace fused (leaky)relu+(leaky)dropout for memory savings (I think, can be made fully allocation-less if never fully allocating random mask in FlashAttention style and recover the mask from the output)), and it seems that for now torch.compile doesn't produce inplace code", I think torch.compile does something better than producing inplace code in your cases.,Probably both fruits can be had which will make it even better! (fully allocationless and fused!) :)
378,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(UnsupportedFakeTensorException: meta converter nyi)ï¼Œ å†…å®¹æ˜¯ (If I initialize a module on the meta device (using 's based `torch.device` contextmanager) and try to export it, things explode:  Stack trace:  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,UnsupportedFakeTensorException: meta converter nyi,"If I initialize a module on the meta device (using 's based `torch.device` contextmanager) and try to export it, things explode:  Stack trace:  ",2023-01-24T00:49:07Z,triaged module: fakeTensor,closed,0,4,https://github.com/pytorch/pytorch/issues/92877,":  tells me you're the one who knows how to solve this. Seems like we should be able to fakify a metatensor (e.g. with `FakeTensor(device=""meta"")`)? ",Yea I can take a look at this. How hipri is it ?,"It's not blocking atm; I can work around the issue. But it is a standard flow internally, where we would like to initialize a (very very large) model on the meta device and then export it, so we have a strong need for it to work soonish (~weeks?)",woooo thanx
296,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([PT-D][Checkpoint] Rename DCP storage layer init())ï¼Œ å†…å®¹æ˜¯ (Rename DCP storage layer init() and update tests accordingly. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[PT-D][Checkpoint] Rename DCP storage layer init(),Rename DCP storage layer init() and update tests accordingly. ,2023-01-23T23:32:29Z,Merged ciflow/trunk,closed,0,26,https://github.com/pytorch/pytorch/issues/92869, rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/92869/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/3998826624, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / winvs2019cuda11.6py3 / test (default, 1, 5, windows.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/92869/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4001553632, rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/92869/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4001584873, rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/92869/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4003351680, rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict pull/92869/head` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/4008219597, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 mandatory check(s) failed (Rule `Distributed`).  The first few are:  pull / linuxbionicpy3.7clang9 / test (dynamo, 1, 2, linux.2xlarge) Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ", rebase, successfully started a rebase job. Check the current status here,"Tried to rebase and push PR CC([PTD][Checkpoint] Rename DCP storage layer init()), but it was already up to date"," merge f ""unrelated test failures https://github.com/pytorch/pytorch/actions/runs/4008358242/jobs/6882601831"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
1428,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix endian handling in THPStorage_fromBuffer)ï¼Œ å†…å®¹æ˜¯ (Fixes CC(Test failure: TestTorch.test_from_buffer on a bigendian machine) This PR fixes a test failure of `TestTorch.test_from_buffer` on a bigendian machine. The root cause of this failure is that current `THPStorage_fromBuffer` does not perform endian handling correctly on a bigendian. In `THPStorage_fromBuffer`, the given buffer is stored as machine nativeendian. Thus, if the specified byte order (e.g. `big`) is equal to machine nativeendian, swapping elements should not be performed. However, in the current implementation, `decode*BE()` always swaps elements regardless of machine nativeendian (i.e. these methods assume buffer is stored as littleendian). Thus, this PR uses the following approaches:  if the specified byte order (e.g. `big`) is equal to machine nativeendian, call `decode*LE()` that does not swap elements by passing `torch::utils::THP_LITTLE_ENDIAN` to `THP_decode*Buffer()`.  if the specified byte order (e.g. `big`) is not equal to machine nativeendian, call `decode*BE()` that always swap elements by passing `torch::utils::THP_BIG_ENDIAN` to `THP_decode*Buffer()`. After applying this PR to the master branch, I confirmed that the test passes on a bigendian machine. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Fix endian handling in THPStorage_fromBuffer,"Fixes CC(Test failure: TestTorch.test_from_buffer on a bigendian machine) This PR fixes a test failure of `TestTorch.test_from_buffer` on a bigendian machine. The root cause of this failure is that current `THPStorage_fromBuffer` does not perform endian handling correctly on a bigendian. In `THPStorage_fromBuffer`, the given buffer is stored as machine nativeendian. Thus, if the specified byte order (e.g. `big`) is equal to machine nativeendian, swapping elements should not be performed. However, in the current implementation, `decode*BE()` always swaps elements regardless of machine nativeendian (i.e. these methods assume buffer is stored as littleendian). Thus, this PR uses the following approaches:  if the specified byte order (e.g. `big`) is equal to machine nativeendian, call `decode*LE()` that does not swap elements by passing `torch::utils::THP_LITTLE_ENDIAN` to `THP_decode*Buffer()`.  if the specified byte order (e.g. `big`) is not equal to machine nativeendian, call `decode*BE()` that always swap elements by passing `torch::utils::THP_BIG_ENDIAN` to `THP_decode*Buffer()`. After applying this PR to the master branch, I confirmed that the test passes on a bigendian machine. ",2023-01-23T18:19:30Z,triaged open source Merged ciflow/trunk,closed,0,5,https://github.com/pytorch/pytorch/issues/92834,Another possible approach is to add a new argument to `THP_decode*Buffer()` for representing endianness of a given buffer. These methods will determine whether swap elements or not based on the comparison between `order` and the new argument.," Thank you for explaining my fix and thoughts in detail. You completely understand what I want to do in this PR and how I minimize the change. I know that my first PR may lead to confusion. My current preference is 2'. 2' means that we **newly add** the `THP_decode*Buffer` functions by replacing `order` with `bool do_byte_swap` (or similar name) instead of changing the existing API. Let me share my thought about 1. and 2. For 2., `THP_decode*Buffer` functions are declared with `TORCH_API`. IMHO, these APIs are already public. Thus, I think that it is good to keep the existing API with the current interface and semantics (buffer is LE). If we can change the interface and semantics, it is fine with me to **change** `THP_decode*Buffer` functions. For 1., currently, there are two use cases of `THP_decode*Buffer` functions. The first one is to call these functions from `THPStorage_fromBuffer` that assumes the buffer is stored as machine nativeendian. The second one is to call these function from `THPStorage_readFileRaw()` that assumes the buffer is stored as littleendian. So, IMHO, in addition to machine's endianness check, we need to pass the endian of buffer to `THP_decode*Buffer` functions. Again, this may change the existing API of `TORCH_API` functions.    If calling `THP_nativeByteOrder()` leads to a performance problem, we can change this function to return a constant by compiletime evaluation. Any comments are appreciated.",(2) sounds good , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
358,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segfault when running torch.atan2)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Probably due to empty tensor:  Please note that this is a machine generated test cases (using fuzz testing).   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,Segfault when running torch.atan2, ğŸ› Describe the bug Probably due to empty tensor:  Please note that this is a machine generated test cases (using fuzz testing).   Versions  ,2023-01-23T16:33:34Z,triaged module: complex,open,0,2,https://github.com/pytorch/pytorch/issues/92818,"The error on the latest master is `""atan2_cpu"" not implemented for 'ComplexFloat'`, would you like to request complexFloat support for atan2?","> The error on the latest master is `""atan2_cpu"" not implemented for 'ComplexFloat'`, would you like to request complexFloat support for atan2? No I just reported this bug which my fuzzer found. Thanks."
769,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix transformer masks usage)ï¼Œ å†…å®¹æ˜¯ (Fixes Issue CC(Major bug in Transformers' masks)  The float usage in masks for the Transformer is considered deprecated, but still accepted.   `key_padding_mask` was ignored in the Transformer's multihead attention in the case where user creates a float mask. Fixing the float case.   Transformer's function `generate_square_subsequent_mask` generates float masks by default. Changing to bool mask.   Also adding a new `generate_padding_mask` in the Transformer, to help user create correct masks. Don't hesitate to correct me for the computation efficiency.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Fix transformer masks usage,"Fixes Issue CC(Major bug in Transformers' masks)  The float usage in masks for the Transformer is considered deprecated, but still accepted.   `key_padding_mask` was ignored in the Transformer's multihead attention in the case where user creates a float mask. Fixing the float case.   Transformer's function `generate_square_subsequent_mask` generates float masks by default. Changing to bool mask.   Also adding a new `generate_padding_mask` in the Transformer, to help user create correct masks. Don't hesitate to correct me for the computation efficiency.",2023-01-23T16:19:47Z,open source,closed,0,2,https://github.com/pytorch/pytorch/issues/92817," :x:  login:  / name: Emmanuelle . The commit (15d463abd721aab85591b0422bea48640d69d29b, 73099c30ec77e9238583d6b4e7a612b8ec3eac64, 7ff89a1a9d74afc7e61fbcec9b0b10c84a8a2e00) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.",Sorry. Someone else had already placed a PR. See CC(Regularize mask handling for attn_mask and key_padding_mask)
669,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SIGSEGV on a big-endian machine when reading a model generated on a little-endian machine )ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When I execute the following python code on a bigendian machine, SIGSEGV occurs. Executing this code on a littleendian machine works well.    This issue occurs on PyTorch 1.11, 1.12, 1.13, and master.  As a snippet to reproduce this issue, the following test causes the failure on a bigendian machine. We expect that this test also passes on a bigendian machine.   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,SIGSEGV on a big-endian machine when reading a model generated on a little-endian machine ," ğŸ› Describe the bug When I execute the following python code on a bigendian machine, SIGSEGV occurs. Executing this code on a littleendian machine works well.    This issue occurs on PyTorch 1.11, 1.12, 1.13, and master.  As a snippet to reproduce this issue, the following test causes the failure on a bigendian machine. We expect that this test also passes on a bigendian machine.   Versions  ",2023-01-23T09:43:57Z,module: serialization triaged module: POWER,closed,0,0,https://github.com/pytorch/pytorch/issues/92808
340,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([RELAND] Add metadata coverage for unsafe_split and unsafe_split_with_sizes)ï¼Œ å†…å®¹æ˜¯ (  CC([RELAND] Add metadata coverage for unsafe_split and unsafe_split_with_sizes))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[RELAND] Add metadata coverage for unsafe_split and unsafe_split_with_sizes,  CC([RELAND] Add metadata coverage for unsafe_split and unsafe_split_with_sizes),2023-01-23T05:07:12Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/92802, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
1984,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([JIT] Applying `conv2d` over Constants Leads to Exception)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug The following program throws an exception in JIT. This seems to be a fault in evaluating a constant expression. i.e., applying `conv2d` over a constant.  PyTorch version: 2.0.0.dev20230119+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: 11.1.06 CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 Nvidia driver version: 515.86.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.5 [pip3] pytorchtriton==2.0.0+0d7e753227 [pip3] torch==2.0.0.dev20230119+cu117 [pip3] torchaudio==2.0.0.dev20230118+cu117 [pip3] torchvision==0.15.0.dev20230118+cu117 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] mkl                       2021.4.0           h06a4308_64)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[JIT] Applying `conv2d` over Constants Leads to Exception," ğŸ› Describe the bug The following program throws an exception in JIT. This seems to be a fault in evaluating a constant expression. i.e., applying `conv2d` over a constant.  PyTorch version: 2.0.0.dev20230119+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: 11.1.06 CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 Nvidia driver version: 515.86.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.5 [pip3] pytorchtriton==2.0.0+0d7e753227 [pip3] torch==2.0.0.dev20230119+cu117 [pip3] torchaudio==2.0.0.dev20230118+cu117 [pip3] torchvision==0.15.0.dev20230118+cu117 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] mkl                       2021.4.0           h06a4308_64",2023-01-21T03:41:21Z,oncall: jit,open,0,4,https://github.com/pytorch/pytorch/issues/92740,Is there any specific reason why you passed a Parameter into a Conv2d op as its input? The input to an op should be a data.,"> Is there any specific reason why you passed a Parameter into a Conv2d op as its input? The input to an op should be a data. Thanks for your reply! The program here is not user facing since the input is a constant. It's generated and found by a fuzzer, but we still reported it because theoretically the usage is valid and it actually tests the functionality of constant folding.","Meanwhile, only using `self.v0 = torch.rand([1, 1, 14, 14])` also triggers the bug  and possibly some users could use constants for debugging.","Is it possible to validate with this sample?  `RuntimeError: Input type (torch.FloatTensor) and weight type (Mkldnntorch.FloatTensor) should be the same` Reason is `Conv2d` op in PyTorch on Linux are automatically converted to MKLDNN format, thus in the error message `weight type` is `Mkldnntorch.FloatTensor`. However, neither `Parameter` nor data defined in `Model __init__` function will be applied this conversion, so they are still `torch.FloatTensor`. By putting the data outside of definition of the `Model` solves this issue."
670,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add rearrange operator to PyTorch natively)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch Today, the rearrange from einops is very essential to write cleaner code within most transformers architecture and beyond.   However, einops comes with some caviats and need to be installed additionally to PyTorch. I strongly believe PyTorch should provide its own optimised rearrange functional operator. Best, T.C  Alternatives Stick with einops, but I would prefer not.  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Add rearrange operator to PyTorch natively," ğŸš€ The feature, motivation and pitch Today, the rearrange from einops is very essential to write cleaner code within most transformers architecture and beyond.   However, einops comes with some caviats and need to be installed additionally to PyTorch. I strongly believe PyTorch should provide its own optimised rearrange functional operator. Best, T.C  Alternatives Stick with einops, but I would prefer not.  Additional context _No response_ ",2023-01-20T08:30:26Z,feature good first issue triaged actionable module: python frontend module: functorch matrix multiplication module: first class dims,open,1,20,https://github.com/pytorch/pytorch/issues/92675, , ,Pytorch provides firstclass dims with similar functionality https://github.com/pytorch/pytorch/blob/master/functorch/dim/README.mdreshapingtensorseinops, well......... it is a bit different haha.,"Also, some users on twitter asking for this: https://twitter.com/ddetone/status/1619055524196253698, https://twitter.com/francoisfleuret/status/1583704549704888320","It allows splitting and flattening dims, what's different?",Can't someone just parse the rearrange string and then convert it into a sequence of first class dims. Seems like a fun project.,"Hi, could I be assigned this?","> However, einops comes with some caviats  could you explain what you view as the caveats?",Sure. This should be prototyped as a standalone Python file that implements rearrange in terms of first class dims. Not sure what the best way to do the parser is,"Should `rearrange` eventually live in `torch/functional.py` alongside e.g. einsum? Should we also assume that `functorch`, and in particular, `functorch.dim` is available as a dependency for the `torch` dir?","> Should we also assume that functorch, and in particular, functorch.dim is available as a dependency for the torch dir? Turns out this is not possible due to circular dependency (functorch depends on torch, so torch cannot depend on functorch). Looks like it will have to live in `functorch`. As for the parser, I went with a very thin wrapper for simplicity.",Once we have a working implementation we can figure out where to put it; we may be able to use a lazy import to deal with the circular dependency,this issue is still open or  was  this closed  with the pull request linked above,It's in progress via linked pr," E.g. in Mamba by  https://github.com/statespaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.pyL7 are used `from einops import rearrange, repeat`. Are these now natively available in PyTorch? (as `from functorch.einops import rearrange` seems to work, but `from torch.func.einops import rearrange` does not)  I guess should there be a rollup issue about remaining popular einops functions used by lucidrains and state_spaces?",.funcs.einops working or not,"What is `torch.func.einops`? Looking at `torch.func` now and its history, I don't see that `einops` ever existed inside it.","functorch/einops exists, I feel like we just never exposed it in torch.func namespace",An addition to `torch.func` would be appreciated! â™¡ 
423,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add `_StorageMeta` metaclass for `StorageBase`)ï¼Œ å†…å®¹æ˜¯ (  CC(Add `_StorageMeta` metaclass for `StorageBase`)  CC(Move PyInterpreter code in `python_variable.cpp` to its own files) Part of CC(PyObject preservation and resurrection for `StorageImpl`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add `_StorageMeta` metaclass for `StorageBase`,  CC(Add `_StorageMeta` metaclass for `StorageBase`)  CC(Move PyInterpreter code in `python_variable.cpp` to its own files) Part of CC(PyObject preservation and resurrection for `StorageImpl`) ,2023-01-19T22:42:13Z,module: internals open source Merged ciflow/trunk topic: not user facing,closed,0,11,https://github.com/pytorch/pytorch/issues/92648,"I am pretty sure we documented this in tensor, but to recap, the problem is resurrection needs to run *before* the conventional deallocation function. However a Python subclass will run the ordinary object dealloc before chaining to the superclass dealloc. We must override it at each subclass creation ",I've added an explanation of why we use the metaclass in the standalone repo: https://github.com/kurtamohler/pyobjectpreservation/commit/33b5f92bb374fd40d76057c10943f32d20ff00aa, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / linuxbioniccuda11.6py3.10gcc7sm86 / test (default, 4, 4, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ",Ho righ! tp_dealloc is not inherited but a brand new one is added! That's the problem! Thanks !, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x ef328a29a74441cc11e82ef440ac174077453725` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job ," merge f ""unrelated CI failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
442,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Move PyInterpreter code in `python_variable.cpp` to its own files)ï¼Œ å†…å®¹æ˜¯ (  CC(Add `_StorageMeta` metaclass for `StorageBase`)  CC(Move PyInterpreter code in `python_variable.cpp` to its own files) Part of CC(PyObject preservation and resurrection for `StorageImpl`) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Move PyInterpreter code in `python_variable.cpp` to its own files,  CC(Add `_StorageMeta` metaclass for `StorageBase`)  CC(Move PyInterpreter code in `python_variable.cpp` to its own files) Part of CC(PyObject preservation and resurrection for `StorageImpl`) ,2023-01-19T22:42:08Z,module: internals open source Merged topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/92647
320,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add meta kernel coverage for aten.unsafe_split, aten.unsafe_chunk)ï¼Œ å†…å®¹æ˜¯ (  CC(Add meta kernel coverage for aten.unsafe_split, aten.unsafe_chunk))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,"Add meta kernel coverage for aten.unsafe_split, aten.unsafe_chunk","  CC(Add meta kernel coverage for aten.unsafe_split, aten.unsafe_chunk)",2023-01-19T02:36:05Z,Merged Reverted Stale ciflow/trunk topic: not user facing,closed,0,18,https://github.com/pytorch/pytorch/issues/92608, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `gh/tugsbayasgalan/92/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/92608`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""test_aot_autograd_symbolic_exhaustive_unsafe_split_cpu_float32 (__main__.TestEagerFusionOpInfoCPU) is failing consistently since this PR was merged"" c landrace See https://hud.pytorch.org/pytorch/pytorch/commit/4386f317b92a400cabc6a25b5849466475eec1a9 for details", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., rebase , successfully started a rebase job. Check the current status here,"Successfully rebased `gh/tugsbayasgalan/92/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/92608`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""test_aot_autograd_symbolic_exhaustive_unsafe_split_cpu_float32 (main.TestEagerFusionOpInfoCPU) is now xpass"" c landrace", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., you need to remove xfail from unexpectedly passing test before relanding,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
267,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([WIP] trying fix bazel with nvcc sccache)ï¼Œ å†…å®¹æ˜¯ (Fixes CC([bazel] build doesn't use sccache))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[WIP] trying fix bazel with nvcc sccache,Fixes CC([bazel] build doesn't use sccache),2023-01-19T00:33:59Z,open source,closed,0,0,https://github.com/pytorch/pytorch/issues/92598
412,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add api to expose operands as an oplist, for copying purposes)ï¼Œ å†…å®¹æ˜¯ (This change allows the XLA downstream to clone IR nodes with/without sharding info, enabling the SPMD feature for XLA:TPU. See https://github.com/pytorch/xla/pull/4555)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"Add api to expose operands as an oplist, for copying purposes","This change allows the XLA downstream to clone IR nodes with/without sharding info, enabling the SPMD feature for XLA:TPU. See https://github.com/pytorch/xla/pull/4555",2023-01-18T21:10:49Z,open source Stale,closed,0,5,https://github.com/pytorch/pytorch/issues/92579,The committers listed above are authorized under a signed CLA.:white_check_mark: login: steventkg  (c02a61b744349bae5482740a5b22b921dff1dc58),"Hi g, could you please sign the CLA linked to by the CLA bot? (It's basically legalize where you allow us to use the code you submit) https://github.com/pytorch/pytorch/pull/92579issuecomment1396092852","The committers listed above are authorized under a signed CLA.:white_check_mark: login: steventkg  (38bf9348e7eb23d94322b14158f8f4e46b376818, dda29fc3269b6cb6a93370d341c6983c395699d5)","The committers listed above are authorized under a signed CLA.:white_check_mark: login: steventkg  (38bf9348e7eb23d94322b14158f8f4e46b376818, dda29fc3269b6cb6a93370d341c6983c395699d5, 7d7d2034d5908ea6910fbf0c340c0d2348e7780b)","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
2109,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(isTuple()INTERNAL ASSERT FAILED at \""C:\\\\w\\\\b\\\\windows\\\\pytorch\\\\aten\\\\src\\\\ATen/core/ivalue_inl.h\"":1400, please report a bug to PyTorch. Expected Tuple but got String)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm loading a python generated jit module into c++, updating parameters to trained values, and then saving. Below is a stripped down version of the code that causes the error. This code works fine with a simpler jit module. I uploaded the zipped version of the offending file (accidentally twice): ScriptUntrained_LibTorchTest03Cmb.zip ScriptUntrained_LibTorchTest03Cmb.zip  An exception is thrown with the msg shown above. Below is the call stack.   Versions libtorchwinsharedwithdeps1.10.2+cu113 (base) F:\Dev\LibTorch\pytorch\torch\utils>python collect_env.py Collecting environment information... PyTorch version: 1.13.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Microsoft Windows Server 2019 Standard GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.17763SP0 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration: GPU 0: Tesla V100SPCIE32GB GPU 1: Tesla V100SPCIE32GB GPU 2: Tesla V100SPCIE32GB GPU 3: Tesla V100SPCIE32GB Nvidia driver version: 516.94 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin\cudnn_ops_train64_8.dll HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.1 [pip3] numpydoc==1.1.0 [pip3] torch==1.13.0 [pip3] torchaudio==0.13.0 [pip3] torchinfo==1.7.1 [pip3] torchvision==0.14.0 [conda] blas                      1.0                         mkl [conda] mkl                       2021.2.0           haa95532_296 [co)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,"isTuple()INTERNAL ASSERT FAILED at \""C:\\\\w\\\\b\\\\windows\\\\pytorch\\\\aten\\\\src\\\\ATen/core/ivalue_inl.h\"":1400, please report a bug to PyTorch. Expected Tuple but got String"," ğŸ› Describe the bug I'm loading a python generated jit module into c++, updating parameters to trained values, and then saving. Below is a stripped down version of the code that causes the error. This code works fine with a simpler jit module. I uploaded the zipped version of the offending file (accidentally twice): ScriptUntrained_LibTorchTest03Cmb.zip ScriptUntrained_LibTorchTest03Cmb.zip  An exception is thrown with the msg shown above. Below is the call stack.   Versions libtorchwinsharedwithdeps1.10.2+cu113 (base) F:\Dev\LibTorch\pytorch\torch\utils>python collect_env.py Collecting environment information... PyTorch version: 1.13.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Microsoft Windows Server 2019 Standard GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.17763SP0 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration: GPU 0: Tesla V100SPCIE32GB GPU 1: Tesla V100SPCIE32GB GPU 2: Tesla V100SPCIE32GB GPU 3: Tesla V100SPCIE32GB Nvidia driver version: 516.94 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin\cudnn_ops_train64_8.dll HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.1 [pip3] numpydoc==1.1.0 [pip3] torch==1.13.0 [pip3] torchaudio==0.13.0 [pip3] torchinfo==1.7.1 [pip3] torchvision==0.14.0 [conda] blas                      1.0                         mkl [conda] mkl                       2021.2.0           haa95532_296 [co",2023-01-18T18:10:44Z,oncall: jit module: windows,closed,0,3,https://github.com/pytorch/pytorch/issues/92560,"While trying to work around this issue by loading the trained weights on the python side, I ran into a similar bug. Not sure whether this should be a separate case. Environment is the same. ScriptUntrained_LibTorchTest03Cmb.zip  ","Hello , I managed to reproduce this error using your libtorch 1.10.2 configuration. However, you will not encounter this issue if you use the latest release of libtorch, so I would advise you to try and update it if possible.",Closing this issue because of lack of activity. Don't hesitate to report a new one if needed.
720,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Major bug in Transformers' masks)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug In torch.nn.functional, line 5174 in current code: !image `attn_mask = attn_mask.masked_fill(key_padding_mask, float(""inf""))` Fills `inf` value wherever `key_padding_mask` is True. However, `key_padding_masks` actually contains inf values instead of True values. So the padding mask is actually ignored. Replacing by: `attn_mask = attn_mask.masked_fill(key_padding_mask < 0, float(""inf"")` Gives the correct expected mask.  Versions Current github code.  (master branch, Jan 18, 2023) )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Major bug in Transformers' masks," ğŸ› Describe the bug In torch.nn.functional, line 5174 in current code: !image `attn_mask = attn_mask.masked_fill(key_padding_mask, float(""inf""))` Fills `inf` value wherever `key_padding_mask` is True. However, `key_padding_masks` actually contains inf values instead of True values. So the padding mask is actually ignored. Replacing by: `attn_mask = attn_mask.masked_fill(key_padding_mask < 0, float(""inf"")` Gives the correct expected mask.  Versions Current github code.  (master branch, Jan 18, 2023) ",2023-01-18T17:45:17Z,high priority module: nn triaged module: correctness (silent),open,0,11,https://github.com/pytorch/pytorch/issues/92554,"You can verify that if you call a Transformer's forward method with src_mask and tgt_mask containing inf values, result is not the same as when using boolean masks.","Hi, The doc seems to imply that `key_padding_mask` for MultiheadAttention should be a boolean or byte Tensor: https://pytorch.org/docs/stable/generated/torch.nn.quantizable.MultiheadAttention.htmltorch.nn.quantizable.MultiheadAttention.forward ? But indeed Transformer does allow `src_mask` and `tgt_mask` to be floats. Tentatively setting high pri for silent correctness.","torch.nn.MultiHeadAttention is defined to accept floats =>  https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html ""For a float mask, the mask values will be added to the attention weight.""","(at least) as early as November 2021, we issued warning about byte tensors being deprecated for torch.nn.MultiHeadAttention, e.g., here =>  CC(Improved Transformer and MultiHeadAttention design) ","I recommend that we correct the documentation for MultiHeadAttention to reflect that byte masks were deprecated a while ago, e.g., here => https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html as well as here => https://pytorch.org/docs/stable/generated/torch.nn.quantizable.MultiheadAttention.htmltorch.nn.quantizable.MultiheadAttention.forward","It's slightly more complicated than this?  because key_padding_mask might be either Boolean or Float.   Are permutations allowed where one mask is Boolean and the other is FLoat, or should we require both of them to be of same type?  I would strongly advocate for the latter!   In that case, the suggested code is sufficient (rather than requiring case splitting for kay_padding_mask being either Bool or float):  In terms of computation, I believe   avoids allocating and instantiating a Boolean mask.","I saw the deprecated warning. However, your own code, in torch.nn.modules.transformers, still creates masks using inf. !image","Presumably you had a test case that demonstrates the problem?  Can you please create a PR and submit it, so we can verify the PR fixes the issue?","My code is not ready yet, I am only starting to prepare everything. I think you should not only rely on me for your tests. But yes, it will be my pleasure to prepare a PR.",Hi! I saw the PR CC(Regularize mask handling for attn_mask and key_padding_mask). Thanks for correcting so fast. You still have an uncorrected float usage in: torch.nn.Transformer.generate_square_subsequent_mask.,"Can you please provide more context as to what you see as the problem?   Maybe we havenâ€™t documented it in the doc strings clearly enough that floats are intended to be legal. Get Outlook for iOS ________________________________ From: Emmanuelle ***@***.***> Sent: Wednesday, January 25, 2023 6:10 AM To: pytorch/pytorch ***@***.***> Cc: Michael Gschwind ***@***.***>; Comment ***@***.***> Subject: Re: [pytorch/pytorch] Major bug in Transformers' masks (Issue CC(Major bug in Transformers' masks)) Hi! I saw the PR CC(Regularize mask handling for attn_mask and key_padding_mask). Thanks for correcting so fast. You still have an uncorrected float usage in: torch.â€Šnn.â€ŠTransformer.â€Šgenerate_square_subsequent_mask. â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving ZjQcmQRYFpfptBannerStart This Message Is From an External Sender ZjQcmQRYFpfptBannerEnd Hi! I saw the PR CC(Regularize mask handling for attn_mask and key_padding_mask). Thanks for correcting so fast. You still have an uncorrected float usage in: torch.nn.Transformer.generate_square_subsequent_mask. â€” Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you commented.Message ID: ***@***.***>"
1398,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(state_dict['args'] empty when trying to load)ï¼Œ å†…å®¹æ˜¯ ( ğŸ›  Hi, I try to load PyTorch model (specifically wav2vec2), using the following command:  On loading the model, I'm getting errors, and when I tried to print the `arg` I found that it was empty.  What is the issue, and how can I resolve it?  Versions PyTorch version: 1.10.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 16.04.7 LTS (x86_64) GCC version: (Ubuntu 5.5.012ubuntu1~16.04) 5.5.0 20171010 Clang version: Could not collect CMake version: version 3.5.2 Libc version: glibc2.23 Python version: 3.8.9 (default, Apr  3 2021, 01:02:10)  [GCC 5.4.0 20160609] (64bit runtime) Python platform: Linux4.4.021genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.0.76 CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: Tesla V100SXM232GB Nvidia driver version: 525.60.13 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==1.10.1 [pip3] torchaudio==0.10.1 [pip3] torchvision==0.11.2+rocm4.2 [conda] Could not collect Thanks)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,state_dict['args'] empty when trying to load," ğŸ›  Hi, I try to load PyTorch model (specifically wav2vec2), using the following command:  On loading the model, I'm getting errors, and when I tried to print the `arg` I found that it was empty.  What is the issue, and how can I resolve it?  Versions PyTorch version: 1.10.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 16.04.7 LTS (x86_64) GCC version: (Ubuntu 5.5.012ubuntu1~16.04) 5.5.0 20171010 Clang version: Could not collect CMake version: version 3.5.2 Libc version: glibc2.23 Python version: 3.8.9 (default, Apr  3 2021, 01:02:10)  [GCC 5.4.0 20160609] (64bit runtime) Python platform: Linux4.4.021genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 12.0.76 CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: Tesla V100SXM232GB Nvidia driver version: 525.60.13 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==1.10.1 [pip3] torchaudio==0.10.1 [pip3] torchvision==0.11.2+rocm4.2 [conda] Could not collect Thanks",2023-01-18T12:58:26Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/92541,Please visit to the PyTorch forum https://discuss.pytorch.org/ for any questions. We use issues for bugs and features only.
802,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_numpy_int_constant_dynamic_shapes (torch._dynamo.testing.make_test_cls_with_patches.<locals>.DummyTestClass))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 72 hours, it has flakily failed in 2 workflow(s). **Debugging instructions (after clicking on the recent samples link):** To find relevant log snippets: 1. Click on the workflow logs linked above 2. Grep for `test_numpy_int_constant_dynamic_shapes` Error retrieving /opt/conda/lib/python3.10/sitepackages/torch/_dynamo/testing.py: Error: Statuscode 301)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DISABLED test_numpy_int_constant_dynamic_shapes (torch._dynamo.testing.make_test_cls_with_patches.<locals>.DummyTestClass),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 72 hours, it has flakily failed in 2 workflow(s). **Debugging instructions (after clicking on the recent samples link):** To find relevant log snippets: 1. Click on the workflow logs linked above 2. Grep for `test_numpy_int_constant_dynamic_shapes` Error retrieving /opt/conda/lib/python3.10/sitepackages/torch/_dynamo/testing.py: Error: Statuscode 301",2023-01-18T07:07:39Z,module: flaky-tests skipped module: unknown,closed,0,0,https://github.com/pytorch/pytorch/issues/92512
843,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(DISABLED test_repro_graph_breaks_in__get_item_by_idx_dynamic_shapes (torch._dynamo.testing.make_test_cls_with_patches.<locals>.DummyTestClass))ï¼Œ å†…å®¹æ˜¯ (Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 72 hours, it has flakily failed in 2 workflow(s). **Debugging instructions (after clicking on the recent samples link):** To find relevant log snippets: 1. Click on the workflow logs linked above 2. Grep for `test_repro_graph_breaks_in__get_item_by_idx_dynamic_shapes` Error retrieving /opt/conda/lib/python3.10/sitepackages/torch/_dynamo/testing.py: Error: Statuscode 301 )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,DISABLED test_repro_graph_breaks_in__get_item_by_idx_dynamic_shapes (torch._dynamo.testing.make_test_cls_with_patches.<locals>.DummyTestClass),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 72 hours, it has flakily failed in 2 workflow(s). **Debugging instructions (after clicking on the recent samples link):** To find relevant log snippets: 1. Click on the workflow logs linked above 2. Grep for `test_repro_graph_breaks_in__get_item_by_idx_dynamic_shapes` Error retrieving /opt/conda/lib/python3.10/sitepackages/torch/_dynamo/testing.py: Error: Statuscode 301 ",2023-01-18T06:54:50Z,triaged module: flaky-tests skipped oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/92475,This is the long standing  
389,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Reland] Add sym_size/stride/numel/storage_offset to native_function.yaml (#91â€¦)ï¼Œ å†…å®¹æ˜¯ (Pull Request resolved: https://github.com/pytorch/pytorch/pull/91919 Approved by: https://github.com/ezyang Fixes ISSUE_NUMBER)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Reland] Add sym_size/stride/numel/storage_offset to native_function.yaml (#91â€¦,Pull Request resolved: https://github.com/pytorch/pytorch/pull/91919 Approved by: https://github.com/ezyang Fixes ISSUE_NUMBER,2023-01-18T06:35:23Z,Merged Reverted ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/92402," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D42565586,This pull request was **exported** from Phabricator. Differential Revision: D42565586,This pull request was **exported** from Phabricator. Differential Revision: D42565586, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"This diff regressed one pretty important model we have for export, and we did our best to fix forward today. Unfortunately, it seems we need more time to investigate the interaction between this diff and inference mode. Meanwhile to unblock our model enablement work, we may need to revert this diff.  "," revert m ""Caused a regression for an export model."" c landrace", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
448,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add gpt2 to unittest; fix regression)ï¼Œ å†…å®¹æ˜¯ (I'm slightly not sure about if we should design the api with `opset_version` as positional argument. However, the scope of the PR is only to fix the regression, under the assumption that `opset_version` is a positional argument.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Add gpt2 to unittest; fix regression,"I'm slightly not sure about if we should design the api with `opset_version` as positional argument. However, the scope of the PR is only to fix the regression, under the assumption that `opset_version` is a positional argument.",2023-01-18T01:51:49Z,module: onnx open source release notes: onnx,closed,0,3,https://github.com/pytorch/pytorch/issues/92366, ,I could be missing out some discussion. Why are we considering not have opset_version api? Which version should we stick to?,"> I could be missing out some discussion. Why are we considering not have opset_version api? Which version should we stick to? There is no discussion yet afaik. There could be many ways how opset_version can be specified (positional, optional, config etc), but that's fine and can be consolidated later. I wanted to limit this PR to be very simple and just enable gpt2 unittest with the current api."
1059,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(torch.stack not behaving properly on mps leads to Dataloader yielding same target values each batch)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug When feeding a training loop with data using and   on an MPS device, the target data won't update during the iterations. This is shown in the example below. Instead of  the values  appear as targets. This happens with Pytorch **1.3.1** and **2.0.0.dev20230116**  MPS output:  CPU output:   Versions [pip3] numpy==1.23.5 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchvision==0.14.1 [conda] numpy                     1.23.5           py39h1398885_0 [conda] numpybase                1.23.5           py39h90707a3_0 [conda] pytorch                   1.13.1                  py3.9_0    pytorch [conda] torchaudio                0.13.1                 py39_cpu    pytorch [conda] torchvision               0.14.1                 py39_cpu    pytorch )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,torch.stack not behaving properly on mps leads to Dataloader yielding same target values each batch," ğŸ› Describe the bug When feeding a training loop with data using and   on an MPS device, the target data won't update during the iterations. This is shown in the example below. Instead of  the values  appear as targets. This happens with Pytorch **1.3.1** and **2.0.0.dev20230116**  MPS output:  CPU output:   Versions [pip3] numpy==1.23.5 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchvision==0.14.1 [conda] numpy                     1.23.5           py39h1398885_0 [conda] numpybase                1.23.5           py39h90707a3_0 [conda] pytorch                   1.13.1                  py3.9_0    pytorch [conda] torchaudio                0.13.1                 py39_cpu    pytorch [conda] torchvision               0.14.1                 py39_cpu    pytorch ",2023-01-16T22:40:17Z,triaged module: mps,closed,0,10,https://github.com/pytorch/pytorch/issues/92265,I don't have mps device available now. Do you mind helping me test the following script? ,> I don't have mps device available now. Do you mind helping me test the following script? >  >  Sure!  I also iterated over the TensorDataset. ,Thanks for providing the result. It has to be some logic within DataLoader unveil some problem for ops of mps tensor. How about ,,"Great, we have been able to narrow down the root cause. How about: ",Now I got lost :) ,Then the rootcause problem is the `torch.stack` operation over mps Tensor. cc:  ,I thinks it is the same as CC(torch.stack gives wrong results on MPS ),Hi  this is fixed in latest pytorch nightly (torch 2.0.0.dev20230215). Please give it a try and let me know if you still any issues   To install latest nightly: `pip3 install pre forcereinstall torch indexurl https://download.pytorch.org/whl/nightly/cpu`,Works! Thanks alot!
1979,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(`model.to(""cuda:0"")` does not release all CPU memory)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug As per title, placing a model on GPU does not seem to release all allocated CPU memory:  prints:  Is this expected? The model is about 3 GB. The same can be checked with `free h`.  Versions PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: 14.0.01ubuntu1 CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.057genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU Nvidia driver version: 515.86.01 cuDNN version: Probably one of the following: /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_train.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_train.so.8.7.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.4 [pip3] torch==1.13.1 [pip3] torchmodelarchiver==0.6.1 [pip3] torchtbprofiler==0.4.0 [pip3] torchworkflowarchiver==0.2.5 [pip3] torchaudio==0.13.0 [pip3] torchinfo==1.7.0 [pip3] torchserve==0.6.1 [pip3] torchtriton==2.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,"`model.to(""cuda:0"")` does not release all CPU memory"," ğŸ› Describe the bug As per title, placing a model on GPU does not seem to release all allocated CPU memory:  prints:  Is this expected? The model is about 3 GB. The same can be checked with `free h`.  Versions PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: 14.0.01ubuntu1 CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.057genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU Nvidia driver version: 515.86.01 cuDNN version: Probably one of the following: /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_adv_train.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.7.0 /usr/local/cuda11.7/targets/x86_64linux/lib/libcudnn_ops_train.so.8.7.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.4 [pip3] torch==1.13.1 [pip3] torchmodelarchiver==0.6.1 [pip3] torchtbprofiler==0.4.0 [pip3] torchworkflowarchiver==0.2.5 [pip3] torchaudio==0.13.0 [pip3] torchinfo==1.7.0 [pip3] torchserve==0.6.1 [pip3] torchtriton==2.",2023-01-16T15:01:17Z,module: memory usage triaged,open,4,0,https://github.com/pytorch/pytorch/issues/92252
1474,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(module:mps Reliable PyTorch crash via failed MPSNDArray failed assertion)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I am regularly able to trigger a  by running the following PyTorch code:  Commenting out the `model.to(""mps"")` gives me the expected `tensor([[50257, 50362,   764, 50256]])` result, as does using   Versions % python collect_env.py  Collecting environment information... PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: Could not collect Libc version: N/A Python version: 3.9.6 (default, Oct 18 2022, 12:41:40)  [Clang 14.0.0 (clang1400.0.29.202)] (64bit runtime) Python platform: macOS13.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchvision==0.14.1 [conda] numpy                     1.22.3          py310hdb36b11_0   [conda] numpybase                1.22.3          py310h5e3e9f0_0   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,module:mps Reliable PyTorch crash via failed MPSNDArray failed assertion," ğŸ› Describe the bug I am regularly able to trigger a  by running the following PyTorch code:  Commenting out the `model.to(""mps"")` gives me the expected `tensor([[50257, 50362,   764, 50256]])` result, as does using   Versions % python collect_env.py  Collecting environment information... PyTorch version: 1.13.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.1 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: Could not collect Libc version: N/A Python version: 3.9.6 (default, Oct 18 2022, 12:41:40)  [Clang 14.0.0 (clang1400.0.29.202)] (64bit runtime) Python platform: macOS13.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==1.13.1 [pip3] torchaudio==0.13.1 [pip3] torchvision==0.14.1 [conda] numpy                     1.22.3          py310hdb36b11_0   [conda] numpybase                1.22.3          py310h5e3e9f0_0   ",2023-01-15T17:22:13Z,triaged module: mps,closed,0,2,https://github.com/pytorch/pytorch/issues/92222,"Also, I get the same crash when I compile the model preuse: ",Works when installing latest nightly. Closing
504,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unit test for is_causal Better Transformers (#91900))ï¼Œ å†…å®¹æ˜¯ (Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/91900 Test Plan: buck test  :test_transformers  r test_train_with_is_causal buck test mode/opt :test_transformers  r test_is_causal_gpu flake8 test_transformers.py Differential Revision: D42453642)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Unit test for is_causal Better Transformers (#91900),Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/91900 Test Plan: buck test  :test_transformers  r test_train_with_is_causal buck test mode/opt :test_transformers  r test_is_causal_gpu flake8 test_transformers.py Differential Revision: D42453642,2023-01-12T20:57:17Z,fb-exported Merged ciflow/trunk topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/92102,This pull request was **exported** from Phabricator. Differential Revision: D42453642,This pull request was **exported** from Phabricator. Differential Revision: D42453642,This pull request was **exported** from Phabricator. Differential Revision: D42453642, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 additional jobs have failed, first few of them are: trunk Details for Dev Infra team Raised by workflow job "," merge f ""the macos failures are unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
729,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(ModuleNotFoundError : Trying to verify the installation of PyTorch but not working even though 'torch - 1.13.1' installed in system)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm trying to verify the installation of PyTorch is not working, getting errors again and again. Installed pytorch with conda :    Updated conda too :    And when I try to mkl this happens :   And this :    Versions OS                           macOS             Ventura 13.1 pytorch                   1.13.1                 py3.10_0 conda                     22.11.1         py310hca03da5_4   )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,ModuleNotFoundError : Trying to verify the installation of PyTorch but not working even though 'torch - 1.13.1' installed in system," ğŸ› Describe the bug I'm trying to verify the installation of PyTorch is not working, getting errors again and again. Installed pytorch with conda :    Updated conda too :    And when I try to mkl this happens :   And this :    Versions OS                           macOS             Ventura 13.1 pytorch                   1.13.1                 py3.10_0 conda                     22.11.1         py310hca03da5_4   ",2023-01-12T07:14:55Z,triaged module: macos,closed,0,1,https://github.com/pytorch/pytorch/issues/92065,"Hi, There are no 1.13 binaries on conda. You can see  CC(Missing python 3.11 on anaconda for torch 1.13.1) for more details."
584,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Fix doc of `torch.nn.MultiLabelSoftMarginLoss`)ï¼Œ å†…å®¹æ˜¯ ( ğŸ“š The doc issue The documentation states that $y[i] \in$ {0, 1} and later on, it also say ""label targets padded by 1 ensuring same shape as the input"". This is contradictory. Looking at the source code, I believe the bit about padding is incorrect and 1 values will turn the loss the wrong way.  Suggest a potential alternative/fix remove ""padded by 1"" )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Fix doc of `torch.nn.MultiLabelSoftMarginLoss`," ğŸ“š The doc issue The documentation states that $y[i] \in$ {0, 1} and later on, it also say ""label targets padded by 1 ensuring same shape as the input"". This is contradictory. Looking at the source code, I believe the bit about padding is incorrect and 1 values will turn the loss the wrong way.  Suggest a potential alternative/fix remove ""padded by 1"" ",2023-01-11T12:11:08Z,module: nn triaged topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/92000,If you would like to submit a PR I would be happy to review this,I don't feel I have completely understood this implementation so I'd better leave it to someone more competent.,"Indeed, ""padded by 1"" is incorrect. The shape of input and target tensor are compared, and an exception is raised if they differ in either dimension 0 or 1. See the code snippet below and the resulting output:  output:  Creating PR"
318,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Add sym_size/stride/numel/storage_offset to native_function.yaml)ï¼Œ å†…å®¹æ˜¯ (  CC(Add sym_size/stride/numel/storage_offset to native_function.yaml))è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,Add sym_size/stride/numel/storage_offset to native_function.yaml,  CC(Add sym_size/stride/numel/storage_offset to native_function.yaml),2023-01-09T21:32:53Z,Merged Reverted ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/91919, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""Break internal build"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
389,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Unit test for is_causal Better Transformers)ï¼Œ å†…å®¹æ˜¯ (Test Plan: buck test  :test_transformers  r test_train_with_is_causal buck test mode/opt :test_transformers  r test_is_causal_gpu Differential Revision: D42373034)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,Unit test for is_causal Better Transformers,Test Plan: buck test  :test_transformers  r test_train_with_is_causal buck test mode/opt :test_transformers  r test_is_causal_gpu Differential Revision: D42373034,2023-01-09T18:21:47Z,fb-exported topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/91900,The committers listed above are authorized under a signed CLA.:white_check_mark: login: jcrousse  (a012f076ce39c3590159fd8d7e6580b255c63414),This pull request was **exported** from Phabricator. Differential Revision: D42373034,This pull request was **exported** from Phabricator. Differential Revision: D42373034,"  :white_check_mark: login: jcrousse  (2a4ed000021fe57e94a0d5ebcc64c248e7cedfdc, 12a297684c2c170474aec31a22331f267d57fc7e, 89859d82daa58b26a40bf3349920012b2935032b, 90b96c603708ff8ccd0d55c6327c714ea67e36f4) :x: The commit (6e429489ba98600d5da5128e44c637a9c22ee5b4, de99559e8894475262fd4f59aae605fa8f3f2071). This user is missing the User's ID, preventing the EasyCLA check. Consult GitHub Help to resolve.For further assistance with EasyCLA, please submit a support request ticket.",I think there is a merge conflict from CC([SDPA] Guard mem efficient attention in deterministic mode) Sorry I should have given a heads up,Reopened https://github.com/pytorch/pytorch/pull/92102 closing this one.
1025,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Tensor copying not always detecting when src and dest refer to same memory location)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug Hi there! In certain cases it seems that torch does not detect when the src and dest of a tensor copy refer to the same memory location, and therefore performs an incorrect copy. For example, in the following snippet:  Even though the same operations are run every loop, the result of the shift is different every time (the left column is not consistent across multiple runs of the script either, while the right column is):  If I replace the problematic line with `buffer[:, 1 : num_steps] = buffer[:, 0 : (num_steps  1)].clone()`, everything works as expected. Moreover, if I make the buffer have a simpler shape by dropping the leading `num_envs` dimension, then torch successfully detects the problem and raises the error:   Versions )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Tensor copying not always detecting when src and dest refer to same memory location," ğŸ› Describe the bug Hi there! In certain cases it seems that torch does not detect when the src and dest of a tensor copy refer to the same memory location, and therefore performs an incorrect copy. For example, in the following snippet:  Even though the same operations are run every loop, the result of the shift is different every time (the left column is not consistent across multiple runs of the script either, while the right column is):  If I replace the problematic line with `buffer[:, 1 : num_steps] = buffer[:, 0 : (num_steps  1)].clone()`, everything works as expected. Moreover, if I make the buffer have a simpler shape by dropping the leading `num_envs` dimension, then torch successfully detects the problem and raises the error:   Versions ",2023-01-09T16:19:17Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/91894,"reliably detecting arbitrary memory overlaps is a numerically expensive problem and we are not doing it, this is expected. "
1517,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(None Gradient, when multiplying tensor by constant)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug tensor. grad gives `None` when initializing a tensor with `requires_grad=True`, and multiplying it with a constant value.  **output: None, tensor(1.)**  The same doesn't happen when we init the tensor without the `required_grad` parameter and then do it separately.  **output: tensor(1.) tensor(1.)**  Versions PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 470.161.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] torch==1.12.1 [conda] numpy                     1.23.4                   pypi_0    pypi [conda] torch                     1.12.1                   pypi_0    pypi  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,"None Gradient, when multiplying tensor by constant"," ğŸ› Describe the bug tensor. grad gives `None` when initializing a tensor with `requires_grad=True`, and multiplying it with a constant value.  **output: None, tensor(1.)**  The same doesn't happen when we init the tensor without the `required_grad` parameter and then do it separately.  **output: tensor(1.) tensor(1.)**  Versions PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 470.161.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] torch==1.12.1 [conda] numpy                     1.23.4                   pypi_0    pypi [conda] torch                     1.12.1                   pypi_0    pypi  ",2023-01-08T07:45:58Z,,closed,1,1,https://github.com/pytorch/pytorch/issues/91851,"this is expected behavior, `a` is nonleaf tensor in this case, and thus doesn't retain gradient by default. You can call `a.retain_grad()` if you want to save a's gradient. "
706,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([Bug/functorch] Cannot use `tensor.detach().numpy()` for `GradTrackingTensor`: Cannot access data pointer of Tensor that doesn't have storage)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I'm trying to convert some intermediate tensors to numpy arrays. This is a common use case for Reinforcement Learning (RL) tasks. I put the sampling process in my objective function. The PyTorch module takes `observation` tensor and produces `action`, then the `action` is converted to a numpy array and sent to the RL environment. The pipeline is:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",rag,[Bug/functorch] Cannot use `tensor.detach().numpy()` for `GradTrackingTensor`: Cannot access data pointer of Tensor that doesn't have storage," ğŸ› Describe the bug I'm trying to convert some intermediate tensors to numpy arrays. This is a common use case for Reinforcement Learning (RL) tasks. I put the sampling process in my objective function. The PyTorch module takes `observation` tensor and produces `action`, then the `action` is converted to a numpy array and sent to the RL environment. The pipeline is:   Versions  ",2023-01-06T16:29:29Z,triaged module: functorch,open,3,10,https://github.com/pytorch/pytorch/issues/91810,A temporary workaround:  which rely on the implementation details of `torch._C._functorch`.,Also a problem for `numpy(force = True)`?,> Also a problem for `numpy(force = True)`? `tensor.detach().numpy(force=True)` is not working. It still raises `RuntimeError: Cannot access data pointer of Tensor that doesn't have storage`.,"Hi, I think this is due to functorch: when using transforms, you get non pure Tensor objects. Leading to the error you see here.","> I think this is due to functorch: when using transforms, you get non pure Tensor objects. Thanks for the clarification. I think the `GradTrackingTensor` does not expose their memory storage because users may use _""assignment""_ which breaks the purefunctional pattern.  The unwrapped tensor may share the storage with the wrapped gradtracking tensor. So the following may also break the purefunctional transformation:   My use case as described in  CC([Bug/functorch] Cannot use `tensor.detach().numpy()` for `GradTrackingTensor`: Cannot access data pointer of Tensor that doesn't have storage)issue1522837470 is differ than above. I use `.numpy()` method to convert the `tensor` to numpy `ndarray`s. They will not share memory storage. So I think the `.numpy()` operator should be available in eager transformations for `GradTrackingTensor`. It is really common for Reinforcement Learning (RL) use cases."," In general, should there be some method `materialize()` or `to_dense` which returns a dense tensor (may be too restricted?): materializes zero tensor, applies neg/transpose bits / unwraps tensors  some of this is already done as part of numpy(force=True), but it may make sense make it a separate method", `.detach()` should not be a GradTrackingTensor ?,"The expected behavior should be that .numpy() on a GradTrackingTensor returns a numpy array. In the long term, we'll get rid of GradTrackingTensor and put the metadata directly on the Tensor so this won't be a problem, but there should be a short term fix for this. > are we considering here that the output of .detach() should not be a GradTrackingTensor ? The output of detach() is a GradTrackingTensor because one is able to perform an inplace operation that will make the Tensor require_grad. It doesn't need to be (if it isn't, then an inplace operation that makes the Tensor require grad ends up erroring out), but that certainly changes the behavior.","Ho right, later inplace! Is the `.numpy()` call going into `torch_function`? Is that the expected way to override it for subclasses?","`.numpy()` goes into torch_function. Unfortunately one cannot override for subclasses that go into C++ (either TensorImpl subclass or torch_dispatch subclass), and functorch subclasses do not have torch_function on them. Could either do something like:  use PyOperator (e.g. ""If functorch is active (or modes are active), call this PyOperator, else, something else"").  set torch_function on them"
557,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([MPS] Add testcase for copying cpu tensors into strided mps tensors )ï¼Œ å†…å®¹æ˜¯ (Fixes  CC(Unpredictable behavior between tensors on separate devices (mps and cpu)) If the destination is a strided MPS tensor and the source is a CPU tensor, we cannot perform a blit directly to copy the memory from the CPU tensor into the MPS tensor. We need to scatter the data into the right indices. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,[MPS] Add testcase for copying cpu tensors into strided mps tensors ,"Fixes  CC(Unpredictable behavior between tensors on separate devices (mps and cpu)) If the destination is a strided MPS tensor and the source is a CPU tensor, we cannot perform a blit directly to copy the memory from the CPU tensor into the MPS tensor. We need to scatter the data into the right indices. ",2023-01-06T00:43:44Z,triaged open source Merged ciflow/trunk release notes: mps ciflow/mps,closed,0,12,https://github.com/pytorch/pytorch/issues/91784, please take a look at the failures, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `cpu_to_strided_mps_fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout cpu_to_strided_mps_fix && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `MPS`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `cpu_to_strided_mps_fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout cpu_to_strided_mps_fix && git pull rebase`)"," merge f ""Lint+MPS is green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
204,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Clean up gpt example)ï¼Œ å†…å®¹æ˜¯ ()è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",gpt,Clean up gpt example,,2023-01-04T21:58:09Z,open source release notes: onnx,closed,0,2,https://github.com/pytorch/pytorch/issues/91728, ,I'm leaving some temporary debug code and other lint issue unchanged .. Need another format solely clean up.
753,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Segmentation fault after trying to create a tensor with float values)ï¼Œ å†…å®¹æ˜¯ (I'm running PyTorch with ROCm inside a docker container on Ubuntu 20.04, and PyTorch gives me a segmentation fault whenever i try to create a tensor with float values in it using my GPU (e.g. ). It's weird because it works perfectly fine with int tensors (e.g. ). Also there is no such problem if i use my CPU (e.g. ). Example code to reproduce the error is present below:  The error message is as follows:  I've tried debugging with gdb in an attempt to pinpoint the source of the issue:   Versions  )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,Segmentation fault after trying to create a tensor with float values,"I'm running PyTorch with ROCm inside a docker container on Ubuntu 20.04, and PyTorch gives me a segmentation fault whenever i try to create a tensor with float values in it using my GPU (e.g. ). It's weird because it works perfectly fine with int tensors (e.g. ). Also there is no such problem if i use my CPU (e.g. ). Example code to reproduce the error is present below:  The error message is as follows:  I've tried debugging with gdb in an attempt to pinpoint the source of the issue:   Versions  ",2023-01-04T15:30:05Z,needs reproduction module: rocm triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/91699,Can you please try with the latest nightlies? (As I can not reproduce it on CUDA and ROCM CI is also running tests that exercises the same code)  ,I can confirm.   I've seen this with https://github.com/sniklaus/3dkenburns Using rocm with a Radeon Vega Mobile card (CEZANN) on laptop. ,  Can you both please let us know which gfx architecture your GPUs have? `rocminfo | grep gfx` should provide that info.,Sure. ,"I think that card is not officially supported on rocm. Some people suggest to use the following workaround:  Which seems to solve this particular issue, however later down the line I see the following error: ",close this as the gfx90c is not officially supported
1484,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯([discussion] Analyzing a list of tensors stored as intermediate values / saved_for_backward in autograd graph)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch It's useful for understanding memory usage and if memory can be saved by refatoring to fusion + inplace The parent issue would be  CC([feature request] Caching allocator diagnostics and memory allocation tracing/visualization), here's the scope is only on stored intermediate values in autograd graph. But overall, being able to get a list of all tensors and storages currently allocated (including from C++) is very useful for simple form of memory profiling (even ignoring possible memory overlaps / views etc) E.g. it would be interesting to compare theses lists for vanilla fp32 TransformerEncoder, autocast+bf16 TransformerEncoder, autocast+fp16 TransformerEncoder, BetterTransformer / efficient_attention / flash_attention variants of TransformerEncoder. This utility should also clearly demonstrate the effect of activation checkpointing / CPU offloading techniques; Reversible Transformers / Reversible ConvNets and would be very useful for pedagogical use and debugging Some previous discussion: https://discuss.pytorch.org/t/memorysizeofalltensorsreferencedbyautogradgraph/169227/4  Alternatives _No response_  Additional context _No response_ )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,[discussion] Analyzing a list of tensors stored as intermediate values / saved_for_backward in autograd graph," ğŸš€ The feature, motivation and pitch It's useful for understanding memory usage and if memory can be saved by refatoring to fusion + inplace The parent issue would be  CC([feature request] Caching allocator diagnostics and memory allocation tracing/visualization), here's the scope is only on stored intermediate values in autograd graph. But overall, being able to get a list of all tensors and storages currently allocated (including from C++) is very useful for simple form of memory profiling (even ignoring possible memory overlaps / views etc) E.g. it would be interesting to compare theses lists for vanilla fp32 TransformerEncoder, autocast+bf16 TransformerEncoder, autocast+fp16 TransformerEncoder, BetterTransformer / efficient_attention / flash_attention variants of TransformerEncoder. This utility should also clearly demonstrate the effect of activation checkpointing / CPU offloading techniques; Reversible Transformers / Reversible ConvNets and would be very useful for pedagogical use and debugging Some previous discussion: https://discuss.pytorch.org/t/memorysizeofalltensorsreferencedbyautogradgraph/169227/4  Alternatives _No response_  Additional context _No response_ ",2023-01-04T11:12:40Z,module: autograd good first issue triaged actionable,open,0,9,https://github.com/pytorch/pytorch/issues/91692,It should be possible to implement this with saved tensor hooks: https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html.,"Interesting. Does it also include tensors saved_for_backward from C++, right?", ,"Even without the hooks, if you have the loss, you can just traverse the graph and look for the `_saved_*` attributes similarly to how we do it for torchviz visualization: https://github.com/szagoruyko/pytorchviz/blob/0adcd83af8aa7ab36d6afd139cabbd9df598edb7/torchviz/dot.pyL102L117","Oh, that's very cool. Does it also include any tensors  savedforbackward from C++ (if that exists at all?)? In general, I think, it's worth including such a function in stock pytorch under some torch.testing or torch.profiling as it's a very direct demonstration of what's taking memory","> Does it also include any tensors savedforbackward from C++ (if that exists at all?)? Yes it includes everything stored in the graph! Yes,  is also fleshing out the autograd graph doc. Maybe that would be a good place to share this code sample?","probably assigning some readable names to these intermediate tensors would be a challenge, but even without them, it should be very useful for simple memory profiling as stored activations probably often dominate the used memory!","> probably assigning some readable names to these intermediate tensors would be a challenge If you're ready to use anomaly mode / build a custom Mode / use Module global hooks, you can easily know either the Module or the line of code where the Tensor comes from or the line of code where it was saved tbh.",might be a bit related to memorydebugging utils theme by   especially for demonstrating visually how activation checkpointing / reversible transformers work in terms of memory allocation
438,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(AssertionError: tensor's device must be `meta` when trying to export a fake-initialized module)ï¼Œ å†…å®¹æ˜¯ ( This raises:  Which is strange, as all the tensors are clearly FakeTensors. Seems like a decomp generates some intermediate that is tripped an internal assert.)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,AssertionError: tensor's device must be `meta` when trying to export a fake-initialized module," This raises:  Which is strange, as all the tensors are clearly FakeTensors. Seems like a decomp generates some intermediate that is tripped an internal assert.",2023-01-04T01:15:10Z,triaged module: fakeTensor,open,0,1,https://github.com/pytorch/pytorch/issues/91670,"I tried poking around  the issue is coming from the fact that we have multiple `FakeTensorModes` floating around, and we end up dispatching to an op `torch.mul(a, b)` where `a` and `b` each have different fake mode. We might want to think some more about what multiple fake modes interacting is supposed to mean  since, for example, in dynamic shape world, fake_mode carries a shape env containing guard info (and what happens when there are multiple shape envs floating around?) Just as a quick/incomplete hack, I made a fix for this specific issue (when we change the state of `mode.in_kernel_invocation`, we need to do it for **all** current fake modes attached to any of the input args). It's not landable in its current state though: "
940,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(Support forward hooks with torch.compile)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch This is mentioned in the announcement post, but I couldn't find an issue: https://pytorch.org/getstarted/pytorch2.0/hooks. Creating one explicitly after a chat with . None of forward, forward_pre or backward hooks appear to fire on the compiled module. !image (notebook: https://gist.github.com/kunalb/6c3eae96636f972d73b6357f92935980)  Alternatives _No response_  Implementation Suggestion (taken from  CC(Interoperability with PyTorch hooks)) We should check for hooks, inline them if they exist, and guard on them not changing. To make the guards cheaper, we could add a ""hooks changed"" callback to PyTorch, allowing us to invalidate generated code rather than adding guards. )è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",chat,Support forward hooks with torch.compile," ğŸš€ The feature, motivation and pitch This is mentioned in the announcement post, but I couldn't find an issue: https://pytorch.org/getstarted/pytorch2.0/hooks. Creating one explicitly after a chat with . None of forward, forward_pre or backward hooks appear to fire on the compiled module. !image (notebook: https://gist.github.com/kunalb/6c3eae96636f972d73b6357f92935980)  Alternatives _No response_  Implementation Suggestion (taken from  CC(Interoperability with PyTorch hooks)) We should check for hooks, inline them if they exist, and guard on them not changing. To make the guards cheaper, we could add a ""hooks changed"" callback to PyTorch, allowing us to invalidate generated code rather than adding guards. ",2023-01-03T22:38:23Z,feature module: autograd triaged oncall: pt2 module: functorch module: dynamo,closed,2,9,https://github.com/pytorch/pytorch/issues/91665,(in addition to niceness of supporting hooks) I also suggest that there should also be a way in PyTorch to faithfully declare that no hooks will be added / should be run on a given module + maybe some concept of ownership of module (then for sequential modules with shapepreserving ops can make inplace optimizations in forward and backward). This should allow more free and correct optimization + reasoning + introduction of inplace for more complex/unfusable yet ops: *  CC([discussion] In-place gradient (grad_input) computation for better memory utilisation) *  CC([feature request] Core API for invertible/inplace and flow-like ops + memory-saving (hookless?) reversible sequential container for RevNets to allow for much larger batch-sizes in academic setting),"Note that there are two distinct types of hooks here, which have differing levels of implementation complexity. Forward hooks solely affect the forward execution of the graph, and are relatively simple to support: Dynamo simply needs to know that a module can have forward hooks, and be sure to trace into them. Backward hooks affect the backward execution of the graph. Dynamo cannot trace into them; they execute too late. The only way to trace backward hooks is to promise that they are side effect free (sound to trace), and then directly propagate them to the graph we pass to proxy tensor tracing (so they trace out into the backwards.) Also , which uses them. Also  (specifically `allow_in_graph`)",ğŸ‘   and I have discussed some places to get started on forward hooks.  He may not have had a chance to dive in yet.,"Yeah, tentatively should be able to dive in starting Thursdayfriday. I'm mostly interested in getting forward hooks working.",This is done now. I forgot to close the issue a while back.,"In what version of `pytorch` is this fix available? I tried the same notebook with Pytorch 2.0.1, but the issue persist: !image","+1, does not work for me either.",could you try with the latest nightly? or at least with 2.1? I am pretty sure 2.0 would not have had these fixes.,"yes, it works."
409,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(trainer)ï¼Œ å†…å®¹æ˜¯ ( ğŸš€ The feature, motivation and pitch a trainer please, which obey the rules in **the zen of python.** **bad case: pytorch lightning, fairseq, transformers**  Alternatives _No response_  Additional context _No response_)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",transformer,trainer," ğŸš€ The feature, motivation and pitch a trainer please, which obey the rules in **the zen of python.** **bad case: pytorch lightning, fairseq, transformers**  Alternatives _No response_  Additional context _No response_",2023-01-03T12:37:21Z,triaged,open,0,2,https://github.com/pytorch/pytorch/issues/91624," take a look at PyTorchaccelerated, it was designed with simplicity and transparency in mind; hopefully you agree it follows the zen of Python!",Hi  would you be able to provide more detail please?
2071,"ä»¥ä¸‹æ˜¯ä¸€ä¸ªgithubä¸Šçš„pytorchä¸‹çš„ä¸€ä¸ªissue, æ ‡é¢˜æ˜¯(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model within a company that transforms SSL certificates for security purposes)ï¼Œ å†…å®¹æ˜¯ ( ğŸ› Describe the bug I currently have difficulty using the code in companies with enhanced security when using external networks. **from torch.hub import  download_url_to_file url = 'https://download.openmmlab.com/pretrain/third_party/resnet50_v1c2cccc1ad.pth' download_url_to_file(url)** The following error occurs when I execute the code because the SSL certificate is tampered with by internal security.   File ""C:\Users\user\miniconda3\envs\torch112\lib\sitepackages\torch\hub.py"", line 727, in load_state_dict_from_url     download_url_to_file(url, cached_file, hash_prefix, progress=progress)   File ""C:\Users\user\miniconda3\envs\torch112\lib\sitepackages\torch\hub.py"", line 593, in download_url_to_file     u = urlopen(req)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 216, in urlopen     return opener.open(url, data, timeout)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 519, in open     response = self._open(req, data)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 536, in _open     result = self._call_chain(self.handle_open, protocol, protocol +   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 496, in _call_chain     result = func(*args)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 1391, in https_open     return self.do_open(http.client.HTTPSConnection, req,   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 1351, in do_open     raise URLError(err) urllib.error.URLError:  It seems that there was a similar issue as follows CC(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model) And there is a case of solving the cause in the)è¯·æ ¹æ®ä»¥ä¸Šå†…å®¹æ ‡æ³¨è¿™ä¸ªissue,                ç”¨ä¸€å¥ç®€çŸ­çš„è¯æè¿°è¿™ä¸ªissueç±»å‹æ˜¯bugæŠ¥å‘Šè¿˜æ˜¯å…¶å®ƒç±»å‹ï¼ˆä¾‹å¦‚ç”¨æˆ·æå‡ºéœ€æ±‚ï¼Œè¯·æ•™é—®é¢˜ç­‰ï¼‰ï¼Œè®¾è®¡çš„æ¨¡å‹æˆ–è€…ä¸»è¦å¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œç”±äºä»€ä¹ˆé—®é¢˜å‡ºç°äº†ä»€ä¹ˆbugï¼Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«è¿™ä¸ªå¥å­ï¼Œä¸éœ€è¦åŒ…å«å…¶ä»–å†…å®¹",yi,SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model within a company that transforms SSL certificates for security purposes," ğŸ› Describe the bug I currently have difficulty using the code in companies with enhanced security when using external networks. **from torch.hub import  download_url_to_file url = 'https://download.openmmlab.com/pretrain/third_party/resnet50_v1c2cccc1ad.pth' download_url_to_file(url)** The following error occurs when I execute the code because the SSL certificate is tampered with by internal security.   File ""C:\Users\user\miniconda3\envs\torch112\lib\sitepackages\torch\hub.py"", line 727, in load_state_dict_from_url     download_url_to_file(url, cached_file, hash_prefix, progress=progress)   File ""C:\Users\user\miniconda3\envs\torch112\lib\sitepackages\torch\hub.py"", line 593, in download_url_to_file     u = urlopen(req)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 216, in urlopen     return opener.open(url, data, timeout)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 519, in open     response = self._open(req, data)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 536, in _open     result = self._call_chain(self.handle_open, protocol, protocol +   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 496, in _call_chain     result = func(*args)   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 1391, in https_open     return self.do_open(http.client.HTTPSConnection, req,   File ""C:\Users\user\miniconda3\envs\torch112\lib\urllib\request.py"", line 1351, in do_open     raise URLError(err) urllib.error.URLError:  It seems that there was a similar issue as follows CC(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model) And there is a case of solving the cause in the",2023-01-03T05:54:24Z,triaged module: hub,open,0,8,https://github.com/pytorch/pytorch/issues/91608, Can you help this issue?,I'm looking into it ,"The issue does look similar to  CC(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model), however I'm not sure there is much we (the pytorch maintainers) can do in this case. For  CC(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model) we were able to renew the certificates ( CC(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model)issuecomment585849493) because the URLs were owned by pytorch, but in this case the offending URL is https://download.openmmlab.com/pretrain/third_party/resnet50_v1c2cccc1ad.pth which belongs to the `openmmlab` domain. It is owned by https://github.com/openmmlab/mmcv, not by pytorch. Perhaps you'll have a better chance at getting this fixed by submitting an issue to their repo?"," The example of the website I wrote an example is the web address of openmmlab, as you explained.  when I used pytorch library, I can't download any pretrained models because SSL certificates for all **https** websites are modified onpremise environment with for security reasons. In other words, not only the openmmlab website but also all websites accessed by **https** are occured this issue. Currently, I am executing the program by modifying the source code file **hub.py** directly to avoid this problem.  I suggest that you modify **hub.py** to select true/false whether to check SSL or not.",">  I suggest that you modify hub.py to select true/false whether to check SSL or not. Could you please provide more details about your suggestion? From my current (possibly limited) understanding, it seems that you have a very specific usecase (tampering with SSL certificates). I'm not sure that this warrants adding support for that, but I'm interested in looking into what you have in mind. Meanwhile, does this help?  CC(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model)issuecomment954160699","> Meanwhile, does this help?  CC(SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model)issuecomment954160699 I see that you already had looked into this in your original message (the way it is formatted makes it fairly hard to parse). Since this suggestion seems to work, I think we can consider this problem solved. I don't think we'll introduce a flag in torchhub to handle that, as it seems to be only affecting some restricted scenarios."," It was my mistake to press close above. Some projects allow to choose whether to use the option to troubleshoot SSL problems. When you see an example of the link below, https://github.com/ytdlp/ytdlp This project offers the following options:  > nocheckcertificates         Suppress HTTPS certificate validation If possible, I would appreciate it if you could add a argument to select true/false from **download_url_to_file(url, check_certificate)** function  whether to use ssl or not as the option provided in the above project.", I experience the same issue trying to download ResNet50
