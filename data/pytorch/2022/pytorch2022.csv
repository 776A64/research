rag,Rename `Tensor._storage` to `Tensor.untyped_storage` and update docs,Fixes CC(Deprecation warning in `Tensor.storage()` should suggest alternate API) ,2022-12-27T21:05:13Z,triaged open source module: deprecation Merged ciflow/trunk release notes: distributed (fsdp) topic: deprecation module: python frontend ciflow/periodic module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/91414,Test failures are flaky.," merge f ""flaky test failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[skip ci] Trying to make nt passthrough with PT2 stack,Stack from ghstack:  CC([skip ci] Trying to make nt passthrough with PT2 stack) ,2022-12-27T20:28:01Z,Stale release notes: fx module: inductor module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/91411,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,PyObject preservation and resurrection for `StorageImpl`,"It may be useful for `StorageImpl` to support PyObject preservation and resurrection. TensorImpl has this behavior. When a reference count to the PyObject for a tensor goes to zero, but its associated TensorImpl is still being used, the ownership is flipped so that the TensorImpl hangs onto the PyObject, preventing it from getting deallocated. If the PyObject ever needs to be resurrected again, ownership is flipped again and the PyObject is given back to Python. The Python object will retain all of the properties it had before, since it was never deallocated. See CC(Preserve PyObject even when it is dead from Python side)  ",2022-12-26T23:08:25Z,module: internals triaged enhancement better-engineering,open,0,18,https://github.com/pytorch/pytorch/issues/91395,", since it is subtle but can work for any py object, not just in pytorch","Hey  , I'd like to better understand why this will be useful for storages. One possible use case is if someone wants to add a property to an UntypedStorage that remains alive even after all the Python references to it are deleted. Perhaps like this:  You also mentioned that PyObject preservation would allow us to make weakref work properly. In the podcast, you said that this is useful for creating caches that contain nonowning references to the cached objectsif the cache is the only reference to the object, then allow the object to be freed. Another use case is to store private fields for objects in a separate dictionary that is keyed by weak references, rather than attaching properties directly to the object like in the code example above. I can definitely see why someone would want to do these things with tensors. But I don't know how often people use storages in the first place, let alone if people want to use storages in these somewhat obscure ways. If you're sure that this will be useful, then greatI just want to make sure I understand the idea fully","So, this ask is primarily coming from . Let me dig up his use cases. * https://github.com/pytorch/pytorch/blob/0b255b3f80d9d365b5f4a064c88278270d6bfac4/torch/_subclasses/meta_utils.pyL147L153 we have to manually detect dead storages and run callbacks on them, because weakrefs don't work. * ""well, i mentioned to Greg wanting to calculate the max batch size symbolically. for that, ideally youd set a cleanup (finalize) function on fake tensors storage to tell you a deallocation has happened""; once again, can't do this because weakref doesn't work * ""similarly, for the cuda graph tapes impl (which we're deferring to post release, so no rush on review there or anything), i want to have a weakref to the tensor storage instead of the tensor itself"" (I'm not exactly sure why but it seems plausible) So this is all mostly about weakrefs. I don't think this feature would be useful for users, it'd mostly be a libraryside usage thing.","Got it, that makes sense, thanks!",I will start with writing the standalone example repo first,"I have a working standalone example repo that shows how PyObject preservation can be implemented: https://github.com/kurtamohler/pyobjectpreservation It still needs polishing and more comments in the code, but I think the main idea is there. Let me know what you think I'll go ahead and start implementing this for storages next","The overall structure looks good, I need to carefully read over MyClassBase.cpp but the broad strokes architecture looks great","There seem to be some errors in MyClassBase.cpp. The one I noticed is you're using shared pointer to point to the C++ type from MyClassBase PyObject. But this is not enough; you need to be able to toggle between owning and not owning from Python to C++, because when the ownership goes from Python to C++, having a shared ptr from C++ to Python means cycle aka you will never deallocate. It looks like the code is a little too much simplified from the conversion from intrusive pointer to shared pointer. I recommend preserving intrusive pointer and then also trying to keep more of the original code structure, including helper functions, e.g.","Thanks for the feedback, I'll make those changes",btw is doing the same thing as https://github.com/pytorch/pytorch/pull/81616 also part of this work?,"I'm having some trouble with adding a `PyObjectSlot` member to `c10::StorageImpl`. Here's my diff:  Click to expand   When I try to build this, I get an error that I haven't been able to figure out yet:  Click to expand   , do you happen to know what the problem might be? This stackoverflow question must be relevant, since the error happens at an `emplace_back` call on a vector containing  `TensorImpls`, but adding the suggested constructor defs to `PyObjectSlot` didn't fix the error: https://stackoverflow.com/questions/62751522/resulttypemustbeconstructiblefromvaluetypeofinputrangewhencreating","Nevermind, I understand what was wrong. `std::atomic` is not movable, so I'll have to add something like ","fwiw, we deleted the move constructor/assignment on TensorImpl, and maybe you can also do that for storage. not sure, do it in a separate pr","Another build issue showed up because `PyObjectSlot` doesn't support `std::swap` yet due to the atomic. Seems like making a template specialization for the swap function should fix it. , do you see any problem with doing something like the following?  Click to expand ```cpp include  include  include  include  namespace c10 { class PyInterpreter { }; class PyObject { }; class PyObjectSlot; inline void swap(PyObjectSlot& lhs, PyObjectSlot& rhs); class PyObjectSlot {  public:   PyObjectSlot()     : pyobj_interpreter_(nullptr) {}   PyObjectSlot(PyObjectSlot&& other)     : pyobj_interpreter_(other.pyobj_interpreter_.load()) {}   PyObjectSlot& operator=(const PyObjectSlot& other) {     pyobj_interpreter_ = other.pyobj_interpreter_.load();     pyobj_ = other.pyobj_;     return *this;   }   friend void swap(PyObjectSlot& lhs, PyObjectSlot& rhs);   void set_pyobj_interpreter(PyInterpreter* pyobj_interpreter) {     pyobj_interpreter_ = pyobj_interpreter;   }   PyInterpreter* pyobj_interpreter() {     return pyobj_interpreter_;   }  private:   std::atomic pyobj_interpreter_;   PyObject* pyobj_; }; inline void swap(PyObjectSlot& lhs, PyObjectSlot& rhs) {   std::swap(lhs.pyobj_, rhs.pyobj_);   lhs.pyobj_interpreter_.exchange(     rhs.pyobj_interpreter_.exchange(lhs.pyobj_interpreter_)); } class StorageImpl {  private:   PyObjectSlot pyobj_slot_; }; } // namespace c10 int main() {   // Try moving PyObjectSlot   c10::PyObjectSlot a;   c10::PyInterpreter interp_a;   a.set_pyobj_interpreter(&interp_a);   c10::PyObjectSlot b(std::move(a));   std::vector> v;   v.emplace_back(0, std::move(b));   std::cout ","Swapping atomics is generally dicey business. I would still try to see if we can get rid of swapping on storage first, and discuss alternatives only if we can't easily","Ok makes sense, I'll try getting rid of swaps and moves. Maybe the bare StorageImpls in those places can be replaced with intrusive pointers",Could this mechanism be used with objects exposed with pybind11? We have this problem with PTD that allows for subclassing and it breaks badly.,This requires pretty involved control on the object and its type (see details in https://github.com/kurtamohler/pyobjectpreservation) so that won't work with pybind11 bound objects AFAIK
transformer,"memory location error:  init of LayerNorm with kaiming_uniform_(), xavier_uniform_()."," ðŸ› Describe the bug When initializing the `named_parameters()`, the code raises memory error once it reaches `""norm1.weight""`.  It happens if we try to use `xavier_uniform_()` or `kaiming_uniform_()`.   Versions build version 1.13.0+cu117   C++14 Debug and Release ",2022-12-25T15:45:17Z,module: cpp triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/91379,"Actually, it appears the issue is not with transformer, but with its LayerNorm, or any other LayerNorm. doing this in constructor causes exception to be raised: ","Ok, I did an experiment and found a solid fix.  Creating a custom implementation of LayerNorm **still was causing this error**, which was super weird:  Changing it from {features} to {1, features} suddenly resolved all the memory crashes. I suspect the implementation in the Libtorch library has the same issue so it might be very easy to fix for you.  however, still crashes inside forward() method: ","For further visitors:  Make sure you are using `try{} catch(c10::Error& err)` It will tell you the error which is more descriptive than what seams like a memory crash :) After I wrapped the code in try/catch I found out that `kaiming_init()` indeed didn't like the dimensions of the `LayerNorm.weight`. The `err.what()` contained the message `""Fan in and fan out can not be computed for tensor with fewer than 2 dimensions""` And wrapping the `torch::layer_norm()` in try/catch revealed `""Expected weight to be of same shape as normalized_shape, but got weight of shape [1, 64] and normalized_shape = [64]""`.  This should get me back on track. I need to keep the weight shape as {features} rather than {1,features} and stop using kaiming init for the LayerNorm. "
transformer,Internal Assert failed," ðŸ› Describe the bug I am using an EncoderDecoder model from hugging face with Bertbase uncased as both encoder and decoder. I am getting this by running this code.  from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer training_args = Seq2SeqTrainingArguments(     output_dir=""./"",     learning_rate=5e5,     evaluation_strategy=""steps"",     per_device_train_batch_size=4,     per_device_eval_batch_size=8,     predict_with_generate=True,     overwrite_output_dir=True,     save_total_limit=3,     fp16=True,  ) trainer = Seq2SeqTrainer(     model=bert2bert,     tokenizer=tokenizer,     args=training_args,     compute_metrics=compute_metrics,     train_dataset=train,     eval_dataset=test, ) RuntimeError: false INTERNAL ASSERT FAILED at ""../c10/cuda/CUDAGraphsC10Utils.h"":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32508  Versions 20221224 21:02:31  https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com):44",2022-12-24T21:03:33Z,module: cuda triaged module: cuda graphs,open,0,0,https://github.com/pytorch/pytorch/issues/91375
rag,Support for saving multiple storages/tensors that view same data as different dtypes," ðŸš€ The feature, motivation and pitch Saving and loading multiple tensors or storages that view the same data with dfferent dtypes is not currently possible:  In the past, it would have been pretty difficult to add support for this because of the way storage types were defined (different Python and C++ classes for each storage dtype). But now that storages have been refactored so that they all use the same untyped storage class underneath, enabling serialization of views of the same data with different dtypes should be much more straightforward now  Alternatives _No response_  Additional context _No response_  ",2022-12-22T20:17:51Z,feature module: serialization triaged,open,0,1,https://github.com/pytorch/pytorch/issues/91325,"As mentioned in  CC(zip file serialization endianness detection is broken)issuecomment1405080053, we should still disallow serializing views that would differ on systems that have different endianness"
transformer,[Torch2 CPU] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum," ðŸ› Describe the bug I'm trying to compile a UniXcoder model (variation of BERT) from huggingface transformers on CPU. I use python version '3.8.16' and torch version '2.0.0.dev20221222+cpu'. When performing `model = torch.compile(model)` with the default mode, as well as `mode=reduceoverhead` on a machine with 8GB ram, I encounter the error provided below.  Any idea how to get through it? Thank you!  Error logs   Minified repro _No response_ ",2022-12-22T15:38:38Z,bug oncall: pt2,closed,0,9,https://github.com/pytorch/pytorch/issues/93495,"Do you get the same error when you try `backend=""eager""` or `backend=""aot_eager""` as an option in `torch.compile`?","> Do you get the same error when you try `backend=""eager""` or `backend=""aot_eager""` as an option in `torch.compile`? Thank you for replying. I tried them both, it actually doesn't crash anymore, but  the runtime performance is really bad. I compare the runtime of the original model (without compile) to the model compiled with both `eager` and `aot_eager` backends: **Without compile:** `Total time 28.479018211364746` **eager:** `Total time 104.0422477722168` **aot_eager:** `Total time 175.95780992507935` any idea why?", Do you mind share the reproducer code?,">  Do you mind share the reproducer code? Sure, in the attached zip you'll find:  unixcoder.py (as given by https://github.com/microsoft/CodeBERT/tree/master/UniXcoder)   code.py  simple python script that creates embedding for source code fragments. code.zip",Is there an update on this? I'm having the exact same issue.,"The issue is not caused by `[WARNING] Using FallbackKernel: aten.cumsum`. For `aten.cumsum`, using fallback is on purpose for low efficiency of decomposition  CC([inductor] Lower aten.cumsum). The error above is `Cannot allocate memory`. Actually, I could run the code with inductor mode without error and it seems that the error is due to inefficiency of inductor mode. I compared the performance in four modes.          mode  6.040424   Notice that the first time of compiling mode is usually timeconsuming and it is better to run repeatedly. I've run 10 times and excluded the first performance here. Details of data:         without compile 8.862784147 7.331763983 8.797510624 6.193632603 6.044025898 5.985580683 5.909735918 6.417853832 5.958158016 5.787334919 inductor 522.4309599 18.03655243 15.26723146 15.8309319 15.37905502 14.9801507 14.20492935 14.25522494 13.8550992 14.27873015 14.07980537 eager 76.17847276 5.888897419 5.793441057 5.563881159 5.665488482 5.629543304 6.018662691 5.815788269 5.927585363 6.117486238 6.155511618 aot_eager 142.1525922 6.320661306 5.981152534 5.77075839 5.898153543 5.977357864 5.99329257 6.369976044 6.220510244 5.851990461 6.020391941  ","For the reasons why inductor performs worse than eager, here are some updates. 1. 7500 `mkl_linear` of inductor cost an additional 1s compared to `aten::addmm` of eager; 2. Inductor kernels cannot do vectorization due to `to_type` (float, int, long) 1300 times; 3. Both using the same ops, it is strange that inductor is slower than eager. Also, the number of callings sometimes does not match.     * `aten::bmm`: inductor 629ms 2400; aten 525ms 2400     * `aten::as_strided`: inductor 41ms 13600; 9ms 27500     * `aten::empty`: inductor 35ms 7400; aten 10ms 9100  Profilings **Inductor**                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls           mkl::_mkl_linear        25.21%        4.348s        25.41%        4.383s     600.434us          7300                  aten::bmm         3.65%     628.859ms         3.65%     628.897ms     262.040us          2400           aten::as_strided         0.24%      41.000ms         0.24%      41.000ms       3.015us         13600                aten::empty         0.21%      35.677ms         0.21%      35.677ms       4.821us          7400       graph_7_kernel_cpp_4         0.13%      22.478ms         0.13%      22.478ms       4.496ms             5      graph_10_kernel_cpp_4         0.13%      22.447ms         0.13%      22.447ms       4.489ms             5      graph_7_kernel_cpp_60         0.13%      22.298ms         0.13%      22.298ms       4.460ms             5      graph_7_kernel_cpp_11         0.13%      22.153ms         0.13%      22.153ms       4.431ms             5      graph_7_kernel_cpp_46         0.13%      22.033ms         0.13%      22.033ms       4.407ms             5      graph_7_kernel_cpp_25         0.13%      22.026ms         0.13%      22.026ms       4.405ms             5      graph_7_kernel_cpp_39         0.13%      21.936ms         0.13%      21.936ms       4.387ms             5      graph_7_kernel_cpp_67         0.13%      21.891ms         0.13%      21.891ms       4.378ms             5      graph_7_kernel_cpp_74         0.13%      21.852ms         0.13%      21.852ms       4.370ms             5      graph_7_kernel_cpp_81         0.13%      21.704ms         0.13%      21.704ms       4.341ms             5      graph_7_kernel_cpp_32         0.13%      21.702ms         0.13%      21.702ms       4.340ms             5      graph_7_kernel_cpp_18         0.13%      21.650ms         0.13%      21.650ms       4.330ms             5     graph_10_kernel_cpp_11         0.13%      21.563ms         0.13%      21.563ms       4.313ms             5      graph_7_kernel_cpp_53         0.12%      21.488ms         0.12%      21.488ms       4.298ms             5     graph_10_kernel_cpp_32         0.12%      21.449ms         0.12%      21.449ms       4.290ms             5     graph_10_kernel_cpp_53         0.12%      21.269ms         0.12%      21.269ms       4.254ms             5     graph_10_kernel_cpp_74         0.12%      21.265ms         0.12%      21.265ms       4.253ms             5     graph_10_kernel_cpp_18         0.12%      21.226ms         0.12%      21.226ms       4.245ms             5     graph_10_kernel_cpp_60         0.12%      21.201ms         0.12%      21.201ms       4.240ms             5     graph_10_kernel_cpp_46         0.12%      21.122ms         0.12%      21.122ms       4.224ms             5     graph_10_kernel_cpp_81         0.12%      21.089ms         0.12%      21.089ms       4.218ms             5     graph_10_kernel_cpp_67         0.12%      20.947ms         0.12%      20.947ms       4.189ms             5     graph_10_kernel_cpp_39         0.12%      20.828ms         0.12%      20.828ms       4.166ms             5     graph_10_kernel_cpp_25         0.12%      20.755ms         0.12%      20.755ms       4.151ms             5     graph_29_kernel_cpp_18         0.12%      19.973ms         0.12%      19.973ms       3.995ms             5     graph_29_kernel_cpp_67         0.11%      19.611ms         0.11%      19.611ms       3.922ms             5     graph_29_kernel_cpp_53         0.11%      19.512ms         0.11%      19.512ms       3.902ms             5     graph_29_kernel_cpp_81         0.11%      19.491ms         0.11%      19.491ms       3.898ms             5     graph_29_kernel_cpp_11         0.11%      19.443ms         0.11%      19.443ms       3.889ms             5     graph_29_kernel_cpp_60         0.11%      19.437ms         0.11%      19.437ms       3.887ms             5     graph_29_kernel_cpp_39         0.11%      19.427ms         0.11%      19.427ms       3.885ms             5     graph_29_kernel_cpp_32         0.11%      19.388ms         0.11%      19.388ms       3.878ms             5      graph_29_kernel_cpp_4         0.11%      19.256ms         0.11%      19.256ms       3.851ms             5     graph_29_kernel_cpp_46         0.11%      19.079ms         0.11%      19.079ms       3.816ms             5     graph_29_kernel_cpp_74         0.11%      18.881ms         0.11%      18.881ms       3.776ms             5     graph_29_kernel_cpp_25         0.11%      18.763ms         0.11%      18.763ms       3.753ms             5      graph_11_kernel_cpp_4         0.11%      18.587ms         0.11%      18.587ms       4.647ms             4     graph_11_kernel_cpp_18         0.10%      17.638ms         0.10%      17.638ms       4.410ms             4     graph_11_kernel_cpp_60         0.10%      17.627ms         0.10%      17.627ms       4.407ms             4     graph_11_kernel_cpp_32         0.10%      17.560ms         0.10%      17.560ms       4.390ms             4     graph_11_kernel_cpp_39         0.10%      17.406ms         0.10%      17.406ms       4.351ms             4     graph_11_kernel_cpp_81         0.10%      17.397ms         0.10%      17.397ms       4.349ms             4     graph_11_kernel_cpp_74         0.10%      17.299ms         0.10%      17.299ms       4.325ms             4     graph_11_kernel_cpp_67         0.10%      17.143ms         0.10%      17.143ms       4.286ms             4     graph_11_kernel_cpp_11         0.10%      17.117ms         0.10%      17.117ms       4.279ms             4     graph_11_kernel_cpp_25         0.10%      16.873ms         0.10%      16.873ms       4.218ms             4     graph_11_kernel_cpp_53         0.10%      16.753ms         0.10%      16.753ms       4.188ms             4      graph_30_kernel_cpp_4         0.10%      16.701ms         0.10%      16.701ms       4.175ms             4     graph_11_kernel_cpp_46         0.10%      16.700ms         0.10%      16.700ms       4.175ms             4     graph_30_kernel_cpp_32         0.10%      16.577ms         0.10%      16.577ms       4.144ms             4     graph_30_kernel_cpp_46         0.10%      16.511ms         0.10%      16.511ms       4.128ms             4      graph_27_kernel_cpp_4         0.09%      16.361ms         0.09%      16.361ms       4.090ms             4     graph_30_kernel_cpp_67         0.09%      16.314ms         0.09%      16.314ms       4.079ms             4     graph_30_kernel_cpp_60         0.09%      16.040ms         0.09%      16.040ms       4.010ms             4                  aten::mul         0.09%      15.999ms         0.11%      19.498ms      97.490us           200     graph_27_kernel_cpp_53         0.09%      15.978ms         0.09%      15.978ms       3.994ms             4     graph_30_kernel_cpp_18         0.09%      15.946ms         0.09%      15.946ms       3.986ms             4     graph_30_kernel_cpp_81         0.09%      15.874ms         0.09%      15.874ms       3.969ms             4     graph_30_kernel_cpp_25         0.09%      15.857ms         0.09%      15.857ms       3.964ms             4     graph_27_kernel_cpp_74         0.09%      15.855ms         0.09%      15.855ms       3.964ms             4     graph_27_kernel_cpp_25         0.09%      15.853ms         0.09%      15.853ms       3.963ms             4     graph_30_kernel_cpp_74         0.09%      15.847ms         0.09%      15.847ms       3.962ms             4     graph_30_kernel_cpp_11         0.09%      15.841ms         0.09%      15.841ms       3.960ms             4     graph_27_kernel_cpp_46         0.09%      15.778ms         0.09%      15.778ms       3.945ms             4     graph_27_kernel_cpp_67         0.09%      15.759ms         0.09%      15.759ms       3.940ms             4     graph_27_kernel_cpp_81         0.09%      15.722ms         0.09%      15.722ms       3.930ms             4     graph_27_kernel_cpp_39         0.09%      15.717ms         0.09%      15.717ms       3.929ms             4     graph_27_kernel_cpp_18         0.09%      15.700ms         0.09%      15.700ms       3.925ms             4     graph_30_kernel_cpp_39         0.09%      15.699ms         0.09%      15.699ms       3.925ms             4     graph_27_kernel_cpp_32         0.09%      15.568ms         0.09%      15.568ms       3.892ms             4     graph_27_kernel_cpp_60         0.09%      15.539ms         0.09%      15.539ms       3.885ms             4     graph_27_kernel_cpp_11         0.09%      15.447ms         0.09%      15.447ms       3.862ms             4     graph_30_kernel_cpp_53         0.09%      15.394ms         0.09%      15.394ms       3.849ms             4       graph_1_kernel_cpp_4         0.08%      14.516ms         0.08%      14.516ms       4.839ms             3      graph_12_kernel_cpp_4         0.08%      14.502ms         0.08%      14.502ms       4.834ms             3       graph_2_kernel_cpp_4         0.08%      13.993ms         0.08%      13.993ms       4.664ms             3      graph_1_kernel_cpp_60         0.08%      13.971ms         0.08%      13.971ms       4.657ms             3      graph_2_kernel_cpp_18         0.08%      13.653ms         0.08%      13.653ms       4.551ms             3      graph_2_kernel_cpp_60         0.08%      13.610ms         0.08%      13.610ms       4.537ms             3      graph_2_kernel_cpp_11         0.08%      13.578ms         0.08%      13.578ms       4.526ms             3      graph_1_kernel_cpp_67         0.08%      13.546ms         0.08%      13.546ms       4.515ms             3     graph_12_kernel_cpp_18         0.08%      13.534ms         0.08%      13.534ms       4.511ms             3      graph_15_kernel_cpp_4         0.08%      13.534ms         0.08%      13.534ms       4.511ms             3      graph_2_kernel_cpp_46         0.08%      13.529ms         0.08%      13.529ms       4.510ms             3      graph_2_kernel_cpp_74         0.08%      13.517ms         0.08%      13.517ms       4.506ms             3      graph_1_kernel_cpp_46         0.08%      13.508ms         0.08%      13.508ms       4.503ms             3      graph_1_kernel_cpp_25         0.08%      13.467ms         0.08%      13.467ms       4.489ms             3      graph_5_kernel_cpp_18         0.08%      13.464ms         0.08%      13.464ms       4.488ms             3       graph_5_kernel_cpp_4         0.08%      13.456ms         0.08%      13.456ms       4.485ms             3      graph_1_kernel_cpp_81         0.08%      13.396ms         0.08%      13.396ms       4.465ms             3      graph_1_kernel_cpp_74         0.08%      13.355ms         0.08%      13.355ms       4.452ms             3      graph_1_kernel_cpp_53         0.08%      13.348ms         0.08%      13.348ms       4.449ms             3     graph_12_kernel_cpp_11         0.08%      13.332ms         0.08%      13.332ms       4.444ms             3     graph_15_kernel_cpp_32         0.08%      13.317ms         0.08%      13.317ms       4.439ms             3      graph_5_kernel_cpp_39         0.08%      13.305ms         0.08%      13.305ms       4.435ms             3      graph_2_kernel_cpp_53         0.08%      13.267ms         0.08%      13.267ms       4.422ms             3              Self CPU time total: 17.247s **Eager**                                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                 aten::addmm        52.64%        3.280s        58.06%        3.618s     495.552us          7300                   aten::bmm         8.43%     525.192ms         8.43%     525.192ms     218.830us          2400                   aten::add         7.74%     482.587ms         7.75%     483.054ms     123.860us          3900                   aten::div         6.84%     426.328ms         7.23%     450.471ms     346.516us          1300                 aten::copy_         6.53%     407.151ms         6.53%     407.151ms      37.699us         10800                  aten::gelu         5.66%     352.548ms         5.66%     352.548ms     293.790us          1200              aten::_softmax         4.21%     262.120ms         4.21%     262.120ms     218.433us          1200     aten::native_layer_norm         2.44%     152.096ms         2.56%     159.429ms      63.772us          2500                aten::linear         0.66%      41.319ms        59.65%        3.717s     509.139us          7300                aten::expand         0.47%      29.157ms         0.51%      31.720ms       2.600us         12200             aten::transpose         0.41%      25.438ms         0.48%      29.791ms       3.505us          8500                aten::matmul         0.41%      25.335ms         9.55%     595.245ms     248.019us          2400                  aten::view         0.35%      21.498ms         0.35%      21.498ms       0.877us         24500                     aten::t         0.31%      19.162ms         0.71%      43.998ms       6.027us          7300               aten::reshape         0.29%      17.767ms         0.39%      24.554ms       4.815us          5100               aten::permute         0.25%      15.397ms         0.27%      16.905ms       3.522us          4800          aten::index_select         0.24%      14.656ms         0.26%      15.906ms      53.020us           300                   aten::mul         0.22%      13.540ms         0.26%      16.038ms      40.095us           400              aten::_to_copy         0.20%      12.321ms         0.45%      28.054ms      12.197us          2300                 aten::clone         0.17%      10.861ms         1.44%      89.456ms      74.547us          1200                 aten::empty         0.16%       9.740ms         0.16%       9.740ms       1.070us          9100            aten::as_strided         0.15%       9.316ms         0.15%       9.316ms       0.339us         27500            aten::layer_norm         0.15%       9.231ms         2.71%     168.660ms      67.464us          2500               aten::softmax         0.14%       8.645ms         4.27%     265.976ms     221.647us          1200                    aten::to         0.13%       8.003ms         0.54%      33.950ms      13.580us          2500        aten::_reshape_alias         0.12%       7.332ms         0.12%       7.332ms       1.438us          5100                   aten::sum         0.11%       6.898ms         0.13%       8.194ms      40.970us           200         aten::empty_strided         0.09%       5.850ms         0.09%       5.850ms       2.543us          2300          aten::_unsafe_view         0.08%       4.827ms         0.08%       4.827ms       2.011us          2400                   aten::sub         0.06%       3.961ms         0.07%       4.541ms      45.410us           100            aten::contiguous         0.06%       3.736ms         1.49%      92.583ms      77.153us          1200            aten::empty_like         0.05%       3.253ms         0.10%       6.278ms       5.232us          1200                  aten::add_         0.04%       2.275ms         0.04%       2.275ms      22.750us           100                aten::select         0.03%       2.108ms         0.04%       2.192ms       3.131us           700             aten::embedding         0.03%       1.945ms         0.30%      18.883ms      62.943us           300                 aten::slice         0.03%       1.902ms         0.03%       1.996ms       3.327us           600                  aten::tanh         0.03%       1.873ms         0.03%       1.873ms      18.730us           100                    aten::ne         0.03%       1.745ms         0.03%       1.745ms       8.725us           200             aten::unsqueeze         0.02%       1.406ms         0.02%       1.537ms       3.074us           500                aten::cumsum         0.02%     975.000us         0.02%       1.542ms      15.420us           100                  aten::rsub         0.01%     645.000us         0.08%       5.186ms      51.860us           100               aten::detach_         0.01%     444.000us         0.01%     566.000us       5.660us           100                 aten::fill_         0.01%     328.000us         0.01%     328.000us       1.640us           200                     detach_         0.00%     162.000us         0.00%     162.000us       1.620us           100               aten::type_as         0.00%     139.000us         0.01%     614.000us       6.140us           100          aten::resolve_conj         0.00%      13.000us         0.00%      13.000us       0.001us         19400               aten::dropout         0.00%       9.000us         0.00%       9.000us       0.002us          3700            aten::lift_fresh         0.00%       3.000us         0.00%       3.000us       0.030us           100              Self CPU time total: 6.231s",Inductor mode generates 50 graphs and each one occupies about 1 GB of memory. It is found that inductor uses a high memory due to mkl linear weight prepack which would store an extra copy of the weight.,Perhaps we need a config to disable the weight prepacking for memory savings
transformer,[Torch 2.0] Fake Tensor Fails for codegen model," ðŸ› Describe the bug   Error logs /home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/eval_frame.py:373: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled.Consider setting `torch.set_float32_matmul_precision('high')`   ""TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled."" Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/utils.py"", line 1054, in run_node     return node.target(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_subclasses/fake_tensor.py"", line 915, in __torch_dispatch__     r = func(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_ops.py"", line 284, in __call__     return self._op(*args, **kwargs or {})   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_prims_common/wrappers.py"", line 209, in _fn     result = fn(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_prims_common/wrappers.py"", line 119, in _fn     result = fn(**bound.arguments)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/",2022-12-22T11:33:54Z,bug oncall: pt2,closed,0,6,https://github.com/pytorch/pytorch/issues/91306,"It looks like this model calls `torch.where` with a uint8 conditional tensor, which is deprecated in eager mode(code), but doesn't currently work in our primtorch decomps (code). We should just allow it in the primtorch decomp, and emit the same deprecation warning.","I'm not sure we should allow it in primtorch, or use this as a forcing function to finally be able to deprecate it. ",fair point :)," you should modify your code to use a bool tensor instead of a byte tensor, do you need guidance on how to do so",">  you should modify your code to use a bool tensor instead of a byte tensor, do you need guidance on how to do so If you have a guide. Please send me. ",`dtype` here https://github.com/huggingface/transformers/blame/52dd2b61bff8af5b6409fdd5ec92a9b3114f3636/src/transformers/models/codegen/modeling_codegen.pyL101 has to be changed to `torch.bool`
transformer,`nn.TransformerEncoderLayer` fastpath (BetterTransformer) is slower than the normal path when no mask is provided," ðŸ› Describe the bug Reproduction:    script   Results (fp32):    Versions The above is run on a single A10080GB. PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.24.3 Libc version: glibc2.31 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0125genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.0.221 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA DGX Display GPU 4: NVIDIA A100SXM480GB Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] torch==1.13.1 [pip3] torchaudio==2.0.0.dev20221220+cu116 [pip3] torchtriton==2.0.0+0d7e753227 [pip3] torchvision==0.15.0.dev20221220+cu116 [conda] numpy                   ",2022-12-22T11:09:19Z,module: nn triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/91305,"From looking at the optimum issue, is it safe to say that the slow down is not as severe in the Pytorch TransformerEncoderLayer. And the work around would be to not enable the BetterTransformer path? Better Transformer uses the NestedTensor subclass under the hood. This subclass excels at capitalizing on ragged sequence lengths. When no mask is provided then overhead of the conversion between Dense and NestedTensors can cause slow downs.   That all being said maybe we should guard against the fast_path if no mask is provided and fallback to the functional in that case.    ","Yes it's likely we have a bug / bad implementation on our side in optimum, the slowdown is more severe than here!",Thanks for your guidance    Seems to be fixed on optimum side now: https://github.com/huggingface/optimum/issues/626issuecomment1363412786 ,"Is this still an issue, or can we close this?   "
yi,Converting booleans into floats does not consistently yield exact 0 or 1," ðŸ› Describe the bug **Updated**: Problem solved by using `.isclose()` I wrote a function to compute the fullsequence accuracy of predicted sequences against the target sequences. Part of the function looks like the following:  One thing that is puzzling me is that sometimes `abs_correct` will contain all `False` even though most of the values inside `batch_overlap_rate` is 1. I do not know why I got the following. Is it an error?  If I print `batch_overlap_rate` and `batch_overlap_rate2` out, essentially both are a torch.Tensor of `torch.Size([1000])` and consists of lots of 1.0s, but comparing the former with 1.0 yields only `False`. I checked the data types of both variables. Nothing different. The function is fixed, and this issue only occurs **occasionally** but in a very particular way. Only sequences of certain lengths lead to this problem. There are two other metrics inside this function (e.g., overlap rate), their values are always right.  **Updated**: The problem seems to be linked to floating precision. However, the output of `(Y == Ypred).float()` should always be either 0 or 1, right?   Versions ",2022-12-22T05:53:27Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/91294,I can not reproduce the issue you are providing. If you can produce a standalone script that reproduces the issue that would be very helpful. That being said the problem appears to be due to fp precision issues and any static casts that might be made when converting to and from tensors of different datatypes. I am going to close this issue for now. 
rag,Fix dtype mismatch for unallocated storage deserialization,Fixes CC(Saving and loading unallocated storage and tensor at the same time can result in incorrect dtype) ,2022-12-22T00:30:09Z,module: serialization triaged open source Merged ciflow/trunk,closed,1,10,https://github.com/pytorch/pytorch/issues/91285, I think you might want to check this?,"Yeah, I think when I do CC(Support for saving multiple storages/tensors that view same data as different dtypes) I'll hopefully be able to make the serialization code a little less insane", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / macos12py3arm64 / test (default, 2, 2, macosm112) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,not able to import pipelines as torch.distributed is missing," ðŸ› Describe the bug RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): No module named 'torch.distributed' This is the error while im trying to import pipeline from transformer. My tensorflow is upraded to latesr version 2.11.0 and i am running this on windows 11 on visual code using python 3.10 Detailed Error if this helps: File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\sitepackages\transformers\utils\import_utils.py:1093, in _LazyModule._get_module(self, module_name)    1092 try: > 1093     return importlib.import_module(""."" + module_name, self.__name__)    1094 except Exception as e: File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\importlib\__init__.py:126, in import_module(name, package)     125         level += 1 > 126 return _bootstrap._gcd_import(name[level:], package, level) File :1050, in _gcd_import(name, package, level) File :1027, in _find_and_load(name, import_) File :1006, in _find_and_load_unlocked(name, import_) File :688, in _load_unlocked(spec) File :883, in exec_module(self, module) File :241, in _call_with_frames_removed(f, *args, *",2022-12-20T16:22:11Z,oncall: distributed pipeline parallelism,open,0,2,https://github.com/pytorch/pytorch/issues/91173,How do you install torch?,"Hey sappa, +1 to  's question. And which version of PyTorch are you using? Default distributed support was not enabled until 1.7.0 release. If you are using an earlier version, you might need to build from source. "
finetuning,[FSDP] FSDP with CPU offload consumes `1.65X` more GPU memory when training models with most of the params frozen," ðŸ› Describe the bug Context: We have more and more situations where a large part of the model that's being trained is frozen. As these are very large LLMs, we want to leverage FSDP with CPU offloading to fit such large model training with only a tiny fraction of training params on consumer GPUs. To this end, below is an example of finetuning `bigscience/mt0small` using LoRA parameter efficient finetuning method with/without FSDP. To finetune with FSDP:  1. Following https://github.com/huggingface/accelerate/issues/807, to avoid `AssertionError: expects all parameters to have same requires_grad`, created a custom `auto_wrap_policy` such that the layers with trainable params are in separate FSDP units than those which are frozen. The result model along with FSDP options are given below. We are using Accelerate's FSDP integration . the trainable params have `lora_` prefix:  2. The number of trainable params are given below:  3. Now, the issue is that in comparison to **plain Pytorch**, FSDP consumes **1.65X more GPU memory** instead of reducing the same by a large amount (expectation) while also greatly increasing the memory consumed on CPU. Below are ",2022-12-20T10:49:54Z,high priority triage review oncall: distributed triaged module: fsdp,open,2,20,https://github.com/pytorch/pytorch/issues/91165,"Thanks for providing so many details! This helps us out a lot. We may need more time to investigate why CPU offload consumes so much more GPU memory. For now, have you guys tried setting `limit_all_gathers=True`? We plan to default to this soon. Without this, the memory usage may be unexpectedly high (as long as the CPU thread runs faster than the GPU execution, which is almost always). This may or may not be related to this issue. IIUC, CPU offload was required for the 1T parameter training run (cc: varma ), so the memory savings were proven there. Conceptually, I am not seeing how frozen parameters/layers affect the GPU memory usage aside from the optimizer state as you mentioned. In that case, if you think it is helpful, could you verify what optimizer states are being created when using FSDP? Are you seeing that optimizer states are being created even for `FlatParameter`s that do not require gradient?","Hello , thanks for your quick reply. Apart from the optimizer states only for only the tiny portion of trainable params, offloading to CPU should further reduce GPU memory usage. In case of CPU offloading, the expected behaviour would be to only move model shards (T5 block one at a time as they belong to separate FSDP units) from CPU to GPU, do the forward pass computation and then GPU to CPU (similar to B/w pass), **hence the max memory consumption on GPU should be that of a single T5 block only**. Let me know if there are gaps in my understanding.", Do you have a recommendation for how to try to reproduce this?," From the source code, it seems that another problem of FSDP is that the freezed parameters are not deallocated until the end of the backward. If most parameters are frozen in this case, then the GPU would have a nearly whole copy of the model parameters. ",">  From the source code, it seems that another problem of FSDP is that the freezed parameters are not deallocated until the end of the backward. If most parameters are frozen in this case, then the GPU would have a nearly whole copy of the model parameters. This is unfortunately a known problem :( We would need to add special hooks to reshard the frozen parameters after they are used for gradient computation. We have not had any bandwidth to look into this though.","> >  From the source code, it seems that another problem of FSDP is that the freezed parameters are not deallocated until the end of the backward. If most parameters are frozen in this case, then the GPU would have a nearly whole copy of the model parameters. >  > This is unfortunately a known problem :( >  > We would need to add special hooks to reshard the frozen parameters after they are used for gradient computation. We have not had any bandwidth to look into this though. Is it possible to use a `sized_policy` to deal with those frozen parameters?"," I do not think so. The issue is more fundamental than simply choosing the wrapping policy and comes from when a parameter corresponds to a module whose output activations require gradient but itself does not require gradient  in that case, FSDP's prebackward hook runs but its postbackward hook does not."," thanks for the quick reply. u r right, wrapping policy only effects how to shard the model. hope it'll be optimized in the next version.;)","> We would need to add special hooks to reshard the frozen parameters after they are used for gradient computation. We have not had any bandwidth to look into this though.  any idea about how to add the hook? e.g., torch.autograd.Function."," I am testing https://github.com/pytorch/pytorch/pull/101982/ out. I am seeing memory savings in my experiments, but   is not at the moment. We are investigating."," , just to update the thread as we discussed offline, I could also observe the memory savings as well.","Since https://github.com/pytorch/pytorch/pull/101982 landed, I would recommend people try things out again (e.g. with a nightly). There should be memory savings now.","Thank you  and . Will be trying this out this week and report back. This should work with CPU offloading too, right?"," Yes, it should work with CPU offloading. (It turns out that CPU offloading is connected to ""freeing the parameters"", so once we fixed ""freeing the parameters"", the parameters should be offloaded to CPU as well if enabled.)","> When trying to use FSDP with CPU offloading using bigscience/mt0xxl model (13B params) on a A100 80GB GPU it results in OOM GPU error whereas Plain Pytorch consumes 56GB GPU memory. Tried the latest nightly '2.1.0.dev20230620' Using 2 A100 GPUs, able to run `bigscience/mt0xxl` but doesn't result in any significant memory savings at all compared to Plain PyTorch while consuming a whole lot of CPU memory.   As I stated earlier, the expected behaviour is > Expected behaviour: Efficiently deal with frozen weights during training such that large models could be offloaded on  CPUs/sharded across GPUs properly with storage of the optimizer state only for the trainable parameters, e.g., we can see that using plain PyTorch, mt0xxl (13B params) model takes up 56GB on GPU, now, it would be really helpful if one could do CPU offloading such that training could work on a 16GB or 24GB GPU using FSDP with CPU offloading. Steps to reproduce: 1. Tried the latest nightly '2.1.0.dev20230620' 2. install peft, transformers and accelerate from source. 3. remove line 1316 from `accelerator.py` as FSDP in nightly doesn't support it.  4. Follow https://github.com/huggingface/peftcaveats related to FSDP. Use the below example to track GPU and CPU memory usage: FSDP config:  Code  output logs: ", Can you please share the manually warp code for loraï¼Ÿ I encounter the same problem. Many Thanks!,  Is this issue solved in recent pytorch versions? Should I not bother with trying FSDP for reducing memory in a finetuning task with most parameters frozen?,Still having OOM issue with CPU offload on a 7b model in mixed precision,"Hello, come across this important discussion. In the latest PyTorch Nightlies (for example in the LLaMAreceipes implementation),  runing PEFT with FSDP indeeds saves memory. For example, when using a minibatch size of 4 to train 13B model on 4 GPU (with pure BF16), VRAM per GPU is 25G (PEFT) vs 55g (full model). Could we confirm this issue has resolved in the latest PyTorch Nightlies? Many thanks. Also note to future readers, there are some excellent discussion on this led by :  CC([FSDP] ValueError: `FlatParameter` requires uniform `requires_grad`)  CC(when i try to use FullyShardedDataParallel to finetune a LLM with huggingface peft library, pytorch raise error ValueError: `FlatParameter` requires uniform `requires_grad`)  CC([FSDP] ValueError: `FlatParameter` requires uniform `requires_grad`) Also looking at LLaMArecipes implementation, seems they largely followed the rec from this post). They will use the customized lambda_policy_fn only when using peft. "," There's a lot of issues open regarding FSDP + PEFT. I've been looking at a lot of these discussion threads and unfortunately even with CPU Offloading and a wrapping policy suggested hereissuecomment1626894957), I'm still facing persistent OOM issues. Thisissuecomment1544607463) was another issue we'd like to hear updates on. Is it possible to consolidate everything regarding this topic (current workarounds, PT nightlies, current problems, WIP if any) either on this thread or in a new issue on this repo? That would really help a lot of us currently struggling to evade OOMs with FSDP + PEFT."
transformer,"`torch.compile` with `""inductor""` backend is much slower than `torch._dynamo.optimize`"," ðŸ› Describe the bug I don't quite understand the difference, to me it's only the `_TorchCompileInductorWrapper`. With `torch._dynamo.optimize()`: !image With `torch.compile()`: !image Reproduce using this Dockerfile:  And this script, by commenting the lines  to use one or the other:   Versions   (the , sorry)",2022-12-19T13:06:33Z,oncall: pt2 module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/91095,"LIkely the difference is in enabling/disabling cudagraphs, you can do it with `torch.compile` by passing `mode=reduce_overhead` to torch.compile","Can confirm the difference is in passing `mode=""reduceoverhead""` versus `model=""default""`. Thank you, feel free to close!"
rag,[MPS] Fix tensor with non-zero storage offset graph gathering,"Previously, the ""can slice"" flag in Placeholder constructor in `OperationUtils.mm` is conditioned on whether the numbers of dimensions of base shape and view shape are the same. This doesn't consider the situation that a view tensor could be the base tensor's sliced and then unsqueezed version, resulting in different num of dims. For example, if we want to stack `y_mps` and `x_mps` on the last dim:  the kernel will unsqueeze both of them on the last dim and then concatenate them, which is equivalent to:  `x_mps.unsqueeze(1)` is an unsqueezed and contiguous tensor with a storage offset, this kind of tensors should be sliceable without cloning its storage. Fixes CC(torch.stack gives wrong results on MPS ) Fixes CC([MPS] Incorrect results of two multiplied chunked tensors produced by `unsafe_chunk`) ",2022-12-18T04:31:46Z,triaged open source Merged ciflow/trunk module: mps release notes: mps ciflow/mps keep-going,closed,0,9,https://github.com/pytorch/pytorch/issues/91071," label ""module: mps"""," label ""ciflow/trunk"""," label ""keepgoing""","Modulo the nit, changes look good.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3x8664 / test (default, 1, 3, macos12) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,lint transformer.py,Stack from ghstack:  CC(lint transformer.py),2022-12-16T23:37:11Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/91048," merge f ""fixes a lint that is broken on master"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Trying to extract a concrete int out of a symbolic int," ðŸ› Describe the bug   Error logs   Minified repro I am not sure, if I still need to submit a minified repro since the example I provided is already pretty small and the minified examples don't seem to work for me (`TORCHDYNAMO_REPRO_AFTER=""aot""` says `Input graph did not fail the tester` and `TORCHDYNAMO_REPRO_AFTER=""dynamo""` fails with a `name 's0' is not defined`). Here's a `collect_env` instead: ",2022-12-16T22:35:19Z,triaged bug,closed,0,4,https://github.com/pytorch/pytorch/issues/93487,This is inductor doesn't work with symbolic shapes on master.  plz submit your PR lol,Was the PR submitted/merged for this issue? I am getting this error on `2.0.0.dev20230116+cu116` (nightly).,No still not yet,is fixed now.
rag,*_scatter ops should preserve input stride/storage_offset,"It turns out that we *do* need to update *_scatter ops to return the exact same strides as their inputs. I added a test to `test/test_functionalization.py`, which now trips thanks to Ed's functionalization stride debugging check. It only actually ends up tripping silent correctness if you try to .backward() on that function.   CC(AOT Autograd refactor + cleanup, handle intermediate views of bases, use view replay, fix nontensor input handling)  CC([test] inductor should take storage_offset() into account when cloning inputs)  CC(*_scatter ops should preserve input stride/storage_offset) ",2022-12-16T19:47:50Z,Merged ciflow/trunk release notes: python_frontend topic: bc breaking module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/91029,"Done. I also had to update a ton of expect tests: eagerly applying a mutation back to the base during functionalization was also needed to get that test passing, and causes our graphs to wobble.","This is also (unfortunately) technically BCbreaking, although hopefully: (1) very few people are actually using these *_scatter ops (2) even fewer people rely on the old behavior of `*_scatter(non_contiguous_input)` > contiguous output", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,A Simple Function Causing Graph Break," ðŸ› Describe the bug An error raised when calling `torch._dynamo.optimize` when optimizing huggingface GPT2 and enforcing a single optimized graph for the whole model.  Below is a minimal repro. If `transformers.is_torch_tpu_available()` is removed, then the code works fine.  I'd expect dynamo can just trace through this simple function.  Error logs _No response_  Minified repro _No response_",2022-12-16T18:42:16Z,triaged bug,open,0,1,https://github.com/pytorch/pytorch/issues/93486,The decorator `._dynamo.assume_constant_result` https://github.com/pytorch/pytorch/blob/b0cda0b38cf5e0516269e38314b1290fc1c26779/torch/_dynamo/eval_frame.pyL697 Might be able to work around this issue.  In this case it looks like `transformers.is_torch_tpu_available()` is using `functools.lru_cache`. We should add support for `functools.lru_cache` in cases where a function takes no arguments.  In those cases it should be pretty easy for dynamo to infer that the result of the function is a constant automatically.
transformer,CUDA 12 Support," ðŸš€ The feature, motivation and pitch CUDA was released on December 12 2022, with support for FP8 operations. Getting PyTorch to build using CUDA 12 would unlock performance gains, especially for Lovelace and Hopper architectures, e.g. using Transformer Engine. Magma may need to be updated as CUDA 12 dropped deprecated functions. Relevant issue: Magma: Can't build with CUDA 12 cc:   Related:  CC(Feature request: INT4 format support) (cc: )  Alternatives _No response_  Additional context _No response_ ",2022-12-16T07:13:23Z,module: cuda,closed,0,7,https://github.com/pytorch/pytorch/issues/90988,We are working on the CUDA12 bringup internally and will follow up with needed code changes.,: awesome! Is there a rough timeline on when it'll be available?,> We are working on the CUDA12 bringup internally and will follow up with needed code changes. and python 3.11 please :P ,Is cuda 11.8 getting skipped?, NVIDIA's PyTorch docker container uses CUDA 11.8: https://docs.nvidia.com/deeplearning/frameworks/pytorchreleasenotes/rel2211.htmlrel2211,"> Is cuda 11.8 getting skipped? No, we are working to enable nightly builds for 11.8 here: https://github.com/pytorch/pytorch/pull/90826 > Is there a rough timeline on when it'll be available? We are working on some critical issues we need to fix before enabling the support. ",Closing this in favor of  CC([CUDA][CUDA 12] CUDA 12 Support Tracking Issue) which has a lot more details on the plan.
transformer,TorchDynamo doesn't inline modified nn.Modules forward - Fails with Huggingface Accelerate," ðŸ› Describe the bug Error was raised when running the huggingface BLOOM model with accelerate support for large model after applying `dynamo.optimize`. The example model and code were taken from the PyTorch conference demo. Note that the standard version, without arguments `device_map` and `offload_state_dict`, works fine.  Requirements   Error logs   Minified repro  ",2022-12-16T01:43:34Z,high priority triaged bug oncall: pt2,closed,0,9,https://github.com/pytorch/pytorch/issues/93484,"The root cause of this issue is that we're attempting to run the hooks directly of this module. https://github.com/huggingface/accelerate/blob/ca6505a6a86163dd2aa62ad5b6f24c87079ae372/src/accelerate/hooks.pyL151 we call `old_forward` which is that forward for of the prior `nnmodule` before it has been copied to fake tensors.  I think this is a dynamo issue  we should be inlining and compiling `nn.Module.forward`  when it has been modified, instead of invoking it.   ,  ,    maybe related to : https://github.com/pytorch/pytorch/pull/91018/","I think for  call, we are inlining already: https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/variables/nn_module.pyL220. It seems we doesn't support hooks yet. But anyway, to support HF accelerate is important, I'll take a look. ",Any update on this ? I am hoping this makes it into PT 2.0 official release...,"i don't have much context on this issue, but i wonder if it has been fixed by https://github.com/pytorch/pytorch/pull/92125? It's possible that even if the approach in 92125 helps, it's not enough until we also land https://github.com/pytorch/pytorch/pull/94951 which is blocked by some other ongoing changes. After these changes, we should be tracing module.__call__ and then inlining both the hooks and the .forward.  We should be noticing and recompiling if forward got changed.  (but only if forward of the _original_ module got changed, not OptimizedModule.forward.", has 92125 been merged to mainline ? I can rebuild and check if it fixes my issue.,"yea, merged last week.","> yea, merged last week.   The FakeTensor errors for BLOOM HF are still observed with TOT pytorch (both ROCm and CUDA) **Repro code:**  One finding we have made is that if we do not use HF's `device_map=""auto""` arguement and move the model to a single GPU then the model will complete e.g.  cc: amd  ", are you still planning on working on this?,"Repros don't give the same error anymore, e.g. I get:  If I use `CUDA_VISIBLE_DEVICES=0`, then the repros complete. Closing as the original issue seems fixed."
transformer,Support for Transformer Models on Android with Vulkan Backend," ðŸš€ The feature, motivation and pitch Hello We are currently using a number of different transformer models (plain BERT encoders with attached classification head) on Android. In order to increase the performance of the inference on mobile devices, we're currently evaluating the Pytorch Vulkan backend. We therefore compiled Pytorch from source for desktop to export our model for mobile and specifically for the Vulkan backend. Afterwards, we compiled Pytorch with Vulkan backend for Android, included the resulting aars in our TestingApp and tried running the model. The following error occured:  After some digging around in the source code, we found that the dtype for which there is no corresponding format is `Long`. We assume that the model requires a `Long` as index to the embedding bag. To make sure that nothing was wrong with our custom build, we also tried to run a basic mobilenet v2 model on Android with Vulkan backend and it seemed to work as expected. We therefore assume that this is an unsupported feature.  Just out of curiosity we also tried passing `Int` and `Float` values to the model, just to see what would happen. With `Int` we received th",2022-12-15T14:26:43Z,triaged module: vulkan,open,8,4,https://github.com/pytorch/pytorch/issues/90920, do you have an idea here? :),"JIA  sorry for pinging, but do you have insights on this?","The Vulkan backend doesn't support strides, see: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.hL6",">  ðŸš€ The feature, motivation and pitch > Hello >  > We are currently using a number of different transformer models (plain BERT encoders with attached classification head) on Android. In order to increase the performance of the inference on mobile devices, we're currently evaluating the Pytorch Vulkan backend. We therefore compiled Pytorch from source for desktop to export our model for mobile and specifically for the Vulkan backend. Afterwards, we compiled Pytorch with Vulkan backend for Android, included the resulting aars in our TestingApp and tried running the model. The following error occured: >  >  >  > After some digging around in the source code, we found that the dtype for which there is no corresponding format is `Long`. We assume that the model requires a `Long` as index to the embedding bag. To make sure that nothing was wrong with our custom build, we also tried to run a basic mobilenet v2 model on Android with Vulkan backend and it seemed to work as expected. We therefore assume that this is an unsupported feature. >  > Just out of curiosity we also tried passing `Int` and `Float` values to the model, just to see what would happen. With `Int` we received the same error whereas with `Float` the following error occured: >  >  >  > Can you please confirm that this is in fact an unsupported feature? Is there currently some kind of workaround for this? Is there a plan to support this in the future? >  > Thank you in advance for your reply and have a great day! >  >  Alternatives > _No response_ >  >  Additional context > _No response_ Actually, I encountered the same question, can you please support us ? The Index that embedding need is Long. "
rag,[test] inductor should take storage_offset() into account when cloning inputs,"The problem is that for code like this (existing test: `TestInductorOpInfoCUDA.test_comprehensive_as_strided_partial_views_cuda_float64`)  Inductor has some logic to clone the inputs, but it doesn't respect the storage offset of said inputs. The above example breaks, because we will put torch.ops.aten.as_strided(x, size, stride) into the graph, and the storage_offset will be inferred from the input, to (incorrectly) be 0 instead of 5. We are also forced to use  This used to work before the current refactor, because aot autograd was not passing outputthatviewinput ops into the graph at all, and instead, it would just dump the (hardcoded) sizes/strides of views as outputs into the graph. With the changes in the aot autograd PR, we're now trusting inductor to return us an output alias with the correct metadata. Which... we kinda need to, since we talked a few weeks ago about how inductor can choose its own output memory format, so we need inductor to tell us what the strides of the outputs are in general.   CC(AOT Autograd refactor + cleanup, handle intermediate views of bases, use view replay, fix nontensor input handling)  CC([test] inductor should t",2022-12-14T22:36:06Z,Stale ciflow/trunk module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/90870," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," , I'm interested if you have any thoughts. This fixed an `as_strided` test involving partial views in the inductor op info test suite, but it looks like it's breaking an inductor + distributed test.  Which now fails with a CUDA misaligned address (full paste here)  I'm wondering: (1) do you think that morally, inductor should respect storage offset when its clones inputs? It definitely needs to for this test, although user code with .as_strided() on an existing input view is definitely a corner case that we can xfail. (2) If that's the right fix, then I'm pretty stumped by the distributed test failure. Do you have any tips on where to look?"," is your theory that the change to inductor to respect original offsets is correct, but leads to unaligned addresses for some views?  And these unaligned addresses only upset inductor (but worked when the program ran in eager)?  (And that such cases are happening in FSDP somewhere?)  Any more details you can provide?","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Support arbitrary masks for _nested_tensor_from_mask in nn.TransformerEncoder," ðŸš€ The feature, motivation and pitch I am working on transformers and am using the Better Transformer. To enjoy the speed improvement, we need to pass a src_key_padding_mask to the nn.TransformerEncoder. The problem is that, the src_key_padding_mask has to be leftaligned, which means all True values must be in front of all False values. That is the common case in NLP as sentences have different lengths, but in computer vision, the True values in a mask are usually arbitrarily dispersed. I am wondering if you could support transforming a tensor with an arbitrary mask to a nested tensor, and also support transforming the nested tensor back to a normal tensor with 0 paddings according to the same mask. This feature would be very useful for vision transformers when people want to use masks and enjoy the inference speed improvement for whatever their tasks, for example, dynamic pruning, etc.  Alternatives _No response_  Additional context _No response_ ",2022-12-14T21:37:02Z,module: nn triaged module: nestedtensor,open,0,1,https://github.com/pytorch/pytorch/issues/90866,"Hi  , thanks for the feature request, I have added this to our op coverage tracker CC(General NestedTensor op coverage tracking issue)! "
rag,"Revert ""Revert ""[functorch] Refactor life handle storage (#90317)""""","  CC(Revert ""Revert ""[functorch] Refactor life handle storage (90317)"""") Adds the fix for Wsigncompare. See original PR (https://github.com/pytorch/pytorch/pull/90317) for commit message",2022-12-14T19:50:18Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/90856," merge f ""test failures are unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,FYI: script for counting external PRs,,2022-12-13T21:04:41Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/90794
gpt,Experiment with gpt2,Fixes ISSUE_NUMBER,2022-12-13T18:30:14Z,open source release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/90779
gpt,Experiment with gpt2,Fixes ISSUE_NUMBER,2022-12-13T18:25:55Z,release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/90778
transformer,In distributed get SIGTERM and run crash," ðŸ› Describe the bug Hi everyone, I'm tying to finetune GPT2small on the OpenWebText dataset (A big dataset consist of 40GB+), by running a slightly modified HuggingFace run_clm.py script in distributed. However, I get SIGTERM in different states of the training/inference without further explanation. Didn't find a similar reported issue with a working solution yet so decided to post and ask for your help.  Scheme of how I run my code:     torchrun \     standalone \     nnodes=1 \     nproc_per_node=${NUM_GPU} \     run_clm.py     ddp_timeout 3240000 \     ddp_find_unused_parameters False \     arg_1 arg_2 ... arg_n  My errors:  Error on finetune: When I tried to finetune my model, I suffered from error midway while tokenizing my loaded dataset after successfully tokenized chunk of data, but errored in the next chunk: > Running tokenizer on dataset:   1% 1/81 [00:04     sys.exit(main())   File ""/venv/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/venv/lib/python3.8/sitepackages/torch/distributed/run.py"", line 719, in main     run(args)   File ""/venv",2022-12-12T08:55:18Z,oncall: distributed,open,0,0,https://github.com/pytorch/pytorch/issues/90688
rag,[FSDP][Easy] Move to `_storage()` in test file,"Stack from ghstack:  CC([FSDP][Perf] Preallocate full prec sharded param) [FSDP][Perf] Preallocate full prec sharded param  CC([FSDP][Easy][BE] Minor cleanup of postbackward logic) [FSDP][Easy][BE] Minor cleanup of postbackward logic  CC([FSDP][Perf] Preallocate sharded grad in default stream; save one copy when downcasting grad) [FSDP][Perf] Save one copy when downcasting grad  CC([FSDP][Perf] Preallocate padded unsharded grad in default stream) [FSDP][Perf] Preallocate padded unsharded grad in default stream  CC([FSDP] Sanitize `HandleConfig` for mixed precision) [FSDP] Sanitize `HandleConfig` for mixed precision  CC([FSDP] Tighten postbwd cast to `reduce_dtype`) [FSDP] Tighten postbwd cast to `reduce_dtype` * * CC([FSDP][Easy] Move to `_storage()` in test file) [FSDP][Easy] Move to `_storage()` in test file**  CC([FSDP] Save `_stream_to_name` for debugging) [FSDP] Save `_stream_to_name` for debugging  CC([Reland][FSDP] Another fix for `DTensor`, `use_orig_params=True`) [Reland][FSDP] Another fix for `DTensor`, `use_orig_params=True` This is to silence some deprecation warnings.",2022-12-10T16:07:10Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/90622,"> How come this was not throwing before? It is just that they added deprecation warnings now (no error), so I wanted to migrate.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Fix straggler logging.CODE,"  CC(Handle tensor default func args when inlining)  CC(Fix straggler logging.CODE)  logging.CODE was entirely removed, this was left behind ",2022-12-09T22:06:28Z,ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,11,https://github.com/pytorch/pytorch/issues/90572, accept2ship, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: inductor ,inductor / cuda11.6py3.10gcc7sm86 / test (inductor_timm, 1, 2, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""flaky CI"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 04a3639cb59a8775d78eb82286d8bc832d063495` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job ,someone else fixed it first
transformer,Bugs about BART of Hugging Face using Pytorch 2.0," ðŸ› Describe the bug This is the code of using BART of Hugging Face with Pytorch 2.0:  When using Pytorch 2.0 without `torch.compile` with `device=torch.device('cpu')` or `device=torch.device('cuda')`, it works well without any warning. When using Pytorch 2.0 with `torch.compile` and `device=torch.device('cpu')`, it works well but with the following warning:  When using Pytorch 2.0 with `torch.compile` and `device=torch.device('cuda')`, it doesn't work well with the following errors:   Versions PyTorch version: 1.14.0.dev20221208+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.041genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 GPU 3: NVIDIA Ge",2022-12-09T10:02:27Z,module: crash triaged bug oncall: pt2 module: inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/90537,"  i'm seeing what looks like a really long compile time when i run the above repro on tip of master. dynamo log level DEBUG stops here and hangs or takes a long time (>5min)  also if I ctrl+c, it doesn't exit.",", I see BartForConditionalGeneration pass on the dashboard, do you know what's the difference? ",Closed as error is fixed. Repro with:  Log: 
transformer,TransformerEncoderLayer produce different result with padding," ðŸ› Describe the bug  Ideally, this two cases should produce exactly same result, but they are different. If I change the _record value bigger, like 5k, the difference shrink. I also checked if adding src_mask will aviod the difference, but it produce the same result without scr_mask. I'm wondering if this is the desired result. Thanks for your help!  Versions PyTorch version: 1.12.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: CentOS Stream 9 (x86_64) GCC version: (GCC) 11.3.1 20220421 (Red Hat 11.3.12) Clang version: 14.0.5 (Red Hat 14.0.51.el9) CMake version: version 3.20.2 Libc version: glibc2.10 Python version: 3.7.6 (default, Jan  8 2020, 19:59:22)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.14.0142.el9.x86_64x86_64withcentos9 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3070 GPU 1: NVIDIA GeForce RTX 3070 GPU 2: NVIDIA GeForce RTX 3070 GPU 3: NVIDIA GeForce RTX 3070 GPU 4: NVIDIA GeForce RTX 3070 Nvidia driver version: 515.43.04 cuDNN version: Could not collect HIP runtime version",2022-12-09T09:31:55Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/90534,"I think the difference comes from the multiattention part. According to the paper ""Attention Is All You Needâ€œ, the attention matrix will be divided by sqrt(d_k), which is the sequence len. So adding a padding record will change the value of d_k, thus affect the final result. I think the more reasonable way is to divide the matrix by the sqrt of the length of the unpadding data.",This issue is reproducible.,I made a mistate about src_key_padding_mask. The mask matrix should take True as padding value.
yi,[Dynamo] torch._dynamo.exc.BackendCompilerFailed: tvm raised AttributeError: Can't pickle local object 'WeakValueDictionary.__init__.<locals>.remove'," ðŸ› Describe the bug This looks like subgraph not able to pickle whatever output eager mode is returning. And if I remove `torch.save(result, filename)` at line 111 of `_dynamo/optimization/subgraph.py`, a seg fault is thrown. My repro:  complete stacktrace:   Versions PyTorch version: 1.14.0a0+gitd957829 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.15 (default, Nov  4 2022, 20:59:55)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.053genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 510.85.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn",2022-12-08T23:55:03Z,triaged oncall: pt2 module: dynamo,closed,0,6,https://github.com/pytorch/pytorch/issues/90511,Related to  CC(`make_fx` makes tensors unpickleable) ?," Thanks for the reply! It is possible that this issue is related to CC(`make_fx` makes tensors unpickleable). One odd thing is that this error seems to be a problem of saving `example_outputs` from eager mode while running `tvm` as a backend. But if that is the case, then all other noneager mode backends should have the same error but I can't get a repro on inductor or ts on the same workload."," inductor never tries to pickle the example_inputs/example_outputs, so it won't hit pickling errors. We should fix pickling, but possible workarounds are: 1) Don't save the tensors to disk 2) Copy to a fresh tensor before saving Looks like  has a fix for the other pickling issue in CC(Don't put tracing state on Tensor)  can you test if that fixes this issue too?", Thanks for the reply! I saw that  's PR is merged so I did a rebase but unfortunately this error persists. Should we put in a workaround for non eager mode saving backends for dynamo for now?,"Ideally we would fix pickling. Either way, let's just make SubGraph not save example_outputs() to disk.  I'm not convinced that caching optimization is worth it.",close as this is fixed
transformer,Introduce causal mask,"Summary: Introduce causal mask This PR introduces a causal mask option is_causal, since current custom kernels do not support arbitrary masks. (Also, building these masks is expensive, and avoiding to build them is a significant opportunity since causal mask is extremely common for training Transformer models.) Test Plan: sandcastle & github ci/cd Differential Revision: D41723137",2022-12-08T23:43:49Z,fb-exported Merged release notes: AO frontend,closed,0,41,https://github.com/pytorch/pytorch/issues/90508,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137
rag,Saving and loading unallocated storage and tensor at the same time can result in incorrect dtype,"In CC(move TypedStorage handling to assertEqual), it was shown that the following checks https://github.com/pytorch/pytorch/blob/b0bd5c4508a8923685965614c2c74e6a8c82f7ba/test/test_serialization.pyL688L689 did not previously check if a loaded storage's dtype matches the saved storage, and indeed there are some cases where they do not match. I will look into it and fix it ",2022-12-08T21:33:50Z,module: serialization triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/90497
dspy,"Failure in guard_fail_fn callback - raising here will cause a NULL Error on guard eval Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/site-packages/torch/_dynamo/guards.py"", line 897, in guard_fail_hook     guard_fn.guard_fail_fn(GuardFail(reason, orig_code_map[code])) "," ðŸ› Describe the bug `.compile() class model():      def __init__():           .....      def forward():           return process()      def process():           .... ` [20221209 01:13:39,987] torch._dynamo.guards: [ERROR] Failure in guard_fail_fn callback  raising here will cause a NULL Error on guard eval Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/guards.py"", line 897, in guard_fail_hook     guard_fn.guard_fail_fn(GuardFail(reason, orig_code_map[code])) TypeError: 'NoneType' object is not callable  Versions [20221209 01:13:39,987] torch._dynamo.guards: [ERROR] Failure in guard_fail_fn callback  raising here will cause a NULL Error on guard eval Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/guards.py"", line 897, in guard_fail_hook     guard_fn.guard_fail_fn(GuardFail(reason, orig_code_map[code])) TypeError: 'NoneType' object is not callable ",2022-12-08T17:22:47Z,high priority bug oncall: pt2 module: dynamo,closed,0,1,https://github.com/pytorch/pytorch/issues/90480,I think this issue has already been fixed by https://github.com/pytorch/pytorch/pull/89731 could you reopen if not fixed after tonight's nightly?
yi,[stateless] add weight tying support,  CC([stateless] add weight tying support) ,2022-12-08T16:27:51Z,Merged ciflow/trunk release notes: nn topic: bc breaking topic: deprecation module: functorch,closed,0,5,https://github.com/pytorch/pytorch/issues/90477,"This changed the algorithm from make_functional and from the original PR CC([release] Add warning to stateless.functional_call for deprecated behavior), so some new numbers from that In running a script like this on resnet18 (no tied parameters), this went from 58% slower than vanilla (not using functional_call at all) to 63% slower. This is a 3.5% slowdown from the functional_call before this PR Raw numbers in case anyone wants to see: ","If I'm understanding the numbers, they are measuring some fixed number of iterations of resnet18 using the functional_call API. Shouldn't resnet be faster on GPU than CPU?","> If I'm understanding the numbers, they are measuring some fixed number of iterations of resnet18 using the functional_call API Yep! They're running it 10 times and then this is the average time as reported > Shouldn't resnet be faster on GPU than CPU? Yeah I didn't explain these tables well. From the docs, my understanding is that this reports the amount of time used by the CPU and GPU separately. In this case, what we're worried about is the CPU time since the code for the stateless call happens on CPU"," merge f ""failures from flaky test and unrelated mps test"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
agent,Strange issue with tensor asyncio and RPC," ðŸ› Describe the bug Hi Folks, I have a bizarre issue and can't narrow down a problem.  I'll update a case if you can provide more debug commands for this issue.  platform and system    * Latest torch version, CUDA 11.7, Python 3.10 and 3.11 ( test both)    * Test CPU , CUDA , Linux , MacOs, WSL2 , Docker. Consider on the same host, communicating over RPC, mainly to address the issue of torch.mp. i.e., I need one process to detach the tensor from CUDA to the CPU, send  it to the agent and upload it back to GPU. Thus,  Agent  Observer.   (Observer takes the tensors and sends via async RPC on request from Agent, it async call  and Observer responds as torch.future).  The agent takes the torch future and puts the asyncio queue. Now in the asyncio dequeue phase (i.e, asyncio wait on queue),  does    It looks like somewhere in downstream code, it either has a lock, but on readonly, it works; on lefthand assignment, it fails for tensors. But if that is the case, the last call some_foo (x, y) should produce the same behavior. i.e., if I pass x and y to downstream code, I have an issue now. Does RPC logic in torch, do anything specific with the tensor it rec",2022-12-08T10:36:56Z,oncall: distributed,open,0,0,https://github.com/pytorch/pytorch/issues/90459
yi,`torch.linalg.solve` yields much lower precisions in `1.13.0` than previous versions," ðŸ› Describe the bug After upgrading to `torch 1.13.0`, `torch.linalg.solve` suddenly gives solutions with much lower precisions, **regardless of device (`cpu` or `gpu`) or type (`float64` or `float32`)**. The errors quickly escalate in my numerical calculations and break down my simulations. Take the following data as an example (I know it is somewhat illconditioned, but the changes in behaviors are real)  With `torch 1.12.1`, the relative errors are around machineprecision (a few 1e16), which is *consistent with the precision obtained from `numpy` or `cupy`*  However, **with `torch 1.13.0`, the relative errors are huge (max at 5e11)**   Below are more comparisons using `torch.float64` and `cuda`  And more comparisons using `torch.float32` and `cpu`   Versions For tests with `torch 1.12.1`, the output is  For tests with `torch 1.13.0`, the output is  ",2022-12-08T06:04:45Z,triaged module: linear algebra,closed,0,11,https://github.com/pytorch/pytorch/issues/90453,cc:  who is a linalg guru,That matrix is very badly conditioned:  as such errors of that order are expected.  See https://pytorch.org/docs/master/notes/numerical_accuracy.htmlextremalvaluesinlinalg,"> That matrix is very badly conditioned > as such errors of that order are expected. See https://pytorch.org/docs/master/notes/numerical_accuracy.htmlextremalvaluesinlinalg Yes, they are illconditioned but are helplessly, physically motivated. Do you mind elaborating a bit what have changed in `0.13.0`? Are there some settings that I can use to make `torch` produce results similar to previous versions?  "," I tried to find what have changed in the blog posts but didn't find any clue behind this change of behaviors.  With `torch 1.12.1` or earlier, I can obtain the same answer in my test use cases from either `torch`, `numpy`, or `cupy` (including the example given above). And I've already spent quite some time to adapt my code from using `numpy` to `torch` to take advantage of the GPU acceleration.  Thus, I do wonder if this precision change is somewhat permanent or is there some hope on revival of old behaviors?"," I compiled PyTorch from source and can confirm that the commit 54949a5abc9890143de4b5dd2f13ff98446376a3 is responsible for this behavior change. **Here I want to argue that, even if the matrix is illconditioned, the solution should be around machineprecision** (one may question the correctness of the solution, but that's another issue).  Using `torch` with one commit prior (65a37923f9b14c7c9e80535d771ef9e4e92d0502) gives the expected results  Using `torch` with 54949a5abc9890143de4b5dd2f13ff98446376a3 gives the new results  However, my current knowledge is not enough to understand what had changed in the commit 54949a5abc9890143de4b5dd2f13ff98446376a3. Any ideas/suggestions would be greatly appreciated!","That commit is mostly a better engineering commit. Now, it introduces an optimisation. To avoid performing a copy of the matrix `A`, if `A` is Ccontiguous (rowmajor), we factorise `A^T` instead and then call `lu_solve` with `adjoint=True`. It may be the case that the backend library (the relevant BLAS implementation that you use) is a bit less precise when called with `adjoint=True`. In my opinion, errors of that order are reasonable for linear algebra. As discussed in the note linked in  CC(`torch.linalg.solve` yields much lower precisions in `1.13.0` than previous versions)issuecomment, it is expected that different different devices and different backends give marginally different solutions. What I would suggest if you do care about these is that you try to find a small repro in C / C++ and you report the accuracy mismatch to the relevant BLAS implementation that you are using."," Thanks a lot for the explanations. Sorry if I misunderstand something, but I am not using a specific BLAS implementation  the error seems to be quite consistent across *very* different devices/platforms and BLAS backends (`mkl/cuda/generic/open`, see tests below). Is there any way that I may confirm the issue is due to some BLAS implementation? Thanks you again in advance.  **1**. The test results in the first comment of this issue were from **WSL2 Ubuntu 20.04**, a laptop with Intel i711800H + RTX 3070 Mobile, where the `cpu` results used `MKL` and the `cuda` results used (I think) `cuBLAS` (if not, I guess it is something in `MAGMA`). Below shows the config:  **2**. The test results in the sixth comment (where I found the exact commit responsible for this issue) were from a **CentOS 7 server** with Intel(R) Xeon(R) CPU E52650 v2, where I only (compiled and) used `cpu` with `BLAS_INFO=generic` and `USE_MKL=OFF`. Below shows the config:  **3**. On a **MacBook Pro with M1 Pro**, I can also reproduce the errors (from either `torch` installed via `pip` or `torch` compiled by `MacPorts`)  **4**. On a **Ubuntu 20.04 server with ARM NeoverseN1**, I can also reproduce the errors with `OpenBLAS`  **5**. On a CentOS 7.9 server **but with AMD EPYC 7552 + Nvidia Tesla V100SPCIE32GB**, I can also reproduce the errors  I can do more tests on more platforms/backends if necessary (if these turn out to be useful tests).","Thank you for the rather comprehensive dissection! Alas, I still think that this error is coming from the backend implementation. As you can see in that PR, we dispatch to the same backends, only that in the initial PR, when the matrix A is rowmajor, we copied it into a column major matrix (which is slow) and then called the backend `getrs` with `trans='N'`. What that PR introduces is the optimisation of, rather than copying `A` into a columnmajor matrix, we simply call `getrs` with `trans='T`'. This is mathematically equivalent, but it's clear from your experiments that this codepath may not be as numerically stable when called with illconditioned matrices as the `trans='N'` path. Again, here we are simply calling the given backends and perform a semanticpreserving optimisation as per the backend docs. I think this particular discrepancy should be reported to the relevant backends, given that you have a concrete reproducer. Note that having less wellmaintained paths in some of this libraries is not uncommon. We have found during the years many bugs in different libraries. For example, for MAGMA we discovered that the path `trans='T'`in `getrs` was buggy and I had to implement this option in terms of the `trans='N'` option in https://github.com/pytorch/pytorch/pull/77634/. See in particular https://github.com/pytorch/pytorch/blob/983d4f6fbb51381faa4af651f49142f45c834f8d/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cppL2460L2462","Yo can also see the bounds in the `getrs` docs about the guarantees given by the CPU backend. Given the condition number of the given matrix, I would not be surprised that the provided solutions are indeed within the range specified there. Now, if you want to recover the previous implementation, what you can do is to bypass the optimisation that that PR added altogether by making the matrix columnmajor as per:  You can see that by doing this you get the same results as with PyTorch 1.12. Now, as in PyTorch 1.12 (before that speed optimisation was implemented, you have to copy the data once.  ",Thank you very much for the explanations and the workaroundðŸ‘! My apologies for the late response.  Lesson learned that there are various codepaths maintained at different levels (indeed fascinating ðŸ˜‚).,Closing this. Feel free to reopen if you have further issues.
rag,RuntimeError: Placeholder storage has not been allocated on MPS device!," ðŸ› Describe the bug I get an error every time I attempt to use MPS to train a model on my M1 Mac. The error occurs at first training step (so first call of `model(x)`). MRE:  I receive the following traceback:   Versions  Also note if relevant I'm running Mac OS 13.0. I also have tried this on the 1.13 stable release, same issue. ",2022-12-08T03:04:44Z,triaged module: mps,open,8,5,https://github.com/pytorch/pytorch/issues/90440,"I don't think this is a bug in PyTorch! You haven't allocated your torch.zeros to the device in the forward pass. If you do that, it runs, at least for me. `    def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) out, _ = self.lstm(x, (h0, c0)) out = self.fc(out[:, 1, :]) return out`","> I don't think this is a bug in PyTorch! You haven't allocated your torch.zeros to the device in the forward pass. If you do that, it runs, at least for me. > ` def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) out, _ = self.lstm(x, (h0, c0)) out = self.fc(out[:, 1, :]) return out` Thatâ€™s indeed correct . We see these errors when all the tensors are not mapped to the device . Also there were some bugs in LSTM layer which got fixed in 2.0 release. I would recommend  to try that latest release with MacOS 13.3 OS version ","Dear Kulin, Thank you! He needs to normalise his training loss too, but ... The error that is of most interest to me right now is the *""aten::empty.memory_format""* that seems to arise whenever I switch a model that works perfectly well (but slowly) on the CPU (intel i9 I 8core; running *whisper*) to the ""mps"" device (*AMD Radeon Pro 5500M*). It throws an error somewhere reported as lying around lines 114345 of the *module.py* buried deep in the anaconda3/lib/ .../sitepackages/ storage, but there is no reference to this ""*aten*"" process anywhere in that file. My supposition is that I have done something stupid/ignorant in innocence, or there is some kind of hardware (memory) limitation, but nobody seems to have an answer. It may be to do with *SparseMPS*, but I frankly doubt it. So I started reading and wonder whether this hypothesis has any value: this error arises for me running whisper, and I am loading the models programmatically, but when I am using the ""mps"" device it will try to load them *using the mps*, and if there is some element of quantization involved in this, that will create an error because *quantization is supposed to be done on the CPU*. That I am trying to run these models on a 16GB MacOS Ventura 13.3 makes me wonder whether there is some kind of ""*automatic quantization optimization*"" going on that tries to make the models smaller because I don't have enough RAM. I hesitated to post this because I don't know enough about quantization, or indeed anything else, but is it possible that this is the issue and that it is somehow registering as a different error couched in terms of ""aten::...."" because the code at module.py 114345 looks more like a quantization process than some sort of backprop differentiation? If this has enough legs to merit being posted as an issue, I'll happily do so, but my default assumption is that I am an idiot! Apologies if this just confirms my stupidity! Best, John On Wed, Apr 12, 2023 at 2:20â€¯PM Kulin Seth ***@***.***> wrote: > I don't think this is a bug in PyTorch! You haven't allocated your > torch.zeros to the device in the forward pass. If you do that, it runs, at > least for me. > def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), > self.hidden_size, device=device) c0 = torch.zeros(self.num_layers, > x.size(0), self.hidden_size, device=device) out, _ = self.lstm(x, (h0, c0)) > out = self.fc(out[:, 1, :]) return out > > Thatâ€™s indeed correct . We see these errors when all the tensors are not > mapped to the device . Also there were some bugs in LSTM layer which got > fixed in 2.0 release. I would recommend  >  to try that latest release with MacOS > 13.3 OS version > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >","I fix it by adding device before when create each network, device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"") self.lstm = nn.LSTM(..., device=device)","I ran into a similar issue, got the same error message ""RuntimeError: Placeholder storage has not been allocated on MPS device!"" when using a LSTM model. All tensors and the model were correctly mapped to the device in the code. However, my code worked fine when I updated torch to version 2.2.1 (I was using version 2.0.1) I have M1 Mac and was using the ""mps"" device. Before I updated torch, I tried to run on ""cpu"" device and then the output tensor from the forward pass contained NaN. I didn't look into it since this issue is fixed in latest versions of torch :)"
transformer,Fakify params and weights under private config,"  CC(Fakify params and weights under private config) Previously, we planned to lift the parameters and weights while exporting and implement our own transformer to ""unlift"" the lifted weights and params back to the graph as attributes. But this is bit challenging because:  We need to maintain correct ordering for weights and parameters that are passed as inputs so that we know how to map them back.   Some weights are unused in the graph, so our transformer needs to be aware of which weights and parameters are not used in the graph. And we need to distinguish which are real user input and which are parameters.   There can be more edge cases we haven't seen in other models yet.  I am aware that   and  mentioned that functionalization won't work with faketensor attributes but this is fine for the short term as we don't expect users to be modifying weights and params in inference mode. In fact, we explicitly disable attribute mutation in torchdynamo export mode right now.  Given above condition, it might be ok to just fakify params when we need. I use a flag to guard against this change. Differential Revision: D41891201",2022-12-07T21:55:58Z,Merged ciflow/trunk release notes: fx ciflow/inductor,closed,0,15,https://github.com/pytorch/pytorch/issues/90417,"Yeah I think this will be helpful to unblock all our use cases, including ASR and DPE models, since all models have weights. Right now it's nonscalable on our side to unlift params and buffers, because we have to handle so many corner cases.","I think I've mentioned (to  ?  ?) that I'm open to the idea of automatically fakeifying real tensors when in fake tensor mode in general, and then we would not have to guard this under a config flag. To understand if this is safe or not, we have to understand why we didn't add support for this initially. Consider the following program:  What exactly is this program supposed to do? Naively, we would perform a resize_ on the original tensor x, but this is problematic, because the entire point of fake tensor mode is to avoid doing real compute, and a x.resize_(10) would cause real memory to be allocated. However, nor can we *not* do a resize_ on the original tensor x, because the subsequent computation on x needs the updated metadata from the resize call. It sounds like you don't need this case. I think we'd be OK with throwing if this happens. Can we do this, instead of guarding this in private config?",We should also consider the api for if the user will want to make the lifted inputs static or dynamic,"> I think checking for inplace views is sufficient here > I think I've mentioned (to  ?  ?) that I'm open to the idea of automatically fakeifying real tensors when in fake tensor mode in general, and then we would not have to guard this under a config flag. To understand if this is safe or not, we have to understand why we didn't add support for this initially. >  > Consider the following program: >  >  >  > What exactly is this program supposed to do? Naively, we would perform a resize_ on the original tensor x, but this is problematic, because the entire point of fake tensor mode is to avoid doing real compute, and a x.resize_(10) would cause real memory to be allocated. However, nor can we _not_ do a resize_ on the original tensor x, because the subsequent computation on x needs the updated metadata from the resize call. >  > It sounds like you don't need this case. I think we'd be OK with throwing if this happens. Can we do this, instead of guarding this in private config? Are you referring to mutation on global variable? I think local mutations on the inputs should be handled by functionalization right? ","Mutation on a global variable, but in this case it refers to parameters which you didn't fakeify.","i'm going to let  drive the review, holler if you need second opinion"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", rebase , successfully started a rebase job. Check the current status here,"Successfully rebased `gh/tugsbayasgalan/83/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/90417`)"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Make Transformers compilable by C++17,"`register` keyword is removed in C++17, but keeping it there under ifdef as I have not measured the perf implication on older compiler, though there shouldn't be any: all modern compilers supposed to downright ignore it. This code originates from https://github.com/facebookresearch/xformers/pull/375 will propose similar PR to remove register keyword usage to that repo. Yet another thing discovered while working on https://github.com/pytorch/pytorch/pull/85969",2022-12-07T16:03:08Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/90389," merge f ""All CUDA builds are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[Vulkan] Enable copying QInt8 and QInt32 tensors from cpu to vulkan.,"Summary: Copying QInt8 and QInt32 from cpu to vulkan:   Added shader nchw_to_image_int8   Added shader nchw_to_image_int32 Copying QInt8 and QInt32 from vulkan to cpu Note: This functionality is currently disabled until issues on Android are resolved.  Added shader image_to_nchw_int32  QInt8 works with the same existing image_to_nchw_quantized shaders Added multiple tests for each supported dtype:  cpu_to_vulkan_and_dequantize: These tests check the correctness of copying quantized cpu tensor to vulkan by comparing the output of the following:    cpu float tensor > quantize > to vulkan > dequantize > to cpu    cpu float tensor > quantize > dequantize  cpu_to_vulkan_and_vulkan_to_cpu (currently disabled until copying vulkan quantized to cpu is enabled): These tests check the correctness of copying from cpu to vulkan and from vulkan to cpu by creating a random cpu float tensor, quantizing it, then copying it to vulkan, then back to cpu and comparing the output tensor to the original quantized tensor.  quantize_per_tensor_and_vulkan_to_cpu (currently disabled until copying vulkan quantized to cpu is enabled): These tests check the correctness of copyin",2022-12-07T04:34:15Z,fb-exported Merged ciflow/trunk release notes: vulkan,closed,0,3,https://github.com/pytorch/pytorch/issues/90357,This pull request was **exported** from Phabricator. Differential Revision: D41654287, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[functorch] Refactor life handle storage,"Stack from ghstack:  CC([functorch] Refactor life handle storage) A ""life handle"" is a pointertoboolean that says whether or not a TensorWrapper is alive. A TensorWrapper is alive if we are currently inside of its corresponding transform. An Interpreter is alive if we are currently inside of its corresponding transform. I.e., for vmap(f)(x), the BatchedTensor(x, level=1) is alive inside of the execution of f; and the corresponding VmapInterpreter is alive inside of f. Previously, there was a global map of level to life handle. It is possible to get into a state where we have multiple levels that refer to different Interpreters (if the implementation of an operator calls into functorch) and that messes up the global map. This PR changes it so that  every Interpreter holds a life handle that says if it is alive  to construct a TensorWrapper, one must either (a) directly pass it a life handle, or (b) one must create the TensorWrapper when the corresponding Interpreter is on the stack (and we will automatically grab the life handle by indexing into the DynamicLayerStack with the level) (a) is more robust so I changed most of our C++ callsites to do that",2022-12-06T20:34:05Z,Merged Reverted ciflow/trunk topic: not user facing,closed,0,13,https://github.com/pytorch/pytorch/issues/90317,"> Should we just pass the interpreter instead? This might also make it easier to change the callsites for _make_tensor_wrapper. However, if we want to this all could also be done separately I agree that would be better. I'll file an issue for followup and merge this first to unblock the autograd.Function work. The reason why I didn't want to handle changing the callsites for `_make_tensor_wrapper` in this PR is that we would need to do some redesign of the functorch pythonside API that might rabbithole into changing the pythonside API to look like modes:  How does the developer grab the interpreter? They could grab from the top of the stack (like how pyfunctorch is doing it), or `_grad_increment_nesting` could return an interpreter instead of a level.  The ideal API for entering/exiting a level is not to use `_grad_increment_nesting/_grad_decrement_nesting`, it is to create a GradInterpreter object (mode context manager?) and then `__enter__` it. To create a GradTensorWrapper, we'd probably just call some method on the GradInterpreter (GradInterpreter.lift) instead of having a function `_wrap_for_grad(Tensor, Interpreter)`.","Sorry, I think I misunderstood your comment. To clarify, are you suggesting that in C++, we change the makeTensorWrapper API to accept an Interpreter instead of a (current_level, life_handle) pair because it's possible that the life handle is from the wrong level?","> To clarify, are you suggesting that in C++, we change the makeTensorWrapper API to accept an Interpreter instead of a (current_level, life_handle) pair because it's possible that the life handle is from the wrong level? Ahh yes so there were two parts baked in there. The main suggestion was at the C++ have `makeTensorWrapper` accept an interpreter so that (current_level, life_handle) can never be from different interpreters. At the Python level, I was thinking that this could mean `_grad_increment_nesting ` could return an interpreter instead of an int (until we get modes). This would mean exposing interpreters to the python API which could be done safely but definitely might be more of a pybind headache than it's worth given that we want to move to modes soon","> Ahh yes so there were two parts baked in there. The main suggestion was at the C++ have makeTensorWrapper accept an interpreter so that (current_level, life_handle) can never be from different interpreters. Cool, yeah, I agree this is good to avoid footguns. I've added that in the latest update. > At the Python level, I was thinking that this could mean _grad_increment_nesting could return an interpreter instead of an int (until we get modes). This would mean exposing interpreters to the python API which could be done safely but definitely might be more of a pybind headache than it's worth given that we want to move to modes soon Punting this to the future, I'll file an issue to track it"," merge f ""failures are unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ",This seems to be causing all internal contbuilds to fail with the following signcompare issue:  More details in D42019543. ," revert m ""Causing contbuilds to fail when pytorch is built with Wsigncompare internally  details in D42019543"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,   can we consider turning on Wsigncompare in the OSS PyTorch build to avoid things like this in the future?,">    can we consider turning on Wsigncompare in the OSS PyTorch build to avoid things like this in the future? I would love to do this. There are *a lot* of violations currently though and we'd need a library that can handle sign mismatch arithmetic correctly. I believe there's something proposed for c++20 that we can get a backport of. Basically it will be no cost when it can be (e.g. uint32 compared with int64), but will have a predictable branch when doing things like int32 vs uint64.. https://en.cppreference.com/w/cpp/utility/intcmp In practice, we can turn this on incrementally throughout the codebase to make it a bit more tractable.",> There are a lot of violations currently though Is there a reason why the other violations aren't triggering the Wsigncompare ?
transformer,inductor(CPU): support vectorization of scalar tensor add when the scalar tensor dtype is not float,"  CC(inductor(CPU): support vectorization of scalar tensor add when the scalar tensor dtype is not float) For HF transformer model, there has an NumPy operator which introduces an add: float tensor + double scalar tensor:  and inductor always convert the NumPy.number to a scalartensor which has a double dtype, and it  generates a nonvec code:  After this PR:  the current solution is that manually cast double to float, but I think a better method is that to fully support vectorization of dtype conversion, i.e. make **to_dtype** can be vectorized as aten side. ",2022-12-06T14:23:24Z,open source intel module: inductor ciflow/inductor release notes: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/90283," , we should add a pass to convert the double to float. I'm thinking of adding a dedicated optimization pass to apply potential optimizations one by one. Currently, the conversion from double to float is a candidate and vectorization is another candidate and the two optimizations should be independent. We need to decouple the different optimizations.","Otherwise, I'm kind of concerned it is not sustainable and hard to be maintained.","> Otherwise, I'm kind of concerned it is not sustainable and hard to be maintained. Yes, the vectorization of **to_dtype** is the next step.", Please request my review when the PR is ready.,"  please note: today is last day for cherrypicking, we will have to remove this PR from milestones"
yi,[Vulkan] Partially fix and then disable copying of vulkan quantized tensors to cpu,"Summary: Before this diff, copying of vulkan quantized tensors to cpu was broken. This was mainly caused because the shader only works properly with specific global and local work group sizes, and those specific sizes had been modified in earlier refactoring. As part of this fix, an optimized version of the shader that performs the copying was written, to take advantage of the special case when the plane size (x*y) is multiple of 4). After fixing this, and writing comprehensive tests, it was discovered that the copying still has issues on Android for specific input sizes, e.g. [1, 1, 11, 17]. These issues are currently unresolved, so, copying of quantized vulkan tensors to cpu has been disabled. What is contained in this diff?  Fix for existing issue  New optimized shader (image_to_nchw_quantized_mul4)  New comprehensive tests (which have been disabled)  Disable the copying of quantized vulkan tensors to cpu until issues on Android are fixed. Test Plan: On Mac  On Android  Reviewed By: kimishpatel Differential Revision: D41047098",2022-12-06T11:42:22Z,fb-exported Merged ciflow/trunk release notes: vulkan,closed,0,5,https://github.com/pytorch/pytorch/issues/90275,This pull request was **exported** from Phabricator. Differential Revision: D41047098,This pull request was **exported** from Phabricator. Differential Revision: D41047098,This pull request was **exported** from Phabricator. Differential Revision: D41047098, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Add `TORCH_FAKE_TENSOR_DEBUG` use it to enable storage of traces on fake tensors at init time,  CC(Add `TORCH_FAKE_TENSOR_DEBUG` use it to enable storage of traces on fake tensors at init time) ,2022-12-05T20:38:08Z,Merged topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/90215," merge f ""unrelated XLA failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Godel Model fails to compile (python 3.11), ðŸ› Describe the bug I was trying out PyTorch 1.14/2.0 and installed the latest nightly build. I then used the new `torch.compile()` method and integrated it into my code like described here:  Here is a small code snipit that reproduces the error with GODEL (the model I'm working with):  Fails with the error:   Versions  ,2022-12-05T19:22:18Z,triaged has workaround oncall: pt2,closed,0,6,https://github.com/pytorch/pytorch/issues/90203,"would you be able to run the minifier and post the results for us? https://pytorch.org/docs/master/dynamo/troubleshooting.html if so that helps triage, if not we can still try to repro based on the above. thanks in advance! (see section ""Minifier for any backend"" since this looks like a dynamo issue not a backend issue) set environment variable `TORCHDYNAMO_REPRO_AFTER=""dynamo""`","It could be related to python version. We are not even running the model, the error happens during the import. Is it possible to open a python prompt and run these commands? ~~~ >>> import dis >>> dis.opmap[""JUMP_ABSOLUTE""] 110 >>> dis.opmap {'POP_TOP': 1, 'ROT_TWO': 2, 'ROT_THREE': 3, 'DUP_TOP': 4, 'DUP_TOP_TWO': 5, 'ROT_FOUR': 6, 'NOP': 9, 'UNARY_POSITIVE': 10, 'UNARY_NEGATIVE': 11, 'UNARY_NOT': 12, 'UNARY_INVERT': 15, 'BINARY_MATRIX_MULTIPLY': 16, 'INPLACE_MATRIX_MULTIPLY': 17, 'BINARY_POWER': 19, 'BINARY_MULTIPLY': 20, 'BINARY_MODULO': 22, 'BINARY_ADD': 23, 'BINARY_SUBTRACT': 24, 'BINARY_SUBSCR': 25, 'BINARY_FLOOR_DIVIDE': 26, 'BINARY_TRUE_DIVIDE': 27, 'INPLACE_FLOOR_DIVIDE': 28, 'INPLACE_TRUE_DIVIDE': 29, 'RERAISE': 48, 'WITH_EXCEPT_START': 49, 'GET_AITER': 50, 'GET_ANEXT': 51, 'BEFORE_ASYNC_WITH': 52, 'END_ASYNC_FOR': 54, 'INPLACE_ADD': 55, 'INPLACE_SUBTRACT': 56, 'INPLACE_MULTIPLY': 57, 'INPLACE_MODULO': 59, 'STORE_SUBSCR': 60, 'DELETE_SUBSCR': 61, 'BINARY_LSHIFT': 62, 'BINARY_RSHIFT': 63, 'BINARY_AND': 64, 'BINARY_XOR': 65, 'BINARY_OR': 66, 'INPLACE_POWER': 67, 'GET_ITER': 68, 'GET_YIELD_FROM_ITER': 69, 'PRINT_EXPR': 70, 'LOAD_BUILD_CLASS': 71, 'YIELD_FROM': 72, 'GET_AWAITABLE': 73, 'LOAD_ASSERTION_ERROR': 74, 'INPLACE_LSHIFT': 75, 'INPLACE_RSHIFT': 76, 'INPLACE_AND': 77, 'INPLACE_XOR': 78, 'INPLACE_OR': 79, 'LIST_TO_TUPLE': 82, 'RETURN_VALUE': 83, 'IMPORT_STAR': 84, 'SETUP_ANNOTATIONS': 85, 'YIELD_VALUE': 86, 'POP_BLOCK': 87, 'POP_EXCEPT': 89, 'STORE_NAME': 90, 'DELETE_NAME': 91, 'UNPACK_SEQUENCE': 92, 'FOR_ITER': 93, 'UNPACK_EX': 94, 'STORE_ATTR': 95, 'DELETE_ATTR': 96, 'STORE_GLOBAL': 97, 'DELETE_GLOBAL': 98, 'LOAD_CONST': 100, 'LOAD_NAME': 101, 'BUILD_TUPLE': 102, 'BUILD_LIST': 103, 'BUILD_SET': 104, 'BUILD_MAP': 105, 'LOAD_ATTR': 106, 'COMPARE_OP': 107, 'IMPORT_NAME': 108, 'IMPORT_FROM': 109, 'JUMP_FORWARD': 110, 'JUMP_IF_FALSE_OR_POP': 111, 'JUMP_IF_TRUE_OR_POP': 112, 'JUMP_ABSOLUTE': 113, 'POP_JUMP_IF_FALSE': 114, 'POP_JUMP_IF_TRUE': 115, 'LOAD_GLOBAL': 116, 'IS_OP': 117, 'CONTAINS_OP': 118, 'JUMP_IF_NOT_EXC_MATCH': 121, 'SETUP_FINALLY': 122, 'LOAD_FAST': 124, 'STORE_FAST': 125, 'DELETE_FAST': 126, 'RAISE_VARARGS': 130, 'CALL_FUNCTION': 131, 'MAKE_FUNCTION': 132, 'BUILD_SLICE': 133, 'LOAD_CLOSURE': 135, 'LOAD_DEREF': 136, 'STORE_DEREF': 137, 'DELETE_DEREF': 138, 'CALL_FUNCTION_KW': 141, 'CALL_FUNCTION_EX': 142, 'SETUP_WITH': 143, 'LIST_APPEND': 145, 'SET_ADD': 146, 'MAP_ADD': 147, 'LOAD_CLASSDEREF': 148, 'EXTENDED_ARG': 144, 'SETUP_ASYNC_WITH': 154, 'FORMAT_VALUE': 155, 'BUILD_CONST_KEY_MAP': 156, 'BUILD_STRING': 157, 'LOAD_METHOD': 160, 'CALL_METHOD': 161, 'LIST_EXTEND': 162, 'SET_UPDATE': 163, 'DICT_MERGE': 164, 'DICT_UPDATE': 165} ~~~ Otherwise, I will try to build 3.11 python locally and try this out.","Yes, actually I think this is because many jumprelated bytecode ops have been replaced in the Python 3.11 version. This is the relevant snapshot !image Link  https://docs.python.org/3/whatsnew/3.11.htmlreplacedopcodes  This might be a good task to start diving deeper into TorchDynamo bytecode analysis if you are interested.","We don't currently support py 3.11, but we're working on it.  In the time being, please try a supported version if possible (3.83.10)","> We don't currently support py 3.11, but we're working on it. In the time being, please try a supported version if possible (3.83.10) Ok, thank you so much!","Closing this, as this is currently tracked at  CC([dynamo] Python 3.11 support)"
transformer,wav2vec2 model: error trying to do inference," ðŸ› Describe the bug I tried to do torch.compile() on the second most downloaded model of facebook on huggingface: https://huggingface.co/facebook/wav2vec2large960hlv60self I took the example of usage they provide and just added the torch.compile() step. I also tried with an another custom audio sample earlier, and strangely, it ran, but very slowly compared to the original model... But it had the same prediction result  Error logs Here is the warnings and the traceback:   Minified repro I didn't try the minifier, but with the huggingface example, if you add just the `torch.compile()`, I assume you can reproduce the problem Versions: Working on a WSL UBUNTU 22.04 LTS torch                   1.14.0.dev20221204+cpu torchaudio              0.14.0.dev20221204+cpu torchvision             0.15.0.dev20221204+cpu ",2022-12-05T14:18:16Z,triaged bug,open,0,4,https://github.com/pytorch/pytorch/issues/93464,"working on setting up a repro.  ~(Btw if you can run the minifier, it would help us.)~ I wasn't able to get minifier to work for this model, digging deeper",we've gotten a minimum repro and are in the process of fixing it see updates here:  CC(`unsqueeze` and `expand` metas produce wrong strides),"actually is this issue fixed? if I trust myself from the past, CC(`unsqueeze` and `expand` metas produce wrong strides) was supposed to be a repro of this and has since been closed.   ","I just tried to paste my original example in colab, and it runs. However, inference seem to be way slower with compiled version than with original version, trying with a couple example from the dummy_asr dataset"
yi,Trying pytorch 2.0 snippet from the website - sm89 error," ðŸ› Describe the bug trying the snippet from the website  I got Value 'sm_89' is not defined for option 'gpuname' I am indeed using a rtx4090 and cuda 11.6 BUT works fine without torch.compile  Versions Collecting environment information... PyTorch version: 1.14.0.dev20221205+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 525.60.11 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.6.0 ",2022-12-05T11:41:23Z,high priority triaged module: third_party module: inductor,closed,0,16,https://github.com/pytorch/pytorch/issues/90170,"By default `compile` uses `inductor` as backend. Currently, inductor supports Volta and Ampere devices only. Your RTX 4090 is based on Ada Lovelace architecture, which is not supported by the torch inductor.  There are other backends available in `torch.compile`.  For example:  worked on my 3080. You may get a list of available backends like so: ","The issue likely in Triton, that needs to limit the compilation to latest architecture supported by toolkit rather than GPU sm arch. Updating cudatoolkit might solve the problem as well (as 11.6 to the best of my knowledge does not support Lovelace) ","The error comes from triton, your cuda version (and thus ptxas version) is too old to support 4090, if you install cuda 11.8 on your system and set `TRITON_PTXAS_PATH` to point to it that should work. ",But if I install cuda 11.8 will I be able to install pytorch from wheels ? I think it offers only 11.6 and 11.7 at the moment.,Using backend='nvprims_aten' worked.,"Yeah if you install cuda 11.8 you can still use pytorch compiled with any other cuda version, you just need 11.8 ptxas, not even necessarily in your PATH, just discoverable by `TRITON_PTXAS_PATH`",no luck.  Installed cuda 11.8 tried to set `TRITON_PTXAS_PATH` to `/usr/local/cuda11.8` or `/usr/local/cuda11.8/bin` (where ptxas is) and I am now getting this message:  plus subsequent more related errors but always due to .target sm_89,"Ok, sorry about that, the fix should be on the triton side, we'll work with them to enable it. ",But in essence why don't you guys release a wheel for cuda 11.8 ? I noticed there are always a shift of one minor release vs Cuda last one.,We are working on it. ,"I took a stab at it with https://github.com/openai/triton/pull/1038. I've got a Docker image I'm building using pytorch with triton's new MLIR backend on my 4090 and it seems to be working reasonably well. If anyone is blocked by the lack of support for newer architectures, let me know and I can post it somewhere.",Hi ! Thank you for the work on newer architectures! Could you please post it for us to try the new featuresðŸ¤©?,> Hi ! Thank you for the work on newer architectures! > Could you please post it for us to try the new featuresðŸ¤©? Here you go: https://gist.github.com/ConnorBaker/10988159828943187d75737566c7e342 I'm working on a version which uses some of the optimizations Polly provides but I don't know if they'll make an impact/I'll be able to get it working.,closing since https://github.com/openai/triton/pull/1039 is merged.," the code example in your original post won't work unless the user installs Triton from master or cherrypicks that commit into their own build, or PyTorch updates the version of Triton they're using to do the same. The version of Triton PyTorch uses is pinned here: https://github.com/pytorch/pytorch/blob/master/.github/ci_commit_pins/triton.txt I understand that they're holding off on updating to newer Triton releases until the Triton team has ironed out more of the performance regressions caused by the MLIR rewrite.","Right, good to clarify. I had just quick hacked the llvm.. I am stuck by dynamic shapes anyway, so have to wait longer ... :)"
yi,Simplify by using yield from,,2022-12-05T07:15:25Z,better-engineering Merged topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/90160,The XLA failure is preexisting.," merge f ""Preexisting XLA failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[Feature Proposal] Extend torch hub to better support cloud serving and edge deployment," ðŸš€ The feature, motivation and pitch **TL;DR**: Extend torch hub to better support cloud serving and edge deployment by: 1. Extend the `hubconf.py` as an entrypoint for serving and deployment, like HuggingFace Inferece Handler 2. (Optional) Allow thirdparty infra providers to integrate model deployment and performance reporting to the hub website.  Background I was a PhD researcher a while ago and am now an AI practitioner in the industry. From my personal experience and observation, the pytorch hub provides incremental value to researchers, because: 1. Researchers usually rely on conferences and their own networks to find the latest models. 2. Researchers are familiar with cloning a repo, peeking into the source and doing modifications. The simple `torch.hub.load` interface is not very much helpful to them. However, industry community usually don't have the advantages mentioned above, therefore the hub can be a valuable information source to them. But when they choose a model, they not only want an interface to run some sample data, deployment easiness and model performance are also important factors to consider.  That's why huggingface transformer",2022-12-04T23:28:48Z,feature triaged module: hub,open,0,1,https://github.com/pytorch/pytorch/issues/90147,"Hi , sorry for the delay in responding to you. Thanks for posting this. I'm trying to understand precisely what you're proposing, feel free to correct me if I'm making wrong assumptions.  Are you suggesting that every model registered in torch.hub should be made compliant with this structure, or would it be optional? In the former case it would be a bit difficult to bring in, as it'd require us to go to multiple contributors and ask them to change their code accordingly. I would not bet on many of them doing it. Also, we provide a hub for models provided by third parties without forcing them to give a pretrained version (although many do provide this). In this context it's not obvious what a benchmark execution would mean (since the model is not tied to a specific dataset or task). Same for `self.get_input`: what would this mean without a precise context? Let me know if I got anything wrong, I'm not sure I clearly understood the kind of changes you were suggesting."
rag,Fix exception cause in storage.py,This change causes the correct message to be shown between the two tracebacks when an error is shown. More context here: https://blog.ram.rachum.com/post/621791438475296768/improvingpythonexceptionchainingwith,2022-12-03T18:23:08Z,open source Merged,closed,0,4,https://github.com/pytorch/pytorch/issues/90118,The committers listed above are authorized under a signed CLA.:white_check_mark: login: coolRR / name: Ram Rachum  (516c00fb9b649e8247ff195d816f1a925ad121a6),The CI failure doesn't have anything to do with my change. ," merge f ""Irrelevant XLA failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[reland][quant] Explictly set default quantized engine instead of relying on the order of supported_qengines (#89804),  CC([reland][quant] Explictly set default quantized engine instead of relying on the order of supported_qengines (89804)) Summary: Fixes:  CC(Use better default QEngine based on the environment) Test Plan: ossci + sandcastle Reviewers: Subscribers: Tasks: Tags:,2022-12-02T00:31:05Z,Merged ciflow/trunk release notes: quantization topic: improvements topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/90036, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,For test_transformer_FuseConvBNNoConvBias set deadline to None,Summary: I am on the current oncall  for aml_ai_platform and this is a fix for one of the failing tests. Test Plan: The function fails the deadline on the first run but passes on subsequent. Implemented the suggested solution of setting deadline=None. Differential Revision: D41669143,2022-12-01T23:52:15Z,caffe2 fb-exported topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/90032,This pull request was **exported** from Phabricator. Differential Revision: D41669143,This pull request was **exported** from Phabricator. Differential Revision: D41669143,no need
yi,libtorch_python.so: undefined symbol: PyInstanceMethod_Type," ðŸ› Describe the bug I run into this error vvv  link of failing CI: https://github.com/pytorch/pytorch/actions/runs/3586511045/jobs/6036274959 cmake lines related to the object: https://github.com/pytorch/pytorch/blob/d8335704a54f89e93f562cda7b49048e1eab1ebb/torch/csrc/jit/codegen/cuda/CMakeLists.txtL165L200 It's a little strange to me how the error was thrown from libtorch_python.so. This feels like a pytorch build issue. I'm also a little bit uncertain how this is invoked. In the setup, I have `libnvfuser_python.so` which depends on both `libnvfuser_codegen.so` and `libtorch_python.so`. While `libnvfuser_codegen.so` is dlopened by torch and does not have dependency on `libnvfuser_python.so`.  Versions This is failing on CI job: linuxbioniccuda11.6py3.10gcc7 / test (deploy, 1, 1, linux.4xlarge.nvidia.gpu) ",2022-12-01T18:22:11Z,module: build triaged topic: build,closed,0,6,https://github.com/pytorch/pytorch/issues/90016,"One thing I did notice is that for linuxbioniccuda11.6py3.10gcc7  CI build uses `Found PythonLibs: /opt/conda/lib/libpython3.10.a (found version ""3.10.4"")` and tests runs uses 3.10.8  But minor version should be compatible...",Bumping threads~ since the `libtorch_python.so` issue is the only failing test on my PR. :cry: ,"bumping threads again: Just realized that the failing config is `deploy` and the test somehow is dlopen `libnvfuser_python.so`. which has python dependency and likely that's what's missing there?!?! I'm not sure where those tests are defined, and why are they explicitly loading `libnvfuser_python.so`? ",from https://github.com/pytorch/pytorch/blob/797544f1c456035db1d3fbbd9141d592a834c60a/.jenkins/pytorch/common_utils.shL147L169 the multipy loader.cpp seems to be https://github.com/pytorch/multipy/blob/60d9ef2df2950eb7cbaed671eb1fe4eb1e02525b/multipy/runtime/loader.cpp which meticulously handles `libtorch_python.so` https://github.com/pytorch/multipy/blob/60d9ef2df2950eb7cbaed671eb1fe4eb1e02525b/multipy/runtime/loader.cppL720L723,Thx for that. I would need to add an entry to skip libnvfuser_python.so as well on deploy.,"I think the issue here is that we are loading libnvfuser_python.so because it's a python extension under `torch._C_nvfuser`. I think there's something I did wrong, that I'm not handling the load properly. I don't understand how this is supposed to work in the deploy tests... I guess I should magically strip libtorch_python.so dependency for the test to pass, but that doesn't work for nvfuser python interface to work independently. Anyway, I decided to work towards a different direction and take nvfuser out of torch package. This worked in my PR :tada:  I'm closing this one as it is a user error."
rag,ShapeEnv.create_symbolic_sizes_strides_storage_offset,"  CC(Keep track of source name on all allocated SymInts)  CC(Rewrite dynamo cond() handling to not recursively call export)  CC(Type torch._dynamo.side_effects)  CC(Convert InstructionTranslatorGraphState and OutputGraphState to NamedTuple)  CC(Type torch._dynamo.symbolic_convert)  CC(Add missing mypynofollow.ini)  CC(Ensure that we fakeify tensor subclasses when they are initially tracked)  CC(ShapeEnv.create_symbolic_sizes_strides_storage_offset) Instead of having storage offset hang out on its own, allocate all of these symbols all in one go. Signedoffby: Edward Z. Yang ",2022-11-30T21:28:07Z,Merged ciflow/trunk release notes: fx ciflow/inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/89962,"> I guess this is towards being able to specialize differently storage offset from the other symints? Yeah, it makes it easier to associate storage offset with the tensor in question", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 3 additional jobs have failed, first few of them are: linuxbinarylibtorchcxx11abi ,linuxbinarylibtorchprecxx11 ,trunk Details for Dev Infra team Raised by workflow job "," merge f ""merge bot is WRONG"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,"When dealing with dupe arguments, prefer leafifying if possible","  CC(When dealing with dupe arguments, prefer leafifying if possible) See code comment for details. I also had to do some extra fixes: * `run_functionalized_fw_and_collect_metadata` now is able to handle duplicated arguments * `aot_wrapper_dedupe` now always returns boxed compiled functions * `aot_wrapper_dedupe` is now applied to inference compiler along with autograd compiler (preexisting) Fixes https://github.com/pytorch/torchdynamo/issues/1939 Fixes DebertaV2ForQuestionAnswering DebertaForMaskedLM DebertaForQuestionAnswering DebertaV2ForMaskedLM Repro command:  Signedoffby: Edward Z. Yang  ",2022-11-30T05:39:04Z,Merged ciflow/trunk topic: bug fixes module: functorch module: dynamo ciflow/inductor release notes: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/89896, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""lint failure only with fix"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[RFC] PyTorch Tensor Parallel(TP) User API for Distributed Training," ðŸš€ The feature, motivation and pitch  ðŸš€ Feature Provide a detailed API design for highlevel PyTorch Tensor Parallelism API design. This is an evolvement of PyTorch Sharding introduced in  CC([RFC] PyTorch Sharder for distributed training) and is directly built on top of DTensor proposed in  CC([RFC] PyTorch DistributedTensor). We want users to only focus on how their modules to be distributed and hide all other details. (Caveat, for now, we only support linear/transformer based models).   Motivation To scale the large model training, especially transformer based model training, multiple parallelism paradigms are proposed and considered. Among them, model parallelism like MegatronLM is getting popular together with 3D parallelism. We have already proposed a standardized sharding api in the past ( CC([RFC] PyTorch Sharder for distributed training)). Now to enable more generic data distributions more than sharding across hosts, we have proposed a new design of Distributed Tensor (DTensor) in  CC([RFC] PyTorch DistributedTensor) and we want to not only provide similar functionality of model parallelism as Megatron on top of DTensor, but also provide bet",2022-11-30T00:29:54Z,triaged module: dtensor,open,10,5,https://github.com/pytorch/pytorch/issues/89884,  It seems that the output(MakeOutputReplicated) in the code is not aligned with the comment (output should be a sharded DTensor).  , Thanks! I have corrected the comment.,"1. I want to learn more about the recommended workflow for applying tensor parallelism to custom `nn.Module`s that are not covered by the highlevel API. Could you provide some examples or guidance?      For example, if the user has a custom attention module that is not equivalent to the one used in `_parallelize_multihead_attn`, then how should the user apply tensor parallelism to the custom attention module? Should the user look into any of the other lowlevel APIs beyond the ones mentioned(`_parallelize_mlp`, `_parallelize_multihead_attn`, and `_parallelize_linear`)? 2. Could you give an example of the model checkpointing workflow for a TPonly module (i.e. no FSDP)?",Has there been any development on this? ," CC([RFC] PyTorch Sharder for distributed training) talked about extending the `PlacementSpec` to accommodate for use cases where tensor need to be placed on UVM/SSD. Although this issue is an evolution of CC([RFC] PyTorch Sharder for distributed training), I don't see a similar description here. Is there plan to make `PlacementSpec` extensible? If there are any public examples that show how to do this, please point me them."
rag,Guarantee symbol allocation for all sizes/strides/storage offset,"  CC(ShapeEnv.create_symbolic_sizes_strides_storage_offset)  CC(Type torch._dynamo.guards)  CC(Guarantee symbol allocation for all sizes/strides/storage offset)  CC(Add definitely_not_01 set to ShapeEnv.) We may need to express guards on the size/stride/storage offset of a tensor, but we cannot do this if it's already been duck sized. This PR guarantees that we allocate a symbol (or negation of the symbol) whenever we ask to create a SymInt, and propagates this symbol to SymNode so that Dynamo can look at it (not in this PR). This PR doesn't actually add guards, nor does Dynamo do anything with these symbols. Signedoffby: Edward Z. Yang ",2022-11-29T23:32:13Z,ciflow/trunk release notes: composability release notes: fx topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/89879
rag,Fix some typed storage is deprecated warnings.,  CC(Fix some typed storage is deprecated warnings.) Signedoffby: Edward Z. Yang  ,2022-11-29T21:19:58Z,Merged ciflow/trunk topic: not user facing module: functorch ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/89867, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: inductor ,inductor / cuda11.6py3.10gcc7sm86 / test (inductor_timm, 2, 2, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""flaky ci"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[replacing D41588356] Trying to fix issues in TP serialization (#89765),Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/89765 Differential Revision: D41588356 LaMa Project: L1142320,2022-11-29T19:11:19Z,fb-exported Stale release notes: package/deploy,closed,0,4,https://github.com/pytorch/pytorch/issues/89861,This pull request was **exported** from Phabricator. Differential Revision: D41588356," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D41588356,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,update transformer init function,Fixes CC(update transformer init function) ,2022-11-29T09:00:33Z,triaged open source Stale,closed,0,7,https://github.com/pytorch/pytorch/issues/89830,"This seems reasonable (assuming tests pass).  An alternative workaround is getting these fields from lower level components. For example, you can find dim_feedforward by going Transformer>TransformerEncoder>feed forward linear layers. What do you think?","> This seems reasonable (assuming tests pass). >  > An alternative workaround is getting these fields from lower level components. For example, you can find dim_feedforward by going Transformer>TransformerEncoder>feed forward linear layers. What do you think? Hi, this is where the problem lies, you have to dig into your model, and I strongly assume that **Transformer>TransformerEncoder>feed forward linear layers** path not exist, because maybe there is a custom TransformerEncoder in user's Transformer module, or TransformerEncoder has 0 TransformerEncoderLayer. it's messed up.  ","  codes modified in a more elegant way. Here, I use property decorator to dynamically get attributes and modules.", Do you think this may be related to  CC(Quantization issue in transformers) ?,">  Do you think this may be related to CC(Quantization issue in transformers) ?  I don't think they are related, and I cannot reproduce your issue on torch version **'1.13.0+cu117'**",  can this be merged?,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,update transformer init function," ðŸš€ The feature, motivation and pitch Once a transformer instance is created, like below: ` model_pytorch = nn.Transformer(nhead=16, num_encoder_layers=2, num_decoder_layers=2) ` I cannot access it's num_encoder_layers by **model_pytorch.num_encoder_layers**, TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer alike.   This is extremely suffering when doing quantization, like below: ` Transformer.from_torch(model_pytorch) ` say I have a selfdefined Transformer class, it's really hard to create my instance from original pytorch instance, because I cannot  access init value elegantly.  Alternatives _No response_  Additional context _No response_ ",2022-11-29T08:58:37Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/89829
,Macro Dynamo Progress Bar,"Video demo if add a 1s sleep: https://www.loom.com/share/436f988653a4455ba53b0b5067b15ab8 This is the simplest progress bar I could think of, goal is to have something that we can enable by default and merge quickly whereas more detailed progress bars could be enabled at a later date CC(Dynamo, FX, Inductor Progress Bars)  An inductor compilation goes through 3 steps whereas most (all) other backends seem to go with just 2 so this meant I had a few choices 1. Since I figured users would be happier if something ends sooner rather than something going from 66% to 100% I opted for having the total always be 3 if triton is installed. This is not ideal if it so happens that triton is installed but if someone is not using the inductor backend 2. Another option would be to have a separate compilation bar just for inductor with 1 step I feel like 1 bar for now makes the most sense and we can have 2 bars when I instrument the inductor passes. I tried 2 and it wasn't super helpful  !Screen Shot 20221128 at 7 16 09 PM",2022-11-29T03:00:46Z,topic: not user facing module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/89818
yi,[quant] Explictly set default quantized engine instead of relying on the order of supported_qengines,  CC([quant] Explictly set default quantized engine instead of relying on the order of supported_qengines) Summary: Fixes:  CC(Use better default QEngine based on the environment) Test Plan: ossci + sandcastle Reviewers: Subscribers: Tasks: Tags: Differential Revision: D41635738,2022-11-28T23:42:52Z,Merged Reverted ciflow/trunk release notes: quantization topic: improvements,closed,0,7,https://github.com/pytorch/pytorch/issues/89804," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""breaking tests https://hud.pytorch.org/pytorch/pytorch/commit/607ff6f4c10914a2a46bab90577cd083a6b3d46d https://github.com/pytorch/pytorch/actions/runs/3596841274/jobs/6058297637 trunk label didnt kick off workflows fast enough"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,">  revert m ""breaking tests https://hud.pytorch.org/pytorch/pytorch/commit/607ff6f4c10914a2a46bab90577cd083a6b3d46d https://github.com/pytorch/pytorch/actions/runs/3596841274/jobs/6058297637 trunk label didnt kick off workflows fast enough"" c nosignal I remember checked the CI before actually, should this error block landing?"
yi,[benchmarks][dynamo] Trying CI - Set train() for TIMM models accuracy tests,Moving to train mode for TIMM models and also raising batch size for accuracy testing. Raising batch size seems to remove a lot of noise/instability coming from batch_norm decomposition.  ,2022-11-28T20:19:21Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,12,https://github.com/pytorch/pytorch/issues/89780,LGTM, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , merge,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , rebase b master, successfully started a rebase job. Check the current status here,"Successfully rebased `timmtrain` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `git checkout timmtrain && git pull rebase`)", Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Trying to fix issues in TP serialization,"Summary: several error comes up last wk for TP:  Issue during deserialization: P555070876 Test Plan: buck run mode/opt //scripts/pls331/aimp:raw_inference_model_dumper  model_entity_id 355981394 config_version_id 3 Reviewed By: dracifer, houseroad Differential Revision: D41391267",2022-11-28T17:46:13Z,fb-exported release notes: package/deploy,closed,0,6,https://github.com/pytorch/pytorch/issues/89765,"   :x: The commit (994ba186e72a8671b46c9b2ee9dbd99f32d87bb0). This user is missing the User's ID, preventing the EasyCLA check. Consult GitHub Help to resolve.For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267
yi,`torch.Tensor.flatten` Trigger Segmentation Fault when trying to provide and output named dim, ðŸ› Describe the bug A test for `torch.Tensor.flatten` triggers segmentation fault when named dim is provided.  Test   Error log `segmentation fault`  Versions  ,2022-11-27T09:54:16Z,module: crash triaged module: named tensor,open,0,2,https://github.com/pytorch/pytorch/issues/89718,The last argument is a name for the named Tensor feature which is not actively worked on.,"torch.Tensor.flatten() takes 2 named dim, start dim, and end dim where only dimensions starting with start_dim and ending with end_dim are flattened. But in the above example dimension provided is out of range (expected to be in the range of [5, 4], but got 9)"
transformer,DDP hangs on forward pass of transformer," ðŸ› Describe the bug I'm facing an issue where DDP is hanging sometimes. A relevant code snippet is below. The setup is fairly simple: I have an encoder, and I want to do a forward pass on one GPU but not the other. Based on which GPU I let it do the forward pass on, I see varying results as to what processes complete. I'm running the file as follows:  Here is the file ddp.py:   Versions Collecting environment information... PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 16.04.7 LTS (x86_64) GCC version: (Ubuntu 5.5.012ubuntu1~16.04) 5.5.0 20171010 Clang version: Could not collect CMake version: version 3.24.2 Libc version: glibc2.23 Python version: 3.8.15 (default, Nov 24 2022, 15:19:38)  [GCC 11.2.0] (64bit runtime) Python platform: Linux4.4.0210genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: GeForce GTX 1080 Ti GPU 1: GeForce GTX 1080 Ti GPU 2: GeForce GTX 1080 Ti GPU 3: GeForce GTX 1080 Ti GPU 4: GeForce GTX 1080 Ti GPU 5: GeForce GTX 1080 Ti GPU 6: GeForce G",2022-11-27T06:08:57Z,oncall: distributed,open,0,3,https://github.com/pytorch/pytorch/issues/89716,"Hi, I would like to help but have a question:  If you are doing just forward pass, why do you need DDP?  DDP is used for summing gradients among multiple devices in the backward pass.","Also, for certain buffers in the model, DDP might trigger a collective in the forward pass. So there can be a collective mismatch.","Hi , thanks for the reply. To answer your question: I would like to use DDP to speed up the training process, but I'd like to do evaluation while traning. This is just showing the evaluation (inference) part, I omitted the training part.  I didn't exactly understand what you mean by ""trigger a collective"" and having a ""collective mismatch,"" could you please elaborate? Is there a way to fix the issue? Thanks!"
rag,move TypedStorage handling to assertEqual," CC(Deprecate TypedStorage, its derived classes, and all of their public methods) added a patch to `torch.testing.assert_close` to handle `torch.storage.TypedStorage`'s. This change is not reflected in the docs and is not intended for the public API. This PR removes the patch ones again and moves the behavior to `TestCase.assertEqual` instead. Meaning, `TypedStorage`'s are again not supported by the public API, but the behavior is the same for all internal use cases.  ",2022-11-23T10:36:25Z,module: mkldnn open source Merged module: testing ciflow/trunk topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/89557,"As for your errors: > Unless I'm mistaken we should simply add `exact_dtype=False` to the `assertEqual` calls. I think we want the dtypes to match when we perform the comparisonotherwise it wouldn't be possible to ensure an exact match, right? I'm not sure why the dtypes disagree. The loaded storage's dtype at least matches the saved storage's dtype in `test/test_serialization.py` in `save_load_check`, right? We definitely need that to be true > The problem is only occurs for the quantized dtypes. It happens, because you can't instantiate a new tensor with such a dtype directly, but our logic currently tries to. You may be able to use something like this trick to interpret the quantized type as the appropriate nonquantized type: https://github.com/pytorch/pytorch/blob/7322f73c8f25caea79509805635cd05fd67dc607/torch/storage.pyL530L543","> I think we want the dtypes to match when we perform the comparisonotherwise it wouldn't be possible to ensure an exact match, right? I'm not sure why the dtypes disagree. The loaded storage's dtype at least matches the saved storage's dtype in `test/test_serialization.py` in `save_load_check`, right? We definitely need that to be true I've looked more closely into the test and as is it makes little sense. Since we operate without any data https://github.com/pytorch/pytorch/blob/b0bd5c4508a8923685965614c2c74e6a8c82f7ba/test/test_serialization.pyL693 the only thing we could be checking with https://github.com/pytorch/pytorch/blob/b0bd5c4508a8923685965614c2c74e6a8c82f7ba/test/test_serialization.pyL688L689 is the dtype. However, due to some quirks in `assertEqual` we aren't doing that either: for CC(use `torch.testing.assert_equal` in `TestCase.assertEqual`) we agreed offline (I'm too lazy now to find the specific commit in the PR that added this) that storages should be compared as if they were a list of Python Scalars. This is why before this PR we had them listed in `sequence_types`. Meaning, an empty tensor will be turned into an empty list and no check is happening at all:  Even if we had some data before, the check wouldn't have been doing a dtype check due to some legacy quirks in `assertEqual` https://github.com/pytorch/pytorch/blob/8b0847811501cd452b131b6153f28c254a3ee44f/torch/testing/_internal/common_utils.pyL1491L1496 In conclusion, before this PR `assertEqual` will only check the values, but never the dtypes of typed storages. Since there are no values, the test is doing nothing for typed storages.  Now for the part why this is failing after this PR. A minimal reproduction is  Is the mismatch between `s.dtype` and `t.dtype` intentional? We create `s` from `t.storage()` that has `dtype`, but we explicitly set `dtype=other_dtype`. If yes, the failing test in this PR actually is a valid failure, since `s_loaded` has `dtype` and not `other_dtype` like `s`. Interestingly, the behavior is correct when only serializing `s`, i.e. `torch.save([s], buffer)`. Plus, if we actually try to do the same with some data, i.e. `t = torch.tensor([0], dtype=dtype)`, we are not even allowed to serialize both at the same time: ","Ah sorry, I forgot about this until now. Yes the mismatch between `s.dtype` and `a.dtype` is intentional. When I wrote `test_save_different_dtype_unallocated`, I should have added a comment to explain it. Historically, it has not been possible to save/load multiple tensors/storages that point to the same location (though I think it may be fairly straightforward to add that feature now that all the storage refactoring is complete). In `torch.save` we check the `data_ptr` of each tensor and storage and throw an error if two have the same `data_ptr` but different dtypes. However, if a tensor and storage are unallocated (as in `tensor([])`), their `data_ptr`s are both 0, and the tensor and storage are not actually pointing to the same memory location, since they're not pointing at any memory location. This means that doing this  and this  produce exactly the same `a` and `s`. We do allow saving and loading unallocated tensors/storages of different dtypes. The only reason why the test for this exists is because I broke this behavior at one point and had to fix it. The history of this is here:  CC(Cannot `torch.load` tensors of different dtypes that view the same data) It probably would have been better if I had not used `a.storage()` in this test, and instead created the storage separately, so that it's clear that `a` and `s` aren't actually linked at all.",And it appears that the dtype mismatch between the saved and loaded storage is an upstream bug. If I change the test to do this:  it fails. I had assumed `assertEqual` checks the dtype. I will look into why this failure happens,> I had assumed `assertEqual` checks the dtype. It will after this PR. Should we maybe xfail this test in this PR and merge it so you can have a testing ground?,"> Should we maybe xfail this test  I took the liberty to do that in 822f6b29f35204a92c810545f59778b5285a075e. In case you want to go another round, I'll revert later.",Expected failure seems like a good idea to me, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,cuFFT error raised after trying FFT," ðŸ› Describe the bug  returns  I'm using cuda 11.6, nvidia driver version 520.56.06, RTX 4090... with Ubuntu 22.04 I have reinstalled CUDA and NVIDIA drivers several times, but the problem is repeating. I think there is no solution in the google..  please help me . thanks.  Versions  ",2022-11-23T10:31:10Z,high priority triage review module: cuda triaged module: fft,closed,0,2,https://github.com/pytorch/pytorch/issues/89556,This is a duplicate of  CC(CUFFT_INTERNAL_ERROR on RTX 4090),æˆ‘é‡åˆ°äº†ä¸€æ ·çš„é—®é¢˜ï¼Œè¯·é—®æ€Žä¹ˆè§£å†³çš„å•Šï¼Ÿéžå¸¸éœ€è¦å¸®åŠ©ï¼ï¼
transformer,[Inductor] [CPU] Vectorization not supporting python pass-in scalar double in speech_transformer," Description Comparing performances of `speech_transformer` with backends inductor and IPEX, inductor is 0.68 IPEX. The main reason is that vectorization does not support python passin scalar double.  Profiling and Code snippet !image  According to the profiling analysis, bottlenecks are `kernel_cpp_8, kernel_cpp_14, kernel_cpp_20, kernel_cpp_2, kernel_cpp_32 and kernel_cpp_26`, which are the implementations for the same Python code snippet:  As `self.temperature` in Python, a.k.a. `__restrict__ in_ptr2` in C++, is a double scalar, vectorization is not applied.  Minified repro `python benchmarks/dynamo/torchbench.py performance float32 dcpuÂ n50 inductorÂ noskip dashboardÂ k ""speech_transformer"" cold_start_latency channelslast`  ",2022-11-23T07:37:23Z,triaged,open,2,0,https://github.com/pytorch/pytorch/issues/93446
rag,as_strided: Fix default storage_offset for reference implementation,"  CC(as_strided: Fix default storage_offset for reference implementation) This fixes the default storage_offset to take it from the input. This was previously untested, so I've also added a new OpInfo which includes samples with nonzero storage_offsets on the input tensor.",2022-11-22T18:48:08Z,open source Merged Reverted ciflow/trunk topic: not user facing,closed,0,26,https://github.com/pytorch/pytorch/issues/89513,wow that's a lot of xfails lol, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / linuxbionicpy3.7clang9slow / test (slow, 1, 1, linux.2xlarge) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / winvs2019cuda11.6py3 / test (default, 3, 5, windows.8xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 STARTUP failures reported, please check workflows syntax! trunk Details for Dev Infra team Raised by workflow job "," merge f ""pip failure is unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c landrace m ""broke master""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""land bot is not working"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m ""Broke multiple workflows, 2 unexpected successes for autograd tests"" c weird", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,">  merge f ""land bot is not working"" PR has been reopened, so there is no way for me to debug what went wrong :(", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,as_strided: Enable sample inputs with non-zero storage_offset,  CC(as_strided: Fix default storage_offset for reference implementation)  CC(as_strided: Enable sample inputs with nonzero storage_offset),2022-11-22T18:48:03Z,open source release notes: python_frontend,closed,0,0,https://github.com/pytorch/pytorch/issues/89512
yi,Remove TORCH_API from inline at::internal::lazy_init_num_thread,"The function signature in its current state is ambiguous.  Its an inline function that is also declared to be imported from the DLL. which leaves it subject to compilers decision to choose one or the other and depending on what the compiler/linker may choose we may get one of the two behaviors for the `aten::init_num_threads` call:  1. Onceperdllinathread (if its inlined) 2. Onceperthread (if its imported) I suspect onceperdllinathread is already the case currently because it being tagged inline So removing the inline will simply make it a little more consistent and clear. The function exists to avoid repeated calls to aten::init_num_threads.  Being in an ""internal"" namespace, the function isnt expected to be called by external plugins which means that the ""onceperdllinathread"" behavior isn't that much of a problem anyway",2022-11-22T18:26:16Z,triaged open source Merged ciflow/trunk ciflow/periodic,closed,0,13,https://github.com/pytorch/pytorch/issues/89511,The committers listed above are authorized under a signed CLA.:white_check_mark: login: ankurvdev / name: Ankur Verma  (a7822dbf92273716d4313c6d31ddda6f2e00dca7),  Please take a look.,Note that this VS 17.4.0 regression is being tracked as thread\_local causing fatal error LNK1161: invalid export specification on VS 2022  Visual Studio Feedback,> Note that this VS 17.4.0 regression is being tracked as thread_local causing fatal error LNK1161: invalid export specification on VS 2022  Visual Studio Feedback  Is it possible to make the change affect only that specific compiler version? Otherwise the change seems to be too broad as it will affect every compiler on every platform.,"Thanks for providing the reference to the bug.  IMO it's probably not worth wrapping this in a compiler version specific change.  If the VS team is ready to roll out a fix for this, then I'm happy to wait it out. However, that said, IMO the semantics this piece of code tries to express (inlining as well as importing) are extremely weird. And that should probably be changed regardless. The code change here would simply imply that `aten::init_num_threads` is called onceperdllinathread (that call the lazy function) as opposed to onceperthread.  Given that plugin dlls rarely call this directly .... thats probably never going to happen and even if it does, I suspect multiple calls to init_num_threads wont be a huge performance hit (why is it exported anyway?) Even so ... if there are concerns  I can move this function into a cpp file so we can reliably switch to onceperthread. The semantics in its current state make it unclear which mode we'll get 1. Onceperdllinathread 2. Onceperthread And depending on the compiler version it could be either. I suspect onceperdllinathread is already the case currently because it being tagged inline. So choosing one way or the other should be done regardless of the VS bugfix. Thoughts ?","`lazy_init_num_thread` was introduced in https://github.com/pytorch/pytorch/pull/37461 for the purpose of reducing number of calls to the heavier `at::init_num_threads` function, but on the other hand, if its called repeatedly is not an end of the world and just a minor performance overhead. With that in mind, removing `TORCH_API` decorator sounds reasonable to me, but please change PR title to something like: ""Remove `TORCH_API"" from inline `at::internal::lazy_init_num_thread`"" and in PR description explain, that behavior of inline functions with external linkage is not defined across shared library boundary and very likely does not result in a singleton. Imo a better change , would be to move `thread_local bool init` to the namespace level and rename it to something like `extern TORCH_API thread_local bool _num_thread_initialized` and reference it from the inline function. This way it will have an unambiguous callonceperthread semantic across all shared libraries.","FYI  a MS C++ bug fix is underway, but it may not be included until version 17.4.4. ",Ping ...  I changed the title and description as requested. Is there any other change needed before this can merge. ?, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `vs2022_17_4_compilation_fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout vs2022_17_4_compilation_fix && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Fix retrying logic for successful unittest tests under --rerun-disabled-tests mode,"When looking into Rockset data for disabled test unittest, for example `testAdd`, I see that it's rerun only 3 times instead of 50+ times as expected under rerundisabled test mode  It turns out that I made a mistake mixing `RERUN_DISABLED_TESTS` and `report_only` into `(RERUN_DISABLED_TESTS or report_only) and num_retries_left < MAX_NUM_RETRIES` in https://github.com/pytorch/pytorch/pull/88646.  The retrying logic for successful tests under rerundisabledtests mode is never executed because num_retries_left would be equal to MAX_NUM_RETRIES (not smaller) if the very first run successes. Thus, the sample test `testAdd` finishes right away (1 success count) * `report_only` and `RERUN_DISABLED_TESTS` are 2 different things and shouldn't be mixed together. RERUN_DISABLED_TESTS has the higher priority. * We also don't want to retry skipped tests under rerundisabledtests mode because they are only skipped due to `check_if_enable` check `Test is enabled but rerundisabledtests verification mode is set, so only disabled tests are run`  Testing * CI https://github.com/pytorch/pytorch/actions/runs/3518228784 generates https://ghaartifacts.s3.amazonaws.com/pytor",2022-11-21T21:22:21Z,Merged ciflow/trunk release notes: releng test-config/default,closed,0,2,https://github.com/pytorch/pytorch/issues/89454, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,torch.nn.TransformerEncoderLayer missing exception description information.," ðŸ› Describe the bug     import torch     encoder_layer = torch.nn.TransformerEncoderLayer(d_model=52, nhead=1)     src = torch.rand(10, 32, 512)     out = encoder_layer(src) When running this code, an AssertionError is thrown directly without giving effective information about the exception. The real error is that d_model is not equal to the third value in src. It is recommended to use the pytorch1.8.0 version. When this happens, throw Display the corresponding exception information.  Versions pytorch: 1.8.0 Python version: 3.8 CUDA/cuDNN version: cuDNN 11.1 GPU models and configuration: RTX3060 Operating Systemï¼šWindows ",2022-11-21T02:34:36Z,module: nn module: error checking triaged,open,0,2,https://github.com/pytorch/pytorch/issues/89394,"Are you strongly attached to 1.8.0? On 1.13, it gives a more informative error:  since d_model is the number of expected features of the input according to the docs, it and the final dim of src must match","> TransformerEncoderLayer Currently doing a test experiment, need to use pytorch1.8 version, and then use TransformerEncoderLayer, this problem is triggered, I feel very confused, I will test with a higher version later, and found that the higher version has provided the corresponding information description, thank you ."
yi,"Reland 2 ""Towards unifying symbolic and non symbolic fake tensor (#89038) (#89143)""","  CC(Reland 2 ""Towards unifying symbolic and non symbolic fake tensor (89038) (89143)"") This reverts commit 8e4c9828f4c990f439179912159086aaed790493. ",2022-11-19T14:51:41Z,Merged ciflow/trunk release notes: composability topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/89346," merge f ""flaky SIGIOT on jit"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,torch.cholesky_inverse: The result accuracy was inconsistent on the CPU and GPU," ðŸ› Describe the bug     import torch     torch.manual_seed(5)      input = torch.rand([3, 3], dtype=torch.float32)     resCPU = torch.cholesky_inverse(input)     print(resCPU)     input = input.cuda()     resGPU = torch.cholesky_inverse(input)     print(resGPU) code result:         tensor([[ 321.4903, 431.7014,  536.8290],                 [431.7014,  582.4526, 724.7808],                 [ 536.8290, 724.7808,  903.7211]])         tensor([[ 321.4903, 431.7013,  536.8291],                 [431.7013,  582.4526, 724.7809],                 [ 536.8290, 724.7808,  903.7211]], device='cuda:0') As can be seen, the results on the CPU and GPU are inconsistent, resulting in accuracy problems.  Versions pytorch: 1.12.1 Python version: 3.8 CUDA/cuDNN version: cuDNN 11.3 GPU models and configuration: RTX3060",2022-11-19T03:27:08Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/89335
gemma,fbgemm_avx512 build failure, ðŸ› Describe the bug I'm building git master with the same Arch recipe. My CPU is Ryzen 2 and does NOT support AVX512. fbgemm is programmed wrongly and demands `fbgemm_avx512` even when the main project has disabled it:   Versions  ,2022-11-18T17:54:18Z,module: build triaged,open,0,0,https://github.com/pytorch/pytorch/issues/89293
yi,Delete .pyi files in torch/nn/parallel folder to enable mypy type checking,Summary: As title. Context in https://fburl.com/4irjskbe Test Plan: CI Differential Revision: D41148641,2022-11-18T14:55:14Z,better-engineering fb-exported Stale topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/89286,This pull request was **exported** from Phabricator. Differential Revision: D41148641,This pull request was **exported** from Phabricator. Differential Revision: D41148641,This pull request was **exported** from Phabricator. Differential Revision: D41148641,You should restore the pyi annotations into the py files as part of this,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
agent,[Inductor CI] Use string format for cuda-arch-list input to prevent 8.0/9.0/10.0 etc from being interpreted as 8/9/10,"Currently or in future whenever we change the cudaarchlist to num.0, github action or some agent would pass just num to TORCH_CUDA_ARCH_LIST This num is not regex matched during cuda arch analysis phase. (here: https://github.com/pytorch/pytorch/blob/c5fafb4e1694f141d8a1a31142cce4049d9057ed/cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmakeL229) Example failure: https://github.com/weiwangmeta/pytorch/actions/runs/3495656108/jobs/5852735299   Unknown CUDA Architecture Name 8 in CUDA_SELECT_NVCC_ARCH_FLAGS This change reminds us to use e.g. '8.0', '9.0', '10.0' etc instead of 8.0, 9.0, 10.0 as GHA or some other agent may erroneously truncate it to pure numbers. ",2022-11-18T09:48:20Z,Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/89279, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 additional jobs have failed, first few of them are: trunk Details for Dev Infra team Raised by workflow job ", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,Support masked_fill to address the GPT2 performance issue,  CC(Support masked_fill to address the GPT2 performance issue)  CC(Redefine the simdlen semantic) ,2022-11-18T07:23:44Z,open source Merged ciflow/trunk topic: not user facing intel module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/89274,Just reland this PR.,"The CI failure looks irrelevant, see https://github.com/pytorch/pytorch/pull/89281", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,Support masked_fill to address the GPT2 performance issue,  CC(Support masked_fill to address the GPT2 performance issue)  CC(Redefine the simdlen semantic) ,2022-11-18T07:23:44Z,open source Merged ciflow/trunk topic: not user facing intel module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/89274,Just reland this PR.,"The CI failure looks irrelevant, see https://github.com/pytorch/pytorch/pull/89281", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,[Dynamo] Fix args bug in TensorVariable.call_method,bug in 7k github models:  ,2022-11-18T01:29:55Z,module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/89258,"> add a test? Yes, I decided to merge this change into https://github.com/pytorch/pytorch/pull/89257, as they can be covered by the same tests. Thanks for your reviewing."
rag,Deprecation warning in `Tensor.storage()` should suggest alternate API,"Since CC(Deprecate TypedStorage, its derived classes, and all of their public methods) deprecated `TypedStorage`, calling `Tensor.storage()` now raises a warning about the deprecation because it returns a `TypedStorage`. The warning should be updated to mention how to get an `UntypedStorage` from the tensor. At the moment, this is done with `Tensor._storage()`, but it should be renamed to `Tensor.untyped_storage()` so that it's public, and there should be documentation for it. It might be good for the warning to mention in which cases `UntypedStorage` and `TypedStorage` are not interchangeable. ~We should also have a function that takes an `UntypedStorage` and a `torch.dtype` and returns a tensor that wraps the storage.~ EDIT: `torch.tensor` does this ",2022-11-17T18:08:55Z,triaged module: deprecation module: python frontend,closed,2,2,https://github.com/pytorch/pytorch/issues/89224,That sounds good. The new untyped_storage API sounds good. Not sure if we really need an API to build a Storage again? The APIs that we have today that take Storage also accept UntypedStorages right? It would be good as well to provide 11 correspondance for python storage object with c++ storage like we do for Tensor.  do you think it is worth reproducing all the complexity we have for Tensor here?, has been asking for it and I think is reasonable 
yi,"Reland ""Towards unifying symbolic and non symbolic fake tensor (#89038)""","  CC(Reland ""Towards unifying symbolic and non symbolic fake tensor (89038)"") This reverts commit cf6003f0469ae1440d4a8585860c2c5f4c738707. Differential Revision: D41363992",2022-11-16T17:00:10Z,Merged Reverted ciflow/trunk release notes: composability topic: not user facing ciflow/inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/89143," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ," merge f ""flaky ci only"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c 'nosignal' m ""This seems to be causing the test_make_fx_symbolic_exhaustive_rad2deg_cpu_float32 and test_make_fx_symbolic_exhaustive_inplace_rad2deg_cpu_float32 test to fail across multiple jobs"" Logs: https://hud.pytorch.org/pytorch/pytorch/commit/e686b8c3ba93cb7caa314c78bf84dbd2d7df9683", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
transformer,AssertionError: Dynamo input and output is a strict subset of traced input/output, ðŸ› Describe the bug Trying `torchdynamo.export` on Huggingface transformers BertModel. Please let me know if I'm not using this api correctly. torch version: built from master on commit https://github.com/pytorch/pytorch/commit/f1a5044de0639180f667d212800aa43f34026b3c  Error logs   Minified repro Pasting raw repro code since it's about `torchdynamo.export`  Edit: versions: transformers                  4.25.0 ,2022-11-15T20:01:45Z,bug oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/93422,Friendly ping.  Do you happen to know the context of this error? I'm happy to contribute. We are observing this on other huggingface transformer models too., might have more context here.,Should be fixed by https://github.com/pytorch/pytorch/pull/92013
transformer,Transformers model tracing not working," ðŸ› Describe the bug When using torch.neuron.trace to compile a transformer model, an error appears : torch.jit.trace failed with following error: 0INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":607, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Code to reproduce   Error message   Versions PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.21.3 Libc version: glibc2.10 Python version: 3.7.10  (default, Feb 19 2021, 16:07:37)  [GCC 9.3.0] (64bit runtime) Python platform: Linux5.4.01088awsx86_64withdebianbustersid Is CUDA available: True CUDA runtime version: 10.0.130 CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 450.142.00 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda11.0/targets/x8",2022-11-15T17:30:08Z,oncall: jit,open,0,2,https://github.com/pytorch/pytorch/issues/89076,"Hi  , Thanks for raising this issue. May I know which transformers version you use? As discussed in https://github.com/huggingface/transformers/issues/13126, this issue shall be fixed by https://github.com/huggingface/transformers/pull/17176 in transformers repo (since v4.22.0 release) ~3 months ago. To confirm this, I try with transformers v4.21.1 (without the fix) and the issue is reproduced, and when I switch the latest transformers (with the fix), the issue does have been solved and PyTorch tracing works fine;","Hi gu, Thank you for your reply. It works after uploading transformers to v4.22.0."
yi,Towards unifying symbolic and non symbolic fake tensor,"  CC(Towards unifying symbolic and non symbolic fake tensor)  CC(SymIntify convolution backend calculation)  CC(SymIntArrayRef type caster)  CC(Add int64_t, SymInt overloads for all binary operators in C++)  CC(Move ConvParams methods directly on struct)  CC(Hide ConvParams struct from ConvUtils.h) Fake tensor behaves pretty differently depending on if you have symbolic shapes or not.  This leads to bugs; for example, we weren't getting correct convolution_backward strides because we bypassed the correct stride logic in fake tensor on symbolic shapes. This PR attempts to unify the two codepaths.  I don't manage to unify everything, but I get most of it.  The algorithm is delicate and I'm still hosing down test failures. Signedoffby: Edward Z. Yang ",2022-11-15T04:49:39Z,Merged Reverted ciflow/trunk release notes: composability topic: not user facing ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/89038," merge f ""lint only, previous ci was clean"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c ghfirst m ""executorch segfaults""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
transformer,[torch] [analytics] add pytorch event logger callsites to transformers and encoder/decoders,Differential Revision: D41227275,2022-11-11T18:44:20Z,fb-exported Merged ciflow/trunk,closed,0,10,https://github.com/pytorch/pytorch/issues/88896,This pull request was **exported** from Phabricator. Differential Revision: D41227275, merge g, Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[Inductor] Input Buffers Should Be Representable As Storage And Layout," ðŸ› Describe the bug InputBuffer's currently do not have an associated storage. This prevents correct handling of aliased inputs  see TODO here.  When calling require_stride_order, because `is_storage_and_layout` returns False for InputBuffers we fall through to an unnecessary copy.  A shorter term fix would be to handle InputBuffers in `require_stride_order`. ",2022-11-11T01:22:15Z,feature triaged oncall: pt2 module: inductor internal ramp-up task,open,0,0,https://github.com/pytorch/pytorch/issues/93617
rag,module 'torch.cuda' has no attribute '_UntypedStorage'," ðŸ› Describe the bug On a machine with PyTorch version: 1.12.1+cu116, running the following code gets error message `module 'torch.cuda' has no attribute '_UntypedStorage'`. However, the error disappears if not using cuda. The same code can run correctly on a different machine with PyTorch version: 1.8.2+cu111   Error message:   Versions Collecting environment information... PyTorch version: 1.12.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.8.15 (default, Oct 12 2022, 19:15:16)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.052genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 510.47.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] t",2022-11-10T20:23:29Z,,closed,0,7,https://github.com/pytorch/pytorch/issues/88839, ,"This problem doesn't exist in the newer pytorch 1.13. I could fix this on the 1.12 branch, but will there be a 1.12.2 release?","No, 1.13 is out, thanks for confirming .  please reopen if error repros on pytorch 1.13","Hi ,  , I'm stuck with this issue and the problem is I cannot use the latest version of pytorch (currently using 1.12+cu11.3). Is there a workaround? or can I please get some context of why this is occuring?",I ran into this problem as well. At this moment we are not planning to move to pytorch 1.13 yet. Can we reopen this issue and maybe get a backport to 1.12? Thanks!,Pytorch doesn't do backport fixes. ,Thanks for the clarification.
transformer,[RFC] PyTorch DistributedTensor," ðŸš€ The feature, motivation and pitch  RFC: PyTorch DistributedTensor We have been developing a DistributedTensor (a.k.a DTensor) concept under the pytorch/tau repo in the past few months, and now we are moving the implementation over to pytorch with the stack https://github.com/pytorch/pytorch/pull/88180. This RFC proposes the DistributedTensor to torch.distributed. Any early feedbacks are welcomed! **Update**:  DTensor now available in PyTorch 2.0 and nightly build! You can now play around with DTensor even in a colab Notebook! see a quick e2e tutorial here https://colab.research.google.com/drive/12Pl5fvh0eLPUrcVO7s6yY4n2_RZo8pLRscrollTo=stYPKb9Beq4e  Introduction We propose distributed tensor primitives to allow easier distributed computation authoring in SPMD(Single Program Multiple Devices) paradigm. The primitives are simple but powerful when used to express tensor distributions with both sharding and replication parallelism strategies. This could empower native Tensor parallelism among other advanced parallelism explorations. For example, to shard a big tensor across devices with 3 lines of code:   Motivation Today there are mainly three ways ",2022-11-10T19:27:21Z,oncall: distributed module: dtensor,open,33,47,https://github.com/pytorch/pytorch/issues/88838,"Wondering if there are applications here for inmemory sharded _datasets_, a usecase not mentioned above. Example: such that you could randomly access into a massive XTB dataset from any node (assuming a solid IB network). Example: if a satellite image covering the entire world is 8TB  if you need to randomly index into this, instead of obtaining multiple jpeg crops from your disk, i could see this implemented as ona huge sharded tensor, preloaded into the nodes, and any node who needs a slice of this can transparently index into this worldtensor. Would eliminate disk access and use rdma  good for throughput and latency.","> Wondering if there are applications here for inmemory sharded _datasets_, a usecase not mentioned above. Example: such that you could randomly access into a massive XTB dataset from any node (assuming a solid IB network). Example: if a satellite image covering the entire world is 8TB  if you need to randomly index into this, instead of obtaining multiple jpeg crops from your disk, i could see this implemented as ona huge sharded tensor, preloaded into the nodes, and any node who needs a slice of this can transparently index into this worldtensor. Would eliminate disk access and use rdma  good for throughput and latency.  Yeah I think this is entirely possible, for indexing into a massive in memory data set which have the same length per imagine, it looks to me a sharded embedding look up on images, which can be easily implemented with DTensor, underlying we will call into necessary collectives to get a imagine slice according to the index.","Nit: why post here instead of https://github.com/pytorch/rfcs? Edit: never mind, found https://github.com/pytorch/rfcs/pull/44 :)","How much control will users have over the sharding and materialization? E.g., to get decent throughput for parameter sharding or CPU offloading, we'll need some sort of prefetching mechanism. Also, what's the story for saving + loading large, sharded models? Torch Snapshot?","I wanna ask when will models such as DeviceMesh, Shard, distribute_tensor can be used? Will documentation be provided for these models? I want to use the tensordot function to realize the largescale tensor contraction of distributed storage, so how can I code to realize it?","> How much control will users have over the sharding and materialization? E.g., to get decent throughput for parameter sharding or CPU offloading, we'll need some sort of prefetching mechanism.  user will have control about how to implement a operator in a distributed fashion, if you want to apply prefetching mechanism on top, I think it would just running this operator on DTensor with a separate CUDA stream (if it's cuda) or do async offloading just like you do with torch.Tensor > Also, what's the story for saving + loading large, sharded models? Torch Snapshot? To save/load large sharded models, we are working on releasing `torch.distributed.checkpoint` to beta for large scale model save/load as part of next release. Right now the functionality is there, but it's under `torch.distributed._shard.checkpoint` but we plan to make it a dedicated subpackage under torch.distributed https://github.com/pytorch/pytorch/pull/88698  ","> I wanna ask when will models such as DeviceMesh, Shard, distribute_tensor can be used? Will documentation be provided for these models? I want to use the tensordot function to realize the largescale tensor contraction of distributed storage, so how can I code to realize it?  We are working on landing it to pytorch soon, you can subscribe to this stack of PRs https://github.com/pytorch/pytorch/pull/88180 and once those are merged (hopefully this week or next week), you should be able to use it immediately in master or nightly build 12 days later. We plan to release DTensor as a prototype feature in the next pytorch release, which we might add more documentation in the code APIs directly, but not in official https://docs.pytorch.org/ yet. It will be added to the doc website when it releases beta. Note that we will release `DeviceMesh` as a beta feature in the next release, and will add documentations on it and demonstrate how to use it :) Feel free to try it out and submit issues to pytorch/pytorch directly or even contributing once it's there :)","Thanks for the great work! I have several questions. 1. How would existing data loader work with the DTensor? Would some DTensoraware wrapper and sampler be needed here? 2. What is needed to make a custom op support DTensor?  3. Have you considered a DTensor abstraction for heterogeneous devices, e.g., some data on CPU and some data on GPU?"," Thanks for all the questions! > 1. How would existing data loader work with the DTensor? Would some DTensoraware wrapper and sampler be needed here? We haven't plan on letting data loader directly generate DTensor, but for data coming from data loader, you can easily mark the data on each rank as a DTensor by using `DTensor.from_local(tensor, mesh, placements)` (i.e. usually data loader would generate a minibatch data on each rank, and we can create a DTensor that shards on dim 0 (minibatch dim). > 2. What is needed to make a custom op support DTensor? Do you mean a custom registered op in pytorch? In order for a custom registered op to work in DTensor, you might need to register a custom rule to describe how to perform ""distributed"" computation for this op, we will open up a API for user to write the rules. > 3. Have you considered a DTensor abstraction for heterogeneous devices, e.g., some data on CPU and some data on GPU? We have been following the concept of torch.Tensor whenever possible in the design of DTensor, which means a Tensor could either be on CPU or GPU, but it can't be partially be on CPU and partially be on GPU, this is currently also true for DTensor. Could you share more details about your use case of heterogeneous devices on a tensor level abstraction?","> We have been following the concept of torch.Tensor whenever possible in the design of DTensor, which means a Tensor could either be on CPU or GPU, but it can't be partially be on CPU and partially be on GPU, this is currently also true for DTensor. Could you share more details about your use case of heterogeneous devices on a tensor level abstraction? One example is to run inference or training on both CPU and GPU and apply various parallelism schemes just as how it is support on either CPU or GPU via DTensor. With distributed tensor abstraction, users can run such heterogeneous compute without the need of changing the original models.",">  Thanks for all the questions! > > 1. How would existing data loader work with the DTensor? Would some DTensoraware wrapper and sampler be needed here? >  > We haven't plan on letting data loader directly generate DTensor, but for data coming from data loader, you can easily mark the data on each rank as a DTensor by using `DTensor.from_local(tensor, mesh, placements)` (i.e. usually data loader would generate a minibatch data on each rank, and we can create a DTensor that shards on dim 0 (minibatch dim). One nice feature (if not already planned) would be to have a zerocopy API that allows data tiles/fragments loaded into separate training processes to be fused into a single, larger DTensor. This could be useful for large images (e.g., medical domains), long sequence lengths in NLP, etc. ","In the spirit of helping with the design process, here are some more concerns around prefetching / user control. > user will have control about how to implement a operator in a distributed fashion, if you want to apply prefetching mechanism on top I could be misunderstanding, but I don't think this solves the problem, for two reasons. First, running an op asynchronously isn't the same as prefetching. Let's say I have this code:  Making any given layer's op asynchronous doesn't do anything. What I need is more like:  And if we want to get fancy, I might want to:  coalesce prefetches for various tensors,  take into account the weird traversal order induced by gradient checkpointing,  handle edge cases where a user wants to measure tensor norms (in fwd, bwd, or elsewhere!),  and more. The second problem is that I don't want a custom op. I _just_ want my tensors prefetched + freed after use. I don't want to manually add prefetching logic for every op in my program. Anyway, hope that's helpful!","> One example is to run inference or training on both CPU and GPU and apply various parallelism schemes just as how it is support on either CPU or GPU via DTensor. With distributed tensor abstraction, users can run such heterogeneous compute without the need of changing the original models.  are you referring to the case when the whole layer can't feed into GPU memory and we need to do checkpoint offloading to CPU for some DTensors in the middle? If so, I think this can be achieved by composing activation checkpointing/offloading with DTensor, just like it current works with torch.Tensor","> One nice feature (if not already planned) would be to have a zerocopy API that allows data tiles/fragments loaded into separate training processes to be fused into a single, larger DTensor. This could be useful for large images (e.g., medical domains), long sequence lengths in NLP, etc.  Yeah I think we should be able to do this by using things like `DTensor.from_local`, from local does not do any copy of local torch.Tensors, it actually just create a DTensor wrapper that holds all those actual data tiles/fragments that are loaded into separate training processes.","> ... >  > * coalesce prefetches for various tensors, > * take into account the weird traversal order induced by gradient checkpointing, > * handle edge cases where a user wants to measure tensor norms (in fwd, bwd, or elsewhere!), > * and more. >  > The second problem is that I don't want a custom op. I _just_ want my tensors prefetched + freed after use. I don't want to manually add prefetching logic for every op in my program. >  > Anyway, hope that's helpful!  Thanks for clarifying the detailed use case! It looks like you want to use DTensor to prefetch whole layer with sharded DTensor parameters and achieve behaviors similar to FSDP. This is definitely sth DTensor want to do, and we should be able to support this with async `redistribute` ability, then pretch the sharded DTensor on a separate stream. coalescing directly with DTensor needs some careful design, but it could be achievable with sth like fully shard on dim 0 and then cat all the sharded DTensor on other dims, then async redistribute the big DTensor (some rough thoughts on it, we can discuss further about the exact API you want to see)",">  are you referring to the case when the whole layer can't feed into GPU memory and we need to do checkpoint offloading to CPU for some DTensors in the middle? If so, I think this can be achieved by composing activation checkpointing/offloading with DTensor, just like it current works with torch.Tensor Oh, I was referring to a case where both CPU and GPU are involved in computation.","> Oh, I was referring to a case where both CPU and GPU are involved in computation.  could you share a small code example about how both CPU and GPU are involved in computation and what roughly you would like DTensor to help here? Thanks!","> > Oh, I was referring to a case where both CPU and GPU are involved in computation. >  >  could you share a small code example about how both CPU and GPU are involved in computation and what roughly you would like DTensor to help here? Thanks! Sure. The rough idea would be something like below based on the proposed API in this RFC. ","just curious, is there a stable version of pytorch support these DTensor and Shard modules?  I tried to `from torch.distributed import DeviceMesh, Shard, distribute_tensor ` it reports ModuleNotFoundError. I use `torch1.13.1+cu117`"," I think 1.13.1 does not have DTensor implementation yet (just checked the release source code), it lives in nightly build and master if you build from source, see this folder. DTensor would be in prototype release with the upcoming PyTorch 2.0 release.",hi  lots of people use databricks and run a cluster. does DTensor also work with databricks + pyspark running a cluster with CPUs? is there even a way to run it in that way?,"I have a questionï¼ŸNowï¼Œ Could DTensor supports treating CPU memory and GPU memory as a unified Memory pool, and then creating a Tensor on this Memory pool? It has  ""tensor.data/tensor.grad/tensor.grad_fn/tensor.backward()"". Based on the following technologiesï¼š 1. communications technologyï¼š a. Intranode use NVLink/NVSwitch b. Internode use IB c: CPU mem to GPU mem use PCIE I expect a model which runs on a single node can autoimplement model parallel(Tensor parallel/ Pipeline parallel) and data parallel on top of  DTensor + Dist nn.Module.   ",> hi  lots of people use databricks and run a cluster. does DTensor also work with databricks + pyspark running a cluster with CPUs? is there even a way to run it in that way?  Sorry for the late reply. It could possibly run in a cluster with CPUs but probably not together with databricks or pyspark.,> Could DTensor supports treating CPU memory and GPU memory as a unified Memory pool  This might be something we want to explore in long term but not the current focus. The current focus is still around homogenous hardware. > I expect a model which runs on a single node can autoimplement model parallel(Tensor parallel/ Pipeline parallel) and data parallel on top of DTensor + Dist nn.Module.  Yeah this is something we are exploring,"Were there any updates to the API? I'm trying to run the very first example from the README, which appears not to be working (anymore). My full code is as follows:  I'm running this script via `mpirun n 4 python3 hello_dtensor.py`. This results in multiple ranks trying to use the same GPU:  If I change the line that creates the tensor to `big_tensor = torch.randn(1000000, 88, device='cuda:{}'.format(rank))`, the code runs but doesn't appear to be doing anything. I.e., the shapes of the big and distributed tensors are the same. Is there something obvious I'm not doing correctly?","> Were there any updates to the API? I'm trying to run the very first example from the README, which appears not to be working (anymore).  Hey sorry for the late reply! We prototype released the APIs and are currently working on enhancing it to push to beta release.  The issue you observed is because when you first do `init_process_group` then construct `DeviceMesh`, we don't automatically set the cuda device for you, because as part of `init_process_group(backend=""nccl"")`, user need to be responsible to set up the cuda device for each process.  However If you just construct DeviceMesh alone without initializing process group, DeviceMesh will automatically set up the devices on each process. I also updated the README to make everything runnable ","How do we handle fused layers with DTensor? For example, in SwiGLU, there are frequently two input matrices in the FF layer. These two matrices are fused into one big matrix. If we apply DTensor Tensor Parallel to shard this big matrix in the output dimension, the sharding will cross the wrong dimensions. I notice that gptfast and torchtitan both avoid using fused layers because of this problem, but that comes with a performance penalty. With megatron layers, I handle the TP sharding and unsharding myself.","> How do we handle fused layers with DTensor? >  Hey , thanks for bringing this up! For fused layers (i.e. fused SwiGLU or even fused QKV), DTensor should be able to represent its sharding layout by implementing a more complicated sharding layout like stride aware sharding, we are currently working on this but haven't enabled it yet, planning to make this work in the upcoming weeks! > I notice that gptfast and torchtitan both avoid using fused layers because of this problem, but that comes with a performance penalty. The fused layers could be more performant, but I feel it might not give too much e2e perf gains to the Transformer model training. Horizontal fusions would give good wins when the CUDA computations are not saturated but usually the compute are saturated already for LLMs. Either way I think it's better if we could support this out of box, but this does involves some work for a more complicated layouts, so stay tuned :) ","On a transformer, with TP + Sequence Parallel (i.e. what torchtitan is doing), we should theoretically be able to hide all the TP comms inside the computation. Is this planned for DTensor? TP is very slow without it.","> On a transformer, with TP + Sequence Parallel (i.e. what torchtitan is doing), we should theoretically be able to hide all the TP comms inside the computation. Is this planned for DTensor? TP is very slow without it.  TP is expected to be slower due to the oncritical path communications happened in each TransformerBlock, that's why TP always need to happen intrahost (where NVLink enable). TP's value is to enable larger model training (i.e. 70B) with hundreds to thousands of GPUs (where FSDP alone would OOM, please take a look at the tutorial we recently added). We are also working on fusing the compute and communication, but NCCL itself aren't performing well with p2p ops so we are working on some alternative solutions (i.e. we found that with NCCL fusing comm and comp is even slower than just exposing the TP comm).  is making a cuda p2p backend that would make comm/computation overlap be performant, and we expect to make no model code change performance improvement with torch.compile, and provide a simple eager API for users who don't want to enable torch.compile.  Wondering how slow you are experiencing? For performance comparison, one common thing I want to mention is that when enabling FSDP + TP and compare the perf with FSDP only, one should bump their local batch size to multiply it by the `model_parallel_degree/tp_degree` so that both setups have the same global batch size."
yi,Cannot install VSIXTorch, ðŸ› Describe the bug I am trying to install VSIXTorch and I am getting this error message   I have a Visual Studio 2022 installed at the moment  Versions This is for the Torch 1.13 ,2022-11-10T18:01:32Z,module: windows module: cpp triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/88824,cc: aubrecht ," So far, VS 2022 isn't supported for this extension. Please try using VS 2019",   Do you have a plan for when it will be available for VS 2022?,  Can you please give me some context what is the relation of VSIXTorch project to the PyTorch repository? I am mostly currious why this issue is tracked here and not at https://github.com/mszhanyi/VSIXTorch/issues?,VSIXTorch now supports VS2022 https://marketplace.visualstudio.com/items?itemName=YiZhang.LibTorch001
yi,Update lr_scheduler.pyi to match lr_scheduler.py,"Following CC(Publicly expose _LRScheduler to LRScheduler), we should also update the pyi file  ",2022-11-10T16:16:37Z,Merged ciflow/trunk release notes: nn topic: improvements,closed,0,4,https://github.com/pytorch/pytorch/issues/88818, Merge started **The `l` land checks flag is deprecated and no longer needed.** Instead we now automatically add the `ciflow\trunk` label to your PR once it's approved Your change will be merged once all checks on your PR pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Return the attention weights using the Transformer Encoder class. ," ðŸš€ The feature, motivation and pitch Hey, when using the Transformer Encoder class it would be nice to have  as an additional variable that can be passed down to . Right now the feature of returning the attention weights is as far as my understanding goes not accessible from the Transformer class since  is manually set to false. It would be great if this was possible since one could very easily compute the attention rollout.   Alternatives _No response_  Additional context _No response_ ",2022-11-10T14:13:50Z,module: nn triaged,open,1,6,https://github.com/pytorch/pytorch/issues/88810,"Hi Paul! Thanks for bringing up the issue. I think we have attn_weights for TransformerEncoder planned at some point, but not sure on the eta... ","Hi, im interested in picking up this issue for a class along with a partner. Is there anyway we can get more on information on this  ? "," This is a reasonably substantial change. I'd estimate at least a couple of weeks for back and forth? And since it's an API change there will be some level of scrutiny from the PyTorch team you'll have to wait on. If you're ok with that, then yes a PR would be nice. Relevant bit of code is here: https://github.com/pytorch/pytorch/blob/a6d72f44a4e8b6e9d2e878f30fd8b1d3e1197f0e/torch/nn/modules/transformer.pyL541. Rough steps: 1. Add a need_weights arg to TransformerEncoder 2. Pass it into sa_block, return your attn_weights along with regular output only if needs_weights == True. 3. Make sure to disable fast path when need_weights==True by adding a condition in why_not_sparsity_fast_path. PyTorch transformer has a fast inference path. You could actually make need_weights work for the fast path by hooking up a couple of things, but let's skip it for now. Just default to slow path when we have this arg.  4. Add a test (and probably expand existing tests) for this new arg in https://github.com/pytorch/pytorch/blob/master/test/test_transformers.py","   On a site note: it would also be super nice, if the attention matrix could be hooked such that one also obtains the gradient of the Loss w.r.t the attentionmatrix. I guess if is set to true, one would still not be able to actually hook the gradient. I now went the long way around by creating a custom layer that inherits from  and changed . However this is not very clean ","I found that it will take the first component when calculating selfattention. Does it have any specific meaning? According to my understanding, shouldn't it be to keep all the components? In addition, assuming that this operation has practical significance, when `batch_first` is `True` and `False`, the meanings of the components obtained by `[0]` should be different (indicating `batch` and `seq` respectively), so is there any ambiguity? https://github.com/pytorch/pytorch/blob/a6d72f44a4e8b6e9d2e878f30fd8b1d3e1197f0e/torch/nn/modules/transformer.pyL544","  > 4. Make sure to disable fast path when need_weights==True by adding a condition in why_not_sparsity_fast_path. PyTorch transformer has a fast inference path. You could actually make need_weights work for the fast path by hooking up a couple of things, but let's skip it for now. Just default to slow path when we have this arg. The condition should probably also include whether any hooks are attached to submodules that are affected by fast path execution. The fact that hooks seem to be ignored in this case is not really documented and cost me half a night of debugging :sweat_smile:   See exemplary code below.  "
rag,torch.CharStorage cause abort when called with torch.save," ðŸ› Describe the bug `torch.CharStorage` causes abort when called with `torch.save`.   This issue is reproducible on torch 1.11.0+cu113 and 1.14.0.dev20221031, but not reproducible on 1.12.1+cu113 (will raise an exception instead of crashes). Therefore this seems like a regression bug.  Versions torch 1.14.0.dev20221031 ",2022-11-10T02:25:17Z,module: serialization triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/88793, , ,"I found there is another similar crash bug related to numpy array serialization:  Outputs on pytorch 1.12.1:  Dumping numpy arrays will result in abort crash on pytorch 1.11.0 and 1.12.1, but will raise an normal `AttributeError` on nightly (1.14.0.dev20221031). I think such serialization bugs should be resolved in a unified way.","Per the PyTorch docs, the second argument of `torch.save` is supposed to be a filelike object (or string or pathlike): https://pytorch.org/docs/stable/generated/torch.save.html?highlight=torch+savetorch.save Are storages and ndarrays really meant to be filelike objects? Maybe the answer is yes, I just wanted to confirm before investigating further EDIT: Ok I see, the expected behavior is to raise a graceful error instead of abort. I'll fix that"
transformer,[FSDP+dynamo]: forward treats parameter-views as params,"  CC(Enable DDPOptimizer by default in dynamo)  CC([FSDP+dynamo]: forward treats parameterviews as params) Dynamo+AotAutograd needs a way to wrap all tensors (whether inputs or params/buffers) in FakeTensor wrappers, and FSDP's mangling of parameters hides them from this wrapping. This PR unblocks running hf_bert and hf_T5 with FSDP under dynamo, whether using recursive wrapping around transformer layers or only applying FSDP around the whole model.  Perf/memory validation and possibly optimization is the next step. `python benchmarks/dynamo/distributed.py torchbench_model hf_Bert fsdp dynamo aot_eager` `python benchmarks/dynamo/distributed.py torchbench_model hf_Bert fsdp dynamo aot_eager fsdp_wrap` `python benchmarks/dynamo/distributed.py torchbench_model hf_T5 fsdp dynamo aot_eager` `python benchmarks/dynamo/distributed.py torchbench_model hf_T5 fsdp dynamo aot_eager fsdp_wrap` The problem: Dynamo (Actually aot_autograd) trips up with FSDP becuase it must wrap all input tensors in FakeTensor wrappers, and it only knows to wrap graph inputs or named_(parameters, buffers).  FSDP's pre_forward hook sets views (which are not nn.param) into the flatpa",2022-11-09T23:54:14Z,Merged ciflow/trunk release notes: distributed (fsdp) module: dynamo ciflow/inductor,closed,1,4,https://github.com/pytorch/pytorch/issues/88781, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,I think it would be better to desugar them as inputs because there are multpile assumptions throughout inductor about parameter data_ptrs being static,"> I think it would be better to desugar them as inputs because there are multpile assumptions throughout inductor about parameter data_ptrs being static Commenting for my own learning: Could you explain more what ""desugar them as inputs"" entails? Also, to clarify, FSDP _will_ change the data pointers across iterations. If inductor has those assumptions, then what happens when they are violated?"
yi,Bug fix: make sure `copy_impl` doesn't read out of bounds,Stack from ghstack:  CC(Bug fix: make sure `copy_impl` doesn't read out of bounds) Fixes CC(Out of bounds read in `copy_impl` due to unsafe `fbgemm` APIs).,2022-11-05T16:54:16Z,module: crash open source Merged ciflow/trunk topic: bug fixes topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/88544, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Out of bounds read in `copy_impl` due to unsafe `fbgemm` APIs," ðŸ› Describe the bug  Repro   Details This repro will call `copy_impl`, which has a number of calls to `fbgemm::Float16ToFloat_simd`. On my machine, it goes to:  Which dispatches to:  And finally:   Mitigation Pass src size to fbgemm functions and check that dst has enough space before calling them.  Notes I'm currently _not planning_ to verify other `fbgemm` APIs or different code paths in `copy_impl`.  Versions master (dc00bb51b8d370bf3891f0edb2c6e0c2914e329a) ",2022-11-05T16:31:37Z,high priority module: crash module: cpu triaged module: complex bug,closed,0,4,https://github.com/pytorch/pytorch/issues/88543, to investigate why we hit this code path in the first place.,"Well, this feels like classic nullpointer deref: "," I think it's more general: an out of bounds read, see the snippet above. in your example, it fails when trying to access address zero because memory is not mapped, but maybe with some other configuration it could just read out of bounds and not fail at all. i haven't looked very deep.","Note: I'm working on a fix for this segfault now. I'll just fix the crash and we can have another discussion later about how backends should handle stuff like broadcasting because it seems inconsistent (e.g., this fbgemm code and tensor iterator behave differently, which seems like a bug to me). UPD: Since this fbgemm code doesn't support broadcasting, we dispatch to TensorIterator unless the shapes are the same. Otherwise, it leads to inconsistencies (see https://github.com/pytorch/pytorch/pull/88544discussion_r1022299567)."
transformer,PyTorch is not compatible with Python 3.11.0," ðŸ› Describe the bug import argparse import yaml import pandas as pd import torch from TorchCRF import CRF import transformers from data import Dataset from engines import train_fn import warnings warnings.filterwarnings(""ignore"") parser = argparse.ArgumentParser() parser.add_argument(""data_file"", type=str) parser.add_argument(""hyps_file"", type=str) args = parser.parse_args() data_file = yaml.load(open(args.data_file), Loader=yaml.FullLoader) hyps_file = yaml.load(open(args.hyps_file), Loader=yaml.FullLoader) train_loader = torch.utils.data.DataLoader(     Dataset(         df=pd.read_csv(data_file[""train_df_path""]),         tag_names=data_file[""tag_names""],         tokenizer=transformers.AutoTokenizer.from_pretrained(hyps_file[""encoder""], use_fast=False),     ),     num_workers=hyps_file[""num_workers""],     batch_size=hyps_file[""batch_size""],     shuffle=True, ) val_loader = torch.utils.data.DataLoader(     Dataset(         df=pd.read_csv(data_file[""val_df_path""]),         tag_names=data_file[""tag_names""],         tokenizer=transformers.AutoTokenizer.from_pretrained(hyps_file[""encoder""], use_fast=False),     ),     num_workers=hyps_file[""num_workers""",2022-11-04T20:27:12Z,,closed,1,2,https://github.com/pytorch/pytorch/issues/88518,Sample project https://github.com/donhuvy/vy_thesis . Related   https://stackoverflow.com/questions/74322227/packagerequirementstensorflow2100transformers4221torchcrf1  https://stackoverflow.com/questions/74322491/ispytorchversion1121compatiblewithpython3110,There's already an open issue tracking this  CC(Support Python 3.11). Closing this one in favour of that other one.
llm,Generalize gesvdjBatched to run whith full_matrices==false,"  CC(Generalize gesvdjBatched to run whith full_matrices==false) As brought up in  CC(SVD with `full_matrices=False` slower than `full_matrices=True` & `full_matrices=False` on GPU 500x slower than CPU)issuecomment1268296036, our heuristic for which SVD backend to choose was not great in some cases. The case in which there could be some improvements is when we have a large batch of very small nonsquare matrices. This PR, adapts the calling code to gesvdj by creating two temporary square buffers to allow to call gesvdjBatched, and then copies back the result into the output buffers. We then modify the heuristic that chooses between gesvdj and gesvdjBatched. ://github.com/pytorch/pytorch/issues/86234",2022-11-04T16:41:33Z,module: performance open source module: linear algebra Merged ciflow/trunk release notes: performance_as_product topic: performance,closed,2,7,https://github.com/pytorch/pytorch/issues/88502,The benchmarks show that the batched version is always faster: These are run on CUDA 11.7.1 on an RTX 2060 , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Linear Algebra`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," I deleted a test that was testing the behaviour of the `svd` function when given `NaN`s. This behaviour is not defined in the docs, and it's not consistent between backends or drivers, so I don't think we should test it. For example, this new backend does not raise an error when an input with nans is provided, it simply returns `S = [nan, nan, ...]`.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Support src_mask and src_key_padding_mask for Better Transformer,"Fixes T135842750 (followup for CC(Enable `src_mask` in fast path of `TransformerEncoderLayer `))  Description At present, having both `src_key_padding_mask` and `src_mask` at the same time is not supported on the fastpath in Transformer and MultiHead Attention. This PR enables using both masks on the fastpath on CPU and GPU: if both masks are passed, we merge them into a 4D mask in Python and change mask type to 2 before passing downstream. Downstream processing in native code is not changed, as it already supports 4D mask. Indeed, it is done depending on the device:  on CUDA, by `SoftMax.cu::masked_softmax_cuda`. When mask type is 2, it calls either `dispatch_softmax_forward` > `softmax_warp_forward` or `at::softmax` (depending on the input size). In both cases 4D mask is supported.  on CPU, by `SoftMax.cpp::masked_softmax_cpp`. It calls `hosted_softmax` which supports 4D mask.  Tests  Extended `test_mask_check_fastpath` to check that fast path is indeed taken in Transformer when two masks are passed  Added `test_multihead_self_attn_two_masks_fast_path_mock` to check that fast path is taken in MHA when two masks are passed  Added `test_multihead_se",2022-11-04T12:47:27Z,Merged ciflow/trunk release notes: nn,closed,0,14,https://github.com/pytorch/pytorch/issues/88488," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / cuda11.6py3.10gcc7sm86 / test (default, 2, 4, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," The merge above failed, but the errors seem to be unrelated to the PR:  ""ERROR ENCOUNTERED WHEN UPLOADING TO SCRIBE"",  ""KeyError: 'jobs'"" in ""Get workflow job id""   NVIDIA kernel loading error. Could you have a look and say if those are indeed known infra failures?", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `supporttwomasksbettertransformer` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout supporttwomasksbettertransformer && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Add use_lazy_shape flag to GenLazyIr class,"Add use_lazy_shape flag to GenLazyIr class to allow XLA to use its custom shape class. The default value is kept to use lazy shape, so this PR does not introduce any new behaviors.  PyTorch/XLA companion PR: https://github.com/pytorch/xla/pull/4111",2022-11-03T19:42:29Z,open source Merged ciflow/trunk topic: not user facing,closed,1,6,https://github.com/pytorch/pytorch/issues/88444,", this is a small followup to https://github.com/pytorch/pytorch/pull/87823. Adds a new flag `use_lazy_shape` to LazyIr codgen, so PyTorch/XLA codegen doesn't have to override the entire `GenLazyIr.gen()` function. ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 additional jobs have failed, first few of them are: trunk Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,TorchDynamo: Add convolution binary(inplace) fusion for cpu in inference mode,  CC(Take input striding for conv fusion op  based on eager output)  CC(TorchDynamo: skip convolution fusion when convolution's padding is string)  CC(Fake Tensor For (ConvFusion) Propagation)  CC(TorchDynamo: Add convolution binary+unary fusion for cpu in inference mode)  CC(TorchDynamo: Add convolution binary(inplace) fusion for cpu in inference mode) ,2022-11-03T04:51:50Z,open source Merged ciflow/trunk module: inductor ciflow/inductor release notes: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/88403," , please help review this PR. Thanks! ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Test opinfo's aten op coverage,  CC(Test opinfo's aten op coverage),2022-11-02T20:33:17Z,Stale topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/88358,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Modifying lazy backend code for SPMD,Fixes ISSUE_NUMBER,2022-11-02T17:54:55Z,open source,closed,0,1,https://github.com/pytorch/pytorch/issues/88340," :x:  login: g . The commit (d599b470c1e6f931e6eeb80f420c32e18b585d62) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-02T15:55:13Z,Merged ciflow/trunk release notes: vulkan,closed,0,9,https://github.com/pytorch/pytorch/issues/88324, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge f," merge f ""unrelated cuda job failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,[dynamo] `VariableTracker.call_method` requires a name,"Summary: as title Test Plan: Before: N2743445, After: N2748186.  Note there's a new error, but at least we got past the easy one. Differential Revision: D40938415 ",2022-11-02T14:29:00Z,fb-exported Merged ciflow/trunk release notes: dataloader module: dynamo ciflow/inductor,closed,0,16,https://github.com/pytorch/pytorch/issues/88311,This pull request was **exported** from Phabricator. Differential Revision: D40938415,This pull request was **exported** from Phabricator. Differential Revision: D40938415, rebase, successfully started a rebase job. Check the current status here,"Tried to rebase and push PR CC([dynamo] `VariableTracker.call_method` requires a name), but it was already up to date", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Move the OpInfo same-storage error to the autograd test,"  CC(Move the OpInfo samestorage error to the autograd test) This check was previously located at the `non_contiguous` test (quite and odd location). Even more, at https://github.com/pytorch/pytorch/pull/86378discussion_r993658395, Kshiteej found that this assert was not doing anything really. We move it to the autograd test and make it a proper `self.assert`. We also disallow returning 1tuples from sample_input functions, as they were breaking this assert.",2022-11-02T13:02:11Z,open source Merged module: testing ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/88306,">We also disallow returning 1tuples from sample_input functions, as they were breaking this assert. No objection to the change, but can you explain how it breaks this check?","That's a good point.  I tried reproducing this issue and I was not able to, so I'm not sure now what was going there really. I'm leaving them all as generators for simplicity and consistency of the code though.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. You can rebase by leaving the following comment on this PR: ` rebase` Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / cuda11.6py3.10gcc7sm86 / test (default, 1, 4, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""flaky CI tests"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-02T03:29:13Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88285
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-02T00:12:36Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88274
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T23:49:46Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88270
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T22:00:28Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88256
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T19:20:30Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88240
rag,[codegen] using TORCH_LIBRARY_FRAGMENT for some namespaces,"  CC([codegen] using TORCH_LIBRARY_FRAGMENT for some namespaces) Summary: Sometimes we want to extend an existing custom namespace library, instead of creating a new one, but we don't have a namespace config right now, so we hardcode some custom libraries defined in pytorch today, i.e. quantized and quantized_decomposed Test Plan: ci Reviewers: Subscribers: Tasks: Tags:",2022-11-01T18:58:48Z,Merged ciflow/trunk topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/88229,Can you fix the failing test?,"> Can you fix the failing test? done, please take a look again","IMO, this should be configurable on a per namespace basis. If custom namespace lives entitely in yaml it is reasonable to want it to be closed.","> IMO, this should be configurable on a per namespace basis. If custom namespace lives entitely in yaml it is reasonable to want it to be closed. I think one way to support this is to add a tag for a function. For example,  This means that we are expecting a `quantized` library already exist in PyTorch (`quantized/library.cpp`) and just extending it by adding a new `add.out` operator.","> > IMO, this should be configurable on a per namespace basis. If custom namespace lives entitely in yaml it is reasonable to want it to be closed. >  > I think one way to support this is to add a tag for a function. For example, >  >  >  > This means that we are expecting a `quantized` library already exist in PyTorch (`quantized/library.cpp`) and just extending it by adding a new `add.out` operator. should this config be library specific instead of operator specific?",The config should not live on an op. Should be somewhere library info., merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / linuxbioniccuda11.7py3.10gcc7 / test (distributed, 1, 3, linux.8xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge f," merge f ""failing tests unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T18:25:05Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88225
yi,Add missing args to DDP constructor in distributed.pyi,Summary: As title. And remove all unnecessary `pyrefixme` for the unknown arg in callsite. Test Plan: CI Differential Revision: D40874013,2022-11-01T16:30:54Z,fb-exported Merged ciflow/trunk,closed,0,6,https://github.com/pytorch/pytorch/issues/88209,This pull request was **exported** from Phabricator. Differential Revision: D40874013,This pull request was **exported** from Phabricator. Differential Revision: D40874013,This pull request was **exported** from Phabricator. Differential Revision: D40874013,This pull request was **exported** from Phabricator. Differential Revision: D40874013, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Update functionalization metadata more eagerly. *_scatter ops should preserve input stride/storage_offset,"Two major changes in this PR: (1) outputs of `*_scatter` ops will now reflect the same storage size, stride and storage_offset of their inputs. See more details at https://github.com/pytorch/pytorch/pull/87610/filesr1007264456. This fixes some silent correctness issues with functionalization advertising incorrect strides/storage_offsets on some tensors. (2) That actually isn't enough: We need to ensure that any time someone calls `.stride()` or `.storage_offset()` on a tensor, its metadata is not stale. To fix this, I updated all of the custom metadata calls on `FunctionalTensorWrapper` to perform a sync first if metadata is out of date. As a motivating example, consider this code:  Functionalization will temporarily turn this code into:  As mentioned in the above comment, this isn't 100% correct  `a_diag_updated` has different metadata than `a_diag`. If user code queries the metadata on `a_diag` at this point, we'll return incorrect metadata. The fix: By eagerly running syncing logic before any metadata on `a_diag` is accessed, we'll do the following:  This ensures that any user code that calls `a_diag.stride()` or `a_diag.storage_offset()` sees ac",2022-11-01T15:30:05Z,,closed,0,7,https://github.com/pytorch/pytorch/issues/88198," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Darn, I'm getting the cursed error:  Maybe, somehow when we regenerate the views for a mutated view, that regenerate tensor isn't getting proxies attached to its sizes/strides","So I am actually not sure why you need to sync_ the metadata on each pass now. Don't you just need to update the inplace tensor metadata; the other tensors not changing their sizes and strides is fine, because whatever they used to have, should still be valid!","Yeah, I think you're right. At first I was worried about other views of the base that also came from slice/select calls, but their metadata should be accurate to begin with  the only time the metadata becomes ""wrong"" is when we convert an inplace op into an outofplace.","There are a bunch of things going on here, but when I ablate everything except the clone preserving strides changes (https://github.com/pytorch/pytorch/pull/89474) it still fails `python test/test_ops_fwd_gradients.py k test_forward_mode_AD_as_strided_scatter_cpu_complex128`","So, I'm a little confused about whether or not this PR is still necessary. The stated justification was to make ""do not use unsafe restriding for subclasses"" PR be able to go in, c.f. https://github.com/pytorch/pytorch/pull/87610/files/401ddeda1d769bedc88a12de332c7357b60e51a4diffc4e35e76279419d9edda979a995412ee494fad3a97d3fb59486a1f0e326d48be But as best as I can tell, this already has gone in to master without trouble. So is this just for a hypothetical situation where functionalization can produce incorrect strides?",we think this is obsolete
yi,misc symintifying fixes,"  CC(Update functionalization metadata more eagerly. *_scatter ops should preserve input stride/storage_offset)  CC(misc symintifying fixes)  CC(dont clone symints, dont clobber symint proxies)",2022-11-01T15:30:00Z,Stale release notes: composability topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/88197,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Add ability to freeze storages inside functionalization,  CC(Add ability to freeze storages inside functionalization)  CC(Inline Alias into FunctionalStorageImpl) Signedoffby: Edward Z. Yang ,2022-10-31T21:42:18Z,Merged ciflow/trunk release notes: composability,closed,0,6,https://github.com/pytorch/pytorch/issues/88141,"> To be clear though  is it important for this to be exposed to python? (doesn't matter too much since it's a private API). In the prototype at https://gist.github.com/ezyang/eace99f43628422139d476bf6919782e the POC is done entirely in Python so yes I need an access point, but I don't intend to make it public.", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""labels have been added"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Inline Alias into FunctionalStorageImpl,  CC(Add ability to freeze storages inside functionalization)  CC(Inline Alias into FunctionalStorageImpl) Signedoffby: Edward Z. Yang ,2022-10-31T21:42:14Z,ciflow/trunk release notes: composability topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/88140," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
chat,Formalize instructions for customers to make request to the DevX team,"**What** Create wiki page that explains to customers how they should file an issue to the DevX team The instructions should explain: 1. Where to file the ticket 2. How to get it added to the DevX board for grooming The idea is to have a link that we'll share with customers whenever they file a support request **Why** We currently get customer requests on multiple different channels (Workplace, Slack, Chat, and more). It's easy for requests to get missed/dropped on there. By ensuring all requests have a ticket attached that we can view from one central location, we can be more responsive about triaging, prioritizing, and implementing customer requests",2022-10-31T18:42:24Z,triaged module: infra,closed,0,1,https://github.com/pytorch/pytorch/issues/88111,"Added general instructions on how to get help, and specific instructions on how to file issues, in https://github.com/pytorch/pytorch/wiki/Gettinghelpasacontributorfileanissue"
yi,Update _distributed_c10d.pyi,Summary: `_distributed_c10d.pyi` is out of sync with the C++ binding. This change updates it. Test Plan: TBD Differential Revision: D40840836,2022-10-31T13:55:43Z,fb-exported Merged ciflow/trunk release notes: distributed (c10d),closed,0,6,https://github.com/pytorch/pytorch/issues/88088,The committers listed above are authorized under a signed CLA.:white_check_mark: login: yhcharles / name: Charlie Yan  (daa002644c5bfbc75eb02ccf7445a89509b5a5a2),This pull request was **exported** from Phabricator. Differential Revision: D40840836,This pull request was **exported** from Phabricator. Differential Revision: D40840836,/easycla, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,AttributeError: 'BertModel' object has no attribute 'save', ðŸ› Describe the bug Tried to save the model using jit after dynamic quantization using the following code  But it shows the below error  Even the code   also shows the same error  Versions torch @ https://download.pytorch.org/whl/cu113/torch1.12.1%2Bcu113cp37cp37mlinux_x86_64.whl transformers==4.23.1 Python 3.7.15 ,2022-10-31T11:11:12Z,oncall: quantization,closed,0,8,https://github.com/pytorch/pytorch/issues/88077,"looks like this is an issue with the AutoModel object, the actual quantization didn't have any errors, its just that this custom object doesn't have a save function, did you try the state dict method?",  as you suggested I tried with  method like this  but it shows that ,  Seems it is not an issue with Automodel class. Because the Bertmodel also shows the same error   Shows ,we have a tutorial for Bert: https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html can you try the serialization code there and see if addresses this issue?,>  as you suggested I tried with `state_dict` method like this >  >  >  > but it shows that `AttributeError: 'collections.OrderedDict' object has no attribute 'save'` also i think you are mixing up serialization (jit) with saving (state_dict),"  okay, could you please share the code snippet here for dynamic quantization of bert model and its save procedure using jit? ","  Also how can I define the second argument(example input) in torch.trace method  What should be the **values**, **dimension** and **type** of the example input argument that  method excepts?","the serialization+save is here: https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.htmlserializethequantizedmodel the example input is a tensor input or tuple of inputs, can find more info here: https://pytorch.org/docs/stable/generated/torch.jit.trace.html?highlight=torch+jit+tracetorch.jit.trace"
llm,Inductor accuracy failure from PegasusForCausalLM (maybe bmm related)," ðŸ› Describe the bug Applying https://github.com/pytorch/pytorch/pull/87943 plus some fixes perturbs the output of AOTAutograd sufficiently to cause inductor to generate an accuracy failure. Don't worry, I have a repro >:)  Error logs   Minified repro https://gist.github.com/ezyang/23655080188e0d31ae12525ec5c8762c ",2022-10-30T02:13:59Z,triaged bug,closed,0,6,https://github.com/pytorch/pytorch/issues/93595,"With one change, I triggered DebertaForQuestionAnswering and PegasusForCausalLM, so DebertaForQuestionAnswering might be failing similarly. I'll try minifying it as well. UPDATE: Probably not related","This seems like fp accuracy issue, I'm getting  and for some reason we cannot generate fp64 ref, because otherwise we would compare ref and res with fp64 and see who is further away (which could have cured this). ","Here is another repro with a similar problem, ","So the problem, per above, is not comparing with fp64 (because `convert_element_type` hardcodes fp32 dtype, and we cannot generate fp64 reference), fixing that in the repro makes the repro pass (ref and res are the same distance from correct fp64 answer). "," so there might be a bug somewhere, but this minified repro doesn't uncover it.   how do we handle comparisons when fp64 computation fails? There might be quite a few cases, due to hardcoded fp32 conversions. Currently we just compare `ref` and `res`, but it might result in false failures, like here. "," , is there any reason that we didn't turn on `cos_similarity` for `same_two_models`? Turning that on does make these two minified tests we are discussing here pass."
llm,Minifier transforms accuracy failure into runtime failure (on PegasusForCausalLM)," ðŸ› Describe the bug Checkout and build PyTorch at e4a8661ab84022c1bff622c6d2f6e679180b1df5 Here is a minifier: https://gist.github.com/ccca1bf4703b4ed8cf1ac7c3b0ab7b9e When I hack up the minifier to run the test, it fails with an accuracy error. https://gist.github.com/b69c1f74caf6b1f7419dff7a37191c14  However, when I finished minifying, I got the repro at  CC(Inductor gives obscure error when FX graph to be compiled returns tuple) which no longer fails with an accuracy error, it just hits an assert. Generating asserts is useful. But I would prefer not to be blocked on fixing the assert failure before I can minify the accuracy failure.  Error logs _No response_  Minified repro _No response_",2022-10-30T01:25:13Z,triaged bug,closed,0,3,https://github.com/pytorch/pytorch/issues/93594,"According to  the minifier replaced the list output with a tuple, triggering inductor compile failure, and then happily minified the program down to nothing.",Here is a shitty patch that forces minifier to only consider failures at the end failures  It doesn't work correctly; at the end of minifying the minifier screwed the pooch  but I did get a useful checkpoint that does still accuracy fail,"Closing, minifier is propably broken in other ways now"
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-10-28T14:29:26Z,release notes: vulkan,closed,0,1,https://github.com/pytorch/pytorch/issues/87985," :x:  login:  / name: Kimish Patel . The commit (0db76c7e205d20832fee738057500dc8bd2e5830) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."
yi,[PT 1.13] resize bilinear yields different results in newer versions," ðŸ› Describe the bug First, I tried to generate the inputs with the following code with PyTorch 1.13.  Then I tried to do the comparison with the Python environment with PyTorch 1.10.  As can be seen, the results are not within numerical differences caused by floating point representation. Attachments: bilinear_data.zip  Versions PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.6 (x86_64) GCC version: Could not collect Clang version: 12.0.0 (clang1200.0.32.28) CMake version: version 3.18.0 Libc version: N/A Python version: 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang1200.0.32.2)] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.4 [pip3] onnx2pytorch==0.4.1 [pip3] onnx2torch==1.3.0 [pip3] pytorchlightning==1.6.3 [pip3] torch==1.10.0 [pip3] torchmetrics==0.8.2 [pip3] torchvision",2022-10-28T07:05:46Z,module: cpu triaged module: regression module: interpolation,closed,1,5,https://github.com/pytorch/pytorch/issues/87968,"Fun fact, `python c ""import torch;x=torch.rand(10);torch.save('foo', x)""` crashes the runtime, we should fix it","5 does this sounds like an acceptable change of accuracy to you? But in general, 2e5 difference for small issues seems fine.","Checking in details the repro code on different versions: 1.10, 1.12, 1.13 IMO, precision issue is related to https://github.com/pytorch/pytorch/commit/658f958bc4bb314d9c6030eeaf3e1784792b5d15  no issue between 1.10 and 1.12 , 1.10 and 1.13.0.dev20220802+cpu, 1.13.0.dev20220902+cpu and 1.13.0  reported issue between 1.13.0.dev20220802+cpu and 1.13 There is no issue between 1.10, 1.12 and 1.13 if `align_corners=False`.   any ideas about the loss of precision here ?  Shouldn't we also static_cast `scale` here ? https://github.com/XiaobingSuper/pytorch/blob/f3ad87eae4a0dccb2e8aa01e379e99e67194dc49/aten/src/ATen/native/UpSample.hL289 ","Ok, reverting CC(fix upsample bf16 issue for channels last path by using high pricsion to compute index) brings down divergence to 0. Modified the test to compute upsample from an torch.arange, to avoid dealing with any denorms, i.e. collecting data using following script:  and the difference between 1.12 and 1.13 remains `1.5e5`: ",Somehow following fixes it:  I..e results from 1.13 are technically not wrong (as `accscalar_t` for float is double)
yi,[FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`,Stack from ghstack:  CC([FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`) [FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by ``!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration * * CC([FSD,2022-10-27T22:22:57Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/87941, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 33b85d292b06dac62af023ffb2f6af509e99828d` returned nonzero exit code 1  Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[FSDP()][24/N] Refactor `_lazy_init()`,Stack from ghstack:  CC([FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`) [FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by ``!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration  CC([FSDP(),2022-10-27T22:22:41Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/87939, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[FSDP] Simplify `_reset_lazy_init()`,Stack from ghstack:  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by `contract`!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration  CC([FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`) [FSDP()][26/N] Move `_lazy_init()` into `_fs,2022-10-27T22:22:25Z,ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/87937,I coalesced this with https://github.com/pytorch/pytorch/pull/87938 because I realized they should be landed together as an atomic change.
yi,[FSDP()][22/N] Refactor `_cast_buffers()` in `_lazy_init()`,Stack from ghstack:  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by `contract`!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration  CC([FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`) [FSDP()][26/N] Move `_lazy_init()` into `_fs,2022-10-27T22:22:17Z,ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/87936,I coalesced this PR with the preceding one (https://github.com/pytorch/pytorch/pull/87935) since I needed to fix `_cast_buffers()` altogether. I will close this one soon.
transformer,AttributeError: 'torch.dtype' object has no attribute 'numel', ðŸ› Describe the bug Tried the following code for dynamic quantization  And it shows the following error while executing   Versions Python 3.7.15 transformers==4.23.1 torch==1.12.1 ,2022-10-27T04:41:52Z,oncall: quantization,closed,2,3,https://github.com/pytorch/pytorch/issues/87843,I'm also facing the same issue **Versions** Python 3.7.15 transformers==4.23.1 torch==1.14.0.dev20221027+cpu,  Because the quantized model contains the  object which is not handle by the   method  now,this is all related to save_pretrained not being made to work with quantized models. quantized models have specific supported methods of saving/serialization so errors are expected when other methods are used. in general you need to use state_dict to save https://pytorch.org/docs/stable/quantization.htmlsavingandloadingquantizedmodels we have a tutorial for Bert which has code for serialization: https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html There's also this other thread that is dealing with the same issue:  CC(AttributeError: 'BertModel' object has no attribute 'save')
transformer,Any ideas on how we can convert a model from huggingface (transformers library )to tensorflow lite?," ðŸ› Describe the bug I want to convert CamembertQuestionAnsewring model to tensoflow lite, i download it from huggingface platform, because when i want to save the model locally it gives me the model with 'bin' format.    i'm asking here because huggingface use pytorch pretrained models.  when i try to convert the model it gives me this error : AttributeError: 'CamembertForQuestionAnswering' object has no attribute 'call' by using tf_model.h5 file.   Also i can't load it using : tf.keras.models.load_model() it gives me : ValueError: No model config found in the file at .  when i want to save the transformers model locally it gives me the model with 'bin' format, so i download it from the platform.  Versions https://huggingface.co/etalabia/camembertbasesquadFRfquadpiaf?context=Etalab+est+une+administration+publique+fran%C3%A7aise+qui+fait+notamment+office+de+Chief+Data+Officer+de+l%27%C3%89tat+et+coordonne+la+conception+et+la+mise+en+%C5%93uvre+de+sa+strat%C3%A9gie+dans+le+domaine+de+la+donn%C3%A9e+%28ouverture+et+partage+des+donn%C3%A9es+publiques+ou+open+data%2C+exploitation+des+donn%C3%A9es+et+intelligence+artificielle...%29.+Ainsi%2C+Etalab+d%C3%A",2022-10-26T16:00:34Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/87789,this is not a question answering forum
transformer,Unable to see the weight files after quantization," ðŸ› Describe the bug I have tried the following code for dynamic quantization  After the execution, I could see that there is a new folder named **quantized** created in the directory which contains only the  file. contents are as follows  I can't see any other **.bin** or **.wt** files after quantization. Why it is so?  Versions Python 3.7.15 transformers==4.23.1 torch==1.12.1 ",2022-10-26T14:53:43Z,oncall: quantization,closed,0,2,https://github.com/pytorch/pytorch/issues/87784,"hi 1995 , `save_pretrained` is not a PyTorch API.  Are you asking about HuggingFace's `save_pretrained` function?  In that case, maybe post on their discussion forum?","are you following the supported method for saving/loading quantized models? https://pytorch.org/docs/stable/quantization.htmlsavingandloadingquantizedmodels my guess is whatever save_pretrained does, isn't aligned with quantization out of the box since  for example  there are no longer weight tensors because they have been packed into the quantized module"
transformer,[NVFuser] Upstream push 1026,"  CC([NVFuser] Upstream push 1026) Syncing nvfuser devel branch to upstream master. https://github.com/csarofeen/pytorch/ Codegen changes include: * codegen improvement:     i. allow nonroot trivial reductions, allow empty/noop fusion     ii. fixes vectorization checks and size calculation     iii. bank conflict handle improvement     iv. enables transpose scheduler * misc:     i. CI tests failure fixes     ii. cpp tests file clean up     iii. trivial forwarding supports added in codegen runtime     iv. added factory methods support in codegen Commits that's in this PR from the devel branch:  RUN_TORCHBENCH: nvfuser Differential Revision: D40869846",2022-10-26T09:34:52Z,open source Merged ciflow/trunk release notes: jit skip-pr-sanity-checks,closed,1,12,https://github.com/pytorch/pytorch/issues/87779,"Quick update on things that's being worked on / tracked: 1. Maxwell failure is coming from synchronization... we relies on thread independent scheduling for reduction, which doesn't work with prevolta device. With transpose scheduler enabled, now we can hit synchronization even for pw kernels. We need to disable nvfuser on prevolta device and update our python tests to skip those. I know that we recently added AXX GPUs on CI, just want to make sure that we are still at least running something to check the functional correctness. 2. There's some codegen failures regarding empty fusion with concrete shapes....  is working on that :stuck_out_tongue_closed_eyes:  3. I also saw a few codegen error on ROCM. Looks like there's some missing synchronization templates in the runtime system.   "," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Added `RUN_TORCHBENCH: nvfuser` in the commit message, hopefully this will trigger torchbench runs."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",Failure looks strange and seems like there's just no runner picking it up?!?! The upstream commit I have looks clean on PyTorch CI HUD. I'm trying it again.,Strangely I don't see any log from the CI failure..... I'll try rebasing. :crying_cat_face:  ," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",CI are green now!!! Yay... Turns out there was no real failure but flaky tests :crying_cat_face:  cc'ing  ," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,fix sym_storage conversion and some cleanup,  CC(Fix all references to torchdynamo from the merge)  CC(fix sym_storage conversion and some cleanup) ,2022-10-25T18:58:59Z,Merged ciflow/trunk release notes: fx,closed,0,3,https://github.com/pytorch/pytorch/issues/87718," merge f ""test failures all look fake"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Pass in Storage Offset For Input Tensors,  CC(Pass in Storage Offset For Input Tensors) Fix for  CC([Bug]: TorchInductor Input As_Strided Calls Dont Compose With Offset Inputs) ,2022-10-25T17:23:27Z,Stale ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/87709," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.", rebase, successfully started a rebase job. Check the current status here,Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/eellison/344/orig` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/3517976326,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,"KPI: Average time to first response, average time to respond to any turn over message",Integrate with GitHub API to collect:  average time to first response   average time to turn over message Data will be collected only from GitHub discussions.,2022-10-25T14:56:32Z,module: windows triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/87699,"Grafana plugin we use doesn't have support for fetching comments and related metadata. I investigated few options and tested some stuff.  **Details** I was able to fetch all necessary metadata and parse it through curl + Powershell. Thou GitHub requires paging (max 100 comments at a time), so it's not just single query to GitHub, but some caching + crawling mechanism is needed. **Options** We do have these options for this ticket:  Drop it for now in case we evaluate it does not worth the effort.  Extend Grafana GitHub plugin (implementation wise plugin seems fairly short and simple, my guess would be couple of days of development. What I don't know at this moment is how deployment & merge part would go ...)  Write some REST API in Azure (serverless API?) Implementation wise second and third options are quite similar, they need to crawl and cache. Benefits / Disbenefits:  GitHub plugin  No cost for us, no maintanance needed. Cache is already implemented in plugin (might be needed to extend the API).  REST API  Easier to deliver than GitHub plugin (no need to learn existing code, no problems with delivery). Cost us regularly money and we'll need to maintain it. **Results of internal discussion** At this moment these metrics do not worth investment. Closing ticket as won't fix."
rag,KPI: Average age of issues,Integrate with GitHub API to collect:  average age of issues with `module: windows` tag.,2022-10-25T14:52:42Z,module: windows triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/87698,Average age of the issues are now shown in 'Days' format.
rag,[LTC] Remove tensor.storage_,"Summary: Since LTC now supports functionalization, we don't need to fake a storage to support is_alias_of anymore. Let's remove it. Test Plan:  ./build/bin/test_lazy gtest_filter=LazyOpsTest.IsAliasOf",2022-10-24T22:24:33Z,open source Merged ciflow/trunk,closed,0,27,https://github.com/pytorch/pytorch/issues/87645,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: alanwaketan / name: Jiewen Tan  (a9bad4c9c5e75e014ba96835f63601729f0b2215, 7534405c73e1ab5655f2235b0a32ee77fdc04297, 8965d36a3e239f8076cbf969ba8ce20d0e1a949d)",The iOS failures don't seem to be related.,Thanks Jack and Brian for approving this pull request.,  merge, Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  EasyCLA Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 additional jobs have failed, first few of them are: build Details for Dev Infra team Raised by workflow job ", merge f," merge f ""The iOS failures are not related.""", merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""The iOS failures are not related.""", merge,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
agent,Fix use after free in tensorpipe agent,"Stack from ghstack: * * CC(Fix use after free in tensorpipe agent) Fix use after free in tensorpipe agent** Fixes CC(Use after free in TensorPipeAgent), which identifies use after free for reverse device maps. This is only in the dynamic RPC feature and not effecting stable RPC code path. Unfortunately the test `TensorPipeRpcTest.test_dynamic_rpc_existing_rank_can_communicate_with_new_rank_cuda` that is failing is also running into separate issue. I've temporarily disabled some of the test code to investigate the error in asychronously. Testing plan:  tested all the dynamic RPC tests",2022-10-24T19:30:44Z,Merged ciflow/trunk release notes: distributed (rpc) topic: bug fixes,closed,0,3,https://github.com/pytorch/pytorch/issues/87627,, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Reenable test_fake_crossref_backward_amp_cholesky_inverse_cuda_float32,  CC(Reenable test_fake_crossref_backward_amp_cholesky_inverse_cuda_float32)  CC(Disallow duplicated py_imp registration)  CC(Fix checking if CompositeImplicitAutograd key exists in Library)  CC(Prefer python meta function over c++ meta function),2022-10-22T12:55:49Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/87548
yi,RAM leak when copying tensor from cpu to cuda," ðŸ› Describe the bug If you profile run the following code then you'll see that line 8 grabs ~2GB of RAM, but it doesn't get released: ` import gc import torch from memory_profiler import profile  def test(n):     t1 = torch.ones(n)     t2 = torch.ones(n).to(""cuda"")     del t1     del t2     gc.collect()     torch.cuda.empty_cache() if __name__ == ""__main__"":     test(3000000) `  Versions PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.12 (default, Sep 10 2021, 00:16:05)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.214120.368.amzn2.x86_64x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 510.47.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] botorch==0.2.1 [pip3] gpytorc",2022-10-22T02:17:55Z,module: cuda module: memory usage triaged,open,0,4,https://github.com/pytorch/pytorch/issues/87539,"This is caused by lazy loading cuda libraries at this point, the memory is not expected to get released. "," Thanks for replying! Could you elaborate, please?", any updates? ,"It's not a leak, it's RAM required by cuda libraries"
yi,Disallow duplicated py_imp registration,"  CC(Consistent meta reg across dispatcher, pydispatcher, active_meta_table)  CC(Fix stride for prims.where)  CC(Fix _refs for aten.zeros/ones/empty/randn)  CC(Disallow duplicated py_imp registration)  CC(Fix checking if CompositeImplicitAutograd key exists in Library)  CC(Prefer python meta function over c++ meta function)",2022-10-21T21:03:11Z,ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/87503," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.",Folded into https://github.com/pytorch/pytorch/pull/87426
rag,as_strided_scatter storage offset defaults to None not 0,  CC(as_strided_scatter storage offset defaults to None not 0) Signedoffby: Edward Z. Yang ,2022-10-21T16:57:55Z,Merged ciflow/trunk,closed,0,9,https://github.com/pytorch/pytorch/issues/87481, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,This is true for as_strided as well btw:  CC(as_strided_ doc is missing),I checked `as_strided` but it seems to be None,"Ho nice, I guess it was updated recently!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""ci sev problem"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Some operations do not keep `channels_last` memory format which yields accuracy drop," ðŸ› Describe the bug I tried to switch to `torch.channels_last` in order to reduce the training time but I noticed that the new training shows quite a performance drop. While looking deeper into the cause, I noticed that there are some modules which do not keep the `torch.channels_last` memory format:  which yields  As `nn.Linear` does not keep the channels_last memory layout on the GPU, this then causes a problem, as the following modules are now in `torch.channels_last` but the input is not anymore:  which is a problem for the `Conv2d`:  So I think the `nn.Conv2d` silently assumes that the input has the correct memory layout which is not the case due to a previous module that brought it back to be contiguous.  Versions PyTorch version: 1.12.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.3 (default, Jul  2 2020, 16:21:59)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.4.0107genericx86_64withglibc2.10 Is CUDA av",2022-10-21T13:09:03Z,triaged module: memory format,open,0,1,https://github.com/pytorch/pytorch/issues/87451,"I had another look and noticed that the error that I can see seems to be due to cuDNN using TF32 memory format. By adding  the difference for the outputs of `Conv2d` w/ and w/o `torch.channels_last` vanishes. Probably, cuDNN is using a different algorithm for the convolution if the module is put into `channels_last` mode and can make use TF32 such that we loose precision (but the computation itself is faster). So in summary, there seems to be another problem with my training, which I need to find  for `Conv2d` everything seems fine if the input is not in `torch.channels_last`. The only thing to look into would be the operations that do not keep `channels_last` (`ConvTranspose2d` on `cpu` and `Linear` on `cpu` and `cuda`)."
yi,consider numel args when identifying aligned args,Fixes ISSUE_NUMBER https://github.com/pytorch/torchdynamo/issues/1527  ,2022-10-20T19:46:19Z,Merged ciflow/trunk module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/87394,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: stumpOS  (18a7cad753b0d69e5d9b564f382505aeac270d8d, 01ef16373d4e0cd3599e1e02a299c09aa42a3751, 0914bbab7d214fc962a2c0946871e8a4865a638a, ffcdf3779de10fdc1aa5a6204da645d64dab8102)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[functorch][docs] Downgrade the warning about forward-mode AD coveragâ€¦,"â€¦e ( CC([functorch][docs] Downgrade the warning about forwardmode AD coverage)) Previously we claimed that ""forwardmode AD coverage is not that good"". We've since improved it so I clarified the statement in our docs and downgraded the warning to a note. Test Plan:  view docs Pull Request resolved: https://github.com/pytorch/pytorch/pull/87383 Approved by: https://github.com/samdow Fixes ISSUE_NUMBER",2022-10-20T18:55:26Z,ciflow/trunk,closed,0,0,https://github.com/pytorch/pytorch/issues/87386
rag,[functorch][docs] Downgrade the warning about forward-mode AD coverage,"Stack from ghstack:  CC([functorch][docs] Downgrade the warning about forwardmode AD coverage) Previously we claimed that ""forwardmode AD coverage is not that good"". We've since improved it so I clarified the statement in our docs and downgraded the warning to a note. Test Plan:  view docs",2022-10-20T18:06:24Z,Merged ciflow/trunk,closed,0,3,https://github.com/pytorch/pytorch/issues/87383," merge f ""docsonly change; docs builds have passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Enable `src_mask` in fast path of `TransformerEncoderLayer `," Issues Fixes  CC(Transformer and CPU path with `src_mask` raises error with torch 1.12)issuecomment1179435674  Description Passing a 2D attention mask `src_mask` into the fast path of `TransformerEncoderLayer` in CPU was causing an error and so was disabled in https://github.com/pytorch/pytorch/pull/81277. This PR unrolls this fix, enabling `src_mask` on the fast path:  Either attention mask `src_mask` of shape `(L, L)` or padding mask `src_key_padding_mask` of shape `(B, L)` are now allowed on the CPU fast path. If softmax is applied along the last dimension (as in multihead attention), these masks are processed without expanding them to 4D. Instead, when iterating through the input, `Softmax.cpp::host_softmax` converts the index to match the mask dimensions, depending on the type.  If softmax is applied along the dimension other than the last, `Softmax.cpp::masked_softmax_cpu` expands masks to 4D, converting them to `mask_type=2`. Theoretically one could also add special optimized cases for `dim=0, 1, 2` and process them without mask expansion, but I don't know how often is that used  Tests:  `test_transformerencoderlayer_fast_path` is extended t",2022-10-20T17:39:24Z,Merged ciflow/trunk release notes: nn,closed,0,19,https://github.com/pytorch/pytorch/issues/87377,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: sgrigory / name: Grigory Sizov  (f897a73e3db09ed47586e32fb12ab20bd68bf0ff, a4e9b1e1a69dce87c829dc43aafdd0a3ab80df8f, 7b3e282d2e3e2304727ffab5676f3d22240b39fe)",,/easycla, rebase," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge," Merge failed **Reason**: Approval needed from one of the following (Rule 'superuser'): EscapeZero, weiwangmeta, suphoff, ymao1993, xcheng16, ... Details for Dev Infra team Raised by workflow job "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `enablesrcmaskbettertransformer` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout enablesrcmaskbettertransformer && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,[Bug]: OPTForCausalLM failing with TORCHDYNAMO_DYNAMIC_SHAPES=1: UNPACK_SEQUENCE AssertionError: assert len(seq.items) == inst.argval," ðŸ› Describe the bug     Error logs https://gist.github.com/db994c33e9020d15e4c2ae8dbcf91d2a  Did Dynamo succeed?  [ ] Does dynamo.optimize(""eager"") succeed?  Did AOT succeed?  [ ] Did dynamo.optimize(""aot_eager"") succeed?  Did Inductor succeed?  [ ] Does dynamo.optimize(""inductor"") succeed?  Minified repro  minifier did not work (minifier ran, but resulting program did not actually fail)",2022-10-20T15:58:53Z,triaged bug oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93561,Close as issue seems fixed. Used the same repro command  Log 
transformer,"[Bug]: speech_transformer failing with TORCHDYNAMO_DYNAMIC_SHAPES=1: RuntimeError: expand(CUDABoolType{[10, 1, 204, 320]}, size=[-1, 204, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"," ðŸ› Describe the bug  Cannot minify as the exception occurs outside of dynamo compiled code  Error logs https://gist.github.com/110f0d3706cee14947dd8c4eed9a4c72  Did Dynamo succeed?  [ ] Does dynamo.optimize(""eager"") succeed?  Did AOT succeed?  [ ] Did dynamo.optimize(""aot_eager"") succeed?  Did Inductor succeed?  [ ] Does dynamo.optimize(""inductor"") succeed?  Minified repro  minifier doesn't work on this example ",2022-10-20T15:45:55Z,triaged bug oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/93560, ,Close as issue seems fixed. Used the same repro command Download data:  Run the command  Log: 
transformer,TorchScript bug: torch.nn.transformer gives inconsistent results after conversion to TorchScript," ðŸ› Describe the bug torch.nn.transformer gives inconsistent results after conversion to TorchScript   Versions PyTorch version: 1.12.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.6 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.102) CMake version: Could not collect Libc version: N/A Python version: 3.9.12 (main, Apr  5 2022, 01:52:34)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS12.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.12.1 [pip3] torchliter==0.3.3 [pip3] torchtext==0.13.1 [pip3] torchvision==0.13.1 [conda] numpy                     1.23.3                   pypi_0    pypi [conda] torch                     1.12.1                   pypi_0    pypi [conda] torchliter                0.3.3                    pypi_0    pypi [conda] torchtext                 0.13.",2022-10-20T15:35:20Z,oncall: jit,closed,1,2,https://github.com/pytorch/pytorch/issues/87368,Same issue on Linux using GPU PyTorch version: 1.12.1 Cuda 11.3 Python 3.9.13,"There are dropout layers in transformer modules. If set module and scripted module to eval, then the outputs are consistent."
agent,Use after free in TensorPipeAgent," ðŸ› Describe the bug CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) introduced a useafterfree bug:  In this loop it deletes the iterator it is currently using, in other words the `erase` call invalidates the current iterator which is then wrongly used to get the next iterator, i.e. there is a (hidden) call to `++it` on the invalidated iterator. This leads to crashes e.g. in TensorPipeRpcTest.test_without_world_size_existing_rank_can_communicate_with_new_rank_cuda The correct code would look similar to the below code for devices:   Versions Started at 1.12.0, still there in 1.13 and master ",2022-10-20T12:38:11Z,high priority triage review oncall: distributed,closed,0,3,https://github.com/pytorch/pytorch/issues/87359,Huang could you please follow up on this,"Hey , thanks for catching this. This definitely looks like an issue. Luckily, this code path is only executed for a prototype feature (when `world_size` is not specified in `init_rpc`) so the stable RPC features / framework do not encounter this bug. I will send out a fix for this shortly.",BTW: I suggest to use a forloop for this to properly scope the `iter` variable. In the patch I'm using I had problems due to `iter` being used for the deviceloop already and polutes the scope. I.e. `for(auto iter = devices_.begin(); iter != devices_.end(); /*empty*/) {`
transformer,[torch.float16] Tensor product @ produces different output when batch size is changed," ðŸ› Describe the bug **Issue:** With torch.float16 precision, tensor product @ from several released pytorch versions produce different output results when batch size is changed. (nn.Linear is also affected) This bug may be serious in networks with multiple stacked linear layers like Transformers, since errors could be accumlated through layers. It seems like this bug existed from version 1.10 to 1.14, and as far as I know many new models from various work were trained on these releases.  **How to reproduce?**  **Expected behaviour:** Zeros should be printed. **Result:**      ",2022-10-20T09:51:21Z,module: numerical-stability,closed,0,5,https://github.com/pytorch/pytorch/issues/87355,"This is expected, https://pytorch.org/docs/stable/notes/numerical_accuracy.html, cublas uses different kernels for different size inputs under the hood, pytorch calls directly into cublas kernels for these operations. ","> This is expected, https://pytorch.org/docs/stable/notes/numerical_accuracy.html, cublas uses different kernels for different size inputs under the hood, pytorch calls directly into cublas kernels for these operations. Hi, thanks for your reply.  Even though it's expected, inconsistancy across releases is still problematic, I guess I have to do more investigation about this.",Inconsistency across releases is most likely due to different cuda versions. ,"> Inconsistency across releases is most likely due to different cuda versions. Yes, you're probably correct. Seems like cuda 11.3 has some changes on matrix production ops. But I am still wondering why nn.Linear has different behaviour with ops ""@""? And more, torch.mm, torch.bmm, torch ""@"" are not the same. Guess they decide to use cuda kernel depends on context in different ways as well.  Anyway, thanks for your notice and your reply saved my day.","`nn.Linear` calls `addmm` that fuses addition operation to matmul, that results in a different and more accurate result than doing addition separately. With `@` you are doing addition separately. `mm` calls into cublasGemm, `bmm` calls into `cublasBatchedGemm` (even if batch size is 1), so whatever cublas decides to do in these cases. "
transformer,dynamo/aot fails when run with autograd.detect_anomaly context manager,"I was trying to debug https://github.com/pytorch/torchdynamo/issues/1712 and more specifically why it was doing ""double"" backward() call. We have a context manager called autograd.detect_anomaly which allows you to see which part of ""forward"" made the ""backward"" error out. Enabling that context manager errors out.  ",2022-10-19T06:20:27Z,triaged oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/93547,it only fails with `nan` checking on. `torch.autograd.detect_anomaly(check_nan=False)` passes,It seems torch.autograd.detect_anomaly compose fairly with torch.compile but could be wrong since there's no anomaly in the body.  Repro internally D51598964. Need to download the dataset and rename the data.  Will close it for now but feel free to reopen it if it's still an issue.
transformer,Quantization Aware Training not supported for nn.Embedding layers.," ðŸ› Describe the bug It seems like Quantization Aware Training with Torch>=10.2 does not support nn.Embedding layers. The following example code:  Runs into the following error:  Issues  CC(Is it planning to support nn.Embeddings quantization?)) and [ CC([quant] Add support for Embedding/EmbeddingBag quantization via dynamic quant APIs)]( CC([quant] Add support for Embedding/EmbeddingBag quantization via dynamic quant APIs)issue998785028) offer solutions for static and dynamic quantization but not for quantization aware training. Is there any solution to this? If not, fixing this issue would be interesting for applications in transformers among others.  Thanks for the help!  Versions PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 8.3.1 20190311 (Red Hat 8.3.13) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.17 Python version: 3.9.12 (main, Sep 14 2022, 07:55:59)  [GCC 4.8.5 20150623 (Red Hat 4.8.544)] (64bit runtime) Python platform: Linux5.4.861.el7.x86_64x86_64withglibc2.17 Is CUDA availa",2022-10-18T08:58:45Z,oncall: quantization,closed,0,4,https://github.com/pytorch/pytorch/issues/87187,"to unblock, can you try setting the correct qconfig to embeddings manually?","Hi , thanks for getting back to me. I guess this could be done manually but it seems very heavy.  All the examples I find prepare QAT using:  If I understand what you're suggesting, I would need to pass a qconfig_dict dictionary that maps modules to their respective qconfig. But `torch.quantization.prepare_qat` sets this qconfig_dict to None without the option to pass one:  Shouldn't this be fixed so that Embedding layers can be quantized in default configurations like it was the case on previous versions to torch?","Hi Git ,  Your example above uses the Eager mode quantization workflow.  In this workflow, the user is responsible for manually specifying how to quantize everything, so the user is expected to specify how they would like to quantize each layer.  The default config to quantize embeddings is different compared to the default config to quantize conv/linear, which is why the default config is not working for you.  You can use something like this to easily do this:  We also have a graph based workflow, FX graph mode quantization.  In that workflow, we have default per layer quantization configurations.  Currently they don't include embeddings  that is a bug we will fix, thanks for the report.",looks like this one is fixed
yi,fsdp lazy_init typo,"Minor typo, changed with > without",2022-10-18T08:12:21Z,open source Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/87184,The committers listed above are authorized under a signed CLA.:white_check_mark: login: chiaolun / name: Chiao  (bfec297c23bca497115f1861b4079dabc71ebc49), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `patch1` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout patch1 && git pull rebase`)", Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,DDPOptimizer+inductor OOMs with hf_GPT2_large and timm_vision_transformer_large,Repro: `gpui oversubscribe python run_benchmark.py distributed ngpus 1 model torchbenchmark.models.hf_GPT2_large.Model trainer torchbenchmark.util.distributed.core_model.trainer.Trainer job_dir /fsx/users/dberard/scratchlocal/benchfast/benchmark/logs cluster local torchdynamo inductor optimize_dynamo_ddp` Error:  ,2022-10-18T03:26:24Z,triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/93543,  is this still valid?,Can we have an updated cmd for the test?,Closing it as it seems not interesting any more but feel free to reopen.
transformer,Add tensor.to()/tensor._to_copy() support to nested_tensor," ðŸš€ The feature, motivation and pitch This is quite a common operation so could be used for many things, but a major thing this enables is a module using nested_tensor as a parameter. Many PyTorch frameworks, here huggingface transformers' `Trainer` class will attempt to load a model on the CPU before copying it over to a CUDA device (or this is what it looks like at least). Say we have a module somewhere in the module tree which has  Then when we attempt to train, the framework will try to copy the tensor, which will fail    Alternatives ModuleList or arrange for model to never be copied  Additional context _No response_ ",2022-10-16T07:24:28Z,triaged module: nestedtensor,closed,0,3,https://github.com/pytorch/pytorch/issues/87034,"Thanks for opening up this issue, we are actually working on implementing this function and should have a PR soon. The author of that PR is . Will post updates on this issue.",The PR is CC(Add support for .to() for NestedTensor backends),Looks like it's merged now so closing. Thanks  !
rag,Saving and loading from physical storage," ðŸš€ The feature, motivation and pitch Physical storage allows us to work with very large tensors. Ideally, saving a tensor from physical storage into a file would not create a copy of the full tensor in memory. I believe that a call to `torch.save` will already do that. However, when loading the tensor, it will be directly loaded on memory, not on physical storage. This is a problem if the tensor is too big. A nice feature would be that a tensor that was saved on physical storage would be loaded on physical storage again, without creating a full copy in memory:    Alternatives TorchSnapshot offers an alternative where we can directly save a tensor from physical to physical storage, then load a tensor in place on another one in any memory location without creating a full copy in memory (hence solving this problem). I don't think PyTorch intends to do inplace tensor loading but still, I think that keeping the memory location (as we keep the device) would be a nicetohave feature.  Additional context We have implemented in TorchRL a buffer API where big tensors that would not fit in RAM are stored on disk (say > 1Tb). Through the use of memmap arrays, we",2022-10-16T05:38:58Z,module: memory usage module: serialization triaged,open,0,2,https://github.com/pytorch/pytorch/issues/87033,"Supporting outparameter tensorloading would also be a nice thing! also some relevant discussion in  CC(large model, low memory: need `torch.load` that loads one submodule at a time) and  CC(`torch.load(..., weights_only=True)` currently raises a Deprecation warning + [proposal] `weights_only=True` should become default for safe legacy-loading pickles) /  CC([feature request] LazyTensor that provides/loads/computes its contents only upon request to be returned from torch.load) (maybe does hdf5 already support slice loading/saving directly to the disk file via mmap?)  ",and  CC(RFC: multi-part `torch.load`/`torch.save` to support huge models and/or low CPU memory)
chat,AttributeError: Can't get attribute 'ChatDataset' on <module '__mp_main__' from 'c:\\Users\\hp\\Desktop\\Pytorch\\train.py'> Traceback (most recent call last):," ðŸ› Describe the bug import json from nltk_utlis import tokenize, stem, bag_of_words import numpy as np import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader from model import NeuralNet if __name__ == '__main__':     with open('intents.json', 'r') as f:         intents = json.load(f)     all_words = []     tags =[]     xy = []     for intent in intents['intents']:         tag = intent['tag']         tags.append(tag)         for pattern in intent['patterns']:             w = tokenize(pattern)             all_words.extend(w)             xy.append((w, tag))     ignore_words = ['?', '!', '.', ',']     all_words = [stem(w) for w in all_words if w not in ignore_words]     all_words = sorted(set(all_words))     tags = sorted(set(tags))     x_train = []     y_train = []     for (pattern_sentence, tag) in xy:         bag = bag_of_words(pattern_sentence, all_words)         x_train.append(bag)         label = tags.index(tag)         y_train.append(label)      x_train = np.array(x_train)     y_train = np.array(y_train)     class ChatDataset(Dataset):         def __init__(self):             self.n_samples = len(x_train)             s",2022-10-15T16:30:49Z,module: dataloader triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/87016,"Please move you class definition out of 'if __name__ == ""__main__"".","> Please move you class definition out of 'if **name** == ""**main**"". Even after removing if **name** == ""**main**"". it is receiving the same error",I mean  Please take a reference from https://pytorch.org/docs/stable/data.htmlplatformspecificbehaviors,> I mean >  >  >  > Please take a reference from https://pytorch.org/docs/stable/data.htmlplatformspecificbehaviors Thank you that thing worked but now a new error has arisen. **ValueError: optimizer got an empty parameter list** can you please help me out with this,"> Thank you that thing worked but now a new error has arisen. >  > **ValueError: optimizer got an empty parameter list** >  > can you please help me out with this Based on this Error, I would guess it comes from your model `NeuralNet` where `parameter` are not properly attached. Please check https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.htmlpytorchcustomnnmodules If the Error still persists, please share a minimum reproducible script to us. I am closing this Issue since it's not datarelated."
rag,Fixed partitioner issue with getitem and made metadata a storage more consistent,  CC(Fixed partitioner issue with getitem and made metadata a storage more consistent),2022-10-15T02:49:10Z,Merged ciflow/trunk release notes: fx,closed,0,3,https://github.com/pytorch/pytorch/issues/87012, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[MPS] Copy from CPU always add storageOffset,Because why wouldn't it? Fixes  CC([MPS] tensors loaded via torch.load() -- when transferred to GPU -- do not honour indices/views),2022-10-14T04:29:30Z,Merged ciflow/trunk release notes: mps ciflow/mps,closed,0,3,https://github.com/pytorch/pytorch/issues/86958," merge f ""MPS tests passing"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[DONOTLAND] Skip py_imp if meta function is already registered to C++ dispatcher #86825,  CC([DONOTLAND] Skip py_imp if meta function is already registered to C++ dispatcher 86825),2022-10-13T21:33:41Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/86942
rag,Fix incorrect tensor storage check ,"Fix incorrect tensor storage check  This change contains an incorrect check for storage: https://github.com/pytorch/pytorch/pull/86557 **self.storage is not None** should have been: **not torch._C._has_storage(self)** These fixes were run through the DirectML test suite, and confirm the check is now working correctly.",2022-10-12T22:47:37Z,triaged open source Merged ciflow/trunk,closed,0,7,https://github.com/pytorch/pytorch/issues/86845, If you get a moment to review. This is a fix to the previous commit around private use. , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / linuxbioniccuda11.7py3.10gcc7 / test (nogpu_NO_AVX2, 1, 1, linux.2xlarge) Details for Dev Infra team Raised by workflow job "," merge f ""flaky cuda build failure: Stream content length mismatch. Received 619773476 of 626228992 bytes."""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,DISABLED test_self_remote_rref_as_rpc_arg_sparse (__main__.TensorPipeTensorPipeAgentRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_self_remote_rref_as_rpc_arg_sparse` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-12T21:42:47Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/86840,This occurs before CC(Revert distributed test parallelization) was landed. Closing this issue to see if CC(Revert distributed test parallelization) fixed it. 
yi,"[DONOTLAND, Experiment] Skip py_imp if meta function is already registered to C++ dispatcher","  CC([DONOTLAND, Experiment] Skip py_imp if meta function is already registered to C++ dispatcher)",2022-10-12T20:44:40Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/86825
rag,Allow PrivateUse1 backends to not have Storage (#86557),"This is cherrypick of https://github.com/pytorch/pytorch/pull/86557 to enable the DirectML outoftree plugin in 1.13. Link to landed master PR (if applicable): https://github.com/pytorch/pytorch/pull/86557, https://github.com/pytorch/pytorch/pull/86845 Criteria Category: Critical fixes for enabling the DirectML outoftree backend plugin in 1.13.",2022-10-12T17:08:41Z,open source ciflow/trunk,closed,0,1,https://github.com/pytorch/pytorch/issues/86803,Will reopen with this change also CPed: https://github.com/pytorch/pytorch/pull/86845
yi,Make stateless.functional_call support weight tying," ðŸš€ The feature, motivation and pitch A tied weight is a model that looks like  Currently functorch's `make_functional` supports weight tying by only returning one copy of it to the user from. In other words, if `model` is an instance of `Foo` from above and a user calls `model_fn, params = make_functional(model)`, then `params` will only have one element and any updates done to that tensor will reflect in both usages of the parameter This is not true in `stateless.functional_call`. If you pass a value for `weight`, `tied_weight` will not use that value. Similarly, you could pass a separate value for `weight` and `tied_weights`. This has caused issues for AOTAutograd) and will cause issues for functorch as we deprecate `make_functional` for rationalization Pitch is that `stateless.functional_call` should support weight tying (i.e. passing a value for `weight` and having it be used for `tied_weight`).  Alternatives We would likely need to error if this supports weight tying and a user passed a value for both `weight` and `tied_weight` (since it's ambiguous which to use). That's BCbreaking on a now public API so if we don't want to do this some alterna",2022-10-11T18:34:08Z,module: nn triaged module: functorch,closed,0,0,https://github.com/pytorch/pytorch/issues/86708
transformer,DISABLED test_transformer_offload_true_no_shard_norm_type_None (__main__.TestParityWithDDP),"Platforms: rocm This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_transformer_offload_true_no_shard_norm_type_None` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-11T06:53:45Z,oncall: distributed module: flaky-tests skipped module: fsdp,closed,0,3,https://github.com/pytorch/pytorch/issues/86675,"varma  I saw unit test flakiness for `NO_SHARD` and CPU offloading when using subtesting when writing unit tests for `use_orig_params=True`. As a result, I separated CPU offloading tests into their own test. Either `NO_SHARD` has an actual stream synchronization issue, or there is something else at play. Unfortunately, I was not able to see any stream synchronization issue. Also, we cannot directly say that only the `NO_SHARD` code path has an issue; it is possible that calling allgather in the preforward _masks_ the underlying issue, which may be common to sharded and nonsharded cases.","https://github.com/pytorch/pytorch/pull/87930 The PR sets `non_blocking=False` for the gradient offload in the postbackward for `NO_SHARD` and `CPUOffload(True)`. In local testing, this seems to have eliminated the test flakiness. I will close this for now to reenable the unit test.",> Another case of trunk flakiness has been found here. Reopening the issue to disable. Please verify that the platforms list includes all of [linux]. Closing this again because the trunk flakiness mentioned here was from a commit that landed before the commit to fix (https://github.com/pytorch/pytorch/pull/87930).
rag,out of bounds for storage of size," ðŸ› Describe the bug Hello, I recently used MQbench for multiGPU QAT quantization training and reported a bug that was out of storage. when I changed the code to single GPU training it worked. The detailed error message is as follows.  mutiGPU training main function code is as follows.   Versions PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.18.4 Libc version: glibc2.28 Python version: 3.7.3 (default, Jan 22 2021, 20:04:44)  [GCC 8.3.0] (64bit runtime) Python platform: Linux5.4.56.bsk.10amd64x86_64withdebian10.12 Is CUDA available: True CUDA runtime version: 11.3.109 GPU models and configuration:  GPU 0: A100SXM80GB GPU 1: A100SXM80GB GPU 2: A100SXM80GB GPU 3: A100SXM80GB GPU 4: A100SXM80GB GPU 5: A100SXM80GB GPU 6: A100SXM80GB GPU 7: A100SXM80GB Nvidia driver version: 450.142.00 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_",2022-10-11T06:41:48Z,oncall: distributed oncall: quantization triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/86674," Can you confirm this script works with a singleGPU and also works on multiGPU without QAT?  I think we need some tutorials on DDP and FSDPbased QAT, wdyt?",">  Can you confirm this script works with a singleGPU and also works on multiGPU without QAT? >  >  I think we need some tutorials on DDP and FSDPbased QAT, wdyt? Yes, I confirm that this code works on singleGPU with QAT or on multiGPU without QAT.",">  I think we need some tutorials on DDP and FSDPbased QAT, wdyt? I don't see how DDP/FSDP will fail because of quantization specific reasons. in this case can we tune the parameters for multiGPU training to see whether it is actually caused by OOM? does multGPU training require more memories than singleGPU training?"," Hi, have you solved this problem? I also met this problem when QAT with multi GPUs using DDP, but it works when training with only one GPU","could you open the issue in MQBench? https://github.com/ModelTC/MQBench, closing for now"
rag,DISABLED test_periodic_model_averager_param_group (__main__.TestDistBackendWithSpawn),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_periodic_model_averager_param_group` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-11T04:20:09Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/86669,Fixed by https://github.com/pytorch/pytorch/pull/86756
yi,[ONNX] Avoid copying constant in scalar_type_analysis pass,"  CC([ONNX] Avoid copying constant in scalar_type_analysis pass)  CC([ONNX] Fix scalar_type_analysis metadata for copied constant) To prevent unnecessarily increasing model size due to duplicating constants in graph. Follow up work is needed to apply CSE on graph, in case the cast nodes are constant folded, and reintroducing duplicated constants. This issue was discovered in CC([ONNX] CSE pass in export pollutes Scope information). ~~This PR also adds unit test coverage for scope information of nodes when they are altered by CSE and related passes.~~ Edit: Convert to draft now that it appears some legacy caffe2 tests are breaking, which might depend on the behavior of copying constants.",2022-10-10T23:39:02Z,module: onnx open source Stale release notes: onnx topic: bug fixes,closed,0,2,https://github.com/pytorch/pytorch/issues/86648,The committers listed above are authorized under a signed CLA.:white_check_mark: login: BowenBao / name: Bowen Bao  (c5b5cf5058cc9f7cebb9d2cf55557b7772e6137e),"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Allow PrivateUse1 backends to not have Storage,"Allow PrivateUse1 backends to not have Storage To unblock the DirectML backend, this change would be needed for 1.13 as well. The DirectML backend creates tensors using the open registration pattern documented here: https://pytorch.org/tutorials/advanced/extend_dispatcher.html registration example However, DirectML tensors are opaque, and do not have Storage. The DirectML Tensor Impl derives from OpaqueTensorImpl, which does not have a storage. Because of this various places in the code fail that expect storage to be present. We had made various changes intree to accommodate this: a.	def __deepcopy__(self, memo): https://github.com/pytorch/pytorch/blob/b5acba88959698d35cb548c78dd3fb151f85f28b/torch/_tensor.pyL119 or self.device.type in [""lazy"", ""xla"", ""mps"", ""ort"", ""meta"", ""hpu"", 'dml'] b.	def _reduce_ex_internal(self, proto): https://github.com/pytorch/pytorch/blob/b5acba88959698d35cb548c78dd3fb151f85f28b/torch/_tensor.pyL275 if self.device.type in [""xla"", ""ort"", ""hpu"", ""dml""]: c.	TensorIteratorBase::build has an unsupported list for tensors without storage. https://github.com/pytorch/pytorch/blob/b5acba88959698d35cb548c78dd3fb151f85f28b/aten/src/A",2022-10-09T16:34:26Z,triaged open source Merged ciflow/trunk,closed,0,19,https://github.com/pytorch/pytorch/issues/86557,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: smk2007 / name: Sheil Kumar  (4411a1a72f56232e1ca28ae7bfd78cbcb8b84003, a89d4bb4af9c122a98a57cd7f4bf1042ba529ada, d1bf7d864d0a591d43f53b903dcc6180c495e72e)","Can you sign the CLA? That's needed to land the PR. > To unblock the DirectML backend, this change would be needed for 1.13 as well. The branch cut for the 1.13 release was a few days ago. There's an option for cherrypicking commits into the branch, but there are some criteria for what's allowed to be cherrypicked. You can see the details for the process here:  CC([v.1.13.0] Release Tracker)","> Can you sign the CLA? That's needed to land the PR. >  > > To unblock the DirectML backend, this change would be needed for 1.13 as well. >  > The branch cut for the 1.13 release was a few days ago. There's an option for cherrypicking commits into the branch, but there are some criteria for what's allowed to be cherrypicked. You can see the details for the process here: CC([v.1.13.0] Release Tracker) Thanks bdhirsh!  I see some of the CI checks failed. They dont seem related to the changes. Is there a way to rerun these failing CI tests?",> Thanks for identifying issues with using the `PrivateUse1` key! These changes look reasonable to me. I believe i need a maintainer to approve. Is there someone I need to ping to make this happen? Thanks!, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge l, Merge started **The `l` land checks flag is deprecated and no longer needed.** Instead we now automatically add the `ciflow\trunk` label to your PR once it's approved Your change will be merged once all checks on your PR pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `user/sheilk/privateuse1` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout user/sheilk/privateuse1 && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","> Can you sign the CLA? That's needed to land the PR. >  > > To unblock the DirectML backend, this change would be needed for 1.13 as well. >  > The branch cut for the 1.13 release was a few days ago. There's an option for cherrypicking commits into the branch, but there are some criteria for what's allowed to be cherrypicked. You can see the details for the process here: CC([v.1.13.0] Release Tracker)  Created this PR for CPing into release/1,13: https://github.com/pytorch/pytorch/pull/86803", sounds good!  you'll also have to put in a request to the infra team on the release tracker issue:  CC([v.1.13.0] Release Tracker),">  sounds good!  you'll also have to put in a request to the infra team on the release tracker issue: CC([v.1.13.0] Release Tracker) Thanks, I also opened this PR into master, which contains a fix to this same change. The storage check was done incorrectly. This should be done correctly now! https://github.com/pytorch/pytorch/pull/86845"
rag,DISABLED test_periodic_model_averager (__main__.TestDistBackendWithSpawn),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_periodic_model_averager` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-07T18:48:56Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/86477,Fixed by https://github.com/pytorch/pytorch/pull/86756
transformer,Allow casting (instead of converting) tensors to/from channels_last to enable mixing Conv2d & Attention," ðŸš€ The feature, motivation and pitch Transformers are all the rage, and those networks tend to use `BTC` tensor format: batch size, tokens, channels. Regular CNNs however use `NCHW`, and this leads to code mixing the formats looking like this (from the MixTransformer backbone of SegFormer):   i.e. lots of permute memory churn.  Pytorch has experimental support for `channels_last` for NCHW ops: https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html That beta offers an API whereby you can convert a channels_first NCHW tensor to channels_last format. That's great.  However, when mixing BTC modules with NCHW modules, it would be really useful to ""castwithoutconverting"" tensors from one format to the other.  Then the attention modules could be essentially operating on `BHWC`reshapedas`BTC` tensors, while the CNN modules would operate on channels_last`NCHW` tensors (= same effective memory layout, just slightly different tensor meta info).  You'd still have to deal with the `reshape` and castto`channels_last` line noise, but at least you're not shuffling memory around.   Alternatives Create a supertensor that appears like it's `BTC` in Tra",2022-10-07T04:54:58Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/86447,Reading  CC(torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application)issuecomment1176482782 it sounds like this might kindofsortof already be possible?  I guess using as_strided would enable the cast I'm looking for. 
transformer,[PT1.13 cherry pick]Fix Transformer's issue when padding is long type & unit test,Summary: Fix the issue described in  CC(TransformerEncoder src_key_padding_mask does not work in eval()) Test Plan: buck test mode/opt caffe2/test:test_transformers  test_train_with_long_type_pad Differential Revision: D40129968,2022-10-06T05:53:35Z,fb-exported cla signed topic: bc breaking,closed,0,8,https://github.com/pytorch/pytorch/issues/86353,The committers listed above are authorized under a signed CLA.:white_check_mark: login: zrphercule / name: Rui Zhu  (17adaf8962f21a2e62e945f565eae12487ea6bc5),This pull request was **exported** from Phabricator. Differential Revision: D40129968,This pull request was **exported** from Phabricator. Differential Revision: D40129968,This pull request was **exported** from Phabricator. Differential Revision: D40129968,"Removing milestone for now, as PR is not yet passing the CI.  any plans to fix the signal and land it?","Seems phabricator's linking has some issue, will export a new one","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," if this is BCbreaking and was cherry picked into 1.13 release could you please write a description for it, see  here for instructions. The release is soon so it would be great if you could do this ASAP. Thank you!"
yi,Error when applying CUDA Graph to a Recommendation System called DLRM," ðŸ› Describe the bug When I apply CUDA Graph to DLRM, a TypeError occured: Module.apply() takes 2 positional arguments but 16 were given  Versions CUDA 10.4 pytorch ",2022-10-05T16:35:26Z,triaged module: cuda graphs,closed,0,7,https://github.com/pytorch/pytorch/issues/86281,"Hi , Could you please share a little more details about your use case? A minimal code snippet would be the most helpful in understanding the issue and debugging it.","> Hi , >  > Could you please share a little more details about your use case? A minimal code snippet would be the most helpful in understanding the issue and debugging it. A  Hi, Aidyn I want to apply cuda graph to dlrm. There exist an function to apply cuda graph after capture graph in your code:         def functionalized(*user_args):              Runs the autograd function with inputs == all inputs to the graph that might require grad              (explicit user args + module parameters)              Assumes module params didn't change since capture.             out = Graphed.apply(*(user_args + module_params))             return out[0] if output_was_tensor else out However, when I tried to apply cuda graph to dlrm: a TypeError occured: Module.apply() takes 2 positional arguments but 16 were given. user_args and module_params are 2 positional arguments, there are more input when building DLRM for user_args, it is no longer 1 positional arguments and I don't know why DLRM has more positional arguments on module_params than other models. ",", Are you using https://github.com/facebookresearch/dlrm?","A  Yes, that is it!",", I am not sure what is happening in facebookresearch/dlrm. However, this DLRM implementation was designed to work with CUDA Graphs.","A Ok, thanks a lot!","Closing for now, feel free to reopen this issue if you think there's a bug."
transformer,TransformerEncoder/TransformerDecoder has same initial parameters for all layers, ðŸ› Describe the bug Example code (based on documentation):  Then you have:  Etc. This is because:  Where `_get_clones` uses `deepcopy`. The same problem is for `TransformerDecoder`. This is unexpected behavior to me but maybe it is expected and just not well documented? Note that `Transformer` itself does not have this problem because it calls `_reset_parameters` after all the layers are assigned.  Versions (I have an outdated version but I checked the source code of the current version.) ,2022-10-05T15:14:54Z,module: nn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/86274,I think this is expected and not well documented. But it could definitely be changed. Is the desire to make sure the weights are initialized differently between layers? ,"I assume that users usually do not want this. Users of `nn.Transformer` anyway would not have the problem though, and other users might already have overwritten the param init *if they know about this*.","I see. Would you be open to making a PR fixing this? We have a couple of options 1. Add a reset_parameters() function to both TransformerEncoder and TransformerDecoder but do not automatically call it. Let people call it themselves.  2. Do option 1 but also add a call to reset_parameters() in the init function. 3. No code changes and add documentation that all layers will be initialized the same.  I was thinking to just do option 1. As a user, what do you think? ","> Would you be open to making a PR fixing this? Yes, if I find the time. Not sure exactly when. > As a user, what do you think? My main issue here is about unexpected behavior  or rather, it would be nice if a normal use of those modules would yield expected behavior. The problem is introduced due to the use of `deepcopy`. So maybe it makes sense to fix the param init right after the `deepcopy`, i.e. in the `TransformerEncoder` and `TransformerDecoder`, so basically your option 2. Then the `_reset_parameters` in the `Transformer` itself might not be needed anymore? I'm not sure if people might pass `custom_encoder` or `custom_decoder` thought which depend on the `Transformer._reset_parameters` though."
transformer,Autocast with BF16 on CPU slows down model more than 2X," ðŸ› Describe the bug Trying to run inference with autocast and bf16 slows down a Hugging Face BERT model immensely. I would expect inference acceleration. This makes it hard to just include autocast in a script for inference and have it run bfloat16 or fp16 depending on the availability of CUDA, since the model might be slowed on. The benchmark is very simple. Code to replicate:   Output:   System Info:  Torch Version: 1.10.0 Transformers Version: 4.9.1 CPU Info:   Versions Collecting environment information... PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: 11.1 ROCM used to build PyTorch: N/A OS: Amazon Linux 2 (x86_64) GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.115) Clang version: Could not collect CMake version: version 3.22.3 Libc version: glibc2.26 Python version: 3.8.12  (default, Oct 12 2021, 21:59:51)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.10299.473.amzn2.x86_64x86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.1.105 GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 510.47.03 cuDNN version: Probably one of the following: /usr/local/cuda11.0/targets/x86_64linu",2022-10-05T13:01:39Z,module: performance triaged module: bfloat16 module: amp (automated mixed precision),open,0,4,https://github.com/pytorch/pytorch/issues/86270, could this thing be somehow tested automatically? in HF there're a lot of models that could be relatively easy to automate, so do you think that this is a specific model issue and not a hardware issue? ,"i don't know, but if there are some published popular model results with specified hardware, at least you know that on this/that hardware the speedup is indeed there (i think, nvidia gpu's support natively bf16 starting from A100 / Ampere  but the memory savings + more speed from less memory usage should already be there if memoryaccesstimebound)",Agreed! 
yi,PixelShuffle check that output is not null before applying kernel (#85155),* Checks that output tensor is not null before applying kernel in `pixel_shuffle` op * Checks that output tensor is not null before applying kernel in `pixel_unshuffle` op * Add test case testing `pixel_shuffle` with shapes producing empty output * Add test case testing `pixel_unshuffle` with shapes producing empty output Fixes CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions) FYI  ,2022-10-05T06:50:14Z,open source Merged cla signed ciflow/trunk,closed,0,9,https://github.com/pytorch/pytorch/issues/86262,"FWIW the CI error seems unrelated. To avoid these errors in the future, you may want to base your PRs off `viable/strict` rather than `master`. If the error is still there when you want to merge the PR, you'll need to merge via ` merge f ""the CI error is unrelated""` or smth"," merge f ""CI error unrelated to the content of the PR""", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `Core Reviewers`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""CI error unrelated""",">  merge f ""CI error unrelated"" Thanks"," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,SVD with `full_matrices=False` slower than `full_matrices=True` & `full_matrices=False` on GPU 500x slower than CPU," ðŸ› Describe the bug   If you run the above test, you should see that the SVD for CUDA will be extremely slow, especially for `Nx3x2` and `Nx60x3`. Originally I was going to attach this to CC(SVD is slow on GPU vs CPU for skinny matrices), but didn't want to reopen the closed issue and suspect it may be something different since it only happens when `full_matrices=False`. I'm also not sure if setting `full_matrices=False` is expected to be slower than running with `full_matrices=True`, but it should at least be documented, since I'd expect it to have to do less work and intuitively it should be faster.    Environment Collecting environment information... PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.16.3microsoftstandardWSL2x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collec",2022-10-04T21:12:33Z,module: performance triaged module: linear algebra,closed,0,10,https://github.com/pytorch/pytorch/issues/86234,"For SVD on CUDA, you can try a cuda 11.7 nightly build with `driver=` option.   doc: https://pytorch.org/docs/master/generated/torch.linalg.svd.htmltorch.linalg.svd  nightly build: https://pytorch.org/getstarted/locally/ For nonsquared small matrices (m, n <=32), setting `full_matrices=False` may cause a slower driver being selected. For tall and thin matrices (Nx60x3), the `gesvda` driver may give a better performance.","I'll try out the other drivers, but for my use in practice I would want to stick to stable for now, just so other features remain reliable. I imagine the gesvda driver is probably the one that would work best, as per the benchmark from a previous issue. my specific usecase is 3x3, 3x2, or 2x2 so the matrices I'd need to factor are just tiny and not tall. I haven't written an SVD algorithm (or at least can't remember doing so), but is the cost of specialcasing extremely tiny matrices worth doing? I also made this post in specific because it seems that this issue is hit only when `full_matrices=False`, but when `full_matrices=True`, the performance is fine.","Implementing a reliable, stable and fast SVD algorithm is *very* difficult. Even for small matrices (see e.g. here). I agree with  that playing around with the different algorithms that we expose through the `driver` kwarg may be what you want. Now, I modified a bit the test to compare with/without the `full_matrices` flag, and I got some very surprising resuts:  other than that, I think that the CUDA results are always comparable with those from CPU even in these notsocommon cases. Note how in the only case when CUDA was much slower than CPU, that is `shape=[1000, 60, 3]`, `gesvda` is able to run at a comparable speed. At any rate, you may want to consider benchmarking CUDA vs casting to CPU, run the op on CPU and then casting back to CUDA for your given inputs, as you may be able to squeeze some efficiency that way."," That's not a cusolver issue but likely our heuristics here https://github.com/pytorch/pytorch/blob/63d8d4f6ec5c973ad7b8669cd39ee9b550e5f55b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cppL848L853 We may need to revisit our u ,s, vt settings or cusolver calls on `full_matrices=False` here.",That is indeed annoying. We don't have a fast and accurate way to compute the SVD for large batches of rectangular matrices :(,"Actually, what we could do in that case is to call the batched path and then copy the relevant slices of the output to `U` and `V`. I'll put up a PR later this week for this case.","That sounds like a good plan, but note that it's not always faster in this way.",I'll throw in some benchmarks on the house :D, do you mind tagging me in the PR? Not that I have much to comment but am curious,"For reference, after https://github.com/pytorch/pytorch/pull/88502issuecomment1305477400 is merged, the new results of the benchmarks are as follows: "
yi,Stop modifying the global logger on `import functorch`,Stack from ghstack:  CC([easy] Add spaces to vmap over as_strided error message)  CC(Stop modifying the global logger on `import functorch`)  CC(Reintroduce the functorch docs build (85838)) Fixes  CC(functorch import messed up python logging module) `logging.basicConfig` modifies the global logger which affects other programs. importing a package should generally be sideeffect free so this PR gets rid of that call. Test Plan:  tested locally,2022-10-03T19:22:42Z,Merged cla signed ciflow/trunk release notes: fx fx,closed,0,5,https://github.com/pytorch/pytorch/issues/86147,`format_str` is dead too now right,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," merge f ""preexisting failures"""," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,TransformerEncoder src_key_padding_mask does not work in eval()," ðŸ› Describe the bug Described here: https://discuss.pytorch.org/t/modelevalpredictsallthesameornearlysameoutputs/155025/12 Starting with version 1.12, TransformerEncoder uses two different code paths, which perform differently between training and evaluation mode. In training, the `why_not_sparsity_fast_path` will be set, and the sparsity fast path will not be used. During eval(), if using a `src_key_padding_mask`, output will be modified: https://github.com/pytorch/pytorch/blob/b04b2fa9aa52cacbdc9aaaf477d55b0af845ce81/torch/nn/modules/transformer.pyL271  The output is usually some constant value. It seems like their might be some misalignment with the expected `src_key_padding_mask` and the `logical_not()` operation. Perhaps the mask inversion should not be performed.  Versions Python version: 3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 516.94 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XN",2022-10-03T15:46:50Z,high priority module: nn triaged,open,0,9,https://github.com/pytorch/pytorch/issues/86120,Shared internally. Thank you for calling this out! ,"Hi  , thanks for bringing this! For better understanding of this issue, could you please give a simple reproduce of this issue? I tried to repro it but I dont think I reproed this issue (Probably due to I misunderstood it.) Thanks!"," Sorry it took so long, it was a bit difficult to pin down and the encoder shows the issue intermittently. The issue seems to be related to type conversion of the pad mask. If the pad mask is boolean, then there is no issue; however, when inputting the pad mask as long, the result is different between train and eval (probably due to bool conversion at the `logical_not()`). I found you can also see differences in the results in train() as well using different types. However, the differences are not apparent at most steps. Here is the output from a run I did:  Only a handful of steps in the loop will produce a difference. Most of the time, this code will print differences, but a few times I ran it, it did not."," Thank you very much, we found the reason is mainly because the old version of PyTorch does not support Long type mask. Fix and your unit test: https://github.com/pytorch/pytorch/pull/86353/files","Hi   Can you please explain if both mask = None and src_key_padding_mask = None in the encoder part of the transformer, will I experience this issue or not?","Thanks !  no, this issue is only seen when using src_key_padding_mask.","> Hi  Can you please explain if both mask = None and src_key_padding_mask = None in the encoder part of the transformer, will I experience this issue or not? Hi  No, this bug is actually not related to BetterTransformer; instead it is the old version of PyTorch Transformer did not handle nonbool src_key_padding_mask properly. If you runs the model in inference mode (which will lead you to BetterTransformer), or you are not using src_key_padding_mask with a nonbool dtype, you will not encounter this bug.",Hi  and  thank you for your answers!,We should allow only bool dtype of `src_key_padding_mask`
yi,Add PyInterpreterTLS,"  CC(Add PyInterpreterTLS)  CC(Add some documentation to deploy.h) This is set by torchdeploy to indicate the current Interpreter that you are interacting with.  This will allow Python op registrations to contextually select the correct interpreter to attempt to handle an operator. There is some perf concern as we have to set the TLS on every InterpreterSession entry point.  Hopefully you are not going over these in a tight loop. There is some encapsulation concern.  I sprayed this over every method in InterpreterSession, but I am not sure if this gets every possible way to end up ""in"" libtorch in a context where you could trigger an operator that was overridden by a Python interpreter, and it makes sense to ask the Python interpreter to figure it out (clearly, noninterpreter associated operations should never get the Python treatment). Signedoffby: Edward Z. Yang ",2022-10-03T04:41:15Z,cla signed ciflow/trunk,closed,0,10,https://github.com/pytorch/pytorch/issues/86095," and I discussed this and we're going to try an alternate strategy, where libc10 maintains a vector of ALL known PyInterpreters, and we use TLS to speed up finding the 'active' interpreter for a thread (but in the slow path, we will iterate through the list ""looking"" for the one that thinks it owns a thread)"," the strategy we discussed doesn't fully work. Although each libpython instance ""knows"" if it has relevant state on a thread, the TLS used by each libpython instance is disjoint, so if you touch multiple Python interpreters from a single OS thread, they will all report that they have nonempty thread state, and you are in the same situation of not knowing which one to use. This strategy *does* help if a thread is never used by more than one interpreter, which would be the situation with threading module. So I could use this to solve that problem. But I wouldn't be able to get rid of the TLS on torchdeploy API entry points, as that would be needed to disambiguate if multiple interpreters were used from one thread. Maybe we should reconsider putting the TLS setter on InterpreterSession? We could just have the last session locked ""win""; you should still get a reasonable error message if it's inconsistent, and I don't have to spray the function call everywhere. WDYT?","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," and I discussed this some more and we have a new strategy. First, the easy case is when there is some Tensor argument that belongs to a Python interpreter. In that case, we just use that Python interpreter. But what if there is no argument that belongs to a Python interpreter? In this case, it doesn't matter which interpreter we choose to run the operation on: the Python operator registration is supposed to be equivalent to some C++ code, so we simply need to make sure that whatever Python objects we temporarily allocate to actually run the interpreter get disposed of when we come back. There are two cases to consider: 1. Input tensors. It is possible, if an invocation comes entirely from C++, for none of the Tensors to have PyObjects. We must create PyObjects for them, but we must also not permanently associate the PyObject with the Tensor because the caller may pass the tensor on to its own interpreter. To handle this, we simply create a PyObject that points to Tensor, *without* setting the pyobj on Tensor. In essence, this is what we used to do before we implement PyObject preservation. Furthermore, if the tensor is passed back into Python (e.g., via some inner Python to C++ call), we must reallocate a new PyObject for it in the same way (without setting pyobj). So we will set TLS which *disables PyObject preservation* to ensure that we get pristine C++ objects that can be reused by the other interpreter. 2. Output tensors. Given the TLS above, any allocated tensor will not actually have a PyObject set on them, so once again there is nothing to do.", what about the case when you have a tensor that's shared between multiple python interpreters i.e. we created a model one interpreter and then transferred that model to a different interpreter with the same underlying storage. Is the PyObject only set on the at::tensor and not the at::Storage? https://www.internalfb.com/code/fbsource/[e5194e7ea445]/fbcode/multipy/runtime/interpreter/interpreter_impl.h?lines=2027,"According to Zach when I last asked him about this, we are supposed to be detaching all the tensors when you do this cross interpreter transfer, so this never shows up in reality.",And yes we don't do PyObject preservation on storage, new approach sounds reasonable to me There's a third case where there's two input objects on different interpreters but it seems reasonable to me to discard the PyObjects regardless, what's the performance hit do you expect of not caching PyObjects? Presumably there was one otherwise you wouldn't be caching them,"The caching actually is nothing to do with perf, and for making sure attributes stored in `__dict__` get preserved. Which there aren't any in this case."
transformer,[ONNX] Add transformer tests,Scripted `nn.Transformer` used to be exportable but no longer ( CC([ONNX][bug] `nn.Transformer` contains unsupported tensor scalar type)). We need tests for it to prevent regression.,2022-10-02T16:05:59Z,module: onnx triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/86066
transformer,PyTorch don't have an op for aten::constant_pad_nd but it isn't a special case.," ðŸ› Describe the bug When using `torch.onnx.export` for compiling a model from `transformers`, an error indicated the op `aten::constant_pad_nd` isn't supported but it isn't a special case.  Code to reproduce:   Error Message:   Versions  ",2022-10-02T04:15:16Z,module: onnx onnx-needs-info,closed,0,3,https://github.com/pytorch/pytorch/issues/86063,Can you test with the latest pytorch nightly build and report back? Thanks!,"Hi  , thanks for the advice! I upgraded PyTorch to `1.12.1+cu102` and changed `optset_version` to 14 for `torch.onnx.export`, then everything worked fine and smoothly. I reported the updated code and environment here for future reference:  Code   Versions    Validation Code  Onnx Checker Didn't raise errors   Dummy Input ",Problem solved so close the issue.
rag,[MPS] Pin_memory on MPS backend raises exception of setting wrong device storage," ðŸ› Describe the bug    Versions Collecting environment information... PyTorch version: 1.13.0a0+git614d6f1 Is debug build: True CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.6 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.102) CMake version: version 3.23.2 Libc version: N/A Python version: 3.10.6 (main, Aug 30 2022, 04:58:14) [Clang 13.1.6 (clang1316.0.21.2.5)] (64bit runtime) Python platform: macOS12.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.13.0a0+git614d6f1 [conda] Could not collect ",2022-10-02T01:51:20Z,triaged module: mps,closed,0,1,https://github.com/pytorch/pytorch/issues/86060,"Hi . Thank you for reporting the issue. Currently, there's no support for pinned memory on the MPS backend.    "
rag,"A bunch of coverage improvements (re for models in inference snext50, BERT_pytorch, mobilenet_v3_large, pytorch_CycleGAN_and_pix2pix, dcgan, resnet18, mnasnet1_0)","  CC(Symintified factory functions)  CC(A bunch of coverage improvements (re for models in inference snext50, BERT_pytorch, mobilenet_v3_large, pytorch_CycleGAN_and_pix2pix, dcgan, resnet18, mnasnet1_0))  CC(Ported linear to symints)  CC(Ported reshape to symints and added a shim for BC)",2022-10-01T09:36:02Z,Merged cla signed ciflow/trunk release notes: fx fx,closed,0,3,https://github.com/pytorch/pytorch/issues/86050," merge f ""ci failures are irrelevant"""," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"[Distributed] Loading distributed checkpoint with FSDP fails with varying key errors (pos.embedding, shared.weight)"," ðŸ› Describe the bug Using 1.13.0.dev20220928+cu116 and FSDP, save out a distributed checkpoint.  Saving proceeds smoothly, and distributed checkpoint is created. However, in attempting to load the just saved checkpoint, receive varying key errors depending on the model that was saved. DeepVit: ~~~ Traceback (most recent call last): (RANK 3)   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/utils.py"", line 140, in reduce_scatter     local_data = map_fun()   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/state_dict_loader.py"", line 89, in local_step     local_plan = planner.create_local_plan()   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/default_planner.py"", line 85, in create_local_plan     return create_default_local_load_plan(self.state_dict, self.metadata)   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/default_planner.py"", line 131, in create_default_local_load_plan     md = metadata.state_dict_metadata[fqn] KeyError: 'pos_embedding' ~~~ T5: ~~~ Traceback (most ",2022-09-30T03:00:50Z,oncall: distributed,open,0,1,https://github.com/pytorch/pytorch/issues/85949,  Is this a distributed checkpointing issue or an FSDP issue or both?
rag,[ONNX] Upload code coverage to codecov,TODO: Limit the report scope to torch.onnx,2022-09-29T16:32:17Z,triaged open source better-engineering cla signed Stale topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/85900,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Justin Chu . The commit (47581dadc6d6f0794f051ff0709ba2613640503f, 4c2277486c71bae4c5761c7546abefddd870b25f) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `justinchu/codecov` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout justinchu/codecov && git pull rebase`)","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Collect Operator Coverage,Aten decomposes operators at the FX level (torch/_decomp/decompositions.py). To collect the decompose operator coverage both for FX and TorchInductor. ,2022-09-29T05:49:04Z,triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93703,Is this what you're looking for? Also discussed in https://devdiscuss.pytorch.org/t/setofopsforabackendtoregister/1024. Closed for now but feel free to reopen it if anyone feels necessary.
rag,Slightly beefed up dynamic shapes tests for storage_offset,  CC(Slightly beefed up dynamic shapes tests for storage_offset) Signedoffby: Edward Z. Yang ,2022-09-28T14:09:30Z,Merged cla signed topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/85806, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!"
transformer,RuntimeError: CUDA error: device not ready," Issue description **when I use torch.cuda.Event for Record the time that my model used, an error has occurred.**  Code example here is my test code  Traceback (most recent call last):   File ""/home/crj/PycharmProjects/huggincface/test.py"", line 23, in      elapsed_time = start.elapsed_time(end)   File ""/home/crj/anaconda3/envs/wav2vec2/lib/python3.7/sitepackages/torch/cuda/streams.py"", line 204, in elapsed_time     return super(Event, self).elapsed_time(end_event) RuntimeError: CUDA error: device not ready CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1.  System Info Collecting environment information... PyTorch version: 1.12.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.16.3 Libc version: glibc2.10 Python version: 3.7.12  (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.13.041genericx86_64withdebianbullse",2022-09-28T09:43:55Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/85796,"I had the same issue and this code worked for me : import gc gc.collect() torch.cuda.empty_cache() If is still doesn't work for you then please check this link, there are multiple ways people have discussed this issue. It might be helpful : https://stackoverflow.com/questions/54374935/howtofixthisstrangeerrorruntimeerrorcudaerroroutofmemory","> I had the same issue and this code worked for me : import gc gc.collect() torch.cuda.empty_cache() >  > If is still doesn't work for you then please check this link, there are multiple ways people have discussed this issue. It might be helpful : https://stackoverflow.com/questions/54374935/howtofixthisstrangeerrorruntimeerrorcudaerroroutofmemory thanks,when I add torch.cuda.synchronize() after end.record(),It word well! people discussed this issue here too: https://stackoverflow.com/questions/6551121/cudacudaeventelapsedtimereturnsdevicenotreadyerror"
yi,"Revert ""Revert ""Symintifying slice ops (#85196)""""","  CC(Revert ""Revert ""Symintifying slice ops (85196)"""") This reverts commit 3a171dfb0c08956d55f341039cf35e3a18269c34.",2022-09-27T19:15:31Z,Merged cla signed release notes: vulkan,closed,1,4,https://github.com/pytorch/pytorch/issues/85746,"There are some new torchgen changes here, mostly just making it possible to turn off symint codegen for executorch.", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,create swin-block in torchvision," ðŸš€ The feature, motivation and pitch I'm working on swin transformer, and noticed the excellent performance of swin transformer, now there is a swin_transformer in torchvision, but I hope to have a swinblock to help me quickly build a network.  Alternatives _No response_  Additional context _No response_ ",2022-09-27T10:55:16Z,feature triaged module: vision,closed,0,2,https://github.com/pytorch/pytorch/issues/85697,66666 it might also be helpful to post this in the torchvision repo so that they're sure to see it,"66666 closing this since you have opened pytorch/vision CC(``bincount`` feature implementation).  You can ""Transfer issue"" (bottom of the right column on the issue) to `torchvision`. That avoids extra work for the OP as well as automatically removes the issue from this tracker. Plus, it keeps all the comments from the original thread, which admittedly are not existent here."
rag,Update hierarchical_model_averager.py,Fixes ISSUE_NUMBER,2022-09-26T19:13:00Z,oncall: distributed Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/85648," merge f ""string change"""," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey varma. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[Distributed: RPC] Failed to initialize RPC with >18 workers, ðŸ› Describe the bug Failed to initialize RPC with >18 workers. Here is a minimal script:  Run in command line:  It works for me for 18 workers but raises errors when I change the proc number larger than 18 (e.g. 19) Output:  Different machines report different maximum worker numbers on my side.  singlesocket 16C32T CPU: raise error 19+ procs  singlesocket 6C12T CPU: raises error 9+ procs  dualsocket 22C44T CPU (44C88T in total): raise error 9+ procs  Versions  ,2022-09-25T14:39:54Z,oncall: distributed triaged module: rpc,open,0,4,https://github.com/pytorch/pytorch/issues/85607,Huang  Do you mind taking a look?,"Hi , this is likely due to the threadpool getting exhausted. Can you retry your code with this:  I've filed this as an issue before ( CC(Provide error message when thread pool is exhausted in RPC)), but will mark it as highpri since its been hit twice now so we can provide a better error message","Huang Adding `num_worker_threads` reports the same error (connection reset by peer) for 19 workers:  Script    Also, it's not quite stable for 18 workers: ","I also encounter this issues, I could launch 30+ rpc. But it doesn't work after 40 RPCs initializing. Here is error log. [W tensorpipe_agent.cpp:916] RPC agent for 21 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 15 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 31 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 19 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 23 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 22 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 6 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 2 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 12 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 11 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 26 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 30 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 20 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 5 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 1 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 32 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 34 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 27 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 4 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 28 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 37 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 39 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:530] RPC agent for 0 encountered error when accepting incoming pipe: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 37: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 7: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 28: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 4: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 39: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 34: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 27: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 32: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 1: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 5: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 10: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 20: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 30: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 26: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 3: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 15: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 25: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 21: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 16: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 36: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 35: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 31: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 24: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 19: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 22: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 23: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 6: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 8: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 2: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 12: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 14: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 11: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:916] RPC agent for 25 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 35 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 24 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 8 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 14 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 3 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 10 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for 36 encountered error when reading incoming response from 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 7 encountered error when sending outgoing request CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for 16 encountered error when reading incoming response from 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""/data00/home/guyi.1016/workspace/torzilla/tztest/rpc/main.py"", line 52, in      main()   File ""/data00/home/guyi.1016/workspace/torzilla/tztest/rpc/main.py"", line 38, in main     torzilla.lanuch(   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/lanucher.py"", line 12, in lanuch     raise e   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/lanucher.py"", line 10, in lanuch     proc.start()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 19, in start     self._on_start()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 54, in _on_start     self._spawn(num_process, spw_kwargs)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 62, in _spawn     return mp.spawn(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 240, in spawn     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 198, in start_processes     while not context.join():   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 160, in join     raise ProcessRaisedException(msg, error_index, failed_process.pid) torch.multiprocessing.spawn.ProcessRaisedException:  Process 2 terminated with the following error: Traceback (most recent call last):   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 69, in _wrap     fn(i, *args)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 88, in _on_process_entry     raise e   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 86, in _on_process_entry     p.start()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 19, in start     self._on_start()   File ""/data00/home/guyi.1016/workspace/torzilla/tztest/rpc/main.py"", line 24, in _on_start     self._try_init_rpc()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 110, in _try_init_rpc     self._init_rpc(**rpc_kwargs)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 31, in _init_rpc     return rpc.init_rpc(**rpc_args)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/rpc/rpc.py"", line 47, in init_rpc     return _rpc.init_rpc(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)"
transformer,Enabling Transformer fast path for not batch_first,Summary: The fast path for the `forward()` method in `MultiheadAttention` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Test Plan: Added unit test for fast path for both values of `batch_first` producing identical outputs. `buck test mode/devnosan //caffe2/torch/nn/modules/tests:test_activation` passed Reviewed By: mikekgfb Differential Revision: D39669982,2022-09-23T22:27:51Z,fb-exported cla signed Stale ciflow/trunk,closed,0,12,https://github.com/pytorch/pytorch/issues/85576,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",The committers listed above are authorized under a signed CLA.:white_check_mark: login: soumyodipto / name: Soumyodipto Mukherjee  (f49d97d95b232937697ec1a21a17d68e4ea5ed80),This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This is the updated diff with  and 's suggested changes,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Execute smoke test for Better Transformer feature ," ðŸš€ The feature, motivation and pitch Run the smoke test with the RC binary (available on September 31) on A100 machines.  Alternatives _No response_  Additional context _No response_ /pytorchdevinfra",2022-09-22T20:19:44Z,module: ci triaged,open,0,2,https://github.com/pytorch/pytorch/issues/85499,Are we still doing Better Transformer for release/1.13? Removing milestone for now, is this still needed?
transformer,Issue with converting Comet model to ONNX. Split-node error.,"I'm trying to convert the Comet model to ONNX. In particular I'm working with the referenceless version of the metric (""wmt21cometqemqm""). It is based on BERT and contains a regression head. The export to ONNX succeeds. And the resulting model passes the checks. However, performing inference fails with an error  Cannot split using values in 'split' attribute. Below I provide details of the error. I wonder if the ONNX supports such configuration. I also prepared a short script that may be used to reproduce this issue.  The error details   Versions Collecting environment information... PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.13.4 Libc version: glibc2.10 Python version: 3.7.12  (default, Oct 26 2021, 06:08:53)  [GCC 9.4.0] (64bit runtime) Python platform: Linux4.19.021cloudamd64x86_64withdebian10.12 Is CUDA available: False CUDA runtime version: 11.3.109 GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN ve",2022-09-22T14:30:12Z,module: onnx triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/85475,"Hi, Could you 1. try with nightly PyTorch 2. validate with the following strict mode checker if you want to run it on ONNXRUNTIME:  NOTE: if this still passed, please raise an issue ONNXRUNTIME","Hi, I gave a try to both of your suggestions. Anyway the result is more or less the same. Only the number of the split node in the error message has changed, when I used the nightly build. Will post it on ONNXRUNTIME if nothing else helps. Thank you.","FWIW, ONNXRUNTIME has a folder to demonstrate the exporting and running of huggingface/transformers model: https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/transformers/models",chojnowski  any news ?,Unfortunately I couldn't spend more time on it. And this idea was put on hold for now.,`torch.onnx.export` is in maintenance mode and we don't plan to add new operators/features or fix complex issues. Please try the new ONNX exporter and reopen this issue with a full repro if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial
yi,"[PolishTypo] Alisa->Alias, indivually->individually, jEverything->Everything","Polish comment typo, `Alisa>Alias`, `indivually>individually`, `jEverything>Everything`",2022-09-22T14:07:24Z,triaged open source cla signed Stale,closed,0,6,https://github.com/pytorch/pytorch/issues/85472,   Typo change should have no influence on CircleCIbuild. Can you help me rerun `CircleCI Checks / build`? ,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: HongyuJia . The commit (bde2d949e0d2a281a1c140ccb540ae46642281fc) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.", Please sign the CLA.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",/easycla
transformer,CyclicLR memory leak fix,"Hi, we noticed in our team that by using CyclicLR, there is a problem with memory clearance on GPU (probably it will be the case without the GPU as well, but that was our use case) After initializing CyclicLR, GPU memory is not cleared even after the model, optimizer and scheduler are out of scope (e.g. reference count is zero). This is because `__init__` method inside `CyclicLR` creates reference to its own methods and it will not get removed until `gc.collect()` is called manually. This is a problem if people want to test multiple models in one run of a script, after testing the first model, second one will fail on `CUDA out of memory error` because the first one is not cleared from the memory. I propose a simple fix by using `weakref`, similarly as in `_LRScheduler` base class, but if you have any comments I am happy to change it.  Here is the code to reproduce the bug: ",2022-09-22T06:56:49Z,triaged open source Merged cla signed release notes: python_frontend topic: improvements,closed,0,14,https://github.com/pytorch/pytorch/issues/85462,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!," Sorry for the late reply, both test and renaming to private variables are done. One can easily verify that the test will fail if you include something like this inside CyclicLR's __init__ method: ", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ,Could you please fix the lint issues?,"> Could you please fix the lint issues?  Sorry! When I execute `lintrunner` locally I have no errors, probably a bad setup. Fixed.", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.",Seems I do not have permission to add any label?,"Added! In the future, you can ask the bot to do it for you as needed:  help", h
yi,[Quant] Make x86 backend default when querying qconfig,This PR is a followup of CC([Quant] Add unified x86 quant backend) [[Quant] Add unified x86 quant backend](https://github.com/pytorch/pytorch/pull/84329) It makes `x86` backend default when querying `qconfig`. Users get x86's qconfig/qconfig_mappings if backend is not specified.,2022-09-22T06:32:44Z,triaged open source Merged cla signed ciflow/trunk,closed,1,6,https://github.com/pytorch/pytorch/issues/85461,"> Looks reasonable, but just to be safe can we rebase this on top of https://github.com/pytorch/pytorch/pull/84329/files and then put for review? Currently this doesn't include the base revision, so review is a bit confusing and it could be easy to miss a detail. Thanks. Maybe we rebase it after https://github.com/pytorch/pytorch/pull/84329 is merged.",Hi  Do you have more comments?, Is it good to land? We hope to land it before 1.13 branch cut. Thanks., merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey Weiwen. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[RFC] Separate CPU offload activation to its own wrapper,"  CC([FSDP][BE][Docs] Expose helper classes)  CC([FSDP] Doc to explain running submodules)  CC([FSDP] Fix clip_grad_norm for CPU offload)  CC([FSDP] assert to runtime error)  CC(CheckpointSequential support nonreentrant)  CC([RFC] Separate CPU offload activation to its own wrapper) Passing in `offload_to_cpu=True` to checkpoint_wrapper is a bit confusing, because this causes the activation checkpoint args to be ignored and we do CPU offloading. This isn't ideal from API design perspective, so proposing to make `offload_wrapper` its own concept. Now, offload to CPU + checkpoint can be composed together, such as  Will polish / add tests if this proposal sounds good. Differential Revision: D39719854",2022-09-22T06:12:40Z,oncall: distributed Merged cla signed ciflow/trunk release notes: distributed (fsdp),closed,2,8,https://github.com/pytorch/pytorch/issues/85459,"Related:  CC([feature request] Autocast module and function wrappers) Also, if autocast also existed as explicit wrapper, these problems would be easier to understand:  CC(Simultaneously using `torch.no_grad` and `autocast` causes `RuntimeError: expected scalar type Half but found Float` for some operations.)  CC(with torch.cuda.amp.autocast() get out of memory error when using with torch.no_grad() during validation)","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",discussed with  to punt the renaming to the future when API is more finalized and functional approach is being taken.,"varma has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","upload test status  Blocked is the failure, unrelated. Landing"," merge f ""CI failure unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey varma. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Fix thread-allocation in `_vec_log_softmax_lastdim`," Problem history There seems to always have been a bug in `_vec_log_softmax_lastdim `. In particular, there were two issues with it   Bug 1  Before AVX512 support was added, `CHUNK_SIZE` had been heuristically chosen in `_vec_log_softmax_lastdim`:  `CHUNK_SIZE = (128 / sizeof(scalar_t)) * Vec::size();` It was  `256` for float32, bfloat16, and float16. When AVX512 support was added, `CHUNK_SIZE` became `512`. The rationale behind determining `CHUNK_SIZE` has not been described, and seems flawed, since the number of OpenMP threads used currently depends upon it.  Bug 2 `grain_size` had been defined as `internal::GRAIN_SIZE / (16 * dim_size * CHUNK_SIZE)` So, `grain_size` was usually 0, as it was `8 / (dim_size)`, so, it's always replaced by `CHUNK_SIZE`, viz. 256. Since `256` was always the `grain_size` for `at::parallel_for`, few threads were used in certain cases.  Problem caused by bugs With `outer_size` of say, 700, only 3 threads would have been used with AVX2, irrespective of the value of `dim_size`! When AVX512 support was added, since `CHUNK_SIZE` became `512`, only 2 threads were used if `outer_dim` was 700. In the Transformers training examp",2022-09-21T09:58:06Z,module: cpu triaged open source Merged cla signed ciflow/trunk intel,closed,3,16,https://github.com/pytorch/pytorch/issues/85398, label ciflow/trunk,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: sanchitintel  (2aea9ec811f184664e8ed2d3200c36bc088c3117, d65a38889c1d98db2d50fd8981e669136c9e4a2c)","Unrelated CI failure  `RuntimeError: Failed running call_function (*(FakeTensor(FakeTensor(..., device='meta', size=(2, 2)), cpu), 0.1, 10, torch.quint8), **{})`", merge," Merge failed **Reason**: Approval needed from one of the following (Rule 'superuser'): kkosik20, jg2912, swolchok, qihqi, shajrawi, ... Details for Dev Infra team Raised by workflow job ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Hi  , just wanted to run this change by you. Thanks!",Would this PR fix  CC(AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU)?,"And if so, please throw in some relevant benchmarks once 's concerns are addressed ","Hi ,  yes, this PR fixes CC(AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU). I've updated the benchmark data in the PR description. Thanks!", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: GraphQL query  fragment PRReviews on PullRequestReviewConnection {   nodes {     author {       login     }     state   }   pageInfo {     startCursor     hasPreviousPage   } } fragment PRCheckSuites on CheckSuiteConnection {   edges {     node {       app {         name         databaseId       }       workflowRun {         workflow {           name         }         url       }       checkRuns(first: 50) {         nodes {           name           conclusion           detailsUrl         }         pageInfo {           endCursor           hasNextPage         }       }       conclusion     }     cursor   }   pageInfo {     hasNextPage   } } fragment CommitAuthors on PullRequestCommitConnection {   nodes {     commit {       author {         user {           login         }         email         name       }       oid     }   }   pageInfo {     endCursor     hasNextPage   } } query ($owner: String!, $name: String!, $number: Int!) {   repository(owner: $owner, name: $name) {     pullRequest(number: $number) {       closed       isCrossRepository       author {         login       }       title       body       headRefName       headRepository {         nameWithOwner       }       baseRefName       baseRepository {         nameWithOwner         isPrivate         defaultBranchRef {           name         }       }       mergeCommit {         oid       }       commits_with_authors: commits(first: 100) {         ...CommitAuthors         totalCount       }       commits(last: 1) {         nodes {           commit {             checkSuites(first: 10) {               ...PRCheckSuites             }             status {               contexts {                 context                 state                 targetUrl               }             }             pushedDate             oid           }         }       }       changedFiles       files(first: 100) {         nodes {           path         }         pageInfo {           endCursor           hasNextPage         }       }       reviews(last: 100) {         ...PRReviews       }       comments(last: 5) {         nodes {           bodyText           createdAt           author {             login           }           authorAssociation           editor {             login           }           databaseId         }         pageInfo {           startCursor           hasPreviousPage         }       }       labels(first: 100) {         edges {           node {             name           }         }       }     }   } } , args {'name': 'pytorch', 'owner': 'pytorch', 'number': 85398} failed: [{'message': 'Something went wrong while executing your query. Please include `040C:7DAB:194D17A:33EF84A:63E2542E` when reporting this issue.'}] Details for Dev Infra team Raised by workflow job ", merge that's a new one..., Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,PixelShuffle check that output is not null before applying kernel (#85155),Checks that tensor is not null before applying kernel in `pixel_shuffle` op Signedoffby: Thytu  Fixes CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions),2022-09-20T17:00:59Z,triaged open source cla signed,closed,0,23,https://github.com/pytorch/pytorch/issues/85347,"Yes, add a test please. `math_pixed_shuffle` is used in any dispatch that's not CPU. For example, CUDA, XLA, META, LTC (LazyTensor Core)...",>  Roger that,Test added but not sure if I it's the right place to put it as a `FunctionalTest` (it's rather a UT IMO) but didn't find another place to test in the C++ API. Would you prefer the test to be in test/test_nn.py or in another place ?,"It crashs here when running test_ops_gradients.py : https://github.com/pytorch/pytorch/blob/8bad17f6b97d719a7dcece99ac91126a3029a10b/torch/autograd/__init__.pyL300L302 I currently don't know why (I don't know this part of the stack), I'm still inspecting.","As per  CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions)issuecomment1250791994, you added the check to just one of the two functions and not the other, so when the other one is being executed it segfaults :D","> As per  CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions) (comment)issuecomment1250791994), you added the check to just one of the two functions and not the other, so when the other one is being executed it segfaults :D I thought the same but I'm not sure as:  I'm testing by running `python m pytest test/test_ops_gradients.py k test_fn_fwgrad_bwgrad_nn_functional_pixel_shuffle_cpu_float64` (so it runs on CPU) and it fail.  I force check that it's not caused by `math_pixel_shuffle` by changing it to :   The following code works : ",The new sample inputs look like the right coverage but I think the crashes just need more debugging to understand. ,It may be that the backward for this function also needs a similar treatment. The equivalent of `native_functions.yaml` is called `derivatives.yaml`.,"> It may be that the backward for this function also needs a similar treatment. The equivalent of `native_functions.yaml` is called `derivatives.yaml`. Alright I will take a look at it, thanks",> It may be that the backward for this function also needs a similar treatment. The equivalent of `native_functions.yaml` is called `derivatives.yaml`. As it's `auto_linear` it's autogenerated. Quick question :what would the grad supposed to be as the output is empty? `1`? Three options: 1. I missinterpreted the debug and the issue come from elsewhere (don't think so) 2. The `derivative` as to be handwritten (no longer autogenerated) 3. `pixel_shuffle` shouldn't return an empty output What are your thought on it?,`auto_linear` means that the forward AD is autogenerated. You should look into the formula for the first parameters., any update here?,>  any update here? No progress Tell me if I'm wrong but to fix the fw/bw grad it will require replacing the autogenerated `auto_linear` by a hand defined derivative fn. Is it?,"As mentioned in https://github.com/pytorch/pytorch/pull/85347issuecomment1258387365, `auto_linear` is just for forward AD and forward AD just works. You are having issues with backward AD. The backward AD is the formula after the `self:` in the entry for this function in `derivatives.yaml`. It seems that it's not handling well empty inputs. Have a go at it, and if you get stuck, ping me and I'll try to give you a hand.",>  Giving it a try,"So if I understand well, the backward of pixel_shuffle, is just pixel_unshuffle. Right?",yep! It looks like pixel_unshuffle may need a similar fix,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: Thytu / name: Valentin De Matos  (95e1481a4e26b49f14db9196ae5ccdf43c4a0902, cab68d7126fa1600e4a8cc3138ed76dd9b9d83bd, bdda9b6f8f1e2ac09749d9fea08c109ce72555aa, af9efc6c384884d8001f9d5a25b5ab4a7c8210e4, 3cdf429c75e394a0b9cabbd3463836eb326f74b4, 8bad17f6b97d719a7dcece99ac91126a3029a10b, b786597aa296561197e2f3a0965ec8af29122735)","Sorry for the waiting time, I locked a time slot to work on it this evening :) ","> As mentioned in  CC(PixelShuffle check that output is not null before applying kernel (85155)) (comment), `auto_linear` is just for forward AD and forward AD just works. You are having issues with backward AD. The backward AD is the formula after the `self:` in the entry for this function in `derivatives.yaml`. It seems that it's not handling well empty inputs. >  > Have a go at it, and if you get stuck, ping me and I'll try to give you a hand. I admit to be stuck on this one... * Test failing : `test_fn_fwgrad_bwgrad_nn_functional_pixel_shuffle_cpu_float64` * To reproduce : `python bb test_ops_gradients.py v importslowtests importdisabledtests k test_fn_fwgrad_bwgrad_nn_functional_pixel_shuffle_cpu_float64` * Test exec `pixel_unshuffle_cpu`",Oh shit ðŸ¤¡,you may want to close this PR and push a new one :)
yi,"[PolishTypo] inherentely->inherently, intentially->intentionally","Polish comment typo, `inherentely>inherently`, `intentially>intentionally`",2022-09-20T06:34:29Z,open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/85325, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[PyTorch] StorageImpl: cache size_bytes.is_symbolic(),"  CC([PyTorch] StorageImpl: cache size_bytes.is_symbolic()) We've got 6 bools' worth of extra space, so let's try caching this. Differential Revision: D39636570 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!",2022-09-19T22:41:32Z,Merged cla signed topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/85309, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,"Deprecate TypedStorage, its derived classes, and all of their public methods","Part of CC(Remove `TypedStorage` and use only `UntypedStorage`)   BCbreaking note  Deprecate `torch.Tensor.storage()` in favor of `torch.Tensor.untyped_storage()` Version 1.13  Version 2.0   Deprecate `torch.TypedStorage` and all its methods in favor of `torch.UntypedStorage` Version 1.13  Version 2.0  If you need to access individual elements in a storage as a particular dtype, you can simply create a tensor to view it: ",2022-09-19T21:02:03Z,open source Merged cla signed with-ssh ciflow/trunk release notes: distributed (c10d) module: python frontend ciflow/periodic module: dynamo ciflow/inductor,closed,0,42,https://github.com/pytorch/pytorch/issues/85303,", I've added a test to check that it only gets raised once unless warnings are cleared. I didn't add all of the functions to the test, but I can if we want to be that thorough",nah that's good enough,A good follow up would be to make sure pytorch proper doesn't raise these deprecations, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ,"> A good follow up would be to make sure pytorch proper doesn't raise these deprecations By this, do you mean because a `DeprecationWarning` doesn't raise by default? Actually, I forgot to use DeprecationWarning, I will fix that","> By this, do you mean because a DeprecationWarning doesn't raise by default? Actually, I forgot to use DeprecationWarning, I will fix that Different: what I mean is that we shouldn't trigger the deprecation warning if the user didn't explicitly use typed storage. If pytorch internally is hitting the DeprecationWarning, we should fix it (because otherwise it will spam users)","I see, good point"," , there are some places where internal calls are raising the warning. For instance, when serializing tensors. I'm not sure what would be the best way to avoid internally generated warnings, but I thought of two options. The first one is to use the `warnings` module filter to suppress the warning at every internal call site. But I don't think that's very good, since the filter has to do a string search and it will probably affect performance significantly. The other is to add an underscored version of each function which does everything except for raising the warning. The public function would just raise the warning and then call the underscored function. We would change all the internal call sites to use the underscored version to avoid raising the warning. Something like this:  I think I like this solution, but what do you think? Is there a better way? I tried googling a common solution for this and haven't found anything yet","No warn variants sgtm, esp if you only need a few / we have a strategy for getting rid of them","Actually, I just realized that I can't do this for dunder functions. So instead, I think I'll have to add a kwarg `_internal=False` to all the functions. If it's False, the warning will get raised","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: kurtamohler  (895f95c6488196de139dd96b339d09cba3390688, 9f654a8ce3ebab4cfbb3b0aa7606b9d6c8d5b7ee)","I've fixed a lot of the places where internal storage method calls were raising the warning. There may be more though, because I don't really have a great way to find all of them. I found some call sites with grep and some by running a bunch of the unit tests that don't deal with storages directly and raise an error if `_warn_typed_storage_removal` is called. Do you think this is alright,  , or should I keep looking for more internal call sites? (I also renamed a couple nonpublic things, like `TypedStorage._storage` > `TypedStorage._untyped_storage` to make them a bit clearer)",I'm happy to ship this as is. Do you want another look over?,> I'm happy to ship this as is. Do you want another look over? Sounds good,looks like you've still got some tests to fix,"Well, I think I need to suppress more of the warnings before we can merge this. In the linuxbionicpy3.7clang9 / test (crossref, 2, 2, linux.2xlarge) job, 48% of the lines in the log are the warning message, which is way too many. I'm not sure why it's getting raised so many times in that job though, it doesn't happen locally for me. One thing that would reduce the number of warnings is to use the internal TypedStorage functions in the unit tests instead of the public ones. I'm not sure if that's a good idea though. Or I could add a warning filter to tests that use the public functions","> One thing that would reduce the number of warnings is to use the internal TypedStorage functions in the unit tests instead of the public ones. I'm not sure if that's a good idea though. Or I could add a warning filter to tests that use the public functions For the tests, it's fine to suppress them entirely, since you're testing deprecated apis, the deprecation doesn't apply.",For some reason I'm not able to ssh into CI jobs anymore. Looking into it,"I can't ssh into CI jobs because Quansight's `pytorchjumphost.quansight.dev` server is down. While that's being fixed, I decided to put a short stack trace into the warning messages so the logs will show more information about where the duplicate warnings are coming from","There are two errors that keeps coming up in the linuxbionicpy3.7clang9 / test (crossref, 1, 2, linux.2xlarge) CI job (and others), which appear to be part of the command `/opt/conda/bin/python bb test_serialization.py v importslowtests importdisabledtests`:  click to expand   But for some reason when I log into that CI machine and run `/opt/conda/bin/python bb test_serialization.py v importslowtests importdisabledtests`, I don't get these errors. Maybe there's some environment state that the CI job sets up before running the tests that I'm not aware of. The tests do fail if I run `.jenkins/pytorch/test.sh` though. EDIT: Here's the warning that's popping up: click to expand   So I guess somewhere in `.jenkins/pytorch/test.sh`, this CI job must be overriding the `Tensor._storage()` method with `__torch_function__`, and whatever it's being overridden with ends up calling `Tensor.storage()`, which produces the error. I'm not very familiar with `__torch_function__`, so I'm not yet sure how to avoid this `Tensor.storage()` call. EDIT 2: I found that if I do this, I get the errors, without having to run all of `.jenkins/pytorch/test.sh`:  And this reproduces locally, thankfully. I'm not really sure what crossref is, but I'll find out. Also, I found that I can now reproduce a lot of the duplicate warnings locally with `PYTORCH_TEST_WITH_CROSSREF=1 python test/test_proxy_tensor.py`. So once I understand this crossref thing, I should be able to fix those too","One of the CI jobs is failing because MultiPy depends on `TypedStorage._storage`, which I renamed to `TypedStorage._untyped_storage` in an attempt to make things more clear. I could just restore the old name. Although I think the better way is to change MultiPy to use the public method `TypedStorage.untyped()` instead","My last update fixed most of the duplicate TypedStorage warnings, at least for the `linuxbionicpy3.7clang9 / test (crossref, 1, 2, linux.2xlarge)` CI job. There are a handful left that I need to look into, and I'll also look at all the other CI logs as well","Some of the duplicate warnings happen because `gradcheck` evidently resets the warnings. For instance:  Output:  I'm not sure yet where in `gradcheck` warnings are getting reset. But this only happens in a handful of places in the CI tests, so I think it can looked into later. Also, I'm still fixing up a few other internally generated warnings. Very close to being finished, I think","Weird, gradcheck shouldnt reset warnings.   do either of you know?",Turns out `gradcheck` has this behavior just because of the `warnings.catch_warnings()` context manager: https://github.com/pytorch/pytorch/blob/7f88934a8fb9b376b32c722ac2f05959da34c147/torch/autograd/gradcheck.pyL835 For instance:  Output:  There is an old issue open for this on the `warnings` repo: https://github.com/python/cpython/issues/73858,"Looking through the logs, the warnings aren't very noisy anymore. Most of the test logs have less than 30 of them, and most of these are just from separate test file executions (separate processes). Only two of the jobs have more than 100 warnings. I am sifting through all the logs now and soon I will post an overview of all the duplicate counts for each job and what I know about them. I think there's only a handful of root causes left now, and I think almost all of them can probably be investigated/fixed after this PR is merged (for instance, the duplicates from inbetween `warnings.catch_warnings()` calls). But I am sifting through the logs that generate the most duplicates now, and I'll post a summary soon"
rag,Remove `TypedStorage` and use only `UntypedStorage`,"Now that typed storages have been removed from the C++ side and `torch.UntypedStorage` is in place, we can remove `torch.TypedStorage` and all of its subclasses, and just use `UntypedStorage` instead. First, we should add warnings to all of the methods of `TypedStorage` and its subclasses for at least one whole release.",2022-09-19T21:00:55Z,triaged module: python frontend,open,0,0,https://github.com/pytorch/pytorch/issues/85302
rag,Templatize checkInBoundsForStorage and setStrided for SymInt,  CC(Add braces around single line conditional)  CC(Remove improper asserts.)  CC(Templatize checkInBoundsForStorage and setStrided for SymInt)  CC(Fix bug in computeStorageNbytes) Signedoffby: Edward Z. Yang ,2022-09-17T16:00:27Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/85205
rag,Fix bug in computeStorageNbytes,  CC(Add braces around single line conditional)  CC(Remove improper asserts.)  CC(Templatize checkInBoundsForStorage and setStrided for SymInt)  CC(Fix bug in computeStorageNbytes) Signedoffby: Edward Z. Yang ,2022-09-17T15:57:39Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/85204
yi,Symintifying slice ops,  CC(Symintifying slice ops)  CC(OpInfo for Slice),2022-09-16T23:44:36Z,Merged cla signed Reverted release notes: vulkan,closed,0,8,https://github.com/pytorch/pytorch/issues/85196,Testing?,commandeering this PR, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""Break internal build Exutorch"" c ghfirst", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
transformer,[ONNX][bug] `nn.Transformer` contains unsupported tensor scalar type, ðŸ› Describe the bug PyTorch fails to export a model containing an `nn.Transformer` module. It fails with `RuntimeError: unexpected tensor scalar type`. Here's a minimal repro script:   Versions ,2022-09-15T22:00:46Z,module: onnx triaged onnx-triaged bug,closed,0,12,https://github.com/pytorch/pytorch/issues/85116," There are three places in our code base that will raise ""unexpected tensor scalar type"". CC([ONNX] Improve torch.onnx diagnostics)",", to clarify: I expect that the sample code I provided executes without any errors. This is not a diagnostic issue; rather, it's a bug in either the exporter such that it can't handle `nn.Transformer` or a bug in `nn.Transformer` where it's written in a way that it cannot be exported.","FYI, this bug is a regression from earlier releases. The same code snippet I provided above exports successfully when PyTorch is installed with `pip install torch==1.11 extraindexurl https://download.pytorch.org/whl/cu113`. The regression was introduced somewhere between the `1.11.0` and `1.12.0` releases.", Understood. Thanks for the info! We are in the process of improving diagnostics so this is relevant. Please feel free to share thoughts if you have any suggestions!,"Notes: is_cuda, is_grad_enabled",I am able to export the nonscripted version of the module with the latest pytorchnightly. Does that work for you?,"Yes, that works. The nonscripted version works on 1.12.1 as well, though, so this bug is kinda different. FWIW, I'm attempting to script because tracing my actual network (not the minified version I pasted here) takes ~45 mins to export on a trivial sequence of 3 elements. And when I try to run that network, I get random results. The network is a basic autoregressive loop with a Transformer core and a decode loop of 32 steps.",Thanks for the info. I was not able to reproduce yet because apparently nn.Transformer introduced new ops it uses internally the onnx exporter doesn't yet support. So I haven't been able to get to the type error.  any suggestions? I don't think it's possible to trace only the Transformer and script the rest of the decoder?," what you mentioned is probably achievable, but I'm more curious as how it regress from 1.11. It could be though that `nn.Transformer` has changed. ",Apparently there is a `_nested_tensor_from_mask` op being used since 4 months ago  https://github.com/pytorch/pytorch/blame/4cfd09d7bc51f700373a3ee633776bdc903f9cde/torch/nn/modules/transformer.py,Now I observe `torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'prim::is_cuda' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues`,We are not planning adding new ops to torch.onnx.export API. Please try `torch.onnx.dynamo_export` and reopen this issue with an updated repro if the issue persists
finetuning,[MPS] division-by-zero returns 0.0 instead of Inf, ðŸ› Describe the bug Divisionbyzero correctly returns Inf onCPU:  MPS returns 0.0 instead:  noticed this whilst we were trying to find the source of NaN and Inf during stablediffusion finetuning:   https://github.com/lstein/stablediffusion/issues/517issuecomment1248573390  Versions  ,2022-09-15T20:31:59Z,triaged module: mps,closed,2,3,https://github.com/pytorch/pytorch/issues/85106," `torch.ones()`, or making the 0 into a `0.`, seem to be ways to make it return inf.",Thanks san for the report. I've tried to reproduce this issue with latest nightly but without success. Could you please give a try on latest nightly build (torch1.13.0.dev20221004) and let me know if you still see the issue? Thanks.,"yup, confirmed working on latest nightly (`1.14.0.dev20221013`). thanks!"
rag,change the type of storage_offset to SymInt,Fixes ISSUE_NUMBER,2022-09-15T19:54:45Z,Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/85102, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
chat,torch.atan2 crash with segmentation fault in the nightly version," ðŸ› Describe the bug `torch.atan2` crash with segmentation fault  Output:  Also reproduced in the gist  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220913+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.4 LTS (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.10 Python version: 3.7.6 (default, Jan 8 2020, 19:59:22) [GCC 7.3.0] (64bit runtime) Python platform: Linux4.15.0176genericx86_64withdebianbustersid Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.19.2 [pip3] torch==1.13.0.dev20220913+cpu [pip3] torchaudio==0.13.0.dev20220913+cpu [pip3] torchvision==0.14.0.dev20220913+cpu [conda] blas 1.0 mkl [conda] mkl 2020.2 256 [conda] mklservice 2.3.0 py37he8ac12f_0 [conda] mkl_fft 1.3.0 py37h54f3939_0 [conda] mkl_random 1.1.1 py37h0573a6f_0 [conda] numpy 1.19.2 py37h54",2022-09-15T00:14:21Z,module: error checking triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/85059,Should be fixed by https://github.com/pytorch/pytorch/pull/85294
rag,[reland 2] Call jit decomp in VariableType to improve forward AD coverage,Reland of https://github.com/pytorch/pytorch/pull/84675,2022-09-14T01:03:06Z,oncall: jit Merged cla signed ciflow/trunk release notes: autograd topic: improvements ciflow/periodic fx,closed,0,13,https://github.com/pytorch/pytorch/issues/84976," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hopefully internal builds should passing now. The issue was that certain builds (e.g. lite_trainer) depended on the VariableType files, but do not depend on the jit. Including all of those additional dependencies to get jit to work on lite trainer will blow up the binary size so I don't think we want that. Instead, what I'm doing in this updated PR (see latest commit for new changes) is the same thing that is done with VariableHooksInterface, i.e. provide the apis conditionally depending on whether the library depends on jit.","I've confirmed that internal tests are passing now. I've cleaned up this PR a little bit and addressed the comments, so it should be ready for another look! > There's a question of what we want to do in the long term. Should we take the TorchScript dependency, or should we just invoke the decomposition in Python? Since the TorchScript route is easy to do and proven to work, we should go with this PR for the shorttomedium term. This route makes sense to me as well"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,SymInt support for multiply_integers,These helpers are used in symintifying `reshape`. Ported from symbolicshapes branch    CC(SymInt support for computeStride)  CC(SymInt support for multiply_integers)  CC(StmInt support for InferSize),2022-09-12T23:56:47Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/84904
yi,multiply_integers,  CC(computeStride)  CC(computeStride)  CC(multiply_integers)  CC(Add SymInt support for infer_size_dv),2022-09-12T23:49:00Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/84900
rag,[TensorImpl] Make set_storage_keep_dtype virtual,"Summary: For lazy tensor impl, set_storage_keep_dtype needs to be virtual to take care of dummy storage created for frontend tensors. For lazy tensor impl, simple move of storage doesn't work because it may not have been evaluated and for other book keeping that TensorImpl need to do, this function should be virtual.",2022-09-12T08:48:21Z,triaged open source cla signed Stale,closed,0,11,https://github.com/pytorch/pytorch/issues/84855," could you please elaborate maybe with some examples why a dummy storage option won't work?  This PR only makes `LTCTensorImpl::set_storage_keep_dtype` virtual, if it's called via a base class pointer, it would still be using the base class implementation? For example,  This could be inconsistent and confusing.","  I have added my comments below  >  could you please elaborate maybe with some examples why a dummy storage option won't work? >  > This PR only makes `LTCTensorImpl::set_storage_keep_dtype` virtual, if it's called via a base class pointer, it would still be using the base class implementation? This PR is making the base tensor TensorImpl::set_storage_keep_dtype` virtual, so that it can be overridden by other derived classes of TensorImpl. If there is no overloading, then the base class TensorImpl::set_storage_keep_dtype` would be called, the purpose of making this virtual is to handle derived class specific functionality. This would be similar to existing virtual methods in TensorImpl like has_storage(), storage() etc. >  > For example, >  >  >  > This could be inconsistent and confusing."," Any thoughts on what  mentioned. Basically, in your example we are doing  struct A {     **virtual** void foo() {std::cout << ""A\n""; } };",">  Any thoughts on what  mentioned. Basically, in your example we are doing struct A { virtual void foo() {std::cout << ""A\n""; } }; I think i agree that the proposed change is making the base TensorImpl method virtual and thus doesn't apply to your example . But I also don't understand the implications of making this change well enough or whether there is a better way to achieve the goals.  Maybe  can help.",Could you give us some more context on the lazy tensor implementation you're working on?,"Hi , we are facing issue with jit.load using Habana backend. In rebuildTensor() function in unpickler.cpp, empty tensor is created and then some storage from some other tensor is attached to it. storage() of tensor in lazy mode may be just dummy storage, if ""impl>set_storage_keep_dtype(storage_tensor.storage());"" is called, we want control for our impl to evaluate the ops accumulated till that point, so that storage is actual storage. I see set_storage_offset is already virtual, that's why proposed that if set_storage_keep_dtype can also be made virtual. Other option is to create a new function that encapsulates set_storage_keep_dtype, set_storage_offset, set_sizes_and_strides into one function and make it as virtual and call only from rebuildTensor.",What is stopping Habana from having a real storage for its serialization format?,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",The committers listed above are authorized under a signed CLA.:white_check_mark: login: amitchawla1  (d7b175570216ee703dbf6c60b4922a3dacd2f935),", problem is set_storage_keep_dtype is setting storage directly and we can't assume it to be actual storage, since this function is used at many places.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,MPS: allow selecting specific MTLDevice by registryID via environment variable," ðŸš€ The feature, motivation and pitch Hello :wave: _very_ new to pytorch and running it on both M1 macs and on a mac with a couple Radeons. I would like to be able to use more than one of the Radeons at times (in different processes), or to select one specifically so I can use the other GPU for other tasks while I work on other things, etc. I think this should (hopefully?) be pretty trivial  I'm not familiar enough with the build system and intricacies of conda/pip etc to have run a build just yet, but I have a draft PR on my fork here: https://github.com/jaeschliman/pytorch/pull/1 Open to any changes, pointers, etc. Thanks!  Alternatives At first considered selecting by peerGroup and peerIndex, but Apple's docs state that registryID is stable across processes, so it looks like the correct choice.  Additional context https://developer.apple.com/documentation/metal/mtldevice/2915737registryid?language=objc ",2022-09-10T13:32:52Z,triaged enhancement module: mps,open,0,2,https://github.com/pytorch/pytorch/issues/84813,"Just curious: why wouldn't follow a pretty standard approach of  `{device}:{ordinal}`, i.e. `x=torch.empty((3, 3), device='mps:1')`?","  The use case I hope to enable is pinning a script to a chosen device without modifying the source of the script. In other words, naming the device by an ordinal in the script source seems orthogonal to what I hope to accomplish here. Does that make sense? Again, I am very new to pytorch, so apologies if I missed something that seems obvious. Thanks!"
yi,macOS Pyinstaller: libc++abi: terminating with uncaught exception of type c10::Error: Type c10::intrusive_ptr<ConvPackedParamsBase<2>> could not be converted to any of the known types," ðŸ› Describe the bug I'm trying to package a python app using pyinstaller.  Torch: 1.12.1 Pyinstaller: 5.3 OS: macOS 12.5 (M1) [UPDATE] This appears to be a problem with `torchvision`. If I remove the dependency, no crash. When I try to run my UNIX executable, I get the following:    Versions [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.1 [pip3] pytorchlightning==1.4.2 [pip3] torch==1.12.1 [pip3] torchfidelity==0.3.0 [pip3] torchaudio==0.12.1 [pip3] torchmetrics==0.6.0 [pip3] torchvision==0.13.1 [conda] numpy                     1.23.1                   pypi_0    pypi [conda] pytorchlightning         1.4.2                    pypi_0    pypi [conda] torch                     1.13.0.dev20220907          pypi_0    pypi [conda] torchfidelity            0.3.0                    pypi_0    pypi [conda] torchmetrics              0.6.0                    pypi_0    pypi [conda] torchvision               0.14.0.dev20220907          pypi_0    pypi ",2022-09-09T20:50:33Z,module: cpp-extensions triaged module: third_party needs research module: m1,open,0,6,https://github.com/pytorch/pytorch/issues/84782,"Looks like importing from `pytorch_lightning` causes the same error, but could still be an import of `torchvision` under the hood.","Hmm, `oncall: jit` feels like a wrong tag here (as it has nothing to do with it, likely crash happens because libtorch_cpu somehow getting packaged twice by pyinstaller.... At least I can reproduce the crash while trying to package something as simple as: ","I found the workaround from this thread to work for the timebeing. After pyinstaller finishes, I can run:  https://github.com/pyinstaller/pyinstallerhookscontrib/issues/375issuecomment1053572749 This indicates it might be a pyinstaller bug, rather than a torch issue."," thank you for confirming. I.e. it does not look like the bug is in PyTorch or TorchVision, but rather in a way how pyinstaller packages those libraries together. Do you mind opening an issue there and I would gladly coordinate with them if there is a metadata we can provide in the PyTorch dependent packages to avoid this doublepacking issue","Meta question, should we transfer this issue to `pytorch/vision` since this is more of an issue with vision than core pytorch?",I don't think it's specific to torchvision (nor to M1 to be frank)
transformer,add autocast to test transformers,Add additional test for autocast + transformer fastpath. ,2022-09-08T22:35:58Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/84723
transformer,Disable Transformer/MHA fast path when autocast is enabled,Differential Revision: D39362298,2022-09-08T22:04:48Z,fb-exported Merged cla signed release notes: nn topic: bug fixes,closed,0,11,https://github.com/pytorch/pytorch/issues/84722,This pull request was **exported** from Phabricator. Differential Revision: D39362298, merge g, Merge failed **Reason**: PR CC(Disable Transformer/MHA fast path when autocast is enabled) has not been reviewed yet (Rule superuser) Details for Dev Infra team Raised by workflow job ,This pull request was **exported** from Phabricator. Differential Revision: D39362298, merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!",This pull request was **exported** from Phabricator. Differential Revision: D39362298, Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Error when trying to export MONAI model to ONNX," ðŸ› Describe the bug When trying to export the SwinUNETR model from MONAI, I get the error:  In a different issue, I read that this issue might get fixed by changing `x_shape = x.size()` to `x_shape = [int(s) for s in x.size()]` in the problematic code  I found out that problem manifests at `proj_out()`. Doing this, though, results in a different error:  Making this change in all places where I find `x_shape = x.size()` results in a floating point exception! Here is a minimal example demonstrating the issue:   Versions Collecting environment information... PyTorch version: 1.12.1.post200       Is debug build: False                 CUDA used to build PyTorch: 11.2                                                                               ROCM used to build PyTorch: N/A                                                                                OS: CentOS Linux release 7.9.2009 (Core) (x86_64)                                                              GCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.12)                                                            Clang version: Could not collect                                                      ",2022-09-08T12:41:08Z,module: onnx triaged onnx-needs-info,closed,0,5,https://github.com/pytorch/pytorch/issues/84692,"It seems to me that you are exporting a transformer with static input shape, which is unlikely correct, as transformer is complicated. Please refer to torch.onnx.onnx, and see how you can utilize `dynamic_axes`.","I don't quite understand why it would be a problem to export with static axes. Nonetheless, I tried changing dynamic_axes to:  or even  With the exact same error message.",The following repro on latest torch master branch with monai/einops   produced this error    Could you review the provided script to make sure it runs on latest torch?, Tried with pytorch 1.13 and got the same error you posted.,"`torch.onnx.export` is in maintenance mode and we don't plan to add new operators/features or fix complex issues, including checkpoint activation Please try the new ONNX exporter and reopen this issue with a full repro if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial"
yi,[TensorExpr] applying `rfactor` for a Mul Reducer with init value different than 1 results in wrong results," ðŸ› Describe the bug `rfactor` initializes the `rfac_init` with the initializer of the original reduce op. If the original Reducer uses Mul as the ReduceInteraction with an init value different than 1, the result after `rfactor` will be wrong (the same issue for an Add reducer with an init value different than 0). https://github.com/pytorch/pytorch/blob/8bd9fe3f493073bf8f4a2e428c3048096fb36052/torch/csrc/jit/tensorexpr/loopnest.cppL3380L3381  CPP UT to reproduce: The tensor of size `2 x 8` has values all set to 1. The reduce axis is the last dim (where `size == 8`). The Reducer has the init value = 2. The ReduceInteraction is Mul. The expected result will be `tensor([ 2, 2])`. With `rfactor`, the result becomes `tensor([ 4, 4])`.   Output log:   Stmt without and with `rfactor`: The **correct** final stmt without `rfactor`:  The **wrong** final stmt with `rfactor`:   Versions ",2022-09-08T06:04:46Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/84685
rag,[reland] Call jit decomposition in VariableType to increase forward AD coverage (#84151),"Stack from ghstack:  CC([reland] Call jit decomposition in VariableType to increase forward AD coverage (84151)) This reverts commit acb4a09628284201281e262aaee58e3dc6be9c2b. In addition, we also fix a memory leak in layer norm.",2022-09-08T00:05:08Z,oncall: jit Merged cla signed Reverted ciflow/trunk release notes: autograd topic: improvements ciflow/periodic,closed,0,9,https://github.com/pytorch/pytorch/issues/84675,"Made an update to address your comments   (1) completely agree with this point, it has been reverted! (2) I'd rather have this in a different PR since the changes required would probably be mostly unrelated to the ones here",> (2) I'd rather have this in a different PR since the changes required would probably be mostly unrelated to the ones here SGTM, merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This causes internal asan linktime failures:   I'd recommend ghimporting this PR the next time you plan to reland and ensuring the internal signal is green before merging the PR, given that this is the second revert."," revert m ""causing asan xplat linktime errors like ld.lld: error: undefined symbol: torch::jit::has_jit_decomposition(c10::FunctionSchema const&)"" c ""ghfirst""", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
yi,Convert ConcretePyInterpreterVTable into Meyer singleton,  CC(Convert ConcretePyInterpreterVTable into Meyer singleton)  CC(Convert NoopPyInterpreterVTable into a Meyer singleton) Signedoffby: Edward Z. Yang ,2022-09-07T19:47:01Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/84657,"Internal is complaining that LeakSanitizer says this code is leaking, so I wanted to see if a Meyer singleton would fix it.", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Convert NoopPyInterpreterVTable into a Meyer singleton,  CC(Convert ConcretePyInterpreterVTable into Meyer singleton)  CC(Convert NoopPyInterpreterVTable into a Meyer singleton) Signedoffby: Edward Z. Yang ,2022-09-07T19:43:58Z,cla signed Reverted,closed,0,3,https://github.com/pytorch/pytorch/issues/84656," revert m ""this breaks some build configs"" c weird", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
yi,Add underlying_store property for PrefixStore,Stack from ghstack: * * CC(Add underlying_store property for PrefixStore) Add underlying_store property for PrefixStore**  CC(Add host and port to TCPStore pyi definition) Add host and port to TCPStore pyi definition Add a property to `PrefixStore` to retrieve the underlying store it is wrapping around. Open for suggestions on property name. This change is based on discussion in D39225101 where we need to read properties of the store that PrefixStore is wrapping around. Differential Revision: D39311151,2022-09-07T15:26:15Z,oncall: distributed Merged cla signed release notes: distributed (c10d),closed,0,5,https://github.com/pytorch/pytorch/issues/84640,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84640**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 16d676377f (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Huang has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge," successfully started a merge job. Check the current status here and land check progress here. The merge job was triggered with the land checks (l) flag. If you did not specify this flag yourself,  you are likely enrolled in the land checks rollout. This means that your change will be merged once all checks on your PR and the land checks have passed (**ETA 4 Hours**). If you need to coordinate lands between different changes and cannot risk a land race, please add the `ciflow/trunk` label to your PR and wait for signal to complete, and then land your changes in proper order. Having `trunk`, `pull`, and `Lint` prerun on a PR will bypass land checks and the ETA should be immediate. If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Add host and port to TCPStore pyi definition,"Stack from ghstack:  CC(Add underlying_store property for PrefixStore) Add underlying_store property for PrefixStore * * CC(Add host and port to TCPStore pyi definition) Add host and port to TCPStore pyi definition** `host` and `port` are already exposed in the `TCPStore` pybind definition, this is a small change adding it in the pyi stub Differential Revision: D39311153",2022-09-07T14:39:21Z,oncall: distributed Merged cla signed release notes: distributed (c10d),closed,0,5,https://github.com/pytorch/pytorch/issues/84636,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84636**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 3c7897e4ef (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Huang has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge," successfully started a merge job. Check the current status here and land check progress here. The merge job was triggered with the land checks (l) flag. If you did not specify this flag yourself,  you are likely enrolled in the land checks rollout. This means that your change will be merged once all checks on your PR and the land checks have passed (**ETA 4 Hours**). If you need to coordinate lands between different changes and cannot risk a land race, please add the `ciflow/trunk` label to your PR and wait for signal to complete, and then land your changes in proper order. Having `trunk`, `pull`, and `Lint` prerun on a PR will bypass land checks and the ETA should be immediate. If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,"Fix exception handling, improve overheads and avoid constructing storage for element size for DLPack",These changes were proposed by  in CC(Tensor cuda array interface: avoid constructing storage for element size) and CC(DLPack interface: fix excpetion handling and improve overheads) that fix CC(Improve speed/overhead of CUDA array interface) and CC(Handle python exceptions correctly in DLPack Capsule Destructor) respectively.  The reason I am creating the pull request is CLA check (see original PRs).   ,2022-09-07T00:14:05Z,triaged open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/84612,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84612**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (17 Pending) As of commit c9040ecef1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"A : we could also fix CC(Do not call nullptr deleter in at::fromDLPack (dlpack)) by checking whether the source tensor has a `deleter` in `fromDLPack`: Changes these lines: https://github.com/pytorch/pytorch/blob/4dfa6d28a139e8325fe9b255af86e8d1360ae7ee/aten/src/ATen/DLConvertor.cppL254L257 To this:  This would make it conform to the DLPack spec which mentions that the deleter maybe `nullptr`. This isn't really related to the other changes here though, so this could be another PR.", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey A. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"[ONNX] when exporting a PyTorch model, creates an unnecessary variable that messes up inferencing."," ðŸ› Describe the bug I have a NLP application I've built on PyTorch w/ transformers RoBERTa. As expected, the inferencing should take 2 parameters, input_ids & attention_mask.  However, if I export the said model and view it on netron, it shows 3 inputs: input_ids,attention_mask, and a onnx::Reshape_2 input that shouldn't be there.  My personal prediction is that because my forward pass is implemented as follows:  Onnx converter thinks that labels, is another input that's required for inferencing. But it's not named as ""labels"" aswell, so that is also confusing.  After renaming this onnx:Reshape_2 as ""labels"" , If, one tries to take this model, and run it with an inferencing script in another python environment, we get the following error: The script:  Error Message:  This error basicly says, that my input of 6 zeros (a dummy version of my 6 classed labels) is invalid, and needs 2 , if I substitute: labels = np.zeros(6).astype(np.float32) with labels = np.zeros((2,6)).astype(np.float32) then the error says  When I inspect the problem with netron, it shows an input right at the end of the computational graph, that seems useless as predicted to the who",2022-09-03T12:16:45Z,module: onnx triaged onnx-needs-info,closed,0,4,https://github.com/pytorch/pytorch/issues/84514,Here is how I create my export aswell: ,"> Here is how I create my export aswell: >  >  Hi  , `torch.onnx.export` provides **dynamic_axes** for user to keep their calculation with dynamic shapes. I think that might be one of the reasons you are getting a weird graph. Please find the usage of it in this link: https://pytorch.org/docs/stable/onnx.htmltorch.onnx.export","I would suggest extracting the loss computation part out from your forward function (so that it is not part of your model), as it is not needed for inference and should not be traced during export.","Thank you very much for comments  : the dynamic_axes implementation solved my issue. ðŸ‘   : Noted, it was in my TODO list for a while now. ðŸ‘ "
transformer,"For PyTorch Nightly, failure when changing MPS device to CPU after PYTORCH_ENABLE_MPS_FALLBACK occurs."," ðŸ› Describe the bug When trying to generate text with a GPT2 from the transformers library, I get this error: NotImplementedError: The operator 'aten::cumsum.out' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on  CC(General MPS op coverage tracking issue). As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS. So I activated the environment variable (I set it in the terminal, because it didn't work with the version in the following code), but afterwards another error occurs (I posted it after the code used). I need to mention that if I only use CPU from the start, the generation works without problems.    Versions Collecting environment information... PyTorch version: 1.13.0.dev20220831 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.5.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version:",2022-09-02T14:28:54Z,triaged module: mps,open,5,1,https://github.com/pytorch/pytorch/issues/84489,Hotfix doesn't work anymore. !image
yi,OpInfo: Use yield consistently instead of appending to lists,  CC(OpInfo: Add test that sample_inputs_func returns a generator)  CC(OpInfo: Use yield consistently instead of appending to lists)  CC(OpInfo: Sample input cleanup (4/n))  CC(OpInfo: Sample input cleanup (1/n)) The yield syntax reduces the numbers of tensors held inmemory when running opinfo tests and also avoids constructing all samples when only the first sample is going to be used.,2022-09-02T01:56:49Z,open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/84454,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84454**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (33 Pending) As of commit dd6044c6a5 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Incredible effort! It would be good to add a test to make sure that people keep using this pattern rather than the list pattern.,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: peterbell10  (99a49b36bea6cc914ceb61bf65afc8685326e056, e688d5fbdcdab1398757fe7bc72e69e05268fbc3, 06a3543158cf7e8c780f3669effd5fd941664cd5, 828b9bda3124180432a777bf672145e2b2f00744, a168ebc8545bf9bd97e3981945e3f4db2b55129b, 150dd4d387abe604a62ea9a061af13c7c72537e0, 00956ca297d8afe99ea185a0957086c76a81d327, 51115024502f506f0b9324c7bf7391af80613a5b, 9d8240d0a5de79be40913ddd9f965a04c2f092b5, 1a5a7bbd1538b51e0467786a7dfe518f4f628354, 70d3d34cc82649de6dcc1f82b32845358e56f199, 8b0aedb7578fb51b6058be2abf47f2e8c18be97d, dd6044c6a574e10c9c751422ec5c76ae6e59e337, 4510f66ddb2fc9d00d59af318192db43e9a04f33, 239320fd7b7c476f22adf5ea291a5c8eb6ebf1f2, 62bc7e8a3960e376a56555706d27d9c410d79ea8, 77ebeb8a733c2a373e83392d81d5d9973abcba12, f8c989e24962294f8c57b1467747d3da35195d04, 84ae19ca161ca8084e0e69a265397621171d51ec, edcb2f40f998a88d14fe9e2b8fe258e0f97aa034, 8df6fa13fbbc21a66e5cffa5062c2547090fef21, 7ebdcb32cf68347a5fcfbe053e9b06bc7a9c4b85, 3a291ee94a0dfd1377087bfafe52d1605af9a4c3, 745fafa4ca7da8a07cc7c2f4943188f144adf9b0, f5132f1884a7764c2c91412a999cc63f606f18e4, 49c92c6dbc431b0ede43c36ae41438b7f3893f46)"
transformer,[ONNX] Exporting the operator zero to ONNX opset version 9-15 is not supported.," ðŸ› Describe the bug When i convert transformer to onnx, throwing the above error. RuntimeError: Exporting the operator zero to ONNX opset version 9 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub.  Model Structure   main code   Versions ",2022-09-01T08:01:16Z,needs reproduction module: onnx triaged onnx-needs-info,closed,0,3,https://github.com/pytorch/pytorch/issues/84406,Would you be able to break it into a simple reproduction? I can't repro with the provided code.,Like  requested. Also please consider using the latest pytorchnightly version when you create your reproducible example.,"Weâ€™ve gone ahead and closed this issue because it is **stale**. If you still believe this issue is relevant, please feel free to reopen the issue and we will triage it as necessary. Please specify in a comment any updated information you may have so that we can address it effectively. We encourage you to try the latest pytorchpreview (nightly) version to see if it has resolved the issue."
yi,Refactor PyInterpreter to use normal vtables,"  CC(Refactor PyInterpreter to use normal vtables) I realized that we can deal with the dead vtable problem by... introducing another indirection!  The resulting code is worse (you have to do one more dereference to get to the vtable), but the reduction in boilerplate is, IMO, worth it. I did this refactor because I'm about to add a lot more methods to PyInterpreter to handle expunging SymInt from TensorImpl. Signedoffby: Edward Z. Yang ",2022-09-01T02:16:06Z,Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/84388,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84388**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures As of commit 359101b96f (more details on the Dr. CI page): Expand to see more  * **2/2** failures introduced in this PR   :female_detective: 2 failures *not* recognized by patterns: The following CI failures may be due to changes from the PR    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.",took a look. we're executing Python anyway; what's one more layer of indirection between friends?
yi,ROCm support for test_lazy_init,"Added ROCm support for the test_lazy_init unit test by including a condition on TEST_WITH_ROCM to switch CUDA_VISIBLE_DEVICES with HIP_VISIBLE_DEVICES.  This is needed because HIP_VISIBLE_DEVICES is set when running the singleGPU tests in CI: https://github.com/pytorch/pytorch/blob/a47bc96fb7176d43752d3e376697971d4ba47317/.jenkins/pytorch/test.shL38, but this test sets CUDA_VISIBLE_DEVICES, which takes lower precedence than HIP_VISIBLE_DEVICES on ROCm. **Testing Logs (to show behavior difference)** 12:40:41 Aug 30 11:40:41 CUDA_VISIBLE_DEVICES='0': 0 12:40:41 Aug 30 11:40:41 1 12:40:41 Aug 30 11:40:41 CUDA_VISIBLE_DEVICES='32': 32 12:40:41 Aug 30 11:40:41 1 12:40:41 Aug 30 11:40:41 HIP_VISIBLE_DEVICES='0': 0 12:40:41 Aug 30 11:40:41 1 12:40:41 Aug 30 11:40:41 HIP_VISIBLE_DEVICES='32': 32 12:40:41 Aug 30 11:40:41 0 **Passing UT** Aug 30 17:03:15 test_lazy_init (main.TestCuda) Aug 30 17:03:17 Validate that no CUDA calls are made during import torch call ... ok (2.471s)",2022-08-31T09:32:18Z,module: rocm triaged open source Merged cla signed ciflow/trunk,closed,0,6,https://github.com/pytorch/pytorch/issues/84333,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84333**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 8eaf38788f (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Rebased on viable/strict as suggested to retest CI,PR now passes the CI after rebase  , merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Profiler] Capture storage data pointer,"  CC([Profiler] Compute unique IDs for Tensors)  CC([Profiler] Clean up Tensor representation)  CC([Profiler] Capture storage data pointer) This is approximately a reland of the storage half of https://github.com/pytorch/pytorch/pull/80266 I've directly represented and exposed storage impl rather than using it as a first guess for an ID. (Mostly for testing, which happened to save me as I was initially recording the wrong thing.) Differential Revision: D39136546",2022-08-30T16:05:37Z,cla signed release notes: profiler,closed,0,1,https://github.com/pytorch/pytorch/issues/84276,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84276**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 9 New Failures As of commit c69af93b2e (more details on the Dr. CI page): Expand to see more  * **9/9** failures introduced in this PR   :detective: 9 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/8119092339?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 1, 2, linux.2xlarge) (1/9) **Step:** ""Test"" (full log  diagnosis details)   20220831T17:29:05.5897688Z RuntimeError: test_proxy_tensor failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
rag,Tensor cuda array interface: avoid constructing storage for element size,Fixes CC(Improve speed/overhead of CUDA array interface) ,2022-08-30T13:47:30Z,open source,closed,0,6,https://github.com/pytorch/pytorch/issues/84271,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84271**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 2cbcf55fb7 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," Thank you for approving the PR. I'm working for NVIDIA and still waiting on them for some guidance on what type of CLA (individual/company) I should accept. This may take some time, sorry for that!","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Matt Joux . The commit (2cbcf55fb7dae00bfa8152a302cba6d01e424e9b) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.",Closing since this was already part of https://github.com/pytorch/pytorch/pull/84612 (easier due to CLA).
llm,[No CI] Demo: Subgraph Rewriter for call_module,  CC([No CI] Demo: Subgraph Rewriter for call_module)  CC(Pretty print stack trace with gm.print_readable()),2022-08-29T23:21:37Z,cla signed not4land fx,closed,0,1,https://github.com/pytorch/pytorch/issues/84233,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84233**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :white_check_mark: 1 Base Failures As of commit e662892e65 (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 7c7a60c405 on Aug 29 from  3:58pm to  6:19pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * Lint / lintrunner on Aug 29 from  3:58pm to  6:19pm (b5cbc56c48  8aba2535e4)  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
llm,Inductor misaligned address error in PegasusForCausalLM,Note this adds a CI skip for PegasusForCausalLM.  I don't think the following error is related to this PR:  Likely just uncovered by different fusions happening. _Originally posted by  in https://github.com/pytorch/torchdynamo/issues/1077issuecomment1229762518_ ,2022-08-29T04:39:28Z,oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93651,PegasusForCausalLM is no longer skipped
transformer,Support setting strides on quantized weights of Embedding," ðŸš€ The feature, motivation and pitch For quantized Embedding, deberta in transformers will choose weights from original weights, but I get:  `*** RuntimeError: Setting strides is possible only on uniformly or per channel quantized tensors`. Below are the details of quantized weights of Embedding. torch.per_channel_affine_float_qparams should be a per channel quantization schema, right?  tensor([[ 0.0064, 0.0069, 0.0082,  ..., 0.0215, 0.0056, 0.0003],         [0.0069,  0.0087, 0.0225,  ..., 0.0136, 0.0069,  0.0065],         [0.0071,  0.0225, 0.0071,  ..., 0.0182, 0.0219,  0.0151],         ...,         [0.0037,  0.0060, 0.0203,  ..., 0.0176,  0.0088,  0.0102],         [0.0126, 0.0350, 0.0074,  ..., 0.0284,  0.0032, 0.0218],         [ 0.0065, 0.0123,  0.0018,  ..., 0.0170,  0.0065,  0.0018]],        size=(50265, 768), dtype=torch.quint8,        quantization_scheme=torch.per_channel_affine_float_qparams,        scale=tensor([0.0013, 0.0022, 0.0037,  ..., 0.0014, 0.0013, 0.0047]),        zero_point=tensor([ 87.2115,  50.1056,  93.9225,  ...,  77.6577,  81.5797, 115.6229]),        axis=0)  Alternatives No  Additional context _No response_ ",2022-08-28T09:36:52Z,oncall: quantization low priority triaged,open,0,1,https://github.com/pytorch/pytorch/issues/84178,"per_channel_float_qparams is a different qscheme: https://github.com/pytorch/pytorch/blob/main/c10/core/QScheme.hL19, please feel free to add support for it here: https://www.internalfb.com/code/fbsource/[f4da2ffcd586948ec1fcafced11e99a3b6d934a1]/fbcode/caffe2/aten/src/ATen/native/TensorShape.cpp?lines=1228"
transformer,FSDP Forward order differs from that of first run," ðŸ› Describe the bug Hi, I am using fsdp(integrated with hf accelerate) to extend support for the transformer reinforcement learning library to multigpu. This requires me to run multiple .generate calls, forward passes, and then a backwards pass.  I am getting a warning: `UserWarning: Forward order differs from that of the first iteration on rank 0  collectives are unchecked and may give incorrect results or hang`. Some insight would be appreciated. Minimum code to reproduce:  A similar issue has already been opened on accelerate's repo with some discussion here  Versions PyTorch version: 1.12.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Amazon Linux release 2 (Karoo) (x86_64) GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.115) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.26 Python version: 3.8.5 (default, Feb 18 2021, 01:24:20)  [GCC 7.3.1 20180712 (Red Hat 7.3.112)] (64bit runtime) Python platform: Linux5.10.126117.518.amzn2.x86_64x86_64withglibc2.2.5 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: NVI",2022-08-27T23:34:52Z,oncall: distributed module: fsdp,open,0,7,https://github.com/pytorch/pytorch/issues/84175,"Upon checking, the reason for this warning is when FSDP finds that not same parameter is used by using an allgather to check that all ranks are running ``forward()``. But the code looks legit to me. :  , varma ", ,"I suspect what is happening is that the dummy forward pass via `outputs = model(query_tensors)` is being considered as part of the 1st iteration. Specifically, the 1st iteration includes `outputs = model(query_tensors)`, `logits, _, v = model(input_ids)`, and `logits, _, vpred = model(input_ids)`, while the subsequent iterations only include `logits, _, v = model(input_ids)` and `logits, _, vpred = model(input_ids)`. For FSDP, the end of an iteration is determined by the end of a backward pass via `.backward()` in this case triggered by `accelerator.backward(vf_loss)`. If your model can train without hanging or erroring, then you can simply ignore the warning.","Interestingly the dummy forward pass is required as a hacky fix to another error thrown if this is not done: here.  I believe I encounter this warning even if all iterations follow the first iteration's format, but I need to verify this.",Unfortunately training does not proceed properly. :(. Reward is not increasing., Would you be able to follow up here?,>  Would you be able to follow up here? My concern is that debugging this requires peeking into the HF accelerate code and requires setting up the appropriate libraries. It is most likely not an FSDPspecific issue but rather an integration issue somewhere.
rag,Create codeql.yml to leverage github Code Scanning,Enables Code Scanning on Python and JS. It detects bugs like https://github.com/pytorch/pytorch/blob/1dabb51a16eb6cf81475efecb1d39c4683af50fb/benchmarks/distributed/rpc/rl/launcher.pyL99 and others. /pytorchdevinfra,2022-08-27T22:41:28Z,module: ci triaged open source cla signed Stale topic: not user facing,closed,0,15,https://github.com/pytorch/pytorch/issues/84174,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84174**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 2 New Failures As of commit ba78edc868 (more details on the Dr. CI page): Expand to see more  * **2/2** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/8053531809?check_suite_focus=true) Lint / lintrunner (1/2) **Step:** ""Run lintrunner on all files"" (full log  : 20220827T22:43:51.1593993Z [command]/usr/bin/git config local nameonly getregexp http\.https\:\/\/github\.com\/\.extraheader ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",Thank you for adding this! You can do a rebase to fix those build failures due to a missing CI action.,"As expected, there are too many errors (or noise depending on how people view them) https://github.com/pytorch/pytorch/pull/84174/checks?check_run_id=8278980270 and a failed codeql scan will block the PR in its current state: * Do you know if it's possible to tune the scan to run only on the content of the PR and commit during pull_request and push events instead of scanning the whole repo? * I wonder if it's possible to config the scanner to report only high severity issue and above to make it less noisy * Let's not run this in every pull request until it's proven to be useful there","> * Do you know if it's possible to tune the scan to run only on the content of the PR and commit during pull_request and push events instead of scanning the whole repo? Yes! I think this is the default behavior. This PR reported all errors because it's the first scan. So everything is ""new"". Afterwards only new alerts should be reported in future PRs.  > * I wonder if it's possible to config the scanner to report only high severity issue and above to make it less noisy Yes to this too. Any query can be turned on or off.  https://docs.github.com/en/codesecurity/codescanning/automaticallyscanningyourcodeforvulnerabilitiesanderrors/configuringcodescanningexcludingspecificqueriesfromanalysis Something like   Can also be used to turn off alerts based on severity. I find some low severity queries helpful for finding bugs and higher severity ones noisy. So selecting individual may be helpful.  > * Let's not run this in every pull request until it's proven to be useful there We can also configure paths for it to run on https://docs.github.com/en/codesecurity/codescanning/automaticallyscanningyourcodeforvulnerabilitiesanderrors/configuringcodescanningavoidingunnecessaryscansofpullrequests", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `justinchu/codeql` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `git checkout justinchu/codeql && git pull rebase`)","> Yes! I think this is the default behavior. This PR reported all errors because it's the first scan. So everything is ""new"". Afterwards only new alerts should be reported. ~~Oh nice, it's good to know. Now the check is green~~ or is it? ","Hmm, should this be part of a lint workflow?","> > Yes! I think this is the default behavior. This PR reported all errors because it's the first scan. So everything is ""new"". Afterwards only new alerts should be reported. >  > ~Oh nice, it's good to know. Now the check is green~ or is it? It will only be show added alerts in other PRs after this ""base"" is merged. This PR will always contain all errors","> Hmm, should this be part of a lint workflow? That would be great. I haven't looked into how that is possible","> > Hmm, should this be part of a lint workflow? >  > That would be great. I haven't looked into how that is possible It looks like a reusable workflow would work here https://docs.github.com/en/actions/usingworkflows/reusingworkflowscreatingareusableworkflow having `lint.yml` calling `codeql.yml`","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Justin Chu . The commit (2772b4aef41c60ec0c7d53b45b814c9a6c57f867, 2111eba28aebc1c043488922ca7462ae4c13b10b, 914be8296efbdd0d4246c3489282df714cfbc234) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,mps: weird results given by Transformer CausalLM," ðŸ› Describe the bug  This should give something like: > using `device= cpu""` actually results in this:  However it actually gives:  Same behavior not only applies to `bigscience/bloom560m`, all CausalLM seems to results in similar behavior.  Just FYI To get rid of warning and rule out weird behavior from MPS fallback, I added some code to Transformer source `sitepackages/transformers/models/bloom/modeling_bloom.py` This does not effect the reported bug behavior.   Versions  ",2022-08-27T18:14:18Z,high priority triage review triaged module: mps,closed,0,9,https://github.com/pytorch/pytorch/issues/84169,"I had the same issue running `neox` on the M1. https://github.com/zphang/minimalgptneox20b/issues/5 With `mps`, I got ""...developed by EleutherAI. in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in"" and with cpu on the same machine, I got ""... developed by EleutherAI. It is a stateoftheart language model..."". That implementation doesn't use `transformers` at all, it's just plain pytorch. I tested on `1.13.0.dev20220803`.","I have the same issue with galactica6.7b used with huggingface's `transformers`. This is a minimal example to reproduce:  output with mps:  output when removing `to(""mps"")` (running in cpu mode):  torch version: `1.14.0.dev20221117` I'm using a Apple M1 Max Macbook with MacOS 13.0","Setting `use_cache` to `False` fixed it for me. ,e.g. ",What can be the cause of this? I have the same problem in a different pacakage but my model does not have a genrate function with use_cache,> What can be the cause of this? I have the same problem in a different pacakage but my model does not have a genrate function with use_cache Is this still happening with latest nightly ?,"Hey, yes i just tested it. It is still happening wit pytorch2.1.0.dev202 (installed today). The quality of the output is way worse when unsing ""mps"" compares to ""cpu"" on mac","Thanks   and Zhang, we will investigate the issue. ",Zhang thanks for filling this issue. Could you please try latest nightly? This should be fixed there: pip3 install pre forcereinstall torch indexurl https://download.pytorch.org/whl/nightly/cpu ,I can confirm the problem is gone with `torch2.1.0.dev20230804` on macOS 13.5 (22G74).  > Zhang thanks for filling this issue. Could you please try latest nightly? This should be fixed there: pip3 install pre forcereinstall torch indexurl https://download.pytorch.org/whl/nightly/cpu >  > 
rag,Call jit decomposition in VariableType to increase forward AD coverage,"Stack from ghstack:  CC(Call jit decomposition in VariableType to increase forward AD coverage)  CC([reland] Move decompositions and helpers for jvp from functorch into core) This PR:  updates forward AD codegen in core to generate code that tries calling into decompositions registered to jit when     (1) the function is not inplace or out variant     AND (2) the function is differentiable (requires_derivative=True)     AND (3) there are no forward AD formulas registered     To simplify things we always generating the if/else (as long as (1) is true), but generate 'false' when either (2) or (3) are false.   removes the mechanism from functorch      (follow up) some functorch tests should be updated here so they no longer have to compute the Jacobian with vjp    factors out some logic to generate the any_has_forward_grad condition       (bcbreaking) when TensorList inputs unexpectedly have forward grad, the error will no longer contain the name See https://github.com/pytorch/pytorch/pull/84151issuecomment1238519247 for codegen output and more discussion.",2022-08-26T21:54:41Z,oncall: jit Merged cla signed Reverted release notes: autograd,closed,0,9,https://github.com/pytorch/pytorch/issues/84151,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84151**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit f234e34dc7 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"For your codegen output: `const auto& opt_op = c10::Dispatcher::singleton().findSchema(full_name);`  This looks wrong to me  `findSchema` returns a value, not a reference, unless this is some c++ism.  findSchema is expensive, so we could cache the value by assigning it to a static variable: `static auto opt_op = ...` ","Here is an entire codegened function in VariableType:   Click to show  ```cpp ::std::tuple native_batch_norm_backward(c10::DispatchKeySet ks, const at::Tensor & grad_out, const at::Tensor & input, const c10::optional & weight, const c10::optional & running_mean, const c10::optional & running_var, const c10::optional & save_mean, const c10::optional & save_invstd, bool train, double eps, ::std::array output_mask) {   auto& grad_out_ = unpack(grad_out, ""grad_out"", 0);   auto& input_ = unpack(input, ""input"", 1);   auto _any_requires_grad = compute_requires_grad( grad_out, input, weight, save_mean, save_invstd );   (void)_any_requires_grad;   check_no_requires_grad(running_mean, ""running_mean"", ""native_batch_norm_backward"");   check_no_requires_grad(running_var, ""running_var"", ""native_batch_norm_backward"");   std::shared_ptr grad_fn;   if (_any_requires_grad) {     grad_fn = std::shared_ptr(new NativeBatchNormBackwardBackward0(), deleteNode);     grad_fn>set_next_edges(collect_next_edges( grad_out, input, weight, save_mean, save_invstd ));     grad_fn>grad_out_ = SavedVariable(grad_out, false);     grad_fn>input_ = SavedVariable(input, false);     grad_fn>weight_ = SavedVariable(weight, false);     grad_fn>running_mean_ = SavedVariable(running_mean, false);     grad_fn>running_var_ = SavedVariable(running_var, false);     grad_fn>save_mean_ = SavedVariable(save_mean, false);     grad_fn>save_invstd_ = SavedVariable(save_invstd, false);     grad_fn>train = train;     grad_fn>eps = eps;   }   at::Tensor result0;   at::Tensor result1;   at::Tensor result2;   ifndef NDEBUG   c10::optional grad_out__storage_saved =     grad_out_.has_storage() ? c10::optional(grad_out_.storage()) : c10::nullopt;   c10::intrusive_ptr grad_out__impl_saved;   if (grad_out_.defined()) grad_out__impl_saved = grad_out_.getIntrusivePtr();   c10::optional input__storage_saved =     input_.has_storage() ? c10::optional(input_.storage()) : c10::nullopt;   c10::intrusive_ptr input__impl_saved;   if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();   endif   auto _tmp = ([&]() {     if (true && (isFwGradDefined(grad_out)  isFwGradDefined(save_invstd))) {       static c10::OperatorName full_name(""aten::native_batch_norm_backward"", """");       static c10::optional opt_op = c10::Dispatcher::singleton().findSchema(full_name);       return impl::run_jit_decomposition_with_args_for_jvp, const at::Tensor &, const at::Tensor &, const c10::optional &, const c10::optional &, const c10::optional &, const c10::optional &, const c10::optional &, bool, double, ::std::array>(""native_batch_norm_backward"", *opt_op, ks, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);     } else {       at::AutoDispatchBelowADInplaceOrView guard;       return at::redispatch::native_batch_norm_backward(ks & c10::after_autograd_keyset, grad_out_, input_, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);     }   })();   std::tie(result0, result1, result2) = std::move(_tmp);   ifndef NDEBUG   if (grad_out__storage_saved.has_value() &&       !at::impl::dispatch_mode_enabled() &&       !at::impl::tensor_has_dispatch(grad_out_))     AT_ASSERT(grad_out__storage_saved.value().is_alias_of(grad_out_.storage()));   if (grad_out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_out_))     AT_ASSERT(grad_out__impl_saved == grad_out_.getIntrusivePtr());   if (input__storage_saved.has_value() &&       !at::impl::dispatch_mode_enabled() &&       !at::impl::tensor_has_dispatch(input_))     AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));   if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))     AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());   if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {     AT_ASSERT(result0.storage().use_count() == 1, ""function: native_batch_norm_backward"");   }   if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))     AT_ASSERT(result0.use_count()  One thing to note is that the backward graph will be created for both the nondecomposed and decomposed levels, but this is probably OK because the graph corresponding to the decomposed operations should be destroyed as soon as we call set_history at the higher level. An alternative to this is to actually modify codegen to no longer create the graph at this higher level when we know we are decomposition is met, but that requires more changes and increases complexity of the codegen and is probably slower because we would no longer be utilizing preexisting backward kernels. See https://docs.google.com/document/d/1k0RLf1GrGNYLglPf9sonjjoP_wghlfFbsDxca7ImQ/editheading=h.bcl5pn5tsxf9 for discussion of other alternatives.", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""Regressed test_jvpvjp_nn_functional_layer_norm_cuda_float32, see https://hud.pytorch.org/pytorch/pytorch/commit/42d99e6f196233627a28b8e9efb26a0a166fa370"" c landrace", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
agent,DISABLED test_dynamic_rpc_existing_rank_can_communicate_with_new_rank (__main__.TensorPipeTensorPipeAgentRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_dynamic_rpc_existing_rank_can_communicate_with_new_rank` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-08-26T12:52:23Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/84116,I can repo it locally looks like calling RPC to call the method fails here..
agent,Add watchdog to TorchElastic agent and trainers,"Summary: D38604238 (https://github.com/pytorch/pytorch/commit/3b11b80fc3f9f9a0171abb5eb2299835feba8b04) introduced a named pipe based watchdog timer. This diff uses the named pipe based watchdog timer in TorchElastic agent and training worker processes (in the StuckJobDetector class) to allow the TorchElastic agent to detect the stuck of a training process, and kill the process to create a core dump. Test Plan:      Differential Revision: D38930476",2022-08-25T20:16:10Z,oncall: distributed fb-exported Merged cla signed release notes: distributed (pipeline) topic: new features,closed,0,10,https://github.com/pytorch/pytorch/issues/84081,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84081**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b6a7e6b3bc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476, merge (Initiating merge automatically since Phabricator Diff has merged)," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,sync AveragedModel buffers when use_buffers=False,"Fixes CC(Buffers in AveragedModel are not synchronized with the source model when use_buffers=False) As described in the issue, the AveragedModel will deep copy the model during initialization, which means that the buffers in the averaged model cannot be updated together with the model. One solution is to make the buffers equal to the source model every time when calling `update_parameters`.",2022-08-25T13:13:54Z,triaged open source Merged cla signed,closed,0,16,https://github.com/pytorch/pytorch/issues/84054,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84054**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (18 Pending) As of commit 8dea46b1f4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",The committers listed above are authorized under a signed CLA.:white_check_mark: login: RangiLyu  (8dea46b1f4c8a23bf0d0b9e6de0368fdef5e0c57),"  Hi, sorry for bothering. Is there a plan to merge this PR?","Hi sorrysince it's approved, feel free to comment with ` merge` and it will merge it for you", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. You can rebase by leaving the following comment on this PR: ` rebase` Details for Dev Infra team Raised by workflow job , rebase, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `fix_avgmodel_buffer` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout fix_avgmodel_buffer && git pull rebase`)", merge,Just set up the job so I didn't forget. Sorry about that  and thanks for the PR, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Buffers in AveragedModel are not synchronized with the source model when use_buffers=False," ðŸ› Describe the bug The AveragedModel will deep copy the model during initialization, which means that the buffers in the averaged model cannot be updated together with the model.  This can easily lead to bugs when using AveragedModel if the model has some buffers that are updated during training.  Here is an example:  The result would be:  This leads to the completely wrong inference results of the ema model.  Suggestion Synchronize the averaged model's buffer with the source model when calling `update_parameters` if `use_buffers=False`. I created a simple PR to solve this:  CC(sync AveragedModel buffers when use_buffers=False)  Versions PyTorch version: 1.12.1 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Manjaro Linux (x86_64) GCC version: (GCC) 11.3.0 Clang version: 14.0.6 CMake version: version 3.23.3 Libc version: glibc2.36 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.1361MANJAROx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.7.99 GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 SUPER Nvidia driver ve",2022-08-25T13:10:29Z,module: optimizer triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/84053
rag,"""RuntimeError: src_total_size >= storage_byte_offset INTERNAL ASSERT FAILED"" when training using MPS on YOLOv5"," ðŸ› Describe the bug I was training on the latest version of YOLOv5 by the latest PyTorch nightly using MPS acceleration on an Apple Silicon based MacBook, but after just ONE successful epoch, I got this: RuntimeError: src_total_size >= storage_byte_offset INTERNAL ASSERT FAILED at ""/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Copy.mm"":129, please report a bug to PyTorch. So I am reporting a bug here. Note: the command I used to train the network is an example command that comes with the YOLOv5 train.py, I just added device mps parameter, so I think the problem is not the way I train.  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220824 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.5.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS12.5.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU mo",2022-08-25T07:24:01Z,triaged module: mps,closed,0,2,https://github.com/pytorch/pytorch/issues/84042,Hi Maxwell  thank you for the report! This issue is fixed in the nightly build  could you please try the latest PyTorch nightly ( torch1.13.0.dev20220930) and let me know if you still see any issues? Thanks!,"Yes, I tried this morning (UTC 2:47) and this issue is fixed. Thank you for your kind support and fix. Wish you a good day!"
yi,[WIP] symintifying reshape ,Fixes ISSUE_NUMBER ,2022-08-24T17:34:47Z,oncall: jit open source cla signed Stale release notes: jit,closed,0,6,https://github.com/pytorch/pytorch/issues/84000,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84000**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 30 New Failures, 1 Base Failures, 1 Pending As of commit d5e6399226 (more details on the Dr. CI page): Expand to see more  * **30/31** failures introduced in this PR * **1/31** broken upstream at merge base 50ae5c9141 on Sep 07 from  8:37am to  9:19am   :detective: 13 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/8233920729?check_suite_focus=true) pull / linuxfocalpy3.7clang7asan / test (default, 5, 5, linux.2xlarge) (1/13) **Step:** ""Test"" (full log     :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) on Sep 07 from  8:37am to  9:19am (0dddefe242  a47bc96fb7)  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","The tests are failing in crazy ways, so we might want to find a less dangerous way of doing this","btw, re reshape_alias, the changes you have to make should be substantially simplified post this stack https://github.com/pytorch/pytorch/pull/84579 so I highly recommend doing this on the branch (where the change is already available)","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Nick Korovaiko . The commit (3e5b65fef6be636f1ad158778e8575408a7114af, 44cdb40a6a59d629b92326bf049d0adee2c7cc27, 2e2f56791d810c284afd059817284ca7fa7668f3, 18f00da8b21a3ec92330b45249bc8bae17be937d, ae94f95ae6d2462b279413ef8e4bd33fa220f314, 6ca4ad580c3aaec232cbd0b06a0d4eddea004997, 0e1092a25e1636a2d2d844984f643ee1bbbb3b61, 758c271a6618e3534b68ebb33c4be526ddf1de7e, 5389b06fe401cb9cbf222009278437a3e4940aa9, ed027406eef239b654bfb9e53a9bfb4648f0c756, f55e49fc6ca3fa125fdcbd83df55401877389bb1, d5e6399226c906f17d729178b766075d18c6e969) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
chat,"RuntimeError: dst.nbytes() >= (dst.storage_offset() * dst.element_size()) INTERNAL ASSERT FAILED at ""/Users/davidlaxer/pytorch/aten/src/ATen/native/mps/operations/Copy.mm"":130, please report a bug to PyTorch. "," ðŸ› Describe the bug This assertion is being raised:  in  ~/pytorch/aten/src/ATen/native/mps/operations/Copy.mm   Versions % python collect_env.py  Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: macOS 12.5.1 (x86_64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.22.1 Libc version: N/A Python version: 3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: N/A CUDA runtime version: Could not collect GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] torch==1.13.0a0+gitb2ddef2 [pip3] torchmetrics==0.9.3 [pip3] torchvision==0.14.0a0+a61e6ef [conda] blas                      1.0                         mkl    anaconda [conda] captum                    0.5.0                         0    pytorch [c",2022-08-24T17:04:56Z,triaged module: mps,closed,0,0,https://github.com/pytorch/pytorch/issues/83995
rag,General NestedTensor op coverage tracking issue," This issue is to have a centralized place to list and track work on adding support to new ops for the NestedTensor backend. There are  large number of operators in pytorch and so they are not all implemented yet for the NestedTensor backends as it is still in the prototype phase. We will be prioritizing adding new operators based on user feedback. If possible, please also provide link to the network or usecase where this op is getting used. If you want to work on adding support for such op, feel free to comment below to get assigned one. Please avoid picking up an op that is already being worked on or that already has a PR associated with it.  Op coverage requests  [ ] aten::convolution  [ ] aten::max.dim  [x] detach CC([Nested Tensor] detach)   [x] to CC(Add support for .to() for NestedTensor backends)  [ ] eq  [ ] masked_select  [ ] index_select  [ ] narrow  [ ] alias  [ ] Broadcasting Ufuncs along implicit NT dim  [ ] Zerocopy Nt construction from Size info  [ ] BCE/ Other loss functions for NT  [ ] nested tensor creation from arbitrary masks rather than leftaligned  Backward op coverage requests  [ ] gelu, relu backward  [ ] layernorm backward ",2022-08-24T16:57:07Z,feature triaged module: nestedtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/83994," CC(Add eq, to, masked_select, index_select, narrow to nested tensors)","also, having clean nestedtensor support for depthwise conv is important for Conformer models which have them in convolution blocks"
rag,Support the XPU backend untyped storage,Simple add XPU backend in untyped torch storage.,2022-08-24T01:10:09Z,open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/83952,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83952**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (1 Pending) As of commit 7b8224a3f2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , It is glad to see the storage has been decoupled to be untyped. Add the necessary code to support the XPU backend in untyped storage., merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[deploy][inference] Load extra_files of package in cpp (needed for saved requests and warmup),"Summary:  Adds a method to the package struct to enable reading of data saved in ""extra_files"" of the package  D38378610 **[torchrec] Allow storing binary data in extra_files**  added how to save and read the extra files in python  For warmup, I need a way to save the requests, which can only be read as binary in python and the extra_files provides a convenient way to do so Test Plan: Tested that I can save a recordio file in python as binary, and read it as a string in cpp Differential Revision: D38914842",2022-08-23T01:25:12Z,fb-exported cla signed Stale,closed,0,5,https://github.com/pytorch/pytorch/issues/83882,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83882**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :white_check_mark: 1 Base Failures As of commit da0081924e (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 7cfc8b7820 on Aug 22 from  4:25pm to  8:36pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * Lint / lintrunner on Aug 22 from  4:25pm to  8:36pm (3db6859c4e  658f958bc4)  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D38914842,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Shabab Ayub . The commit (da0081924e2e9351341f7814e6617d907f233882) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
agent,Named pipe based watchdog timer,"Summary: This diff implements a named pipe based watchdog timer (`FileTimerClient` and `FileTimerServer`). This is similar to the existing `LocalTimerClient` and `LocalTimerServer` (https://fburl.com/code/j4b9pyya). The motivation is from the need of handling various timeout issues. The training process occasionally get stuck. We need a proper watchdog to monitor the liveness of the training processes. This timer allows the TorchElastic agent (as the watchdog) to monitor the progress of the training processes that it spawned. If a timeout occurred, he TorchElastic agent can take some action to kill the stuck process and creating a core dump for it. `LocalTimerClient` and `LocalTimerServer` require  a `multiprocessing.Queue()` to work. So they can only be used between `multiprocessing` parent and child processes. `FileTimerClient` and `FileTimerServer` does not have such limitation. Test Plan:  Unit Test   Differential Revision: D38604238",2022-08-18T18:41:00Z,oncall: distributed fb-exported Merged cla signed,closed,0,10,https://github.com/pytorch/pytorch/issues/83695,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83695**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (3 Pending) As of commit 051ff2bbb2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38604238,This pull request was **exported** from Phabricator. Differential Revision: D38604238,This pull request was **exported** from Phabricator. Differential Revision: D38604238,This pull request was **exported** from Phabricator. Differential Revision: D38604238, ,This pull request was **exported** from Phabricator. Differential Revision: D38604238, merge (Initiating merge automatically since Phabricator Diff has merged)," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Huggingface Transformers Trainer Test,"Transformers trainer API uses TorchDynamo. As we cleaned up Dynamo, we did not percolate the changes to trainer API, leading to some failures  https://github.com/huggingface/transformers/issues/18127 This is a tracker to improve the situation 1) Better API  Currently, we pass strings and then depending on the strings, we find the backend for Dynamo. Instead, we could just simplify all this, and directly do torchdynamo.optimze(backend_str)  https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.pyL639L666 2) Remove ctx manager. 3) Decompositions might not be working correctly for relu. I had to switch to cos instead of relu in this PR to see the memory footprint reduction  https://github.com/huggingface/transformers/pull/18685 4) Add a test in Dynamo CI that brings huggingface and tests Dynamo related tests ~~~ pytest tests/trainer/test_trainer.py k torchdynamo ~~~ ",2022-08-18T17:48:34Z,triaged oncall: pt2 module: dynamo,closed,2,1,https://github.com/pytorch/pytorch/issues/93632,Close the issue tracker as it seems fixed.
rag,"Coverage for nondeterministic_seeded, respect it in constant prop","  CC(Don't extract tensor metadata from sparse tensors)  CC(Refactor is_X_like, better invariant checking for SymInt overload)  CC(Refactor CppSignatureGroup to collect signatures as list.)  CC(Coverage for nondeterministic_seeded, respect it in constant prop)  CC(Be more conservative about propagating constants.)  CC(Address CR comments for ""Delete ProxyTensor wrapper subclass"")  nondeterministic_seeded was not applied to enough functions.  I added   some heuristics to codegen for identifying functions that are likely   to be random and added a bunch of these tags to functions.  Not sure   I got all of them.  Don't constant propagate through nondeterministic functions in FX   tracing. It would be better to do some testing for the tag but this would be quite an effort. Signedoffby: Edward Z. Yang ",2022-08-18T03:30:15Z,Merged cla signed fx,closed,0,4,https://github.com/pytorch/pytorch/issues/83650,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83650**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (2 Pending) As of commit 240d82b943 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Fix issue in softmax.cu with transformer error when mask seqlen > 1024 ,Fixes CC(Transformer encoder error when encoding long sequence (more than 1024 tokens)) Adds  test to catch this issue.   fix to softmax.cu that broadcasts src_key_padding_mask to regular attention_mask shape,2022-08-18T00:59:56Z,Merged cla signed release notes: cuda release notes: nn topic: bug fixes,closed,0,10,https://github.com/pytorch/pytorch/issues/83639,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83639**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 02020155aa (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `erichan1/btseqlen1024fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout erichan1/btseqlen1024fix && git pull rebase`)", rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `erichan1/btseqlen1024fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout erichan1/btseqlen1024fix && git pull rebase`)", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[functorch] reclassifying max_unpool failures,Stack from ghstack:  CC([functorch] reenable some linalg.det tests)  CC([functorch] reclassifying max_unpool failures)  CC([functorch] reclassify linalg.eigh in vmap testing)  CC([functorch] reclassify svd as an allowed failure; add test)  CC([functorch] add some vmap+jvp inplace+view tests)  CC([functorch] relax as_strided batching rule)  CC([functorch] annotate test_jvpvjp) They're expected because this variant of max_unpool is nondetermnistic Also removed a skip that seems to pass alright on my machine,2022-08-17T19:13:15Z,cla signed Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/83618,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83618**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 38b4f1560f (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7900726444?check_suite_focus=true) pull / linuxfocalpy3.7gcc7 / test (default, 1, 2, linux.2xlarge) (1/1) **Step:** ""Get workflow job id"" (full log | diagnosis details)   20220818T15:10:18.1833888Z   test_add_done_ca...arg() takes 0 positional arguments but 1 was given     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
finetuning,Multi-Grad Hooks," ðŸš€ The feature, motivation and pitch For FSDP, one advantage of the `FlatParameter` is that registering a hook on its `AccumulateGrad` object gives us the correct time to schedule a reducescatter: The hook runs when all constituent original parameters' gradients are ready. An alternative to this paradigm is to have a ""multigrad hook"" that only runs once all passedin tensors' gradients are ready. In that case, we can explore a new design for FSDP where we are not constrained to a single (large) `flat_param.grad`. Under such a new design, the `FlatParameter` would **only be for communication** and no longer relevant for autograd. One effort this half is to provide an option to expose the original parameters in FSDP. The current approach manually sets `.data` to be views into the `FlatParameter`, and another approach is to use a tensor subclass like `IndirectParameter`. Regardless of the approach, as long as we have the `FlatParameter` interface with autograd and thus have its `.grad` attribute populated, we cannot save memory when only _some_ of the original parameters require gradients. The parts of the `.grad` corresponding to parameters that do not",2022-08-17T19:01:52Z,feature module: autograd triaged actionable,closed,0,9,https://github.com/pytorch/pytorch/issues/83617,Does the following work for you?  This will output ,No because the multihook is not called if some of the hooks are unused,"From offline discussion:  There is no way to do without this at python level  There are two main avenues to do this:    We can change the engine to do it:      Add a new special kind of hook that the engine knows about      We can discover if/when it needs to be called ""for free"" by reusing the traversal that we already do to compute dependencies_      This new state will need to be stored on graph_task to ensure that multiple backward can properly run in parallel    Create a custom Node that is always needed_ if linked to the graph.      This hook will just be another Node in the graph      Will need to allow for an edge to have multiple endpoint < major change","I created https://github.com/pytorch/pytorch/pull/84773 for a different design, which aims to make fewer modifications to the engine/graph itself, but may require more changes on the Python side. It does the job by exposing a way to query the TLS graph task's exec_info which is a map mapping the Node to a bool indicating whether it will be executed in the current backward pass (as determined by the inputs= argument for .grad or .backward). Below, I've modified Alban's example above using the new API to address Ed's comment. ",This alternate API seems fine too!,"I think the primary risk is if this adds overhead, but  can probably test this",Thanks  for prototyping this so quickly! I will find some time to try it out soon.,"Hey , we're landing https://github.com/pytorch/pytorch/pull/86260, which is a cleaned up version of  CC(Multi-Grad Hooks)issuecomment1244048429. One improvement is that it now support multiple backwards on different threads on the same graph. One thing to note is that if an error happens during backward the state of the hook is still kept alive by the graph, so that can be problematic if someone catches the error and runs backward on the same graph. We may find a more automatic solution later, but for now to get around this one should just remove and reregister the hook to reset its state (this would reset the state for all backwards running concurrently). Let us know how it works for you!","Hi . Thanks so much for pushing this forward! Sadly, from the FSDP side, there have been a series of blockers that have emerged between our current state and adoption of multigrad hooks. Integrating multigrad hooks will not be in scope for this half :/"
transformer,Move odd num_head in TransformerEncoder to slow_path,"Summary: odd nhead is not supported for masked softmax, therefore we just move it to use old slow_path Test Plan: CI Differential Revision: D38720086",2022-08-15T22:27:50Z,fb-exported Merged cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/83483,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83483**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 052ed6f068 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38720086,"Failure is because we did not add odd nhead check to creating a NestedTensor. So when nhead is odd, we are sending a NestedTensor to slow path, which cannot accept NestedTensor. Best fix is probably to check if the first or any encoder layer in TransformerEncoder has odd nhead. Kind of ugly... but what can you do. ",This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086, merge (Initiating merge automatically since Phabricator Diff has merged)," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Only support when num_heads is even in transformer," ðŸ› Describe the bug I have verified that up until v.1.11 I can set `nhead=1` in `TransformerEncoderLayer`, but in v.1.12 this throws a Runtime Error:  As far as I can tell the docs do not specify that `nhead` must be even, unless I'm missing it.   Versions  ",2022-08-12T22:37:27Z,high priority triage review module: cuda triaged module: regression,closed,0,7,https://github.com/pytorch/pytorch/issues/83355,Reproduced! We are fixing. Thanks  ,"It could be tricky to fix the masked softmax kernel, so probably the easiest thing is to send odd number of heads to the slow path. ",Yep we'll send to slow path. Root cause is here https://github.com/pytorch/pytorch/blob/b2363520363d8c85496f6954fde005834c188b5d/aten/src/ATen/native/cuda/SoftMax.cuL1009. I would hope there aren't too many users of odd num MHA heads... but this is definitely a major issue. ,"To be clear, I really was looking just to maintain support for 1 head, not an odd number of heads generally. In my case testing showed that 2 heads was not better than 1 lol.","Then probably you want to use 2 heads, because 2 heads would go to a fast code path, whereas one head will go to a slow one, so it'll probably end up being slower than 2.","In my testing 2 heads is also slower than using 1. I also am relying on a pretrained model with 1 head, so having to use 2 heads would be an issue there as well.",https://github.com/pytorch/pytorch/pull/83483 Fixes and adds test
yi,"when input type of torch.cholesky_inverse is complex128, cpu's running result is inconsistent with gpu"," ðŸ› Describe the bug  **The result on cpu begins as follows:**        tensor([[[[ 2.2772e+01+0.0000e+00j, 1.4934e+011.6002e+01j,             7.4759e+00+1.0612e+01j, 1.6203e+002.7951e+00j,            5.8908e01+7.1717e01j],           [1.4934e+01+1.6002e+01j,  2.3012e+01+0.0000e+00j,            1.2838e+012.2871e+00j,  3.2950e+00+2.4935e01j,            5.4043e018.5046e01j],           [ 7.4759e+001.0612e+01j, 1.2838e+01+2.2871e+00j,             8.5023e+00+0.0000e+00j, 2.0702e+004.6753e01j,             5.8712e02+6.5857e01j],           [1.6203e+00+2.7951e+00j,  3.2950e+002.4935e01j,            2.0702e+00+4.6753e01j,  1.1360e+00+0.0000e+00j,            3.8658e014.2250e01j],           [5.8908e017.1717e01j, 5.4043e01+8.5046e01j,             5.8712e026.5857e01j, 3.8658e01+4.2250e01j,             5.4342e013.6619e01j]].......... **The result on gpu begins as follows:**        tensor([[[[ 2.3904e+014.5969e15j, 1.6702e+011.5623e+01j,             9.0184e+00+9.8650e+00j, 3.6530e+001.1465e+00j,            1.0580e01+1.1141e+00j],           [1.6702e+01+1.5623e+01j,  2.4514e+012.7789e15j,            1.3767e+015.5089e01j,  3.3668e+002.2533e+00j,            1.1135e+004.862",2022-08-12T03:18:53Z,triaged module: complex module: linear algebra,closed,0,5,https://github.com/pytorch/pytorch/issues/83311,Still reproducible on 1.12:  ,"The input matrix has values between 0 and 1 on the diagonal (it's eigenvalues), so it is likely to be badly conditioned for inverse. Especially after the ""outer"" product which will square the singular values of the input and blow up the condition number of the product. So the behavior is expected, that is, the input is badly conditioned and that the algorithms on CPU and on CUDA are likely different and unstable for closetosingular inputs.",'s comment shows that we still have an issue in master.,"It is very likely a singularity issue, take a look at: ","Yup,  is right. I take that back. This is expected as the matrix is very illconditioned."
transformer,Pytorch based Bert NER for transfer learning/retraining,"I trained an Bertbased NER model using Pytorch framework by referring the below article. https://www.dependsonthedefinition.com/namedentityrecognitionwithbert/. After training the model using this approach, I saved that model using torch.save() method. Now, I want to retrain the model with new dataset. Can someone please help me on how to perform retraining/transfer learning as I'm new to NLP and transformers. Thanks in advance.",2022-08-11T19:44:16Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/83290
transformer,Transformer encoder error when encoding long sequence (more than 1024 tokens)," ðŸ› Describe the bug In evaluation mode, Transformer encoder shows a runtime error when the number of tokens is greater than 1024.  runtime error:  But if the number of tokens is less than or equal to 1024, then it can run without error.   Versions PyTorch version: 1.12.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 8.4.03ubuntu2) 8.4.0 Clang version: 9.0.0 (https://github.com/condaforge/clangdevfeedstock 284a3d5d88509307bcfba64b055653ee347371db) CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.12 (main, Jun  1 2022, 11:38:51)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.16.3microsoftstandardWSL2x86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU Nvidia driver version: 516.59 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.12.1+cu116 [pip3] torchaudio==0.12.1+cu116 [pip3] torchvi",2022-08-10T04:11:41Z,module: nn triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/83142,I'm aware of where this issue is called  we take a fastpath at 1024 tokens for mask addition and there's a bug in there. Debugging. ,Working on the fix. Put up test first.  Edit: Done! https://github.com/pytorch/pytorch/pull/83639
transformer,"Fix typo in norm_first description, respectivaly - > respectively","Fix typo in norm_first description, respectivaly  > respectively Fixes CC(Typo in norm_first parameter description of transformer layer, respectivaly  > respectively)",2022-08-10T01:48:08Z,triaged open source Merged cla signed,closed,1,7,https://github.com/pytorch/pytorch/issues/83139,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83139**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit fd486799dd (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,Merge failed due to HTTP Error 403: Forbidden Raised by https://github.com/pytorch/pytorch/actions/runs/2833997951, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"Typo in norm_first parameter description of transformer layer, respectivaly - > respectively", ðŸ› Describe the bug https://github.com/pytorch/pytorch/blob/b2363520363d8c85496f6954fde005834c188b5d/torch/nn/modules/transformer.pyL358L359 https://github.com/pytorch/pytorch/blob/b2363520363d8c85496f6954fde005834c188b5d/torch/nn/modules/transformer.pyL563L565  Versions We don't need a version to fix a typo.,2022-08-10T01:47:51Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/83138
rag,Remove unbalanced `pragma diagnostic pop`,"Detected by internal CI, we should have it in OSS as well:  Regression introduced by https://github.com/pytorch/pytorch/pull/82883",2022-08-09T19:09:18Z,Merged cla signed topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/83095," merge f ""benign change, needed for diff train import""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[FSDP] specifying `device_id` will move ignored parameters,"If the wrapped module is on CPU, then specifying `device_id` will move the entire wrapped module to the device corresponding to `device_id`: https://github.com/pytorch/pytorch/blob/6c60a656b02fbd09661b282bec53940b184db3ca/torch/distributed/fsdp/fully_sharded_data_parallel.pyL1048 We should clarify the semantics for the `ignored_modules` argument and whether FSDP should move the ignored modules' parameters. ",2022-08-09T15:52:23Z,triaged module: fsdp,closed,0,2,https://github.com/pytorch/pytorch/issues/83078," Today in the latest version, the ignored modules are not moved to the device. Is it expected and going to stay this way in the future?"," Yep! I think not moving ignored modules to device will be the future behavior. In that case, I think that this issue can be closed."
transformer,DPP training incompatibility with checkpoint and detach," ðŸ› Describe the bug I am using pytorch ddp to train my model. Turns out if I use ddp, then I can not use checkpoint or detach gradient. The incompatibility is a big problem, because these techniques are important for my use. My model consists of two part roughly, a language model for generate representation,  where weights are detached, another part of the model is trained  with gradients.  the code of the language model:  Note in the other part of my model, there are recycles with detach.  When using ddp, I have to turn on the `find_unused_parameters=True `, otherwise a error would be raised: `RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. ` Seems like if you have a model with detached params, you have to turn on this. Here comes the problem, if I keep `find_unused_parameters=True ` and enable checkpoint, an error would be raised because a variable is marked twice. I conjecture that during forward, those detached parameters are marked as ready because of  `find_unused_parameters=True `, and somehow they are marked ready again and causes this error. I am wondering in what cases  a param would be m",2022-08-09T15:15:27Z,oncall: distributed triaged module: ddp,open,1,2,https://github.com/pytorch/pytorch/issues/83074,"Thanks for filing this issue! Would it be possible to get a minimal repro (DDP unused params + module + checkpoint + detach) that raises the error that you're seeing? Assuming that you're using `torch.utils.checkpoint.checkpoint` for checkpointing, you could consider passing `use_reentrant=False` flag into checkpoint function: https://github.com/pytorch/pytorch/blob/a58876ace78df1cfeb136cad592487f34d7e02f1/torch/utils/checkpoint.pyL164. Please note that to use this you should use a nightly build of pytorch as there are some recently landed bugfixes in the nonreentrant checkpoint code.  With the reentrant checkpointing implementation, DDP with unused parameters should work for checkpointing and we have unittests here: https://github.com/pytorch/pytorch/blob/a58876ace78df1cfeb136cad592487f34d7e02f1/test/distributed/test_c10d_common.pyL566, but would be valuable to get a repro of your use case to see if we missed any gaps.",Thanks. Would you like to explain the reason here? why this incompatibility occurs? How this argument help solve the error?
agent,Can't pickle local object 'CDLL.__init__.<locals>._FuncPtr',"I am using `torch.multiprocessing.Process` in a reinforcement learning project.  Here's bit of my codes:  Here's the errors I got:  I wonder how to load cdll properly in multiprocessing, any solutions? ",2022-08-09T12:07:41Z,module: multiprocessing triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/83065,"`CDLL` is not pickable, and cannot be passed between processes. Avoid loading dll inside of the class init, do it lazily on demand."
transformer,'Wav2Vec2ForCTC' object has no attribute 'conv'," ðŸ› Describe the bug hi there. i run my code on Colab. i want to statically quantize my Wav2Vec model. before that i try dynamic quantization but it was not useful because i didn't speed up inference time ,unfortunetly got slower than regular model. but i got error: `'Wav2Vec2ForCTC' object has no attribute 'conv'` here is my code:  Quantize snippet:  and stacktrace:      Versions  ",2022-08-08T20:17:03Z,oncall: quantization triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/83020, Is this still an issue in the latest master? Can you try using FX graph mode quantization to see if this is also an issue there? We don't maintain this model so unless it's specifically a quantization bug I don't think we can fix this from our side.,Closing this for now. Please feel free to reopen if this is still an issue for you
transformer,[FSDP] TypeError: load_state_dict() got an unexpected keyword argument 'strict'," ðŸ› Describe the bug When loading trained FSDP model, the below error is observed. The expected behaviour should be to have `load_state_dict` consistent with the PyTorch API. A similar issue is raised here https://github.com/huggingface/transformers/issues/18511 when using FSDP integration of transformers.   Versions Collecting environment information... PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0122genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration: GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX Nvidia driver version: 510.73.08 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNP",2022-08-08T12:46:53Z,high priority triage review oncall: distributed triaged module: fsdp,closed,0,5,https://github.com/pytorch/pytorch/issues/82963,cc:  varma ,"This seems like a bug in FSDP where we don't propagate kwargs: https://github.com/pytorch/pytorch/blob/a58876ace78df1cfeb136cad592487f34d7e02f1/torch/distributed/fsdp/fully_sharded_data_parallel.pyL2316, so we should be able to have a quick fix from our side. Although, one workaround could be to pass in strict as an arg instead of kwarg:  which is not great for readability, but would be good if you can try that and see if it works around the issue.","Hello, Thank you for the reply and suggestion for bypassing the issue, as per https://github.com/huggingface/transformers/issues/18511issuecomment1211512941, the above workaround works. As a longterm thing, it would be great to have the FSDP function match the API of PyTorch for better interoperability.",Fixed https://github.com/pytorch/pytorch/pull/83309,I'm still seeing this error in torch 2
rag,"Slice operation on ""ragged"" dimension in NestedTensor"," ðŸš€ The feature, motivation and pitch  Motivation In preproc we often wants to operates over variablewidth list, such as token ids in text domain, or sparse features in recommendation domain; one common operation is to slice over each list (e.g. only need first k elements). One way is to use Arrow's List type:   I was thinking nested tensor may also work well for this use case (especially when doing preproc after Tensor collate). But looks like slice is not yet supported on ragged dimension?  Wondering if there is any plan to support this? Thanks!  Alternatives _No response_  Additional context Variable width data is often modelled as the flattened value and the offset tensor. For the above (simplified 1D) case, one way is to model it as the following internal representation (which is the Arrow Layout, other layout variations exist, such as use the `lengths`):  So we kind wants to to a ""batched slice"" over `values` over the ranges `(0, 3), (4, 7), (9, 11)`. The ranges is kind of like `offsets, offsets + 3` (needs to capped by the end of each list. General nD Tensor slice support is more complicated, but the similar idea may still work?  The request o",2022-08-06T04:56:02Z,triaged enhancement module: nestedtensor,open,0,1,https://github.com/pytorch/pytorch/issues/82926, 
transformer,Adding a warning of non-compatibility with forward hooks for the fast path of TransformerEncoderLayer," ðŸ“š The doc issue In TransformerEncoderLayer, it would be helpful if it explicitly points out that naive hooks tend not to work under `fast path`, see this discussion.  The reason for such a notification is that **attention maps** are generally plotted when using Transformers, and hooking is debatably the most direct way to get attention weights.  Suggest a potential alternative/fix Add a warning notifying users that forward hooks might not be compatible with the fast path of `nn.TransformerEncoderLayer`. ",2022-08-05T23:01:59Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/82919
yi,Introducing DependencyViewer for querying Node's depedency relationships,  CC(Introducing DependencyViewer for querying Node's depedency relationships) This PR refactor the dependency map builder from the CapabilityBasedPartitioner into a standalone component.  ,2022-08-05T20:28:15Z,cla signed Stale release notes: fx fx,closed,0,3,https://github.com/pytorch/pytorch/issues/82906,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82906**  * :x: Python docsfailed to build  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit f58aff6b95 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7922882980?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/1) **Step:** ""Unknown"" (full log | diagnosis details)   20220819T23:16:28.3252550Z [error]The operation was canceled.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
agent,LayerNorm+CUDA+JIT," ðŸ› Describe the bug File In an RL workflow, when I use CUDA, JIT, and a LayerNorm together in this script, I get the following error  Following the advice above (manager.cpp for debugging, disable NVFUSER), I get  This does not occur if: 1. I run on CPU 2. I uncomment any of the print statements inside the LNGRU 3. I turn off the first layer norms 4. I assert that both i_n and i_h have a shape (any shape that tests True) 5. I attempt to reproduce with JUST the LNGRU or even the Agent (I tried making a minimal working example with both but could not reproduce the bug). See below:   Versions PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Linux Mint 20.3 (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.9.13  (main, May 27 2022, 16:56:21)  [GCC 10.3.0] (64bit runtime) Python platform: Linux5.4.0122genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.64 GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Nvidia driver version: 515.43.04 cuDN",2022-08-05T16:26:12Z,oncall: jit module: nvfuser,closed,1,13,https://github.com/pytorch/pytorch/issues/82889, does the same error occur with LayerNorm+CUDA but without JIT?,"No, only the combination of all 3, plus whatever's going on in the gist I linked. Sorry I couldn't refine it down to a smaller reproduction script :(. It occurs on the **2nd** step through the network", ,It's probably both TS and nvfuser here. Looks like even fallback path causes failures in layer here. I'll take a better look,Am I supposed to see the exception thrown within the try block?    Doesn't seem to give me any error. I tried this on our nightlies as well as on 1.12 pytorch container. `pytorch/pytorch:1.12.0cuda11.3cudnn8runtime`.,"To be clear, that code block was me attempting to create a more minimal working example and failing! I put the print statements in prematurely.  I can only provoke the failure using the file linked in the original post or variants like it. Regardless of the training loop, it occurs on the second step through the network, before any actual training has been done, and only when CUDA and JIT are enabled, with no print statements inside the LNGRU",Got it. Have the repro locally. Something went wrong in our fusion pass.. I'm working on it.,I think there's something funny with ConstantChunk here.... We are definitely abusing that and should not have push it for nonpw operations...., so does the issue go away when we turn off nvfuser?,>  so does the issue go away when we turn off nvfuser? It does if I run with `PYTORCH_JIT_USE_NNC_NOT_NVFUSER=1 python repro_82899.py cuda True` Does NNC default not fusing normalizations? Is NNC using the same ConstantChunk optimization?,"ConstantChunk is inserted as part of CanonicalizeOps, which I think will happen either way: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/profiling_graph_executor_impl.cppL340 Is that the optimization you're talking about or is there another detail ConstantChunk that you're referring to?","I was referring to this https://github.com/pytorch/pytorch/blob/aad3b8e4d3b00f9bd95bc52fd5242dd0f1c43557/torch/csrc/jit/passes/tensorexpr_fuser.cppL485L503 nvfuser doesn't fuse ConstantChunk, instead we rely on this old optimization pass that we stole from legacy fuser: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/graph_fuser.cppL875 I patched it in CC(Limits constant chunk propagation for pwnodeonly), which seems to run the original script without issue. I'll try to reverseengineer a python test for it after local test passes.","FYI, a simpler repro:  I'll put this in a python test."
transformer,Fix profiling with record_shapes=True and nested tensor,"We probably want more systematic fix for other tensorImpls that don't support sizes (and also there are other places in profiler that call sizes unchecked, which should be fixed), but this is to unblock Transformer profiling. ",2022-08-04T23:30:09Z,Merged cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/82854,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82854**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (1 Pending) As of commit 4ccd3b8f94 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Is the goal to actually get shapes out, or just to not break? Because if it's the latter I think we should just change  to  The reason is that we're looking at better support in https://github.com/pytorch/pytorch/pull/81824 (currently being refactored to use the underlying data for better overhead) and then we could handle `SizesStridesPolicy` in a fully robust way. WDYT?","Yeah the goal is to just not break, looking forward to proper support. ","Unit test passes as it should (nested tensors call regular mm underneath, and regular mm has correct sizes), and I'd rather not rip it out because otherwise CC([TorchTidy] Refactor profiler to use SizesAndStrides) will break nested tensor profiling again. ","> Unit test passes as it should (nested tensors call regular mm underneath, and regular mm has correct sizes), and I'd rather not rip it out because otherwise CC([TorchTidy] Refactor profiler to use SizesAndStrides) will break nested tensor profiling again. Gotcha. Thanks for the explanation.", merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,TransformerEncoderLayer fast path errors under `torch.autocast`," ðŸ› Describe the bug Pytorch errors when the fast path is taken in `TransformerEncoderLayer` under an `autocast` context. Minimal reproducer below.  Fails with error:    Versions Note: I checked that the error happens on colab for both 1.12 release and current 1.13 nightly. Collecting environment information... PyTorch version: 1.13.0.dev20220803+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.5 Libc version: glibc2.26 Python version: 3.7.13 (default, Apr 24 2022, 01:04:09)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.188+x86_64withUbuntu18.04bionic Is CUDA available: True CUDA runtime version: 11.1.105 GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 460.32.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.5 /usr/lib/x86_64linuxgnu/lib",2022-08-04T00:06:46Z,,closed,2,2,https://github.com/pytorch/pytorch/issues/82783, ,Closing as a duplicate of  CC(Runtime Error raised by `torch._native_multi_head_attention` working with `torch.cuda.amp.autocast`). Fix in progress https://github.com/pytorch/pytorch/pull/84722.
rag,[NestedTensor]Remove tensor buffer replace with Storage type, Description In order to enable NestedTensor views NestedTensorImpl no longer stores its data in a at::Tensor buffer_ instead it conforms to the practice of most TensorImpls and uses a Storage class. This change will enable NestedTensor to use the view constructor defined on the base TensorImpl.  Issue CC(Swap Nested Tensor buffer_ with a buffer_ of type Storage.)  Testing  The existing nested_tensor tests are utilized since this is core functionality and would break these tests if not successful.  Performance One change that has potentially large performance impact is that most nested_tensor kernels call `get_buffer` to get the buffer in Tensor form and perform ops on this buffer. Previously this was free since we stored the data as a Tensor but now each kernel must construct a Tensor from the storage. The most performance critical/heavy user of nested tensors is BetterTransformer. I would be curious to see if this change significantly impacts performance for this and other workloads.,2022-08-03T21:32:28Z,module: nestedtensor Merged cla signed ciflow/trunk release notes: nested tensor,closed,0,5,https://github.com/pytorch/pytorch/issues/82757,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82757**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (8 Pending) As of commit b493a2dbb1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," Benchmarking Linear with a an nt of nested size [[1,1]] and a weight of size [1,1] in order to profile new overhead of get_buffer()  ", merge l, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Add Mask Identifier for Better Transformer Fast-Path,Differential Revision: D38398590,2022-08-03T19:13:09Z,fb-exported cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/82747,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82747**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures, 5 Pending As of commit 96d6dca58f (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7661996109?check_suite_focus=true) pull / linuxbioniccuda11.6py3.10gcc7 / test (default, 2, 4, linux.4xlarge.nvidia.gpu) (1/1) **Step:** ""Test"" (full log | diagnosis details)   20220803T23:36:23.2734647Z FAIL [0.026s]: tes...tal_tree_cuda_detailed (__main__.TestProfilerTree)     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D38398590,This pull request was **exported** from Phabricator. Differential Revision: D38398590,This pull request was **exported** from Phabricator. Differential Revision: D38398590
transformer,[Apple Silicon M1 MPS device] bad performance metrics for BERT model training," ðŸ› Describe the bug When using `mps` device, BERT finetuning on MRPC task leads to bad performance metrics in comparison to CPU training. Also, speedup is only ~30% when compared to CPU training.  Steps to reproduce 1. Code is given below. The is run on a Mac Pro with M1 chip having 8 CPU performance cores (+2 efficiency cores), 14 GPU cores and 16GB of unified memory.  2. The output logs:   Expected behaviour: Same performance metrics when using `mps` device in comparison to `cpu` training.  Versions Collecting environment information... PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.5 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.10.5 (v3.10.5:f377153967, Jun  6 2022, 12:36:10) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS12.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XN",2022-08-03T08:19:42Z,triaged module: correctness (silent) module: mps,closed,0,17,https://github.com/pytorch/pytorch/issues/82707,"This is related to  CC(MPS device appears much slower than CPU on M1 Mac Pro). I suspect it's because of overhead of using `MPSGraph` for everything. On the Apple M1 Max, there is:  10 Âµs overhead to create a new `MTLCommandBuffer` for each op  15 Âµs overhead to encode the `MPSGraph` for each op, if it's already compiled into an `MPSGraphExecutable`. This doesn't change even if you put multiple ops into the `MPSGraph`; overhead just becomes (15 Âµs) x (ops)  100 Âµs (I need to recreate the benchmark that proved this) overhead to encode an `MPSGraph` if it isn't already compiled into an `MPSGraphExecutable` That means 25  110 Âµs of overhead for a single operation, like the `negate` operator. Multiply that by 100s of thousands, and you get many, many milliseconds of extra execution time. This overhead can be avoided and brought down to 0.5 Âµs (encode time) / 3.0 Âµs (GPU sequential throughput), but you must use custom Metal shaders for trivial ops instead of a separate `MPSGraph` instance for each op. On the CPU, everything is instantaneous. Overhead of calling into something might be 25  110 _nanoseconds_, not 25  110 _microseconds_. RNNs like BERT have smaller tensors and more sequential in nature, so they often run better on CPUs  which have smaller vector width and higher clock speed. Perhaps this would not be true if a GPU had overhead of  For a longer explanation, you might be interested in this PyTorch Forums post. Since making the post, I have not only fused multiple unary operations into a single shader dispatch  I also fused binary/ternary operations and created a fullfledged JIT graph compiler. Unlike XLA, this has such low overhead that it takes *less* time to compile than to execute once. It can recompile the same set of ops countless times, much like how OoOE happens on a modern CPU. But it's limited to elementwise ops, which are the most bandwidthintensive kind of op. Matrix multiplications happen outside of the graph. TL;DR  I hypothesize that BERT could run 10100 times faster on the M1 GPU if certain optimizations are made.",Speedup is important but I am more concerned about performance metrics `accuracy` and `F1` scores. Performance drop of 17% in `Accuracy` and 9% in `F1` score compared to CPU training when there are no code changes expect for using `mps` device.,"For more context, inference of a trained/finetuned transformer model seems to give expected performance metrics. Hence, training is going haywire while using `mps` device.","Is it possible to unravel the BERT model's layers into a sequence of primitive operations? Then, perhaps I could replicate the results from scratch using the `MPSGraph` framework. I'm using an M1family Mac as well, so numerical precision should be exactly the same. I have noticed that CPU and GPU return slightly different values for activation functions. They're the same within a factor of (1 + 1e5), but that means transcendental functionals may behave differently. In the Metal Standard Library, there are often two variants of complex math functions. One is in the `fast::` namespace, and it's fast and less precise. The other is in the `precise::` namespace, and it's slower but more precise. I suspect that `MPSGraph` uses lowerprecision versions of complex math functions, which don't handle infinities and denormals.", could you please try latest pytorch nightly (such as 1.13.0.dev20220805) and let me know if you are still seeing the issue? (`pip3 install forcereinstall pre torch extraindexurl https://download.pytorch.org/whl/nightly/cpu` will bring the latest nightly version),"Thank you  for the detailed insights into what might be happening behind the scenes.  Thank you  for the suggestion to test out `mps` support using PyTorch nightly. Observations: 1. With PyTorch nightly, the performance is similar (same for the first 2 decimal points) (0.3% F1 drop and 0.6% Accuracy drop) as seen below.  **Therefore, model correctness/performance metrics seem to be resolved.** 2.  We can also observe **~60%** speedup compared to the ~30% speedup from the torch 1.12.0 version. However, this is nowhere near the 10X eval speedup for Bert mentioned in the blogÂ Introducing Accelerated PyTorch Training on Mac | PyTorch. To provide more context, I observed a speedup ofÂ ~7.5X over CPU for the task of image classification using ResNet which is in line with the blog post. **Therefore, now the pressing question is of performance/speedup for transformerbased models.** ","> **Therefore, now the pressing question is of performance/speedup for transformerbased models.** I've been waiting for this chance for a very long time. I wanted a realworld use case (specifically an RNN) to showcase measurable speedups from reducing driverside overhead. If you could humor me, I'd like to research the performance regression by benchmarking PyTorch against the rebooted S4TF. Please let me know whether you're interested in helping with this investigation.  Stage 1 My custom backend isn't yet in the state where it can test this BERT model. There's still quite a bit of work and debugging to complete. However, lowering it down to `MPSGraph` is a good start. This is something called narrowing a bug  eliding as much context as possible, to remove confounding variables and examine only the root cause. Despite creating ML frameworks, I'm not an expert at using ML frameworks. I have never used S4TF, PyTorch, or TensorFlow to train an actual model, except when running a precreated tutorial notebook on SwiftColab. I care more about making the software accessible to other people, who want to do training. So I may need help understanding or extracting the model's size parameters, sequence of raw ops, etc.   Stage 2 Investigate ways to run the model faster than one `MPSGraph` per op. Try fusing hotpaths or creating custom Metal shaders, just for demonstration. These emulate the final product of my prototype Metal backend, and may signal whether my 10100x speedup hypothesis is reasonable. Then, see whether such optimizations are possible in PyTorch. Odds are, they aren't. There's something unique about Swift, how it handles ARC and does automatic differentiation in the language compiler. There's also the high maintenance cost of my highly tuned command stream + memory management. It requires writing _lots_ of lowlevel code in a safe, productive programming language. It's counterproductive to translate it into C++, or worse, ObjectiveC (Swift's predecessor). Still, this investigation could reveal limitations of PyTorch's MPS backend, including _reproducible_ evidence of why MPS is sometimes slower than the CPU. ","> > **Therefore, now the pressing question is of performance/speedup for transformerbased models.** >  > I've been waiting for this chance for a very long time.  Apologies if you all know and have digested this a while back but related to speedup of Transformers and RNNs on Apple devices, specifically ANE, there are lessons and code to be learned in two links below. Obviously that is yetanotherbackend compared to CPU and MPS+GPU. The data layout discussion might be relevant for MPS speedup while the 4D format for convolutions seems very ANE specific. Just some additional background info for Transformer speedup on devices. https://github.com/apple/mlanetransformers https://machinelearning.apple.com/research/neuralenginetransformers","> don't handle infinities and denormals I am getting **infinite loss** in my project when enabling **MPS**, which never occurs in either CPU or CUDA. Can this be related to what  describes? It is a bit hard to isolate the issue, unfortunately. ","The project I'm making always uses the `precise::` namespace. This isn't currently possible in PyTorch, which uses `MPSGraph` for activations. The biggest bottleneck is memory bandwidth, not ALU time  clock cycles are basically free. There's no reason to avoid `precise::`.   could you show some of the source code that's causing infinite loss? If so, I might be able to trace back the culprit and show how using `precise::` would change results.","The full project is not open source, however the loss function is available here: https://github.com/alin256/multimodepredictionwithmtploss/blob/09b1ba9a7fb9e3d19528c230d63c8d89f5bb8895/mtp_loss.pyL39 From what I am getting from the testing, it actually seems that the initialization of the neural network causes the problem:  It is really weird that neither 'CPU' nor 'CUDA' versions get this error. I am wondering if the initialization of ANN is different for MPS. The full output that I get printing the mean for every line: ","I looked deeper, and something really dodgy was going on: My input tensors' means on GPU are nans  ","Could you extract the raw data of one of those tensors before computing the mean? We can start by putting it into a calculator or simple script and validating that the CPU has it right. Understanding what values trigger the `NAN` will help investigate the GPU problems. I'm hoping for the three raw data's that cause:  Use a ` ` if the data is quite large, otherwise you will flood the GitHub thread.","> 1. With PyTorch nightly, the performance is similar (same for the first 2 decimal points) (0.3% F1 drop and 0.6% Accuracy drop) as seen below.  **Therefore, model correctness/performance metrics seem to be resolved.** > 2. We can also observe **~60%** speedup compared to the ~30% speedup from the torch 1.12.0 version. However, this is nowhere near the 10X eval speedup for Bert mentioned in the blogÂ Introducing Accelerated PyTorch Training on Mac | PyTorch. To provide more context, I observed a speedup ofÂ ~7.5X over CPU for the task of image classification using ResNet which is in line with the blog post. **Therefore, now the pressing question is of performance/speedup for transformerbased models.** Thanks  for trying out the torchnightly. I will consider the Correctness issue fixed with latest torch nightly. For the performance I would like to provide more context. The numbers in the Blog post were collected on M1 Ultra machine which has 4x the number of GPU cores and 128GB of unified memory. Also the benchmark used was the `torchbench` benchmark `hf_Bert` model at the specified Batch sizes.   We are continuing to make perf improvements so stay tuned on that.  ","> > don't handle infinities and denormals >  > I am getting **infinite loss** in my project when enabling **MPS**, which never occurs in either CPU or CUDA. Can this be related to what  describes? It is a bit hard to isolate the issue, unfortunately.  , can you please file a separate issue on this and we will take a look. Thanks!",">  we were still discussing performance issues on this thread, and a potential investigation. I don't think it's correct to mark it as closed on GitHub. The bug was tracking the `accuracy` and `f1` performance metrics in the BERT network, which was addressed. The other point on Performance was raised about not matching the Blogpost which I provided more clarification. If this is not satisfactory  , feel free to reopen the issue. About discussing the performance issues on this thread, I would appreciate we create a specific issue and track it there. This issue is getting conflated with parallel threads. ","(Bumping up this old bug, let me know if there's a better place) I'm also noticing that mps produces consistently lower results than what model is trained on cpu. However, the results are much smaller than OP's original scenar, are are usually maybe .02 lower. There's an argument for this to be within the room for error, however I'm noticing this across runs extremely consistently (failed to see one where mps converges on a higher f1 score) Mps  epoch 0: {'accuracy': 0.7769607843137255, 'f1': 0.8330275229357799} epoch 1: {'accuracy': 0.8406862745098039, 'f1': 0.8873483535528597} epoch 2: {'accuracy': 0.8382352941176471, 'f1': 0.8862068965517241} time taken in seconds for training using device mps: 576.6726970672607 cpu epoch 0: {'accuracy': 0.7573529411764706, 'f1': 0.8070175438596492} epoch 1: {'accuracy': 0.8504901960784313, 'f1': 0.8924162257495591} epoch 2: {'accuracy': 0.8676470588235294, 'f1': 0.9049295774647886} time taken in seconds for training using device cpu: 1144.207025051117"
rag,Swap Nested Tensor buffer_ with a buffer_ of type Storage., Summary Currently Nested Tensors store their data on an `const at::Tensor buffer_` . This poses a problem though when constructing a view of a NestedTensor most of the view machinery is expecting a Storage storage_. This issues is to track the changes needed to update NestedTensors storage.   PRs  Issues ,2022-08-02T21:01:44Z,triaged module: nestedtensor,closed,0,0,https://github.com/pytorch/pytorch/issues/82671
yi,Use enable_tracing flag for ProxyTorchDispatchMode instead of modifying torch dispatch mode stack inner attributes, Description This PR removes fiddling with the mode stack using copies and ExitStack in favor of a simpler and more straightforward approach.  Issue https://github.com/pytorch/pytorch/pull/81764discussion_r934917799  Testing No new tests are needed.,2022-08-02T11:34:09Z,open source better-engineering Merged cla signed fx,closed,0,10,https://github.com/pytorch/pytorch/issues/82643,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82643**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (4 Pending) As of commit 9d7de52326 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2784245585, merge g, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2788006900, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[WIP] TP + FSDP Integration change,"  CC([WIP] TP + FSDP Integration change) This is PR or diff is not aiming for land now. Context: We want to have FSDP + TP 2D Parallel enabled for transformer based model. Original PR: https://github.com/pytorch/pytorch/pull/82166 Andrew's Rebased commit: https://github.com/awgu/pytorch/commit/5b7ef1a6ea2e802fb6f1276baea6a030090320f4 This PR includes the use of Distributed Tensor. Down the road, we can discuss whether we only want to enable it for distributed tensor or not. For now, I just keep the logic both for sharded tensor and distributed tensor. Will update a unit test for TP + FSDP using DT soon. Differential Revision: D38266184",2022-08-01T17:10:06Z,oncall: distributed cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/82581,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82581**  * :x: Python docsfailed to build  * :x: C++ docsfailed to build  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 15 New Failures, 4 Pending As of commit 1f1caee096 (more details on the Dr. CI page): Expand to see more  * **15/15** failures introduced in this PR   :detective: 15 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7680998044?check_suite_focus=true) pull / linuxdocs / builddocs (cpp) (1/15) **Step:** ""Unknown"" (full log  diagnosis details)   20220804T22:26:48.8672507Z NameError: name 'DeviceMesh' is not defined     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: fduwjj / name: Hugo  (6963562bf2d20aa13bdd752d98bb5e3343415e62, 1f1caee096ff66815aa1a9b93937825382c9b14c)",Not needed.
transformer,RFC: Add flag for RNN decomposition to all RNN modules,"**tl;dr** The basic proposal here is to add a flag to RNN (and subclasses like GRU or LSTM) where instead of running the RNN kernel, it will run the linear, dropout, etc. calls that create an equivalent decomposition. Without this, the monolithic rnn functions and buffers returned from the _cudnn_rnn function make it difficult to extend RNNs in the cases of extending RNNs, computing per sample gradients, and AOTAutograd. The proposed API adds a flag to the RNN class that determines whether or not to use the decomposed version, which will be defaulted to False in order to not incur perf penalties  Problem The basic problem with the RNN kernel in particular is that the cuda versions pass around buffers that are used during the backward computation. This is particularly problematic when someone wants to use a custom derivative for the RNN since CUDA doesn't have any stability guarantees for what is being passed back in the buffers. Therefore, a developer cannot even try to recompute the intermediate values and pass those to CUDA's RNN backwards function and hope to produce correct results.  Use Cases  RNN Experimentation For a long time (even since iss",2022-08-01T16:31:07Z,feature module: rnn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/82577,"> Expanded Weights and per sample gradients The problem also extends to things like functorch transforms, right? We can't write an efficient batching rule for cudnn_rnn_forward, because who knows what it is saving into the buffers it gets passed. > AOTAutograd The AOTAutograd example is interesting. There's a question of if the user should pass `use_decomposition=True` before using AOTAutograd. If we want ""dynamo on by default"", then the user shouldn't need to change their code. This means that somewhere, AOTAutograd needs to intercept cudnn_rnn_forward and decompose it and we would want to ensure that the cudnn_rnn_backward call doesn't make it to the AOTAutograd graph (because that is not something we know how to decompose).","> This means that somewhere, AOTAutograd needs to intercept cudnn_rnn_forward and decompose it and we would want to ensure that the cudnn_rnn_backward call doesn't make it to the AOTAutograd graph (because that is not something we know how to decompose). Is this not possible?  It does seem ideal if users don't have to change their code, like with our other decompositions.","> > This means that somewhere, AOTAutograd needs to intercept cudnn_rnn_forward and decompose it and we would want to ensure that the cudnn_rnn_backward call doesn't make it to the AOTAutograd graph (because that is not something we know how to decompose). >  > Is this not possible? It does seem ideal if users don't have to change their code, like with our other decompositions. We can't really do this with torch_dispatch mode today (because that happens after autograd), but we could do it with a torch_function mode, or figure out how to get a torch_dispatch mode to occur before autograd.",.Module side
transformer,Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment, ðŸ› Describe the bug Hi there. i run my code on Colab. i want to qunatize my **Wav2Vec** model but i got error. Code:  The **ERROR**:  `Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment`  Versions  ,2022-08-01T15:05:21Z,oncall: quantization triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/82570,i solved it buy adding `inplace=True` but another problem showed up is that the average execution time of quantized model is more than nonquantized!,Let's close this for now since the original problem was solved.  if the execution time thing is still a problem for you please open a new issue.
rag,MacOS MPS: src_total_size >= storage_byte_offset," ðŸ› Describe the bug When converting a result from MPS backend to CPU, I get this error: src_total_size >= storage_byte_offset INTERNAL ASSERT FAILED at ""/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Copy.mm"":129, please report a bug to PyTorch.  This is the code that fails:  This, as a workaround, appears to not fail (note: the tensor is already a float):  It says to report it so I have put it here.  Versions Nightly pip for 1st August ",2022-08-01T13:01:55Z,triaged module: mps,closed,0,2,https://github.com/pytorch/pytorch/issues/82566,Thanks  for reporting the issue. Can you please provide a small testcase to reproduce this INTERNAL_ASSERT ?,The code above is what triggers the assert. Typically this code works for all other libtorch backends. This 'o' is a tensor that is from the result of a model that used MPS. You could also just create a tensor and move it to MPS. I haven't been able to reproduce in python.
rag,Saved tensor hooks checkpoint implementation cannot robustly clear storage," ðŸ› Describe the bug In the saved tensor hooks based checkpointing approach (https://github.com/pytorch/pytorch/blob/386b39831745d9b87f24481b7c919816a80686cb/torch/utils/checkpoint.pyL349),  when autograd needs to unpack an activation, it potentially reruns the forward to recompute all activations, and then returns the activation for the index it is unpacking. However, we currently do a `storage.pop()` for this to ensure we don't hold references to the tensor after the backward is over. This raises the issue that if the same tensor is unpacked twice, without a pack in between, we'll run into an error. A (silly) example repro is here: https://github.com/pytorch/pytorch/blob/386b39831745d9b87f24481b7c919816a80686cb/test/test_autograd.pyL4598 Another concern is if a tensor is packed by autograd but never unpacked, thus leading to `storage` leaking. Although, we are unsure if this can occur in practice.  Versions main ",2022-07-29T16:24:36Z,module: checkpoint module: autograd triaged needs design,closed,0,1,https://github.com/pytorch/pytorch/issues/82482, 
transformer,[FSDP] caffe2 error in forward method when using fsdp," ðŸ› Describe the bug When using FSDP, during inference/evaluation using transformers (gpt2, blenderbot, t5 ...) for generation, i.e., `model.generate()`, caffe2 error is thrown.  Steps to reproduce the error: 1. Code is here: run_seq2seq_no_trainer.py 2. Using ðŸ¤— Accelerate's FSDP integration with config.yaml being below.  Using 2 Nvidia Titan RTX GPUs.   3. Running below launch command:  4. Output with the error and trace stack:  5. The error disappears and everything works if I run `model(**dummy_batch)` before prediction loop. The concerned snippet in run_seq2seq_no_trainer.py is shown below:  On uncommenting the corresponding lines, everything works and below is the output: !Screenshot 20220729 at 2 50 40 PM  Expected Output No caffe2 allocation error is thrown.  Versions Collecting environment information... PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0]",2022-07-29T09:24:57Z,high priority triage review oncall: distributed triaged module: fsdp,closed,0,11,https://github.com/pytorch/pytorch/issues/82461,Similar issue faced by another user: https://github.com/huggingface/accelerate/issues/570,", hello,  just pinging you here in case you have the idea or have experienced this issue before.", ,"Hi    sorry I didn't see this earlier.  To your issue  we've run with T5 and validation loops, with many T5 variations, and did not hit this issue. However, in reviewing the above it looks like your code is going through some additional HF code before actually passing the inputs to the model for validation, vs while we are using HF T5 model, but then doing everything else in pure PyTorch. To pin this down a bit more:                                                       1  Can you confirm you see this with T5 in your setup above (original text states it happens with all transformers including T5, etc), but most of the specifics seem to revolve around blenderbot.   Would be a fast test to swap in T5 and see if it shows the same just to make sure if we can reference working T5 code for this issue. 2  The issue arises immediately upon the first validation minibatch correct?  At which point you push in a dummy batch which apparently allocates some embedding related memory for the input, and then things are fine after that.  I can setup and run blenderbot directly if that would help isolate (vs t5). Another fast test of this would be to modify your code to work directly with the sharded model in the validation loop, rather than the HF generate function: ala: ~~~ with torch.no_grad():         for batch in test_loader:             for key in batch.keys():                 batch[key] = batch[key].to(local_rank)             output = model(                 input_ids=batch[""source_ids""],                 attention_mask=batch[""source_mask""],                 labels=batch[""target_ids""],             )             ddp_loss[0] += output[""loss""].item()   sum up batch loss             ddp_loss[1] += len(batch) ~~~ You can review the code for T5 here if that helps to compare: https://github.com/lessw2020/t5_11 If that works then you can work backwards to isolate the issue in the HF utility code. If it's specific to blenderbot, then please let me know and I can setup blenderbot here in place of T5.  Thanks for bringing up the issue and look forward to getting it pinned down. ",I had the same problem.,"I think this issue is because FSDP shards the model params, so after you wrap: FSDP(model) model will have sharded parameters. In the inference path, it appears that unwrapped_model.generate calls an `encoder` directly, which is not an individually wrapped unit, and we call the forward pass:   we can tell encoder is not individually wrapped in FSDP because it doesn't appear to go through FSDP codepath.  I think to help confirm this, can you confirm `model` is FSDP instance and `encoder` is not, and also print out the FSDP wrapping of model? I think the reason `model(batch)` works might be because encoder's params are bubbled up to the top, and the root unit does not reshard after forward, so this sort of works due to an implementation detail.", any update from your side? Would it be possible to get a print out of the FSDP wrapped model? ,">  any update from your side? Would it be possible to get a print out of the FSDP wrapped model? Hi, I have encountered the same problem, and I have tested your comments and what you said is correct. `model` is a FSDP instance while `encoder` is not. So is there any solutions? Thank you very much!","We have no plans to support this at the moment, the workaround is to ensure the desired submodule is wrapped in its own FSDP unit. Then as a result, the submodule's forward pass will trigger the appropriate parameter rebuild.  Added https://github.com/pytorch/pytorch/pull/86343 to clarify this in the docs.",">  ðŸ› Describe the bug > When using FSDP, during inference/evaluation using transformers (gpt2, blenderbot, t5 ...) for generation, i.e., `model.generate()`, caffe2 error is thrown. >  >  Steps to reproduce the error: > 1. Code is here: run_seq2seq_no_trainer.py > 2. Using ðŸ¤— Accelerate's FSDP integration with config.yaml being below.  Using 2 Nvidia Titan RTX GPUs. >  >  >  > 3. Running below launch command: >  >  >  > 4. Output with the error and trace stack: >  >  >  > 5. The error disappears and everything works if I run `model(**dummy_batch)` before prediction loop. The concerned snippet in run_seq2seq_no_trainer.py is shown below: >  >  >  > On uncommenting the corresponding lines, everything works and below is the output: >  > !Screenshot 20220729 at 2 50 40 PM >  >  Expected Output > No caffe2 allocation error is thrown. >  >  Versions > Collecting environment information... PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A >  > OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 >  > Python version: 3.8.10 (default, Jun 22 2022, 20:18:18) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0122genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration: GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX >  > Nvidia driver version: 510.73.08 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True >  > Versions of relevant libraries: [pip3] numpy==1.23.0 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] Could not collect >  > , I tried what you suggested, but weirdly enough when I run model(**inputs), it instantly reaches GPU capacity of 100%, making the whole pipeline stuck. Have you encountered such behavior?","> I think this issue is because FSDP shards the model params, so after you wrap: >  > FSDP(model) >  > model will have sharded parameters. In the inference path, it appears that unwrapped_model.generate calls an `encoder` directly, which is not an individually wrapped unit, and we call the forward pass: >  >  >  > we can tell encoder is not individually wrapped in FSDP because it doesn't appear to go through FSDP codepath. >  > I think to help confirm this, can you confirm `model` is FSDP instance and `encoder` is not, and also print out the FSDP wrapping of model? I think the reason `model(batch)` works might be because encoder's params are bubbled up to the top, and the root unit does not reshard after forward, so this sort of works due to an implementation detail. I had a similar error message inferring a nonHF LLM model trained using FSDP, and this explanation solved my problem: it turned out that I was inferring the model with a different number of GPUs from what the model was trained on. Thanks varma !"
transformer,Quantization issue in transformers," ðŸ› Describe the bug This issue happened when quantizing a simple transformer model Example  The error is   Versions PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: macOS 12.4 (x86_64) GCC version: Could not collect Clang version: 13.0.0 (clang1300.0.18.6) CMake version: version 3.21.3 Libc version: N/A Python version: 3.7.5 (default, Oct 22 2019, 10:35:10)  [Clang 10.0.1 (clang1001.0.46.4)] (64bit runtime) Python platform: Darwin21.5.0x86_64i38664bit Is CUDA available: N/A CUDA runtime version: Could not collect GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A Versions of relevant libraries: [pip3] No relevant packages [conda] blas                      1.0                         mkl   [conda] mkl                       2021.2.0           hecd8cb5_269   [conda] mklinclude               2022.0.0           hecd8cb5_105   [conda] mklservice               2.4.0            py38h9ed2024_0   [conda] mkl_fft                   1.3.0            py38h4",2022-07-28T23:46:27Z,oncall: quantization low priority triaged,open,2,19,https://github.com/pytorch/pytorch/issues/82443,"The PR that introduced this likely landed Jul 15 2022, judging from when iOS tests started to fail (it's not clear when exactly, because the test uses nightly build to generate models). This issue currently blocking enabling DynamicQuantModule in iOS simulator tests:  CC(Re-enable DynamicQuantModule in iOS simulator tests)  /pytorchdevinfra ",Another way to trigger it is to run:  in the latest nightly build.,"https://github.com/pytorch/pytorch/pull/81277 was landed on July 15, and https://github.com/pytorch/pytorch/pull/81013 on July 14","Oooh, I don't think we can quantize that module in eager mode. Once quantized, the `.weight` becomes a method for ""unpacking"" the quantized weights. I think the only way to quantize this type of layers is by using the ""fx quantization"".  ","yeah eager mode quantization does not support quantizing a linear that is used in this way (direct access to weight), is fx graph mode quantization an option here? Does this work before?",yes it works before. What is the right way? The goal is just to include certain quantization ops in the test model.,I'm wondering what changed? we didn't touch eager mode quantization code for a long time I think. Maybe one thing you can do is to add `` for https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.pyL229 not sure if it would work or not though,"Any updates on this? DynamicQuantModule in iOS simulator tests is still not running, it's better to be able to reenable it soon to prevent regressions.","I can't repro the issue actually, are you using pytorch master?","maybe not the root cause, but the crash site was touched by https://github.com/pytorch/pytorch/pull/81277 and https://github.com/pytorch/pytorch/pull/81013.  To repro just run this command on the latest nightly: python3 test/mobile/model_test/gen_test_model.py dynamic_quant_ops If it works, then we can just enable the ios test.",I've just tried rebasing https://github.com/pytorch/pytorch/pull/82439 and the issue is still there., is this the code to repro the issue?,"I see, it failed during torch.jit.script, basically after we quantize the model, linear.weight became a method instead of a value, that's why torch.jit.script failed. I really doubt that this worked before actually","> I see, it failed during torch.jit.script, basically after we quantize the model, linear.weight became a method instead of a value, that's why torch.jit.script failed. I really doubt that this worked before actually It worked before Jul 16th 2022. See this PR https://github.com/pytorch/pytorch/pull/82027 Before Jul 16th DynamicQuantModule in the test worked, after that it needed to be disabled  otherwise it fails with the error from the first post in this issue.","> > I see, it failed during torch.jit.script, basically after we quantize the model, linear.weight became a method instead of a value, that's why torch.jit.script failed. I really doubt that this worked before actually >  > It worked before Jul 16th 2022. See this PR CC(Reenable iOS simulator tests) Before Jul 16th DynamicQuantModule in the test worked, after that it needed to be disabled  otherwise it fails with the error from the first post in this issue. is it possible to trace to the PR when this PR is broken?",  Is there an update on this?,"I don't have any updates, we haven't found the root cause of this issue I think","Hi    I am experiencing this issue when trying to quantize a PyTorch Transformer instance. I need to use ONNX Runtime instead, but it comes with its own drawbacks. We'd love to stick to pure PyTorch. Is anyone looking at this issue?",Hi folks  wondering if anyone successfully was able to quantize a vanilla Transformer model or a `nn.Transformer` layer within PyTorch? I face the same issue.
rag,Rename `_Typed/_UntypedStorage` to `Typed/UntypedStorage` and update docs," Description Since the major changes for `_TypedStorage` and `_UntypedStorage` are now complete, they can be renamed to be public. `TypedStorage._untyped()` is renamed to `TypedStorage.untyped()`. Documentation for storages is improved as well.  Issue Fixes CC(Rename `_Typed/_UntypedStorage` to `Typed/UntypedStorage` and update docs)  Testing N/A",2022-07-28T22:09:52Z,module: internals open source Merged cla signed topic: docs,closed,0,4,https://github.com/pytorch/pytorch/issues/82438,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82438**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: 1 Base Failures As of commit 05d62bd8fd (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base ff5399e528 on Jul 29 from 11:15am to 12:17pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * pull / linuxbionicpy3_7clang8xla / test (xla, 1, 1, linux.2xlarge) on Jul 29 from 11:15am to 12:17pm (ff5399e528  61b21f28d8)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," merge f ""upstream failure""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Rename `_Typed/_UntypedStorage` to `Typed/UntypedStorage` and update docs,,2022-07-28T21:59:56Z,module: internals topic: docs,closed,0,0,https://github.com/pytorch/pytorch/issues/82436
transformer,"[ONNX] When using Apex FusedLayerNorm, Transformers model has static shape (batch size)."," ðŸ› Describe the bug If I use apex FusedLayerNorm to replace torch.nn.LayerNorm in transformers, the batch size of the model becomes static.  Versions Pytorch nightly",2022-07-27T17:06:58Z,module: onnx triaged onnx-triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/82330
chat,Make pytest ending output less chatty,"  CC(Make pytest ending output less chatty) If you still want this in CI, we should have a separate CI only configuration.  The current config is pretty unfriendly for local development. Signedoffby: Edward Z. Yang ",2022-07-26T21:42:45Z,Merged cla signed,closed,0,15,https://github.com/pytorch/pytorch/issues/82262,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82262**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 8b16a9db5a (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7549065995?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 1, 2, windows.4xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220727T22:30:58.7151485Z FAIL [0.016s]: test_simple_out (__main__.TestFunctionalization)     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `gh/ezyang/1285/orig` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/82262`)", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `gh/ezyang/1285/orig` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/82262`)", merge g, successfully started a merge job. Check the current status here, merge g, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2749245960," merge f ""known master breakage""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,UserWarning: operator() sees varying value in profiling," ðŸ› Describe the bug Hello, I have a problem with the following code, causing a warning when executed with `torch.jit.script`. The code seems to work ok, I just find that the warning is a bit weird. This is a minimal example to reproduce the bug.  Spefically, I get this warning:  Some insight on the bug:  The warning appears only when using `torch.jit.script`: if I comment the line in which jit compilation is called everything works fine.  It seems to be caused by the forward method of `MyModule`, if I uncomment at least one line of the `forward` method, no warning is raised.   It also has interactions with the second dimension of input tensors in the dictionary with keys ""a"", ""b"" and ""c"". If they have the same number of channes, no warning is raised. ðŸ‘€   I cannot reproduce this with previous versions of PyTorch.  Another question: is it safe to ignore this kind of warning?  Versions PyTorch version: 1.12.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc v",2022-07-25T11:01:00Z,oncall: jit,open,14,3,https://github.com/pytorch/pytorch/issues/82099,any update on this issue?,Same issue!,"Hi! After some digging it seems that torch script calls nvfuser to optimize computational graph for a particular model. It usually takes few forward passes to completely profile and then optimize the computational graph as suggested here. Therefore, it seems that before profiling is ready there is bunch of loose ends that raise this warning. After ~5 forward passes this warning should be gone as the profiling is done. So I think that this warning can be ignored"
transformer,Fix deserialization of TransformerEncoderLayer (#81832) (#81832),"Summary: When `activation` is a module, it is not saved directly in the state dictionary but instead in `_modules`. When deserialized, the old version of this code would think that activation was missing and set it to RELU. This version first reconstructions the module and then sees if activation is neither a module nor a function before setting it to RELU. Pull Request resolved: https://github.com/pytorch/pytorch/pull/81832 Approved by: https://github.com/kit1980, https://github.com/zrphercule Test Plan: contbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/e68583b4d180066b8e4f108e0d23176a2676421c Test plan from GitHub: pytorch oss tests Reviewed By: jeanschmidt, zrphercule Differential Revision: D38014872 Pulled By: zdevito fbshipitsourceid: 938079d768f7981ca55eed3c8828b29a92e06f41",2022-07-25T09:23:21Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/82094,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82094**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 74e5021fc4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
rag,[Tracking] Operator coverage for symbolic shape tracing," ðŸš€ The feature, motivation and pitch Op Coverage mostly taken from https://github.com/pytorch/pytorch/blob/master/test/test_proxy_tensor.pyL387 Decompositions Needed (i.e. we need to write a Primtorch decomposition or meta function)  [ ] `aten._to_copy.default`  [ ] `aten.mm.default`  [ ] `aten.slice.Tensor` SymInt overloads needed (i.e. we need to support calling it with SymInts as an argument)  [ ] `aten.new_empty`  [ ] `aten.reshape` (tricky since it's CompositeImplicitAutograd?) cc:    ",2022-07-23T05:57:14Z,triaged oncall: pt2 module: dynamic shapes,closed,0,0,https://github.com/pytorch/pytorch/issues/82048
rag,[vulkan] enable storage_ access in VulkanOpaqueTensorImpl to enable serialization,  CC([vulkan] enable storage_ access in VulkanOpaqueTensorImpl to enable serialization) Differential Revision: D38086377,2022-07-22T20:09:12Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/82015,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82015**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 03e4ebda15 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,Issue with FSDP memory reduction scaling up GPUs," ðŸ› Describe the bug We are running FSDP with HuggingFace T5 3B model using the transformer wrapping policy and mixed precision (BF16) with A100 gpus. Scaling up GPUs, by extending to multi nodes (each node has 8 gpus),  we are expecting to have smaller shard on each and overall proportional memory reduction. However, we see that the reserved memory for between 14 nodes does not change very much, whereas the allocated memory reduces to almost half.  Revered memory has been captured using ""torch.cuda.memory_reserved()"" and allocated memory using ""torch.cuda.memory_allocated()"".  As shown in the table below, the reserved memory on 1 node is ~ 28Gb > 2 nodes ~ 26Gb >3 nodes ~25Gb> 4 nodes ~24Gb.   This is the same pattern for TF32 and FP32 where the memory reduction by extending nodes is even less (table below).  This results in OOM when training T5, 11B model, as the memory reduction is not happening as expected. It would be great to narrow down the root cause if this is due to the transformer wrapping policy or other design choices in FSDP.  Code to repro.  Versions PyTorch Nightlies ",2022-07-22T17:49:11Z,oncall: distributed module: fsdp,closed,1,1,https://github.com/pytorch/pytorch/issues/82001,"I think active memory is the appropriate stat to track over allocated memory due to the multiple stream usage in FSDP. Nonetheless, the rate limiter should help with this issue: https://github.com/pytorch/pytorch/pull/83917 I will close this for now, and we can open a new issue if there are similar issues even with the limiter enabled. The limiter targets the high active memory and high reserved memory case and can bring those values down. However, if the workload has low active memory and high reserved memory, then it suffers from fragmentation, which the rate limiter cannot help on the first order."
yi,[resubmit][FX] Fix PyTree unpacking carrying forward type annotations,Differential Revision: D38077793,2022-07-22T17:08:15Z,fb-exported Merged cla signed Reverted fx,closed,0,19,https://github.com/pytorch/pytorch/issues/81999,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81999**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 65fdbc200a (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7505713799?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 2, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220725T18:42:49.8938560Z RuntimeError: test_nn failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D38077793,This pull request was **exported** from Phabricator. Differential Revision: D38077793, merge f flaky,This pull request was **exported** from Phabricator. Differential Revision: D38077793," merge f (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," merge f '[MINOR] Unrelated, unrepreproducible failure'", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert ," revert m  ""test_bce_with_logits_has_correct_forward_grad consistently fails with an error that it takes 2 positional arguments but 3 were given"" c ignoredsignal", successfully started a revert job. Check the current status here, your PR has been successfully reverted.," please investigate the broken test, this test started failing consistently once this PR was merged.  For the future, please note that the Dr. CI comment at the top of this PR provides hints as to which failures are likely to be caused by flakiness and which ones might be legitimate errors You can see builds failing consistently starting with this PR in http://hud.pytorch.org "," As mentioned in the commit message, the test does not reproduce locally, and that part of the code has nothing to do with this PR","Hi James, I hear you on this failure being hard to debug.  It def frustrating that failures don't always reproduce locally and thatâ€™s an area that we want to make easier for developers. That said, looking at the build failures, there was clearly *something* about that PR which caused the build break on that particular configuration. If you look at the failures, the build started failing when the PR was checked in and stopped failing when your PR was reverted.  Looking at the failure in particular, it's saying `TypeError: forward() takes 2 positional arguments but 3 were given`  I see that the PR makes changes related to the forward() method and even adds what looks kind of like a new invocation in each of the two `gen_fn_def` methods https://github.com/pytorch/pytorch/pull/81999/filesdiff4fc0f305bb3a7821b97dc183a9370957e4ce8bbb83dc547bcb51a1de8b79e448R271 Apologies for this being hard to debug, but it does seem like something caused by this PR. Let me pull up some instructions that may help you debug deeperâ€¦","You can use the `withssh` label to ssh into the failing machine, which should let you reproduce the error Instructions: https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions","[edit] Hmm,  have you try running it with torchdynamo installed/enabled?",
agent,DISABLED test_rref_as_arg_synchronization3 (__main__.TensorPipeTensorPipeAgentCudaRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 red and 1 green. ",2022-07-22T04:01:37Z,oncall: distributed module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/81962
transformer,1.12.1/bt fix,"Fixes  CC(Transformer and CPU path with `src_mask` raises error with torch 1.12). Also fixes a separate crash when enable_nested_tensor is enabled (caught by tests added in test_transformers.py).  These three PRs:  CC(disable src mask for transformer and multiheadattention fastpath): Fix CC(Transformer and CPU path with `src_mask` raises error with torch 1.12) (activation.py and transformer.py changes. primary reason for this PR)  CC(Add numerical test comparing BetterDecoder and fairseq decoder): Add tests for the fix (test_transformers.py and test_nn.py changes)  CC([PyTorch] Round T up to next multiple of 8 in NestedTensor case): Fix crash when BetterTransformer gets NestedTensor input (Pulled only the change to NestedTensorMath.cpp, not all changes to keep code minimal)",2022-07-21T23:47:21Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/81952,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81952**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b99b16f2cc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,Add mask identifier for multiplexed src_mask/src_key_padding_mask in BT,"Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/81947 Transformer fastpath multiplexes two arguments, src_mask [seq_len x seq_len] and src_key_padding_mask [batch_size x seq_len], and later deduces the type based on mask shape. In the event that batch_size == seq_len, any src_mask is wrongly interpreted as a src_key padding_mask. This is fixed by requiring a mask_type identifier be supplied whenever batch_size == seq_len. Additionally, added support for src_mask in masked_softmax CPU path. Test Plan: existing unit tests + new unit tests (batch_size == seq_len) Differential Revision: D37932240",2022-07-21T23:19:11Z,fb-exported Merged cla signed,closed,0,22,https://github.com/pytorch/pytorch/issues/81947,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81947**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 688b668a91 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240, Could you please approve the above workflows,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[ONNX] Test and address autograd.func (FusedLayerNorm) shape inference,"This PR addresses the ONNX exporter issue of wrongly inferred static shape by unreliable nodes: 1. Specifically, this unblocks the usage of apex `FusedLayerNorm` (autograd.function) in transformer. Before this PR, the downstream nodes of apex `FusedLayerNorm` are inferred with static shape even though they are unreliable (should be dynamic). 2. Add a general test case using autograd function to wrap `torch.nn.layernorm` which can repro the same issue as apex `FusedLayerNorm` did in transformersembedding layer. 3. Remove a legacy test `test_empty_like_opset7` which still uses deprecated ConstantFill op. As this node is not supported by onnx (checker) anymore, the output of its shape inference leading to unexpected outcome, and is exposed by this PR.  Please advise if there is a better place for the test case. Fixes CC([ONNX] When using Apex FusedLayerNorm, Transformers model has static shape (batch size).) ",2022-07-21T22:21:07Z,oncall: jit module: onnx triaged open source Merged cla signed release notes: onnx topic: bug fixes,closed,0,8,https://github.com/pytorch/pytorch/issues/81931,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81931**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit ab6431a880 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,I just saw ConstantFill being used in opset8 too. Do you know what should be the alternative?,"> I just saw ConstantFill being used in opset8 too. Do you know what should be the alternative? As far as I know, ConstantFill was treated as invalid onnx node during my testing, and providing a different result than the expect file (Not sure if it's used sort of like onnx::Constant at the time). And considering we have another `test_empty_like` with more current version, I didn't thoroughly explore the alternative way to that test. If we decide to keep that test in an alternative way, I will have to talk to onnx team to know that node more.","  folks, Pytorch will release 1.12.1 soon. Do you think we can expedite the code review to try to fit in this fix in time?",">   folks, Pytorch will release 1.12.1 soon. Do you think we can expedite the code review to try to fit in this fix in time?  We have created a release tracker for some fixes CC([ONNX] Cherry pick quantization for 1.12.1 release), CC(ONNX 1.12.1). If needed let's work together to include this fix.","> >   folks, Pytorch will release 1.12.1 soon. Do you think we can expedite the code review to try to fit in this fix in time? >  >  We have created a release tracker for some fixes CC([ONNX] Cherry pick quantization for 1.12.1 release), CC(ONNX 1.12.1). If needed let's work together to include this fix. Can  just include this PR in it after you approve it?", merge g, successfully started a merge job. Check the current status here
yi,[FX] Fix PyTree unpacking carrying forward type annotations,Stack from ghstack:  CC([FX] Fix PyTree unpacking carrying forward type annotations) Resolves  CC([FX] `concrete_args` unpacking erroneously carries over type annotations),2022-07-21T18:32:03Z,Merged cla signed Reverted fx,closed,0,12,https://github.com/pytorch/pytorch/issues/81906,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81906**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 634af35278 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7457762974?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 2, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220721T21:43:39.5162842Z RuntimeError: test_nn failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2716282268, merge f," merge f ""flaky test""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""breaking internal builds"" c ""ghfirst"" ", successfully started a revert job. Check the current status here, your PR has been successfully reverted.,"  I saw this got reverted, but we still need this fix.  how is it breaking internal builds?"
transformer,Fix deserialization of TransformerEncoderLayer,"Summary: When `activation` is a module, it is not saved directly in the state dictionary but instead in `_modules`. When deserialized, the old version of this code would think that activation was missing and set it to RELU. This version first reconstructions the module and then sees if activation is neither a module nor a function before setting it to RELU. Test Plan: pytorch oss tests Reviewed By: zrphercule Differential Revision: D38014872",2022-07-20T23:56:32Z,fb-exported Merged cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/81832,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81832**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 56e0a4c3c4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38014872,This pull request was **exported** from Phabricator. Differential Revision: D38014872, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Add mask identifier for multiplexed src_mask/src_key_padding_mask in BT,"Summary: Transformer fastpath multiplexes two arguments, src_mask [seq_len x seq_len] and src_key_padding_mask [batch_size x seq_len], and later deduces the type based on mask shape. In the event that batch_size == seq_len, any src_mask is wrongly interpreted as a src_key padding_mask. This is fixed by requiring a mask_type identifier be supplied whenever batch_size == seq_len. Additionally, added support for src_mask in masked_softmax CPU path. Test Plan: existing unit tests + new unit tests (batch_size == seq_len) Differential Revision: D37932240",2022-07-20T23:51:20Z,fb-exported cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/81830,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81830**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 11 New Failures As of commit 21373ebb2d (more details on the Dr. CI page): Expand to see more  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7460034510?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/11) **Step:** ""Test"" (full log  :repeat: rerun)   20220722T02:28:21.6464390Z RuntimeError: expected `mask_type` when `mask` supplied     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240
yi,403 when trying to get multiple pytorch components," ðŸ› Describe the bug As part of a dockerfile we're conda installing multiple components from a yml. Today we are getting these errors: CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.298984 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.073712 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.304133 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.075025 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. I tried to build the docker image multiple times.  Then, got the same 403 error just trying to access any of the components directly via a browser.  Versions N/A",2022-07-20T19:04:13Z,,closed,2,4,https://github.com/pytorch/pytorch/issues/81798,Can confirm. Created a fresh conda environment and ran `conda install pytorch torchvision torchaudio cudatoolkit=10.2 c pytorch`. Fails with the same `CondaHTTPError: HTTP 403 FORBIDDEN` errors as  mentioned.,I accidentally logged a duplicate of this bug ( CC(Pytorch fails to install on Windows with HTTP 403 errors)).  It's the same issue I'm sure. Seems similar to a past bug: CC(404 when trying to get pytorchmutex1.0cuda.tar.bz2 from Conda)  which was flagged as high priority. Is there a way to escalate the priority of this one as well?  As it's been down for hours...,Same here. ,This is a Conda issue  https://github.com/conda/conda/issues/11638 Closing this as a duplicate of  CC(pytorch conda repo broke today)
yi,[MPS] Get the correct size of the view tensor when copying from cpu to mps ,"Fixes:  CC(Macbook M1 GPU support ""mps"" results in wrong (random) conversion when casting a tensor to LongTensor type.),  CC(MPS device neural net in validation gives the same results after n-th batch element) * Get the correct size of the view tensor when copying from cpu to mps * Use 'computeStorageNbytesContiguous' to get the size just when src is a view * Add asserts and tests to check for storage_offset  * Add testcase for  CC(MPS device neural net in validation gives the same results after n-th batch element) * Replace assert_allclose with assertEqual * Replace TORCH_CHECK with TORCH_INTERNAL_ASSERT",2022-07-19T21:21:47Z,open source Merged cla signed ciflow/trunk,closed,0,4,https://github.com/pytorch/pytorch/issues/81730,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81730**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 09900ad8d0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Remove remaining `eval` calls from `torch/storage.py`,,2022-07-19T15:38:28Z,open source Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81701,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81701**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6cc61c08a2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Remove `eval` from `torch.storage._TypedStorage.__new__`,,2022-07-19T02:56:07Z,open source Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81679,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81679**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 399a4a9d3e (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Error if trying to enable_reentrant_dispatch but no TorchDispatchMode is set,"Stack from ghstack:  CC(Error if trying to enable_reentrant_dispatch but no TorchDispatchMode is set)  CC(Enable reentrant dispatch for decompositions) If you enable_reentrant_dispatch when TorchDispatchMode is not set, and then dispatch again, then torch dispatch will fail because no mode can be found. This throws a warning that should be easier to debug.",2022-07-19T01:03:50Z,cla signed Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/81677,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81677**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 7 New Failures As of commit 123d82b6a4 (more details on the Dr. CI page): Expand to see more  * **7/7** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7400947934?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 2, 2, linux.2xlarge) (1/7) **Step:** ""Test"" (full log  :repeat: rerun)   20220719T02:08:11.6299596Z RuntimeError: test_ops failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."
yi,optimize_for_mobile has an issue with constant operations at the end of a loop," ðŸ› Describe the bug I've been working on a model that I want to optimize for mobile, but when optimizing the model for mobile, I ran into an issue with an operation at the end of a loop. I'll use the code example below to show the issue:  If you run the code, you would expect the output of the torch script model to be the same as the output of the original model, but the outputs of the models are slightly off. The original traced module works perfectly fine, but the mobile script runs into an issue where the output is slightly off for all values in the output vector (excluding the first). Taking a look at the mobile torch script code, I think I see what the issue is: !image At the beginning of each loop, it applies the constant multiplication operation. The first operation is applied at _3 and _4. This operation should be applied after the loop instead of at the beginning. Another issue (which I think is the main problem) is after the multiplication operation is performed on the Y tensor (at _3 and _4), the value is never used. As seen in _5, the _1 tensor is used which is the tensor before the constant operation is applied. Instead, the forward met",2022-07-18T18:50:32Z,oncall: mobile,open,0,2,https://github.com/pytorch/pytorch/issues/81651,I was able to simplify the repro a bit:   Looks like the issue is related to the INSERT_FOLD_PREPACK_OPS pass., thanks for the simplified repro. I can take a look.
yi,Modifying the source code,"I have been trying to make some changes in the source code of PyTorch library, particularly in aten/src files. I want to modify the files on a Linux system.  Issue description I had downloaded the Pytorch Library from source code. I have made made the changes in the .cpp files, particularly, I have added a few printf() statements and then I build using `python3 setup.py develop` .  The source code gets built but on running the Pytorch code, I am unable to see the anything from the printf()'s I added. Any help would be appreciated.",2022-07-18T07:22:43Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/81627,"That should work. If those `printf`s didn't fire it's because that code didn't run. That can be because your code didn't exercise that path, or because you are not using the version of PyTorch you just compiled (you may be using one installed from pip or conda). In any case, the right place to discuss these points is the forum: https://discuss.pytorch.org/","I want to find in which C file the thread is created when I use `torch.set_num_thread()`. For this I did a grep in pytorch source code and then in all those places where there was an occurance of set_num_threads, I added a `printf()`. I built it again and still cannot have anything on the terminal from the `printf()` statements added.","Consider debugging your program for example with `gdb`. But again, the right place to discuss these points is the forums.",Can you please tell how to use gdb in source code. I have also posted on discussions forum
transformer,fix view_copy kernel striding check logic,"The composite kernel for `view_copy` that we generate is specialcased a bit for efficiency to avoid having to do extra clones in some cases. That logic was slightly wrong though, and is fixed here (it needs to mirror the logic in `reshape()`). It manifested as a debug assert firing for Lazy Tensor, which I confirmed no longer fires when running this script:  Stack from ghstack:  CC(fix view_copy kernel striding check logic)",2022-07-15T14:56:47Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81553,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81553**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 2a9c14b548 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Remove unused storage_size,,2022-07-14T22:38:17Z,oncall: distributed better-engineering Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81514,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81514**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6027b9f607 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Make C10_DEFINE_TLS_static use a template functor rather than a lambda. (For non C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE cases),"  CC(Optimize ThreadLocalDebugInfo lookup)  CC(Make C10_DEFINE_TLS_static use a template functor rather than a lambda. (For non C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE cases))  CC([Trivial] Delete C10_DECLARE_TLS_class_static and C10_DEFINE_TLS_class_static) There is a nontrivial cost to storing an accessor lambda as a pointer and calling it at each access: https://godbolt.org/z/s8zf19TsM By contrast, if we pass a trivial struct then the compiler can inline and optimize everything away. It seems that roughly 4% of the time to make an empty Tensor comes from this overhead when checking whether to profile memory in the allocator. That said, it's hard to get a precise estimate (even with multiple runs and 50+ replicates). But the benchmark does confirm that the extra instructions actually matter. Differential Revision: D37842249",2022-07-14T02:03:56Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/81452,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81452**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 17 New Failures As of commit e913b8db12 (more details on the Dr. CI page): Expand to see more  * **17/17** failures introduced in this PR   :detective: 17 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7332425282?check_suite_focus=true) pull / linuxjammycuda11.6cudnn8py3.8clang12 / build (1/17) **Step:** ""Build"" (full log  :repeat: rerun)   20220714T02:09:50.8661826Z [error]Process completed with exit code 1.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
transformer,[FSDP] `test_mp_embedding_reduce()` fails with `transformer_auto_wrap_policy()`,"`test_mp_embedding_reduce()` fails if we wrap the `TransformerWithSharedParams` with the typical `transformer_auto_wrap_policy()` with `transformer_layer_cls={TransformerEncoderLayer, TransformerDecoderLayer}`. For now, we only wrap with a single toplevel `FullyShardedDataParallel`, but we should investigate whether this failure is due to how the unit test is written or due to a gap in the mixed precision implementation. ",2022-07-13T20:41:38Z,triaged module: fsdp,open,0,0,https://github.com/pytorch/pytorch/issues/81426
agent,Implement shape/size functions for nestedtensor," ðŸš€ The feature, motivation and pitch Currently, nt.shape does not work. It just throws an error:  The shape function has been implemented, as seen in this colab, it just hasn't been migrated to torch. I do not see any downside to implementing this function, as it is easy it implement (the code is already written; it just needs to be migrated), and it's much better than throwing an error.  Alternatives _No response_  Additional context My use case: I am working with graph neural networks, which involve a variable number of agents, each of which can observe a variable number of objects in the environment. if fully functional, nestedtensor would significantly simply the problem, as it would obviate the need to store and manipulate tensors of batch indices. ",2022-07-13T14:50:48Z,triaged module: nestedtensor,open,2,6,https://github.com/pytorch/pytorch/issues/81405, ,"`my_dataset = TensorDataset(tensor_x,tensor_y)  create your dataset`  I am trying to create a dataset. `tensor_x` data used for a model that has 2 inputs. I am running a Multitask learning experiment. Running into the same error: ","  which version of PyTorch are you using? The snippet you're referencing calls into `.size(0)`, which we support for NestedTensor.",PyTorch version `1.12.1` Python version `3.9.2`, do I need to update anything? How do I solve this error?, use the latest pytorch nightly
rag,"[Profiler] Make KinetoEvent a view of Result (Part 4 (final), stragglers)","  CC([Profiler] Start moving python bindings out of autograd)  CC([Profiler] Make KinetoEvent a view of Result (Part 4 (final), stragglers))  CC([Profiler] Make KinetoEvent a view of Result (Part 3, forwarded from `result_`))  CC([Profiler] Make KinetoEvent a view of Result (Part 2, python and stacks))  CC([Profiler] Make KinetoEvent a view of Result (Part 1: trivial fields)) This PR just moves all the KinetoEvent methods which didn't fit the previous categories. Now that we no longer need to set Kineto event fields in `EventFieldsVisitor` we can remove the reference wrapper and rename the visitor to `AddKinetoMetadata` since that's all it does now. Differential Revision: D37490053",2022-07-12T14:59:33Z,Merged cla signed ciflow/trunk release notes: profiler ciflow/periodic,closed,0,10,https://github.com/pytorch/pytorch/issues/81322,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81322**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 3 New Failures As of commit 0da3ad1e5e (more details on the Dr. CI page): Expand to see more  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7717258108?check_suite_focus=true) periodic / ios1251arm64metal / build (1/3) **Step:** ""Unknown"" (full log  : ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge l, successfully started a merge job. Check the current status here,"Merge failed due to 2 additional jobs have failed, first few of them are: periodic ,periodic / buckbuildtest / buckbuildtest Raised by https://github.com/pytorch/pytorch/actions/runs/2810411058", merge l, successfully started a merge job. Check the current status here,"Merge failed due to 1 additional jobs have failed, first few of them are: periodic Raised by https://github.com/pytorch/pytorch/actions/runs/2815669061"," merge f ""I have tried to land this responsibly but infra is too flaky.""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,disable src mask for transformer and multiheadattention fastpath,Disable fastpath if src_mask passed to TransformerEncoderLayer and MultiheadAttention.   Refactored test_transformerencoder from test_nn.py to test_transformers.py. Added a src_mask test there.  Added a specific src_mask test in test_transformers.py Fixes  CC(Transformer and CPU path with `src_mask` raises error with torch 1.12),2022-07-11T21:22:52Z,Merged cla signed release notes: nn topic: bug fixes,closed,0,4,https://github.com/pytorch/pytorch/issues/81277,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81277**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 5b06d9ad70 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,fixing call_module on subscripting into generator,"named_modules() return a generator, which is not subscriptable and causes node support query to fail",2022-07-11T18:02:35Z,triaged open source Merged cla signed module: fx,closed,0,4,https://github.com/pytorch/pytorch/issues/81258,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81258**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c0f077e898 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,grad not preserved during copying or pickling," ðŸ› Describe the bug I would like to (cloud)pickle some Tensors. However, their `grad` attribute is lost during pickling. The following snippet   yields the following output:  I can work around this using a custom reduction function for `torch.Tensor` like  which yields the expected output  This is basically what Dask does. But a better place to fix this would seem to be in `Tensor.__reduce_ex__`  Versions PyTorch version: 1.12.0+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Fedora Linux 35 (Container Image) (x86_64) GCC version: (GCC) 11.2.1 20220127 (Red Hat 11.2.19) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.34 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.18.9200.fc36.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.0 [pip3] torch==1.12.0+cpu ",2022-07-10T18:28:35Z,triaged module: python frontend,open,0,2,https://github.com/pytorch/pytorch/issues/81186,Can you confirm that `copy.deepcopy()` does preserve the grad field right?  ," Yes, `copy.deepcopy()` works."
yi,[di] avoid copying optional input for get_real_inputs_from_optional_inputs_v2 when possible,Summary: avoid copies and casting by moving out of the inputs list Differential Revision: D37572556,2022-07-08T20:42:22Z,oncall: jit fb-exported Merged cla signed,closed,0,17,https://github.com/pytorch/pytorch/issues/81137,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81137**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 93077fbfdb (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556, merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Transformer and CPU path with `src_mask` raises error with torch 1.12," ðŸ› Describe the bug The following code, which runs on torch 1.11 cpu, doesn't anymore on torch 1.12:  It raises   The `.to(torch.bool)` is only here to silence the warning: `UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly.`, the original code on v1.11 doesn't use it. This also happens on an x86 ubuntu machine, when using cpu, but it does not happens when using CUDA. Because of this condition: https://github.com/pytorch/pytorch/blob/e9b3bc2eadb8ffe10c002abcd5a34a5b7d36f390/aten/src/ATen/native/transformers/attention.cppL136L144 using `src_mask = torch.zeros(32, 10)` makes the error go away, but it must not be right because I believe the size of `src_mask` should be `(seq_len, seq_len)`.  Instead, on torch 1.11, using `(32, 10)` was raising `RuntimeError: The shape of the 2D attn_mask is torch.Size([32, 10]), but should be (10, 10).`.  Also, the comment saying CPU path doesn't support mask makes me wonder if I'm looking at the right code ðŸ¤”   Versions Collecting environment information... PyTorch version: 1.13.0a0+git4c57cf9 Is debug build: False CUDA used to ",2022-07-08T19:11:15Z,high priority triage review module: regression,closed,0,6,https://github.com/pytorch/pytorch/issues/81129,This is a new transformer implementation issue. Thank you for bringing this up  we'll get a fix asap. ,"Relevant function is  https://github.com/pytorch/pytorch/blob/e9b3bc2eadb8ffe10c002abcd5a34a5b7d36f390/aten/src/ATen/native/transformers/attention.cppL118L150 Tagging . Looks like cpu path assumed src_key_padding_mask with shape (batch size, seqlen) and not src_mask with shape (seqlen, seqlen)? src_mask works fine on the cuda path since _masked_softmax takes care of it.  I think the solution is to pass a flag saying whether this is a src_mask or a src_key_padding_mask and deal with the mask accordingly. There's not an easy way to tell if the given attention_mask is one or the other if the batch size and sequence length are the same.  Edit: For now solution I think is to just disable fast path if src_mask is passed in. ","IMO, best solution is probably to have parity between the CPU and CUDA paths in masked_softmax w.r.t. ""transformer mask"" support, then we don't have to continue layering more special cases onto the specialcase fix for the gap in support.","Main PR to fix is https://github.com/pytorch/pytorch/pull/81277. I've also taken the liberty to increase our test coverage in test/test_transformers.py. For cherrypicking onto the 1.12.1 release we will also need https://github.com/pytorch/pytorch/pull/79796 because it created test_transformers.py but didn't get into the 1.12 release. : In addition to the above two PRs, we'll also need https://github.com/pytorch/pytorch/pull/81013 because my fix sits on top of that PR. ", Fix was CC(disable src mask for transformer and multiheadattention fastpath) which landed yesterday. Should work now  let me know if issues remain. ,Thanks for the quick fix !
yi,Add `scalarType` to JitType pyi,Fixes mypy errors when calling `.type().scalarType()` on an `_C.Value` object. Reference https://github.com/pytorch/pytorch/blob/28776c45e393ec7d6731a5613f5d79d309036d3a/torch/csrc/jit/python/python_ir.cppL891L896,2022-07-08T15:51:18Z,module: typing triaged open source Merged cla signed topic: not user facing,closed,0,11,https://github.com/pytorch/pytorch/issues/81112,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81112**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 01ff7db3b2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , could you help take a look? thanks, merge, successfully started a merge job. Check the current status here,Merge failed due to This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. Raised by https://github.com/pytorch/pytorch/actions/runs/2715553287, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `justinchu/jittypehint` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `git checkout justinchu/jittypehint && git pull rebase`)", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[FSDP] Rename `transformer_auto_wrap_policy()`,"Per varma's suggestion, we can rename `transformer_auto_wrap_policy()` to reflect that it works for any `layer_classes: Set[Type[nn.Module]]`, not just transformer layer classes. https://github.com/pytorch/pytorch/blob/c0e15fe3f4d5622c7a687cf0d503902be9296596/torch/distributed/fsdp/wrap.pyL81L86 https://github.com/pytorch/pytorch/blob/c0e15fe3f4d5622c7a687cf0d503902be9296596/torch/distributed/fsdp/wrap.pyL117L122  We need to be wary of any existing model code assuming this `transformer_auto_wrap_policy()` name. If breaking this backward compatibility, then we may need to deprecate `transformer_auto_wrap_policy()` (e.g. adding a warning) and add a new function with the new name that does the exact same thing. ",2022-07-07T16:17:17Z,triaged module: fsdp,closed,0,2,https://github.com/pytorch/pytorch/issues/81050,"transformer_auto_wrap_policy() possibly has already been used for multiple use cases, since we are working on nonrecursive wrapping, once it is finalized, we can revisit naming here later on?",https://github.com/pytorch/pytorch/pull/88450
yi,Add doc string for Library.impl,Stack from ghstack:  CC(Add doc string for Library.impl),2022-07-07T15:30:33Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81047,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81047**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 4a3e4eb46e (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,torch.utils.bottleneck spams output and crashes," ðŸ› Describe the bug I have a Python script that trains an NN, and am trying to optimize it using bottleneck, but I can't. The command `python m torch.utils.bottleneck TA_generator.py` runs through my code about 3 times and then fills the console with spam. It should be noted that `python TA_generator.py` runs without issue:  `pretraining_test.txt` is simply a text file of 100,000 lines. I would post a stack trace, but the console is filled with spam so quickly that it is impossible to get any Traceback. Attached is a screencap of some spam  it's the same message on repeat. !spam  Versions Collecting environment information... PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.10.2 Libc version: glibc2.27 Python version: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0180genericx86_64withglibc2.27 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: GeForce RTX 208",2022-07-07T01:30:18Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/81026,"Hi, We use github issues only for bug reports or feature requests. Please use the forum to ask questions: https://discuss.pytorch.org/","I'm sorry if this is unclear, but I don't see how this isn't a bug. My script runs fine until I try to profile it using the bottleneck module, then it spams my output. I feel that something is wrong with bottleneck if this is the case.","themememan  The issue referenced in the output is  CC([utils.bottleneck] Bottleneck crashes with multi-threaded data loader) . The message suggests to not use this feature in conjunction with DataLoader and `num_workers > 0`. It's not spam output, but useful information that references an existing issue, which is therefore not a bug."
transformer,[Bootcamp T124004534] Better Transformer fastpath diagnostics,Summary: update the boolean logic for TransformerEncoder => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/transformer.py?lines=206212%2C228230 and TransformerEncoderLayer => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/transformer.py?lines=410417%2C432436 according to the decision logic => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/activation.py?lines=10591091%2C11031109 Differential Revision: D37666451,2022-07-06T22:54:53Z,fb-exported Merged cla signed,closed,0,16,https://github.com/pytorch/pytorch/issues/81013,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81013**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 0e8900f836 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451, merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[ONNX] Fix bug using std::copy_if,"  CC([ONNX] Fix bug using std::copy_if) `std::copy_if` requires preallocated destination. Hence `scopes` needs to either be pre allocated, or use `std::inserter` for copy. Ref: https://en.cppreference.com/w/cpp/algorithm/copy",2022-07-06T20:55:34Z,oncall: jit module: onnx open source Merged cla signed release notes: onnx topic: bug fixes onnx-needs-import,closed,0,8,https://github.com/pytorch/pytorch/issues/80999,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80999**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c20d981e9c (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Curious what the bug was? (Helpful to include in the PR description),, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `gh/BowenBao/152/orig` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/80999`)", merge g, successfully started a merge job. Check the current status here
yi,[bug][nvfuser] Applying nvfuser to the model leads to runtime error," ðŸ› Describe the bug  I tried to enable nvfuser when training my model but got the above runtime error. I also tried running my code without scripting the model and everything goes fine. It seems that the error is caused by invoking the `torch.cat` operator with a single tensor. However, after checking the source code, I can verify that each `torch.cat` operator is invoked with a list of tensors. Therefore, I am not sure what is causing this issue. Any help is appreciated. Thanks.  Versions  FYI, ",2022-07-04T22:00:51Z,triaged module: nvfuser,open,0,23,https://github.com/pytorch/pytorch/issues/80851,Can you please provide a script reproducing the error?  ,"> Can you please provide a script reproducing the error? >  >  , Thank you so much for checking on this issue! Since we are working on a closesourced repo, would you (or anyone else who would be helping to debug this) be open to being added into this private repo? Alternatively we could prepare a code snippet that reproduce this error, however, that might take some time (this error message arises during 'torch.autograd.grad' and we are not yet certain which layers/ops led to this bug). Thanks!","I'm taking a wild guess that there's something with the concat/chunk optimization pass inside fusion. Would you mind running your script with debug dump? `PYTORCH_JIT_LOG_LEVEL=graph_fuser python your_script.py &> log`, We should be able to see the last fusion graph and figure out a repro from there. FYI, there's a `debug nvfuser` section in https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/codegen/cuda/README.md","I won't be working on debugging this issue, but Nvidia's nvfuser team might be interested. ,  can you guys please discuss what the options are?","> I won't be working on debugging this issue, but Nvidia's nvfuser team might be interested. ,  can you guys please discuss what the options are? Hopefully it'll be easy to get some debug dump so we can reverse engineer a repro from there. Not sure how tricky it would be for me to get into their closesourced repo, I would at least need to get management approval. Let's consider that as a backup plan~",",  can you guys start by following the steps here https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/cudageneralideasofdebugnvfusermalfunctioning, or is something else a better approach?","Yeah, I think doing this `PYTORCH_JIT_LOG_LEVEL=graph_fuser python your_script.py &> log` would be a reasonable first step. Should be able to workout a repro with that."," Thanks. I have attached the compilation log here. nvfuser_compilation.log FYI,  ","errr, I actually don't see anything went wrong in the graph log (all aten::cat are operating on list of tensors), looks like we are not trying to insert any chunk neither.... Unfortunately we don't have a graph dump in aliasdb. Let's verify if this is nvfuser specific. We can run things with `PYTORCH_JIT_USE_NNC_NOT_NVFUSER=1 PYTORCH_JIT_LOG_LEVEL=profiling_executor_graph_impl python your_script.py &> log` Would be nice to also run that without NNC (if NNC runs fine without any issue) and give me both of the logs `PYTORCH_JIT_LOG_LEVEL=profiling_executor_graph_impl python your_script.py &> log`",Hi   Thanks for your suggestions. The first command gives the following error:  and the second command gives the same error.,"So this is not nvfuser specific, as NNC also repros this one. I can still help tho. Would you be able to push the full log somewhere when running with the second config: `PYTORCH_JIT_LOG_LEVEL=profiling_executor_graph_impl python your_script.py &> log`. < this should dump a graph somewhere and we can stare at that to figure out a repro....",Here is the full log:  Sorry but I do not see a graph dumped somewhere.,"Hi . Sorry for bothering you again, but I am wondering what your advice on the next step would be if   does not dump the graph information.","Oops, sorry I missed last message. My bad that I gave the wrong instruction :face_with_head_bandage: ... It should be `PYTORCH_JIT_LOG_LEVEL=profiling_graph_executor_impl`. It's using the native jit log https://github.com/pytorch/pytorch/blob/c94706c0118a41d0400e78b01dae897d0f1f9457/torch/csrc/jit/jit_log.hL8L39 We should be able to see a very long log dumping graph from this file: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp We'll be able to see which graph partition hits that assert then.",Hi  I have attached the compilation log here: nvfuser_compilation.log Thank you for your help.,"At the very end of the log, it prints out a `Profiled Graph` and then the assert failure. That graph is unfortunately a backward graph so it's somewhat cryptic and I can't figure out how to reverseengineer the forward graph from that.  :face_with_head_bandage:  I'm still leaning towards that one of the passes here (https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/profiling_graph_executor_impl.cppL658L678) introduced the issue. Let's dump the log in verbose mode with `PYTORCH_JIT_LOG_LEVEL="">>profiling_graph_executor_impl""`. We'll unfortunately see a much bigger log, but hopefully it would expose precisely which pass generates an invalid graph. :crossed_fingers:  Sorry for this back and forth. :cry: ",Hi. I have attached the verbose compilation log to this thread. Because the file is too large I have to zip it before uploading. Thank you. nvfuser_verbose_compilation.log.zip,"Looks like it's coming from autodiff. We just forgot to update the output type. A simpler repro:  Here we seems to forgot to update the output type for `AutogradAdd` https://github.com/pytorch/pytorch/blob/bcc8f592ba060cdc011c7534a2832ea9b816f733/torch/csrc/jit/runtime/autodiff.cppL354L357 I tried to patch that one and copy input type to output. Which seems to have fixed the original issue. But looks like `AutogradAdd` doesn't really handle list tensors. (Even though there's list in a few other places in symbolic_script.cpp) https://github.com/pytorch/pytorch/blob/bcc8f592ba060cdc011c7534a2832ea9b816f733/torch/csrc/jit/runtime/register_prim_ops.cppL2518L2549 Here's the updated graph with AutogradAdd return type patched  I can try to add list support in `AutogradAdd`. but I feel like I'm not on the right track.... Pinging  , not sure who's maintaining autodiff at this moment."," Sorry for bothering you again, but may I know whether there is any update on this bug? Thank you.","Oops, totally forgot about this one. Thanks for the reminder~~ I was trying to see if there's any Meta folks that can help us with Autodiff issue. Let me ping   to see if she can point us to the right person.", is the correct person for autodiff issues. ,"Hi,  May I know whether there is any update on this issue? Thanks.","Sorry I missed the message. I saw  self assigned, wondering if we have assigned any meta folks handling this issue?"
yi,"Got ""RuntimeError: y.get_desc().is_nhwc() INTERNAL ASSERT FAILED""  while applying conv2d over a transposed tensor"," ðŸ› Describe the bug I got this error on pytorch version 1.12.0. The code that can reproduce the error:  The error message:   Versions PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (x86_64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.20.2 Libc version: N/A Python version: 3.7.3 (default, May 29 2019, 18:19:34)  [Clang 10.0.1 (clang1001.0.46.4)] (64bit runtime) Python platform: Darwin21.5.0x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [conda] Could not collect ",2022-07-04T12:37:46Z,triaged module: mkldnn,closed,0,2,https://github.com/pytorch/pytorch/issues/80837,"Root cause is this ambiguity between `is_contiguous`. the given input has shape `[1, 1, 321, 201]` and stride `[129042, 1, 201, 1]`, this is considered as contiguous (memory format is channels last) for pytorch. but onednn considered as not nhwc since  and the dim0 stride do not match, but actually it does not matter since size0 is 1 and so the index could only be 0. we need to get rid of the ambiguity between pytorch and onednn, decide `use_channels_last` from pytorch side and pass it to onednn. On onednn side we don't make judgement, just believe in that pytorch passed down physically contiguous tensors in the aligned memory format (and it should be so).",it has been fixed by https://github.com/pytorch/pytorch/pull/83653. 
gpt,"When running GPT trainning with megatron,  the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers"," ðŸ› Describe the bug 1.When running GPT trainning with megatron, the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers 2.code MegatronLM github branch master and I changed /MegatronLM/megatron/tokenizer/bert_tokenization.py and /MegatronLM/megatron/tokenizer/tokenizer.py for berttokenizer data preprocess needs. tokenizer.zip bert_tokenization.zip 3.training data ~103MB vocab_processed.txt mygpt2_test_0704_text_document.zip mygpt2_test_0704_text_document.bin is ~103MB which exceed size limit, if you need , i can send it. 4.bash 0704_gpt_train.sh 0704_gpt_train.zip 5.env: linux:Linux version 4.15.0167generic (builddamd64045) (gcc version 7.5.0 (Ubuntu 7.5.03ubuntu1~18.04)) python env.txt 6.error log: 0704.log  Versions Collecting environment information... PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47) ",2022-07-03T23:41:43Z,oncall: distributed module: elastic,open,0,1,https://github.com/pytorch/pytorch/issues/80824,Do you run the program with nohup?
yi,Improve readability of cuda_lazy_init,This PR cleans up the implementation of `cuda_lazy_init.cpp` and improves its readability. No behavioral changes are introduced.,2022-07-01T23:00:39Z,Merged cla signed release notes: cuda topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/80788,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80788**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6b04de21b7 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,wt1 - trying a reusealbe workflow,Fixes ISSUE_NUMBER,2022-06-30T14:55:02Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/80610,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80610**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 12 New Failures, 20 Pending As of commit e8d5df4a50 (more details on the Dr. CI page): Expand to see more  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7138406429?check_suite_focus=true) linuxbinarymanywheel / manywheelpy3_10rocm5_1_1build / build (1/12) **Step:** ""Build PyTorch binary"" (full log  :repeat: rerun)   20220630T19:45:39.4424294Z CMake Error at caffe2/CMakeLists.txt:1609 (target_link_libraries):     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
yi,[GHA] Remove new lines from PR_BODY too to appease batch env var copying,"https://github.com/pytorch/pytorch/pull/80543 doesn't work in preventing batch from interpreting the multiline env vars. We will remove the lines from these env vars instead, since PR_BODY and COMMIT_MESSAGES are both used to determine what disabled tests to not skip. Test plan is using the following below and making sure tests still pass, which they do. Summary: previous versions of sparsity utils either allowed for a leading '.' for fqns, or would only allow for that. Per discussion with ao team about fqns don't have a leading '.' fqn of root module is '' these utilities have been updated to align with these definitions. module_to_fqn was changed to not generate a leading '.' and output '' for root module fqn_to_module was changed to output the root rather than None for path='' get_arg_info_from_tensor_fqn had explicit handling for a leading '.' that was removed. The previous implementation overwrote the tensor_fqn if it had a leading '.' which resulted in undesirable behavior of rewriting arguments provided by the user. Also refactored utils to be simpler and added comments, formatting and test Test Plan: python test/test_ao_sparsity.py python te",2022-06-29T17:42:56Z,module: rocm Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/80548,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80548**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b10da2e9c2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge l trying out land validation, successfully started a merge job. Check the current status here,Successfully started land time checks. See progress here: https://hud.pytorch.org/pytorch/pytorch/commit/40ba6dd2b195b6dca672cbee883dcf08c9c3b6b3,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Playing around,Fixes ISSUE_NUMBER,2022-06-29T17:38:04Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/80546,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80546**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 7cab6dd4a5 (more details on the Dr. CI page): Expand to see more  *Commit 7cab6dd4a5 was recently pushed. Waiting for builds...*  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
rag,Forward AD mismatched stride when size = 0 becomes mismatched storage offset after slice, ðŸ› Describe the bug `gradcheck` triggers INTERNAL ASSERT for `torch.diff` in forward mode.   Versions pytorch: 1.11.0 ,2022-06-29T12:51:51Z,triaged module: forward ad,closed,0,7,https://github.com/pytorch/pytorch/issues/80507, ,"fwiw, I can reproduce this in `master`.","This is happening because forward AD doesn't really care when strides are different between primal and tangent (when the size in that dimension is 1), but when we do a certain slice on these primal and tangent (for example `[1:]`), they both become zero numel tensors (expected) except the storage offset is now different  and forward AD complains about this. What we might want to do is just make forward AD not care about storage offset being different in some cases as well, but I'm not sure about the implications of that yet. Minimal repro: ","From offline discussion with , in the case when there is no exact match in metadata, but the underlying storage representation is the same, we should update forward AD to perform a outofplace view during set_fw_grad to ensure the metadata matches exactly.",Can we instead ignore storage_offset on 0element tensors?,> Can we instead ignore storage_offset on 0element tensors? Hmm yeah I think this makes sense. The invariant that the same indices in the tangent and primal tensors must index the same locations in storage would be vacuously true., Actually another alternate way to make the old solution work would be to allow this specific usage of as_strided in the batching rule since it is basically a noop
transformer,[bug] libtorch bug in nn::MultiheadAttention and nn::Transformer," ðŸ› Describe the bug the attn_mask does not work in nn::MultiheadAttention ~~~ include  namespace nn = torch::nn; // seq_length x batch_size x feature_size torch::Tensor x = torch::randn({3,1,4}); torch::Tensor attn_mask = nn::TransformerImpl::generate_square_subsequent_mask(3); torch::Tensor attn_mask_bool = attn_mask.to(torch::kBool); nn::MultiheadAttention multihead_attention(4,1); std::tuple output_with_attn_mask = multihead_attention >forward(x,x,x,{},true,attn_mask_bool); torch::Tensor attn_output, attn_output_weights; std::tie(attn_output, attn_output_weights) = output_with_attn_mask; //unpacking tuple into variables std::cout << attn_output_weights << std::endl; ~~~ ~~~ attn_mask: /*  0  1  1  0  0  1  0  0  0 [ CPUBoolType{3,3} ] */ attn_output_weights: (1,.,.) =    0.1918  0.5302  0.2780   0.2074  0.1919  0.6007   0.1948  0.5092  0.2960 [ CPUFloatType{1,3,3} ] ~~~ * 1 the attn_mask does not affect the attn_output_weights. * 2  the API in libtorch for MultiheadAttention and Transformer is not comparable with that of in pytorch. e.g. libtorch api have no batch_first param.  * 3 more detail comparison see https://github.com/walkacross/pytorchl",2022-06-29T03:12:17Z,module: cpp module: nn triaged module: correctness (silent),open,1,7,https://github.com/pytorch/pytorch/issues/80494,Hey there ! Thanks for bringing this up. Responding to your points: 1. This is a problem and I will look to repro and fix this week.  2. Unfortunately the cpp api is not as well developed as the python api and the lag will probably continue. We will do our best to add features like batch_first but cannot guarantee. ," thanks for your response, have a good day.","It's been 9 months, and this bug still exists. why?? (everyone who use libtorch to inference experiences the wrong code)     ","Hi, I'm afraid the c++ API does not have a maintainer at the moment. So development in that area is quite slow.","hi , thanks for your kind response. this situation  c++ API does not have a maintainer at the moment   seems stay a long time, does pytorch team has any plan to change this situation?  considering every version release involves in libtorch, but unfortunately, little improvement happens.  Besides, nn::MultiheadAttention is a key part of Transformer model, but the code is wrong, it has affacted many researchers.","We are very much open to onboarding a maintainer for the C++ API! Unfortunately, we don't have anyone interested at the moment. Also since the C++ API is not used as much as other features, we tend to focus more on these other features (while maintaining releases for libtorch of course). Broadly, we don't expect the C++ API to bring a lot of benefits compared to the python one.","got it, anyway, thanks for your feedback"
yi,Mark underlying type as C10_UNUSED,Fixes regression detected in https://github.com/pytorch/pytorch/pull/79978 As result some of the internal workflows fail as follows: ,2022-06-28T00:59:49Z,Merged cla signed ciflow/trunk,closed,0,5,https://github.com/pytorch/pytorch/issues/80415,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80415**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit a216c4c30d (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge f, successfully started a merge job. Check the current status here, your PR has been successfully merged.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,support nested_tensor * scalar,"In transformer, the scale step in attention has a `nested_tensor / scalar` operation. There are two ways to support that: 1. directly support `nested_tensor / scalar`: * pro: straightforward, good UX * con: is dispatching `mul(nested tensor, regular tensor)` a good practice? 2. let user manually convert `scalar` to `nested_scalar = torch.nested_tensor([broadcast_scalar])` * pro: dispatcher only has to deal with `mul(nested tensor, nested tensor)` * con: confusing manual conversions, bad UX",2022-06-25T15:50:12Z,Merged cla signed topic: improvements release notes: nested tensor,closed,0,6,https://github.com/pytorch/pytorch/issues/80284,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80284**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b3d9565f73 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Stuff like op(cuda_tensor, scalar_tensor) is allowed, so I think mixing scalar tensors with nested tensors should be allowed too. FWIW, that should work fine with the dispatcher youâ€™ll always dispatch to a nested tensor kernel.","I'm very much in favor of Option 1. Please see my inline comment for a tour of our Python frontend bindings. I'd have also expected mul.Scalar to be used, but it seems like it's not.", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Modifying Adam to support complex numbers as 2d real numbers,This commit addresses issues in CC(Cleanup optimizer handling of complex numbers.),2022-06-25T11:09:53Z,triaged open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/80279,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80279**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 3e6f0a0c50 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , does this look alright?, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Disable AVX512 CPU dispatch by default,"As it can be slower, see  CC(AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU) Update trunk test matrix to test AVX512 config in `nogpu_AVX512` flavor. Kill `nogpu_noAVX` as AVX support were replaced with AVX512 when https://github.com/pytorch/pytorch/pull/61903 were landed",2022-06-24T22:49:24Z,Merged cla signed ciflow/trunk,closed,0,4,https://github.com/pytorch/pytorch/issues/80253,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80253**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 419899ecbd (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7066712836?check_suite_focus=true) pull / linuxfocalpy3.7gcc7 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220627T05:30:09.4877508Z The PR is introduc...m to confirm whether this change is wanted or not.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge f, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU, ðŸ› Describe the bug Benchmarking latest nightly using WLM_Transformers example  AVX512 kernels are 18% slower than using AVX2 ones:   Versions 1.13.0.dev20220610+cpu)  ,2022-06-24T22:45:49Z,high priority module: performance triaged module: regression,closed,0,9,https://github.com/pytorch/pytorch/issues/80252,"Oops! I hope this wouldn't affect the PyTorch 1.12 release plan. I'll submit a PR to disable AVX512 dispatch for those kernels that are slower with AVX512, i.e. enable AVX512 dispatch only for those kernels that are faster with AVX512. Perhaps we can also enable/disable AVX512 dispatch according to the platform, as more recent or upcoming platforms might have better AVX512 performance. Regardless, I'll see if there's a way to boost AVX512 performance of existing ATen kernels as well. Memorybound kernels are more likely to be slower with AVX512, than with AVX2, so maybe down the line we'll see if HBM can help. Thanks!","person for the time being I plan to disable it across the board by default, because at least on any of the workflows we usually benchmarked it did not yield any gains on EC2 C5 machine. Let's continue discussion here on some microbenchmarks one could have run to more easily identify what is going on.",By dropping (profile.py)[https://gist.github.com/malfet/0a063a0663230a4cdcd2eb75324e221a] into (https://github.com/pytorch/examples/tree/main/word_language_model) int looks like `aten::_log_softmax ` is the one that regressed the most: ,Discussion:  let's file an issue to improve benchmarking  possibly creating a benchmarking job," , there are some existing ATen op benchmarks at https://github.com/pytorch/pytorch/tree/master/benchmarks/operator_benchmark/pt, and we can run them with different `ATEN_CPU_CAPABILITY`. We might need to add some more, though. Some ops work better with AVX512, but there's significant runtorun variation. I've noticed that preloading a thirdparty memory allocator (jemalloc/tcmalloc) produces more consistent results. I'll post results later. I guess we can also benchmark on AWS m6i.64xlarge AWS instance, which is Ice Lake, and has 48 KB L1D caches. Thanks!","Hi      ,  Even after we'd finish benchmarking & analyzing performance of ATen kernels (and potentially enable dispatch of some AVX512 kernels), can we still let AVX2 be the default CPU capability, so that users may use AVX512 ATen kernels by using the environment variable `ATEN_CPU_CAPABILITY=AVX512`? IMHO, there are at least 4 factors that should be considered while benchmarking & comparing AVX2 & AVX512 performance of each ATen kernel: 1. Input(s) size(s)  2. Number of threads 3. CPU platform 4. Memory allocator (AVX512 might do better with nondefault memory allocators with large pagesizes.)  For some combination of the aforementioned factors, some ATen kernels might exhibit better performance with AVX512, but worse with another combination.  However, currently, PyTorch dispatches an ATen kernel according to the highest CPU capability, so the dispatcher doesn't consider these factors. So, would it be okay to add a Python API to change `ATEN_CPU_CAPABILITY` at runtime, which would allow users to finetune performance of workloads, based on the analysis of data gleaned via prior benchmarking? Thanks!","person I've already switched to `avx2` by default in https://github.com/pytorch/pytorch/pull/80253 As for the rest, I think majority of the users would not know which kernel to pick, so if we want to use AVX512, then we should code the heuristics into the AVX512 kernel implementation","Hi , please help with a doubt. >  if we want to use AVX512, then we should code the heuristics into the AVX512 kernel implementation If  `ATEN_CPU_CAPABILITY` would be `AVX512`, but for some particular inputs of a kernel, if it's determined heuristically that the `AVX512` kernel might be slower, should its `AVX2` counterpart be dispatched instead? Thanks!",This specific issue was fixed in CC(Fix threadallocation in `_vec_log_softmax_lastdim`). Thanks!
yi,[FSDP] Clean up `_lazy_init()`,Stack from ghstack:  CC([FSDP] Move `_post_backward_called` to `_init_param_attributes`) [FSDP] Move `_post_backward_called` to `_init_param_attributes` * * CC([FSDP] Clean up `_lazy_init()`) [FSDP] Clean up `_lazy_init()`**  CC([Easy][FSDP] Add `zero_grad()` to unit test train loop) [Easy][FSDP] Add `zero_grad()` to unit test train loop  CC([FSDP] Remove `self.numel_padded_per_param` (unused)) [FSDP] Remove `self.numel_padded_per_param` (unused)  CC([FSDP] Move tensor sharding logic to `FlatParamHandle`) [FSDP] Move tensor sharding logic to `FlatParamHandle`  CC([FSDP] Deduplicate `_orig_size` and `_unsharded_size`) [FSDP] Deduplicate `_orig_size` and `_unsharded_size`  CC([FSDP] Introduce `FlatParamHandle`) [FSDP] Introduce `FlatParamHandle` This PR cleans up `_lazy_init()`. The explanations are left as PR comments. Differential Revision: D37726059,2022-06-23T22:20:31Z,oncall: distributed Merged cla signed,closed,0,9,https://github.com/pytorch/pytorch/issues/80185,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80185**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit f9ad1d74ca (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"> I think this might have a small semantic change, where cast_buffers() is no longer done every iteration  I think this is fine since it was probably redundant. But one case I can think of is we checkpoint in full precision for buffers. If user calls forward after that, we should make sure they were restored (I think they are already when exiting checkpoint logic, but need to check) It looks like there is no cast in the post load state dict hooks. I will test this and add the cast to the post load state dict hook as needed. IMO, it makes more sense (for both clarity and performance overhead) to have that cast once and directly where we need it rather than having it be a noop all but one of the time when calling it in `_lazy_init()` every iteration.","> I think this might have a small semantic change, where cast_buffers() is no longer done every iteration  I think this is fine since it was probably redundant. But one case I can think of is we checkpoint in full precision for buffers. If user calls forward after that, we should make sure they were restored (I think they are already when exiting checkpoint logic, but need to check) https://github.com/pytorch/pytorch/blob/10c689856d5f913b93daa404955927a5a5716805/torch/distributed/fsdp/fully_sharded_data_parallel.pyL1909L1930 The `_post_state_dict_hook()` casts buffers. https://github.com/pytorch/pytorch/blob/10c689856d5f913b93daa404955927a5a5716805/torch/distributed/fsdp/fully_sharded_data_parallel.pyL1992L2005 `state_dict()` is the only method that casts buffers (namely, back to the original dtype) other than `_cast_buffers()`. Since `state_dict()` and `_post_state_dict_hook()` always are called in pairs, there should be no need to `_cast_buffers()` every iteration."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,GEGLU activation," ðŸš€ The feature, motivation and pitch Seems to be the latest and greatest activation function for transformers. See https://arxiv.org/abs/2002.05202v1  Alternatives One could copypaste the implementation from https://github.com/pfnetresearch/deeptable/blob/237c8be8a405349ce6ab78075234c60d9bfe60b7/deep_table/nn/layers/activation.py  Additional context _No response_ ",2022-06-23T19:41:15Z,module: nn triaged enhancement needs research,open,22,1,https://github.com/pytorch/pytorch/issues/80168,"Hey , thanks for the request! Note that we are very conservative with adding new modules to PyTorch core. In general, there should be a very good performance or popularity reason for them to be officially maintained here rather than implemented in a separate repo. Let's see if the request becomes popular enough to make the addition worthwhile."
yi,[MPS] segmentation fault when multiplying two scalar tensors," ðŸ› Describe the bug When multiplying two scalar tensors in mps kernel, a segmentation fault occurs. ```mm MPSGraphTensor* aTensor = [mpsGraph constantWithScalar:.4f                                                //shape:@[]   ",2022-06-23T19:06:53Z,needs reproduction triaged module: mps,closed,0,7,https://github.com/pytorch/pytorch/issues/80165,"Hmm, I'm not sure what this one as about. Can you please provide a reproducer? I can not repro the crash by running the following: ","  Please clone this branch https://github.com/qqaatw/pytorch/tree/add_huber_loss_mm Build and run the following:   Then, comment out the shape arguments below like what I do in the issue content. https://github.com/qqaatw/pytorch/blob/d49ea958ee31478077567e239d2157680aa89f91/aten/src/ATen/native/mps/operations/LossOps.mmL1079L1084 Build and run the following again: "," I struggle to understand what is has to do with PyTorch? This sounds like a bug that should be filed against Apple (which develops the framework), rather than PyTorch that just uses it.",Thanks  for filling this issue  I was able to reproduce locally as well. I'll check with the relevant engineers and update you back,"Hi ,  This issue has been fixed in macOS 13 beta 2. In order to avoid doing checks based on the OS in PyTorch's MPS backend, I think the best would be to specify directly the shape (`[mpsGraph constantWithScalar: x shape:@[] ... ];`.","Hi , Thank you so much for your quick feedback!  Maybe this issue is worth noting on the wiki, which prevents people who want to contribute new mps ops from running into the bug."," , I agree with , please use shape for now to avoid OS specific checks unless absolutely needed. That's a good point, we can add a section to note issues such as these."
transformer,[feature request] Add support for a custom DatasetFetcher in DataLoader ," ðŸš€ The feature, motivation and pitch I'm working on a timeseries dataset, and I stumbled upon a limitation withing pytorch's DataLoader that I'd like to address here. I have a custom mapstyle dataset that implements a `__getitem__` and `__len__` methods. `__getitem__`  has a following signature and returns by default features of shape `(seq_length, n_features)` and an integer label.  Sequence length is fixed by default, but can be changed by passing a `seq_length` argument. I'm using an LSTM + Transformer model that is able to process an input of variable sequence length.\ BUT I'm not leveraging this feature of my model because pytorch's DataLoader expects all the samples returned by the dataset to be of the same shape. A traditional approach to solving this problem is **sequence padding**. This doesn't really solve the problem in my opinion (the model still gets constant sequence length as an input) and leads to a range of further questions (which values to use for padding, how to interpolate ect.). **What I would like to do instead** is to use a custom DatasetFetcher to create batches that have constant sequence length within a batch, but **varyin",2022-06-23T12:08:54Z,module: dataloader triaged enhancement module: data,open,0,1,https://github.com/pytorch/pytorch/issues/80134,Have you considered passing in a custom `collate_fn` to `DataLoader` that purpose? The input to `collate_fn` is a batch; you are free to pick a sequence length and pad/trim the sequence in there as you need. You can also import and call the `torch.utils.data.default_collate` after your custom step if you'd like as well.
transformer,create input tensor on GPU device takes extremely long time when inferencing with transformer models," ðŸ› Describe the bug Hi Team, Good day! Problem: create input tensor on GPU device takes extremely long time when inferencing with transformer models Description:  I use below sample full code, to examine inferencing speed with BERT model, it uses bertbasemultilingualuncased from huggingface's implementation, takes random 5000 texts from SQuAD1. To speed up, the code is optimised so that it first tokenize all the 5000 texts, and sort the tokenized texts by number of tokens, and batching texts with similar length in one batch sent for inference.  Particularly, I have recorded the time used for model inferencing, below line   And the time took to put input tensor created to GPU device   Noticed, with maximum sequence length of 128, when set with a big batch size, say 512, the time used for model inference is blazingly fast, less than 0.1s for all 5000 texts, but it takes insanely long time, almost 2s in total to put input tensor to GPU for all 5000 texts.  However, with same maximum sequence length of 128, when set with a small batch size, say 16, the time used for model inference slows down to more than 2s for all 5000 texts, but it speeds up to put i",2022-06-23T09:05:17Z,module: cuda triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/80129,">Noticed, with maximum sequence length of 128, when set with a big batch size, say 512, the time used for model inference is blazingly fast, less than 0.1s for all 5000 texts, but it takes insanely long time, almost 2s in total to put input tensor to GPU for all 5000 texts. > However, with same maximum sequence length of 128, when set with a small batch size, say 16, the time used for model inference slows down to more than 2s for all 5000 texts, but it speeds up to put input tensor to GPU, which takes less than 0.2s for all 5000 texts. CUDA operations are executed asynchronously so you would need to synchronize the code via `torch.cuda.synchronize()` before starting and stopping the timers which is not the case in your posted code.  The very first CUDA operation will initialize the CUDA context, load the kernels etc. and is expected to be slow. Since you are not synchronizing your timers will accumulate the previous runtimes when a sync is enforced and give wrong results. ","Got it, Thank you very much! "
chat,[PyTorch][ATen][AMD] always declare ROCmBackwardPassGuard,"Summary:  Problem `ROCmBackwardPassGuard` is only defined and instantiated when `USE_ROCM` is set, which complicates our internal build system that distinguishes a CPUonly module from a GPUaware module.  Solution Despite its name, `ROCmBackwardPassGuard` does not depend on ROCm features. Regardless of its usefulness for nonROCm code, we don't need `ifdef USE_ROCM` for the declaration of `ROCmBackwardPassGuard` like other components like CUDAspecific `NoTF32Guard`. Note: this patch should not have any side effects (`ROCmBackwardPassGuard` is not instantiated when `USE_ROCM` is not set) except for additional tiny compilation time for these 10 lines. Reviewed By: minsii Differential Revision: D36670235",2022-06-22T19:31:55Z,module: rocm fb-exported cla signed Stale,closed,0,11,https://github.com/pytorch/pytorch/issues/80069,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80069**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 4f7de45155 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,  amd Would one of you mind taking a look at this PR? 1. Why needed? Our internal build system distinguishes a CPUonly module from a GPUaware module.  `context.cpp` belongs to CPUonly and does not take an AMDGPU flag.  The internal workaround does not work. This PR seems the best way to fix the issue so far. 2. Why `if !defined(_WIN32)`? `TORCH_API` + `thread_local` is not allowed on Windows.  This is okay since the path above does not use Windows.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",iwasaki Do you plan to merge this PR?,"Sorry, I completely missed this notification. Yes I would like to if the PyTorch reviewers can approve this change.",This pull request was **exported** from Phabricator. Differential Revision: D36670235
rag,[ONNX] Increase coverage of quantization operators,Uptodate supported list: https://pytorch.org/docs/master/onnx_supported_aten_ops.html?highlight=onnx  Extending support for operators used by quantization. A few thoughts/options. 1. Support all operators documented here https://pytorch.org/docs/stable/quantizationsupport.html?highlight=quantization. The list is nonexhaustive and there may exist undocumented quantized operators.     * torch          [x] quantize_per_tensor          [x] quantize_per_channel          [x] dequantize     * other aten          [ ] slice     * torch.Tensor          [x] view          [x] as_strided          [x] expand          [x] flatten          [x] select          [x] ne          [x] eq          [x] ge          [x] le          [x] gt          [x] lt          [ ] copy_          [x] clone          [ ] dequantize          [ ] equal          [ ] int_repr          [ ] max          [x] mean          [ ] min          [ ] q_scale          [ ] q_zero_point          [ ] q_per_channel_scales          [ ] q_per_channel_zero_points          [ ] q_per_channel_axis          [ ] ~resize_~ [resize_ is not traceable]          [ ] sort          [ ] topk     * torch.nn.quantized          [,2022-06-22T15:56:11Z,module: onnx triaged onnx-triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/80039,"Seen:  Weights and biases: some are quantized, others not", can you link the other tracking issue here?
agent,RPC init fails and crashes when world_size is greater than 18," ðŸ› Describe the bug Hi! When I use RPC, I find that when the `world_size` is greater than 18, the program will crash. I've tested it on two servers and got the same result. Program to reproduce:  When `tot_processes = 18`, the program can exit without any error. But if `tot_processes = 19`, it will crash as below.   Versions  ",2022-06-22T07:06:01Z,oncall: distributed triaged module: rpc,open,0,15,https://github.com/pytorch/pytorch/issues/80017,verified that I can repro this locally.  Huang  (for tensorpipe) could we take a look at this and see what happens as we go from 18 > 19 procs?,"Hi , thanks for the script and logs. I am able to repro and hitting the error during shutdown, but the reasoning is the same. Each RPC process select a thread from a pool of threads to handle requests to avoid blocking/stalling the event loop. During initialization and graceful shutdown, all RPC processes send RPC requests to each other to perform rendezvous which exhausts the threadpool and causes the error. A workaround is to increase this thread pool size (default is 16). Here is an updated script which works:  TODO: we should error out with a more descriptive message, CC(Provide error message when thread pool is exhausted in RPC)","On one of my friend's environment, we get outputs like:  But the program can finish other works and it seems like these errors don't affect the functionality. The difference here is that in his environment the program will not raise an error so the following works run well. For example, we can get the expected results for the program below even we use more than 20 processes. Note that he uses PyTorch 1.8.0. ","Even add `num_worker_threads=32`, I still cannot run the program with 20 processes successfully. I guess there are some system settings that affect this.","> Even add num_worker_threads=32, I still cannot run the program with 20 processes successfully. I guess there are some system settings that affect this. Do you get the same error as the original post? Or is it a different one due to system settings?","Oh, I get a different error. ","Huang Hi! On my friend's machine, I tried the latest pytorch and still got the error reported above. However, using my friend's conda environment with a selfbuilt old version pytorch, the error disappeared. Here's the environment. ","> connect: Resource temporarily unavailable This looks system resource specific.  Check `ulimit a` for any limits set to your server, particularly the number of max processes. It could also be an issue with not enough RAM or swap space. ","I don't think so in my case. Because I use the same machine, and only the difference of conda environment causes the difference of results. `ulimit a` of both soft and hard limits are the same. ","Well, you mentioned you are using different version of pytorch which will have different implementations and thus different resource usages. > l: lockedinmemory size (kbytes)  64 Your `lockedinmemory size` seems quite low, can you try increasing that (https://gemfire.docs.pivotal.io/97/geode/managing/heap_use/lock_memory.html) and trying again? Let me know, thanks!", Any updates on the follow up above? If there are no further updates I will close out the issue.,I still have the error.  ,HI  Huang  Have you solved this issue ?  I also met this issus and It doesn't work when I tried all approches above. ," No, I couldn't make it work."," I faced the same problem, and solved it with the solution mentioned in the discussion. https://discuss.pytorch.org/t/rpcbehaviordifferencebetweenpytorch170vs190/124772/16"
agent,DISABLED test_device_map_gpu_default_to_non_default (__main__.TensorPipeTensorPipeAgentCudaRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 red and 1 green. ",2022-06-22T03:55:39Z,oncall: distributed triaged module: flaky-tests module: rpc skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/80008
transformer,[release/1.12] nn: Disable nested tensor by default,Better transformers (and by extension nested tensor) are identified as a prototype feature and should not be enabled by default for the 1.12 release. Signedoffby: Eli Uriegas ,2022-06-20T18:38:49Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/79884,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79884**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9c125264db (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
yi,Trying to run all bazel tests on gpu machine,"This is exploration of CI capabilities, please don't review.",2022-06-18T22:26:59Z,open source cla signed Stale,closed,0,2,https://github.com/pytorch/pytorch/issues/79844,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79844**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 Flaky Failures As of commit 35c2db6352 (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs   :snowflake: 1 failure **tentatively classified as flaky** but reruns have not yet been triggered to confirm:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6955918249?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7bazeltest / buildandtest (1/1) **Step:** ""Test"" (full log  :repeat: rerun) :snowflake:   20220619T17:05:01.1648185Z unknown file: Failure     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
gpt,[ONNX] Internal assertion failure when export gpt2 model with `export_modules_as_functions=True`," ðŸ› Describe the bug Hello, I'm using `torch.onnx.export` to convert my gpt2 model to onnx format. But when I set `        opset_version=15` and `export_modules_as_functions=True`, the program reports an error:   Versions Collecting environment information... PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (x86_64) GCC version: Could not collect Clang version: 13.0.1 CMake version: version 3.22.3 Libc version: N/A Python version: 3.7.9 (v3.7.9:13c94747c7, Aug 15 2020, 01:31:08)  [Clang 6.0 (clang600.0.57)] (64bit runtime) Python platform: Darwin21.5.0x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.11.0 [pip3] torchvision==0.12.0 [conda] Could not collect",2022-06-18T15:37:54Z,needs reproduction module: onnx triaged onnx-triaged,closed,0,8,https://github.com/pytorch/pytorch/issues/79839,Does `export_modules_as_functions=False` give you the same error?," No, setting `export_modules_as_functions=False` removes this error",Could you share a script that will help us reproduce the error?, here is the script , any ideas?,Thanks for the script. We are tracking a few instabilities with export_modules_as_functions. Will update here, ,Fixed by CC([ONNX] Fix bug using std::copy_if)
agent,Separate faulty agent RPC tests,Separates faulty agent RPC tests from rpc_test.py to faulty_agent_rpc_test.py towards fixing issue CC(Split up and reorganize RPC tests). Run a specific test: cpurun pytest test/distributed/rpc/test_faulty_agent.py vsk  Run all FaultyAgentRpcTests:Â  cpurun pytest test/distributed/rpc/test_faulty_agent.py vs,2022-06-17T21:27:01Z,oncall: distributed Merged cla signed release notes: distributed (rpc) topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/79810,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79810**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit f9f5433090 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Looks good, thanks for sending this PR out! Let's fix these lint failures:  And also add a title / description to the PR that describe the change. Let's also include the github issue in the description.","hanna has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, successfully started a merge job. Check the current status here,"Hey hanna. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"Add test for torchscripting nn.TransformerEncoder, including fast path","Summary: Add test just to check if TransformerEncoder will crash when enumerating over params [with_no_grad, use_torchscript, training]. Motivation for this was that TransformerEncoder fast path (so with_no_grad=True) and use_torchscript=True would crash with the issue that NestedTensor doesn't have size. This was caused because the TransformerEncoder fast path generates a NestedTensor automatically as a perf optimization and torchscript attempts to find intermediate tensor sizes while it optimizes. But NestedTensor has not implemented a size method, so things fail. This test goes together with this fix https://github.com/pytorch/pytorch/pull/79480 Test Plan:  Test runs and passes together with the changes from the PR above (I made another diff on top of this with those changes). Does not pass without the fix. Reviewed By: mikekgfb Differential Revision: D37222923",2022-06-17T17:35:56Z,module: tests fb-exported Merged cla signed release notes: jit release notes: nn topic: bug fixes topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/79796,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79796**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6d647a8ad3 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37222923, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This PR also included this one https://github.com/pytorch/pytorch/pull/79576 (also approved). I intended to merge these as stacked PRs, but ended up just merging this one by mistake which included code for both PRs. "
finetuning,[FSDP] Test that module using mixed precision can be loaded into non-mp module," ðŸš€ The feature, motivation and pitch In FSDP, mixed precision checkpoint is taken with the full parameter precision, but we're missing a test that does this and restores into a nonmixed precision module. Having this test will help provide confidence that use cases which use these such as finetuning / inference work as expected.  Alternatives _No response_  Additional context _No response_ ",2022-06-17T01:38:57Z,triaged better-engineering module: fsdp,open,0,0,https://github.com/pytorch/pytorch/issues/79766
transformer,[AI Accelerators] softmax kernel for Nested Tensor (CPU),"Summary: Impl better softmax kernel for Nested Tensor CPU. Test Plan: Benchmark results: On CPU (command: buck run mode/opt c fbcode.platform=platform009 //pytext/fb/tools:benchmark_transformers   transformer large usetrtkernel False batchsize 16 avgsequencelength 64 maxsequencelength 256 iters 10 userealdatadistribution module native usent True usecpu True With mask (previous impl): NT: 4573.14 ms/iter, 0.14 TFLOP/s, Speedup: 2.33x; Without mask: NT: 3530.55 ms/iter, 0.18 TFLOP/s, Speedup: 1.51x Reviewed By: mikekgfb Differential Revision: D35679352",2022-06-16T22:29:21Z,fb-exported Merged cla signed release notes: nn release notes: cpp topic: performance,closed,0,22,https://github.com/pytorch/pytorch/issues/79756,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79756**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9d420b51e4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,  merge g, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2531813087,This pull request was **exported** from Phabricator. Differential Revision: D35679352, merge g, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2532363313," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,  merge g, successfully started a merge job. Check the current status here, your PR has been successfully merged.
agent,DISABLED test_custom_stream (__main__.TensorPipeTensorPipeAgentCudaRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 red and 1 green. ",2022-06-16T21:41:35Z,oncall: distributed triaged module: flaky-tests module: rpc skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/79750
yi,[DataPipe] Count number of successful yields for IterDataPipe,Stack from ghstack:  CC([DataPipe] Snapshotting prototype)  CC([DataPipe] Basic snapshotting with IterableWrapper)  CC([DataPipe] Full graph fastforwarding)  CC([DataPipe] Simple graph snapshotting)  CC([DataPipe] Count number of successful yields for IterDataPipe) This PR adds an attribute and logic to count the number of successful yields from `IterDataPipe`. This information can be useful to fastforward a DataPipe (or the entire graph) back to a certain state.,2022-06-15T23:38:32Z,Merged module: data cla signed release notes: dataloader topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/79657,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79657**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 55bdafcccc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here, your PR has been successfully merged.
yi,[torch] Add more functions to __init__.pyi.in for torch._C for Node and Value,"Summary: https://github.com/pytorch/pytorch/pull/78757 recently added a lot of functions to the type stub, but it missed a few of them. This change will make sure every function is included, by making sure this list is uptodate with: `torch/csrc/jit/python/python_ir.cpp`. This change only does this for Node and Value. Differential Revision: D37189713",2022-06-15T22:55:05Z,fb-exported Merged cla signed topic: not user facing,closed,0,14,https://github.com/pytorch/pytorch/issues/79654,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79654**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 931c49008c (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,"Adding `destroy` as a function too, then will merge",This pull request was **exported** from Phabricator. Differential Revision: D37189713, merge g, successfully started a merge job. Check the current status here, your PR has been successfully merged.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","I don't have write access, so I can't apply a label. So I'll leave this in a comment instead: topic: not user facing"
transformer,"[FSDP] RuntimeError when using FSDP with auto wrap for sequence-to-sequence language models such as T5, Pegasus"," ðŸ› Describe the bug When using FSDP with the latest PyTorch Nightly version, it throws below error when using  sequencetosequence language models such as T5, Pegasus on tasks such as translation, summarization ...  Steps to reproduce the error: 1. Run below command for the official transformers script run_translation.py  to train T5 on translation task using FSDP integration.  2. The following error is the output with the stack trace:   Expected Output No errors and runs for sequencetosequence models.  Versions PyTorch version: 1.12.0.dev20220505+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.090genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration:  GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX Nvidia driver version: 470.57.02 cuDNN version: Probably one of the following: /usr/loca",2022-06-15T12:06:11Z,oncall: distributed module: fsdp,closed,1,16,https://github.com/pytorch/pytorch/issues/79605,Including similar issue raised in transformers repo  CC(Registering of kldivergence for independent distribution),"Hello, any update on this?","Hello, any update on this?"," Unfortunately our oncall missed this one, moving it back to the triage queue. Could you please link a pointer to the code that absorbs the following FSDP configuration, I couldn't find it in the HF file you linked:  In PT 1.12, an autowrapping policy for transformers is available: https://github.com/pytorch/pytorch/blob/master/torch/distributed/fsdp/wrap.pyL82  could you try plugging in this policy as opposed to the sizebased wrapping policy and letting us know how that goes?",the error means the shared parameters are not wrapped in the same FSDP unit.  use transformer wrapper policy instead of sizebased wrapping policy should resolve the problem.  Wondering whether HF transformer integration codes can change the wrapping policy to use transformer wrapper policy. ?," varma   HF is not using the transformer wrapper atm, and this is definitely the issue here for T5 (and presumably pegasus).  I've contacted Stas at HF to try and get this corrected on their side.  ","Hello, Thank you, I'll work on it by testing out the Transformers wrapping.","   There's a full tutorial on how to implement transformer wrapper here, and has the import and code for T5 as one of the examples: https://github.com/lessw2020/transformer_central/tree/main/transformer_wrapping_tutorial Offhand, you should be able to modify their accelerator script to remove the min_num_params wrapper and replace it with the transformer wrapper for the model you are running. I've been using T5 HF + transformer wrapper, though not with HF trainer, so I know it works.  Please let us know how it goes and happy to help if you hit issues.","Hello , varma,   Thank you for your pointers and guidance. Transformer wrapping fixes the issue for T5. Also, I tried the `MixedPrecision` and it is also working but I don't know about correctness, I thought it was still an issue as per this  CC([FSDP] [Mixed Precision] using param_dtype breaks transformers ( in attention_probs matmul)).  Let me know if it is acceptable to use FSDP `Mixed Precision` with transformers. I have raised the following PRs to enable transformer wrapping in ðŸ¤— accelerate and ðŸ¤— Trainer: 1. ðŸ¤— Accelerate  https://github.com/huggingface/accelerate/pull/522 2. ðŸ¤— Trainer  https://github.com/huggingface/transformers/pull/18134 Successfully able to run the below command for the official transformers script run_translation.py  to train T5 on translation task using FSDP integration with `transformer wrapping` and mixed_precision.  Below is the screenshot of the output showing successful training and model architecture with FSDP wrapping on T5Block. !Screenshot 20220714 at 6 06 08 PM","Regarding  > Also, I tried the MixedPrecision and it is also working but I don't know about correctness Since you mentioned it is working I'm assuming it didn't run into the crash described in that PR, regarding correctness, in general we should see throughput speedup, memory reduction, at the possible cost of training loss being higher than full precision.","   thanks for putting in the PR over there to get this updated! As varma mentioned, mixed precision with FSDP works extremely well (for transformers etc).  Your loss may lag relative to fp32 for 13 of the first epochs but after that bfloat will catch up and end up at same place as fp32 ...but 2x+ as fast per epoch.   If you are an ampere, definitely use bfloat16 and esp for T5.   For fp16, you have to add a bit more code to implement the sharded grad scaler. I'm making the video + code tutorial for mixed precision today and tomorrow, so let me send that over for you as soon as it's finished. ","Hi     Here's the mixed precision tutorial for FSDP. https://github.com/lessw2020/transformer_central/blob/main/mixed_precision/readme.md It's pretty straightforward as it's controlled via custom policies.   Please make sure you do a BFloat16 supported check as shown in the tutorial and for FP16, you'll need to use the ShardedGradScaler (also shown in tutorial). Please let me know if any questions!","Thank you everyone ðŸ¤—. This is resolved, will close the issue.","This issue is caused by we tie `word_emb` with `lm_head`, which is the meaning of `multiple_views`. I solved it by disable `tie_word_embedding`","Hi, I'm facing the same issue but on the nn.Linear module, the stack trace is as follows, this is happening when I'm using FSDP. I have tried using `transformer_auto_wrap_policy` but that doesn't seem to help. Any ideas?  I am using the Linear module in a sequence, if that affects it? I am wrapping the entire model (it's a transformer based model) in one FSDP module, if it's recommended to wrap modules inside separately, let me know. ",I'm having the same problem. Did you figure it out? 
transformer,Add numerical test comparing BetterDecoder and fairseq decoder,Summary: Add a new file test_transformers.py to put transformers tests in and move away from huge monolithic test_nn.py. A todo item is to move existing transformer tests from test_nn.py to test_transformers.py. Add a numerical test comparing torch.nn._transformer_decoder_layer_fwd and fairseq decoder. Both decoders use the weights of a common nn.TransformerEncoder. Contains both forced decoding and incremental decoding Stacked on top of https://github.com/pytorch/pytorch/pull/79438 Test Plan:  Test runs and passes! Differential Revision: D37157391,2022-06-14T23:10:29Z,fb-exported cla signed,closed,0,14,https://github.com/pytorch/pytorch/issues/79576,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79576**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b4d1fbc23f (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37157391,Corresponding test for https://github.com/pytorch/pytorch/pull/79438,This pull request was **exported** from Phabricator. Differential Revision: D37157391,> LGTM! Thanks!   Can you also approve https://github.com/pytorch/pytorch/pull/79438? It's the PR this one is stacked on top of. ,This pull request was **exported** from Phabricator. Differential Revision: D37157391, merge g , successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2505610792,This pull request was **exported** from Phabricator. Differential Revision: D37157391, merge, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2518175005,Closing because code in this PR already committed in https://github.com/pytorch/pytorch/pull/79796
rag,[PyTorch] Don't create a new Storage in FreeMemory unnecessarily,"  CC([PyTorch] Don't create a new Storage in FreeMemory unnecessarily) No reason to go through an extra heap allocation. Differential Revision: D37157595 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-06-14T22:42:21Z,Merged cla signed topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/79573,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79573**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit cec0d99c7e (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,internal benchmarks look good on this one!,"clicked rerun on failed job, waiting for it before merging", merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"Torch FX return error ""`__cuda_array_interface__` must be a dict"""," ðŸ› Describe the bug During development of raftstereo model on torchvision, I get an error `""TypeError: ``__cuda_array_interface__`` must be a dict""` after testing it with `torch.fx.symbolic_trace`. The error comes from this particular line and this test is run on cpu device. Here is a minimum example to reproduce:  The error I got:  Currently I have found a way to fix this by creating global function that is wrapped by `torch.fx.wrap`. Here is the simple code with the fix to illustrate:  I would like to ask if my current fix is appropriate or is there a better suggestion for the fix? Also I think the current error message `""TypeError: ``__cuda_array_interface__`` must be a dict""` is not helpful. It would be great if we can have a better error message. Thanks in advance!  Versions  ",2022-06-14T12:46:11Z,triaged module: fx,closed,4,4,https://github.com/pytorch/pytorch/issues/79513,"I think I know what the problem is. FX tracers support ALL attribute accesses, returning proxies when this happens. torch.tensor tests to see if there is `__cuda_array_interface__` to see if it can treat it as if it were a tensor. If you pass a proxy tensor, proxy tensor is like ""yeah, I got one of those"" and then torch.tensor is like ""tf is this, this is a Proxy, not a dict"" and errors. We can make a nicer error message probably by explicitly testing if the input object is a Proxy and just erroring. But the upshot is you won't be able to symbolic trace through code that uses torch.tensor. Consider using `make_fx` instead!","Hey , do you have any news on this? Having the same issue..",Can you try `make_fx`? :),Sorry I missed your first response . I have tried using `make_fx` and indeed it does work like a charm :) Code after using `make_fx`: 
yi,"Random number generation yields different values on different devices, despite the same manual seed."," ðŸ› Describe the bug Given the **same** manual seed, cpu and cuda generate two different sets of random numbers. See the following code.  But that shouldn't be the case, as it breaks reproducibility. At the beginning of pytorch programs, you often select either the CPU to use as the device, or the GPU if its available. But now, depending on which device is selected, different sets of random numbers will be produced from a program that was otherwise seeded to run a single deterministic way.  Versions Collecting environment information... PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.16.3microsoftstandardWSL2x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Ti Nvidia driver version: 512.95 cuDNN version: Could not",2022-06-14T02:56:21Z,triaged module: random module: numerical-reproducibility module: determinism,closed,0,1,https://github.com/pytorch/pytorch/issues/79496,This is expected https://pytorch.org/docs/stable/notes/randomness.html
rag,Symbolic storage size,  CC(Add support for directly passing symint to empty)  CC(Add support for multiply on direct SymInt)  CC(Symbolic storage size) Signedoffby: Edward Z. Yang ,2022-06-14T02:25:11Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/79492,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79492**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 69ca6f4b1a (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Implement NestedTensor size function," ðŸš€ The feature, motivation and pitch NestedTensor does not implement a size method. NestedTensor has been integrated into nn.Transformer, which has revealed a couple of use cases where the sizes of intermediate tensors in the model are needed. The three main cases are:  PyTorch profiler fails on nn.Transformer because it attempts to find the size of all intermediate tensors, which includes NestedTensors in the transformer.   Autograd with NestedTensor fails because it requires the size of the tensor  Torchscript with NestedTensor fails because torch.jit.script(model) attempts to find the size of intermediate tensors for performance optimizations.  Alternatives Two main ways to fix: 1. In all code that tries to find size of intermediate tensors, check for NestedTensor, and if it's a NestedTensor, do not ask for size. This seems feasible for the above use cases (profiler: has a WIP patch to check for NestedTensor, autograd: WIP in  CC(Add Autograd Support for Nested Tensor), torchscript: could probably make a torchscriptspecific patch). But I imagine that there are many such use cases and it would be hard to cover them all.   2. Implement NestedTensor",2022-06-13T23:31:58Z,module: nestedtensor,closed,3,2,https://github.com/pytorch/pytorch/issues/79477,"Linking https://github.com/pytorch/pytorch/pull/79480 . Thanks for the fix! Unfortunately wasn't able to test this before the branch cut. It looks like it was failing some of the tests, so potentially needs more work. ",Note that the size of a nested tensor is a nested data structure itself. It's unlikely we would ever implement `sizes()` directly for nested tensors as its signature returns an `IntArrayRef`. I think it's better to abstract out callsitespecific querying of tensor size. A good example for autograd is providing an `is_same_size()` abstraction that can be implemented differently for nested vs. nonnested tensors (see CC(update is_same_size to work with nested tensor dispatch)).
yi,[META] Sign up to discuss significantly modifying CI,"Our CI currently tests tens of thousands of tests across many different platforms and compilers. As PyTorch grows and different modules would like to add to our CI, we should meet and discuss the added value of increased testing vs our constraints of CI capacity and time to signal (TTS). We should also discuss any changes that may largely affect CI. Please sign up for a slot (Tuesdays 4:055:00pm ET) below! Please add a topic and an RFC/document that should be prepared ahead of time so we can spend more time in discussion. The PyTorch Dev Infra team will be defacto attendees. **Please include emails so the invites could be sent out accordingly.**  6/14/22 [EXAMPLE] **Topic**: Let's Add More Fun(c) to CI **Presenter(s)**: Firstname Lastname (presenter.com) **RFC/Document**:  CC(RFC: Move functorch into pytorch/pytorch) is a good example **Invitees/Attendees**: Team MemberA (teammateA.com), Team MemberB (teammateB.com), Module ExpertA (expertA.com), etc..  6/21/22 **Topic**:  **Presenter(s)**:  **RFC/Document**:  **Invitees**:   7/12/22 **Topic**: **Presenter(s)**:  **RFC/Document**:  **Invitees**:   7/26/22 **Topic**: **Presenter(s)**:  **RFC/Document",2022-06-13T23:28:00Z,module: ci triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/79476
yi,RuntimeError: Found an argument of type numpy.int64 at index 0. ,I got the following error when using `aot_module` on model fastNLP_Bert and speech_transformer . RuntimeError: Found an argument of type numpy.int64 at index 0. Nontensor arguments must be marked static. Please set the static_argnums correctly to mark the argument at index 0 static. The aot_module is used like below:    ,2022-06-13T18:29:33Z,oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93766,This is not a supported way of calling `aot_module`
transformer,Refactor out QKV in projection,Summary: Refactor to reduce amount of copied code for decoder by finding common chunks for encoder and decoder. QKV in projection is a reasonable unit to copy out. Test Plan: buck run mode/opt c fbcode.platform=platform010 c fbcode.enable_gpu_sections=true c fbcode.nvcc_arch=a100 //pytext/fb/tools:benchmark_transformers  transformer batchsize 64 avgsequencelength 235 maxsequencelength 256 iters 100  module native Benchmark and numerical tests work fine. Reviewed By: mikekgfb Differential Revision: D36138504,2022-06-13T18:05:39Z,fb-exported Merged cla signed release notes: nn release notes: cpp topic: improvements topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/79437,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79437**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit ef77e92aac (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36138504,This pull request was **exported** from Phabricator. Differential Revision: D36138504,This pull request was **exported** from Phabricator. Differential Revision: D36138504,This pull request was **exported** from Phabricator. Differential Revision: D36138504, merge g , successfully started a merge job. Check the current status here,This pull request was **exported** from Phabricator. Differential Revision: D36138504,Merge failed due to New commits were pushed while merging. Please rerun the merge command. Raised by https://github.com/pytorch/pytorch/actions/runs/2504577805, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Performance drops after running tensor multiplication for 15 seconds on M1 MAX (Pytorch MPS).," ðŸ› Describe the bug Performance drops to half of the original performance after running tensor multiplication for 15 seconds on M1 MAX (Pytorch MPS).  Python version: 3.9.7 OS: macOS 12.4 Pytorch version: 1.13.0.dev20220612 The code below reproduces the error:  The above code works fine for RTX 3090 and performance never drops (by changing mps to cuda), but the performance will drop from 71 it/s to 25 it/s after 15 seconds running on M1 Max chip.  Please look into the issue, I suspect that's the reason why training speed of some transformer models dropped after running for a few minutes.  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220612 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.22.3 Libc version: N/A Python version: 3.9.7  (default, Sep 29 2021, 19:24:02)  [Clang 11.1.0 ] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuD",2022-06-13T06:42:11Z,module: performance triaged module: mps,closed,0,5,https://github.com/pytorch/pytorch/issues/79402,"I don't think it is a matmul problem, this behaviour seems to stem from repeatedly calling `torch.rand`. For M1 Max this happen around `40` seconds mark. The speed drop from `~165 its/s` to `~110 its/s`, then `~60 it/s` ",Are you sure it's not related to throttling at all? I only have the Mac Mini but I assume laptops may need to throttle after running for a while.,"> I don't think it is a matmul problem, this behaviour seems to stem from repeatedly calling `torch.rand`. For M1 Max this happen around `40` seconds mark. The speed drop from `~165 its/s` to `~110 its/s`, then `~60 it/s` >  >  Thanks. It does look like the issue of `torch.rand`. The following code works fine for me:  The performance of the above code never throttles. ","For comparison, on an M1 in a Mac Mini, the above is: 12 it/s If I move the x = torch.ones in to the loop, it is 10.5 it/s If I change torch.ones to torch.randn, it is 10.5 it/s It doesn't appear to reduce over time, but my version may be older.  Edit: Confirmed same numbers on today's nightly. So just to note, the torch.rand issue does not appear on the Mac Mini.","I will close the issue, as this is not a `matmul` problem but rather a `torch.rand` problem. Thanks! "
transformer,[PT-D] Use process group of the partial tensor so sub pg comm will be enabled during reshard,"  CC([PTD] Use process group of the partial tensor so sub pg comm will be enabled during reshard) During the debugging for TP enablement for Transformer model, looks like Partial tensor does not use its own pg during resharding while using the default pg instead. Switch to use its own pg as the fix. Differential Revision: D37093468",2022-06-11T23:48:52Z,oncall: distributed Merged cla signed sharded_tensor release notes: distributed (sharded) topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/79357,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79357**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 0e7e424955 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"Autogen Tags enum, and allow specifying tags while defining an op","Stack from ghstack:  CC(Autogen Tags enum, and allow specifying tags while defining an op) clone of https://github.com/pytorch/pytorch/pull/77313. Internal Changes will be coming soon",2022-06-10T21:48:56Z,oncall: jit Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/79322,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79322**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit dc23a6b599 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"MPS: cherry-pick ""add layer_norm_backward"" (#79189)","Layernorm backward.  This is a much needed feature fix for running hf_Bert and many other Transformer networks on MPS backend.  Fixes ISSUE_NUMBER Pull Request resolved: https://github.com/pytorch/pytorch/pull/79189 Approved by: https://github.com/razarmehr, https://github.com/albanD Fixes ISSUE_NUMBER",2022-06-10T14:29:45Z,open source cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/79276,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79276**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 51a96db93a (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
yi,[ao][sparsity] Support for sparsifying data operations on raw torch tensors.,"  CC([ao][sparsity] Support for L2 norm based block data sparsifier)  CC([ao][sparsity] L1 norm based block data sparsifier)  CC([ao][sparsity] Support for embeddings and embedding bags in BaseDataSparsifier)  CC([ao][sparsity] Support for the nn.Parameter in BaseDataSparsifier)  CC([ao][sparsity] Implemented state dict and serialization functionalities)  CC([ao][sparsity] Support for sparsifying data operations on raw torch tensors.)  CC([ao][sparsity] Base class for Data Sparsifier) The users can now pass in raw torch tensors and the base class handles all the parametrizations and masking Example      >>> data_list = [('tensor_1', torch.randn(3,3)), ('tensor_2', torch.randn(4,4))]     >>> defaults = {'sparsity_level': 0.7}     >>> sparsifier = DerivedDataSparsifier(data_list = data_list, **defaults)  Some sparsifier that inherits BaseDataSparsifier     >>> new_tensor_to_add = {'name': 'tensor_3', 'data': torch.randn(5,5), 'sparsity_level': 0.3}     >>> sparsifier.add_data(**new_tensor_to_add)     >>>  tensor_1 and tensor_2 will have sparsity_level of 0.7 but tensor_3 will have sparsity_level=0.3 Test Plan:  Differential Revision: D37164121",2022-06-10T00:09:43Z,Merged cla signed,closed,0,9,https://github.com/pytorch/pytorch/issues/79252,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79252**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 44833a20a0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,More descriptive name for the PR title is needed. `Support for toech tensors` doesn't tell a lot about where is this support added," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Compilation failure seen on 1.12 rc2 when trying to use pretty_print_onnx function," ðŸ› Describe the bug On 1.12rc2, Getting compilation error if trying to use function ""torch::jit::pretty_print_onnx"" by including file  Getting following error: /sitepackages/torch/include/torch/csrc/jit/serialization/export_bytecode.h:11:10: fatal error: torch/csrc/jit/mobile/function.h: No such file or directory 11 | include  This is because of no ""mobile"" folder in torch installation. Probably 'include/torch/csrc/jit/mobile/*.h' needs to be included in setup.py  Versions Collecting environment information... PyTorch version: 1.12.0a0+gite1edcac Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.2 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 12.0.1 (ssh://gerrit:29418/tpc_llvm10 4f0317d1e33979c9fd564a3e129129f6d711409d) CMake version: version 3.20.2 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.090genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runt",2022-06-09T06:57:26Z,oncall: jit,closed,0,2,https://github.com/pytorch/pytorch/issues/79190,Are you have a problem compiling PyTorch or compiling something that includes PyTorch headers?,"I am seeing compiliation failure with our builds that uses Pytorch headers. Pytorch compiles OK. It is due to diff in ""torch\\csrc\\jit\\serialization\\export.h"", which now includes include  which has references to  include  include  This ""mobile"" folder is not found in Torch .whl pkg."
transformer,[transformer] BT enablement on fairseq - pytorch change,"The fairseq diff is split into two parts. The first diff (this one) This diff is about creating a mask left align function to check the mask condition for nested tensor. It is necessary for torchscript deployment. The second diff (D37082681) Fork the inference path inside the forward function. If loaded the checkpoint file and perform the inference, we will deploy BT. Otherwise, fairseq take the position. Reviewed By: mikekgfb Differential Revision: D36057338",2022-06-09T05:45:46Z,fb-exported Merged cla signed release notes: nn topic: bug fixes topic: performance,closed,0,10,https://github.com/pytorch/pytorch/issues/79186,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79186**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9c624344d4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36057338,This pull request was **exported** from Phabricator. Differential Revision: D36057338,This pull request was **exported** from Phabricator. Differential Revision: D36057338,This pull request was **exported** from Phabricator. Differential Revision: D36057338,"> Confused about this  I just see this PR adding a `_nested_tensor_from_mask_left_aligned` op. Some questions: >  > * How does this related to the PR summary? AFAICT just adding this op won't enable its use. The diff is optimized to split into two parts now. Part1 only includes this op change and associated with D36057338. Part2 only includes fairseq change (D37082681) but depend on Part1. The internal CI tests show the op runs well. > * How does this differ from `_nested_tensor_from_mask`  does it just avoid checks by making the assumption the mask is leftaligned? If so, couldn't we avoid quite a bit of duplication? It is the front part of ""_nested_tensor_from_mask"" implementation. But the purpose is to help check the left aligned in advance. ",This pull request was **exported** from Phabricator. Differential Revision: D36057338, merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey wei. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,MPSNDArray Error: buffer is not large enough," ðŸ› Describe the bug Installed latest PyTorch nightly build for MPS on M1, 1.13.0dev20220608, set environmental variable PYTORCH_ENABLE_MPS_FALLBACK=1 to resolve issue with aten::index.Tensor. Ran StableBaselines3 to train a PPO agent. Conducted one successful iteration then received an error:     Versions  ",2022-06-09T02:41:49Z,high priority triaged module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/79181,"Likely a duplicate of  CC([Device MPS] Error: buffer is not large enough. Must be 19200 bytes) And I can reproduce the crash easily (though `TORCH_SHOW_CPP_STACKTRACES=1` does not work for it :( ), so here is the backtrace with `lldb` attached: ",Here is the one line reproducer to the problem: ,"We aren't able to reproduce this issue, could you please update to latest nightly and try again?"
transformer,`cumsum` op: pytorch failed to run GPT-2 model in M1's MPS device," ðŸ› Describe the bug My transformers inference script is running successfully in device CPU, but when using device MPS in MacOS M1 Pro, it will report 'aten::cumsum.out' op is missing, so I set environment variable 'PYTORCH_ENABLE_MPS_FALLBACK', but it will report the next error for huggingface transformers GPT2 model:   Script using huggingface transformers version 4.19.2   Versions Collecting environment information... PyTorch version: 1.13.0.dev20220601 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.0.0 (clang1300.0.29.30) CMake version: Could not collect Libc version: N/A Python version: 3.8.13  (default, Mar 25 2022, 06:05:16)  [Clang 12.0.1 ] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.0rc2 [pip3] torch==1.13.0.dev20220601 [pip3] torchaudio==0.14.0.d",2022-06-08T07:02:33Z,triaged module: mps,closed,6,2,https://github.com/pytorch/pytorch/issues/79112,"Hi , thanks for the issue. Currently we don't have support for `cumsum` op in MPS layer but looking into it. As we have the API support available we will enable it for MPS backend.","> x = torch.addmm(self.bias, x.view(1, x.size(1)), self.weight) > RuntimeError: tensors must be 2D  this crash is fixed in the latest pytorch nightly  please give a try and let me know if you still see any issues. >  it will report 'aten::cumsum.out' op is missing For op support, please add a request here:  CC(General MPS op coverage tracking issue)."
rag,Add operator and derivative coverage for nestedtensors, Summary Compared to the out of tree nested_tensor impl: https://github.com/pytorch/nestedtensor core does not have as many nestedtensor implementations.  As well since the out of tree version did not support autograd the derivative implementations are not defined yet.    Operator Coverage forward and backward defined  Factory Functions:  [x] _nested_tensor_from_mask  [x] _nested_tensor_from_padded  [x] to_padded_tensor  [x] nested_tensor  MHA Ops:  [x] linear  Other Ops  [x] numel  Linked PR:  CC(Add factory function derivatives)  CC(implement numel and tests for nested tensor)  CC(Register nested tensor linear kernel) ,2022-06-07T18:58:53Z,module: autograd triaged module: nestedtensor,closed,0,1,https://github.com/pytorch/pytorch/issues/79044,Looks like this is completed
yi,Add a test that shows that lazy_ir reuse breaks SizeNodes,I suspect Bin's lazy_ir reuse breaks `SizeNode` as these eventually point to DeviceData leaves and are updated after each mark_step whereas `SizeNode` persist across multiple mark_steps,2022-06-07T17:44:23Z,triaged lazy,open,0,2,https://github.com/pytorch/pytorch/issues/79032,cc:  ,"Okay, I wrote a test : https://github.com/Krovatkin/pytorch/pull/new/krovatkin/reuse_sym"
transformer,Add check for no grad in transformer encoder nestedtensor conversion â€¦,"Cherry picked bug fix for 1.12 release. Summary: Before, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail. Fix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor. Original PR: https://github.com/pytorch/pytorch/pull/78832",2022-06-07T17:30:40Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/79029,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79029**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 2c95c31d38 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6783296606?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/1) **Step:** ""Download build artifacts"" (full log  :repeat: rerun)   20220607T22:04:23.0970066Z [error]No files ...ath: test/**/*.xml. No artifacts will be uploaded.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","Test failure is numerically different 1/72 tensors and is for convolution, which this diff does not affect (only transformer). Likely a flaky test. "
transformer,Can we have Additive Attention?," ðŸš€ The feature, motivation and pitch Like MultiheadAttention which was proposed in **`Transformer`** paper similarly, can we have Additive attention  proposed in `**Neural Machine Translation by Jointly Learning to Align and Translate**`.  Alternatives `nn.MultiheadAttention`  Additional context _No response_ ",2022-06-06T18:38:56Z,triaged,open,0,2,https://github.com/pytorch/pytorch/issues/78954, ,"Hey there   apologies for the wait! I don't believe we have immediate plans to implement this, so if we were to do this the eta would look like a couple months out. "
rag,[PyTorch] (reapply) Avoid initializing storage for empty Optionals,"  CC([PyTorch] (reapply) Avoid initializing storage for empty Optionals) We don't need to initialize for the nonconstexpr case ever, or in the constexpr case after C++20. Differential Revision: D36519379",2022-06-06T17:30:03Z,oncall: distributed Merged cla signed ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/78947,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78947**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 1cd91cb799 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,reapply of https://github.com/pytorch/pytorch/pull/77858 which was reverted due to ASAN failures; I've piggybacked a patch (see rpc_agent.cpp),suddenly test_fs_sharing is failing. not a believer; rebasing, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Support indexing of the underlying tensors for nested tensors,Fixes CC(Support indexing of the underlying tensors for nested tensors),2022-06-06T15:49:06Z,Merged cla signed topic: not user facing release notes: nested tensor,closed,0,7,https://github.com/pytorch/pytorch/issues/78934,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78934**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 3ce833bca2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Generally looks great! Thank you for sending this!,"  could you also try to rebase or merge master please? There is a conflict that seems to prevent CI from running, which can hide lint failures and the like.",">   could you also try to rebase or merge master please? There is a conflict that seems to prevent CI from running, which can hide lint failures and the like. just merged master. CI is running now", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
finetuning,[Python 3/Pytorch] Error on Fine-tuning BERT for question answering -> RuntimeError: Overflow when unpacking long," ðŸ› Describe the bug While trying to finetune a BERT model on COVIDQA dataset for the question answering task, the following error has occurred.  _RuntimeError: Overflow when unpacking long_ The instructions I am following are on this link. The error seems to occur on the following line: ` return {key: torch.tensor(val[idx],dtype=torch.int64) for key, val in self.encodings.items()}` I am trying to train the model on an ubuntu server, using python 3.6.6 and PyTorch 1.10.0+cu102 (using 1 GPU) Can anyone help?  Versions Collecting environment information... PyTorch version: 1.10.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.10.2 Libc version: glibc2.9 Python version: 3.6.6  (default, Oct  9 2018, 12:34:16)  [GCC 7.3.0] (64bit runtime) Python platform: Linux4.15.0180genericx86_64withdebianbustersid Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration: GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX Nvidia driver version: 470.129.06 cuDNN ver",2022-06-06T13:40:12Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/78925,"This is probably not a framework problem, I suggest going to https://discuss.pytorch.org/",r
transformer,Add check for no grad in transformer encoder nestedtensor conversion,"Before, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail. Fix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.",2022-06-03T17:52:03Z,Merged cla signed release notes: nn topic: bug fixes,closed,0,3,https://github.com/pytorch/pytorch/issues/78832,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78832**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 8df8a9f742 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Add methods to torch._C pyi,"Add python bound methods in `Block`, `Node`, `Graph` and `Value` to `torch._C.__init__.pyi` to enable proper type hints in editors.",2022-06-02T20:26:39Z,module: cpp module: typing triaged open source Merged cla signed release notes: jit,closed,0,10,https://github.com/pytorch/pytorch/issues/78757,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78757**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit cfe02c31e1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," given that these are all jit objects, can you take a look at this?", merge g, successfully started a merge job. Check the current status here, merge, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp) are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2450881856, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Getting NotImplementedError when trying to implement E2E support for `prim::is_nested` Op in torch-mlir.," ðŸ› Describe the bug Hello, I am trying to add support for the Prim::is_nested Op in torchmlir. I have posted a similar issue in torchmlir: https://github.com/llvm/torchmlir/issues/880 and this PR https://github.com/llvm/torchmlir/pull/881 points to the commit in question. It has the corresponding lowering code and E2E test code. When trying to run the nested test case I get the following error summary. Kindly advice on the best way to debug the error.   Versions Collecting environment information... PyTorch version: 1.13.0.dev20220523+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04 LTS (x86_64) GCC version: (Ubuntu 9.4.05ubuntu1) 9.4.0 Clang version: 13.0.0 CMake version: version 3.22.4 Libc version: glibc2.35 Python version: 3.9.0 (default, May 19 2022, 12:51:15)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.16.0051600genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of rel",2022-06-02T20:04:49Z,triaged module: nestedtensor,open,0,3,https://github.com/pytorch/pytorch/issues/78754,"Since ""clone"" is marked as a missing operation let's start with implementing that first, since it's an operation we'll probably want anyway. However, it's unlikely that this will resolve the issue.",999  I just landed support for clone (which will be in the next nightlies). Can you try again once it became available?,yup sounds good. Thanks!
agent,Update distributed/CONTRIBUTING.md to remove ProcessGroupAgent references and add test instructions,Stack from ghstack:  CC(Reenable assert after test update) Reenable assert after test update * * CC(Update distributed/CONTRIBUTING.md to remove ProcessGroupAgent references and add test instructions) Update distributed/CONTRIBUTING.md to remove ProcessGroupAgent references and add test instructions** ProcessGroupAgent was deprecated so these references to the tests should be removed from comments and docs.  ,2022-06-01T16:21:36Z,oncall: distributed Merged cla signed release notes: distributed (rpc) topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/78625,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78625**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 1dfc0dd653 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6696616477?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220601T19:46:05.2303512Z The PR is introduc...m to confirm whether this change is wanted or not.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge this on green,"Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Trying to get some OpInfo based test for MPS,"This does a few changes:  new `TORCH_CHECK` to make sure the mps backend returns nice error instead of hard crash  make print behavior smoother by converting to cpu   Add an OpInfobased test to make sure mps output matches the cpu one There are several issues with the opinfo test:  One test just hangs forever  Some function still hard crash and probably need more TORCH_CHECK  Many functions are failing in a flaky way: they sometimes pass, sometimes fail (usually because the output is all 0s) Next steps:  Fix all the things in the `BLOCKLIST`  Make `ALLOWLIST_OP` not depend on dtype anymore (so that we can just enable ops directly via a boolean flag). This is not done now because many op fail int/bool dtypes.  Move `ALLOWLIST_OP` to be a boolean flag directly in `common_method_invocations.py` like `supports_mps=True`.  Make `supports_mps=True` the default for all ops  Extend this to check that gradients match What needs to happen before merging:  Ensure all the `TORCH_CHECK()` are not overly restrictive  Ensure the newly added test is not flaky (add more stuff to the `BLOCKLIST`)",2022-05-30T22:30:44Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/78504,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78504**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit ea534eccd4 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6717530537?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7 / test (default, 1, 4, linux.4xlarge.nvidia.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220603T00:47:15.3661284Z RuntimeError: test_ops failed! Received signal: SIGIOT     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",Closing as this landed in another PR.
transformer,torch.fx: symbolic_trace: ones() received an invalid combination of arguments," ðŸ› Describe the bug Run the following standalone python file.  I get the error:  Full error log: https://gist.github.com/silvasean/a11f5219e8d931014ae4046a1fafcef7  Versions PyTorch version: 1.13.0.dev20220530+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Debian GNU/Linux rodete (x86_64) GCC version: (Debian 11.2.019) 11.2.0 Clang version: 13.0.13+build2 CMake version: version 3.22.4 Libc version: glibc2.33 Python version: 3.9.12 (main, Mar 24 2022, 13:02:21)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.17.61rodete1amd64x86_64withglibc2.33 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] ireetorch==0.0.1 [pip3] numpy==1.23.0rc1 [pip3] torch==1.13.0.dev20220530+cpu [pip3] torchmlir==20220530.482 [pip3] torchvision==0.14.0.dev20220530+cpu [conda] Could not collect ",2022-05-30T12:59:44Z,triaged module: fx,open,0,2,https://github.com/pytorch/pytorch/issues/78487,"This also affects any model using deformable convolutions, e.g., trace_dcn_bug.py gives  I suspect this is related to CC(torch.fx.symbolic_trace fails on torch.arange with inputdependent size) with a potential solution provided by ModelTC/MQBench CC(Data loading processes should disable OpenMP).",May I know whether there is any applicable solution to this issue? I'm also facing similar issue where I deserialize the torch.fx GraphModule which and dumped using pickle.
transformer,[onnx] RuntimeError: Attribute 'axes' is expected to have field 'ints'," ðŸ› Describe the bug RuntimeError: Attribute 'axes' is expected to have field 'ints' When I want to export the transformer model containing the proj_adaptive_softmax layer to the onnx format, I get an error: RuntimeError: Attribute 'axes' is expected to have field 'ints', ==> Context: Bad node spec: input: ""567"" output: "" 584"" name: ""Unsqueeze_445"" op_type: ""Unsqueeze"" attribute { name: ""axes"" type: INTS } It means that axes type must be int. I found the corresponding line of code: ` It means that indices_i must be a tensor of type int, but the problem is that indices_i is already a tensor of int64. When I replace it with a constant tensor equal to indices_i, no error is reported ` Why is this?  Versions Pytorch Version == 1.7.1+cu101 OS == Linux onnx == 1.10.2 CUDA Version == 10.1",2022-05-30T03:09:12Z,module: onnx triaged onnx-needs-info,closed,0,5,https://github.com/pytorch/pytorch/issues/78481,"hi , could you please try a newer version of PyTorch (current release is 1.11)? If error persists, please create a complete end to end repro for us to investigate. Thanks.", I have updated the version of torch but still the same error occurs Pytorch Version == 1.11.0+cu102 OS == Linux onnx == 1.10.2 CUDA Version == 10.2 Below is my main codeï¼š  model.py ,"Hi  We are happy to help  please help us by providing a minimal script that can reproduce what you have seen, using the latest pytorch nightly build. Ideally it should be a single script so that we can run and diagnose. ",Single repro file by merging the above  Now observing error `torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::_scaled_dot_product_attention' to ONNX opset version 12 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.` on pytorch '2.0.0a0+git1997753'.,"Closing this as although `scaled_dot_product_attention` op has been added to torch.onnx.export, aten::unflatten hasn't and we don't plan adding new operators to torch.onnx.export Please try the new ONNX exporter and reopen this issue if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial"
transformer,13% performance regression in MPS since d63db5234," ðŸ› Describe the bug I have been testing PyTorch MPS support in spaCy over the last week or so. Performance has been great, though I have noted a somewhat large performance regression since yesterday. With bisecting I found that it occured in d63db52349ae3cffd6f762c9027e7363a6271d27. It's kind of hard to provide a minimal reproducer, but we are basically using the BERT model from Huggingface transformers with the following change:  Without this change, the attention scores are incorrect ( I still have to look a bit deeper to understand what is causing this particular problem). But with the change above, we can reproduce CPU accuracies. I have been testing with out German transformer model, which is a finetuned BERT model. * Before d63db52349ae3cffd6f762c9027e7363a6271d27: ~7700 words per second. * After d63db52349ae3cffd6f762c9027e7363a6271d27: ~6700 words per second.  Versions ",2022-05-29T07:31:54Z,triaged module: mps,closed,0,4,https://github.com/pytorch/pytorch/issues/78472,"Inserting calls to `contiguous()` around transpose/view is likely to have performance impact. I guess it would be better to highlight wrong accuracy in your issue description; now it seems like a performance problem at the first glance, which it is not IMHO.  The workaround would surely be helpful for debugging though."," I donâ€™t agree. Even though needing `contiguous` is an issue that should be addressed, it is still a performance regression. Doing the same sequence of operations is slower than before.","> I donâ€™t agree. Even though needing `contiguous` is an issue that should be addressed, it is still a performance regression. Doing the same sequence of operations is slower than before. , could you please try the following build from this page https://hud.pytorch.org/pr/78496 and let me know if you are still seeing the issue? Latest nightly build also contains them (1.13.0.dev20220531). If you are manually building PyTorch, sha 017b0ae9431ae3780a4eb9bf6d8865dfcd02cd92 contains these changes (https://github.com/pytorch/pytorch/pull/78496). ","I think the performance regression is largely gone (it's about 7600 words per second now). Furthermore, I only need one `contiguous` call now. I opened a separate issue about that yesterday ( CC(MPS matrix multiplication fails on a permuted tensor without `contiguous`))."
rag,Some helper code for determining missing meta coverage for XLA ops,  CC(Make Meta into a backend component)  CC(Register std_mean ref as a decomposition)  CC(Don't check for linalg errors on meta tensors)  CC(Reenable TestMeta native_batch_norm and native_layer_norm)  CC(Reenable TestMeta slice)  CC(Some helper code for determining missing meta coverage for XLA ops)  CC(Reenable TestMeta testing for isnan)  CC(Reenable tensor_split meta testing)  CC(prod ref)  CC(Register PrimTorch sum as a decomposition.)  CC(Unconditionally transform dtype arguments to double for upcast) When I ran it I got this:  Signedoffby: Edward Z. Yang ,2022-05-29T02:03:53Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/78464,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78464**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 769bd464d1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,[cuDNN][TF32] Threshold adjustments for TF32 on `>=sm80`,CC    Change to transformer multilayer test can potentially be swapped in favor of an rtol change? (see also: CC(Sets rtol>0 for some tests that use tf32)).,2022-05-27T22:15:02Z,module: cudnn triaged open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/78437,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78437**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 New Failures As of commit e543f0199c (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6632612554?check_suite_focus=true) pull / linuxfocalpy3.7gcc7 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220527T22:29:54.0717919Z The PR is introduc...m to confirm whether this change is wanted or not.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,fx.Tracer with param_shapes_constant=True not working for RobertaForMaskedLM," ðŸ› Describe the bug param_shapes_constant=True can't bypass issue mentioned by    CC([FX] Limited control flow tracing for parameter shapes)  Error Message:   Versions PyTorch version: 1.10.2 Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.102.1microsoftstandardWSL2x86_64withglibc2.31 Is CUDA available: False CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA T500 Nvidia driver version: 472.91 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.5 [pip3] numpydoc==1.2 [pip3] torch==1.10.2 [pip3] torchvision==0.11.3 [conda] blas                      1.0                         mkl [conda] cudatoolkit               11.3.1               h2bc3f7f_2 [conda] ",2022-05-27T21:33:23Z,triaged module: fx,open,0,0,https://github.com/pytorch/pytorch/issues/78435
transformer,Torchdynamo for Deepspeed and FSDP,"In AWS we are working with customers to enable gigantic transformer model training on EC2. Furthermore, we attempt to leverage compiler techniques to optimize Pytorch workloads due to its widely adoption. For example, we recently opensourced RAF, a deep learning compiler for training, that shows promising training acceleration for a set of models and verified it works with TorchDynamo. On the other hand, we do see the gap converting Pytorch programs to the IR we are using, especially when it comes to complex strategies such as distributed training. One example is Deepspeed. It implements data parallelism (ZeRO) on top of Pytorch, introducing sharding for optimizer states, gradients and parameters. (FSDP is another approach to do ZeRO). The idea itself is pretty straightforward, but when trying to convert the Deepspeed implementation to RAF via lazytensor, it doesnâ€™t work well. For instance, the NaN check for gradients breaks the graph and result performance degradation; It doesnâ€™t capture CUDA stream usage; etc. In my understanding, those issues could potentially be resolved via Torchï¼¤ynamo as it has the capability to extract structures and call inf",2022-05-26T21:46:51Z,oncall: distributed feature low priority triaged module: fsdp oncall: pt2 module: dynamo,open,7,3,https://github.com/pytorch/pytorch/issues/93756,'re looking into this and will post an update when we can.," We have support for FSDP and DDP with graph breaks currently, you may have seen these announced.  However we're also working on traceable collectives as part of an effort to make it possible to do more than just graphbreaks.  CC([RFC] PT2-Friendly Traceable, Functional Collective Communication APIs)","Pull requests are welcome to improve this, but this won't be a short term priority for H1."
transformer,Add nn.module activation support in BetterTransformer,"Summary: textray is using nn.module as activation function params, since functional is not scriptable in textray's module. Therefore we should support this in nn.module as well. Test Plan: CI Differential Revision: D36678078",2022-05-26T21:38:09Z,fb-exported Merged cla signed release notes: nn topic: performance,closed,0,8,https://github.com/pytorch/pytorch/issues/78394,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78394**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures, 1 Base Failures As of commit 65579265f3 (more details on the Dr. CI page): Expand to see more  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base c88367442d on May 31 from  1:36pm to  4:14pm   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6678786767?check_suite_focus=true) pull / linuxfocalpy3.7gcc7mobilelightweightdispatchbuild / build (1/1) **Step:** ""Build"" (full log  :repeat: rerun)   20220531T21:32:30.3692209Z [error]Process completed with exit code 137.      :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * [pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) on May 31 from  1:36pm to  4:14pm (96c134854d  d71816a51b)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D36678078,This pull request was **exported** from Phabricator. Differential Revision: D36678078," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D36678078,This pull request was **exported** from Phabricator. Differential Revision: D36678078,  merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[docs] Move a sentence from `nn.Transformer` to `nn.TransformerEncoder`,"`nn.Transformer` is not possible to be used to implement BERT, while `nn.TransformerEncoder` does. So this PR moves the sentence 'Users can build the BERT model with corresponding parameters.' from `nn.Transformer` to `nn.TransformerEncoder`. Fixes CC([docs] `nn.Transformer` is not possible to be used to implement BERT)",2022-05-26T03:18:57Z,open source Merged cla signed release notes: nn topic: docs,closed,0,4,https://github.com/pytorch/pytorch/issues/78337,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78337**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (3 Pending) As of commit e9673b306d (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!, merge this please
rag,Code coverage?," ðŸš€ The feature, motivation and pitch The code coverage on codecov: https://app.codecov.io/gh/pytorch/pytorch is 8 months old and it doesn't seem to receive new coverage information. Is there a place where we can find test coverage information without having to run any code locally?  Alternatives _No response_  Additional context _No response_",2022-05-26T00:50:00Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/78328,"Hi, The coverage tool was very flaky and no one was maintaining the integration on our end. So it was discontinued I'm afraid.","> Hi, >  > The coverage tool was very flaky and no one was maintaining the integration on our end. So it was discontinued I'm afraid. I see. Thanks for the info",I am closing this Issue for now as the question has been responded. Feel free to reopen it if the question is not resolved.
transformer,[PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path) Straightforward after previous diffs in stack cleaning up code and adding test coverage. Differential Revision: D36564008,2022-05-25T16:55:15Z,Merged cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/78269,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78269**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9f0925688b (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,XLA is an infra flake, merge f, successfully started a merge job. Check the current status here,Merge failed due to Command `git C /home/runner/actionsrunner/_work/pytorch/pytorch cherrypick x a51ec04800a4329500682925ce50c06c34e2c33d` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/2692324992, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add test_modules test for TransformerEncoderLayer fast path,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path) Extend the existing TransformerEncoderLayer test to cover the fast path. Differential Revision: D36564009,2022-05-25T16:55:09Z,Merged cla signed topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/78268,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78268**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 789d0a70af (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here, your PR has been successfully merged.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add fused kernel for binary_cross_entropy_with_logits,"  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path)  CC([PyTorch] Add fused kernel for binary_cross_entropy_with_logits)  CC([PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits)  CC([PyTorch] Clean up native transformer implementation) I noticed this does a ton of extra dispatches (and, on CUDA, kernel launches), so I'm adding fused kernels. Differential Revision: D36650625 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-05-25T16:55:03Z,cla signed Stale,closed,0,4,https://github.com/pytorch/pytorch/issues/78267,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78267**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 14 New Failures, 1 Flaky Failures As of commit 97d17639f4 (more details on the Dr. CI page): Expand to see more  * **14/15** failures introduced in this PR * **1/15** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs   :detective: 14 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6675060577?check_suite_focus=true) pull / linuxxenialpy3.7gcc7 / build (1/14) **Step:** ""Build"" (full log  :repeat: rerun) :snowflake:   20220531T18:28:32.0177570Z unknown file: Failure     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",> Got any benchmark results? no. what benchmark?,> no. what benchmark? Just curious about general perf on CPU and CUDA before and after the fused kernel.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path)  CC([PyTorch] Add fused kernel for binary_cross_entropy_with_logits)  CC([PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits)  CC([PyTorch] Clean up native transformer implementation) Saves a refcount bump. Differential Revision: D36650626,2022-05-25T16:54:58Z,Merged cla signed release notes: nn topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/78266,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78266**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c4aed91562 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Clean up native transformer implementation,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path)  CC([PyTorch] Add fused kernel for binary_cross_entropy_with_logits)  CC([PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits)  CC([PyTorch] Clean up native transformer implementation) In preparation for supporting norm_first Differential Revision: D36564011,2022-05-25T16:54:52Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/78265,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78265**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 39921e047b (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,Any plan to add Noam scheduling?," ðŸš€ The feature, motivation and pitch I found Noam scheduling(from ""Attention is all you need"" paper) important for training transformers. But it's not included in torch. Do you have plan to add it or want others to add it?  Alternatives _No response_  Additional context _No response_",2022-05-25T12:22:13Z,triaged module: LrScheduler,open,0,2,https://github.com/pytorch/pytorch/issues/78253,I think the idea is that you could use/subclass LambdaLR with ease.,Right. I can implement it with LambdaLR or adopt other implementations. I think it's about policy or plan of this project whether to include Noam scheme. So opened this issue. And you mean it doesn't look good to provide that scheme in pytorch?
rag,Add from_blob with storage_offset arg,  CC(Add from_blob with storage_offset arg) Seperated out from other diff to better comply with github first Differential Revision: D36644764,2022-05-24T21:53:20Z,Merged cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/78217,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78217**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 08ad746024 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp), winvs2019cpupy3 / build, winvs2019cuda11.3py3 / build are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2381160006", merge this please,"It would also be nice to expose `from_blob` to Python:  CC(Support creating a CPU tensor from ctypes pointer in Python / from_blob(ptr, shape, strides, dtype))","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,DOC Corrects default value for storage_offset in as_strided,Fixes CC(Documentation bug for torch.as_strided),2022-05-24T20:48:00Z,module: docs triaged open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/78202,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78202**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c7d83c0a07 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
finetuning,torch.jit.script gives a RuntimeError for a custom MaskRCNN model," ðŸ› Describe the bug We complied a MaskRCNN model shown in [TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL] (https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) with torch.jit.script. They define the model as mentioned below:  torch.jit.script works perfectly for this example. Please find the Google Colab notebook link for the working example here. We then wrapped the ""get_model_instance_segmentation()"" function inside a nn.Module class as shown below:  The CustomModel trains and tests correctly. However, it fails when we try to compile it with the torch.jit.script. The runtime error is as follows:  The link to the Google Colab notebook when CustomModel fails compilation is here. Both notebooks only contain the minimum required code.  Versions PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.4 Libc version: glibc2.26 Python version: 3.7.13 (default, Apr 24 2022, 01:04:09)  [GCC 7.5.0] (64bit runtime) Pyth",2022-05-24T17:49:06Z,oncall: jit,closed,1,3,https://github.com/pytorch/pytorch/issues/78188,"Can you annotate the type of `images` in the forward method?  The error message suggests that `self.model(..)` expects an argument that is a list of tensors. Since `images` is not annotated, torchscript assumes that it is a tensor. If it's not a tensor, it needs to be annotated as a tensor.",Thank you. It works.,Why didn't you use detectron2 instead torchvision?
yi,Allow specifying pickle module for torch.package," ðŸš€ The feature, motivation and pitch I'm working on a service that is to receive files created with `torch.package` to instantiate `torch.nn.Module`s created by a different service. Some of the modules use, somewhere in their structure, lambda functions and other constructs that are not supported by `pickle`, but are supported by the `dill` package. Seeing as how the `torch.save` and `torch.load` functions both support specifying a `pickle_module` to use, I was wondering if it was possible to support specifying it when instantiating `torch.package.PackageExporter` and `torch.package.PackageImporter`, so I could take advantage of the dependency bundling they provide?  Alternatives I can use `torch.save` and `torch.load` with `dill` as the `pickle_module`, though that in and out of itself doesn't bundle any dependencies needed to instantiate the module later on, and so I'd need both services to have the classes and methods, and updates on one side won't be reflected on the other. I could also copy the source code of `torch.package`, make the necessary adjustments to it so that it uses `dill`, and use that code instead, but I'd have to make sure it is ",2022-05-24T12:19:47Z,enhancement oncall: package/deploy imported,open,5,0,https://github.com/pytorch/pytorch/issues/78172
transformer,fix typo in docstring of `Transformer.forward()`,"Fixed the word ""decode"" to be ""decoder"".",2022-05-24T07:19:49Z,open source Merged cla signed release notes: nn topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/78167,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78167**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b44ac4b040 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Device MPS: -> RuntimeError Placeholder buffer size is not large enough to contain the Tensor storage of size," ðŸ› Describe the bug Hello to everyone, I am trying to use the torch nightly version which supports the M1 drivers. My machine is the M1 Pro 14'' base version (16GB Ram, 512GB SSD, 8 CPU cores). I have tried to install the nightly version in two different ways, but I got the same issue. In both ways, I have created a new conda env. 1)  2)  The error:  The code:   Versions Collecting environment information... PyTorch version: 1.13.0.dev20220521 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2) CMake version: version 3.23.0 Libc version: N/A Python version: 3.10.2 (main, Feb  2 2022, 05:51:25) [Clang 13.0.0 (clang1300.0.29.3)] (64bit runtime) Python platform: macOS12.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.6 [pip3] pytorchlightning==1.6.0 [pip3]",2022-05-21T13:00:53Z,module: macos module: mps,closed,0,7,https://github.com/pytorch/pytorch/issues/78042,I did not experience this on `1.13.0.dev20220522`. Can you update and try again?,"I had this problem with 1.13.0.dev20220521, but it went away with 1.13.0.dev20220522.",seems like the problem no longer exist in the latest nightly. I am closing this issue.  please feel free to reopen if the problem persist,"I saw this error printed only after another error occured (the model wasn't converted to mps, but the input was). Just in case you have the same issue.","Hi, Yes, I confirm the issue is fixed in updating to the version `1.13.0.dev20220522`. Thank you for the support  ,  , . Thank you also to , now the issue is solved on my side and as you can see from the code posted I have moved the model and inputs to the mps device. Kind regards, Andrea Bacciu","Hi, I am too facing the issue. System: Macbook M1 Pro 14'' base model Relevant packages versions: torch                         2.0.0.dev20221213 torchmetrics                  0.11.0 torchvision                   0.13.1a0 The error which I am getting: `RuntimeError: Invalid buffer size: 9.75 GB` For a reason, I am predicting the results using the trained model on the entire test data(test data size ~ 19962, RGB image). Dataset Used  celebA's test data  when the third line is executed, the above error is prompted. Is it due to the limitation of my hardware or is it someother issue?","> Hi, I am too facing the issue. >  > System: Macbook M1 Pro 14'' base model >  > Relevant packages versions: torch 2.0.0.dev20221213 torchmetrics 0.11.0 torchvision 0.13.1a0 >  > The error which I am getting: `RuntimeError: Invalid buffer size: 9.75 GB` >  > For a reason, I am predicting the results using the trained model on the entire test data(test data size ~ 19962, RGB image). >  > Dataset Used  celebA's test data >  >  >  > when the third line is executed, the above error is prompted. Is it due to the limitation of my hardware or is it someother issue? I get the same result with any batch_size other than 1"
rag,Move THPStorage definitions out of `torch/csrc/generic`,Fixes CC(Move C++/Python Storage bindings out of `torch/csrc/generic`),2022-05-21T02:18:27Z,module: internals open source Merged cla signed Reverted ciflow/binaries,closed,0,13,https://github.com/pytorch/pytorch/issues/78032,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78032**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: 1 Base Failures As of commit 4530f9b96b (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 5b922c29e9 since May 31   :construction: 1 ongoing upstream failure: These were probably **caused by upstream breakages** that are **not fixed yet**. * pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) since May 31 (a88f155f4b)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",unfortunately bazel error looks real , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""This broke windows binary builds, see: https://hud.pytorch.org/pytorch/pytorch/commit/f0121528364f6023c69f49e69fabc00863a5ef57"" c ""nosignal""",Here's the important part of the log of the failure:  I'm not sure where this `copysign` is coming from. I'm not calling it in `StorageMethods.cpp` or `StorageSharing.cpp`. I guess it's possible that I removed an include file that was required. I'll see what I can figure out,"I added this to `StorageMethods.cpp` and `StorageSharing.cpp`:  Not sure if it'll work, but it did jump out to me since it's a windowsspecific include that these files used to have access to, which I had removed","Hmmm, it looks like `wheelpy3_7cuda11_3build` wasn't run, even though we have the `ciflow/all` label. Not sure what to do about that. Maybe I need to make a new PR",I noticed that `c10/util/math_compat.h` has some `std::copysign` definitions. So I've included that file in `StorageMethods.cpp` and `StorageSharing.cpp`. Hopefully that fixes the error,"It's still failing. Since I'm having trouble figuring out which include is missing, I think I should try just copying all of the includes from `torch/csrc/Storage.cpp` into `StorageMethods.cpp` and `StorageSharing.cpp`. Even though it's overkill, I'm pretty sure that would fix the error","The XLA failure appears to be upstream, so I think this should be good to go now", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[flatbuffer] Move saving storage to the last step.,"Summary: Move storage saving to last step, because otherwise tensors saved after storage are already saved will not have storage. Test Plan: Tested by loading the file in `clowder get GLDGLQnKrIsQFg8DAPxq9vg59ZwZbmQwAAAA orig.pt` and converting to flatbuffer and load again Differential Revision: D36552645",2022-05-20T23:08:41Z,oncall: jit fb-exported Merged cla signed release notes: mobile topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/78024,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78024**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 035e45252a (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36552645," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this on green,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Add meta device support to `_UntypedStorage` and `_TypedStorage`,Fixes CC(Add meta device support for Storages),2022-05-20T20:58:48Z,module: internals open source Merged cla signed module: meta tensors,closed,0,5,https://github.com/pytorch/pytorch/issues/78008,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78008**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 75b566a2a8 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,It's very possible that some of the functions for a meta storage will fail in an ugly way at the moment (like assuming that the data pointer is valid and trying to access it). I will take a look into all of them,Need to disable the test when run on xla, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Add a check to ensure input func to Library.impl is callable,Stack from ghstack:  CC(Add a check to ensure input func to Library.impl is callable),2022-05-20T18:39:16Z,Merged cla signed release notes: composability topic: improvements module: library,closed,0,4,https://github.com/pytorch/pytorch/issues/77990,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77990**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures As of commit d5060040a9 (more details on the Dr. CI page): Expand to see more  * **2/2** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6710704355?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/2) **Step:** ""Test"" (full log  :repeat: rerun)   20220602T15:35:23.4814796Z RuntimeError: No such operator custom_namespace::custom_add     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge this please, merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"expected key in DispatchKeySet(CPU, CUDA, ....) but got: MPS", ðŸ› Describe the bug Error:  This error is thrown from hugging face transformers module. https://github.com/huggingface/transformers/blob/54192058f3826eb38f9aaea02961f1304678198f/src/transformers/generation_utils.pyL1658L1659 minimum reproducible would presumably be:   Versions ,2022-05-20T12:46:18Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/77960,I'm changing to the following as a workaround right now.  however not sure if dtype is also passed in `torch.LongTensor.new`.
rag,[GHF][BE] Use GraphQL fragments,"Introduce `PRReviews`, `PRCheckSuites` and `CommitAuthors` fragments This avoids code duplication and possibility ones query will look different for paginated subquery vs default one",2022-05-20T05:59:56Z,Merged cla signed topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/77945,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77945**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 702baf5748 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this
transformer,"Revert ""Revert ""Switch to use nested tensor by-default in Transformerâ€¦","â€¦Encoder ( CC(Switch to use nested tensor bydefault in TransformerEncoder))"""" This reverts commit 0d6fa91d1ba93b423a31a7811b679a4770470363. Fixes ISSUE_NUMBER",2022-05-20T00:57:50Z,Merged cla signed,closed,0,19,https://github.com/pytorch/pytorch/issues/77924,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77924**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit cdb0a1f3af (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Not ready yet.," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","One last test, come on!",All tests passed!, merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Move C++/Python Storage bindings out of `torch/csrc/generic`,"Since CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage) was merged, we now only have one `THPStorage` class, which means that we can move all storage related stuff out of `torch/csrc/generic` to simplify things further ",2022-05-19T22:00:55Z,module: internals triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/77908
transformer,buffer is not large enough when running pytorch on Mac M1 mps, ðŸ› Describe the bug The bug seems related to CC(`MPSNDArray` or `MPSGraphTensorData` allocated with wrong size) To reproduce the bug:  The error message:   Versions 1.12.0.dev20220519,2022-05-19T20:33:25Z,module: memory usage triaged module: mps,closed,9,34,https://github.com/pytorch/pytorch/issues/77886,Same issue with YOLOv5 on MPS noted in  CC(YOLOv5: MPS on Macbook Air M1 NotImplementedError: Could not run 'aten::empty.memory_format' with arguments...)issuecomment1131558211. I see `buffer is not large enough. Must be 25600 bytes`,"Same, likely be related.  gives  version  `1.12.0a0+git64c741e` with https://github.com/pytorch/pytorch/pull/77966 applied and `PYTORCH_ENABLE_MPS_FALLBACK=1` set",same issue ,This issue is fixed in PR CC(MPS: Fix crashes in view tensors due to buffer size mismatch)  (nightly build 1.13.0.dev20220531 or later).,"Same issue with YOLOv5 on device ""mps"" MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 19200 bytes' CC(MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 20480 bytes ')",> This issue is fixed in PR CC(MPS: Fix crashes in view tensors due to buffer size mismatch) (nightly build 1.13.0.dev20220531 or later). This issue has not been fixed.. ,  can you please reinstall nightly and see if this resolves https://github.com/ultralytics/yolov5/issues/8102,"Hello I am still receiving this error. What do I have to do to resolve this bug/issue? Thanks. pytorch nightly version: torch1.13.0.dev20220607 /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 160000 bytes","I'm also still receiving this error, a fix would be appreciated. PyTorch version 1.13.0.dev20220607 Fusing layers...  YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 25600 bytes",jocher  > can you please reinstall nightly and see if this resolves ultralytics/yolov5 CC(zerodivision crashes on 0.4.0) No it does not  ,Same issue try running YOLOv5s with mps on M1 Pro YOLOv5 ðŸš€ 2022612 Python3.9.12 torch1.13.0.dev20220612 MPS ,"same issue when running with stable baselines 3 contrib PPO recurrent  > /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 576 bytes '","YOLOv5 ðŸš€ v6.1253g75bbaa8 Python3.10.4 torch1.13.0.dev20220616 MPS !image /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 25600 bytes",Same issue  MacBook Pro M1  ,Same issue  macbook pro m1 !image,Same issue +1 ,I confirm I'm experiencing the same YOLOv5 Apple MPS bug with torch 1.12 on MacBook M1: `Error: buffer is not large enough. Must be 25600 bytes` ,> This issue is fixed in PR CC(MPS: Fix crashes in view tensors due to buffer size mismatch) (nightly build 1.13.0.dev20220531 or later).   is this error supposed to exist in torch 1.12? I see the fix was was in a 1.13 nightly.,"Hi, I'm not sure if this made it for the release no. If you're using MPS a lot, I would recommend using the nightly though as we did quite a few fixes that didn't make it to 1.12.","> I'm not sure if this made it for the release no. If you're using MPS a lot, I would recommend using the nightly though as we did quite a few fixes that didn't make it to 1.12. Hmm https://github.com/pytorch/pytorch/pull/78496 was picked into release branch as  https://github.com/pytorch/pytorch/commit/e3e753161c7c65532883b991693451849afdd708","Reopening to investigate if it still crashes on trunk, and if it is not, why Pytorch1.12 is still affected",I'm still getting the issue on 1.13.0.dev20220712 Is there any fix?,"jocher,  this should be fixed in the latest PyTorch nightly (1.13.0.dev20220722). Could you please let me know if you are still seeing the issue on your end?",  NotImplementedError: The operator 'aten::index.Tensor_out' is not current implemented for the MPS device. CC(NotImplementedError: The operator 'aten::index.Tensor_out' is not current implemented for the MPS device.)," I confirm that the original `buffer is not large enough` error is now resolved in latest nightly.  YOLOv5 inference still fails on `operator 'aten::index.Tensor_out' is not current implemented for the MPS device` as mentioned by , but that's a separate issue, so I believe this issue can be closed now.",Thanks a lot  and jocher for checking this! `index.Tensor_out` is already part of https://github.com/kulinseth/pytorch and we hope to get it soon in PyTorch master, awesome! Thanks for the update.,I'm on the latest nightly `torch1.13.0.dev20221003` and still getting this error. Could it be cause `aten::repeat_interleave.self_int` fell back to CPU?  **Edited to hardwrap the long lines**," that's not an error, that's a warning. As the message states not all torch ops are fully converted to MPS yet.",">  that's not an error, that's a warning. As the message states not all torch ops are fully converted to MPS yet. I think you have have to side scroll to see everything jocher, the formatting wasn't great. "" Error: buffer is not large enough. Must be 201452 bytes"" is there on the last line. and then the kernel dies (in Ipython or Jupyter)"
rag,Add meta device support for Storages,"Now that CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage) is merged, it should be relatively straightforward to add support for the meta device in `_UntypedStorage` and `_TypedStorage`. ",2022-05-19T20:07:40Z,module: internals triaged module: meta tensors,closed,1,0,https://github.com/pytorch/pytorch/issues/77885
chat,Make the appropriate backend `DimensionNode` visible to LTC core," ðŸš€ The feature, motivation and pitch With the current implementation of `DimensionNode` in LTC, each backend has its own implementation (PyTorch/XLA, TorchScript).  At the same time, shape inference builds off of LTC core classes leading to this PR failing to build shape inference implementation for `expand.SymInt`. Please make the appropriate backend `DimensionNode` visible to LTC core. **Solution** alternatives based on offline chat with . * IR core to access the correct backend implementation * Use multiple inheritance CC    Alternatives _No response_  Additional context _No response_",2022-05-19T19:39:03Z,triaged lazy,open,0,1,https://github.com/pytorch/pytorch/issues/77880,"Can we leverage the nonnative IR codegen ( CC(Codegen NonNative IR Nodes)) to solve this, by making DimensionNode one of the generated IRs that backends own? I guess that is orthogonal. That would be equivalent to the status quo, but save some codeduplication. We could expand on the IR Builder interface to let backends build their own DimensionNode through a core/backend API."
rag,Restore old names for private funcs in legacy storages,Followup from CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage),2022-05-19T17:39:33Z,module: internals open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/77861,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77861**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (8 Pending) As of commit 041f6740e2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Add missing decref to `createStorageGetType`,Followup from PR CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage),2022-05-19T17:09:09Z,module: internals open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/77860,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77860**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 New Failures As of commit 92dccdcf44 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6511788349?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / build (1/1) **Step:** ""Calculate docker image"" (full log  :repeat: rerun)   20220519T17:11:24.2151219Z [error]Process completed with exit code 1.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[PyTorch] Avoid initializing storage for empty Optionals,"  CC([PyTorch] Avoid initializing storage for empty Optionals) We don't need to initialize for the nonconstexpr case ever, or in the constexpr case after C++20. Differential Revision: D36519379",2022-05-19T16:51:28Z,Merged cla signed Reverted ciflow/trunk,closed,0,17,https://github.com/pytorch/pytorch/issues/77858,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77858**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures, 3 Base Failures As of commit 8f5b3ca515 (more details on the Dr. CI page): Expand to see more  * **2/5** failures introduced in this PR * **3/5** broken upstream at merge base 5994f0674a on May 19 from  7:04am to 12:59pm   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6734675298?check_suite_focus=true) pull / linuxxenialpy3.7clang7asan / test (default, 2, 5, linux.2xlarge) (1/2) **Step:** ""Test"" (full log  :repeat: rerun)   20220604T00:36:09.6189356Z [error]Process completed with exit code 1.      :construction: 3 fixed upstream failures: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * pull / linuxxenialpy3.7clang7asan / test (default, 1, 5, linux.2xlarge) on May 19 from  7:04am to 12:18pm (2d4291fb81  5cdf79fddc)     * :repeat: rerun * pull / linuxxenialpy3.7clang7asan / test (default, 5, 5, linux.2xlarge) on May 19 from  7:17am to 11:58am (ec290949aa  007cc731ce)     * :repeat: rerun * pull / linuxxenialpy3.7clang7asan / test (default, 4, 5, linux.2xlarge) on May 19 from  7:17am to 12:59pm (ec290949aa  5cdf79fddc)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge g,(previous test run was green and all I did was fix indentation for lintrunner),Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2423256541," can you confirm that the 4 failures on this diff aren't blocking? Two of them are initializationorderfiasco and look unrelated, one looks like a timeout while building, and one is clearly unrelated backward compat failure.", merge f, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2437199503, merge f, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""asan builds failed on both PR and trunk, see https://hud.pytorch.org/pytorch/pytorch/commit/17bd683aad6a8aec54a5604c02fd841320981cbf"" c ignoredsignal", successfully started a revert job. Check the current status here,">  can you confirm that the 4 failures on this diff aren't blocking? Two of them are initializationorderfiasco and look unrelated, one looks like a timeout while building, and one is clearly unrelated backward compat failure. I don't know about the previous failures, but latest ones indeed look related, as one can observe in the HUD history: https://hud.pytorch.org/hud/pytorch/pytorch/master/1?name_filter=asan","> >  can you confirm that the 4 failures on this diff aren't blocking? Two of them are initializationorderfiasco and look unrelated, one looks like a timeout while building, and one is clearly unrelated backward compat failure. >  > I don't know about the previous failures, but latest ones indeed look related, as one can observe in the HUD history: https://hud.pytorch.org/hud/pytorch/pytorch/master/1?name_filter=asan I saw those ASAN failures, and they don't look related to me just based on their contents.","...however, the red/green history is pretty conclusive. sigh",looks likely that the change caused a preexisting bug to start getting caught by ASAN. investigating
rag,Running MPS model crash '_mtlIOGPUCommandBufferStorageRebaseShmemHeader'," ðŸ› Describe the bug Running model using 'mps' device resulted in some internal crash `Python[4210:2560233] failed assertion false at line 567 in _mtlIOGPUCommandBufferStorageRebaseShmemHeader` without any additional output/info.  Versions PyTorch version: 1.12.0.dev20220518 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.12 (main, Mar 26 2022, 15:44:31)  [Clang 13.1.6 (clang1316.0.21.2)] (64bit runtime) Python platform: macOS12.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.12.0.dev20220518 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] Could not collect",2022-05-19T10:09:39Z,needs reproduction triaged module: mps,closed,0,5,https://github.com/pytorch/pytorch/issues/77843,"Hi, Would you have any small code sample that we can use to reproduce this please?","hey that is a big project so its hard to tell what exactly cause internal issue, after some print and run trails I seem to be pinpoint issue to following layer: `nn.Conv1d(dim_in, dim_out, kernel_size=3, padding=1, padding_mode='circular', bias=False)` what is strange it runs successfully few loop iterations and then crashes with the above error. if i remove `padding_mode` argument it seems to run fine",Ok! Can you share what are the values of `dim_in`/`dim_out` and the size of the Tensors you pass as input?,"Yup! dim_in = 8 dim_out = 128 input tensors: (32, 8, 64) or (4, 8, 64)","Hi  . I tried the Conv1d with the input sizes you provided (test case below), and I'm unable to reproduce the issue. Please try with the latest nightly build and reopen the issue if you still see the crash. Thank you. "
transformer,MPS device appears much slower than CPU on M1 Mac Pro," ðŸ› Describe the bug Using MPS for BERT inference appears to produce about a 2x slowdown compared to the CPU. Here is code to reproduce the issue:  `torch.backends.mps.is_available()` reports `True` as expected.  I chose a batch size of 64, since that was mentioned on the blog post for the ~5x speedup in Huggingface BERT.  Running this benchmark, swapping ""mps"" and ""cpu"" for the devices, I get: CPU 18.38 s MPS 36.00 s And, for comparison, running this same snippet on an A10G using CUDA 11.6 on an AWS g5.xlarge instance (ubuntu 20.04, pytorch 1.11.0 compiled from source), I get: CUDA 0.82 s I used the `no_grad()` construction due to the comment mentioned on CC(Memory usage and epoch iteration time increases indefinitely on M1 pro MPS).  I used today's nightly build. Furthermore, I also built PyTorch from source and observed no differences on the results here. Lastly, I confirmed with Activity Monitor and `powermetrics` that the GPU is in fact being used, seeing GPU usage at 98.8%, GPU power spike to 15711 mW when running the MPS benchmark, and CPU power spiking as expected when running the CPU benchmark.  So it does seem like things are ""hooked up"" ri",2022-05-18T22:23:55Z,module: performance triaged inference mode module: mps,closed,57,82,https://github.com/pytorch/pytorch/issues/77799,"I'd also add that I saw the same effects when running `timeit` with the toy examples in `docs/source/notes/mps.rst`. (e.g.: `timeit.timeit(lambda: x * 2, number=100000)` on both mps/GPU. ",Example of the toy example run: ,observed the same behavior on M1 Max,same behavior on M1 Pro,"> but sadly I didn't observe any power consumption by the ANE module, which is too bad. The neural engine can't be used for training anyway. It only supports Float16, Int8, and UInt8, and is only accessible through CoreML and MLCompute. PyTorch uses neither of these APIs for training. Could you check in the activity monitor whether it's using multiple CPU cores? If so, it would just be utilizing the AMX to maximum capacity, reaching multiple TFLOPS of compute power on the CPU. Thus, certain models would close the gap in performance between CPU and GPU, making CPU faster.","> The neural engine can't be used for training anyway. It only supports Float16, Int8, and UInt8, and is only accessible through CoreML and MLCompute. PyTorch uses neither of these APIs for training. Very interesting, thanks for the context, I'm among many who are curious about the ANE. I wonder if it could be useful for quantized models, though I suppose this is what Core ML Tools is for. I have checked the activity monitor with the toy multiplication example above and increasing the number of steps to 1e7. I observed:  Running on the CPU device, the toy example completes in 23s, only saturating one core (CPU usage does not exceed 100)  Running on the MPS device, GPU is saturated at 100%, and CPU usage of the python process reaches 175%. Memory usage of the python process increases without end, similar to what was described in CC(Memory usage and epoch iteration time increases indefinitely on M1 pro MPS). kernel_task CPU usage around 3540%, which was not observed on the CPU device. Benchmark takes 441s. ","From  CC(Memory usage and epoch iteration time increases indefinitely on M1 pro MPS)issuecomment1132230314, the GPU may be slower because it spends more of its time writing to new memory than actually executing. Also, since it continues writing to new memory on every iteration, that might destroy cache coherence which brings down performance further. Although if each tensor is deallocated and reallocated on every loop iteration, then this bottleneck would appear regardless of whether there was a memory leak. Metal forces the memory to be overwritten with zeros when you initialize a `MTLBuffer`  something you can sometimes avoid in CUDA.",Fascinating. That hypothesis might also explain why the delta is so much worse with the toy example compared to the full size BERT. ,"I do have a workaround planned for my own framework (s4tf/s4tf) where I will bypass this restriction. It's described in https://github.com/AnarchoSystems/DeepSwift/issues/1issuecomment1129891796, although please don't comment on that thread.  does your backend use `MTLHeap`'s in the way I described to permit zerocost memory allocation? Note: This is still just a theoretical idea. I don't have a proofofconcept of that in action, but it shouldn't be hard to set up.",As of pytorchnightly build `1.13.0.dev20220522`  CPU `18.835263124999983` s  MPS `18.363079624999955` s,"I have another example where mps is roughly 7x slower than cpu, doing nothing but matrix multiplications. I'm on nightly build `1.13.0.dev20220627`:  Prints: ","Interestingly, if I do the same experiment as above using smaller tensors, the disparity grows to mps being roughly 25x slower:  Prints: ","I have observed the same problem, on an AMD GPU, got a 3x slow down compare to CPU. Any fixes available yet?","I wonder if we could convert this to actual MPS code in Swift, then profile how long that code takes. That would determine whether the bottleneck is PyTorchâ€™s fault and something that can be fixed. From a distance, it looks like a fundamental problem of GPU communication overhead. Is your timing mechanism taking the number of seconds for one iteration, then multiplying it by 100,000?","another datapoint here: (nightly build `1.13.0.dev20220704`, i77920HQ / Radeon Pro 560) running the two matrix multiplication tasks aboveissuecomment1168102637:)  ","  I noticed this ticket's been subject to triage, and a few other folks have filed issues regarding similar observations.  Do you know how the figure in the press release (about large MPS performance benefits) was computed? ",This might be why itâ€™s so slow. Worth reading if you have the time. https://discuss.pytorch.org/t/sequentialthroughputofgpuexecution/156303,">   I noticed this ticket's been subject to triage, and a few other folks have filed issues regarding similar observations. >  > Do you know how the figure in the press release (about large MPS performance benefits) was computed? Hi  , those numbers were collected on a M1 Ultra system using torchbench with the Batch size as listed in the figure. The command used was:  and compare the MPS time with CPU time. "," Thanks for the great read.  thanks for sharing, I'll try running that on my system, and I'm curious to dig in, because when I ran my own benchmarks with HuggingFace's BERT on an MPS device on an M1 Pro it was dramatically slower, so maybe I can dig through the benchmark scripts and see if I've got something wrong. And my M1 Studio is on order ðŸ˜„ ",">  thanks for sharing, I'll try running that on my system, and I'm curious to dig in, because when I ran my own benchmarks with HuggingFace's BERT on an MPS device on an M1 Pro it was dramatically slower, so maybe I can dig through the benchmark scripts and see if I've got something wrong. >  > And my M1 Studio is on order ðŸ˜„  can you try with latest nightly how the performance looks like? Also are you running the mentioned network :  to do your measurement.?",Device: M1 Pro 10CPU 16GPU Training my model using mps:  Training my model using CPU: ,"> Device: M1 Pro 10CPU 16GPU >  > Training my model using mps: >  >  >  > Training my model using CPU: >  >   , can you please provide a testcase to repro this? Also what torch nightly version you are using for these numbers?",  Repo: https://github.com/realJustinLee/dumbchatbot ,">  MPS Version > from transformers import AutoTokenizer, BertForSequenceClassification > import timeit > import torch >  > tokenizer = AutoTokenizer.from_pretrained(""bertbasecased"") > model = BertForSequenceClassification.from_pretrained(""bertbasecased"").to(torch.device(""mps"")) >  > tokens = tokenizer.tokenize(""Hello world, this is michael!"") > tids = tokenizer.convert_tokens_to_ids(tokens) > with torch.no_grad(): >     t_tids = torch.tensor([tids]*64, device=torch.device(""mps"")) >     res = timeit.timeit(lambda: model(input_ids=t_tids), number=100) > print(res) CPU M1 8Core 16G RAM Using `mps` : `65.45052295800001` Using `cpu` :  `24.685709957999997`",Same here for LSTM examples. I used Torch Profiler to see the performance difference: CPU:  MPS: ,"The spike in microsecondlevel overhead (CPU time avg) was discussed hereissuecomment1204672455). I think Iâ€™ve found a solution to it, but havenâ€™t put it into practice with an RNN.","> The spike in microsecondlevel overhead (CPU time avg) was discussed hereissuecomment1204672455). I think Iâ€™ve found a solution to it, but havenâ€™t put it into practice with an RNN. Any recent plan to implement it? Looking forward to it.","> Any recent plan to implement it? Iâ€™m not planning to implement it in PyTorch; however, itâ€™s open source and Iâ€™ve explained it in great depth. Someone else could look at it and reimplement it in PyTorch. If anyone does,  me and I can take a look. I just wanted to showcase the hard work and months of planning that went into these optimizations of driver overhead in my project. They are showing realworld performance gains that could make it significantly faster than PyTorch (at least with the current MPS backend) for certain use cases. Current repo: https://github.com/philipturner/metalexperiment1 Will eventually migrate to: https://github.com/s4tf/metal","Regarding a working RNN implementation, it probably won't happen any time within the next few weeks. I'm juggling a bunch of other projects simultaneously, so things will happen slowly regarding the Metal backend. I need to fix an existing bug in constant folding and support `MPSKernel` caching for an `MPSMatrixMultiplication`. Perhaps I also need to support some NDArray ops, which are only exposed with reasonable overhead in `MPSGraph`.","Device: M1 Pro 8CPU 14GPU Just to add some more data points, using the nightly build 1.13.0.dev20220818. **Experiment 1** Running the following code:  I get the following results:  **Experiment 2** Running the following code:  I get the following results: "
yi,TypeError: Trying to convert Double to the MPS backend but there is no mapping for it.," ðŸ› Describe the bug Steps to reproduce:   Versions PyTorch version: 1.12.0.dev20220518 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.9  (main, Dec 20 2021, 02:41:06)  [Clang 11.1.0 ] (64bit runtime) Python platform: macOS12.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.0 [pip3] torch==1.12.0.dev20220518 [conda] numpy                     1.22.0           py39h61a45d2_0    condaforge [conda] pytorch                   1.12.0.dev20220518         py3.9_0    pytorchnightly",2022-05-18T20:02:58Z,triaged actionable module: mps,closed,0,6,https://github.com/pytorch/pytorch/issues/77781,Hi! I am afraid that the MPS framework does not support double precision numbers. You can use `float32` though. We should improve that error message to clarify this (and potentially link to the metal doc referencing this),ah gotcha  I suppose implicit conversion would result in unexpected behavior.,"I'm getting strange behavior casting to `torch.float32`  Converting to a tensor works as expected  And fails as expected without casting  However, this is where it gets weird: if I force the dtype, I get strange values  and if I use another method (`from_numpy` I think is recommended for conversion to tensors)  but it works if I cast before converting to tensor  Looks like I will be doing the numpy casting beforehand. Am I doing something obviously wrong here? ",This comment might be particularly relevant: https://discuss.pytorch.org/t/codedidntspeedupasexpectedwhenusingmps/152016/6?u=philipturner,"Hi, The PR linked above is going to clarify the double situation. The issue when converting to float via `.to()` is known and is already fixed in master.",The error message is now properly updated in master.
rag,General MPS op coverage tracking issue," This issue is to have a centralized place to list and track work on adding support to new ops for the MPS backend. **PyTorch MPS Ops Project** : Project to track all the ops for MPS backend. There are a very large number of operators in pytorch and so they are not all yet implemented. We will be prioritizing adding new operators based on user feedback. If possible, please also provide link to the network or usecase where this op is getting used. As Ops are requested we will add "" *To Triage*"" pool. If we have 3+ requests for an operation and given its complexity/need the operation will be moved ""*To be implemented*"" pool. If you want to work on adding support for such op, feel free to comment below to get assigned one. Please avoid pickup up an op that is already being worked on tracked in ""*In progress*"" pool.  Link to the wiki for details on how to add these ops and example PRs. **MPS operators coverage matrix**  The matrix covers most of the supported operators but is not exhaustive. **Please look at the `In vx.x.x` column, if the box is green, it means that the op implementation is included in the latest release; on the other hand, if the box i",2022-05-18T18:12:47Z,feature triaged tracker module: mps,open,790,1618,https://github.com/pytorch/pytorch/issues/77764,"Are there any linear algebra ops not implemented in MPS that you have made custom shaders for? Any shaders I could ""borrow"" from your project (with full credit) and use in my own? Specifically, it would be helpful to have SVD and reversemode Cholesky operators.","Hey, There are no custom shaders at the moment as everything we needed for the basic networks we looked at was already provided by MPS (or a set of ops in MPS). Also , required functions that are not in the hot path are simply falling back to CPU for now. It is mentioned here as this is something that is possible to be done easily within the integration. But not something that is used today.","I was testing a bunch of speech synthesis and vocoder models, and found the following operators missing so far:  `aten::flip`  `aten::equal`   `aten::upsample_nearest1d.out`","One vote for a CPU fallback for `torch.bincount`. Is there any reason, given the unified memory architecture, that every op not implemented on Metal cannot just fall back to the CPU implementation without memory copy operations? (Based, of course, on my 10,000ft view of the architecture, which I'm sure is wildly oversimplified.)","Tip for everyone: Run your script with PYTORCH_ENABLE_MPS_FALLBACK=1 which will fallback to the CPU. I'm using a custom build which merges pull request CC(MPS Fixes: copy operations, addmm and baddmm) so am not sure if this is included in the current build (Edit: It's not. You need to build PyTorch yourself with the pull request or trust an online build with it).",Testing with some huggingface transformers code: + 1 vote for `aten::cumsum.out` Tried with the fallback env var but doesn't seem to work for me.,"One missing op I ran into and haven't seen mentioned yet is `aten::_unique2`. Edit: This error goes away when passing `PYTORCH_ENABLE_MPS_FALLBACK=1` when using the current `main` branch build. However, instead I get warnings  then  and finally the forward pass through my model crashes with  On `cpu` it works fine. Could be CC(buffer is not large enough when running pytorch on Mac M1 mps) I suppose.",> Testing with some huggingface transformers code: + 1 vote for `aten::cumsum.out` > Tried with the fallback env var but doesn't seem to work for me. +1  setting `PYTORCH_ENABLE_MPS_FALLBACK=1` still results in: , could you open a new separate issue for the cpu fallback failing for you? The error seems to hint at the fact that you're doing moving across device noncontiguous Tensor. Making sure they are might help as a workaround. We can continue this discussion in the new issue you will create. Zhang the fallback is ONLY available if you build from source right now. It will be in the nightly build tomorrow (May 21st).,"Would like to add `aten::_local_scalar_dense` to the list. Also, is it possible to link to some examples in the top post on how we can implement these into Pytorch? I'd love to give it a shot if it's not too hard."," Yep, making the Tensors contiguous worked. But yet another issue revealed itself. I created CC(CPU fallback for `aten::index.Tensor` on MPS crashes or gives wrong result) and CC(MPS backend has problems with printing noncontiguous tensors).",I've got a non supported op: `aten::grid_sampler_2d` ,Not supported  `aten::l1_loss_backward.grad_input`  `aten::kl_div_backward` Code  Output ,"Trying to use affine crop from torchvision, and found the operator `aten::linspace.out` does not seem to be implemented with the MPS backend","Trying to use MPS backend with pytorch geometric, and found the operator `aten::index.Tensor` is not yet implemented.",Found the operator 'aten::grid_sampler_2d' is not current implemented for the MPS device.,Would be great to add `aten::adaptive_max_pool2d` to the list  seems to be fairly common and for me useful in some point cloud architectures.,I ran into this error with `aten::count_nonzero.dim_IntList` (via `torch.count_nonzero()`). I'll take a look at implementing this op with MPS.,The operator `aten::lgamma.out` is curently not yet implemented either.,"Hello, the operator Linspace is not implemented, for you my error message:  NotImplementedError                       Traceback (most recent call last) Input In [2], in ()       7 device = torch.device(""mps"")       9  Create random input and output data > 10 x = torch.linspace(math.pi, math.pi, 2000, device=device, dtype=dtype)      11 y = torch.sin(x)      13  Randomly initialize weights NotImplementedError: The operator 'aten::linspace.out' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on  CC(General MPS op coverage tracking issue). As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS. Thank you !",I would like to add `aten::linalg_householder_product` Using orthogonal parametrization with `PYTORCH_ENABLE_MPS_FALLBACK=1`. I get:  ,"Hi, please consider `aten::avg_pool3d.out`.",The operator `aten::erfinv.out` is not implemented.,The operator `aten::logical_and.out` is not current implemented for the MPS device.,The operator `aten::bitwise_and.Tensor_out` is not yet implemented for the MPS backend.,The operator 'aten::_slow_conv2d_forward' is not currently implemented for the MPS device. Also found this: NotImplementedError: Could not run 'aten::_copy_from_and_resize' with arguments from the 'CPU' backend. after enacting the PYTORCH_ENABLE_MPS_FALLBACK=1 env variable.,Got a message that `aten::softplus.out` is not supported. I'd need that to update OpenPifPaf.,"> Would like to add `aten::_local_scalar_dense` to the list. Also, is it possible to link to some examples in the top post on how we can implement these into Pytorch? I'd love to give it a shot if it's not too hard. You can use this as a guide: https://github.com/pytorch/pytorch/wiki/AddingOpforMPSBackend Please provide feedback if there is anything missing.","> Would like to add `aten::_local_scalar_dense` to the list. Also, is it possible to link to some examples in the top post on how we can implement these into Pytorch? I'd love to give it a shot if it's not too hard. MPS backend already has support for `aten::_local_scalar_dense` (file https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/mps/operations/Scalar.mm). If you are still seeing the issue, could you please share the example you are trying to run? ","The operator 'aten::_index_put_impl_' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature,"
yi,Allow specifying alias analysis while registering new ops,Stack from ghstack:  CC(Allow specifying alias analysis while registering new ops),2022-05-17T22:17:19Z,Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/77690,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77690**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit b7b9e75d19 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6512543659?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220519T21:12:35.0491658Z RuntimeError: test_sparse_csr failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",added another decorator to define and impl in the same line since you approved  ,decorator seems fine, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[LT] Add IR resuing support for manually-implemented ops,Stack from ghstack:  CC([LT] Add IR resuing support for manuallyimplemented ops) Summary: Add CanBeReused methods for manuallyimplemented ops and replace MakeNode with ReuseOrMakeNode.,2022-05-17T00:38:22Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/77616,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77616**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 2156113415 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," , this PR only touches ops for ts_backend, but can answer your earlier question about how to handle manuallyimplemented ops.", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Add support for TxT mask layout for masked_softmax in BetterTransformer,Summary: Expand mask to BxHxDxD when mask is DxD layout Test Plan: buck build mode/opt c fbcode.platform=platform009 c fbcode.enable_gpu_sections=true caffe2/test:nn && buckout/opt/gen/caffe2/test/nn\\binary.par r masked_softmax_DxD Differential Revision: D36428170,2022-05-16T23:07:31Z,fb-exported Merged cla signed topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/77607,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77607**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit ddafd20dc1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36428170,This pull request was **exported** from Phabricator. Differential Revision: D36428170,This pull request was **exported** from Phabricator. Differential Revision: D36428170,This pull request was **exported** from Phabricator. Differential Revision: D36428170, merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Failed to run on iOS - Couldn't find an operator for `aten::conv1d`," ðŸ› Describe the bug Hi, I'm testing one of my projects with `libtorch 1.11.0` for iOS simulator and I'm facing the following issue.  Essentially, `at::_ops::conv1d::call` is found   Also in the linking I `force_load` libraries:   The compilation was done with the following confguration:   Versions ",2022-05-16T13:04:25Z,triaged module: ios,open,0,3,https://github.com/pytorch/pytorch/issues/77538,On feedback only versi run on iOS,"Sorry, I didn't get it. What do you mean?","Hi again, I still cannot solve this error. Any ideas?"
rag,SegFault after average pooling quantization, ðŸ› Describe the bug After adding `ceil_mode=True` in AveragePooling given code return segfault.   Versions [pip3] mypy==0.942 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [conda] Could not collect ,2022-05-15T20:41:48Z,oncall: quantization,closed,0,6,https://github.com/pytorch/pytorch/issues/77511,MaxPooling in that case works fine,Taking a look,"  I'm able to reproduce your error on a local build. Are you implying `ceil_mode = false` worked for you?  using your script, when `ceil_mode = false`, I get the following error  I think the two errors are related, so I can look into this further, but prior to doing so, I'm wondering if your sizes in `input_data = torch.randn((32, 40, 16))` are correct? I noticed you calibrated with `[32, 40, 32]` and you also use `32` for the kernel size, but you're doing inference on a different size (16) for the last dimension.  if you run your script with `valgrind`, you'll see it's segfaulting in `pytorch_qnnp_indirection_init_dwconv` (see below)  i'm checking if the qnnpack backend supports this configuration. it looks like we're accessing invalid memory, which we should throw an exception instead of causing UB","> Are you implying ceil_mode = false worked for you? using your script, when ceil_mode = false, I get the following error Sorry for misunderstanding, it also write RuntimeError for me. But i think it's ok, because i have too small last dimension for `ceil_mode=False` :) But is starts to work, when i change AvgPool to MaxPool. > I think the two errors are related, so I can look into this further, but prior to doing so, I'm wondering if your sizes in input_data = torch.randn((32, 40, 16)) are correct? I noticed you calibrated with [32, 40, 32] and you also use 32 for the kernel size, but you're doing inference on a different size (16) for the last dimension. I want to use tensors of different dimension with this model, so i just reproduce this case in my sample.","I think qnnpack may not support the kernel size being bigger than the input. Max pool is also implemented with qnnpack, but when `ceil_mode = true`, it doesn't use qnnpack (see https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qpool.cppL383). This PR here https://github.com/pytorch/pytorch/pull/77792/files mimics this logic for avgpool, and it seems to resolve the issue when I tried it locally. are you able to build pytorch locally? if not, I think you'll have to keep the input size (+ padding) >= kernel size for now until this change makes its way to a future release",closing due to inactivity
rag,Dot/group_norm/instance_norm/var_mean/index_reduce/matmul/bernoulli/adaptive_avg_pool coverage,  CC(Dot/group_norm/instance_norm/var_mean/index_reduce/matmul/bernoulli/adaptive_avg_pool coverage) Signedoffby: Edward Z. Yang ,2022-05-15T03:08:52Z,Merged cla signed release notes: composability topic: improvements,closed,0,4,https://github.com/pytorch/pytorch/issues/77499,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77499**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 52e4243d4e (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6511998349?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / build (1/1) **Step:** ""Calculate docker image"" (full log  :repeat: rerun)   20220519T17:26:23.1392064Z [error]Process completed with exit code 1.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",logit got moved out of this PR, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
chat,Back out Dispatcher change that makes Messenger Desktop crash on M1 devices,"Summary: This change causes Messenger Dekstop to crash on M1 devices when the user enables background during the call. The change apparently causes the compiler to emit AVX instructions that are not supported by Rosetta. This is a surgical backout that only backs out the changes in C++ side, and not Python bindings which I believe are not shipped with Workplace Chat. Test Plan: Run the application and make sure that it doesn't crash when the background is enabled https://pxl.cl/23VSH Reviewed By: ezyang Differential Revision: D36358832",2022-05-13T13:33:34Z,fb-exported Merged cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/77414,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77414**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (2 Pending) As of commit 4dc9cc52ef (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36358832," force merge this (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)","Merge failed due to Matched rule superuser, but PR has not been reviewed yet Raised by https://github.com/pytorch/pytorch/actions/runs/2320784596"," force merge this (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[distributed] c10d crashing on assert," ðŸ› Describe the bug Finally got a simple script that reproduces the pt1.11/c10d crash on assert and or exit  which on JeanZay HPC most of the time leads to core dumps. The script is totally unrelated to BigScience but the C traceback looks similar.  I attached the 2 needed files: test.txt ds_config.txt please rename those upon saving to:  Here are the right sha's otherwise the initial problem will be fixed shortly and there will be no assert. Please use the following order:  Then just:  Clearly there is some bad interaction happening between deepspeed, which uses pytorch CUDA extensions and pytorch c10d.  Versions PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Red Hat Enterprise Linux release 8.4 (Ootpa) (x86_64) GCC version: (GCC) 8.4.1 20200928 (Red Hat 8.4.11) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.28 Python version: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.18.0305.40.2.el8_4.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: 11.4.152 GPU models and configurat",2022-05-12T21:07:39Z,oncall: distributed,closed,0,6,https://github.com/pytorch/pytorch/issues/77374," since there is an assertion error from the application, the application is trying to exit, so the NCCL kernel exits as well. This looks expected to me. Maybe I'm wrong.   would you please help taking a look at it? Thanks",You consider this exiting expected?  and most of the time in the larger MegatronDeepspeed application this segfaults and dumps corrupted core files. There a similar log plus segfault and core dump happens even if I do `sys.exit(0)` which is not even an assert. Please see more live backtraces and gdb's core dump's bt here:  CC(torch.elastic fails to shutdown despite crashed processes)issuecomment1117938234,"`workCleanupLoop` is a busylooping thread that monitors the state of enqueued collectives. This threads calls `isCompleted()` on each collective work item, which then calls `finishedGPUExecutionInternal()`. In `finishedGPUExecutionInternal()`, a query is made on a `cudaEvent` recorded after the collective, to check if the collective is completed. The ""driver shutting down"" error is thrown during the `cudaEventQuery`, which probably indicates that the CUDA context has been corrupted in previous failure.","I met the exactly same error, I'm also using the same version of pytorch and deepspeed, waiting for solution.","Can anyone confirm if this is still happening?  Should be fixed by https://github.com/pytorch/pytorch/pull/106503 (landed in aug, would be part of the latest release)","Will, I have just tried to retest but the current torch doesn't work with those older versions of other packages that were triggering the situation. But I haven't run into this issue again since then, so as you saying it is probably ok."
yi,"outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/jit/ir/ir.cpp"":1314, please report a bug to PyTorch. "," ðŸ› Describe the bug When I use torch.jit.script for the class, in the line  p = pt_2d[b, :, pix2pt] it will pop up the issue descriped in the title. The snippet :   the error occur in the last line     p = pt_2d[b, :, pix2pt]   Versions Versions of relevant libraries: [pip3] facenetpytorch==2.5.2 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.1 [pip3] numpydoc==1.1.0 [pip3] pytorch3d==0.6.1 [pip3] torch==1.11.0 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] blas                      1.0                         mkl [conda] cudatoolkit               10.2.89              hfd86e86_1 [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.2.0           h06a4308_296 [conda] mklservice               2.3.0            py38h27cfd23_1 [conda] mkl_fft                   1.3.0            py38h42c9631_2 [conda] mkl_random                1.2.1            py38ha9443f7_2 [conda] numpy                     1.20.1           py38h93e21f0_0 [conda] numpybase                1.20.1           py38h7d8b39e_0 [conda] numpydoc                  1.1.0              pyhd3eb1b0_1 [conda] pytorch              ",2022-05-12T15:49:38Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/77354
gpt,Add device_id support to FSDP,"  CC(Add device_id support to FSDP) Adds `device_id` argument to FSDP. If this is specified, and module is on CPU, we move module to this device. In addition, even if device_Id is not specified, we move module to this device to do flattening, sharding, etc but then move it back to CPU before returning to user. This PR also moves module inputs to the `compute_device` during forward of root FSDP instance. Note that we guarantee `compute_device == device_id` if `device_id` is specified. This PR also detects if user passed in multidevice module, which FSDP does not support, and throws an error. Benchmarking on single host 8 GPUs:  GPT 700m parameter model FSDP init on CPU: ~28s, moving to GPU speeds it up to 2s, a 14x win  GPT 6B parameter model FSDP init on CPU: ~47s, moving it to GPU speeds it up to 17s, 2.7x win",2022-05-12T01:53:42Z,oncall: distributed Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/77321,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77321**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit fdf0b665e8 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey varma. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","Hi varma , I keep running into an error when I use FSDP in my system. I have 4 GPU's, in my node, all of which I use while running my bash script which is as follows :   The error that I get is   `ValueError: Inconsistent compute device and `device_id` on rank 0: cuda:1 vs cuda:0` While debugging I noticed that the error comes from _get_compute_device() method. I logged compute_device and device_from_device_id, and noticed that whenever the compute_device moves to cuda:1 the error occurs, as shown in the  following    Would you happen to know the possible reason?", What exactly are you logging? When does the first value become `cuda:1`? Are you calling `torch.cuda.set_device()` on each rank?
yi,"Autogen Tags enum, and allow specifying tags while defining an op","Stack from ghstack:  CC(Autogen Tags enum, and allow specifying tags while defining an op) 1. Autogenerates Enum Tag (containing all valid tags from tags.yaml). 2. Adds an unordered set of tags to OperatorEntry class. 3. Updates Library def to optionally take tags. 4. Updates codegen for registering ops to pass tags to def. 5. Autogenerates python bindings for Tag Enum. 6. Expose tags in `torch.ops` API 7. New test to validate correctness of torch.Tags.inplace_view TODO: Add docs for Tags https://github.com/pytorch/pytorch/runs/6600629359?check_suite_focus=true Differential Revision: D36912769 Internal Changes will be coming soon",2022-05-11T23:50:19Z,Merged cla signed ciflow/trunk,closed,0,22,https://github.com/pytorch/pytorch/issues/77313,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77313**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 55a7a502ad (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,  added a new test in `test_ops.py` to verify correctness for `torch.Tags.inplace_view`. Eventually we should expand this test to add tests for every newly added tag., merge,"Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp), linuxdocs / builddocs (python), winvs2019cpupy3 / build, winvs2019cuda11.3py3 / build, linuxxenialpy3.7gcc7noops / build, linuxbionicpy3.7clang9 / build, linuxxenialpy3.7clang7onnx / build, linuxxenialpy3.7gcc7 / build, linuxxenialpy3.7clang7asan / build, linuxvulkanbionicpy3.7clang9 / build, linuxxenialcuda11.3py3.7gcc7 / build, linuxbioniccuda11.3py3.7clang9 / build, linuxxenialpy3clang5mobilebuild / build, linuxxenialpy3clang5mobilecustombuildstatic / build, pytorchxlalinuxbionicpy3.7clang8 / build, deploylinuxxenialcuda11.3py3.7gcc7 / build are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2431233135", merge,Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp) are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2431583740, merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""Broke OSS buck builds, see https://hud.pytorch.org/pytorch/pytorch/commit/9476a78f3754aa122323b431c59360b254559d16"" c missedsignal"," revert m ""Broke OSS buck builds, see https://hud.pytorch.org/pytorch/pytorch/commit/9476a78f3754aa122323b431c59360b254559d16"" c nosignal"," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", help, merge g, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2476967592, merge, merge, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2477512496
transformer,Switch to use nested tensor by-default in TransformerEncoder,Summary: Switch to use nested tensor as by default setting in TransformerEncoderLayer. Test Plan: CI Torchtext buck test mode/opt pytorch/text/test:integration_tests_test_models  test_xlmr_base_model Reviewed By: frankwei Differential Revision: D36153335,2022-05-11T01:16:44Z,fb-exported Merged cla signed Reverted,closed,0,15,https://github.com/pytorch/pytorch/issues/77217,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77217**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 13 New Failures As of commit 49e9a81ed5 (more details on the Dr. CI page): Expand to see more  * **13/13** failures introduced in this PR   :detective: 13 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6514105745?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/13) **Step:** ""Test"" (full log  :repeat: rerun)   20220519T22:12:48.6865198Z RuntimeError: test_meta failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335, ,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335, merge (Initiating merge automatically since Phabricator Diff has merged), revert this please as it broke many tests: https://github.com/pytorch/pytorch/runs/6515962633?check_suite_focus=true, also the PR signal is mostly red. So we should avoid merging PRs with broken CI.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PT-D][Sharding] Enable more ops needed in the transformer model training,"  CC([PTD][Sharding] Enable more ops needed in the transformer model training) From the code base of MetaSeq Model, we have found that loads of ops are not supported by sharded tensor. In https://github.com/pytorch/pytorch/pull/75374, we have enabled most of ops already and this PR/diff aims at enabling the rest of them. Fix some unit test errors. Differential Revision: D36302780",2022-05-11T01:10:03Z,oncall: distributed Merged cla signed sharded_tensor release notes: distributed (sharded) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/77214,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77214**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures, 1 Base Failures As of commit fd787ad0d1 (more details on the Dr. CI page): Expand to see more  * **2/3** failures introduced in this PR * **1/3** broken upstream at merge base 8b93abd082 on May 10 from  5:00pm to  7:23pm   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6432733993?check_suite_focus=true) pull / linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) (1/2) **Step:** ""Test"" (full log  :repeat: rerun)   20220514T06:25:00.6765626Z AssertionError: 1 unit test(s) failed:      :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) on May 10 from  5:00pm to  7:23pm (81528d4b21  e832ff58bf)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",Rebase and addressed the comment from reviewers., merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Set block dim and grid dim macros within NestedTensorTransformerFunctions.cu,Follow up to https://github.com/pytorch/pytorch/pull/76157discussion_r856668878,2022-05-10T21:12:24Z,Merged cla signed topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/77199,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77199**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 14695bfbae (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,nn.functional.pad accepts bool values but raises internal assert when converted to JIT," ðŸ› Describe the bug While working on Longformer in `transformers`, I came across this comment: https://github.com/huggingface/transformers/issues/13126issuecomment993645323. An incorrect implementation passes a bool as `value` into `nn.functional.pad`, which normally works. However, it raises an internal assert when used with `torch.jit`: Example:  `torch.jit.trace_module(MyModule(), {""forward"": torch.zeros(3, 4)})` results in  `torch.jit.script(MyModule())` results in  Is this considered a user error, or should it be addressed? Maybe we could at least improve the error message for `torch.jit.trace_module`. Best, Patrick  Versions Collecting environment information... PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.3) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.11 (main, May  4 2022, 09:48:34)  [Clang 13.1.6 (clang1316.0.21.2.3)] (64bit runtime) Python platform: macOS12.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration:",2022-05-10T15:11:31Z,oncall: jit,open,0,1,https://github.com/pytorch/pytorch/issues/77167,"I met the same issue, I think it would be better if it could mention which exact layer is raising this?"
yi,"torch.cholesky has been deprecated in favour of torch.linalg.cholesky. However, torch.cholesky_inverse remains as is. It should also be moved to torch.linalg"," ðŸ› Describe the bug Not sure where this should be filed under. But I noticed that, in the most recent version of pytorch `1.11.0+cu102`, when using `torch.cholesky`, it says it's deprecated in favour of `torch.linalg.cholesky`, and will getting phased out in future versions. However, the complementary function `torch.cholesky_inverse` still runs with no deprecation warning. Indeed, there's currently no `torch.linalg.cholesky_inverse`, nor does it seem like there are any such plans. If you saw it fit to move `cholesky` to `torch.linalg`, then it seems only right that `cholesky_inverse` should join it there.  Versions Running PyTorch 1.11.0+cu102 on WSL. ",2022-05-10T14:31:13Z,triaged module: linear algebra,open,1,3,https://github.com/pytorch/pytorch/issues/77166,"Note that `cholesky_solve` is simply implemented as  as such, you also have that  Given that these operations are so easy to implement, we have not prioritised moving them to linalg. In fact, as we are doing with `triangular`, we will likely just move `cholesky_solve`, and expect that the user uses the construction above if they want to materialise the inverse. This is already tracked in  CC(Move `torch.cholesky_solve` into `torch.linalg`.)"," Personally, I think it's absolutely worth keeping around `cholesky_inverse`. Just like how `torch.cat`, `torch.concat`, and `torch.stack`, `torch.hstack`, and `torch.vstack` can be used to implement each other in straightforward ways, it's still absolutely worth keeping them all around to fulfill their common usecases. Similarly, `cholesky_inverse` is such a straightforward description of a commonly desired function. Even if it's not much work to implement it in terms of `cholesky_solve` or `solve_triangular`, I still think getting rid of `cholesky_inverse` is the wrong move here. And if you're going to keep `cholesky_inverse` around, it should be moved to `torch.linalg` along with the other related functions. It's definitely not a priority, but I'd imagine it shouldn't be tremendous work to move `cholesky_inverse` wholesale into the the `torch.linalg` module....","One of the rules we follow to choose what goes in and what doesn't is whether NumPy / SciPy have these functions. In this case, they just have `cho_solve`. As such, you could consider pitching adding this function to SciPy. If they add it, we could then consider adding it."
rag,"Support Tensor source for x.set_(storage, offset, size, strides)","Stack from ghstack:  CC(Add OpInfo based meta tensor tests [RELAND])  CC(Support Tensor source for x.set_(storage, offset, size, strides)) This is convenient for cases where we don't have Storage bound correctly (e.g., meta tensors).  It is also consistent with a universe where we get rid of storages, although arguably this is never gonna happen. Signedoffby: Edward Z. Yang ",2022-05-06T23:50:28Z,Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/77007,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77007**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c174dbea2c (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Disable transformer/MHA fast path when autocast is enabled,  CC([PyTorch] Disable transformer/MHA fast path when autocast is enabled) We need predictable Tensor dtypes because we call data_ptr. Autocast messes with this Differential Revision: D36190109,2022-05-06T01:48:07Z,cla signed Stale,closed,0,8,https://github.com/pytorch/pytorch/issues/76934,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76934**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 10 New Failures, 1 Base Failures As of commit 68221cfcc1 (more details on the Dr. CI page): Expand to see more  * **10/11** failures introduced in this PR * **1/11** broken upstream at merge base b109658649 on May 05 from  4:13pm to  6:52pm   :detective: 10 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6326488492?check_suite_focus=true) pull / linuxxenialpy3.7gcc7 / test (default, 2, 2, linux.2xlarge) (1/10) **Step:** ""Test"" (full log  :repeat: rerun)   20220506T18:13:05.2186076Z RuntimeError: test_jit failed!      :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * pull / linuxbionicrocm5.1py3.7 / test (default, 1, 2, linux.rocm.gpu) on May 05 from  4:13pm to  6:52pm (8ac6b0a010  a0ebf1d386)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","argh, `torch.is_autocast_enabled` does not work with JIT, how is that even possible",,"Errr, that's unfortunate. We don't have `autocastEnabled` exposed... :cry:  https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/autocast.hL12 You can add an entry here for that and query that one for jit instead. https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/python/init.cppL225 These are not thread safe...  https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/autocast.cppL469L477 IIRC, autocast is enabled per thread, maybe we can change `autocast_enabled` to thread_local.",> query that one for jit instead. am I supposed to branch on some sort of torch.is_jit thing? I'm missing a piece I think,  might work,"> is_scripting that's not going to help. I need to make the exact same check (is autocast enabled) regardless of whether the JIT is involved or not. need the JIT folks to fix this, and ideally need to not have to do extra stuff to make perfectly normal functions work in TorchScript.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,improve index_select performance on CPU,"Stack from ghstack:  CC(improve cat performance on CPU)  CC(improve index_select performance on CPU) Improve some hotspot operators in transformer models. This PR is targeting at further improving `torch.index_select` performance on CPU. `index_select` is a critical hotspot in decoderbased Transformer variants with TopK sampling approach, also it is widely used in other scenarios like `nn.Embedding`. The existing implementation on `index_select` has a fast path when `dim == 1`, this PR parallels it. The existing implementation is generally OK when used in embedding,  e.g. with input like '[50000, 128]', while the index_size (50000) is big enough. it will parallel on index_size. But for transformer shapes, the current implementation is not very efficient: The transformer shape is like [5, 8, N, 64] and N ranges from 2 to 114 and dim = 2. * when N is very small: e.g. N=2, the input shape is [5, 8, 2, 64], it is a sequential run, actually most of time is spent is on constructing TensorIterator itself since the problem size is very small. * when N is median size: e.g. N=50, slice_size  grain_size, it will parallel on index_size (e.g. 5). The magic numbe",2022-05-05T02:47:48Z,open source cla signed Stale intel,closed,0,5,https://github.com/pytorch/pytorch/issues/76868,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76868**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9746998bdc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,">  can you please rebase, as import/land attempt fail (build_variables.bzl were moved to top level folder) Probably still need to update this one since we recently started the optimization project on PyG. Need to make sure the current optimization is able to cover the scenarios from PyG models...","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: mingfeima / name: Ma Mingfei  (721c2663124470d436c9bcc6a65882a292ccce0c, d08de41ef603919a932549746c72a02189c7a865, 9746998bdcc51df5248208f7af0beeb6a35e4156)"
yi,Support indexing of the underlying tensors for nested tensors," ðŸš€ The feature, motivation and pitch Nested tensors were recently added to PyTorch core, but operator support is still fairly sparse. Notably, it's not yet possible to index the underlying tensors:  It'd be great to have simple indexing work to get the underlying tensors from a nested tensor. To start, this could support only indexing on the implicit ""batch dimension"" that nested tensor provides & error for anything else.  Alternatives None  Additional context _No response_ ",2022-05-04T20:39:11Z,triaged module: nestedtensor,closed,0,1,https://github.com/pytorch/pytorch/issues/76843,"Here's some useful code pointers for adding support. On the python side, the function called for indexing is `torch._C._TensorBase.__getitem__`, implementing the `Mapping` interface. This is defined for the `_TensorBase` class on the C++ side here: https://github.com/pytorch/pytorch/blob/55f55a4cf6cc50cde9e8e8369d92847ca85b23da/torch/csrc/autograd/python_variable.cppL1393 https://github.com/pytorch/pytorch/blob/55f55a4cf6cc50cde9e8e8369d92847ca85b23da/torch/csrc/autograd/python_variable.cppL1301L1305 https://github.com/pytorch/pytorch/blob/465e0ae2665b5474edf494247c9c809fbaac5210/torch/csrc/autograd/python_variable_indexing.cppL265L341 https://github.com/pytorch/pytorch/blob/465e0ae2665b5474edf494247c9c809fbaac5210/aten/src/ATen/TensorIndexing.hL469L540 This calls `applySelect()` for the 1D integer index case: https://github.com/pytorch/pytorch/blob/52af4fc5baee363bb9c3170017d46d4fe0069c56/aten/src/ATen/TensorIndexing.hL215L236 which dispatches to: https://github.com/pytorch/pytorch/blob/710246ea99478c011c7dff5690d1a3470d9b3aaa/aten/src/ATen/native/native_functions.yamlL3897L3903 The `select.int` function should be implemented for the `NestedTensorCPU` / `NestedTensorCUDA` dispatch keys."
rag,[PT-D][Sharding] Clean up sharded tensor code by leverage handle_torch_function,"  CC([PTD][Sharding] Clean up sharded tensor code by leverage handle_torch_function) In https://github.com/pytorch/pytorch/pull/75374, we have added some ops for sharded tensor and there are some duplicate code which can be removed by `leveraging handle_torch_function` Differential Revision: D36135157",2022-05-04T16:47:25Z,oncall: distributed cla signed sharded_tensor release notes: distributed (sharded) topic: new features,closed,0,2,https://github.com/pytorch/pytorch/issues/76824,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76824**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit d3b1d275c3 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[checkpoint] Handle overlapping storage during save and load," ðŸš€ The feature, motivation and pitch When loading a checkpoint, we should check whether any of the tensors have overlapping storage and, if so, we're loading data from different storage_keys. We should eliminate the superfluous load is such cases. A similar check should be performed when saving a checkpoint and use that to reduce the amount of IO done.  Alternatives _No response_  Additional context _No response_ ",2022-05-03T16:40:09Z,oncall: distributed triaged sharded_tensor,open,0,0,https://github.com/pytorch/pytorch/issues/76745
finetuning,RuntimeError while optimizing a TorchScript model," ðŸ› Describe the bug I run into a peculiar issue when trying to carry out finetuning on a TorchScript model: during the **second** `backward()` call it fails with a `RuntimeError`. The same PyTorch model can be optimized without problems and the issue also disappears if lines 1720 are wrapped into a `torch.no_grad()` block and / or strangely if the `print` statement on line 22 gets uncommented.  Code   Error   Versions PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.22.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.13.040genericx86_64withglibc2.29 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [conda] Could not ",2022-05-03T15:02:40Z,oncall: jit,closed,0,1,https://github.com/pytorch/pytorch/issues/76739,This doesn't repro for me on master any more. Please feel free to reopen if you see an issue on master.
rag,[Model Averaging] Support disabling post-local gradient sync,"I find that sometimes disabling intrasubgroup gradient allreduce can still give a satisfying accuracy for some cases, so better to make such gradient averaging configurable. This does not take into account the saving in the communication of allreducing gradients.",2022-05-03T07:35:41Z,oncall: distributed open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/76723,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76723**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: 1 Base Failures As of commit e832ff58bf (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 81528d4b21 on May 10 from  4:33pm to  9:34pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * pull / linuxbionicrocm5.1py3.7 / test (default, 1, 2, linux.rocm.gpu) on May 10 from  4:33pm to  9:34pm (02713221e3  3a68155ce0)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","> Sounds good, but this is no longer local SGD then? Curious why disabling intranode allreduce can help. This allows disabling gradient averaging. It still keeps the concurrent parameter/model averaging, which is the crux of local SGD.","> Looks like a test is failing, can you take a look? test_ddp_hook_parity_post_localSGD: https://github.com/pytorch/pytorch/runs/6285865880?check_suite_focus=true Should be fixed now.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Support no sharding config,  CC(Support no sharding config)  CC(Provide an auto wrap policy for common transformer models) supporting no sharding config to make it similar to DDP algorithm Differential Revision: D36050353,2022-05-01T06:42:13Z,oncall: distributed cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76628,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76628**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 7554f35024 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6245169194?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220501T06:55:17.4357861Z The PR is introduc...m to confirm whether this change is wanted or not.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," yes, I've been benchmarking this version of ddp vs PyTorch C++ ddp, the perf gap is small and depends on the wrapping strategy. For 1GB bert model on 32 gpus with slow network connected.   1. if the bucket size is small like 25MB for PT DDP and wrapping min size is like 28MB for this version of ddp, this version of ddp has around 6% perf regression. The difference is because this version of ddp wrapping needs one more all reduce, not due to the python context switch;    2. if the bucket size is large like 40MB for PT DDP and wrapping min size is like 40MB for this version of ddp, this version of ddp has around 17% perf regression. because the last FSDP unit in this version of DDP wrapping has large delay to kick off the first all reduce; The difference is not due to the python context switch   3. if the bucket size larger than the model size, both PT DDP and this version of DDP will have single all reduce, they have the similar performance. That means python context switch is not a big concern again.   Overall, I think if the wrapping can be done well in this version of DDP and aligned with PT DDP bucketing orders, then the performance will be comparable. As you can see, we still need to improve our auto wrapping policy for both no_shard and fsdp strategy.   Once the auto wrapping issues is resolved for this API overall, it is promising to make this API back compatible with PT C++ DDP and merge them in the long run. ","closing this, in favor of https://github.com/pytorch/pytorch/pull/76736"
rag,Expand pow and float_pow sampling function for more coverage," ðŸ› Describe the bug This issue is linked to:  CC(xlogy, xlog1py are all skipping TestGradients) with matching PR: CC(Remove pow and float_power TestGradient Skips) After looking into gradient computations for the pow and float pow the nans arise when lhs or base tensor has values that are negative.  Since the exponent or rhs tensor has fractional values for the opinfo test the power function can and does map floats > complex  values in the analytical case. An example of nans in the function.   In order to unskip TestGradients the lhs low value can be set to 0 so that this tensor doesn't include any negative values.   Follow up work would be maybe adding documentation that includes this note. Or updating pow to promote the output tensor to complex type.   Versions Collecting environment information... PyTorch version: 1.12.0a0+git2469525 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.3) CMake version: version 3.22.1 Libc version: N/A Python version: 3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ] (64bit runtime)",2022-04-27T21:05:57Z,module: tests triaged,open,1,7,https://github.com/pytorch/pytorch/issues/76483,Sounds like forward tests should fail also? Why were only gradient tests failing previously?,https://github.com/pytorch/pytorch/blob/a997046017aa9f97213f6bd50bc152a6445e8086/test/test_binary_ufuncs.pyL1048 I am not sure if this is the only place that the forward is tested but reading this looks like the base is bounded below by 1 or 0 . Also running np.power() on tensors with negative elements produces the same NaN behavior ,Also if i change the the low=1 on line 1048 and run the binary_ufunc_tests: `FAILED test/test_binary_ufuncs.py::TestBinaryUfuncsCPU::test_pow_cpu_bfloat16  ValueError: math domain error`,Nice analysis! Once the PR lands let's update the original issue to reflect it no longer applies to the pow and float_power operators,> I am not sure if this is the only place that the forward is tested but reading this looks like the base is bounded below by 1 or 0 . Maybe `test_reference_numerics` would have caught if the OpInfo for pow had a `.ref` field. Would it be `np.power` here? , Holy cow I can't believe that pow doesn't have a reference implementation! We should really prioritize adding them. (Maybe we could sneak pow's reference into this PR?),If I add `ref=np.power` get a number of errors: `ValueError: Integers to negative integer powers are not allowed.`  
rag,Add from_blob variation with storage_offset argument,  CC(Add from_blob variation with storage_offset argument) Adding a from_blob that lets me set storage_offset. Differential Revision: D35979729,2022-04-27T20:46:56Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/76478,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76478**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9dca745331 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
yi,Fix GenLazyIR.node_base_ctor_call,"  CC(Make lazy tensor ptr class customizable)  CC(Make lazy tensor creation and value strings customizable)  CC(Fix GenLazyIR.node_base_ctor_call) Make node_base_ctor_call produce the entire node_bace_ctor_call. Previously it was only producing the beginning of the call, which was unintended. Addresses part of https://github.com/pytorch/xla/issues/3472 Differential Revision: D35980436",2022-04-27T19:57:42Z,cla signed topic: not user facing release notes: lazy,closed,0,2,https://github.com/pytorch/pytorch/issues/76471,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76471**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit ffe9782372 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
yi,[old]Fix GenLazyIR.node_base_ctor_call,old stack.. replaced with  https://github.com/pytorch/pytorch/pull/76471,2022-04-27T18:44:40Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/76458,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76458**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 1a6ffa40a5 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,oops i messed up my stack so i had to make a new one https://github.com/pytorch/pytorch/pull/76471
yi,"Revert ""Revert ""Allow specifying tags for aten operators in native_functions.yaml""""","Stack from ghstack:  CC(Revert ""Revert ""Allow specifying tags for aten operators in native_functions.yaml"""") This reverts commit ea44645c9a682a4e212e64b94a86383c3388ed6b. reland of https://github.com/pytorch/pytorch/pull/72549",2022-04-27T18:15:48Z,oncall: jit cla signed,closed,0,7,https://github.com/pytorch/pytorch/issues/76456,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76456**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 28512057ec (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6203870990?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220428T02:10:40.8676526Z python: can't open...un_server.py': [Errno 2] No such file or directory     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","~Is there a reason for this revert? Can we include that in the PR description?~ Nevermind I see this is a reland, can we include a link to the original PR?","> ~Is there a reason for this revert? Can we include that in the PR description?~ >  > Nevermind I see this is a reland, can we include a link to the original PR? added in the description", merge this please,Merge failed due to Command `git C /home/runner/work/pytorch/pytorch cherrypick x edafdc7562939df7586c72910067bccdde6e3831` returned nonzero exit code 1  Raised by https://github.com/pytorch/pytorch/actions/runs/2236559551, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Provide an auto wrap policy for common transformer models,  CC(Provide an auto wrap policy for common transformer models) Provide an auto wrap policy for common transformer models Differential Revision: D35972488,2022-04-27T17:19:06Z,oncall: distributed cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76455,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76455**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 38295bbab0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"ci: Unblock syncbranches, add a58c6ae and 7106d21 to block list","Adds a58c6aea5a0c9f8759a4154e46f544c8b03b8db1 and 7106d216c29ca16a3504aa2bedad948ebcf4abc2 to the list of excluded commits since this was landed through phab and cherry picked to master directly Similar to https://github.com/pytorch/pytorch/pull/76231 In both cases the original `syncbranches.yml` run came up with `Nothing to do`: * Back out ""record_function: update to use custom_class API"" (a58c6aea5a0c9f8759a4154e46f544c8b03b8db1)   * https://github.com/pytorch/pytorch/runs/6153245247   * Originally landed through GHF, landed internally, reverted using GHF, relanded using GHF, reverted in Phab, cherry picked onto master * [PyTorch] Add native fast path for transformer encoder inference (7106d216c29ca16a3504aa2bedad948ebcf4abc2)   * https://github.com/pytorch/pytorch/actions/runs/2223597371   * Originally landed through GHF, reverted using GHF, relanded using Phab Signedoffby: Eli Uriegas ",2022-04-26T23:15:19Z,cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76417,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76417**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f92eac5718 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,rocm averages 20m to pull docker image,"For the past week, rocm (specifically linuxbionicrocm5.0py3.7 / test) averages about 20m to pull the docker image, while the other linux jobs generally take about 2m. Example GHA runs: https://github.com/pytorch/pytorch/runs/6178824044?check_suite_focus=true, https://github.com/pytorch/pytorch/runs/6178824044?check_suite_focus=true, https://github.com/pytorch/pytorch/runs/6132356305?check_suite_focus=true ",2022-04-26T22:50:26Z,module: rocm,closed,1,2,https://github.com/pytorch/pytorch/issues/76413,"We discussed this with  yesterday.  Proposing CC([ROCm][GHA] keep docker images for at most 1 day) to help.  ROCm runners shouldn't need to completely wipe out all docker images, just the containers.  This should hopefully speed up docker image pulls. Followup: Is there a way to generate this timetopull metric rather than looking at individual job logs?  ","> Followup: Is there a way to generate this timetopull metric rather than looking at individual job logs? I look at rockset to get averages, but outside of that, I'm not sure"
yi,404 when trying to get pytorch-mutex-1.0-cuda.tar.bz2 from Conda," ðŸ› Describe the bug As part of a Dockerfile, we have a line that runs the following: `conda install pytorch torchvision torchaudio torchtext cudatoolkit channel pytorch && pip install torchinfo torchsummary` This has been working fine, but it stopped working today with the error ""404"" NOT FOUND for url https://conda.anaconda.org/pytorch/noarch/pytorchmutex1.0cuda.tar.bz2 If you go to the page https://conda.anaconda.org/pytorch/noarch/, then you do see pytorchmutex1.0cuda.tar.bz2 listed there, but indeed, going to that file opens up a 404 page instead of downloading the file. What could be going on here? Why was that file removed from the conda channel?  Versions N/A ",2022-04-26T21:01:23Z,high priority triage review module: binaries,closed,2,9,https://github.com/pytorch/pytorch/issues/76401,I can reproduce this,Myself and  are currently looking into this,Pinning this issue since this will affect all conda users until it gets resolved,Issue created for anaconda repo here,"Same here. I used the simplest install command `conda install pytorch torchvision torchaudio cudatoolkit=11.3 c pytorch`, and it complains that pytorchmutex1.0cuda.tar.bz2 does not exist when downloaded.","Update to this, this is an outage from anaconda directly. They have an incident page up here: https://anaconda.statuspage.io/incidents/t1xrstmkf13x","Thanks, all. This is fixed.","> Thanks, all. This is fixed.   what is the solution, and why dont you write the solution?"," This was an issue from April, and I didn't solve it. In the link that  shared you can see the updates from the Anaconda team about that issue. (""Our engineering team has traced the issue back to a recent update. Weâ€™ve reverted this change, resolving the issue for all impacted users."" Posted Apr 27, 2022  00:09 UTC in Anaconda status page)"
yi,Whether 'targetSize' in inferExpandGeometryImpl needs to be checked when it is less than 0," ðŸ› Describe the bug when I run below code  I got error message as below: > Traceback (most recent call last): >   File """", line 1, in  >   File ""/home/pytorch/torch/_tensor.py"", line 341, in __repr__ >     return torch._tensor_str._str(self) >   File ""/home/pytorch/torch/_tensor_str.py"", line 439, in _str >     return _str_intern(self) >   File ""/home/pytorch/torch/_tensor_str.py"", line 414, in _str_intern >     tensor_str = _tensor_str(self, indent) >   File ""/home/pytorch/torch/_tensor_str.py"", line 264, in _tensor_str >     formatter = _Formatter(get_summarized_data(self) if summarize else self) >   File ""/home/pytorch/torch/_tensor_str.py"", line 92, in __init__ >     tensor_view = tensor.reshape(1) > RuntimeError: Trying to create tensor with negative dimension 2: [2, 2, 1, 3] It seems to report 'reshape' has invalid parameterï¼Œ actually expand op does.  It causes confuse sometime. Maybe torch should check 'targerSize' in inferExpandGeometryImpl to make sure it is greater than or equal to 0.  Versions '1.12.0a0+git0aa3c39'",2022-04-26T03:37:57Z,module: error checking triaged,open,0,1,https://github.com/pytorch/pytorch/issues/76362,Agree that the error message could be clearer
rag,[ONNX] Check tensor has storage before refer to tensor data ptr,"In the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.",2022-04-25T21:32:40Z,oncall: jit module: onnx open source cla signed release notes: onnx topic: bug fixes,closed,0,3,https://github.com/pytorch/pytorch/issues/76342,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76342**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit dd3a100496 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :female_detective: 1 failure *not* recognized by patterns: The following CI failures may be due to changes from the PR    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add native fast path for transformer encoder inference (re-land),"  CC([PyTorch] Add native fast path for transformer encoder inference (reland)) The current PyTorch multihead attention and transformer implementations are slow. This should speed them up for inference. This was previously erroneously landed via GitHub, but needs to land via fbcode. Differential Revision: D35239925 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-25T19:13:27Z,cla signed release notes: nn release notes: sparse topic: performance,closed,0,1,https://github.com/pytorch/pytorch/issues/76333,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76333**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 98c64024f0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
rag,[checkpoint] Use fsspec to support object storage," ðŸš€ The feature, motivation and pitch Extends FileSystemStorageReader / FileSystemStorageWrite to work with fsspec so they can be used with object storage.  Alternatives _No response_  Additional context _No response_",2022-04-25T18:12:36Z,triaged sharded_tensor,closed,0,1,https://github.com/pytorch/pytorch/issues/76325,Closing as we now have https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/_fsspec_filesystem.py
transformer,improve log_softmax multi-core performance,"Stack from ghstack:  CC(improve log_softmax multicore performance)  CC(add BFloat16 support on CPU for cumsum, cumprod and logcumsumexp and adjust grain size)  CC(improve sort cpu bfloat16 perf by directly comparing on acc_type)  CC(improve sort multicore perf by adjusting grain_size w.r.t. dim_size) Differential Revision: D36337195 This PR is to improve `logsoftmax` multicore performance by decoupling `grain_size` and `chunk_size`. This original implementation has a minimum `grain_size` of `chunk_size` so that log can use vectorized implementation. This limits the parallelism for most of the input shapes in transformers. For example, the shape `[512, 28996]` would use only use 2 cores at most. We can decouple `grain_size` and `chunk_size`, the vec::map will still ensure to use vectorized implementation (it will add padding if necessary and do a vectorized log). Benchmark data on Xeon 6248, 20 cores per socket, dual socket, 2.50GHz:  cpu performance on single socket (20 cores):   cpu performance on 4 cores run   cpu performance on 1 core run  ",2022-04-24T06:43:43Z,module: cpu open source cla signed topic: not user facing intel,closed,0,10,https://github.com/pytorch/pytorch/issues/76279,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76279**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 4 New Failures As of commit 258a41f4b3 (more details on the Dr. CI page): Expand to see more  * **4/4** failures introduced in this PR   :detective: 4 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6749307404?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7 / test (default, 1, 4, linux.4xlarge.nvidia.gpu) (1/4) **Step:** ""Test"" (full log  :repeat: rerun)   20220606T03:07:47.7424398Z RuntimeError: CUDA error: an illegal memory access was encountered     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","TODO:  [ ] experiment with implementation with parallel on inner dimension of logsoftmax. With commonly used shapes in logsoftmax from Transformer models, the lastdim is quite large (20k~300k). And the kernel of logsoftmax requires to read the input for 3 times, so it is possibly to be swapped out of cache. The cuda kernel utilized different block setup rules for different input shapes, which will go parallel on lastdim in case it is big enough so as to improve cache hit rate for the input data. Try to implement similar kernel for CPU (need to plant it into 3rd party, e.g. fbgemm, since the kernel requires sync inside omp loops and currently aten won't compile it)","wei has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","wei has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.",Pls check failed test cases.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: mingfeima / name: Ma Mingfei  (80e1817b47425326a99e72291faaff1701108c78, 97a7e49db6d523ec90b811d7773ddcd43157ecbd, dbfe108107a605874c950a28608ddb19b4ed2d77, d60774c3abfe4082bcbdc295b2cb8f45cd59ce65, 0623b4ad4d290e90818fadcb161978bb1df5bdf2, 455fd15d21dcbbac938f4c7bc96814862eebd6fb, b902e1990c78e2fe256adb88b87a3f4fc6fc0eef, e2ff07fb39fda6507eb2ef3d2f73e22fca893e4b, 258a41f4b399865ca4b7d959158dd41f0bac0cb5)","Hi , can we add this change in CC(Fix threadallocation in `_vec_log_softmax_lastdim`)? CC(Fix threadallocation in `_vec_log_softmax_lastdim`)'s merging had failed earlier but I made more changes &  approved it y'day. Thanks!","> Hi , can we add this change in CC(Fix threadallocation in `_vec_log_softmax_lastdim`)? CC(Fix threadallocation in `_vec_log_softmax_lastdim`)'s merging had failed earlier but I made more changes &  approved it y'day. Thanks! Oh, cool. I will close this one!"
rag,[Model Averaging] Make the error message more informative in hierarchical_model_averager.py,As title,2022-04-24T05:09:16Z,oncall: distributed open source cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76277,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76277**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit cf444eae15 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,The document's representation of the underlying document," ðŸ“š The doc issue Hello,  Thanks you all! I am looking for a function of torch.stft today. But I only found that it ended up returning ._VF.stft.  I didn't find any description of the underlying implementation.  I have some doubts about this.   Suggest a potential alternative/fix _No response_",2022-04-24T03:41:31Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/76276,The documentation for torch.stft can be found here
transformer,deploy: add dummy metadata for builtin packages,"This adds dummy metadata for frozen builtin packages when using `torch::deploy`. This is a bit hacky but unblocks allows Huggingface transformers library to be used within `torch::deploy` which depends on `importlib.metadata.version` to detect whether torch is installed or not. https://github.com/huggingface/transformers/blob/main/src/transformers/utils/import_utils.pyL49 Test plan: Added `importlib.metadata.version(""torch"")` unit test",2022-04-22T00:56:05Z,cla signed release notes: package/deploy topic: bug fixes,closed,0,6,https://github.com/pytorch/pytorch/issues/76211,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76211**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit cee8c44fbd (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,Elastic agents cannot properly shutdown/restart after failure," ðŸ› Describe the bug I am trying to run a distributed training using the Elastic Job Controller (from here: https://github.com/pytorch/elastic/tree/master/kubernetes) and it seems as though that the distributed agenst is  be able to shutdown/restart themselves properly. Here is a stacktrace of what happens, eventually, when one of the pods running the agent, dies: I know the underlying issue might be coming from anywhere but I wanted to reach out to the broader community just to see if anyone ran into a similar issue. I also asked the same question here   Versions PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.1 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.144+x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: Tesla V100SXM216GB GPU 1: Tesla V100SXM216GB GPU 2: Tesla V100SXM216GB G",2022-04-21T21:23:08Z,oncall: distributed module: elastic,closed,0,2,https://github.com/pytorch/pytorch/issues/76198,"Hi I see this error message:  Did you get a chance to see what happened on rank 0? As suggested by the message, rank 1 failing may be a side effect of that.","Yes. Actually, I realized that I was not setting `maxrestarts` to a value greater than 0. Once I started passing `maxrestarts=5` to `python m torch.distributed.run` everything started working as expected."
agent,Add optional timeout argument for RpcAgent join(),Differential Revision: D35825382 Changes:  Adds timeout argument to RpcAgent.join()  Update API.py to also include fix bug (missing timeout for signal)  Change default shutdown timeout to 0 (no timeout). Existing functionality in _all_gather will remain the same and wait indefinitely for signal if no timeout is set for the function. New functionality has user specify timeout for both the signal and rpc calls.,2022-04-21T20:23:39Z,oncall: distributed fb-exported cla signed ciflow/trunk release notes: distributed (rpc),closed,0,9,https://github.com/pytorch/pytorch/issues/76194,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76194**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 40c3c13fe9 (more details on the Dr. CI page): Expand to see more  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base 9bf2c87e2b from Apr 27 until Apr 28   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6214025523?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220428T15:49:52.0871716Z python: can't open...un_server.py': [Errno 2] No such file or directory      :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * trunk / linuxbionicrocm5.1py3.7distributed / test (distributed, 1, 1, linux.rocm.gpu) from Apr 27 until Apr 28 (d0cb31d5bc  833d65aecb)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,"Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[quant][core][gpu][feature] Added support for float->quantized cuda tensor copying,"  CC([quant][core][gpu][improvement] Added support for padding quantized cudnn conv2d operator)  CC([quant][core][gpu][feature] Added support for float>quantized cuda tensor copying) Summary: Previously, support for copying a fp tensor to a quantized tensor was limited to CPU tensors. This PR extends the support to GPU tensors. A corresponding test was added to test_qtensor_float_assignment for cuda tensors Test plan:  Differential Revision: D35817832",2022-04-21T15:55:00Z,cla signed release notes: quantization topic: new features,closed,0,7,https://github.com/pytorch/pytorch/issues/76177,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76177**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 152bb6dd06 (more details on the Dr. CI page): Expand to see more  * **5/5** failures introduced in this PR   :detective: 5 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6194983288?check_suite_focus=true) pull / winvs2019cuda11.3py3 / test (default, 2, 2, windows.8xlarge.nvidia.gpu) (1/5) **Step:** ""Test"" (full log  :repeat: rerun)   20220427T15:10:55.9497321Z Build left local git repository checkout dirty     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
transformer,[PyTorch] Fix bulleted lists in transformer fast path docstrings,  CC([PyTorch] Fix bulleted lists in transformer fast path docstrings)  CC([PyTorch] Add native fast path for transformer encoder inference) The formatting was off. Differential Revision: D35797735,2022-04-20T23:00:11Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/76154,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76154**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit b45fbec0e2 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :female_detective: 1 failure *not* recognized by patterns: The following CI failures may be due to changes from the PR    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"was hoping to land previous PR without another update, but internal CI is failing; rolling this in there"
rag,Coverage test is only checking packages and not all submodules,The check for `ispkg` should be removed from this check: https://github.com/pytorch/pytorch/blob/81722f66306aef05334c27bb2a64137d8fa6493a/docs/source/conf.pyL469 The allowlist need to be updated to reflect that. ,2022-04-20T20:42:44Z,module: docs triaged module: python frontend,open,0,0,https://github.com/pytorch/pytorch/issues/76138
rag,Calling storage() on lazy tensor does not work, ðŸ› Describe the bug     Versions master,2022-04-20T18:22:04Z,triaged module: lazy,closed,1,2,https://github.com/pytorch/pytorch/issues/76124,"This error no longer happens, probably after CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage), or maybe CC(Add meta device support to `_UntypedStorage` and `_TypedStorage`):  However, this doesn't seem right:  I don't know very much at all about lazy tensors, but I assume the storage size is supposed to correspond with the tensor's size","This is LTC's problem,  functionalize storages should fix it, so we can close this."
rag,Allow any operation that takes a Storage to also take a contiguous Tensor instead," ðŸ› Describe the bug We don't like storages, right? So we should make it possible to do stuff in Python userland without having to explicitly refer to storage. Allowing all operations on storages to instead accept contiguous tensors is a good start; e.g., `set_(..., storage_offset, size, strides)` should take a contiguous tensor. One fly in the ointment is that even if a tensor is contiguous, it could refer to the inside of a storage (nonzero storage offset).  This should probably also be disallowed when using the API in this way, or accounted for (in the case of `set_`, you can just implicitly adjust `storage_offset` and it will work out)  Versions master",2022-04-20T13:48:10Z,feature triaged module: python frontend,open,0,0,https://github.com/pytorch/pytorch/issues/76106
rag,[quant][core][gpu][feature] Implemented quantized cuda adaptive average pool2d op,"  CC([quant][core][gpu][feature] Implemented quantized cuda adaptive average pool2d op) Summary: The current implementation of quantized cuda adaptive average pooling uses the following: dequant > fp32 adaptive average pooling > quant. This is the same numerically as quantized adaptive average pooling. This is not the ideal implementation, as we desire to operate on the quantized values directly. However, we are currently blocked on this as we are waiting for cudnn's 8.5.0 release, which is anticipated to support adaptive average pooling. When that support is made available, we will use it directly. Test plan:  Differential Revision: D35768751",2022-04-20T01:14:27Z,cla signed release notes: quantization topic: new features,closed,0,10,https://github.com/pytorch/pytorch/issues/76081,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76081**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 9c052d3e4a (more details on the Dr. CI page): Expand to see more  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6204335663?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/3) **Step:** ""Test"" (full log  :repeat: rerun)   20220428T08:48:45.7766630Z [error]The reque...igured HttpClient.Timeout of 100 seconds elapsing.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.",land the max pool 2d PR first: https://github.com/pytorch/pytorch/pull/76081 and then come back and fix the macros here,rebase on master after https://github.com/pytorch/pytorch/pull/76129 is merged with master. should fix the current CI errors," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
transformer,Can 'aten::to_dense' be suppored by 'CUDA' backend," ðŸš€ The feature, motivation and pitch I have run my code on CUDA 11.3 with torch '1.10.1+cu113': sp_attn = torch.sparse_coo_tensor(i, V, (Nt, Ns) ) out =torch.sparse.mm(sp_attn, V).to_dense() however, I have got a CUDA error like this: NotImplementedError: Could not run 'aten::to_dense' with arguments from 'CUDA' backend.  Motivation I think the sparse matrix is more and more useful, especially with the transformer and its selfattention mechanisms explosion trending. The 'aten::to_dense' should not be available for 'SparseCUDA',  Can it be available for 'CUDA' backend? Thank you.  Alternatives _No response_  Additional context _No response_ ",2022-04-19T17:15:40Z,module: sparse triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/76051,"I don't whether it is a bug caused by torch.sparse.mm(). Because when I use the to_dense() directly after constructing a sparse tensor, it is ok. However, when I use to_dense() after torch.sparse.mm(), it will not be ok.",Hello yiqun  Could please try again with a nightly? If it succeeds then it'll have been resolved in the upcoming release.,"Hi,   I have tried it again with the nightly. It succeeds. However, unfortunately,  some other functions such as ""torch.inverse"" are not stable in the nightly version. Thus, I hope the upcoming release can come out soon. Thank you very much"
transformer,Experimental MetaTensorTracer,Stack from ghstack:  CC(Fix unnecessary recursion in GraphModule.__call__)  CC(Experimental MetaTensorTracer) Experimental `Tracer` implementation that carries a meta tensor for each value and allows access to the concrete shape/dtype/etc values. Confirmed to work with HF models (except for `*ForQuestionAnswering`). See https://github.com/pytorch/PiPPy/pull/138 Example: ,2022-04-18T23:13:12Z,cla signed module: fx release notes: fx topic: new features,closed,0,3,https://github.com/pytorch/pytorch/issues/76003,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76003**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit a2a175f4b3 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Disallow calling tolist on tensors with nullptr storage,Stack from ghstack:  CC(Disallow calling tolist on tensors with nullptr storage) In response to D35571561  This code used to segfault before. Not adding this as a test because we should just return a list of zeros in this case. will fix it in a follow up PR. ,2022-04-18T20:01:31Z,cla signed topic: bug fixes module: python frontend,closed,0,3,https://github.com/pytorch/pytorch/issues/75990,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75990**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 212baf01f0 (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6068514265?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/12) **Step:** ""Test"" (full log  :repeat: rerun)   RuntimeError: tolist() shouldn't be called on a tensor with unallocated storage   >>> torch._efficientzerotensor(2).tolist() Traceback (most recent call last):   File """", line 1, in  RuntimeError: tolist() shouldn't be called on a tensor with unallocated storage    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Make an error message more clear in hierarchical_model_averager.py,As title,2022-04-14T21:07:01Z,oncall: distributed triaged open source cla signed,closed,0,7,https://github.com/pytorch/pytorch/issues/75832,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75832**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f0080849b0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,Merge failed due to Refusing to merge as mandatory check Lint has not been run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2227368077, merge this please,Merge failed due to Refusing to merge as mandatory check Lint failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2227386373, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add native fast path for transformer encoder inference,"  CC([PyTorch] Add native fast path for transformer encoder inference) The current PyTorch multihead attention and transformer implementations are slow. This should speed them up for inference. **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-14T18:06:13Z,cla signed release notes: nn release notes: sparse topic: performance,closed,0,22,https://github.com/pytorch/pytorch/issues/75809,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75809**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 4731e208ea (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"> would be simpler to review if it was splitted more 1) just the files for MHA are about 1200 lines, so more splitting is not going to get you below that number 2) This code was already reviewed internally (though there are changes in this PR to deal with PyTorch core integration); if you use Phabricator, you can see which lines haven't changed and focus on ""is this acceptable in PyTorch core"" vs ""is this good software"" for those > there seems to be API added both via native_function and torch.ops. They seems independent? Also we should justify why we're doing this one way or the other? ~The torch.ops stuff is hit only in tests. I'm happy to move it to native_functions.yaml if you don't mind further clutter there.~ will fix. it's an artifact of being initially developed out of core. > Testing should be done via OpInfo for the most part, I am surprised to see 100s of lines of test with hardcoded values and no OpInfo? This is an extension to an existing feature, so I have mostly extended the existing tests, which evidently did not use OpInfo. Converting existing nonOpInfo tests to OpInfo should be out of scope, but I can look into OpInfo for the new tests in test_native_mha.py ~; can you point me to the doc on how to use it?~ > The doc refers to NestedTensor but that should point to something more specific so that users can use that information I don't know how to do that. Can you be more specific? Are you asking me to add a hyperlink in the docstring? How does that work? > torch.ops is known to be ludicrously slow ( CC(Calling `torch.ops.aten.add_` is ludicrously slow)) so most likely should not be used for anything perf sensitive? ~used only in tests.~ will fix.","> I can look into OpInfo for the new tests I'm not adding new public operators here, just an is_inference_mode_enabled fast path, so I don't see how OpInfo applies.","> just the files for MHA are about 1200 lines, so more splitting is not going to get you below that number Removing 30% of the PR is already a very nice improvement! For the MHA files, Isn't it already split into 3 different native functions?  Also they have independent implementations for CPU/CUDA? We already did the hard work of nicely splitting this up in multiple functions (that have their own bindings and tests). So we might as well make them different PR. > I don't know how to do that. Can you be more specific? Are you asking me to add a hyperlink in the docstring? How does that work? Sure, you can do  for example to link to `torch.Tensor`. Not sure where NestedTensor lives on the pytorch namespace, but similar thing would work. If you don't have a class to link to and just want to link to a page, the syntax is for example:  where the first part is the text that will appear and between the `` the link and the `_` at the end is important.","> So we might as well make them different PR. I would agree with you if this was new code, but this has already been developed in a bunch of separate diffs internally and is logically an `hg mv` into PyTorch core. The kernels do not require detailed rereview from scratch.","Some things making this PR difficult to split:  900+ lines just for the MHA kernels, so it will never be small  test_nn.py is huge; splitting out the (comparatively small and, per Phabricator, mostly unchanged) TransformerEncoderLayer fast path would entail a semimanual edit in `hg split` that would need to be performed exactly the same way twice thanks to dirsync  Replicating changes to diff structure across the 3+ different build systems involved is errorprone In short, the cost in my time is high and the perceived benefit given that the code is not new and Phabricator will show what has been changed is not high.",`torch.is_inference_mode_enabled` seems to not work in TorchScript. Not sure why or what to do about it.,> torch.is_inference_mode_enabled seems to not work in TorchScript. Not sure why or what to do about it. I guess you could add support for it in TS... personally I'd probably see if the requires_grad tests were valid TS and do that instead ,"> > torch.is_inference_mode_enabled seems to not work in TorchScript. Not sure why or what to do about it. >  > I guess you could add support for it in TS... personally I'd probably see if the requires_grad tests were valid TS and do that instead I ran into a surprising behavior here: we are somehow getting `requires_grad=True` on some Tensors even though we are running test_transformerencoder_layer in test_nn.py in inference_mode. In other words, testing requires_grad means that inference_mode is not sufficient to hit the fast path!",Talked to  offline; decided to check requires_grad and torch.is_grad_enabled rather than torch.is_inference_mode_enabled. Remaining problems seem to be:  docs build  CUDA files can't see NestedTensor_get_max_size and this is persisting even after I put `TORCH_API` on it for some reason,will send a followup PR to fix formatting of bulleted list in fast path docstrings.,"linuxbionicpy3.7clang9 test failure is puzzling (gating for the fast path somehow failed only on one platform?), and Dr. CI didn't call it out in the comment, confused about why.","""MultiheadAttention does not support NestedTensor outside of its fast path. The fast path was not hit because some Tensor argument has_torch_function"" on linuxbionicpy3.7clang . Why the heck are there platformspecific torch_functions? I guess I will patch the test to check itself if there are torch_functions set up for some reason and do the right thing.","That must be the crossref config; it runs all the tests with a noop torch function to check if our implementations invariant to a torch function mode. In your case, you should skip the fast path tests here.", merge this please,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2218269607, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert this,"(made a mistake attempting to merge, going to rediff and land from fbcode)",Reverting PR 75809 failed due to Can't revert PR that was landed via phabricator as D35239925 Raised by https://github.com/pytorch/pytorch/actions/runs/2221162526,Manually reverted with my admin powers: https://github.com/pytorch/pytorch/commit/2387efd35601205eabeb0dfe90731aa1dbf0eabb
transformer,[PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys,"  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys) Just as it is often difficult to write a single kernel that can handle both CPU and CUDA, so can it be difficult to do the same for NestedTensor. Differential Revision: D35603836 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-14T18:03:42Z,cla signed topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/75808,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75808**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8cb23aaa26 (more details on the Dr. CI page):  * **3/3** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6069885628?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this, merge this,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2170625831,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2170625826
transformer,[PyTorch] Add test for all-masked case for native softmax,  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys)  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-14T16:44:37Z,cla signed topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/75803,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75803**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 832cfa548b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"previously commented on as https://github.com/pytorch/pytorch/pull/75493, had to send a new PR due to tooling issues", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","  please start adding these labels, we all do it","I do add them, but I don't check github notifications particularly often and you keep beating me to it :)"
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-14T16:43:45Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75802,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75802**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 75cc3e0ae7 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
yi,Large numerical error when applying nn.Linear in RTX A6000 with cuda>=11.1," ðŸ› Describe the bug  Applying a simple Linear layer (Y = aX) is generating incorrect output with A6000 GPUs using cuda >=11.1  Report:  1.In the code snippet, we have a simple linear layer (without bias). First we use nn.Linear to compute Y=aX. Next we explicitly multiply the two tensors, We expect the norm of the difference to be 0 in all cases (can be confirmed by uncommenting the line setting device to ""cpu""). However a significant error is seen with A6000 GPUs using cuda >= 11.1 (Please see output table). 2. No error observed for CPU, and other GPUs at our disposal (NVIDIA RTX, TITAN X ) 3. With A6000, **no error for cuda 11.0**. 4. With A6000, **error confirmed for cuda 11.1 and  cuda 11.3**.  5. This error is deadly serious since it is an insidious corruption, without any obvious outward symptoms. In fact, if we uncomment the other line of code, it can be seen that a significant error is introduced even when multiplying with 1.  Code Snippet:   Output    Versions Collecting environment information... PyTorch version: 1.10.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_",2022-04-13T16:26:38Z,high priority module: cuda triaged module: tf32,open,2,3,https://github.com/pytorch/pytorch/issues/75740,"Marking hipri for potentially silently incorrect behavior. I don't know if this is just a numerical stability problem, though","This is likely related to tf32, if this error is affecting your results turn it off by `torch.backends.cuda.matmul.allow_tf32 = True` https://pytorch.org/docs/master/notes/cuda.htmltf32onampere","> This is likely related to tf32, if this error is affecting your results turn it off by `torch.backends.cuda.matmul.allow_tf32 = True` https://pytorch.org/docs/master/notes/cuda.htmltf32onampere Yes, error disappears when setting the flag to False on A6000 with torch versions: 1.10.1+cu113, 1.9.1+cu111, 1.7.1+cu110"
rag,expand the coverage of conv folding,"Expand the coverage of conv folding, such as conv>mul>add>bn etc.",2022-04-13T08:05:11Z,oncall: jit triaged open source Merged cla signed intel,closed,0,8,https://github.com/pytorch/pytorch/issues/75724,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75724**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 New Failures As of commit 005fcc7c76 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6384864317?check_suite_focus=true) pull / linuxxenialpy3.7clang7asan / test (default, 2, 5, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220511T09:00:40.9078653Z SUMMARY: Undefined.../jenkins/workspace/aten/src/ATen/Utils.cpp:20:3 in     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," , I have modified the code to solve the problem of UT failure, could you please review it again?", , rebase this please, do you mind rebasing ? sorry,">  do you mind rebasing ? sorry Done, sorry for late reply.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Added functorch to functional_autograd_benchmark,Description:  Following https://github.com/pytorch/functorch/issues/497 adding an option to run benchmarks with functorch and compare to original functional autograd results. Running the benchmark we get below table:   Table      Stdout    All functorch errors are reported in its repository.   ,2022-04-12T19:32:21Z,triaged open source cla signed topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/75689,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75689**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 7e86a3b696 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,",  for autograd as well if you guys have any comments (I'll take a pass through this later today, sorry for the delay!)", merge this please,Merge failed due to Refusing to merge as mandatory check Lint has not been run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2189993744,"5, I guess we're not allowed to skip CI :). Can you push a new empty commit (or rebase this) when you get a chance?", sure. I skipped CI to avoid unnecessary CI run if we had to update the the comment., merge this please,"Hey 5. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[FSDP] [Mixed Precision] using param_dtype breaks transformers ( in attention_probs matmul)," ðŸ› Describe the bug Using PyTorch nightly, enable mixed precision support parameters for fsdp. When running with a transformer, you will hit the following error  float expected but received half or bfloat (depending on mp config):  Note that comm mp and buffer mp all work as expected.  You can use partial mixed precision by setting the param_dtype to fp32:  The issue is specific to the attention_probs matmul:  and previous matmuls work as expected:  Enabling stack trace yields: File ""/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/transformers/models/bert/modeling_bert.py"", line 340, in forward     context_layer = torch.matmul(attention_probs, value_layer) RuntimeError: expected scalar type BFloat16 but found Float Exception raised from data_ptr at aten/src/ATen/core/TensorMethods.cpp:18 (most recent call first): frame CC(æœªæ‰¾åˆ°ç›¸å…³æ•°æ®): c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fa8dadacb82 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libc10.so) frame CC(Matrix multiplication operator): c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x5b (",2022-04-12T17:58:38Z,triaged module: fsdp,closed,3,4,https://github.com/pytorch/pytorch/issues/75676,"Small repo to quickly reproduce: https://github.com/lessw2020/mixprecision HF transformer dependency  you may need to **""pip install transformers""** if transformer import fails.  git clone and run ""python fsdp_mp.py"" should put you right into stack trace. adjust active policy to relevant configs to toggle between working (bfSixteen_working) and not (bfSixteen).  ","Discussed with Less offline, pasting result from discussion here:  From some debugging, the issue is in this line: https://github.com/huggingface/transformers/blob/db9f189121b6bf06f8f0825fa6e0f051c8e46b27/src/transformers/models/bert/modeling_bert.pyL985 because the `extended_attention_mask` is computed in a helper function that is unaware of FSDP / mixed precision. There is a hack we can add in the BERT code we can do which enables the model to train, but we're not sure if FSDP can have a fix for this or if we need to fix it in the BERT code somehow. Essentially we need to figure out whether FSDP can somehow inform functions that generate tensors such as these to move the input to fp16 or not. ","Hi, is there a decision made on whether this will be fixed at the FSDP layer or at the HuggingFace layer? ",It is unlikely that we will fix this in the FSDP layer. I will mark this as not planned for now.
yi,Misleading documentation for cholesky_inverse," ðŸ“š The doc issue https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html The documentation suggests that the inputted matrix should be the original positivedefinite matrix A, where as the examples show that the user must first compute the cholesky u and input that into the cholesky_inverse function.  Suggest a potential alternative/fix Change the parameters section so that it is clear that the input must be the cholesky decomposition. ",2022-04-12T11:14:35Z,module: docs triaged module: linear algebra,open,1,0,https://github.com/pytorch/pytorch/issues/75659
yi,jit fails when trying to assign values to model via hook," ðŸ› Describe the bug I'm using hooks to assign new values to the model on forward passes. Jit script fails with:  For Example:  Fails with    Versions PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 11.6 (x86_64) GCC version: Could not collect Clang version: 13.0.0 (clang1300.0.29.30) CMake version: Could not collect Libc version: N/A Python version: 3.9.8 (main, Nov 10 2021, 09:21:22)  [Clang 13.0.0 (clang1300.0.29.3)] (64bit runtime) Python platform: macOS11.6x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] pytorchlightning==1.6.0 [pip3] torch==1.11.0 [pip3] torchmetrics==0.7.3 [pip3] torchvision==0.12.0 [conda] Could not collect",2022-04-12T08:50:35Z,oncall: jit,open,1,1,https://github.com/pytorch/pytorch/issues/75654,I managed to circumvent this issue by using jit.trace instead of script.
yi,"[quant][core][improvement] Added support for data_ptr<T> for quantized tensors to return pointer to underlying int type (e.g., int8* instead of qint*)","  CC([quant][bcbreaking][improvements] Removed quantized_max_pool1d registration)  CC([Quant][core][improvements] Combined dispatch registration for max_pool1d & quantized_max_pool1d)  CC([quant][bcbreaking][improvements] Removed quantized_max_pool2d registration)  CC([Quant][core][improvements] Combined dispatch registration for max_pool2d & quantized_max_pool2d and implemented max_pool2d_with_indices_out_quantized_cpu)  CC([quant][core][improvement] Added support for data_ptr for quantized tensors to return pointer to underlying int type (e.g., int8* instead of qint*)) Summary: Previously, when using `data_ptr` on a quantized tensor, it did not provide support for returning a pointer to the underlying int tensor. Instead, it returned a pointer to a quantized tensor (e.g., qint*), and backend users had to call, e.g., reinterpret_cast(quantized_tensor.data_ptr()) to cast it to an int* pointer. This PR enables direct support for returning the underlying pointer without need for casting. Differential Revision: D35569200",2022-04-12T00:45:03Z,cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/75643,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75643**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit b0e87eb14f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Docs: Detail 3D tensor shape for transformer masks,Fixes CC(mask parameter in Transformer support 3D tensor but not mentioned).,2022-04-09T02:45:30Z,triaged open source cla signed release notes: nn topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/75552,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75552**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 40f4ae3a5e (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5951820182?check_suite_focus=true) pull / linuxxenialpy3.7clang7asan / test (default, 2, 3, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220409T03:25:05.4712079Z RuntimeError: test_cpp_extensions_aot_no_ninja failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add test for all-masked case for native softmax,  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-08T03:57:46Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75493,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75493**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 5395edc79f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"ghexport stopped wanting to update this, so I've had to send a new PR  https://github.com/pytorch/pytorch/pull/75803"
transformer,[PyTorch] Add NestedTensor support functions for transformers,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:44:05Z,cla signed release notes: sparse topic: new features,closed,0,3,https://github.com/pytorch/pytorch/issues/75491,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75491**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8a7da49dd1 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-08T03:38:18Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75490,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 4bc1f6f82c (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 10 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879643784?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (default, 1, 2, linux.2xlarge) (1/10) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:38:09Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75489,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75489**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3b1cec5fb0 (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879665674?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (noarch, 1, 1, linux.2xlarge) (1/11) **Step:** ""Test"" (full log  :repeat: rerun)   20220408T04:46:01.8541854Z AssertionError: Th...eturned by torch._overrides.get_ignored_functions.     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-08T03:37:08Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75488,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75488**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit d39ced1870 (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879389195?check_suite_focus=true) pull / deploylinuxxenialcuda11.3py3.7gcc7 / build (1/11) **Step:** ""Build"" (full log  :repeat: rerun)   20220408T03:53:42.2858252Z C:\\actionsrunner\\...Tensor_embedding': redefinition; different linkage     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:36:59Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75487,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75487**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit ea3beb7cfc (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879386218?check_suite_focus=true) Lint / clangtidy (1/11) **Step:** ""Generate build files"" (full log  :repeat: rerun)   20220408T03:39:26.5908352Z [36;1m  echo ""ERR...t available for the mergebase of your branch""[0m     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:35:27Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75486,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75486**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit eb7bb376e7 (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879376584?check_suite_focus=true) pull / winvs2019cuda11.3py3 / build (1/11) **Step:** ""Build"" (full log  :repeat: rerun)   20220408T03:37:29.5326646Z [36;1m  echo ""ERR...t available for the mergebase of your branch""[0m     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:32:14Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75485,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75485**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 9f66a1044b (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879352686?check_suite_focus=true) pull / winvs2019cuda11.3py3 / build (1/11) **Step:** ""Build"" (full log  :repeat: rerun)   20220408T03:36:02.0126496Z CMake Error at caffe2/CMakeLists.txt:909 (add_library):     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
rag,Merge torch.cuda._UntypedStorage into torch._UntypedStorage,Fixes CC(Combine `torch._UntypedStorage` and `torch.cuda._UntypedStorage`),2022-04-07T21:08:15Z,module: internals triaged open source Merged cla signed,closed,0,12,https://github.com/pytorch/pytorch/issues/75459,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75459**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9095936ce3 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"There are a lot of issues with this PR at the moment. I'm getting CUDA memory leaks that I haven't figured out yet. Also, now that THPStorage is being used underneath all storages, we're getting some failures related to the fact that THCPStorage used to do some things differently than THPStorage","I'm having some trouble with a segfault at the moment. It happens when I run `python test/test_serialization.py k serialization_map_location`, and it seems like the exact spot where it happens is inconsistent. I'll update when I know more","I found another serialization failure. The old serialization format (with `_use_new_zipfile_serialization=False`) doesn't work correctly for CUDA tensors. For instance:  click to expand   When I run this, it gives:  click to expand   Where the first two elements of the loaded tensor are always different garbage values. I don't get this problem with CPU tensors or if I'm using the new serialization format. It looks like the problem starts in the `torch.save` call. If I load the generated file using a different build of pytorch that doesn't have this PR, the loaded tensor has the same exact garbage values, and they're the same every time I load them. The garbage values change if I save again using a build with this PR. I'll figure out what's going on and fix it. The segfault mentioned in my previous comment probably doesn't have the same exact root cause, since that test is only loading an existing file, not saving one. But it is at least similar in that it's also loading a CUDA tensor with the old serialization format, so there is a chance that the failures are related somehow. To give an update about the segfault issue, I found that it's happening on a `std::vector::push_back()` call within `compute_sizes` in `torch/csrc/utils/tensor_new.cpp`. The vector needs to be resized during that call, so it tries to allocate more memory, and the malloc call segfaults for some reason. It's almost definitely not an issue with the vector itself. Something must have messed up the heap before this pointmaybe something wrote to a location it wasn't supposed to. I briefly tried to work around the vector resize by figuring out how large it needs to be and initializing it to the correct size, avoiding the `push_back` call. That prevented the segfault here, but later on after the `compute_sizes` call, something else tried allocating memory and that also caused a segfault. The `torch.save()` issue with old serialization format for CUDA tensors seems much easier to diagnose, so I'll focus on that. With some luck, once I fix out that issue, maybe the segfault issue will reveal itself to be related",I fixed the serialization issues mentioned above. Still more problems to fix though,"Hey , there are still some loose ends here, but I think it's now in a state where it would be helpful to get some feedback, when you have some time. Also, I'm thinking that it might be good to separate cleaning up the generic storage stuff in `torch/csrc/generic` to a followup PR","I've finished up all my TODOs here so far, CI is passing (the one failure is just a failed network connection), and I think all the unresolved threads above are separable. Let me know if there's anything else I should do"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", rebase this please,"Successfully rebased `combinestoragecpucuda` onto `master`, please pull locally before adding more changes (for example, via `git checkout combinestoragecpucuda && git pull rebase`)", merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PT-D][Sharding] Enable ops needed in the transformer model training,"  CC([PTD][Sharding] Enable ops needed in the transformer model training) From the code base of FairSeq and MetaSeq codebase (which is essentially a transformer model), we have found that loads of ops are not supported by sharded tensor. So we now implement a simple version so that we can at least run a transformer example: Ops include: chuck, transpose, view, mask_fill, dropout, softmax and type_as. Isolate the common logic of registering simple ops into a function and for future register, we just need to implement at most three functions for a new op. Differential Revision: D35123021 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-06T22:29:13Z,oncall: distributed cla signed sharded_tensor release notes: distributed (sharded) topic: new features,closed,0,2,https://github.com/pytorch/pytorch/issues/75374,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75374**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 179766d971 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Addressed comments from reviewers and fix linter errors.
transformer,"[PyTorch] NestedTensor kernels for {r,g}elu{,_}","  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) These are simple elementwise ops it's convenient to be able to use with NestedTensor. Differential Revision: D35448205",2022-04-06T22:01:13Z,cla signed release notes: sparse topic: new features,closed,0,6,https://github.com/pytorch/pytorch/issues/75370,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75370**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 45d19b94dc (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879892464?check_suite_focus=true) pull / winvs2019cuda11.3py3 / test (default, 2, 2, windows.8xlarge.nvidia.gpu) (1/1) **Step:** ""Setup Windows"" (full log  :repeat: rerun)   20220408T04:59:53.2550461Z C:\\actionsrunner\\...5158d0e9b7c.sh: line 2: python3: command not found     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","FWIW I ended up not actually needing these for transformers, but I wrote them so here they are.",Followup work is adding documentation for this,hey  can you GH1 land this stack of 3 PRs?, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,[PyTorch] Add test for all-masked case for native softmax,  CC([PyTorch] Add test for allmasked case for native softmax) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-06T18:30:11Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75348,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75348**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8246877e0c (more details on the Dr. CI page):  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5859309035?check_suite_focus=true) pull / linuxbionicrocm5.0py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/3) **Step:** ""Upload test artifacts"" (full log  :repeat: rerun)   20220406T22:33:31.8675019Z RuntimeError: test_mobile_optimizer failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,[PyTorch] Run test_transformerencoderlayer_gelu on CUDA,  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys)  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327729,2022-04-06T18:28:34Z,cla signed topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/75347,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75347**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 6c57de8ee6 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
transformer,[PyTorch] Run test_transformerencoderlayer on CUDA,  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys)  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327731,2022-04-06T18:27:22Z,cla signed topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/75346,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75346**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3d23bf83c3 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
yi,ProcessGroupNCCL is relying on UB to support bool data type,"In https://github.com/pytorch/pytorch/blob/a90bcd2066389595cd2d1302648dc9232dbc4fe8/torch/csrc/distributed/c10d/ProcessGroupNCCL.cppL79 we can see that ProcessGroupNCCL views a tensor of type `bool` as dtype `uint8_t`. Unfortunately, the C++ standard does not specify any detail about how compilers implements `bool`: https://stackoverflow.com/questions/4897844/issizeofbooldefinedintheclanguagestandard https://stackoverflow.com/questions/37418412/usingreinterpretcastwithbool So we are actually relying on UB for this support. Related discussion: https://github.com/facebookresearch/torch_ucc/pull/64 https://github.com/pytorch/pytorch/pull/50250 cc:  Lebedev ",2022-04-06T17:06:04Z,oncall: distributed module: nccl,open,0,1,https://github.com/pytorch/pytorch/issues/75334,"AFAICT, tensor dtypes have well defined representations that are not tied to C datatypes. For example, `torch.long` is always 64bits."
yi,Make LazyIr.h use provided backend namespace,  CC(Make forced eager fallback optional in codegen)  CC(Make default codegen behavior skip Lower function)  CC(Make LazyIr.h use provided backend namespace) Fixes one of the issues in https://github.com/pytorch/xla/issues/3472 Differential Revision: D35411210,2022-04-05T17:53:41Z,cla signed topic: not user facing release notes: lazy,closed,0,3,https://github.com/pytorch/pytorch/issues/75264,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75264**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 79d341a9e8 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5838584970?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7bazeltest / buildandtest (1/1) **Step:** ""Build"" (full log  :repeat: rerun)   20220405T17:56:02.2489774Z [36;1m  echo ""ERR...t available for the mergebase of your branch""[0m     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"large model, low memory: need `torch.load` that loads one submodule at a time"," ðŸš€ The feature, motivation and pitch Here is a puzzle for you: Say, you have 24GB of GPU RAM and 32GB of CPU RAM and a pretrained fp32 pytorch model checkpoint that is 40GB big. Say, you want to inference or finetune in fp16 or bf16 and you have enough CPU and GPU memory to handle a 20GB model. It should be possible, but you can't load the model in half precision because `torch.load` is inflexible and requires that all 40GB be loaded first. The main issue here is that the user has enough memory to start training in halfprecision, but once they saved the checkpoint, they can't resume since they won't have enough memory to allocate the model and load the checkpoint. Are there any plans to make `torch.load` more flexible and not load the whole thing at once but do it one submodule (or even param) at a time and bonus for converting to the target `torch.dtype` on the fly. In other words the hardware requirements should be close to the final model size and not 2x or 3x. Currently it's 3x when the original is in the higher dtype. Here is the breakdown:  With flexible `torch.load`:  (for demonstration 0.01 is an example of a model with 20 equal layers) At H",2022-04-05T01:05:43Z,module: serialization triaged,open,8,4,https://github.com/pytorch/pytorch/issues/75242, ,"oh, this is old  this has already been solved nicely with https://github.com/huggingface/safetensors/ and it provides lots of other goodies, e.g. also allows saving/updating one weight a time. just need to integrate it into pytorch? But from what I remember  has been working on something similar already.","Hey, noticed that this issue was still open but should have been addressed by `torch.load(mmap=True)`, does this work for the mentioned use case?","I'm not sure this the same functionality, won't mmap create an unnecessary load on the system creating a lot of IO? But as I said `safetensors` solves this neatly by loading only the desired tensors in the first place. And I mentioned you since you said you were working on a similar functionality in `torch.load`. Was I wrong?"
gemma,"PyTorch source code compile fail after ""Built target fbgemm_avx2"""," ðŸ› Describe the bug configuration:  Will link against OpenMP libraries: /opt/rh/devtoolset7/root/usr/lib/gcc/x86_64redhatlinux/7/libgomp.so;/lib64/libpthread.so  Found CUDA: /usr/local/cuda (found version ""10.2"")   Caffe2: CUDA detected: 10.2  Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc  Caffe2: CUDA toolkit directory: /usr/local/cuda  Caffe2: Header version is: 10.2  Found CUDNN: /usr/lib64/libcudnn.so    Found cuDNN: v7.6.5  (include: /usr/include, library: /usr/lib64/libcudnn.so) OS: centos7.6 **command to compile:** PYTORCH_BUILD_VERSION=1.6.0+cu102 \\ 	       PYTORCH_BUILD_NUMBER=0 \\ 	       USE_CUDA=1 \\ 	       USE_CUDNN=1 \\ 	       CUDA_HOME=/usr/local/cuda \\ 	       CUDNN_LIB_DIR=/usr/local/cuda \\ 	       USE_SYSTEM_NCCL=1 \\ 	       NCCL_INCLUDE_DIR=/usr/include \\ 	       NCCL_INCLUDE=/usr/include  \\ 	       NCCL_LIB=/usr/lib64 \\ 	       NCCL_LIB_DIR=/usr/lib64 \\ 	       TORCH_CUDA_ARCH_LIST=""3.5;3.7;5.2;6.0;6.1;7.0;7.5+PTX"" \\ 	       CFLAGS=""Wnoerror=deprecateddeclarations"" \\        python3.6 setup.py bdist_wheel [  9%] Generating python/models/seq2seq/__init__.py gmake[2]: *** [third_party/fbgemm/CMakeFiles/fbgemm_generic.di",2022-04-04T13:40:52Z,module: build triaged,open,2,0,https://github.com/pytorch/pytorch/issues/75184
transformer,[PyTorch] Add native fast path for transformer encoder inference,Summary: The current PyTorch multihead attention and transformer implementations are slow. This should speed them up for inference. Test Plan: CI Differential Revision: D35239925,2022-04-03T23:09:13Z,fb-exported cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/75163,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75163**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 4de0b1cd9e (more details on the Dr. CI page):  * **20/20** failures introduced in this PR   :detective: 18 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879608236?check_suite_focus=true) Lint / quickchecks (1/18) **Step:** ""Ensure all test files have header containing ownership information"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D35239925,  can we break this up into more pieces so that it's not a 3000 line PR?,This pull request was **exported** from Phabricator. Differential Revision: D35239925,sent new PR CC([PyTorch] Add native fast path for transformer encoder inference) because apparently I exported this incorrectly once
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-02T21:50:41Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75152,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75152**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3abfe61b67 (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5802100822?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7bazeltest / buildandtest (1/12) **Step:** ""Build"" (full log  :repeat: rerun)   20220402T22:37:03.4570428Z RuntimeError: test_linalg failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,[PyTorch] Run test_transformerencoderlayer_gelu on CUDA, * (to be filled) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327729,2022-04-02T21:50:35Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75151,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75151**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit a718dadd3e (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5802274711?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (noarch, 1, 1, linux.2xlarge) (1/12) **Step:** ""Test"" (full log  :repeat: rerun)   20220402T22:45:58.8783046Z C:\\actionsrunner\\...vation_out_cuda': is not a class or namespace name     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,[PyTorch] Run test_transformerencoderlayer on CUDA, * (to be filled) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327731,2022-04-02T21:50:29Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75150,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75150**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 39206dc561 (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5802099746?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7 / build (1/12) **Step:** ""Build"" (full log  :repeat: rerun)   20220402T23:05:35.6226674Z RuntimeError: test_linalg failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
yi,Split PyInterpreter into its own file.,"  CC(Disable torch function inside __torch_function__)  CC(__torch_function__ mode)  CC(Dedupe no parsing __torch_function__ handler)  CC(Introduce SafePyObject, make TorchDispatchTypeObject use it)  CC(Split PyInterpreter into its own file.) I also took the opportunity to update the documentation a little for clarity. Signedoffby: Edward Z. Yang ",2022-04-02T03:00:38Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75141,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75141**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 166c8e1b18 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
yi,[Codegen] pyi files are not included in the wheel," ðŸ› Describe the bug We generate pyi file after `setup` function. This is not useful if users doing `python setup.py install` And, for the precompiled library, the pyi file would never be packaged into the wheel/conda.  Versions main branch",2022-04-01T20:00:54Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/75131
rag,fix 'pytorch/tools/code_coverage/README.md' for renamed options,README Instructions of the coverage tool should be fixed. 1. Some CMAKE options are not consistent with 'pytorch/CmakeLists.txt'.  'CODE_COVERAGE' should be 'USE_CPP_CODE_COVERAGE'.  'CMAKE_BUILD_CONFIG' should be 'CMAKE_BUILD_TYPE'. 2. Some arguments of 'oss_coverage.py' are incorrect.  Both 'interestedonly' and 'interestedfolder' doesn't work. I guess both of them were meant to be 'interestonly',2022-04-01T07:04:30Z,open source cla signed release notes: releng topic: bug fixes,closed,0,6,https://github.com/pytorch/pytorch/issues/75091,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75091**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit c4801c23bb (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!, do you know maintains the code coverage tool?, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"`cholesky_inverse`: complex autograd, forward AD and correct tests.",As per title.,2022-03-31T19:16:31Z,module: autograd open source module: linear algebra complex_autograd cla signed release notes: autograd topic: improvements module: forward ad,closed,0,3,https://github.com/pytorch/pytorch/issues/75033,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75033**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit eeda6766fb (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5790725320?check_suite_focus=true) pull / linuxbionicrocm5.0py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220401T17:35:48.8879746Z FAIL [0.023s]: tes...id_sampler_cuda (__main__.TestTorchDeviceTypeCUDA)     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
chat,Add forward AD for torch.atan2,This PR adds a formula for the total differential of the atan2 function. Ref.  CC(Rollup: forward-mode AD operator coverage),2022-03-31T17:24:58Z,open source Merged cla signed release notes: autograd topic: improvements module: forward ad,closed,0,6,https://github.com/pytorch/pytorch/issues/75027,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75027**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8425ce373f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Sorry, I haven't run `test_autograd.py` locally to verify (didn't know it has anything forward ADrelated), I run only `test_ops_gradients.py`.","Ok, I see ðŸ˜„  https://github.com/pytorch/pytorch/blob/6905feea1acaa6a7bf673c70c5858553a02c5867/test/test_autograd.pyL392",", the test failure should go away now. ", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Code simplification for _find_process_group function,"Previously the highestlevel process group in `period_process_group_dict` could be `None`, indicating the global group. Now `period_process_group_dict` cannot contain `None` as a process group, so the function `_find_process_group` can just return a process group instead of a tuple  when not found, just return `None`, because now the returned process group cannot be `None`. Proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD))",2022-03-31T07:25:36Z,oncall: distributed triaged open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/75007,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75007**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 03447c394a (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5782746146?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (default, 2, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220401T05:51:02.2186690Z .jenkins/pytorch/t...ped) python backend.py exportmoduleto=model.pt     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","CI error is unrelated  `OMP: Error CC(Use chainerstyle constructor for Conv2d): Initializing libiomp5.so, but found unknown library already initialized.`","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Make all `.pyi.in` files exportable from torch/_C/ folder,Summary: This would eliminate the need for build system changes when new .pyi.in files is added Test Plan: CI Reviewed By: seemethere Differential Revision: D35255502,2022-03-30T17:53:08Z,fb-exported cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/74962,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74962**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b80bc112cb (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D35255502,This pull request was **exported** from Phabricator. Differential Revision: D35255502,This pull request was **exported** from Phabricator. Differential Revision: D35255502,This pull request was **exported** from Phabricator. Differential Revision: D35255502,Should the OSS bazel build also rely on this file?
rag,Combine `torch._UntypedStorage` and `torch.cuda._UntypedStorage`,"Follow up from CC(Virtualize `FloatStorage` and other `Storage` classes). Currently, `torch._UntypedStorage` and `torch.cuda._UntypedStorage` are different classes. They should be combined into one class that encapsulates the functionality of both. `torch._TypedStorage` is already set up this way. Once this is done, `getPyTypeObject()` and all the stuff related to registering different storage types in `torch/csrc/DynamicTypes.cpp` can be removed. ",2022-03-29T23:32:24Z,module: internals triaged,closed,1,0,https://github.com/pytorch/pytorch/issues/74933
agent,[torch][elastic] Make final agent barrier to shutdown properly,"Summary: When workers finish their work TE agent will start `synchronize_barrier` procedure. The barrier will wait for other agents at the end of the execution. There is a race condition may happen: The barrier uses TCPStore which is located on Rank0. When Rank0 finishes the work, other ranks may still be in a process of executing `get_all` method. This means that some of them will fail because the TCPStore will be destroyed. The fix adds additional check on Rank0 process: Rank0 process now waits for all other ranks to finish before terminating the process. Test Plan: unit tests Differential Revision: D35227180",2022-03-29T22:56:33Z,oncall: distributed open source fb-exported cla signed,closed,0,10,https://github.com/pytorch/pytorch/issues/74931,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74931**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8b0a3a6701 (more details on the Dr. CI page):  * **4/4** failures introduced in this PR   :detective: 4 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6029247124?check_suite_focus=true) pull / winvs2019cuda11.3py3 / test (default, 2, 2, windows.8xlarge.nvidia.gpu) (1/4) **Step:** ""Setup Windows"" (full log  :repeat: rerun)   20220414T18:12:30.4608770Z ERROR: Something h... isn't available for the mergebase of your branch     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D35227180,This pull request was **exported** from Phabricator. Differential Revision: D35227180,This pull request was **exported** from Phabricator. Differential Revision: D35227180, merge this (Initiating merge automatically since Phabricator Diff has merged),Merge failed due to Refusing to merge as mandatory check Lint failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2171018932,"Hi !  Thank you for your pull request.  We **require** contributors to sign our **Contributor License Agreement**, and yours needs attention. You currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",Let's retry the CLA check., force merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Dynamo+LTC: force lazy device for tensors created without specifying device,"In PR https://github.com/pytorch/pytorch/pull/72936 we already force tensors to be on lazy device if there is a device argument being specified in a call to aten methods. But it turns out that for some benchmarks (yolov3, hf_Bart), dynamo may generate Fx graphs that create eager tensors on the default device without specifying a device argument.   graph for yolov3: https://gist.github.com/shunting314/eabdf6c769c59bc384469717b8f9bb7f  graph for hf_Bart: https://gist.github.com/shunting314/8d5e2d9348a3258959d3954186c48814 Ideally lazy mode should solve the issue here, but before that, this PR just add an explicit lazy device argument for a list of tensor factory methods. This makes sure we create lazy tensors rather than eager tensors on the default device. Test plan: test thru dynamo ",2022-03-29T22:48:06Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/74930,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74930**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit f6e5902e38 (more details on the Dr. CI page):  * **9/10** failures introduced in this PR * **1/10** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs   :detective: 8 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5762223533?check_suite_focus=true) linuxdocs / builddocs (cpp) (1/8) **Step:** ""Unknown"" (full log  :repeat: rerun) :snowflake:   20220330T22:19:46.7966842Z E           hypoth... on the first call but did not on a subsequent one     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",I added a list for the tensor factory functions in `lazy_tensor_core/lazy_tensor_core/core/tensor_factory_functions.py ` by searching thru native_functions.yaml with regular expression as   pointed above.  
finetuning,Adding novel 'AdaFamily' optimizer ," ðŸš€ The feature, motivation and pitch It could be nice to add the novel 'AdaFamily' optimizer (see [1]) to the already available optimizers under 'torch.optim'. Disclaimer: I am the author of this paper, so I might be a bit biased  :) I know the evaluation in the paper is limited, but it seems to give benefits for both normalsize models (ResNet etc.) and also for compact models (MobileNet etc.). I did also an evaluation with NLP finetuning task which is not in the paper, using the code at [2], where it brings an accuracy improvement of ~ 0.5 percent compared to using 'AdamW'. Furthermore, it is actually not a single method, but a 'family' of algorithms, parametrized by parameter 'myu'. It's implementation is quite simple (there are just a few lines of code changes compared to code for Adam/AdaBelief/AdaMomentum).  The code for 'AdaFamily' optimizer can be found at [4]  If something is unclear, just email me (email: hannes.fassold(at)joanneum.at).  [1] ""AdaFamily: A family of Adamlike adaptive gradient methods"", H. Fassold, ISPR, 2022      https://arxiv.org/abs/2203.01603 [2] https://github.com/hfassold/nlp_finetuning_adafamily [3] https://github.com/",2022-03-29T10:08:43Z,module: optimizer triaged needs research function request,open,0,2,https://github.com/pytorch/pytorch/issues/74904,"Hi, Thanks for opening this issue. We do have some pretty strict rules related to adding new Optimizer/Modules: https://github.com/pytorch/pytorch/wiki/DeveloperFAQihaveanewfunctionorfeatureidliketoaddtopytorchshoulditbeinpytorchcoreoralibraryliketorchvision In particular, this work might be a bit too new to be considered to be added into core just now. And a separate library sounds like the best place for it. Don't hesitate to let us know if there is any low level feature that you're missing to implement this though!","Alright, thanks for the feedback !"
rag,fix PostLocalSGDOptimizer and ModelAverager average bug,Fixes CC(post local sgd  decreases accuracy),2022-03-29T05:07:57Z,oncall: distributed triaged open source cla signed ciflow/trunk,closed,0,38,https://github.com/pytorch/pytorch/issues/74894,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74894**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit a1a94d1ee1 (more details on the Dr. CI page):  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5984418874?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/3) **Step:** ""Test"" (full log  :repeat: rerun)   20220412T05:17:34.8364946Z ERROR: Something h... isn't available for the mergebase of your branch     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","Thanks for the fix ! I can understand that the performance can be improved by flattening all the tensors into a single buffer, and I assume this won't affect the accuracy. What's the fix for the convergence inefficiency reported in  CC(post local sgd  decreases accuracy)? Is it because previously the averaging function can only support the parameters of `types.GeneratorType`?","> Thanks for the fix ! I can understand that the performance can be improved by flattening all the tensors into a single buffer, and I assume this won't affect the accuracy. >  > What's the fix for the convergence inefficiency reported in CC(post local sgd  decreases accuracy)? `utils.average_parameters(iter(params), self.process_group)   ` iter params will get a new copy of params, so it will not execute all redcue params op. i have read your review, and will optimize code as your comment later","i am struggling with the Lint/qucikchecks now the error is following  but i can not get more detail error about this two scripts. I tried to use my local pylint, but not find any error like  trailing newlines. Is there any solution to get more detail error?",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,> iter params will get a new copy of params Can you give a simple example for the proof of concept?,"> > iter params will get a new copy of params >  > Can you give a simple example for the proof of concept? try the unit test below, the main reason is iter a torch.nn.parameter.Paramete will return a new copy so the current code shoud  from  https://github.com/pytorch/pytorch/blob/master/torch/distributed/optim/post_localSGD_optimizer.pyL82 to   self.averager.average_parameters([params]) "," i donot know how to handle the error  due to the first pr to pytorch ""The above files do not have correct trailing newlines; please normalize them"" should someone give some solutions? "," since i donot familar with github commit merge function, so i have to submmit multi cr. [optimize api of ModelAverager]  (https://github.com/pytorch/pytorch/pull/74894/commits/0a4561d6d28c1674a8a95d1fadb3544eb2bed402)    is the major optimzer cr after wayi1 review. other cr is just for code formatting.",i have fixed all checks i can fix. other failed checks seems faild beyond my code. so i think my commit is finished.   hope someone to review my code again.     can you review again if free?,"varma  Can you help review this PR? I am on travel, so probably won't be available until one week later. I will see if I can get a chance to take a quick look in the meanwhile.",new commit has been updated. include following major change: 1. PeriodicModelAverager() and HierarchicalModelAverager() support model.parameters() input and optimzer.param_groups input. 2. add unit test to validate optimzer.param_groups average parameter behaviour. please cr again if you are free   varma ,"> LGTM, thanks for working on this! >  >  It also looks like there is a merge conflict and for some reason CI didn't trigger on your latest update. Do you mind rebasing your PR and resubmitting, and then we should be able to land this change. Thank you! merge conflict has been solved.","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",awesome! I'll get this landed  Could you run some benchmarks on your end that validate the accuracy improvement given by this fix?,"> awesome! I'll get this landed >  >  Could you run some benchmarks on your end that validate the accuracy improvement given by this fix? ok, i will run one resnet50 on imagenet benchmark.   this may take a few days. when finish, i will report."," awesome, thank you! If you also get a chance and can, could you also post the benchmark code so we can help verify on our end? Thank you!  Regardless, I'll also try to run some benchmarks internally as part of landing this to get an idea of the improvement.",">  awesome, thank you! If you also get a chance and can, could you also post the benchmark code so we can help verify on our end? Thank you! >  > Regardless, I'll also try to run some benchmarks internally as part of landing this to get an idea of the improvement. sorry, due to company policy, i can not post current code.  But i will try to use local sgd based on pytorch example when i am free.    https://github.com/pytorch/examples/tree/main/imagenet and actually local sgd and Hierarchical sgd will occur some convergency problem when used in object detection  task, especially faster rcnn with more than 16 subgroups and 16 comn_period_iter.   test code is based mmdetection. this problem can leave for future. https://github.com/openmmlab/mmdetection/blob/master/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py Totally i have tried image classification and object detection tasks.","> sorry, due to company policy, i can not post current code. > But i will try to use local sgd based on pytorch example when i am free. I am particularly interested in whether the current PR can fix  CC(post local sgd  decreases accuracy). Can you confirm this? > especially faster rcnn with more than 16 subgroups and 16 comn_period_iter. I haven't tried this scale with local SGD, and 16 comn_period_iter is often too large even with less than 16 subgroups. If you only have 2 processes per subgroup, I guess you may want to create 4GPU subgroups instead of 2GPU subgroups.","> I am particularly interested in whether the current PR can fix CC(post local sgd  decreases accuracy). Can you confirm this? i will try resnet on imagenet to confirm CC(post local sgd  decreases accuracy) accuracy has been fixed in a few days. > > especially faster rcnn with more than 16 subgroups and 16 comn_period_iter. >  > I haven't tried this scale with local SGD, and 16 comn_period_iter is often too large even with less than 16 subgroups. If you only have 2 processes per subgroup, I guess you may want to create 4GPU subgroups instead of 2GPU subgroups. i suppose there is one situation 16nodes, each nodes has 1 card in low bandwidth.   So we have to create 16 subgroups. then using with Hierarchical sgd, like period_group_size_dict=""[(2,2),(4,4),(8, 8),(16, 16)]"".    comn_period_iter = 16 to make sure low bandwidth and latency has been covered. in this situation, faster rcnn with batch norm still can not converge,  loss will nan quickly. this problem may related to specific model architecture convergency.  if we use faster rcnn with group norm, model can converge although with a few accuracy decrease. if you are interesting, i will report this issue later with reproducible code. from my view, this issue reflects local sgd can not handle with any model architecture due to convergency problem.","> > I am particularly interested in whether the current PR can fix CC(post local sgd  decreases accuracy). Can you confirm this? i will try resnet on imagenet to confirm CC(post local sgd  decreases accuracy) accuracy has been fixed in a few days. > > > especially faster rcnn with more than 16 subgroups and 16 comn_period_iter. > >  > >  > > I haven't tried this scale with local SGD, and 16 comn_period_iter is often too large even with less than 16 subgroups. If you only have 2 processes per subgroup, I guess you may want to create 4GPU subgroups instead of 2GPU subgroups. i suppose there is one situation 16nodes, each nodes has 1 card in low bandwidth.   So we have to create 16 subgroups.  then using with Hierarchical sgd, like period_group_size_dict=""[(2,2),(4,4),(8, 8),(16, 16)]"". comn_period_iter = 16 to make sure low bandwidth and latency has been covered.  in this situation, faster rcnn with batch norm still can not converge,  loss will nan quickly.  this problem may related to specific model architecture convergency. if we use faster rcnn with group norm, model can converge although with a few accuracy decrease. if you are interesting, i will report this issue later with reproducible code. from my view, this issue reflects local sgd can not handle with any model architecture due to convergency problem.","> then using with Hierarchical sgd, like period_group_size_dict=""[(2,2),(4,4),(8, 8),(16, 16)]"". Interesting. Actually I haven't even tested hierarchical SGD on my own use case yet due to some blockers on my side, so I am not sure if there is any convergency bug. Could you please also try it on  CC(post local sgd  decreases accuracy) by setting `period_group_size_dict=""[(, )]""`, which will be equivalent to local SGD? I guess for your 16GPU use case, you can try a conservative config first like `period_group_size_dict=""[(1,4), (2, 8), (4, 16)]""`, so the global averaging frequency will be 4 in this case. > if we use faster rcnn with group norm, model can converge although with a few accuracy decrease. Do you mean in your use case group norm + local SGD can coverage, but batch norm + local SGD cannot converge? Or it's irrelevant to local SGD at all?",">> Could you please also try it on  CC(post local sgd  decreases accuracy) by setting period_group_size_dict=""[(, )]"", which will be equivalent to local SGD i have tested  hierarchical SGD. it can be equivalent to local sgd. >> I guess for your 16GPU use case, you can try a conservative config first like period_group_size_dict=""[(1,4), (2, 8), (4, 16)]"", so the global averaging frequency will be 4 in this case. in low bandwidth, inter node is very slow. so in 16nodes, 1gpu per node case,  we have to set (1, 1) first to decrease internode communicate frequency. on the other hand in period_group_size_dict=[(1,8), (16, 16) ] for 16 nodes ,8 gpu per node case,  faster rcnn model can converge. so i guess the problem due to specific model convergency. >> Do you mean in your use case group norm + local SGD can coverage, but batch norm + local SGD cannot converge? Or it's irrelevant to local SGD at all? yes. faster rcnn batch norm +  local SGD cannot converge may be relevant to the specific model architecture.  but it can reflect some limits to local sgd.  Because without local sgd,  faster rcnn batch norm training can converge very well. relevant paper does not discuss local sgd with object detection model, so the limit may not be found. And on the other hand  i found  faster rcnn batch norm  with gradient accumulation >=4  will also not converge, but the normal train with gradient accumulation=1 converge  very well. So i think we may use local sgd with some specific model or there should be some regularization method to help model converge with local sgd. This can be investigated in the future. Now i have to use other gradient compression method like powersgd or dgc to help object detection model converge.", merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","> > awesome! I'll get this landed > >  Could you run some benchmarks on your end that validate the accuracy improvement given by this fix? >  > ok, i will run one resnet50 on imagenet benchmark. this may take a few days. when finish, i will report. i have tested in resnet50 imagenet benchmark. common params: 1. input size 224 2. batch_size_per_card : 256 3. lr  2.048 4. 8 v100 card 5. epoch=90 post local sgd params: 1. ""comn_period_iter"": 16 2. start_localSGD_iter:  0 3. group_size=1ï¼Œ8 subgroups hierarchical sgd params: 1. ""period_group_size_dict"": [(16,8)] 2. group_size=1ï¼Œ 8 subgroups result: baseline:  val.top1 : 77.04 % val.top5 : 93.38 % post_local_sgd: val.top1 : 76.36 % val.top5 : 92.92 %  hierarchical sgd: val.top1 : 76.60 % val.top5 : 93.07 % this results show the effect of lcoal sgd and hierarchical sgd. and the drop of 0.64% top1 may be related to the high comn_period_iter and high subgroups and low start_localSGD_iter.  this can be further optimized by some method like slow momentum or tune the parms","Thanks for the update !  1. It's not very clear to me if this PR has fixed  CC(post local sgd  decreases accuracy), because you mentioned that ""i compare the PostLocalSGDOptimizer and naive PeriodicModelAverager version. PostLocalSGDOptimizer version can keep accuracy and naive PeriodicModelAverager version will drop accuray."" in  CC(post local sgd  decreases accuracy)issuecomment1067534648. Does this PR make PeriodicModelAverager version on par with PostLocalSGDOptimizer version? 2. In this threadissuecomment1067617987), you mentioned model parity is reached in the following scenario: 8cards 1subgroup(equivelent to mini batch sgd): naive PeriodicModelAverager version: accuracy drops PostLocalSGDOptimizer version: accuracy keeps, top1 75.97%(epoch50) How is the above scenario different from the one you just tested for this PR? 3. > this results show the effect of lcoal sgd and hierarchical sgd. and the drop of 0.64% top1 may be related to the high comn_period_iter and high subgroups and low start_localSGD_iter. this can be further optimized by some method like slow momentum or tune the parms You are right. Additionally, increasing subgroup size to probably 2 or 4, in combination of `post_localSGD_hook` may help a lot here, at the cost of extra communication. Note that even if you try gradient compression, you may still have a lower accuracy with the same  of steps, w/o proper tuning."," Did you get a chance to confirm 1) in the above thread? ""Does this PR make PeriodicModelAverager version on par with PostLocalSGDOptimizer version?"""
agent,Gradient w.r.t. parameters or modules," ðŸš€ The feature, motivation and pitch Often you have multiple models and multiple losses in a system. GAN for example typically has a generator and a discriminator and uses a discriminator loss and a generator loss.  In these cases, it's often the case that you want to compute gradient of a specific loss w.r.t. a specific model. Again in GAN, you want to compute discriminator loss w.r.t. the discriminator and generator loss w.r.t. the generator. Right now, this is not possible. Let's say you have generator G, discriminator D, and noise z vector z. Example from official pytorch DCGAN Tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.htmltraining  In this case you have to do a discriminator forward pass twice on the same data which is completely unnecessary. This is because there are 2 different losses targeting at 2 different modules. Tensor::backward cannot specify the modules for which gradient should be computed. If omit the `detach` and use `zero_grad` to avoid 2 passes of the discriminator, then we would compute generator grad for `discriminator_loss` for no reason. Instead, we could have the following  This is the simplest ex",2022-03-28T15:24:43Z,module: autograd triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/74832,"Hi, Thanks for the details. Isn't the `inputs` argument to `.backward()` doing exactly what you want? (doc here: https://pytorch.org/docs/stable/generated/torch.autograd.backward.html?highlight=backwardtorch.autograd.backward)","Yes thank you, I completely missed it!"
rag,[Model Averaging] Remove unused variable world_size in post_localSGD_hook.py,As title,2022-03-26T18:09:13Z,oncall: distributed open source cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74803,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74803**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit fb23ce848f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Trying to remove a Python header,  CC(Trying to remove a Python header),2022-03-25T18:03:53Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74762,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74762**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ea224cdfa5 (more details on the Dr. CI page):  * **14/14** failures introduced in this PR   :detective: 14 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5696218682?check_suite_focus=true) pull / linuxxenialpy3.7gcc7noops / build (1/14) **Step:** ""Build"" (full log  :repeat: rerun)   20220325T18:24:46.5905728Z FAILED: caffe2/tor...rc/autograd/generated/python_fft_functions.cpp.obj     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
gpt,[FSDP] How to use fsdp in GPT model in Megatron-LM," ðŸš€ The feature, motivation and pitch Are there any examples similar to DeepSpeed â€‹â€‹that can experience the fsdp function of pytorch. It would be nice to provide the GPT model in MegatronLM.  Alternatives I hope to provide examples of benchmarking DeepSpeed â€‹â€‹to facilitate the indepth use of the fsdp function.  Additional context _No response_",2022-03-25T08:30:05Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74741,"Hello! It is better to ask such kinds of questions at https://discuss.pytorch.org/ I am going to close this issue. Feel free to reopen it, if you think that you need to report a bug or make a feature proposal. Thanks."
rag,[Model Averaging] Fix post_localSGD_optimizer,"I find that the original implementation of `post_localSGD_optimizer.step()` is incorrect: Whenever `averager.average_parameters()` is called, the builtin step counter will be increased. Therefore, this should only be called exactly once per `optimizer.step()`. However, if a model has multiple param groups or params, the current implementation will call `averager.average_parameters()` multiple times and overincrease the step counter. Relevant proposals since hierarchical SGD can be supported on `post_localSGD_optimizer`:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer),  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD))",2022-03-25T06:05:33Z,oncall: distributed triaged open source cla signed,closed,0,9,https://github.com/pytorch/pytorch/issues/74737,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74737**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3aa7a56032 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"> Test failure is real >  >  Thanks  ! The deepcopy of an averager now is avoided, and this should be able to fix the error."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey , could you please rebase and address the merge conflict? Thanks!","> Hey , could you please rebase and address the merge conflict? Thanks!  should be fixed."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[PyTorch Distributed][improvement] User configured sigterm escalation time in Rdzv_conf,"Based off issue timeout)) I've come across, ""sigterm_timeout"" parameter has been added to LaunchConfig and propagated to start_processes() during an Elastic Launch. This allows users a customiseable grace period from when they recieve a sigterm to do any clean up in their own program. So now this can be defined with `rdzv_conf sigterm_timeout=300`. Unit testing is included to check the sigterm escalation timeout behaviour, but not the rdzv_conf propagation. For example, if my kubernetes pod is evicted through preemption or general killing, a pod first recieves a sigterm, which is later escalated to sigkill after a grace period. I can easly change this grace period in the pod to give enough time to send checkpoints to a remote machine, but now the elastic agent killing the process after recieving a sigterm is a limiting factor.",2022-03-25T04:18:32Z,oncall: distributed triaged open source cla signed Stale module: elastic oncall: r2p,closed,0,6,https://github.com/pytorch/pytorch/issues/74735,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74735**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ef1f1b5171 (more details on the Dr. CI page):  * **4/4** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5687015918?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/3) **Step:** ""Build python docs"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",Did you actually want me to make the proposed changes?,I would really like this merged rather than patching this all the time. What can I do to get this moving?
transformer,[PyTorch Edge] Make contexts thread local for quantized matmul,"Summary: We don't want to create and destroy a new context with each multiplication Test Plan: From fbcode:   Performance Improvement *Benchmarking done by on a model which performs matmuls of the same shapes and counts as Transformer Model, as determined in D30901505* *Notebook in which Benchmarking was performed: https://www.internalfb.com/intern/anp/view/?id=1582075&revision_id=1891629751047842* **Improvement from this diff alone** ~9.71% Reduction in Latency  Non Thread Local Contexts (before this diff, D35087184 v2): 8.5410ms  Thread Local Contexts (this diff, v12): 7.7113ms **FP32 Matmul vs Quantized Matmul, Overall Improvement from this diff stack** 56% reduction in latency compared to FP32 Matmul, 71% reduction in latency compared to Naive QMatmul  FP32 Matmul: 17.4910ms  Quantized Matmul (after this diff): 7.7113ms  Naive Quantized Matmul (dequantize â†’ fp32matmul â†’ quantize): 26.8639ms Reviewed By: kimishpatel Differential Revision: D34756288",2022-03-24T14:42:07Z,fb-exported cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/74676,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74676**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit cf49eb1f8d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34756288,This pull request was **exported** from Phabricator. Differential Revision: D34756288,This pull request was **exported** from Phabricator. Differential Revision: D34756288,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Add a unit test that launches hierarchical SGD by PostLocalSGDOptimizer,As title. The added unit test requires 4 GPUs. Please add `ciflow/all` to enable this test. Proposal:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer) Parent proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)),2022-03-24T08:45:00Z,oncall: distributed open source cla signed,closed,0,10,https://github.com/pytorch/pytorch/issues/74668,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74668**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit 4fe68da91a (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,The failure on Windows seems to be irrelevant.," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert this,Reverting PR 74668 failed due to Can't revert PR that was landed via phabricator as D35173938 Raised by https://github.com/pytorch/pytorch/actions/runs/2068592927," No need to revert PR https://github.com/pytorch/pytorch/pull/74668. Just adding `` to these tests should work. These tests require 4 processes, but even with 4 ROCm GPUs, the test environment somehow has only provided 3 processes. Curious why these failure were not detected when the PR was submitted. The PR has already labeled ""ci/master"" and ""ci/all"". cc:   varma ","Reverting PR 74668 failed due to Comment  No need to revert PR CC([Model Averaging] Add a unit test that launches hierarchical SGD by PostLocalSGDOptimizer). Just add  to these tests should work. Curious why these failure were not detected when the PR was submitted. The PR has already labeled ""ci/master"" and ""ci/all"". cc:   varma does not seem to be a valid revert command Raised by https://github.com/pytorch/pytorch/actions/runs/2075308562","  Oh hmm. I was looking at https://hud.pytorch.org/pytorch/pytorch/pull/74668?sha=3491f4c36f63b174a768354af4f1edb8f66f4d38 which had some failures (my mistake). I don't think it got reverted anyways.  As for why they didn't run, I think it did run, since it had trunk workflows."
transformer,Support channel first(or any dim) LayerNorm," ðŸš€ The feature, motivation and pitch LayerNorm is widely used in Transformer and recently some CNN model(or Transformer + CNN model)  is trying to use this. But CNN has [B, C, H, W] output, that can not use LayerNorm which only support norm last N channel.  Alternatives Support specific dim of mean, not last N dim.  Additional context _No response_",2022-03-24T03:34:30Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/74661,"related:  CC([docs] Improve documentation for LayerNorm, GroupNorm, etc (+ add python reference impl)),  CC([proposal] Parameter dim for F.linear (and maybe nn.Linear)), ",duplicate of  CC(torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application)?,> duplicate of  CC(torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application)? yes,Closing as a duplicate
yi,Allow torch/csrc/deploy/interpreter/Optional.hpp to be allowed into the wheel distribution,"  CC(Allow torch/csrc/deploy/interpreter/Optional.hpp to be allowed into the wheel distribution) Previously `torch/csrc/deploy/interpreter/Optional.hpp` wasn't getting included in the wheel distribution created by `USE_DEPLOY=1 python setup.py bdist_wheel`, this pr fixes that Differential Revision: D35094459",2022-03-23T21:44:15Z,cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74643,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74643**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit 4a1496825c (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/2** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs * **1/2** broken upstream at merge base c5c2d5a9b8 on Mar 23 from  2:42pm to  4:31pm   :snowflake: 1 failure **tentatively classified as flaky** but reruns have not yet been triggered to confirm:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5668227379?check_suite_focus=true) pull / linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun) :snowflake:   20220323T22:58:13.1470182Z [1m[31mE        ...the first call but did not on a subsequent one[0m      :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * [pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) on Mar 23 from  2:42pm to  4:31pm (f7ee308dfb  d583f9c9d2)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Skip specifying rcond for gelsy driver in tests,Fixes  CC(DISABLED test_linalg_lstsq_cpu_float64 (__main__.TestLinalgCPU)),2022-03-23T21:00:59Z,module: flaky-tests open source module: linear algebra cla signed topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/74638,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74638**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b3dd740f53 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"For reference (we can discuss this in the next linalg meting), perhaps a more solid way of checking that the solution of lstsq is correct is via the first order optimality conditions of the problem, i.e. KKT.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add & use in-place gelu,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) It seems silly that we don't have this, so add it. Differential Revision: D35087899",2022-03-23T19:59:46Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74629,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74629**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 68b2cfc0fb (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
transformer,"Training, Forward / backward pass with _different_ batch-size, no speedup observed when backward pass has smaller batch-size"," ðŸš€ The feature, motivation and pitch Currently, it seems that in training the Pytorch autograd engine is 'optimized' only for the case where the forward and backward pass are done with tensors with identical batchsize (= last dimension of tensor). If the backward propagation operates on a tensor corresponding to a _smaller_ batch size (e.g. because some samples have been discarded from the batch after the forward pass according to some criterion  e.g. loss value), no speedup is observed, although it _should_ be faster theoretically (because of the smaller batchsize in the backward pass). This is also mentioned in the NVIDIA GTC 2022 talk 'Faster Neural Network Training, Algorithmically' [1], minute 20:0021:00 So, although technically it is possible to have a smaller batch size for backward step than in the forward size, currently in Pytorch unfortunately it does not bring a speedup.  It would be great if pytorch autograd engine could be extended, so that using a smaller batch size in the backward pass leads to a speedup in training. There are several curriculumlearninglike methods which could be utilized then successfully in order to speedup the tra",2022-03-23T08:36:15Z,feature module: autograd triaged,open,0,5,https://github.com/pytorch/pytorch/issues/74604,I do not see apriori why wouldn't you see a speedup in this case. Can you provide a small selfcontained script that tests this?,"Apparently, it is about propagating in forward and then in backward through full batches in the computational graph even if some subsets are not used and could be pruned... I wonder if Masked Tensors could provide any benefits in such a case, CC  .",maskedtensor could indeed be used if we provide sufficient operator coverage and specialized kernels that take advantage of the absence. Thanks for pointing this out !  do you have a specific model in mind you'd want to see support for first?,"Good question :) Actually, both methods (papers [1] and [2], see my original post) are designed to be quite general, could be applied to _any_ model. So a 'general' solution would be great, but from your comments I sense that this is not doable. As mentioned in my original post, an actual _speedup_ with these methods (note the method [1] provides additionally also a better generalization capability) can be only expected  for models _without_ batchnormalization layers (see [4]) Most CNNs are using batchnormalization layers. I think these could be replaced, without too much harm, by a combination of group normalization and weight standardization, but on the other hand that makes problem when you want to use a pretrained CNN model. In contrast, transformer architectures used in NLP domain usually do not use batchnormalization, they use layer normalization instead. So I think it makes sense to go for transformer architectures instead in NLP domain. E.g the 'DistillBERT' model is widely used. For this model, I have also a selfcontained test code sample, which is not too complicated, at [6]  . So I think it could make sense to use the **DistilLBERT** model, using my code at [6]. Contact my if you need help (email: hannes.fassold(at)joanneum.at). [5] https://stackoverflow.com/questions/68920059/pytorchnospeedupwhendoingbackwardpassonlyforapartofthesamplesinm  [6] https://github.com/hfassold/nlp_finetuning_adafamily","> I do not see apriori why wouldn't you see a speedup in this case. I think a simple example that shows this is:  And it is indeed expected and not something that we can easily change. The autograd works at the Tensor level (the whole Tensor) and so we won't be able to see if only a subset of it is being used. Masked Tensor does sound like the most promising approach here to be able to skip unnecessary operations. But as Christian pointed out, this will potentially require a large number if custom kernels to get to the performance that you expect."
rag,Thoroughly document `index_put_` and add more test coverage,"While working on CC(index_put : INTERNAL ASSERT FAILED), I found it very difficult to figure out what the behavior of `index_put_` is supposed to be. The documentation didn't really explain it. I had to read the implementation code and try running it with various different arguments to figure out how the `indices` and `values` arguments work. In my opinion, the behavior is a bit complicated, so I figured that the documentation should explain it thoroughly with some good examples. I also added a unit test that covers the broadcasting and expansion cases that I mention in the documentation, to verify that my understanding is correct and because I didn't see existing tests for all of these cases.",2022-03-22T19:33:13Z,module: docs triaged module: advanced indexing open source cla signed Stale,closed,0,5,https://github.com/pytorch/pytorch/issues/74573,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74573**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ab62d33fcd (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5651058834?check_suite_focus=true) pull / winvs2019cuda11.3py3 / build (1/2) **Step:** ""Build"" (full log  :repeat: rerun)   20220322T21:11:50.9138304Z C:\\actionsrunner\\...ult': no appropriate default constructor available     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",I suspect my test is failing in CI because some of the cases can have duplicate indices. I'll fix that.,Related:  CC([docs] Unclear description of indices arg in torch.index_put_),"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,[Model Averaging] Make HierarchicalModelAverager a subclass of averagers.ModelAverager,Make `HierarchicalModelAverager` a subclass of `averagers.ModelAverager` is a preparation step for incorporating hierarchical SGD into `PostLocalSGDOptimizer`. Proposal:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer) Parent proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)),2022-03-22T18:13:51Z,oncall: distributed open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/74564,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74564**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b34c58af61 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[Dynamic RPC] Add graceful shutdown for dynamic RPC members,Stack from ghstack:  CC([RPC small change] Improve logging from 'unknown destination worker') [RPC small change] Improve logging from 'unknown destination worker' * * CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) [Dynamic RPC] Add graceful shutdown for dynamic RPC members**  CC([Dynamic RPC] Allow existing ranks to communicate with newly joined ranks) [Dynamic RPC] Allow existing ranks to communicate with newly joined ranks Changes:  Refactor token implementation that was used in initialization to be a context manager that is also used in shutdown  Graceful shutdown when the local active calls on tensorpipe agent is 0  Expose rpc agent store instance as a readonly property.,2022-03-22T17:47:51Z,oncall: distributed cla signed release notes: distributed (rpc) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/74561,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74561**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit c711103367 (more details on the Dr. CI page): Expand to see more  * **5/5** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6162032014?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/2) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",CI failures are unrelated, merge this,This introduced a serious bug:  CC(Use after free in TensorPipeAgent)
yi,Specifying left-right-padding as tuple for asymmetric padding," ðŸš€ The feature, motivation and pitch Hello,  would it be reasonable to allow padding along a single dimension to be specified as a tuple (left padding, right padding) for asymmetric padding?  It should be clear, but as an example, if I would like to pad this input sequence  like this   I would specify padding = ((2,1))  In 2D:    would result in padding = ((2,1),(1,2)).  Does it make sense, or would you recommend to go with F.pad for asymmetric padding?  Thanks! Best, JZ   Alternatives _No response_  Additional context _No response_ ",2022-03-22T17:06:28Z,feature module: nn triaged module: padding,open,0,1,https://github.com/pytorch/pytorch/issues/74554,"Hey , thanks for the request! Which modules or functions in particular are you referring to (convolution, others)? `F.pad` is generally the way to go for maximum flexibility, but please let us know if it's too slow for your use case."
rag,ComplexHalf Coverage Tracker," ðŸš€ The feature, motivation and pitch There have been multiple requests and issues (see  CC(ComplexHalf support)) to have better operator support for `complex32`. Currently it is almost impossible to do anything with a `Tensor` of `complex32` dtype (even printing doesn't work). All operators which support complex dtypes should ideally support `complex32`. Computation will occur in `complex64` similar to `Half` wherein computation occurs in `float`. **NOTE** : Math op support will mostly be enabled only on CUDA Following is the operator coverage: Fundamental Ops (expected 1.12) * [x] copy_ (to support casting to and from `complex32`) (PR: https://github.com/pytorch/pytorch/pull/73847) * [x] print support (Ref:  CC(Cannot print 32-bit complex tensors)) * [ ] storage support  CC(`storage` does support `complex32` tensor) * [x] type promotion **NOTE**: List will be updated if supporting above list requires supporting another operator  HighPriority Ops (based on user demand) * [x] Support for fft module (expected 1.12)     * [x] fft.fft     * [x] fft.fft2     * [x] fft.fftn     * [x] fft.hfft     * [x] fft.hfft2     * [x] fft.hfftn     * [x] fft.rfft     ",2022-03-22T13:35:08Z,feature triaged module: complex module: half,open,3,4,https://github.com/pytorch/pytorch/issues/74537,Probably prime candidate for being JITerated by default.,Is this still updated?,"Unfortunately, I don't think so.","So, can we close it?"
,RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .," ðŸ› Describe the bug I am fine tuning masked language model from XLM Roberta large on google machine specs. I made couple of experiments and was strange to see few results. `I think Pytorch is not functioning properly. ` I am using pretrained Hugging face model.  I am using  https://huggingface.co/xlmrobertalarge I am not using fairseq or anything.  Here is `NvidiaSMI`  **Training Code**  Also `gradient_checkpointing` never works. Strange.  **Also, is it using all 8 GPU's?**  Versions Versions torch==1.11.0+cu113  torchvision==0.12.0+cu113   torchaudio==0.11.0+cu113  transformers==4.17.0",2022-03-22T05:26:15Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74522,"I think it's possible that you are not using the config/setup properly when training the model given that even batch_size=4 caused OOM, could you submit issue directly to the HuggingFace repo https://github.com/huggingface/transformers? We don't have too much context on model specific issue tbh. They might have a better idea what's wrong with your issue. There's possibility that there's fragmented memory that is not being released properly, although I doubted about it as it seems you might not using all GPUs. But if you think there's fragmented memory, one suggestion is to use `torch.cuda.empty_cache()` to release some memory, you can refer to this post on some previous discussions https://discuss.pytorch.org/t/abouttorchcudaemptycache/34232/20"
transformer,[PyTorch] _addm_activation native function for matmul/bias/activation fusion,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) Here's an extended version of addmm that takes advantage of cublasLt's fused addmm + relu/gelu support. Differential Revision: D35019612",2022-03-21T19:45:29Z,cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/74490,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f2448c136c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,,"> Do we want to expose them in native_functions.yaml if they are private and intended to be called only from some other publicly exposed functions? Don't we need to do that for testing? Also, do we want to keep this private? Fused matmulbias{r,g}elu  seems like something people might want (but what I do I know about AI research).","Yeah, we need it to be able to test from python, and yes, fused matmulbiasgelu/relu is a useful op, but requirements are higher for publicly available op, such as you are expected to provide a gradient computation for it, and documentation. We still can have them in native_functions as private ops starting with `_`, we have a few of those already. ","hey  this is looking green, can I have a review?",". ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it?","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I'll see what I can do.","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively).","> > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? >  > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning.","> > > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? > >  > >  > > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). >  > Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning. OK I've verified that `test_addmm_repro_possible_cublas_gelu_bug_cuda_float32` (and bfloat16, and float64) will fail as advertised if and only if the following patch is applied:  That said, I agree it is suspicious that it doesn't fail when you try via cublas and it does fail when I try via my new codepath, so I am going to look through this diff again to make sure I haven't done something silly like doublegelu.","> I am going to look through this diff again to make sure I haven't done something silly like doublegelu. Nope, didn't find anything silly. We know relu is working, and we're functioning correctly for gelu and set up to do the fusion when we figure out correctness, so I'm going to go ahead and land this.","https://gist.github.com/swolchok/f550a9dfbab879cc514912d5f5ab59af has the version of test_linalg.py with the repro test for gelu epilogue not working properly, in case old versions of this PR aren't accessible after I update it.",Thanks for the repro; I've isolated it to cublaslt and have passed the repro on.
rag,[Model Averaging] Make HierarchicalModelAverager a subclass of averagers.ModelAverager,Make `HierarchicalModelAverager` a subclass of `averagers.ModelAverager` is a preparation step for incorporating hierarchical SGD into `PostLocalSGDOptimizer`. Proposal:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer) Parent proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)),2022-03-22T18:13:51Z,oncall: distributed open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/74564,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74564**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b34c58af61 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[Dynamic RPC] Add graceful shutdown for dynamic RPC members,Stack from ghstack:  CC([RPC small change] Improve logging from 'unknown destination worker') [RPC small change] Improve logging from 'unknown destination worker' * * CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) [Dynamic RPC] Add graceful shutdown for dynamic RPC members**  CC([Dynamic RPC] Allow existing ranks to communicate with newly joined ranks) [Dynamic RPC] Allow existing ranks to communicate with newly joined ranks Changes:  Refactor token implementation that was used in initialization to be a context manager that is also used in shutdown  Graceful shutdown when the local active calls on tensorpipe agent is 0  Expose rpc agent store instance as a readonly property.,2022-03-22T17:47:51Z,oncall: distributed cla signed release notes: distributed (rpc) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/74561,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74561**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit c711103367 (more details on the Dr. CI page): Expand to see more  * **5/5** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6162032014?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/2) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",CI failures are unrelated, merge this,This introduced a serious bug:  CC(Use after free in TensorPipeAgent)
yi,Specifying left-right-padding as tuple for asymmetric padding," ðŸš€ The feature, motivation and pitch Hello,  would it be reasonable to allow padding along a single dimension to be specified as a tuple (left padding, right padding) for asymmetric padding?  It should be clear, but as an example, if I would like to pad this input sequence  like this   I would specify padding = ((2,1))  In 2D:    would result in padding = ((2,1),(1,2)).  Does it make sense, or would you recommend to go with F.pad for asymmetric padding?  Thanks! Best, JZ   Alternatives _No response_  Additional context _No response_ ",2022-03-22T17:06:28Z,feature module: nn triaged module: padding,open,0,1,https://github.com/pytorch/pytorch/issues/74554,"Hey , thanks for the request! Which modules or functions in particular are you referring to (convolution, others)? `F.pad` is generally the way to go for maximum flexibility, but please let us know if it's too slow for your use case."
rag,ComplexHalf Coverage Tracker," ðŸš€ The feature, motivation and pitch There have been multiple requests and issues (see  CC(ComplexHalf support)) to have better operator support for `complex32`. Currently it is almost impossible to do anything with a `Tensor` of `complex32` dtype (even printing doesn't work). All operators which support complex dtypes should ideally support `complex32`. Computation will occur in `complex64` similar to `Half` wherein computation occurs in `float`. **NOTE** : Math op support will mostly be enabled only on CUDA Following is the operator coverage: Fundamental Ops (expected 1.12) * [x] copy_ (to support casting to and from `complex32`) (PR: https://github.com/pytorch/pytorch/pull/73847) * [x] print support (Ref:  CC(Cannot print 32-bit complex tensors)) * [ ] storage support  CC(`storage` does support `complex32` tensor) * [x] type promotion **NOTE**: List will be updated if supporting above list requires supporting another operator  HighPriority Ops (based on user demand) * [x] Support for fft module (expected 1.12)     * [x] fft.fft     * [x] fft.fft2     * [x] fft.fftn     * [x] fft.hfft     * [x] fft.hfft2     * [x] fft.hfftn     * [x] fft.rfft     ",2022-03-22T13:35:08Z,feature triaged module: complex module: half,open,3,4,https://github.com/pytorch/pytorch/issues/74537,Probably prime candidate for being JITerated by default.,Is this still updated?,"Unfortunately, I don't think so.","So, can we close it?"
,RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .," ðŸ› Describe the bug I am fine tuning masked language model from XLM Roberta large on google machine specs. I made couple of experiments and was strange to see few results. `I think Pytorch is not functioning properly. ` I am using pretrained Hugging face model.  I am using  https://huggingface.co/xlmrobertalarge I am not using fairseq or anything.  Here is `NvidiaSMI`  **Training Code**  Also `gradient_checkpointing` never works. Strange.  **Also, is it using all 8 GPU's?**  Versions Versions torch==1.11.0+cu113  torchvision==0.12.0+cu113   torchaudio==0.11.0+cu113  transformers==4.17.0",2022-03-22T05:26:15Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74522,"I think it's possible that you are not using the config/setup properly when training the model given that even batch_size=4 caused OOM, could you submit issue directly to the HuggingFace repo https://github.com/huggingface/transformers? We don't have too much context on model specific issue tbh. They might have a better idea what's wrong with your issue. There's possibility that there's fragmented memory that is not being released properly, although I doubted about it as it seems you might not using all GPUs. But if you think there's fragmented memory, one suggestion is to use `torch.cuda.empty_cache()` to release some memory, you can refer to this post on some previous discussions https://discuss.pytorch.org/t/abouttorchcudaemptycache/34232/20"
transformer,[PyTorch] _addm_activation native function for matmul/bias/activation fusion,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) Here's an extended version of addmm that takes advantage of cublasLt's fused addmm + relu/gelu support. Differential Revision: D35019612",2022-03-21T19:45:29Z,cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/74490,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f2448c136c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,,"> Do we want to expose them in native_functions.yaml if they are private and intended to be called only from some other publicly exposed functions? Don't we need to do that for testing? Also, do we want to keep this private? Fused matmulbias{r,g}elu  seems like something people might want (but what I do I know about AI research).","Yeah, we need it to be able to test from python, and yes, fused matmulbiasgelu/relu is a useful op, but requirements are higher for publicly available op, such as you are expected to provide a gradient computation for it, and documentation. We still can have them in native_functions as private ops starting with `_`, we have a few of those already. ","hey  this is looking green, can I have a review?",". ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it?","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I'll see what I can do.","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively).","> > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? >  > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning.","> > > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? > >  > >  > > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). >  > Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning. OK I've verified that `test_addmm_repro_possible_cublas_gelu_bug_cuda_float32` (and bfloat16, and float64) will fail as advertised if and only if the following patch is applied:  That said, I agree it is suspicious that it doesn't fail when you try via cublas and it does fail when I try via my new codepath, so I am going to look through this diff again to make sure I haven't done something silly like doublegelu.","> I am going to look through this diff again to make sure I haven't done something silly like doublegelu. Nope, didn't find anything silly. We know relu is working, and we're functioning correctly for gelu and set up to do the fusion when we figure out correctness, so I'm going to go ahead and land this.","https://gist.github.com/swolchok/f550a9dfbab879cc514912d5f5ab59af has the version of test_linalg.py with the repro test for gelu epilogue not working properly, in case old versions of this PR aren't accessible after I update it.",Thanks for the repro; I've isolated it to cublaslt and have passed the repro on.
,RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .," ðŸ› Describe the bug I am fine tuning masked language model from XLM Roberta large on google machine specs. I made couple of experiments and was strange to see few results. `I think Pytorch is not functioning properly. ` I am using pretrained Hugging face model.  I am using  https://huggingface.co/xlmrobertalarge I am not using fairseq or anything.  Here is `NvidiaSMI`  **Training Code**  Also `gradient_checkpointing` never works. Strange.  **Also, is it using all 8 GPU's?**  Versions Versions torch==1.11.0+cu113  torchvision==0.12.0+cu113   torchaudio==0.11.0+cu113  transformers==4.17.0",2022-03-22T05:26:15Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74522,"I think it's possible that you are not using the config/setup properly when training the model given that even batch_size=4 caused OOM, could you submit issue directly to the HuggingFace repo https://github.com/huggingface/transformers? We don't have too much context on model specific issue tbh. They might have a better idea what's wrong with your issue. There's possibility that there's fragmented memory that is not being released properly, although I doubted about it as it seems you might not using all GPUs. But if you think there's fragmented memory, one suggestion is to use `torch.cuda.empty_cache()` to release some memory, you can refer to this post on some previous discussions https://discuss.pytorch.org/t/abouttorchcudaemptycache/34232/20"
transformer,[PyTorch] _addm_activation native function for matmul/bias/activation fusion,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) Here's an extended version of addmm that takes advantage of cublasLt's fused addmm + relu/gelu support. Differential Revision: D35019612",2022-03-21T19:45:29Z,cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/74490,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f2448c136c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,,"> Do we want to expose them in native_functions.yaml if they are private and intended to be called only from some other publicly exposed functions? Don't we need to do that for testing? Also, do we want to keep this private? Fused matmulbias{r,g}elu  seems like something people might want (but what I do I know about AI research).","Yeah, we need it to be able to test from python, and yes, fused matmulbiasgelu/relu is a useful op, but requirements are higher for publicly available op, such as you are expected to provide a gradient computation for it, and documentation. We still can have them in native_functions as private ops starting with `_`, we have a few of those already. ","hey  this is looking green, can I have a review?",". ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it?","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I'll see what I can do.","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively).","> > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? >  > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning.","> > > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? > >  > >  > > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). >  > Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning. OK I've verified that `test_addmm_repro_possible_cublas_gelu_bug_cuda_float32` (and bfloat16, and float64) will fail as advertised if and only if the following patch is applied:  That said, I agree it is suspicious that it doesn't fail when you try via cublas and it does fail when I try via my new codepath, so I am going to look through this diff again to make sure I haven't done something silly like doublegelu.","> I am going to look through this diff again to make sure I haven't done something silly like doublegelu. Nope, didn't find anything silly. We know relu is working, and we're functioning correctly for gelu and set up to do the fusion when we figure out correctness, so I'm going to go ahead and land this.","https://gist.github.com/swolchok/f550a9dfbab879cc514912d5f5ab59af has the version of test_linalg.py with the repro test for gelu epilogue not working properly, in case old versions of this PR aren't accessible after I update it.",Thanks for the repro; I've isolated it to cublaslt and have passed the repro on.
transformer,mask parameter in Transformer support 3D tensor but not mentioned," ðŸ“š Documentation (Add a clear and concise description of what the documentation content issue is. A link to any relevant https://pytorch.org page is helpful if you have one.) First, I read doc in this https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html page, which redirect me to the doc on this https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html page. The notes about ""src_mask"" parameter under ""forwardshape"" section says it only support shape of (S, S). However when I track down the source code I found ""src_mask"" is actually the ""attn_mask"" on this https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html page which support 3D tensor. Then I tested feeding 3D mask to TransformerEncoderLayer and it works. Please check. ",2022-03-20T01:27:11Z,module: docs actionable,closed,0,1,https://github.com/pytorch/pytorch/issues/74612,"Thanks for the report, ! We'd accept a PR expanding the supported shapes listed in the `nn.Transformer` documentation: * `src_mask`  supports shape `(S, S)` or `(N * num_heads, S, S)` * `tgt_mask`  supports shape `(T, T)` or `(N * num_heads, T, T)` "
yi,Copying tensors sometimes doesn't copy last N elements when using cuda graphs," ðŸ› Describe the bug Copying tensors sometimes doesn't work when using cuda graphs. Occasionally the last N elements aren't copied. See repro: https://gist.github.com/tomconerlyanth/4970244782fd8e02dd99ea8ca8797eec  Versions tc0 (1m) ~/code$ python collect_env.py Collecting environment information... PyTorch version: 1.10.0a0+gitee11606 Is debug build: False CUDA used to build PyTorch: 11.4 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.3 Libc version: glibc2.27 Python version: 3.9.7 (default, Sep 16 2021, 13:09:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.15683.273.amzn2.x86_64x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.4.152 GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB GPU 4: NVIDIA A100SXM440GB GPU 5: NVIDIA A100SXM440GB GPU 6: NVIDIA A100SXM440GB GPU 7: NVIDIA A100SXM440GB Nvidia driver version: 470.57.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.4 /",2022-03-18T18:26:17Z,high priority module: dependency bug triaged module: correctness (silent) module: cuda graphs,closed,0,2,https://github.com/pytorch/pytorch/issues/74419,"Reproes with a pure c++ outside of pytorch, thanks  for the reproduction https://gist.github.com/tomconerlyanth/c6e3bc8caddaa1bb628ce98593a26484","Thanks for the report  and  for the support in isolating the issue. It should have been fixed by our CUDA team already so I'll close the issue. Please reopen, if you are still running into it. "
rag,"UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead."," ðŸ› Describe the bug  **UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.** When I don't pay attention to this warning, it will cause loss to be `nan`! I think this is where improvements are needed, including the official API DOC.  Versions  ",2022-03-18T08:19:12Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/74411,"Hi, Thanks for your feedback. Could you give some more details on what you're trying to do and when do you get this problem? Some combinations of arguments can give you nan indeed but that's not related to you using these args or not."
rag,[ci] Pull some stragglers into the consolidated workflows,"Stack from ghstack:  CC([ci] Pull some stragglers into the consolidated workflows)  CC(Workflow consolidation for GitHub actions)  CC([ci] inline display_ec2 script) Migrate some of the oneoff workflows into the main workflows, according to how they want to be dispatched",2022-03-16T23:27:23Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74351,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74351**  * :x: Python docsfailed to build  * :x: C++ docsfailed to build  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 5f5c07691a (more details on the Dr. CI page):  * **9/9** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5873804100?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/7) **Step:** ""Build python docs"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
rag,[ci] Pull some stragglers into the consolidated workflows,"Stack from ghstack:  CC([ci] Pull some stragglers into the consolidated workflows)  CC(Workflow consolidation for GitHub actions)  CC([ci] inline display_ec2 script) Migrate some of the oneoff workflows into the main workflows, according to how they want to be dispatched",2022-03-16T23:27:23Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74351,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74351**  * :x: Python docsfailed to build  * :x: C++ docsfailed to build  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 5f5c07691a (more details on the Dr. CI page):  * **9/9** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5873804100?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/7) **Step:** ""Build python docs"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
,Training MLM model XLM Roberta large on google machine specs not fast, ðŸ› Describe the bug I am fine tuning masked language model from `XLM Roberta large` on google machine specs.  I made couple of experiments and was strange to see few results.   I was not able to train model with `batch size ` more than 4 on  of GPU's. It stopped in midway. Here is the code I am using.  **My Questions** How can I train with larger batch size on `a2highgpu4g` machine? Which parameters can I include in `TrainingArguments` so that training is fast and occupies small memory? Thanks in advance.  Versions  ,2022-03-16T15:18:26Z,oncall: distributed triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/74302," can you share more details about what implementation you are using? is it the fairseq implementation or huggingface implementation or sth else? did you download the pretained model from torchhub? For different implementation of the models like fairseq or huggingface, we recommend contacting or submitting issues to their repo directly to get a faster and more accurate response :)",Any help on this? More info here  CC(RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .),">  can you share more details about what implementation you are using? is it the fairseq implementation or huggingface implementation or sth else? did you download the pretained model from torchhub? >  > For different implementation of the models like fairseq or huggingface, we recommend contacting or submitting issues to their repo directly to get a faster and more accurate response :) More detailed description here  CC(RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .)"
transformer,Loading of packaged model object fails with torch==1.11.0, ðŸ› Describe the bug The new release of PyTorch 1.11.0 seems to break the loading of packaged objects.  The code example above works with `torch==1.10.0` but fails with `torch==1.11.0`.  Versions ,2022-03-16T10:37:57Z,oncall: package/deploy imported,closed,0,1,https://github.com/pytorch/pytorch/issues/74290,Seems like this PR solves the issue. https://github.com/pytorch/pytorch/pull/80917
yi,remove _lazy_init() in rebuild full params,"  CC(remove _lazy_init() in rebuild full params)  CC(make sharding strategy configurable and support zero2 algorithm) remove _lazy_init() in rebuild full params, which was introduced in https://github.com/pytorch/pytorch/pull/73300. _lazy_init() synces streams whenever before rebuilding full params. This sync is not needed when BackwardPrefetch.BACKWARD_PRE is enabled. Differential Revision: D34907823",2022-03-15T22:00:38Z,oncall: distributed cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74263,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74263**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit b7371bec9b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"> Stamp to unblock, but don't quite understand the PR comment. >  > It says that this sync is not needed when BackwardPrefetch.PRE is enabled, but by default this is not enabled. So would we need the sync in that case? _lazy_init() will sync streams, since we've synced streams outside _rebuild_full_params() as needed, no need to call _lazy_init() to sync streams inside _rebuild_full_params()","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Create secure credential storage for metrics credentials and associated documentation on how to regenerate them if needed,/pytorchdevinfra,2022-03-15T20:21:20Z,module: ci triaged,open,0,1,https://github.com/pytorch/pytorch/issues/74256,I've added a section to our internal wiki
transformer,Hugging face code with Pytorch throwing error when running using vertex-ai," ðŸ› Describe the bug I am training a NLP Hugging face model in vertexai with custom image.  The same code works in local machine. Here is my code and the error.  The error that I get is:     None	INFO	train data created     None	INFO	training to start     None	ERROR	0% 0/8 [00:09<?, ?it/s] Most of them are warning but still my code stops with error.  Versions Using FROM usdocker.pkg.dev/vertexai/training/tfgpu.28:latest as base image  ",2022-03-15T10:31:00Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74238,"I don't know if this is a PyTorch problem, this issue might be better off in the huggingface transformers repo."
transformer,[WIP] fix: no duplicate final LayerNorm in Transformer, Description This pull request solves CC(Successive Layer Normalization in nn.Transformer) and CC(Two consecutive nn.LayerNorm are used in transformer model when norm_first is False) by implementing the third solution (the minimal and backwardcompatible one) among the ones listed hereissuecomment1066609080).,2022-03-15T09:42:00Z,open source cla signed Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/74237,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74237**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 282617b95e (more details on the Dr. CI page):  * **8/8** failures introduced in this PR   :detective: 8 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5557945955?check_suite_focus=true) linuxxenialcuda11.3py3.7gcc7 / test (default, 2, 2, linux.4xlarge.nvidia.gpu) (1/8) **Step:** ""Test"" (full log  :repeat: rerun)   20220315T18:36:13.5026477Z [  FAILED  ] TransformerTest.Transformer     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","Sure, , thanks for the feedback! You are right. I would be glad to propose another solution if it can somehow help.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,fix: no duplicate final LayerNorm in Transformer, Description This pull request solves CC(Successive Layer Normalization in nn.Transformer) and CC(Two consecutive nn.LayerNorm are used in transformer model when norm_first is False) by implementing the third solution (the minimal and backwardcompatible one) among the ones listed hereissuecomment1066609080).,2022-03-15T08:48:17Z,open source cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/74233,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74233**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 788bd42713 (more details on the Dr. CI page):  * **30/30** failures introduced in this PR   :detective: 30 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5550986772?check_suite_focus=true) winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/30) **Step:** ""Upload test statistics"" (full log  :repeat: rerun)   20220315T09:28:17.6386265Z RuntimeError: test_cpp_api_parity failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",Closed as the pull request branch was created from an outdated fork. CC([WIP] fix: no duplicate final LayerNorm in Transformer) replaces this pull request.
transformer,Preserve codegen on fx graph in transformer,Summary: Use the codegen on the original graph module for the new graph module produced by transformer. Test Plan: Added a unit test: test_custom_codegen_with_transformer Differential Revision: D34867938,2022-03-14T20:23:15Z,fb-exported cla signed module: fx,closed,0,3,https://github.com/pytorch/pytorch/issues/74189,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74189**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit f05360766d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34867938,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Tracing model parameter shapes without instantiating the model parameters," ðŸš€ The feature, motivation and pitch  Request We would like a way to get a model's parameter shapes without instantiating them. The following is an example call signature.   Motivation Hi, I'm a maintainer for the mup package. This repo allows one to implement in their models a special parametrization called maximal update parametrization, or muP, that has the special property that narrow and wide networks share the same optimal hyperparameters (like learning rate, initialization, etc). This is demonstrated below on a Transformer trained with adam, where on the left we have the pytorch default parametrization and the right we have muP. !image This property in particular can be used to tune hyperparameters for extremely large neural networks like GPT3 that is too expensive to train more than once, by just tuning a tiny version of it.  Current Implementation To implement muP in a model, we need to annotate the parameter shapes with two pieces of information. For each dimension in a shape, these are 1) is it a width dimension (which will be scaled up or down)? 2) what is the ""base value"" of this dimension?  our implementation is designed to replicate t",2022-03-13T04:49:16Z,feature module: nn triaged,open,9,12,https://github.com/pytorch/pytorch/issues/74143,"Hey , meta tensors may help solve your problem. They are a special type of tensor that maintain shape information but do not allocate storage for the data. Example:  Note that all `torch.nn` modules have a `device` constructor kwarg that defines their parameters as meta tensors:  See here for more info. The obvious caveat is that any userdefined modules should also define a way to place their parameters on `device='meta'` to work like this.","Hi , we tested `device='meta'` with our package and it seems to work fine as is, which is great news. We have added a blurb in our README about this. However, looking around at some popular github repos, I'm not sure how prevalent is it for models to pass down `device` flags to its submodules. For example, just looking at Huggingface's BERT, this doesn't seem to be the case. So, before this `device` convention is universally adopted by the community, I'm still wondering whether it makes sense to have an automatic way of inferring parameter shapes without needing to change the code of a module.",? Also , the new Deferred Module Initialization feature that we are about to release in an outoftree package will exactly address your particular problem. I would be happy to give you access to our private repo if you want to try it out before we publicly release it.," Hi! I'm the other maintainer of the mup package. Thanks for your response, and we are happy to try this new feature! Please let us know how we can access the private repo.","No problem! I am on PTO today and have limited internet access. Expect to receive an invitation email to the â€œtorchdistxâ€ repo tomorrow morning. If you are on PyTorchâ€™s Slack channel, happy to chat there as well.","  I just sent you an invite to the torchdistx repo. Please reach out to me either over PyTorch's Slack channel or over balioglu at fb.com to chat about the feature. Since the docs are not ""rendered"" yet, you might need some help to locate them.", Thanks! I just asked to be added to torch slack since I don't use FB. Will reach out to you over there once I get in.,  Happy to share that torchdistx is now public: https://github.com/pytorch/torchdistx Check out the docs for Deferred Module Initialization and Fake Tensor which I believe will address your problem. Please let me know if you have any feedback!,Awesome!   Trying this out right now., deferred_init works like a charm for the most part! Though we just encountered an issue with a model that has an unsqueeze call in its forward pass. `*** NotImplementedError: `aten::unsqueeze` cannot be run with fake tensor(s) because the meta backend has no kernel for it. Please file an issue if you want it to be supported.` I assume this is expected since the kernel isn't implemented. Should we file a separate issue if we want to have it supported?,"Hey , great! Happy to hear that it ""almost"" worked. Yes, unfortunately `unsqueeze` does not support the meta backend yet. I have also came across it in a few HG models. The good news though is that it should be fairly straightforward to add meta support for it. Please create a separate issue and ."
yi,for_blob Tensor building API runs into an issue when specifying device," ðŸ› Describe the bug https://github.com/pytorch/pytorch/pull/74113/files uses `for_blob` API to build up a tensor, but changing it to the following:  results in the following error:  trying ` at::kCUDA` has the same result and putting `.device(...)` before the dtype has the same result.  To reproduce, simply patch above PR and make above mentioned change.  Versions main ",2022-03-11T18:06:58Z,module: cpp triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/74114, ,varma in your code snippet I see that you are deallocating the tensor data using  which means that it resides on the host memory. Do you know why the `device` is reported as `CUDA`? Another question; do you remember which version of CUDA you used for this code?,Check out CC(Fix how we handle host memory in CUDA getDeviceFromPtr).
transformer,"lazy_tensor Input tensor is not a lazy tensor: Background_Matting, nvidia_deeprecommender, speech_transformer",,2022-03-11T13:47:14Z,triaged module: lazy,closed,0,1,https://github.com/pytorch/pytorch/issues/74100,deprioritize
rag,Lazy tensors do not have storage: tts_angular,,2022-03-11T13:26:46Z,triaged module: lazy,closed,0,2,https://github.com/pytorch/pytorch/issues/74099,"The error is triggered by a call to `tensor.storage()` at https://github.com/pytorch/pytorch/blob/bb49d0d54c3875e59769a070017706966f6501a3/aten/src/ATen/native/cudnn/RNN.cppL1510. In the current design of LTC, there is no guarantee that a lazy tensor has a backed storage, thus the assertion in `storage()` is intentional. Supporting `storage()` requires a substantial change to LTC, and the value of it is arguable given only very few kernels are calling `storage()` directly. Without supporting `storage()`, one way to solve the issue here is to force the relevant kernel, `lstm.input`, to go through the fallback path, meaning all the inputs are materialized before entering the kernel. I have tried to do that following the example of `normal_` in LTC, however, what makes things complicated is that `lstm.input` is a CompositeImplicitAutograd kernel. We need to register `lstm.input` as AutogradLazy, and handle it similarly to `max_pool3d` in LTC.",deprioritize
transformer,Two consecutive nn.LayerNorm are used in transformer model when norm_first is False," ðŸ› Describe the bug See the code from the master https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL62L67 https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL340L345 When `norm_first` is False, the last layer of the encoder layer is `LayerNorm()`. The problem is that it always puts a LayerNorm in the encoder, see https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL204L205 The consequence is the output of the last encoder layer is fed into another layernorm, so two consectuive layer norm layers are used here. Also, the output of the ith encoder layer is used as the input for the next LayerNorm layer in (i+1)th encoder layer.  **Expected behavior**: When norm_first is False, the Encoder should not contain a layer norm layer.  Versions The master ",2022-03-11T06:42:21Z,module: nn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/74092,"I'd like to propose a solution for this issue. There are different possibilities, though  I can think of three of them that are quite different:  **backwardincompatible solutions:**    **if a final normalization is always  even when   supposed to be applied to the output of the last encoder/decoder layer:** â‡’ an additional parameter  should be required by / as well to apply the final normalization only when     **else:** â‡’ since the final normalization carried out inside / looks redundant because alredy carried out internally to /, it could simply be removed and / would not need the initialization parameter   **backwardcompatible solutions:**     should be passed as argument for the parameter  of / when instantiating them inside the  class, in place of the additional  instance The second (backwardincompatible) solution looks more coherent, as users would not expect a final normalization to be carried out at the end of the encoder/decoder anyway even when specifying , but it would remove the possibility of having a final normalization different from the ones used inside /. The third solution (the backwardcompatible one) is the simplest one and solves the issue with minimal changes. Any feedback?",Is this a duplicate of CC(Successive Layer Normalization in nn.Transformer)?,"> Any feedback? One thing I'm unsure about with the proposed backwardcompatible solution is how this would work with previously serialized models. By default, LayerNorm has learnable parameters (i.e. `elementwise_affine=True`) that end up in the serialized form of the model. Running `new_model.load_state_dict(old_state_dict)` will fail because of the extra `state_dict` key. I think to be truly backwards compatible, the fix needs to be optin rather than changing the default behavior of Transformer, at least at first.","Thanks for the feedback, , you are right. If you are suggesting to go  at least for the moment  for a backwardcompatible solution, then how about simply never applying  inside the  method of /? It should be truly backwardcompatible and give the desired behavior, but at the expense of keeping unused  attributes for / that do not take part in the forward and backward propagations at all, and this would mean that when / are directly instantiated by users  won't have any effect. Do you have more reasonable backwardcompatible solutions to suggest?"
transformer,Tracing model parameter shapes without instantiating the model parameters," ðŸš€ The feature, motivation and pitch  Request We would like a way to get a model's parameter shapes without instantiating them. The following is an example call signature.   Motivation Hi, I'm a maintainer for the mup package. This repo allows one to implement in their models a special parametrization called maximal update parametrization, or muP, that has the special property that narrow and wide networks share the same optimal hyperparameters (like learning rate, initialization, etc). This is demonstrated below on a Transformer trained with adam, where on the left we have the pytorch default parametrization and the right we have muP. !image This property in particular can be used to tune hyperparameters for extremely large neural networks like GPT3 that is too expensive to train more than once, by just tuning a tiny version of it.  Current Implementation To implement muP in a model, we need to annotate the parameter shapes with two pieces of information. For each dimension in a shape, these are 1) is it a width dimension (which will be scaled up or down)? 2) what is the ""base value"" of this dimension?  our implementation is designed to replicate t",2022-03-13T04:49:16Z,feature module: nn triaged,open,9,12,https://github.com/pytorch/pytorch/issues/74143,"Hey , meta tensors may help solve your problem. They are a special type of tensor that maintain shape information but do not allocate storage for the data. Example:  Note that all `torch.nn` modules have a `device` constructor kwarg that defines their parameters as meta tensors:  See here for more info. The obvious caveat is that any userdefined modules should also define a way to place their parameters on `device='meta'` to work like this.","Hi , we tested `device='meta'` with our package and it seems to work fine as is, which is great news. We have added a blurb in our README about this. However, looking around at some popular github repos, I'm not sure how prevalent is it for models to pass down `device` flags to its submodules. For example, just looking at Huggingface's BERT, this doesn't seem to be the case. So, before this `device` convention is universally adopted by the community, I'm still wondering whether it makes sense to have an automatic way of inferring parameter shapes without needing to change the code of a module.",? Also , the new Deferred Module Initialization feature that we are about to release in an outoftree package will exactly address your particular problem. I would be happy to give you access to our private repo if you want to try it out before we publicly release it.," Hi! I'm the other maintainer of the mup package. Thanks for your response, and we are happy to try this new feature! Please let us know how we can access the private repo.","No problem! I am on PTO today and have limited internet access. Expect to receive an invitation email to the â€œtorchdistxâ€ repo tomorrow morning. If you are on PyTorchâ€™s Slack channel, happy to chat there as well.","  I just sent you an invite to the torchdistx repo. Please reach out to me either over PyTorch's Slack channel or over balioglu at fb.com to chat about the feature. Since the docs are not ""rendered"" yet, you might need some help to locate them.", Thanks! I just asked to be added to torch slack since I don't use FB. Will reach out to you over there once I get in.,  Happy to share that torchdistx is now public: https://github.com/pytorch/torchdistx Check out the docs for Deferred Module Initialization and Fake Tensor which I believe will address your problem. Please let me know if you have any feedback!,Awesome!   Trying this out right now., deferred_init works like a charm for the most part! Though we just encountered an issue with a model that has an unsqueeze call in its forward pass. `*** NotImplementedError: `aten::unsqueeze` cannot be run with fake tensor(s) because the meta backend has no kernel for it. Please file an issue if you want it to be supported.` I assume this is expected since the kernel isn't implemented. Should we file a separate issue if we want to have it supported?,"Hey , great! Happy to hear that it ""almost"" worked. Yes, unfortunately `unsqueeze` does not support the meta backend yet. I have also came across it in a few HG models. The good news though is that it should be fairly straightforward to add meta support for it. Please create a separate issue and ."
yi,for_blob Tensor building API runs into an issue when specifying device," ðŸ› Describe the bug https://github.com/pytorch/pytorch/pull/74113/files uses `for_blob` API to build up a tensor, but changing it to the following:  results in the following error:  trying ` at::kCUDA` has the same result and putting `.device(...)` before the dtype has the same result.  To reproduce, simply patch above PR and make above mentioned change.  Versions main ",2022-03-11T18:06:58Z,module: cpp triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/74114, ,varma in your code snippet I see that you are deallocating the tensor data using  which means that it resides on the host memory. Do you know why the `device` is reported as `CUDA`? Another question; do you remember which version of CUDA you used for this code?,Check out CC(Fix how we handle host memory in CUDA getDeviceFromPtr).
transformer,"lazy_tensor Input tensor is not a lazy tensor: Background_Matting, nvidia_deeprecommender, speech_transformer",,2022-03-11T13:47:14Z,triaged module: lazy,closed,0,1,https://github.com/pytorch/pytorch/issues/74100,deprioritize
rag,Lazy tensors do not have storage: tts_angular,,2022-03-11T13:26:46Z,triaged module: lazy,closed,0,2,https://github.com/pytorch/pytorch/issues/74099,"The error is triggered by a call to `tensor.storage()` at https://github.com/pytorch/pytorch/blob/bb49d0d54c3875e59769a070017706966f6501a3/aten/src/ATen/native/cudnn/RNN.cppL1510. In the current design of LTC, there is no guarantee that a lazy tensor has a backed storage, thus the assertion in `storage()` is intentional. Supporting `storage()` requires a substantial change to LTC, and the value of it is arguable given only very few kernels are calling `storage()` directly. Without supporting `storage()`, one way to solve the issue here is to force the relevant kernel, `lstm.input`, to go through the fallback path, meaning all the inputs are materialized before entering the kernel. I have tried to do that following the example of `normal_` in LTC, however, what makes things complicated is that `lstm.input` is a CompositeImplicitAutograd kernel. We need to register `lstm.input` as AutogradLazy, and handle it similarly to `max_pool3d` in LTC.",deprioritize
transformer,Two consecutive nn.LayerNorm are used in transformer model when norm_first is False," ðŸ› Describe the bug See the code from the master https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL62L67 https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL340L345 When `norm_first` is False, the last layer of the encoder layer is `LayerNorm()`. The problem is that it always puts a LayerNorm in the encoder, see https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL204L205 The consequence is the output of the last encoder layer is fed into another layernorm, so two consectuive layer norm layers are used here. Also, the output of the ith encoder layer is used as the input for the next LayerNorm layer in (i+1)th encoder layer.  **Expected behavior**: When norm_first is False, the Encoder should not contain a layer norm layer.  Versions The master ",2022-03-11T06:42:21Z,module: nn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/74092,"I'd like to propose a solution for this issue. There are different possibilities, though  I can think of three of them that are quite different:  **backwardincompatible solutions:**    **if a final normalization is always  even when   supposed to be applied to the output of the last encoder/decoder layer:** â‡’ an additional parameter  should be required by / as well to apply the final normalization only when     **else:** â‡’ since the final normalization carried out inside / looks redundant because alredy carried out internally to /, it could simply be removed and / would not need the initialization parameter   **backwardcompatible solutions:**     should be passed as argument for the parameter  of / when instantiating them inside the  class, in place of the additional  instance The second (backwardincompatible) solution looks more coherent, as users would not expect a final normalization to be carried out at the end of the encoder/decoder anyway even when specifying , but it would remove the possibility of having a final normalization different from the ones used inside /. The third solution (the backwardcompatible one) is the simplest one and solves the issue with minimal changes. Any feedback?",Is this a duplicate of CC(Successive Layer Normalization in nn.Transformer)?,"> Any feedback? One thing I'm unsure about with the proposed backwardcompatible solution is how this would work with previously serialized models. By default, LayerNorm has learnable parameters (i.e. `elementwise_affine=True`) that end up in the serialized form of the model. Running `new_model.load_state_dict(old_state_dict)` will fail because of the extra `state_dict` key. I think to be truly backwards compatible, the fix needs to be optin rather than changing the default behavior of Transformer, at least at first.","Thanks for the feedback, , you are right. If you are suggesting to go  at least for the moment  for a backwardcompatible solution, then how about simply never applying  inside the  method of /? It should be truly backwardcompatible and give the desired behavior, but at the expense of keeping unused  attributes for / that do not take part in the forward and backward propagations at all, and this would mean that when / are directly instantiated by users  won't have any effect. Do you have more reasonable backwardcompatible solutions to suggest?"
rag,update script to calculate operator coverage ,update model generation script to calculate operator coverage ,2022-03-10T01:08:44Z,cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74005,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74005**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit cd20356ce6 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[BE Hackathon][DataPipe] Automatically generate datapipe.pyi via CMake,Stack from ghstack:  CC([BE Hackathon][DataPipe] Automatically generate datapipe.pyi via CMake)  CC([DataPipe] Separating DataPipes from Dataset into different files) Automatically generate `datapipe.pyi` via CMake and removing the generated .pyi file from Git. Users should have the .pyi file locally after building for the first time. I will also be adding an internal equivalent diff for buck. Differential Revision: D34868001,2022-03-09T22:57:25Z,module: data cla signed release notes: dataloader topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/73991,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73991**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 1fb559a545 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,> The part about the build system update LGTM >  > You will most likely have to update your script to run fine from where it runs now (relative imports do help for these). I removed a dependency and I think it should work fine now,"  Should I be removing the generated .pyi file from git VCS?  Here is the trade off (should only impact PyTorch developers): 1. Remove the file from Git  the .pyi will need be generated by running `python setup.py develop` or similar **every time** you checkout a new branch 2. Keep the file in Git  when you go to a new branch, the Git version of the .pyi file will exist. When you run `python setup.py develop` and that will overwrite the Git version with the generated version. The Git version might be out of sync (generated version will overwrite the Git version).       We keep the file, should we add it to `.gitignore` or not?  I am currently keeping the .pyi file in Git but adding it to `.gitignore`. Maybe keeping it in Git and **not** in `.gitignore` is the best idea?","> Remove the file from Git  the .pyi will need be generated by running python setup.py develop or similar every time you checkout a new branch It's annoying because functional API is pure python rather than the binding from C++ to Python. And, that's why I prefer generating pyi file whenever `functional_datapipe` is invoked. Even new DataPipe is implemented, the pyi file should be updated at that time. Then, even though users are using `python setup.py develop`, they should be able to use these APIs. > I am currently keeping the .pyi file in Git but adding it to `.gitignore` If you want to generate pyi via CMake, this is better.","> Remove the file from Git  the .pyi will need be generated by running python setup.py develop or similar every time you checkout a new branch Not sure to follow. My understanding if the file is not in git and is in gitignore: whatever was there before you change branch will remain there. So yes, after the first clone you will have to do a compilation to get the file. And when you change branch (or modify python code locally) that pyi file might get stale until you recompile. If my understanding is correct then removing it and ignoring it sounds like the right move. Having a file here will potentially lead to conflicts. And a stale pyi is not a big issue during development.",> Sorry for coming back on the review but following the discussions here I think we want to actually remove `torch/utils/data/datapipe.pyi` from the repo. The same way it is done for `torch/_C/_VariableFunctions.pyi`. I will be removing that and also add an internal diff for buck," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"RuntimeError: approximate argument must be either none or tanh: hf_Bart, hf_Bert, hf_Longformer, timm_vision_transformer",,2022-03-09T14:48:42Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/73963,This relates to the gelu/gelu_backward declaration changes at https://github.com/pytorch/pytorch/pull/61439
rag,[ao] Removing memoryless observer args for MovingAverage,"Stack from ghstack:  CC([ao] Removing memoryless observer args for MovingAverage) The original implementation of memoryless observers used MinMaxObservers and a memoryless argument to manipulate the behavior of the observer such that it wouldn't keep track of previously observed min and max's. It was later pointed out that this was equivalent to a movingaverageobserver with averaging_constant=1 which is requires less overhead and no 1 off args (memoryless) so this PR refactors the memoryless arg and uses MovingAverage observers instead, although the memoryless adjective is still used, a complete definintion was also added to clarify error messages given these changes. TestPlan python test/test_quantization.py TestQuantizeEagerQAT python test/test_quantization.py TestObserver Differential Revision: D34732080",2022-03-08T22:16:57Z,cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/73947,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73947**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 01c1537c0d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Add a reference to hierarchical SGD,"Add a reference. Also fix the comment: unlike `averagers.py`, currently this is not a base class that can inherit many subclasses. Proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD))",2022-03-05T05:01:28Z,oncall: distributed triaged open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/73823,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73823**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 674907c24b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[Quant][test] Added test to check if fp16 packing->unpacking yields the same result as to(torch.float16).to(torch.float32) [reland],"Stack from ghstack:  CC([Quant][test] Added test to check if fp16 packing>unpacking yields the same result as to(torch.float16).to(torch.float32) [reland]) Summary: A test was added in test_quantized_op.py that checks whether the fp16 packing and subsequent unpacking of a given fp32 tensor produces the same result as to(torch.float16).to(torch.float32) Test Plan: in pytorch main directory, execute  Reviewed By: jerryzh168 Pulled By: dzdang fbshipitsourceid: 5da453e5db4801dde196424282140726c8a4ef1f (cherry picked from commit ac8910e7feb4eebf677c99f287d48915165a87bf) Differential Revision: D34650372",2022-03-04T20:05:51Z,cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/73808,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73808**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 27211f7351 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Catch overflows in calculating storage byte size,"Fixes CC(Segmentation fault in col2im) In the issue the output tensor's shape is `[2, 4, 536870912, 536870912]` which results in a `numel()` slightly below the point of overflow. When the storage is created it does `numel() * 8` which overflows and a much smaller storage is allocated than required.",2022-03-03T02:23:44Z,module: error checking triaged open source cla signed release notes: cpp topic: improvements,closed,1,18,https://github.com/pytorch/pytorch/issues/73719,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73719**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit 85abfb1b01 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thanks for the patch! We will want to benchmark this before merging it though as  mentioned that similar approach before had significant impact.  what would be a good set of benchmark to do here?,In my callgrind benchmark I see a 0.1% increase in instruction count.   I think past objections have been to checking for overflow in `compute_numel()` which is probably more costly because the check is called inside a loop; whereas this check only happens once. Also note that unlike `compute_numel()` this doesn't get called when tensor views are created.,"For a general solution, the checks unfortunately have to be done in a loop because the overflow can happen in any iteration in a loop, and then final result will appear to be within limits.  suggested using intrinsic at least on systems that support it  CC(Segmentation fault in max_pool3d_with_indices)issuecomment1054553201   can you look at adopting this approach? The PR as is improves some cases (where numel doesn't overflow, but storage size does), but even small increase in number of instruction for this limited case doesn't seem worth it.  ",Can you please post instruction count benchmark? Do we need some tests for this behavior? Running them only on linux should be safe I think,"Okay, I've switched it over to check for overflow at every stage of the computation when `__builtin_mul_overflow` is available. Using the same benchmark as before I now get around a 1% increase in instruction count. ","It looks like there are some build issues to work out. Before I dig into it, perhaps we should decide whether any increase is okay because it's likely unavoidable. ","I think 36 instructions is an acceptable cost for a general solution? , ",I'm OK with it because we are about to add symbolic ints which will also have this kind of cost.,+1 to this being fine. It looks like the `mul`/`imul` instruction sets the bit anyway on x86 (https://godbolt.org/z/rbhPb5K9K) so it only adds one *highly* predictable branch instruction which doesn't appear to affect the runtime in the slightest. (https://quickbench.com/q/T10cMIrQtlUMo0qccGKXw24zwdY) ARM adds a whopping *two* extra instructions.,"There's CC(Add overflow check to `TensorImpl::compute_numel`) that fixes numel computation in a very similar way, and doesn't have build issues  should we merge it first?", you gonna import this?, [edit] it's still a draft.  is there anything that prevents you from publishing it?, ping, PTAL," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training," ðŸš€ The feature, motivation and pitch We are working on training large transformer and DNN. Any approach for saving GPU memory is valuable.  [https://arxiv.org/pdf/2202.13808v1.pdf] DropIT seems to be fitting into pytorch tensor management area.  Will be good to explore if we can offer this feature in pytorch training. Cheers  Dr.  Alternatives _No response_  Additional context _No response_",2022-03-03T00:48:13Z,feature module: memory usage triaged,open,0,1,https://github.com/pytorch/pytorch/issues/73709,We generally wait for research to stabilize before incorporating it into PyTorch proper. Was this published in feb/2022?
yi,[Quant][test] Added test to check if fp16 packing->unpacking yields the same result as to(torch.float16).to(torch.float32),"Stack from ghstack:  CC([Quant][test] Added test to check if fp16 packing>unpacking yields the same result as to(torch.float16).to(torch.float32)) Summary: A test was added in test_quantized_op.py that checks whether the fp16 packing and subsequent unpacking of a given fp32 tensor produces the same result as to(torch.float16).to(torch.float32) Test plan: in pytorch main directory, execute  Differential Revision: D34599476",2022-03-02T21:12:06Z,cla signed Reverted,closed,0,6,https://github.com/pytorch/pytorch/issues/73685,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73685**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit ad807c58bf (more details on the Dr. CI page):  * **6/6** failures introduced in this PR   :detective: 6 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5449668241?check_suite_focus=true) linuxbionicpy3.7clang9 / test (default, 2, 2, linux.2xlarge) (1/6) **Step:** ""Test"" (full log  :repeat: rerun)   20220307T15:25:42.6987215Z RuntimeError: test_quantization failed!     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This pull request has been **reverted** by 8a47d9cf869ceb44580a5e333741382323b7e637. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk)."
yi,(torch/elastic) add documentation clarifying that torchrun is a console script to torch.distributed.run,"Summary: resolves  CC([docs] distributed docs still mention/recommend deprecated torch.distributed.launch) Simply clarifies that `torchrun` is a console script that invokes `python m torch.distributed.run`. Test Plan: N/A doc change only, letting github CI validate that the docs build correctly. Differential Revision: D34558538",2022-03-01T19:54:10Z,oncall: distributed fb-exported cla signed,closed,0,22,https://github.com/pytorch/pytorch/issues/73598,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73598**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit fc5640a0ae (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 71aa3ab020 since Mar 02   :construction: 1 ongoing upstream failure: These were probably **caused by upstream breakages** that are **not fixed yet**. * linuxxenialpy3.7clang7asan / test (default, 1, 3, linux.2xlarge) since Mar 02 (e6afa4f771)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D34558538,This pull request was **exported** from Phabricator. Differential Revision: D34558538,"What is `NUM_TRAINERS`? In academic environments often we don't know beforehand how many jobs will be run per node. Often someone will run a job taking 4 GPUs without knowing if other people take all 4 remanining GPUs in one job, or 2 jobs by 2 GPUs or no jobs at all for some time",A link in the docs to the github source code of `torchrun` would be awesome,"> What is `NUM_TRAINERS`? In academic environments often we don't know beforehand how many jobs will be run per node. Often someone will run a job taking 4 GPUs without knowing if other people take all 4 remanining GPUs in one job, or 2 jobs by 2 GPUs or no jobs at all for some time Its a placeholder env var that you pass to `nproc_per_node` (the number of local workers to start for this particular job)",> A link in the docs to the github source code of `torchrun` would be awesome done,> Its a placeholder env var that you pass to `nproc_per_node` (the number of local workers to start for this particular job) Is it mandatory? In practice the person running the first job may not know what other jobs will be run in the future on the same machine,> > Its a placeholder env var that you pass to `nproc_per_node` (the number of local workers to start for this particular job) >  > Is it mandatory? In practice the person running the first job may not know what other jobs will be run in the future on the same machine no its just a way to document that you can pass whatever integer you want to `nproc_per_node` without having to hard code a specific number. Same can be achieved by having documented it as `nproc_per_node=k`,i think it'd better be specified explicitly that it's not mandatory (to have the simplest recipe for this most frequent scenario),"> done hmm, seems this change is not pushed to the PR yet...","> A link in the docs to the github source code of `torchrun` would be awesome done > i think it'd better be specified explicitly that it's not mandatory (to have the simplest recipe for this most frequent scenario) you need to specify `nproc_per_node`, this tells the launcher how many ""copies"" of your trainer to launch (all the copies belong to a single process group, aka job)",This pull request was **exported** from Phabricator. Differential Revision: D34558538,"Right, so in practice this is ~NGPUS, right?",So far I kind of understood that torchrun somehow runs torch.distributed.run script. Is it actually just a symlink? Or does it have more functionality? From this phrasing it is not clear: `is a console script to the main module torch.distributed.run` ,"> Right, so in practice this is ~NGPUS, right? yes","> So far I kind of understood that torchrun somehow runs torch.distributed.run script. Is it actually just a symlink? Or does it have more functionality? From this phrasing it is not clear: `is a console script to the main module torch.distributed.run` its literally called ""console script"" for standard python packaging: https://packaging.python.org/en/latest/specifications/entrypoints/useforscripts","I see! outside of python packaging, `console_script` doesn't have a very precise meaning and could contain extra functionality. For such docs IMO it doesn't hurt to be a bit more verbose, not everyone is wellversed in python packaging terminology...",This pull request was **exported** from Phabricator. Differential Revision: D34558538,"> I see! outside of python packaging, `console_script` doesn't have a very precise meaning and could contain extra functionality. For such docs IMO it doesn't hurt to be a bit more verbose, not everyone is wellversed in python packaging terminology... yea, I added a hyperlink.",This pull request was **exported** from Phabricator. Differential Revision: D34558538,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,`torch.result_type` does not yield correct type in `torch.autocast()` enabled regions, ðŸ› Describe the bug  Description `torch.result_type` incorrectly computes `float32` output within a `torch.autocast()` enabled region  Repro  I am willing to help on the fix  Versions  Versions  ,2022-02-28T21:33:43Z,triaged module: amp (automated mixed precision),closed,0,4,https://github.com/pytorch/pytorch/issues/73538,"This is expected behavior, as `result_type` outputs the output type of pointwise operation on its inputs subject to pytorch type promotion rules https://pytorch.org/docs/master/generated/torch.result_type.html?highlight=result_typetorch.result_type. `matmul`, even without autocast, doesn't comply to these type promotion rules. ","> This is expected behavior, as `result_type` outputs the output type of pointwise operation on its inputs subject to pytorch type promotion rules https://pytorch.org/docs/master/generated/torch.result_type.html?highlight=result_typetorch.result_type. `matmul`, even without autocast, doesn't comply to these type promotion rules. Thanks for the input . From the point of view of the users, does it make sense to compute output's `dtype` based solely on type promotion rules? My understanding is that users want to know the output dtype, regardless of what feature/capability is enabled. By reading the function description (aka `Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors`) it seems the aforementioned expectation includes Autocast.  Maybe Autocast didn't exist when `result_type` was implemented (didn't check that), thus the mention of type promotion doc and no mention of Autocast? If indeed this is the desired behavior, I would suggest (or even push a PR) adding a warning note in the doc highlighting the `result_type`'s result may be incorrect when running on a Autocast enabled region.","But it's not just autocastenabled region, e.g. `mm` on float16/float32 won't work, but `result_type` for float16/float32 will happily output `float32`. Regular pointwise operations (for which `result_type` generally predicts correct result type) will run the same whether in the autocastenabled region or outside of it, difference is only for ops like `mm`, and `result_type` has only tensor args, not operation arg, so it cannot predict output dtype that depends on the operation. "," yes, ops that do not accept mixed precision out of the box (most of them?) will raise an exception related to the type mismatch. But the same op wrapped by autocast would work properly. This in particular shouldn't be a problem for `result_type` On the other hand, you brough a great point! `result_type` only take tensors as input, not operator [name]. What do think think about adding a string `op_name` as optional argument, defaulting to None, in which users could leverage type inference based on operator name when autocast is enabled? My end game is having a way for users to know what is the result type of an operator execution without actually executing it. With this information, they could make informed decisions in their training script or pytorch extension (ONNX runtime). Another alternative would have a dedicated API, say `torch.autocast_result_type(opname: str, *inputs)`? What do you think?"
agent,"PyTorch for ROCm on a Supported Device Throws ""hipErrorNoBinaryForGpu"""," ðŸ› Describe the bug I am trying to install PyTorch with ROCm support on a Unibap iX10 ADS  which uses a slightly modified version of Ubuntu 18.04 server with an AMD Ryzen Embedded V1605B with Radeon Vega Gfx GPU  and whether I install from source of via pip the error  is thrown when I try to use CUDA, even though C HIP code compiles and runs perfectly well.  Using the environment variable AMD_LOG_LEVEL=6 I have found that it seems like it tries to look for an incorrect device; I am including a few test script outputs. To summarize them, square.cpp is a HIP file that squares the elements in an array and compiles and runs without issues; based on the output log, it seems to look for the correct device ""AMD Ryzen Embedded V1605B with Radeon Vega Gfx."" PyTest.py is a simple python script that does nothing but import torch and  and throws the error before crashing. The output log shows that it looks for the device ""amdgcnamdamdhsagfx902:xnack+,"" which is a part of my device  rocminfo lists it as the ISA of Agent 2  though it seems like it is incorrect, since square does not seem to look for it.  I am also including the output of rocminfo for more informa",2022-02-28T21:11:21Z,module: rocm triaged,closed,1,4,https://github.com/pytorch/pytorch/issues/73534,I am facing same issue but for **amdgcnamdamdhsagfx1010:xnack  [Not Found]** ,Can confirm the same issue on gfx1030 device using PyTorch 1.11.0 (RoCM 5.20) Docker: ,"Hi  , I'm able to confirm gfx1030 is working as expected on my local dev box:  reference dockerfile:   Sample output using the docker container built with it:  You can refer to the following document on how to properly install PyTorchROCm on supported GPUs: https://docs.amd.com/bundle/Deep_learning_Guide_5.2/page/Frameworks_Installation_Guide.html At this moment we have `gfx900, gfx906, gfx908, gfx90a, gfx1030` binary supported in PyTorch WHL package and the dependency libraries. ","Hello all, As clear stated by Unibap, only their versions of ROCm, pyTorch, Tensorflow, pyHIP, etc will work on their products based on AMD APUs. Do not expect the reglar ROCm or higher order software to work on APUs. They are not officially supported by AMD. Contact the vendor."
rag,`storage` does support `complex32` tensor," ðŸ› Describe the bug `storage` and `storage_type` does support `complex32` tensor  Interestingly, `storage_offset` supports `complex32` tensor   Versions pytorch: 1.10.1 ",2022-02-28T11:23:57Z,triaged module: complex module: half,closed,0,3,https://github.com/pytorch/pytorch/issues/73502,Hey! Note that complex32 was disabled for the upcoming release and should not be used at the moment as it is WIP,"Hi, based on the replyissuecomment1035337126), `complex32` may be supported in future version, so I think we need to pay some attention on it",Ho yes for sure! Just wanted to let you know that now might be a bit early to use it as there are still quite a few rough edges.
chat,Windows workflows frequently failing with timeout," Current Status Mitigated  Error looks like Example failure looks like: https://github.com/pytorch/pytorch/runs/5346428448?check_suite_focus=true  Incident timeline (all times pacific) Started on 2/22/22 around 12pm PST, with this commit: https://hud2.pytorch.org/pytorch/pytorch/commit/53faf78143bbca3157dfef41dedf76eb0ba0b9f2    Around 2/22/22, windows CI jobs started to fail with timeout errors such as https://github.com/pytorch/pytorch/runs/5346428448?check_suite_focus=true. It looks like this issue started with this commit: https://hud2.pytorch.org/pytorch/pytorch/commit/53faf78143bbca3157dfef41dedf76eb0ba0b9f2 which may have increased the duration of windows tests resulting in this timeout, though this is yet to be determined.    e.g.  2/22 12:22pm Incident began  2/22 12:54pm Manually detected by OSS CI oncall, pinged in chat  [not yet root caused]  [not yet mitigated]  [not yet closed]   User impact  Windows jobs are failing, causing noise on PRs and potentially masking real windows breakages  Root cause To be filled out after mitigation  Mitigation  sent a PR to bump windows test timeout: https://github.com/pytorch/pytorch/pull/73490 SEV is m",2022-02-28T02:59:36Z,triaged ci: sev,closed,0,2,https://github.com/pytorch/pytorch/issues/73495,Should we close this one and continue discussion in  CC(Windows tests frequently timeout) ?,Followup:   Raise an alert when test runtime is 75+% of the specified limit   [BE] Why do we need to limits: one on the workflow and another on the job
yi,"dbr quant: insert activation obs explicitly, instead of relying on hooks","Stack from ghstack:  CC(dbr quant: enable reference module support for torch.qint32) dbr quant: enable reference module support for torch.qint32 * * CC(dbr quant: insert activation obs explicitly, instead of relying on hooks) dbr quant: insert activation obs explicitly, instead of relying on hooks** Summary: Before this PR, DBR quant reused the Eager mode quantization machinery to insert activation observers. This was done for speed of developing the prototype. A drawback of this is that the activation observers are not present in DBR's data structures and live on the modules instead. This PR refactors DBR quant to stop using Eager mode quantization observer insertion for activations, and instead create and track the activation observers in DBR's data structures. This has a couple of benefits: 1. activation observers are now created the same way in DBR for modules and functions 2. we can remove some technical debt due to fixing (1) 3. this will make it easier to support reference modules in a future PR The reason (3) is true is because the current design of reference modules assumes that the activation observer lives on the framework (like in FX gra",2022-02-28T02:26:36Z,cla signed release notes: quantization topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/73492,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73492**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit a3f4a48c38 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," ,  , would you be able to help review this?"," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
agent,(torch/elastic) skip logging structured error info if error_file is not set,"Summary: resolves  CC([torchelastic] properly format (or don't log) trace info in structured error when the agent process is killed prematurely) This `log.error` is not necessary (and its also not humanfriendly formatted) because we end up reraising the same exception after recording the exception into an error_file (if present). Eventually python should handle this error the way it handles any other errors and will write the trace info into the console. This additional logging produces duplicate error console prints, which affects all users whose schedulers do not set `TORCHELASTIC_ERROR_FILE` env var when calling `torch.distributed.run`. Test Plan: Induce an error on the agent process by `kill 15 $AGENT_PID`  Produces {F704936697} In contrast to the duplicated error before: {F704936729} Differential Revision: D34501852",2022-02-26T20:00:03Z,oncall: distributed fb-exported cla signed release notes: distributed (ddp) topic: bug fixes,closed,1,5,https://github.com/pytorch/pytorch/issues/73477,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73477**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 2334765989 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34501852,This pull request was **exported** from Phabricator. Differential Revision: D34501852,This pull request was **exported** from Phabricator. Differential Revision: D34501852,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,different results on each batch_size in torch==1.10.2+cu113 on RTX 3080," ðŸ› Describe the bug on each batch size, the result is different, I tried on googlecolab(with there default pytorch) the outputs were consistent.   > (tensor(11, device='cuda:0'), torch.Size([768])) the same code on google colab but with `torch.use_deterministic_algorithms(False)` because it raises an error  gives: > (tensor(768, device='cuda:0'), torch.Size([768]))    Versions ",2022-02-26T18:38:13Z,triaged module: numerical-reproducibility,open,0,3,https://github.com/pytorch/pytorch/issues/73475,"Hi, This is very much possible if an early layer is not deterministic and leads to small errors that then get amplified by the rest of the network. You can try to use the https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html?highlight=deterministictorch.use_deterministic_algorithms flag to force determinism. Also note that models that have batchnorm or similar models change their behavior after every forward when they are in training mode.","> flag to force determinism. even this didn't work and the results are way more different.  On google Colab(K80) even without determinism still gives the same results. I traced the operations, and even a simple addition like `residual + hidden_states` gives different results on different batch_size ",> the same code on google colab but with torch.use_deterministic_algorithms(False) because it raises an error If that raised an error that means that the algorithm is indeed nondeterministic. Note that nondeterministic doesn't mean that it will always be different. Only that it can be (depending on a lot of things including version of dependency/hardware/input values/etc.
agent,[torchelastic] properly format (or don't log) trace info in structured error when the agent process is killed prematurely, ðŸ› Describe the bug When running with `torchrun` and: 1. `TORCHELASTIC_ERROR_FILE` env var is not set at the agent level 2. The agent process (process that is running `torchrun`) either raises an error or is killed prematurely due to a signal Then we end up seeing an ugly unformatted trace info (`extraInfo.py_callstack`) from the error log of the structured error of the agent process (see screenshot below) !image What should happen is that we either format the trace info in the structured error when logging or just skip the error logging and let python's error handling/console printing do the job. REPRO STEPS: 1. create a `test.py` with a single line `import time; time.sleep(6000)` 2. `torchrun nproc_per_node 2 nnodes 1:1 rdzv_backend c10d rdzv_endpoint localhost:29500 monitor_interval 3 test.py` 3. grab the PID of CC(Don't support legacy Python) and run `kill 15` on it  4. Observe the unformatted error traceback (as shown in the screenshot). The logging is done on this line: https://github.com/pytorch/pytorch/blob/df11e2d6f9782fc3995e17ef09a5ef3812da041d/torch/distributed/elastic/multiprocessing/errors/error_handler.pyL36 ADDITIONAL LINKS: 1) (Orig,2022-02-26T01:16:43Z,triaged module: elastic oncall: r2p,closed,0,13,https://github.com/pytorch/pytorch/issues/73465,"Please see these 2 log files for examples of interleaving and too many errors replayed for each node and/or process: 181Bn48pp12bf162182654.txt 181Bn48pp6bf161305.txt This setup is 48 nodes of 8 gpus each:  Both exemplify what I have been referring to in various issues. Some errors are dumped once per node (first `wc l`, others for each process, 2nd `wc l`) and you can see that at times it's impossible to unravel the Tracebacks as they are interleaved, ` 181Bn48pp12bf162182654.out` is the better example of interleaving. sometimes it's almost perfect, other times it's much much worse interleaving. If need be I can dig up more examples. If you would like any more input about the setup please let me know but otherwise it's:  Thank you!","Looks like you have a single .out file for all the nodes. Are you observing intra or inter node trace interleaving? To make things better intranode, you could try passing `tee 3` (prefixes local rank to each line of stdout and stderr). For intranode... this isn't really controlled by us (neither is intranode, but passing `tee` helps a bit).  The scheduler (I'm assuming SLURM in your case from the srun logs) should have a separate stream per node, I'm assuming you're tee'ing those back to a single console? You could also try using workerperslurmtask, in which case SLURM would create separate output streams for each worker (in this case you wouldn't even need `torch.distributed.run`)","Well, there are typically 2 types of failures: 1. something happened in all processes  e.g. broken code  and they all die at once  same error 2. something happens in one process  e.g. OOM  and then all processes get killed  currently in either event we end up with n_errors == n_gpus, whereas the ideal situation would be, that only a set of these errors is printed. So if it's the same error on all processes it just says  all processes died with the following error, or there were 2 errors, etc. But I suppose the heuristics of comparing the different backtraces could be complicated.","I see, yep this is something we've been wanting to fix. Just to be clear, (besides the duplicate error logs) this isn't a regression from torch1.8 right? AFAIK this has been an issue with the fact that torch runs multiple workers on the same node as different processes.","> Looks like you have a single .out file for all the nodes. Are you observing intra or inter node trace interleaving? From my manual experiments it's typically an intranode issue, please see how I resolve it here: https://github.com/pytorch/pytorch/pull/61803 and propose to add it to torch.distributed for normal prints  as they suffer the same issue, but somehow this needs to be extended to the python exception manager so that when it dumps the traceback it goes through a `flock`'ed print. > To make things better intranode, you could try passing `tee 3` (prefixes local rank to each line of stdout and stderr). For intranode... this isn't really controlled by us (neither is intranode, but passing `tee` helps a bit). I will give it a try  thank you! > The scheduler (I'm assuming SLURM in your case from the srun logs) should have a separate stream per node, I'm assuming you're tee'ing those back to a single console? You could also try using workerperslurmtask, in which case SLURM would create separate output streams for each worker (in this case you wouldn't even need `torch.distributed.run`) SLURM it is. The problem is that if I don't pipe it into a single file it becomes much harder to monitor problems.  So ideally I want a single log file I can `tail f` that tells me the important events but not duplicated 300 times. And then it's fine to have additional log files.","ok, I tried `tee 3` and it prefixes local rank but how does it help?  Are you suggesting that it'd be helpful if I get an intrainterleaved traceback and then I'd grep for a single local rank, and thus be able to extract the noninterleaved traceback? That's doable but that's doing a whole bunch of steps, when I quickly need to determine the cause of failure and act on it. Typically I have the cursor on the last good line of the `tail f file` while it continues spitting data or use console's search to find the place before the crash, and I want to get the normal traceback right there. But now I have to copynpaste this to another file, grep this file, create new file and only then I get to the traceback. This is very painfully inefficient. but doable.",Here is another related issue  CC([elastic launcher] redirects/tee support for global rank),"> I see, yep this is something we've been wanting to fix. Just to be clear, (besides the duplicate error logs) this isn't a regression from torch1.8 right? AFAIK this has been an issue with the fact that torch runs multiple workers on the same node as different processes. This is definitely not a regression and when I tried with 1.11tobe it looked as if someone worked on it and more often than not the tracebacks were either noninterleaved or close to being noninterleaved. Except when it completely fails and gets very interleaved. So perhaps the ""working"" was just a coincidence.  If I try to reproduce it with a small example on a single node 95% of time I get perfect noninterleaved tracebacks. But you have seen now the situation from the logs I attached:  CC([torchelastic] properly format (or don't log) trace info in structured error when the agent process is killed prematurely)issuecomment1052474028","yep I worked on it based on our conversation after torch1.9. The error summary output is supposed to help you quickly figure out what happened by looking at the last 50100 lines (tail 100) of the console output of each node. This is why the â€œRoot Causeâ€ error is reported last. Looks like I missed the edgecase where the agent itself fails or is killed by the scheduler, then the code follows a different branch and error summary doesnâ€™t kick in.  This should help with the duplicate traces: https://github.com/pytorch/pytorch/pull/73477issuecomment1052536699 Iâ€™ll make a separate PR to also write the agent failures using the same error summary format as the worker failures. ","Thanks a lot for making it better. > The error summary output is supposed to help you quickly figure out what happened by looking at the last 50100 lines (tail 100) of the console output of each node. That would be amazing! currently I have to scroll up for 5 min until I find the actual cause, since usually at the end I get many totally unrelated errors which happen due to earlier errors. And I get hundreds of these at the end: ",ah adding ``to your main function will make it so that the exception traces appear in that summary instead of the â€œTo enable trace backâ€¦â€. ,Additionally could the `tee` preamble make a bit of white space after it and be less noisy by itself with square brackets? Currently it's overbearing: ,"also if I orchestrate a failure MegatronDeespeed I consistently get a bunch of segfaults with, so my log ends with:  where the bt is:  perhaps it's totally unrelated to the launcher and something that this framework causes..."
yi,Fix CUDA error when multiplying sparse hybrid tensors with zero dense dimensions,Fixes  CC(TestSparse misses out on TestCase.setUp() + thus disabling doesn't work)   CC(Fix CUDA error when multiplying sparse hybrid tensors with zero dense dimensions) Differential Revision: D34478521,2022-02-25T16:09:48Z,module: sparse open source cla signed release notes: sparse topic: bug fixes,closed,1,3,https://github.com/pytorch/pytorch/issues/73428,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73428**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit dd6b42f062 (more details on the Dr. CI page):  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base 590685dc6e on Feb 25 from  7:37am to  9:03am   1 failure *not* recognized by patterns:    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) on Feb 25 from  7:37am to  9:03am (590685dc6e  3c45fc8e20)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
transformer,ONNX export failed for torch_scatter ops in TAPAS model," ðŸ› Describe the bug I am trying to convert the TAPAS model from HuggingFace into ONNX format. However, it has torchscatter dependency which cannot be export to ONNX. For easy replication:   And I am getting the following error msg:   Versions Collecting environment information... PyTorch version: 1.11.0a0+bfe5ad2 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: version 3.21.3 Libc version: glibc2.31 Python version: 3.8.12  (default, Oct 12 2021, 21:59:51)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.01066awsx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.6.55 GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 450.142.00 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/lib",2022-02-24T21:22:01Z,module: onnx triaged,closed,1,5,https://github.com/pytorch/pytorch/issues/73388,Here is some documentation to get you started on exporting custom operators not in the default namespaces https://pytorch.org/docs/stable/onnx.htmlcustomoperators,"I also have this problem, for `torch_scatter::scatter_max` when trying to export a PyG GAT model to ONNX. Did anyone find a way to support `torch_scatter` ops during ONNX export? A related issue https://github.com/rusty1s/pytorch_scatter/issues/181 and followup https://github.com/pygteam/pytorch_geometric/discussions/3514 aren't particularly helpful.","For TAPAS model at least, the following naive scatter implementation can be used to replace the PyTorch Geometric's scatter ops and let onnx export succeed. Note: This work was intended to make TAPAS running in TensorRT, rather than in PyTorch  in PyTorch it's definitely much slower than PyG, but in TensorRT the endtoend is great for inference. ","Thanks , unfortunately I need 2D `scatter_max`. I'm looking into whether we can get this supported directly, see https://github.com/onnx/onnx/issues/4322 and https://github.com/pygteam/pytorch_geometric/issues/728.","The following repro ` Produces  meaning it does not recognize `torch_scatter::scatter_min` as an operator. Indeed, `torch.onnx.export` only supports operators with domain `aten::*`.  As pointed out by , a custom operator is needed to support this scenario as documented at https://pytorch.org/docs/master/onnx.htmlcustomoperators"
rag,Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer," ðŸš€ The feature, motivation and pitch See https://github.com/pytorch/pytorch/pull/73285discussion_r814232287 Since the current periodic model averager is a special case (2level) hierarchical model averaging, and it can be embedded into `PostLocalSGDOptimizer`, the new hierarchical model averaging module should be able to run in the same way.  Alternatives _No response_  Additional context _No response_ ",2022-02-24T21:01:15Z,oncall: distributed,closed,0,1,https://github.com/pytorch/pytorch/issues/73382,"Another nice usability improvement is renaming `PostLocalSGDOptimizer` as `ModelAveragingOptimizer`, since it can not only support post local SGD, but also (post) hierarchical SGD. cc: varma  "
agent,[Dynamic RPC] Allow for optional world_size argument in init_rpc,"Stack from ghstack:  CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) [Dynamic RPC] Add graceful shutdown for dynamic RPC members  CC([Dynamic RPC] Allow existing ranks to communicate with newly joined ranks) [Dynamic RPC] Allow existing ranks to communicate with newly joined ranks  CC([Dynamic RPC] Allow newly joined ranks to communicate with existing ranks) [Dynamic RPC] Allow newly joined ranks to communicate with existing ranks * * CC([Dynamic RPC] Allow for optional world_size argument in init_rpc) [Dynamic RPC] Allow for optional world_size argument in init_rpc** This PR which allows for optional `world_size` argument in init_rpc. This makes changes in rendezvous to allow for `NoneType` for world_size and creates a new code path when initializing TensorPipe agent for init_rpc. The TensorPipe agent is protected by a critical section enforced using the store, so that only one node can create a TPAgent at a time. This PR does not yet enable RPC commands between ranks. Previously:  Now (only rank is needed):  Tp run the added tests: `pytest test/distributed/rpc/test_tensorpipe_agent.py vsk test_init_rpc_without_world_size` `pytest t",2022-02-24T19:46:36Z,oncall: distributed cla signed release notes: distributed (rpc) topic: new features,closed,0,6,https://github.com/pytorch/pytorch/issues/73372,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73372**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ce6a56dfaf (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Huang has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","> And we were thinking it is because TensorPipe used the same pipe for request and response.  is this correct assumption? I don't know what's the context for this, and this doesn't really depend on TensorPipe but on how the RPC agent chooses to use TensorPipe. Last time I checked this was the case:  The A>B pipe was used for A sending requests to B and B sending responses to A  The B>A pipe (which is separate!) was used for B sending requests to A and A sending responses to B","Huang has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Huang has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", retest this please
rag,[Model Averaging] Support hierarchical model averaging,"Implement hierarchical model averaging proposed in  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)). Unit tests are added. Since I don't have access to 4GPU machines in opensource environment, expect that the branch with the prefix of `ciall` can run the test that requires 4 GPUs. In the future, the internals of `PeriodicModelAveraging` can be simplified as an implementation of a specialized hierarchical model averaging, where `period_group_size_dict` only has a pair of period and world size.",2022-02-23T07:08:53Z,oncall: distributed open source cla signed release notes: distributed (ddp) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/73285,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73285**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 1a233963d4 (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5327261059?check_suite_focus=true) winvs2019cuda11.3py3 / test (default, 1, 2, windows.8xlarge.nvidia.gpu) (1/1) **Step:** ""Test"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",failures are unerlated
transformer,LazyLinear with equal in_features and out_features," ðŸš€ The feature, motivation and pitch I find the LazyLinear module very useful in a particular case of applying an MLP after some feature extractor (e.g., Transformer of some convnet). In this case, the output dimensionality of the feature extractor depends on the choice of a backbone net, so `LazyLinear` allows one not to think about passing the dimensionality to an MLP constructor. However, it is common practice to build MLPs with multiple hidden layers of the same size: `feature_extractor (768) > Linear(768, 768) > ReLU > Linear(768, n_out)`. For instance, this is done in the RoBERTa classification head. Here I discovered that I still need to pass (and calculate) the output dimensionality of the feature extractor since I need to define the `out_features` parameter of the `LazyLinear`. I think it would be great to have a subclass of `LazyLinear` that infers the number of input features and creates a square parameter matrix.  Alternatives I think now it is possible to derive a subclass of LazyLinear and override `initialize_parameters` method.  Additional context _No response_ ",2022-02-21T12:13:41Z,module: nn triaged needs research,open,0,1,https://github.com/pytorch/pytorch/issues/73172,"Hey , thanks for the request! As you mentioned, it should be straightforward to create such a square layer by subclassing `LazyLinear` or `LazyModuleMixin`, but please feel free to post here if you run into problems doing so. Note that, to reduce maintenance costs, we try to minimize the set of modules we provide in PyTorch core to those that are widely used or expected to be present. If this request becomes more popular, we can consider providing such a module in torch.nn."
yi,Fix overflow check in `geometry_is_contiguous`,  CC(Fix overflow check in `geometry_is_contiguous`)  CC(Remove native_functions.yaml dependency from CUDA distributions) The existing check isn't safe for 32bit `size_t` because the max 64bit int will overflow. Differential Revision: D34524229,2022-02-20T20:03:06Z,open source cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/73162,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73162**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 82aa437d4c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
yi,How do we handle metadata-modifying in-place operators (like `squeeze_`) with `__torch_dispatch__`?," This doesn't modify the tensor's shape, as it's inplace. Usually, we can address this by checking whether an operator is inplace or not (https://github.com/pytorch/functorch/blob/main/functorch/_src/python_key.pyL123), but that only allows us to modify the underlying `elem` tensor. How do we update the metadata here? cc:     ",2022-02-20T00:56:15Z,triaged module: __torch_dispatch__,open,0,3,https://github.com/pytorch/pytorch/issues/73150,Need to synchronize the metadata after the operation. If we had a tag for these cases you could only do the synchronization for operators which are known to modify metadata (there aren't that many). Does `resize_` work?,> Need to synchronize the metadata after the operation. Is there a way of doing this now? > Does resize_ work? No,"Here's how we did it for CompositeCompliantTensor. I don't know if this works in general, but it would be nice to have some blessed way of doing this: https://github.com/pytorch/pytorch/blob/932adf26e4b88e97425021aca492c4efd156e1e8/torch/testing/_internal/composite_compliance.pyL136L165"
rag,Support writing tensorboard traces to AWS S3 (and other cloud storage services) in profiler," ðŸš€ The feature, motivation and pitch I would like to write profiler logs directly to S3 so that they can be read from tensorboard remotely. Details of datasets and model runs can already be logged in a tensorboard format and saved directly to S3 with a call like  Those logs can be displayed running `tensorboard logdir=s3://bucket/prefix/`. _Profiling_ logs are handled somewhat differently, and while the pytorch profiler tb_plugin instructions show how to _read_ logs from S3 into a tensorboard instance, currently it doesn't seem possible to _write_ logs to S3 while profiling. Specifically, an example like:  does not write logs to S3 but instead tries to create a local dir of the same name and raises an error `ValueError: No logger registered for the s3 protocal prefix`. (I am unsure whether this should be filed as a bug or a feature request, but since the documentation doesn't spell out that this should be possible I'd consider it the former)  Alternatives A current workaround could be to write all tensorboard logs to a local dir, then copy to S3 with boto. But partial writing to S3 is already supported (in `SummaryWriter`, but not `profiler` AFAICT)",2022-02-19T00:23:03Z,feature triaged module: tensorboard OSS contribution wanted,open,5,4,https://github.com/pytorch/pytorch/issues/73131,"This was briefly raised on discuss.pytorch.org earlier, link for reference: https://discuss.pytorch.org/t/canpytorchprofileradds3support/117397",This would be a great feature to have!," You might want to take a look at https://github.com/pytorch/kineto/blob/6968a24e3f2e72263e25d69a1a7f57dd0dd83181/libkineto/src/ActivityLoggerFactory.hL29 In principle, libkineto supports adding new protocols in this way, and IIRC this is already used internally in Meta for writing to remote storage. The error message you saw comes from the same file.",Any updates..?
rag,Disable test history as it's fragile,Related to CC(Improve test_test_history.py),2022-02-18T18:16:27Z,cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/73093,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73093**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 87860946a1 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   1 failure *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this please, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Fix serialization and deepcopying for wrapper subclasses,"Stack from ghstack:  CC(Fix serialization and deepcopying for wrapper subclasses) This PR addresses broken support for serialization and deepcopy on wrapper subclasses. In particular, the old implementations of `Tensor.__reduce_ex__` and `Tensor.__deepcopy__` fail by assuming the tensor type has storage, which is not the case for wrapper subclasses. To fix this: * For serialization, `Tensor.__reduce_ex__` is expanded to return a new reconstruction function `_rebuild_wrapper_subclass` (that calls `_make_wrapper_subclass`) for cases for tensor subclasses where no storage exists. * For deepcopying, `Tensor.__deepcopy__` is expanded to call `clone()` for cases where no storage exists. The goal is to provide reasonable default implementations usable for cases where a custom tensor type uses `_make_wrapper_subclass()` and at most sets some additional copyable attributes. If anything more complex is needed, users are expected to write their own `__deepcopy__` / `__reduce_ex__` implementations for their type. Note that custom implementations for the above magic methods MUST handle copying the `_is_param` attribute that may be present on the custom tensor ty",2022-02-18T15:32:01Z,cla signed release notes: python_frontend topic: bug fixes,closed,0,9,https://github.com/pytorch/pytorch/issues/73078,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73078**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit fa2325fc07 (more details on the Dr. CI page):  * **4/4** failures introduced in this PR   :detective: 4 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5321769366?check_suite_focus=true) linuxxenialcuda11.3py3.7gcc7 / test (default, 2, 2, linux.4xlarge.nvidia.gpu) (1/4) **Step:** ""Unknown"" (full log  :repeat: rerun)   20220224T18:14:55.0770319Z   test_add_done_ca...arg() takes 0 positional arguments but 1 was given     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","> I think we will want to extend this testing to cover more common subclass implementation. But this PR seems to be a step in the right direction. True, the custom tensors as parameters PR I'm working on has much more extensive tests run across currently 5 different subclass impls.", merge this please,Merge failed due to Command `git C /home/runner/work/pytorch/pytorch cherrypick x 3fed15854337d0dd5b8c7980ee41e82ca7a32985` returned nonzero exit code 1 Automerging test/test_python_dispatch.py Automerging torch/_tensor.py CONFLICT (content): Merge conflict in torch/_tensor.py Raised by https://github.com/pytorch/pytorch/actions/runs/1893983006, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Rename `Typed/UntypedStorage` to `_Typed/_UntypedStorage` (#72540),"Cherrypicking https://github.com/pytorch/pytorch/pull/72540 for 1.11 release Rename UntypedStorage and TypedStorage to _UntypedStorage and _TypedStorage. At this point, users do not need to interact directly with these classes, and they are not documented yet. (cherry picked from commit 329238f612a9d92586bb0e5b33bcc45a0ec6936b)",2022-02-16T14:58:12Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/72914,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72914**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit bc0f664f72 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5218316415?check_suite_focus=true) linuxbionicpy3.7clang9 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220216T15:26:10.3655393Z AttributeError: module 'torch' has no attribute 'complex32'     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,DISABLED test_transformer_module_apply (__main__.TestApply),"Platforms: linux     This test was disabled because it is failing in CI. See recent examples and the most recent     workflow logs.     Over the past 3 hours, it has been determined flaky in 1 workflow(s) with     2 red and 2 green. ",2022-02-16T09:39:54Z,oncall: distributed triaged module: flaky-tests skipped module: fsdp,closed,0,4,https://github.com/pytorch/pytorch/issues/72908,"Note that the last few pytorch bot comments refer the the same instance. This was due to a mistake in my query, which should be fixed by https://github.com/pytorch/testinfra/pull/200"," For oncall: distributed issues could we avoid adding triaged label, to make sure we discuss it during our distributed oncall meeting? Thank you! ",Ah okay. I will remove the bot functionality to add triaged as well for any of the labels as well.,"Cannot reproduce, possibly should have been fixed by https://github.com/pytorch/pytorch/pull/73314"
yi,Type signature for tools.codegen.api.lazy.isValueType is a bit suspect," ðŸ› Describe the bug Current implementation looks like this:  There are two problems: 1. Each CType is listed out explicitly. This is bad because if a new CType is added this site has to be manually updated. Instead, the CType union should have been used instead. This should be an easy fix and I can submit the patch for it. 2. This function is overloaded; it takes both Type and CType, which are semantically different concepts. To make matters worse, a it is never possible for a Type to return true. Calling isValueType on a Type is a code smell; it shouldn't be permitted in the signature at all.    Versions master",2022-02-15T14:20:26Z,triaged module: codegen,closed,0,0,https://github.com/pytorch/pytorch/issues/72852
yi,[DataPipe] Improve .pyi generation,Stack from ghstack:  CC([DataPipe] Improve .pyi generation) Make two functions more flexible and usable from a different repo. Differential Revision: D34227912,2022-02-15T00:03:14Z,module: data cla signed release notes: dataloader,closed,0,4,https://github.com/pytorch/pytorch/issues/72829,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72829**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 49e7c71b33 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Rename undocumented storage types,"Rename `UntypedStorage` and `TypedStorage` to `_UntypedStorage` and `_TypedStorage`. At this point, users do not need to interact directly with these classes, and they are not documented yet.",2022-02-14T18:28:53Z,open source cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/72802,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72802**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 4974722d7c (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5188999970?check_suite_focus=true) linuxbionicpy3.7clang9 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
yi,Update lazy_ir.py from lazy_tensor_staging,"Summary: This diff contains changes from several PRs landed to lazy_tensor_staging branch.  generating 'fallback' overrides for each codegenned op, useful for debugging  supports operators which are missing aten:: symbols for op names, instead using their string counterpart  makes the IR class a base class instead of hardcoding the assumption of TS Test Plan: tested on lazy_tensor_staging branch Differential Revision: D34178476",2022-02-11T19:19:54Z,fb-exported cla signed Reverted topic: not user facing release notes: lazy,closed,0,7,https://github.com/pytorch/pytorch/issues/72730,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72730**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 0e717a8787 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   1 failure *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34178476,This pull request was **exported** from Phabricator. Differential Revision: D34178476,This pull request was **exported** from Phabricator. Differential Revision: D34178476,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This pull request has been **reverted** by 889f3f48b2ef0fb27cff5a1a474d7c316fd7b5d4. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).","This pull request has been **reverted** by 889f3f48b2ef0fb27cff5a1a474d7c316fd7b5d4. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk)."
transformer,[Vulkan] Implement GRU operator,"Summary: Implemented GRU operator in the Vulkan GPU backend: * This is an initial implementation to support an internal model. * Internal name for GRU is `aten::gru.input` * There should be 2 weights and 2 biases per layer. See GRU >> Variables section * For num_layers=1 the weights should contain [weight_ih, weight_hh, bias_ih, bias_hh] (4 elements) * Need to reshape input and hidden state to 2D since Vulkan `mm` and `addmm` ops accept only 2D dim * By design, all weights and biases should be on the CPU where input sequence and hidden state should be on the Vulkan GPU. * Input arguments and return values:     * `input_vk`: input tensor of shape (L, N, H_in) when batch_first=False or (N, L, H_in) when batch_first=True containing the features of the input sequence     * `hx_vk`: initial hidden state for each element in the batch. tensor of shape (D * num_layers, N, H_out)     * `output`: tensor of shape (N, L, D * H_out)) when batch_first=True     * `h_n`: tensor of shape (D * num_layers, N, H_out)     * where         * L = sequence length         * N = batch size         * D = 2 if bidirectional=True otherwise 1         * H_in = input_size ( of expe",2022-02-11T01:34:40Z,fb-exported cla signed release notes: vulkan topic: new features,closed,0,3,https://github.com/pytorch/pytorch/issues/72692,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72692**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 8d4de67255 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33995221,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,[tools_common] Don't remove underscores from call_module targets in get_acc_ops_name,Test Plan: CI. Reviewed By: wushirong Differential Revision: D34148357,2022-02-10T18:49:03Z,fb-exported cla signed module: fx release notes: fx topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/72664,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72664**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 5f2a0d2946 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34148357,This pull request was **exported** from Phabricator. Differential Revision: D34148357,This pull request was **exported** from Phabricator. Differential Revision: D34148357,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,upstream `apex.normalization.FusedRMSNorm`," ðŸš€ The feature, motivation and pitch All T5 models and their derivatives (t5, mt5, t0, etc.) use `RMSNorm`, instead of `LayerNorm`. The former is a subset of the latter, it only scales and doesn't shift. The original need was a discovery that all HF Transformers t5based models were somewhat slow under mixed precision, because of ""manual"" implementation of `T5LayerNorm` where manual up/down casting was causing a significant bottleneck. While researching this I have run into other users who wanted to use a fast `RMSNorm` (but didn't save the references) NVIDIA/apex recently implemented `apex.normalization.FusedRMSNorm` but building apex is far from easy for a lay person.  I have benchmarked it in an ensemble and it gives a pretty significant gain  about 10% improvement on the full backtoback application. https://github.com/huggingface/transformers/pull/14656  so clearly multiple times faster on just the norm part. So to ease user's path to faster t5based models if possible it'd be great to have this subset functionality of `LayerNorm` available in pytorch. It's already in the nvfused branch: https://github.com/csarofeen/pytorch/pull/1428 I will see if",2022-02-10T01:41:39Z,feature module: nn triaged module: norms and normalization,open,12,42,https://github.com/pytorch/pytorch/issues/72643,"(if it's a variant of LayerNorm, should it be supported by LayerNorm natively, e.g. if weight is not None and bias is None, it should call this new fused kernel?)","From https://arxiv.org/abs/1910.07467 > When the mean of summed inputs is zero, RMSNorm is exactly equal to LayerNorm.  !snapshot_10 So it's the same as LayerNorm, but: 1. mean is always 0. 2. no bias  only weight  which is how it's done in T5  perhaps biasenabling should be configurable?","> if it's a variant of LayerNorm, should it be supported by LayerNorm natively, e.g. if weight is not None and bias is None, it should call this new fused kernel?  the problem is that you also want to ignore the average term (E[x]) there? So that would need to be a new boolean flag to enable that. Not sure if that is better than adding a new `RMSNorm` function.","It'd be awesome to have this available in PyTorch!! It was used again, in this big paper: https://arxiv.org/pdf/2112.11446.pdf","Another big model using it: https://arxiv.org/abs/2302.13971   I can help add it if necessary, but in terms of design choices which is better:   adding boolean flag to enable them in LayerNorm?   Creating a new `RMSNorm` class?  Also I've noticed that there seems to be a test for a fused version: https://github.com/pytorch/pytorch/blame/5dd52e250f66a5e3377eb39228cd929871f1eb5d/test/functorch/test_memory_efficient_fusion.pyL155","Hey! Sorry for the delay in answering, it is a tricky one. Given the similarity with LayerNorm I think we want to move forward as follows:  Add a `no_bias` kwarg to LayerNorm which can be used to enable this behavior.  Make sure the doc for this arg mentions RMSNorm explicitly so that it is discoverable by that name.  Using torch.compile should gives you the same performance as the fused implementation from apex. So we don't want to add the fused implementation in PyTorch.","> Using torch.compile should gives you the same performance as the fused implementation from apex. So we don't want to add the fused implementation in PyTorch. `torch.compile` is very very very very far from being ready for general use. Most of my attempts at using it failed. You can see multiple bug reports I have filed about it. It only works in very specific situations. But if this request is put on hold for another year it might come through. Which is fine too, since we do have `apex`. It's just a big pain to install.","`torch.compile` along with `JIT` before it has historically not worked well for NLP models which tend to be more dynamic. It'd be nice to prioritize some core development until compilers like that are stabilized.  tbh, if I could just use `torch.compile` for oneoff modules, that'd be sick. It feels like overkill to try to compile an entire model. ","Just fyi, RMSNorm isn't just LayerNorm without bias (otherwise we could have use the functional method that allows to not pass bias). You need also to remove mean estimation. Totally fine with the plan to use `torch.compile` as the main way forward. I'll just use `apex` version for now.", (also `bias=False` naming is probably better (to match nn.Linear's arguments' naming),"yes `bias` is not great as it is usually just the bias weight but here it also means the centering is also removed (you don't remove the average bias). Do you think `rms_only=True` would be a better name? > tbh, if I could just use torch.compile for oneoff modules, that'd be sick. It feels like overkill to try to compile an entire model.  this is the whole point of torch.compile (and the big difference with existing jit in ML frameworks) it is designed to work with partial graphs and small pieces! What I heard from  is that in the case of RMSNorm, torch.compile is able to generate a good compiled version. So wrapping just that one layer implementing it will give you a fused version.","Thank you for clarifying that one doesn't have to compile the whole model to fuse just one component, albanD. That's great! I disagree that this should be left to users. You want pytorch to be the winning framework, correct? Make it great out of the box. Expecting users to figure out that they need to `compile` `RMSNorm` to make it faster is not a great strategy, IMHO. If it works why not `compile` it by default then and give an option to opt out?","I tend to agree with  as pytorch core/domain libraries is an important source of idioms that are adopted by the community. So if RmsNorm module can be implemented trivially in core by torch.compiling around simple impl, it's great to have it in core + tests + perf tests. Then this could also be a showcase/doc reference of usecase where torch.compile works great (kind of dogfooding).","Yeah, if compiling works, why not offer outofthebox PyTorch modules that have been compiled together? Especially, modules that are used in stateoftheart models.  Most stateoftheart models cannot be built in PyTorch because they require many layers that are not readily available and are hard to implement. The community is needing to use libraries like `apex` which are hard to use. "," the problem is that we need to do a zerotoone of having torch.compile inside regular torch library code. This is a reasonable thing to want to do, but there hasn't really been any emphasis on it (since most of the effort has been on torch.compile with full libraries). Because there is no emphasis on this style of use case, there are big perf gaps (e.g., guard evaluation overhead matters a lot more in this regime). So to make your suggestion into reality, we need to also spend some time making this work well. Or we just enlist the OSS community's help in just adding the FusedRMSNorm into the framework directly and kick the can a few more months.","> I disagree that this should be left to users. You want pytorch to be the winning framework, correct? Make it great out of the box. This is definitely the long term vision where compile will be always there, optimizing what it can and leaving the rest as python code. In that world, users won't have to figure out they need compile. They just always use it and will get the perf out of the box. Right now we're in a weird transition period though as, as you said, this is not stable enough to be in this alwayson state. And so the question is still open on how to best spend our ressources: add more fused oneoff kernels that will be obsolete when compile is the default or work on making compile the default. I think our current stance is to still add these fused kernels for things that are critical right now or that compile will not handle well any time soon (SDPA, adamw for example) and focus the rest of our efforts to making compile the default. To come back to this particular issue, adding the new flag to be able to do RMSNorm is a definite ""yes we want it"" but adding the fused implementation is more ""we would accept a simple PR but no one on the core team is working on it"".","I think, having certain things in core that are torch.compile'd demonstrates to the users maturity of the technology, so it's a good goal to have by itself. Plus, in core you could have automated perf tests comparing it against legacy apex fused kernels. If wanted, this stuff could go into some separate experimental pytorch package (from where kernels can graduate into core). As long as it's built/released/tested along with the rest of pytorch, it's already strictly better than apex. Once pytorch core has torch.compile'd kernels, it will show true commitment to the technology","If things are unstable, yet, it'd be wasteful to allocate resources to do such porting because an automatic solution is ""imminent"", then I think it's perfectly fine to close this feature request and tell the user to use `apex` if they want speed  a user who can figure out how to compile `RMSNorm` will surely be able to build `apex`, so I don't really see any advantage here if it's not something available out of the box.","We **unify** the `LayerNorm` and `RMSNorm` in PreNormalization Transformers in our paper https://arxiv.org/abs/2305.14858. The arithmetic equivalence allows us to convert PreLayerNorm Transformers into PreRMSNorm models without impact on the model functionality. Considering that RMSNorm offers superior efficiency compared to LayerNorm in theory, we believe that providing an official RMSNorm API would greatly benefit the community, allowing them to harness this improvement in both training and inference effectively. We also release our implementation https://github.com/ZixuanJiang/prermsnormtransformer for reference. Thanks for your consideration.","Following up on the discussion here, from further discussion, it seems like `LayerNorm` is given by  $y = \\frac{x  \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$ where the $\\epsilon$ is added for numerical stability and `RMSNorm` is given by  $y = \\frac{x}{ RMS(x)} * \\gamma$ (with maybe an optional $+  \\beta$ per this commentissuecomment1035594258))  Where  $Var(x) = \\frac{1}{n}\\sum\\limits_{i=1}^{n}  (x_i  E[x])^2$ and  $RMS(x) =  \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}   (x_i)^2}$ ($\\epsilon$ could also be used for numerical stability in this calculation) so `RMSNorm` defers from `LayerNorm` on 3 counts  1) don't subtract `E[x]` from the numerator 2) use `RMS(x)` rather than $\\sqrt{Var(x) + \\epsilon}$ in the denominator 3) (perhaps) don't learn an elementwise affine bias  If my understanding here is correct, we would accept the addition of `RMSNorm` as a new `torch.nn.Module` that is separate from `LayerNorm`",It also seems that there's a RMSNorm impl in flashattention (fused with dropout): https://github.com/DaoAILab/flashattention/blob/4f285b354796fb17df8636485b9a04df3ebbb7dc/flash_attn/ops/rms_norm.pyL11,"Am I also understanding correctly that semantically `RMSNorm(x) := F.normalize(x, dim = dim, p = 2, eps = eps) * sqrt(D) * gamma`? modulo different treatment of eps (F.normalize clamps the denom by eps from below, and rmsnorm adds eps to the underthesqrt expression)",There is also an impl of RMSNorm in Triton at https://github.com/kakaobrain/trident/blob/main/trident/kernel/rms_norm.py  maybe can be incorporated into core?,+1,Any progress on this issue ? cc:  ,as a matter of fact the rmsnorm from FasterTransformer is much faster than Apex' https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/kernels/layernorm_kernels.cu,"Could you please share some factual information to support this claim, ?", I can try to make a unitary snippet but can tell you in this PR https://github.com/OpenNMT/OpenNMTpy/pull/2539/files it had a huge impact on inference tok/sec for a Mistral7B LM for instance. (separately from the other change kv_cache from flash2 that had also an impact).,Related:    CC(Compiling RMSNorm Triton Kernal gives error),Should probably be closed now
transformer,Add naive native FFN in aten,"Summary: This is an alias of feed forward network in transformer. We would like to use this API in the prototype of the new transformer encoder. Notice this diff might not bring any perf improvement since it is just a combination of aten ops, but we can improve its kernels later. fp16 is not supported in this version, due to the poor support of Half type in current aten::baddbmm. We should improve this in our future implemented fast FFN kernel. Test Plan: buck build mode/opt c fbcode.enable_gpu_sections=true caffe2/test:nn && buckout/gen/caffe2/test/nn\\binary.par r test_ffn Differential Revision: D33800859",2022-02-09T00:57:07Z,fb-exported cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/72564,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72564**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit d9bdecd5a8 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33800859
rag,add reduce package function to storage.py in order to deperecate presistent_id in OSS,Stack from ghstack:  CC(remove torch from package importer and add fbcode shims for BC)  CC(remove torch from package exporter)  CC(add reduce package function to storage.py in order to deperecate presistent_id in OSS)  CC([pkg] add generic ZipFile Reader/Writer) This PR also adds changes in order to allow for torchscript model serialization without importing torch into torch.package.PackageImporter or torch.package.PackageExporter. This allows us to remove the dependency of torch in torch.package.PackageImporter or torch.package.PackageExporter in the PRs on top of this. Differential Revision: D34096804,2022-02-09T00:47:16Z,oncall: jit cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/72563,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72563**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 0d528f14a9 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   1 failure *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
yi,Allow specifying tags for aten operators in native_functions.yaml,"Stack from ghstack:  CC(Allow specifying tags for aten operators in native_functions.yaml) This PR: 1. Adds a new yaml file. For each new tag, we will require an entry in this file along with a mandatory description. 2. There are checks to ensure that any tags added for native_functions are valid tags (i.e., they exist in tags.yaml) 3. Each NativeFunction object holds tags as a set of strings (this logic can be found in model.py) Most file changes are to update the call to `parse_native_yaml` to include the path to `native_functions.yaml` as well as `tags.yaml`",2022-02-08T22:27:12Z,cla signed Reverted release notes: composability,closed,0,11,https://github.com/pytorch/pytorch/issues/72549,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72549**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit bc975370b0 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5697626730?check_suite_focus=true) trunk / linuxbionicrocm4.5py3.7distributed / test (distributed, 1, 1, linux.rocm.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220325T21:06:01.8892354Z AssertionError: Losses differ between local optimizer and ZeRO     This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","oops, didn't see not for review. Unsubscribing for now, please re","These tags still aren't exposed from Python, is that right? It would be nice to parse in the descriptions so that we can then generate appropriate `__doc__` on the Python side representation (seems better than slinging strings around in Python).","> These tags still aren't exposed from Python, is that right? It would be nice to parse in the descriptions so that we can then generate appropriate `__doc__` on the Python side representation (seems better than slinging strings around in Python). No they aren't yet but the next PR in the stack exposes the tags through the torch.ops API.", merge this please, merge this please,Merge failed due to PR 72549 does not match merge rules Raised by https://github.com/pytorch/pytorch/actions/runs/2041367071, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert this,Reverting as this change broke internal build systems. Significant build system changes still have to go thru the old land path
rag,Rename `Typed/UntypedStorage` to `_Typed/_UntypedStorage`,"Rename `UntypedStorage` and `TypedStorage` to `_UntypedStorage` and `_TypedStorage`. At this point, users do not need to interact directly with these classes, and they are not documented yet.",2022-02-08T19:41:55Z,module: internals triaged open source cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/72540,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72540**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit f0a7f47da2 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"> I am wondering about BC behavior of this renaming. Does this mean that any storage (and thus Tensor) saved before this PR won't be able to be loaded after this PR? Before this PR, when a `TypedStorage` is saved, it's saved in a backward and forward compatible format (assuming the two versions of pytorch support all the same dtypes) so that old versions of pytorch can load it into the corresponding `Storage` class. For the original PR CC([pytorch][PR] Reimplement Python storages as wrappers around an untyped storage), I did some BC/FC testing with this script to make sure of that: https://github.com/kurtamohler/pytorchperftestscripts/blob/master/untypedStorage/serialization_compat_test.py I just reran it now on this PR, and there do appear to be a couple small issues. I'll see what's going on and fix it","> I just reran it now on this PR, and there do appear to be a couple small issues. I'll see what's going on and fix it Turns out that these were just issues with my script, and I've fixed them. I had to make it recognize the `_TypedStorage` name change. Also, the test case for JIT serialization of the `nn.Linear` module was failing because the JIT code that it compiles into has changed slightly since 1.10, a change unrelated to `TypedStorage`. I decided to just use `nn.Conv` instead. I ran the script using the `save` option in pytorch 1.10, then ran it with the `load` option in pytorch 1.10, the current master branch, and the branch for this PR to check that saving from pytorch 1.10 is compatible with all three of these different versions of python. These all passed. I also saved from the master branch and this PR's branch, and those were also fully compatible. So it looks like changing `TypedStorage` to `_TypedStorage` won't affect serialization BC/FC, at least not for all the cases that my script checks"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
finetuning,[FSDP][BE] Remove multiple param logic in FSDP code," ðŸš€ The feature, motivation and pitch Currently we only shard based on flat_param, eventually once it is decided that FSDP will not support nonflat param based training, we should remove/simplify all code such as https://github.com/pytorch/pytorch/blob/master/torch/distributed/fsdp/fully_sharded_data_parallel.pyL275 that assumes that there might be multiple parameters.  We can probably do this in the future once we've investigated all various use cases such as finetuning and are confident that we won't need non flat param approach, simplifying the code base by quite a bit.   Alternatives _No response_  Additional context _No response_ ",2022-02-08T15:54:38Z,oncall: distributed triaged better-engineering module: fsdp,closed,1,1,https://github.com/pytorch/pytorch/issues/72520,"Instead, we are refactoring to readd this logic because we want to support nonrecursive wrapping / functionallike. The only difference is that we should do everything in terms of `FlatParamHandle` instead of `FlatParameter`."
yi,Added missing antialias argument to functional.pyi.in,Description:  Added missing antialias argument to functional.pyi.in  mypy is happy if checking `interpolate` method with antialias argument Related torchvision issue: https://github.com/pytorch/vision/pull/5329,2022-02-07T11:57:59Z,open source cla signed Reverted release notes: nn topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/72420,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72420**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit d09f662f38 (more details on the Dr. CI page):  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base 4eb277ac61 from Feb 05 until Feb 07   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5093608423?check_suite_focus=true) winvs2019cuda11.3py3 / test (force_on_cpu, 1, 1, windows.4xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220207T15:17:31.2425017Z RuntimeError: test_spectral_ops failed!      :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * [linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) from Feb 05 until Feb 07 (1edf6f5647  d8c3ab11ae)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey 5. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This pull request has been **reverted** by 127bf42ee7a7a1a50473b0b7e17ceb69ee5ebb7f. To reland this change, follow these steps.", do you know why this PR is reverted ?,It was though to be the root cause of some CI issue. But it was not. So the revert has been reverted. This is (should be) in master again.,"This pull request has been **reverted** by 127bf42ee7a7a1a50473b0b7e17ceb69ee5ebb7f. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk)."
yi,Make cholesky_inverse documentation mention that batching is supported," ðŸš€ The feature, motivation and pitch Dear PyTorch developers, hi, I am a ML researcher and a PyTorch programmer working heavily in the prediction of Gaussian distribution with full coverariance matrix. In my work, I need to use NN to predict the cholesky decomposition of the covariance matrix, and then operate it quite often.  One of my operation is to compute the inverse of the covariance matrix in a batched manner. I found an implementation using the cholesky, which makes me excited. But later I found it only supports 2d input, so I would like to ask a new feature to make it support batched input [..., 2d].  Thanks a lot in advance. Best, Ge Li  Alternatives _No response_  Additional context _No response_ ",2022-02-04T14:39:04Z,module: docs triaged enhancement module: linear algebra,closed,0,5,https://github.com/pytorch/pytorch/issues/72334,"Even though we should mentoin this somewhere, you can imitate cholesky inv and cholesky solve via  `linalg.solve_triangular`. ~~This is because `LL^TX = B` can be solved via two `solve_triangular`s, one with `L` and the other one with `L^T`. Set `B=Id` and you have the inverse.~~ Even better, you can do `X = L^{1} = solve_triangular(L, Id)` and then your inverse is given by `X^T`. As always, I'd very much recommend you not to use the inverse and then multiply, but doing solves agianst hte matrix that you want to multiply to later on. ",Both `torch.cholesky_inverse` and `torch.cholesky_solve` support batched inputs and they use the same backend function so their performance should roughly be the same. It's an oversight in the documentation that it specifies only 2D inputs. ,"Hi, thanks a lot for your answer and support.  I did not know these cholesky's functions by heart before.  Maybe one other question: Do the cholesky_inverse and cholesky_solve functions offer same numerical stabilities?  We know that normally doing the solve() instead of inverse() can offer better numerical stabilities. ","As Lezcano mentioned above if the intention is to multiply the inverse by some other matrix, often it should be more efficient to use the solve function to do that. Internally, `torch.cholesky_inverse(L)` computes the result by calling `torch.cholesky_solve(eye(L.shape[0]), L)`. Leaving performance aside, I don't know whether the accuracy would differ much. This answer on Math.StackExchange suggests that the forward error of using the inverse explicitly or solving the linear system is about the same.","Thanks again for your answer, I have no more doubt. Please feel free to close this issue, or maybe after the documentation has been updated.  :+1:  Best, Ge Li"
transformer,Transformer Initialization,"While you took care of this in the tutorial on Transformers and `nn.Transformer`. I just used `nn.TransformerEncoder` and realized that this won't initialize parameters in a sensible way on its own. One would create an encoder like this:  This will yield a Transformer that has the same initialization in all layers, which I think rarely is what you want in neural networks, so it is unexpected. You always need to initialize from the outside again. This is not the usual case in PyTorch and not documented, see https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.htmltorch.nn.TransformerEncoder I am not sure which way to better resolve this, through a changed `nn.TransformerEncoder` API, just documentation or whether you are working on a rewrite anyways already. Thus, I raised this as an issue first.  Versions This is a problem in the current version (1.10). This bug does not need to be reproduced, but can be seen relatively easy from the source code of `nn.TransformerEncoder`. ",2022-02-03T10:08:48Z,high priority module: docs module: nn triaged actionable,open,8,4,https://github.com/pytorch/pytorch/issues/72253,We would accept a patch that fixes this. I suspect that making it conform to the initialization requirements of the rest of the library is what makes the most sense.,"This occurs because `TransformerEncoder` / `TransformerDecoder` have the unfortunate distinction of being the only modules that accept a sublayer and **clone** it N times. In contrast, the RNN modules create the N layers internally and initialize their weights from the same distribution, but not with the exact same values. It would be BCbreaking to change this behavior, and there is a longerterm redesign in progress already. For now, we'd accept a PR adding a warning to the docs indicating that explicit manual initialization is recommended.",Was digging through an opensource VALLE implementation and found this exact bug had gone unnoticed. I think it really deserves a loud warning in the docs at the very least (short of a redesign). ,bumping priority to update the doc
yi,RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward," ðŸ› Describe the bug I am reopening the issue  CC(RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.) with the following additional informations: When training a network which contains parametrization in DP (Distributed Parallel) and the caching mode (torch.nn.utils.parametrize.cached), a race condition occurs at a random point in the training causing the error: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. Anomaly detection points to the following culprit:  The training step is defined as follow:  I suspect",2022-01-29T19:25:30Z,module: autograd triaged module: nn.utils.parametrize,open,0,2,https://github.com/pytorch/pytorch/issues/72041,"As mentioned in the previous PR, a minimal repro would certainly help, as the current code is too complex to reason about.","That being said, I do think that the multiprocessing + caching may not work well.  "
yi,[warnings][caffe2] Fix asserts yielding -Wstring-conversion warnings,"Summary: Find and replace `assert(!""` with `assert(false && ""` Excludes headers and paths that contain ""thirdparty"" or ""external"" Clang raises a `Wstringconversion` warning when treating a string as a boolean.  This is not uncommon for asserts though (e.g. `assert(!""should never happen"")`).  Clang does permit `expr && ""string""` though in order to support these assertion use cases. Test Plan: ci pass Differential Revision: D33823092",2022-01-28T19:13:27Z,oncall: jit fb-exported cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/72013,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72013**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 4d683ba99f (more details on the Dr. CI page):  * **1/1** failures possibly\\* introduced in this PR     * **1/1** nonscanned failure(s)   ci.pytorch.org: 1 failed * **Failed:** `pr/pytorchlinuxbionicrocm4.5py3.7`  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33823092
yi,torch/monitor: update pyi definitions,Summary: This updates the .pyi definitions to match the pybind interfaces. Test Plan:  CI Differential Revision: D33830311,2022-01-27T23:06:17Z,fb-exported cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/71950,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71950**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 60c539dc42 (more details on the Dr. CI page):  * **5/10** failures introduced in this PR * **5/10** broken upstream at merge base 0c3bc426a8 on Jan 28 from  8:59am to 11:43am   :detective: 5 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/4985325785?check_suite_focus=true) winvs2019cpupy3 / test (default, 1, 2, windows.4xlarge) (1/5) **Step:** ""Test"" (full log  :repeat: rerun)   20220128T19:32:36.8027657Z RuntimeError: test_ops failed!      :construction: 5 fixed upstream failures: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * [linuxxenialpy3.7clang7onnx / test (default, 2, 2, linux.2xlarge) on Jan 28 from 10:20am to 11:05am (c5df294940  c5df294940)     * :repeat: rerun * linuxxenialpy3.7gcc7 / test (default, 1, 2, linux.2xlarge) on Jan 28 from  8:59am to 11:27am (f499ab9cef  cb823d9f07)     * :repeat: rerun * linuxbionicpy3.7clang9 / test (default, 1, 2, linux.2xlarge) on Jan 28 from  8:59am to 11:27am (f499ab9cef  cb823d9f07)     * :repeat: rerun * linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) on Jan 28 from 10:20am to 11:43am (c5df294940  765669e1b9)     * :repeat: rerun * linuxxenialpy3.7gcc5.4 / test (default, 1, 2, linux.2xlarge) on Jan 28 from  8:59am to 11:27am (f499ab9cef  cb823d9f07)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D33830311,This pull request was **exported** from Phabricator. Differential Revision: D33830311,This pull request was **exported** from Phabricator. Differential Revision: D33830311,This pull request was **exported** from Phabricator. Differential Revision: D33830311,Try rebasing on latest master to make the build errors go away.,This pull request was **exported** from Phabricator. Differential Revision: D33830311,"* linuxbionicpy3.7clang9 / test (xla, 1, 1, linux.2xlarge) (1/2) * linuxbionicpy3.7clang9 / test (noarch, 1, 1, linux.2xlarge) (2/2) are broken in trunk and unrelated to this PR"
yi,Fix hardcoded `TsNode` in `lazy_ir.py` ,"There are a number of places where TorchScript specific classes are hardcoded into the lazy tensor autogen code. Fixing those instances to make them more general.  There is a hardcoded instance of `TsNode` in `tools/codegen/dest/lazy_ir.py`. Fixing that to be `self.node_base` instead.  There are hardocded instances of `TSOpVector` and `TSLoweringContext` as well which were replaced by overridable fields in the dataclass. With these changes, a custom `LazyIR` subclass can be passed into the `gen_lazy_tensor.run` to custom the autogen for lazy tensors. CC:  ",2022-01-27T16:59:22Z,open source cla signed,closed,8,7,https://github.com/pytorch/pytorch/issues/71921,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71921**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit e69285a0a1 (more details on the Dr. CI page):  * **9/9** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/4987509119?check_suite_focus=true) linuxbionicpy3.7clang9 / test (default, 2, 2, linux.2xlarge) (1/7) **Step:** ""Test"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!, Another gentle reminder to please review this at your earliest convenience,"> The original plan was to use this infra to develop wellsupported bridges into TS and XLA. (Possibly also MLIR) but not use it as a point for each vendor to integrate to directly. That's more than fine. I was actually planning on using this change to do the autogen for torchmlir. I wanted to reuse as much of the existing autogen stuff as possible. Hence the reason for this PR. > We'll approve this, but wanted to point out that the codegen infra isn't a stable API, and we still plan on changing it some to accommodate improvements to LTC.  Understood. Though, any future changes I think should be made more general and not reference anything specific like it is now with TS. > Do you think that longer term you would be able to share an integration point with one of the common IRs, or you think you really want to have a direct integration here for various reasons? The plan is to share an integration point with torchmlir. But as mentioned above, to use the autogen for torchmlir, the changes in this PR removing the TS hardcoded elements are required.",> future changes I think should be made more general and not reference anything specific like it is now with TS yea agreed. OK sounds good.  thanks for clarifying!, What are the next steps to getting this merged? I don't have write access and can't merge it myself
rag,decouple storage and serializer from exporter,"Stack from ghstack:  CC(add zip reader)  CC(remove importer's reliance on load_tensor, break bc on map_location by using None)  CC(decouple storage and serializer from exporter)",2022-01-26T19:10:57Z,oncall: jit cla signed Stale,closed,0,2,https://github.com/pytorch/pytorch/issues/71873,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71873**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit df365a75a5 (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   2 failures *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Regression in multi-node training speed with Transformers + PyTorch," ðŸ› Describe the bug We have seen a regression in multinode training speed with the Huggingface Transformers CLM test case happening at PyTorch commit 38ac9e6 which reverts commit 3957ed4 ""[DDP] Disable reducer hooks from running outside of DDP backwards"". Between those two commits I get around 28 `train_samples_per_second` (as reported by the CLM test) using two nodes with 4xNVIDIA A100 GPUs each. Afterwards speed drops to around 10 `train_samples_per_second`. This can be reliably reproduced every time. My testing script can be found here, although it's a bit difficult to reproduce at it requires building the singularity containers and a cluster with at least two nodes: https://github.com/mvsjober/pytorchclmtestcase/blob/main/pytorch_clm_test.sh The PyTorch container has been created with this build recipe. It's a CentosOS 7.7 image with CUDA 11.1, cuDNN 8.1.0, NCCL 2.8.4.  Versions Collecting environment information... PyTorch version: 1.10.0a0+git38ac9e6 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.12) Clang version: Could no",2022-01-26T12:57:06Z,high priority triage review oncall: distributed triaged module: regression module: ddp,open,0,3,https://github.com/pytorch/pytorch/issues/71855,Adding high priority due to perf regression ," is the regression still happening? also wondering what is the QPS before commit 3957ed4?  If 3957ed4 does help the QPS improvement, that mostly means this commit disabled the grad sync when directly using the local module wrapped by DDP, is that what you want in the application? "," Yes, regression is still happening. I just ran my test with the most recent release and I got around 8 `train_samples_per_second` and I also tried to most recent commit (which was c6f1bbc) and I got around 13. Compared to around 28 after commit 3957ed4. And yes, before that ""magic"" commit it was also low. According to my notes with the previous commit 5a4282d it was 10.36. I'm not sure regarding the grad sync, the application is the Huggingface Transformers CLM test case so perhaps this should be brought up with them?"
rag,Remove state_dict from AveragedModel and use buffers instead,Fixes  CC(Improve use_state_dict in AveragedModel)),2022-01-25T14:08:35Z,module: optimizer cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/71763,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71763**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit dab9741598 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
rag,[acc_tracer] Add test coverage for retracing,"Summary: Added coverage for reshape specifically which required a fix. The problem for `acc_ops.reshape` as best as I understand:  `torch.reshape` requires the `shape` arg to be a `tuple` of `ints`  If `torch.reshape` is passed a `tuple` where the first element is not an `int`, it throws a TypeError e.g. `TypeError: reshape(): argument 'shape' (position 2) must be tuple of ints, not tuple`   If the `shape` we're reshaping to is an FX Proxy then this type error will be thrown. This happens when the first element of the `shape` tuple is a Proxy because it's inputdependent.  As a workaround we use `tensor.reshape` instead of `torch.reshape`, which doesn't do equivalent type checking for a `tuple` of `ints`. Also remove unnecessary `acc_utils.get_field_from_acc_out_ty()` with cast to `TensorMetadata`. Test Plan: Added test coverage Differential Revision: D33760455",2022-01-25T05:32:05Z,fb-exported cla signed module: fx,closed,0,6,https://github.com/pytorch/pytorch/issues/71752,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71752**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 905952fc4f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455
rag,add move constructor for serialized storage context,  CC([torch::deploy] remove asserts from deploy)  CC(add move constructor for serialized storage context)  CC([pkg] detect bad packages while interning),2022-01-24T23:52:42Z,oncall: jit cla signed Stale,closed,0,2,https://github.com/pytorch/pytorch/issues/71731,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71731**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 2e34756dfc (more details on the Dr. CI page):  * **7/8** failures introduced in this PR * **1/8** broken upstream at merge base ada6f3b447 on Jan 24 from  3:26pm to  4:18pm   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5396599235?check_suite_focus=true) Test tools / test (1/1) **Step:** ""Test tools"" (full log    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * Lint / quickchecks on Jan 24 from  3:26pm to  4:18pm (64162588d1  cda6f40151)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llm,[FX] Support call_method in NormalizeArgs," ðŸš€ The feature, motivation and pitch Currently NormalizeArgs only supports normalizing `call_function`s and `call_module`s. Ideally it would support `call_method`s as well. Example code:  Output shows flatten args for `call_method` and `call_function` not normalized with standard FX tracing (all kwargs usage or default values), as expected:  Now when we normalize we see that the `call_function` is normalized with all kwargs and default values, but the `call_method` is not.    Alternatives _No response_  Additional context _No response_",2022-01-24T18:44:37Z,triaged module: fx,open,0,0,https://github.com/pytorch/pytorch/issues/71715
rag,Fix unused variable warning in AveragePool2d,Differential Revision: D33692619,2022-01-20T22:08:42Z,fb-exported cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/71585," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/rbarnes/pytorch/blob/1c85f83b10ca584a68ad09e08eaf15110968a949/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71585**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 348f210429 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33692619,This pull request was **exported** from Phabricator. Differential Revision: D33692619
yi,Allow specifying num_samples to RandomSampler even when replacement=False,"Fixes CC(Allow specifying num_samples to RandomSampler when replacement=False) CC(Allow specifying num_samples to RandomSampler in any case(38032)) Hi, I modified the RandomSampler to satisfy the requirement of CC(Allow specifying num_samples to RandomSampler when replacement=False). I also added and deleted some test cases in the test/test_dataloader.py to match with the new requirement.",2022-01-20T18:49:34Z,triaged open source cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/71568," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/PM25/pytorch/blob/88ede0f27ac90bc51c9dd2cca60e30ee4de90f70/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ","Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71568**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 9ff8361f54 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,"ok, no problem. I have just added the test cases."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",Thanks for the review!
yi,torch.jit.script failed to compile nn.MultiheadAttention when specifying the kdim and vdim parameters.," ðŸ› Describe the bug nn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum code to reproduce the behavior:  Full error stack:   Versions PyTorch version: 1.9.0 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.17 Python version: 3.6.13  (default, Jun  4 2021, 14:25:59)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0142genericx86_64withdebianbustersid Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: GeForce GTX 1080 Ti Nvidia driver version: 460.73.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_tra",2022-01-19T07:52:50Z,oncall: jit,open,0,1,https://github.com/pytorch/pytorch/issues/71470,"I think the issue comes from the following annotation in torch.nn.functional.multi_head_attention_forward https://github.com/pytorch/pytorch/blob/b56ba296b1cb5d65a3fe2e33cc1d910481baa644/torch/nn/functional.pyL5084 When specifying kdim & vdim,  `use_separate_proj_weight` becomes true, and `in_proj_weight` becomes None. Changing `in_proj_weight: Tensor` to `in_proj_weight: Optional[Tensor]`, and add a new assert statement here https://github.com/pytorch/pytorch/blob/b56ba296b1cb5d65a3fe2e33cc1d910481baa644/torch/nn/functional.pyL5231 should fix the issue."
rag,Remove simd qualifier for pragma omp loop in upsample_nearest_op.h,Summary: Fixes  Test Plan: Sandcastle Reviewed By: luciang Differential Revision: D33641869,2022-01-19T01:03:22Z,fb-exported cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/71462," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/rbarnes/pytorch/blob/de3fd864f455569c644a74a4395f26942b3ca859/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71462**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit de3fd864f4 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33641869
rag,is_alias_of changes for storageless tensors,"The is_alias_of() method should check has_storage() before returning true/false for storageless tensors. Also, it should be possible to override the default behavior by making it virtual. Fixes CC(is_alias_of support for storageless tensors)",2022-01-17T06:18:32Z,triaged open source cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/71378," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/sujoysaraswati/pytorch/blob/7f68c29626d171e23eccf2ca9c162c9d41fd9dd6/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71378**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 7f68c29626 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Hey! From more internal discussion on this, I think the main point are:  We should rename is_alias_of to has_same_storage. This is a better description of what it does and ensure its semantic is clear.  The places where this function is used assumes that the given Tensor is a strided Tensor. The rest of the code around it assumes that the Tensor is strided to implement its feature. So allowing this function to be virtual doesn't help if it is not properly looking like a strided Tensor.  In your case, if your Tensor is properly strided, then it should expose all these things properly (and thus wouldn't need it to be virtual).","Hi , in our case we have lazy tensors and they can be strided/view tensors. We do the bookkeeping at our side to create the tensors in the device and run ops with the strided tensor semantics. However, since this is a lazy tensor, there is no storage attached so we would not be able to expose a storage for them and use has_same_storage functionality. This is why we wanted to create it as a virtual so that we can refer to our bookkeeping and return true/false based on whether they are strided or not. Is there anything wrong to keep the strided tensor bookkeeping inside the backend bridge for a lazy tensor? It feels a bit restrictive, as we do create lazy tensors with strided/view semantics, but we can't have an API like is_alias_of that can be overridden and hence we are blocked on DDP usage. > Hey! >  > From more internal discussion on this, I think the main point are: >  > * We should rename is_alias_of to has_same_storage. This is a better description of what it does and ensure its semantic is clear. > * The places where this function is used assumes that the given Tensor is a strided Tensor. The rest of the code around it assumes that the Tensor is strided to implement its feature. So allowing this function to be virtual doesn't help if it is not properly looking like a strided Tensor.  In your case, if your Tensor is properly strided, then it should expose all these things properly (and thus wouldn't need it to be virtual).","For the DDP use in particular my understanding is the following: `is_alias_of` is only one of the functions that is being used there to make their bucketting system work. They also need advanced viewing/striding functions etc. So you do need to provide the full ""strided Tensor API"" to be able to use that feature. The fact that there is a storage, size, stride, etc are just pieces of that API. If you do have all of these, I don't see why you cannot have a Storage object here since you do have one. Note that when I talk about Storage object here, I mean the ""Storage"" objects stored by your TensorImpl. This doesn't have to be the PyTorch vanilla Storage though. Advanced features like Functionalization have a special Storage class that they set on their custom TensorImpl. In your case, you can have a LazyStorage object that will allow you to use all the Tensor methods asis without issues."," I want to connect you with  from our side working on DDP design for lazy tensor core.  It's still in early stages, but we'll want to solve the same things so let's work together on a solution.",">  Thanks for the suggestions  . Currently we have the lazy tensor as storageless (similar to torch/XLA), so the TensorImpl doesn't use a storage object and has_storage()/storage() will return false/raise error. I will sync up with  about the Lazy Tensor Core and how it plans to use any LazyStorage.  ","Hi  ,   we have followed your suggestion and used a dummy storage for our lazy tensor. With this, I am abandoning this PR. "
rag,is_alias_of support for storageless tensors," ðŸš€ The feature, motivation and pitch Motivation:  Torch DDP code uses is_alias_of() for two tensors to check if they share the same storage. The current is_alias_of() API on the tensor checks if the underlying Storage class for both the tensors has a common storage_impl_. This is also not a virtual method, so not an overridable API for the tensor currently. For backends like HPU, that use lazy storageless tensors and uses DDP for distributed training, the is_alias_of() API doesn't work.  Pitch:  The is_alias_of() API by default should check if the tensor has storage, before comparing the storage_impl_. This should ensure for storageless tensors, the default API doesn't fail with no storage error. Also, the is_alias_of() API needs to be virtual, so that a backend can override it and provide its own implementation for  is_alias_of(). The backend can provide a result by using its own mechanism to detect alias tensors that are storageless. For HPU lazy storageless tensors, the backend would determine if they tensors are sharing storage internally on the device and return a result accordingly.  Alternatives _No response_  Additional context _No response_",2022-01-17T06:18:14Z,triaged module: partial aliasing module: ddp module: lazy,open,0,0,https://github.com/pytorch/pytorch/issues/71377
yi,"Trying to run build_libtorch.py getting, ""No module named 'typing_extensions'"," ðŸ› Describe the bug File ""/home/jet/pytorch/tools/codegen/gen.py"", line 3, in      from typing_extensions import Literal ModuleNotFoundError: No module named 'typing_extensions' CMake Error at cmake/Codegen.cmake:243 (message):   Failed to get generated_headers list Call Stack (most recent call first):   caffe2/CMakeLists.txt:2 (include)  Versions Python 3.8.12   aarch64 Jetpack 4.6 Nvidia Jetson AGX Xavier",2022-01-16T01:53:54Z,triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/71360,"Hi, This is my first time contributing to OSS. Please correct me if I missed something.  This appears to be a setup issue. At a top level, this error suggests a dependency is missing from your python environment. I see a couple of possibilities: 1. Install the dependencies(conda install) OR 2. Find suitable prebuilt PyTorch pip wheel installers for your setup Background: I found build_libtorch.sh here.  the script content says:  Which leads us to the docs, specifically the Install dependencies topic:  Also, there is a note just above this command, which may apply to you: `If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are available here(https://devtalk.nvidia.com/default/topic/1049071/jetsonnano/pytorchforjetsonnano/) ` Notes to self: typing_extensions module was included  since Feb 2021.","Hi . I agree with  that it seems like this is a dependency install issue. If you're trying to build pytorch/pytorch, instructions for what to install are here. If this doesn't work, it would be great to get some information on what's happeningwhat command are you trying to run, what have you done before, etc?",I installed typing_extensions for python version 3.8. I was able to build libtorch on the AGX Xavier with python version 3.8.   Python versions 3.6 and 3.9 did not work.   `sudo python3.8 tools/build_libtorch.py`
rag,[RFC] Hierarchical Model Averaging (Hierarchical SGD)," ðŸš€ The feature, motivation and pitch  Infeasibility of Frequent Global Synchronization Nowadays powerful industry users (not necessarily in FAANG) like Cruise, Microsoft, and Tesla are moving towards scaling largescale training to 100+ or even 1K+ GPUs. DDP, as the most welladopted data parallelism implementation in PyTorch, fully synchronizes the gradients of all the processes at each iteration by default, which may lead to a low scaling efficiency in this case (let's assume that it may not be feasible to afford highbandwidth network environment like InfiniBand/EFA or highend GPUs for many companies). The key is that, the global synchronization at each iteration can become infeasible at a very large scale, at least for some industry users. Therefore, we may need to consider replacing the default SyncSGD with a combination of frequent partial synchronization and infrequent global synchronization. One solution I contributed to PyTorch is postlocalSGD), based on model averaging paradigm, as implemented in this module. Now it seems that the ownership is transferred to varma.  Hierarchical Model Averaging One effective approach is **hierarchical model a",2022-01-14T20:20:53Z,oncall: distributed feature module: ddp,closed,2,8,https://github.com/pytorch/pytorch/issues/71325,"Thanks for the suggestion ! I was wondering if our current DDP communication hooks are generic enough for you to try out these different ideas and implement them on your end in the mean time? It does make sense to have native support in PyTorch for some of these ideas, but we would like to primarily make DDP hackable/pluggable such that ideas like these can be prototyped easily by users themselves and then we can incorporate some of these techniques as builtin features into DDP itself. I guess since this idea also involves model averaging, we might need a generic pluggable model averaging component as well here? ","> I was wondering if our current DDP communication hooks are generic enough for you to try out these different ideas and implement them on your end in the mean time?  I don't expect PTD to allocate anyone to implement this in the near future. Actually I plan to work on the implementation at Cruise. IMO, the current DDP should be already flexible enough for supporting this idea  in other words, I don't request any enablement work from DDP. > I guess since this idea also involves model averaging, we might need a generic pluggable model averaging component as well here? We will see if that is necessary. Will need some code reviews from PTD when iterating over the implementation, in order to better incorporate into PyTorch.",Maybe related: https://twitter.com/m_ryabinin/status/1481994243761713153,"> Maybe related: https://twitter.com/m_ryabinin/status/1481994243761713153 Thanks  for the pointer! It's an interesting read. I think this paper focuses more on a faulttolerant allreduce implementation over extremely heterogeneous and extremely unreliable (potentially **preemptible**) devices. In the authors' words, the feasible environment is **spot instances or volunteer PCs**. Specifically, the heterogeneous experimental setups in the paper are: 1) 81 heterogeneous GPUs across 64 servers and **across 2 data centers**, and **additional latency is injected**.  2) 66 **preemptible** heterogeneous GPUs, and 34 of them are various devices rented on a public marketplace, spread **across 3 continents**. Honestly, I don't think the above extreme heterogeneity and unreliability would be practical for almost all the decent industry users. Except for the extreme setup in the paper, I really doubt Moshpit SGD that requires tworound allreduce involving all the nodes could still outperform more commonly used model averaging approaches or Gossip SGD, and I will be also very surprised if Yandex will have a feasible production environment to adopt Moshpit SGD. Note that if the paper targets a federated learning setup, I don't think PyTorch DDP is a fair comparison, since it's hard to imagine that allreduce will be used for federated learning. By contrast, this RFC targets a much more common scenario in industry  a large number of homogeneous GPUs: each machine usually has multiple GPUs (so the  of machines are much smaller than the  of GPUs), and these machines are usually in the same cluster.","Yeah, I mostly referred to its separation of nodes into groups and aggregation happening within those groups, so maybe that aspect is related to the proposed hierarchical grouping / aggregation.","Thanks for the proposal Yi! I wonder if this can be incorporated into the model averaging library, or if we can directly use ModelAverager/PeriodicModelAverager to implement this idea (both of the interfaces look quite generic and pluggable): https://github.com/pytorch/pytorch/tree/master/torch/distributed/algorithms/model_averaging","> Yeah, I mostly referred to its separation of nodes into groups and aggregation happening within those groups, so maybe that aspect is related to the proposed hierarchical grouping / aggregation. Thanks ! Both ideas are indeed relevant in this sense.","> Thanks for the proposal Yi! I wonder if this can be incorporated into the model averaging library, or if we can directly use ModelAverager/PeriodicModelAverager to implement this idea (both of the interfaces look quite generic and pluggable): https://github.com/pytorch/pytorch/tree/master/torch/distributed/algorithms/model_averaging Yes, I am working on prototyping this idea based on the existing model averaging foundation."
rag,[RFC] Cross-Process Performance Analysis: Straggler Detection," ðŸš€ The feature, motivation and pitch  Motivation: Limitation of Existing Profiling Approach To conduct PyTorch distributed training performance analysis, currently a recommended way is profiling by using PyTorch profiler or Nsight. As a result, every profiled process can output a trace file, and users usually only need to take one or two representative files to analyze (mostly for the case of data parallelism). However, one limitation of such approach is no easy support for crossprocess performance analysis. A prominent example is **straggler detection**, because it's hard to identify straggling by only looking into a small number of individual trace files, and if the training runs at a relatively large scale, it's also impractical for users to eyeball a large number of trace files.  Impact Note that as we scale out trainings to more and more machines nowadays, straggler issues can become increasingly common, especially given that the most welladopted PyTorch distributed training implementation, DDP is highly sensitive to stragglers  the allreducebased gradient synchronization has to wait for the slowest process at every iteration. I believe that st",2022-01-14T09:31:25Z,high priority triage review oncall: distributed feature module: c10d module: ddp,open,2,8,https://github.com/pytorch/pytorch/issues/71303,"I wonder if we need to develop something entirely new here, or if we just need to take something like `_get_ddp_logging_data` and make the data/insights more relevant/easily digestible by end users. For example, straggler detection can be handled with `_get_ddp_logging_data` (admittedly in a not very user friendly manner) today.  I know you mentioned ease of use, will the use case not be satisfied by using DDP APIs such as `get_logging_data` or enabling this detection with TORCH_DISTRIBUTED_DEBUG?  Longer term, ideally outputs from something like `_get_ddp_logging_data` should be visible in Tensorboard for OSS users. There is already a distributed profiling view developed by Microsoft: https://pytorch.org/blog/pytorchprofiler1.9released/, we could possibly look into adding to this. ","> will the use case not be satisfied by using DDP APIs such as get_logging_data or enabling this detection with TORCH_DISTRIBUTED_DEBUG? I think both routes could work, so I don't have any preference here. Personally I would not prefer the name of **TORCH_DISTRIBUTED_DEBUG** mode in this context, because this mode is mainly designed for debugging distributed training failures that normally lead to a certain uninformative error (e.g., NCCL timeout). I am not sure if we want to view straggler as a **bug** here. This may cause some user confusion. > Longer term, ideally outputs from something like _get_ddp_logging_data should be visible in Tensorboard for OSS users. This can be a good idea. Just check if we can have any more lightweight solution before reaching this stage. Additionally, would love to see if we can have some analytics on top of the raw data to summarize the extent of straggling. This can help a lot on a fleetwide performance analysis. > There is already a distributed profiling view developed by Microsoft: https://pytorch.org/blog/pytorchprofiler1.9released/, we could possibly look into adding to this. This can be a good entry point. I have used this distributed view on TensorBoard, and I can confirm it does not have any support on straggler detection yet.","I guess the approach might depend on the scope. If you only care about DDP stragglers, having a custom comm hook totally makes sense. If you would like to cover all collectivebased features (DDP/ZeRO/FSDP), having a context manager might be more generic. It can be sth like below, where `sd.summary()` can tell how much time is wasted due to straggler on each process.  ","> I guess the approach might depend on the scope. If you only care about DDP stragglers, having a custom comm hook totally makes sense. If you would like to cover all collectivebased features (DDP/ZeRO/FSDP), having a context manager might be more generic. It can be sth like below, where `sd.summary()` can tell how much time is wasted due to straggler on each process. >  >  Thanks for the suggestion !  > I guess the approach might depend on the scope. If you only care about DDP stragglers, having a custom comm hook totally makes sense. Currently my end will only need to target DDP scope. > If you would like to cover all collectivebased features (DDP/ZeRO/FSDP), having a context manager might be more generic. Personally I probably will prefer an alternate approach, which can be a separate module to run some offline analysis over traces collected by torch profiler. A few reasons: 1. In practice, users may want to run profiling first instead of directly running straggler detection, so it would be nice if we can incorporate such a new context manager into profiler context manager. We still have the advantage of not being restricted to any specific collective communication. 2. Microsoft folks working on profiler told me that they plan to upstream a module PT1.11, which can provide an API to conveniently export perf stats from trace files. This makes such route more viable in the future. 3. This approach can not only support straggler detection, but also other tracebased analysis as well. On the other hand, I think a debug DDP comm hook (or some analysis based on DDP logging data) for straggler detection can still make sense. 1. It's more lightweight than enabling profiling, which is often more than adding a context manager for multinode training in practice. Profiling often requires other setup like uploading trace files to a cloud storage w/o exceeding the rate limit. 2. It's much easier for industrial users to deploy a DDP comm hook (or enable DDP logging) than to add a context manager out of training loop fleet wide, because setting up a DDP comm hook or (DDP logging) can be totally decoupled from the training loop, which means such setup could be done in one place. In contrast, for industrial users every use case usually has a different training loop in different files, and hence the workload will be much larger, unless a higherlevel API (probably based on Lightning or Ignite) has unified these loops in one place.",Adding hipri as straggler detection has come up quite a bit both internally and for use cases in OSS,Did some other PR meet this need? Trying to track down some stragglers but it seems discussion of them in general is rare across the PyTorch ecosystem.,The way we do this internally is we have a high performance structured logging system (Scuba) and we send data from all ranks there and then can look for patterns across multiple ranks. Not sure what the best OSS equivalents here are though.,
transformer,Use SLEEF functions for erf/exp on macOS ARM64,"The NEON `Vectorized` implementation does not use SLEEF functions in order to compile on mobile platforms. However, SLEEF is already compiled on macOS ARM64 and is safe to use on that platform. Since transformers rely heavily on `erf` (GELU) and `exp` (softmax), using the SLEEF implementations of `erf`/`exp` can provide large speedups of transformers on Apple M1 Macs. For instance, we have found spaCy transformer models to be 20% faster when using the SLEEF implementation of these functions. This PR is a followup to CC(Use SLEEF functions for NEON vectors on macOS ARM64), but with a more limited scope as suggested in https://github.com/pytorch/pytorch/pull/70354pullrequestreview852154370. Profile of a tranformer model before this change:  Most time is spent in `erff`/`expf`. Profile of the same transformer model after this change:  Time spent in `erff` is reduced from 1.94min to 1.18min, time spent in `expf` is reduced from 60s to 8s.",2022-01-14T08:44:21Z,open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/71302," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/danieldk/pytorch/blob/62506db82b777a642edcb45e4192aa53a5d9678d/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71302**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 62506db82b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",Superseded by merging of CC(Use SLEEF functions for NEON vectors on macOS ARM64).
transformer,ONNX: export custom model to ONNX leads to different output," ðŸ› Describe the bug Hi all, I have finetuned a `sentencetransformers/LaBSE` on a binary sentence classification task and I am now trying to export it to ONNX. However, the ONNX model's outputs do not match the PyTorch ones.    Steps to reproduce the behavior:   Also, I'm getting this warning, which I suspect to be the reason of the outputs divergence:  Am I doing something wrong here? Please let me know if you need anything else from my side.  Versions PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 10.14.6 (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.9.9 (main, Jan  6 2022, 16:18:49)  [Clang 10.0.1 (clang1001.0.46.4)] (64bit runtime) Python platform: macOS10.14.6x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] numpy==1.21.1 [pip3] torch==1.10.0 [pip3] torchvision==0.11.1 [conda] ",2022-01-12T10:19:45Z,module: onnx triaged onnx-needs-info,closed,0,2,https://github.com/pytorch/pytorch/issues/71207,Can you please try to reproduce using the latest pytorch nightly?,"Hi  The warnings above are actually harmless and can be ignored. The numerical differences can only be address with a repro, as requested in Feb 2 by a colleague Since a repro was not provided, weâ€™ve gone ahead and closed this issue because it is stale. If you still believe this issue is relevant, please feel free to reopen the issue and we will triage it as necessary. Please specify in a comment any updated information you may have so that we can address it effectively. We encourage you to try the latest pytorchpreview (nightly) version to see if it has resolved the issue. Thanks, ONNX Converter team"
transformer,traced module fails on second execution," ðŸ› Describe the bug I have a model that I save usint torch.jit.trace and torch.jit.save. When I load it, and execute it, the first execution works correctly, but the second one fails with the following error  The model consists of a normalizing flow conditioned on a transformer. When tracing the transformer part alone it doesn't causes this error, but when tracing the full model, it does!  Versions Checked with torch 1.8.0 and torch 1.10.0. Happens both on CPU and GPU",2022-01-11T18:22:14Z,oncall: jit,open,0,3,https://github.com/pytorch/pytorch/issues/71165,"Update: it seems to be happening only when the forward method has a keyword argument. When I change my forward method from `def forward(self, x, cond, sldj, reverse=False):` to `def forward(self, x, cond, sldj):` the error goes away! Update 2: actually that was on a minimal example I had, but on the full code, doing that, it still fails...","it seems the error also happens when i return only one element of the tuple a module returns.  This works: `trace = torch.jit.trace(lambda x,y: model.output_mod_glows0, (noise,latent), check_trace=False)` And this fails `trace = torch.jit.trace(lambda x,y: model.output_mod_glows0[0], (noise,latent), check_trace=False)` This seems to be the main failure mode: when an intermediate tensor is not used for the output", can you provide a repro script for this?
rag,Rollup: forward-mode AD operator coverage,"NOTE: See also the tracker) for forwardoverreverse formulas:  Sometimes implementing a forward AD formula will also implicitly support its forwardoverreverse formula, make sure to comment/mark as completed on that issue as well to keep it up to date. These are ops that currently have `supports_forward_ad=False` (usually implicitly) in their OpInfo. Because not all ops have OpInfos yet, some ops that don't support forward AD may be missing from this list. **Please file an issue or comment here if there is an op you'd like to see supported, but is not on this list**. A complete list of formulas is included at the bottom. For a short tutorial on adding a forward AD formula see:  CC(Forward mode AD for linear algebra functions) If you take a brief look at the list below, you'll notice that formulas in `derivatives.yaml` don't generally correspond onetoone with OpInfo entries, so it is sometimes hard to know just from staring at the op name, which formula is needed to support it. To figure out what formula corresponds to a particular op, set `supports_forward_ad=True` in the OpInfo and run the test `test_forward_mode_AD`, the error will tell you which fo",2022-01-10T18:23:09Z,module: autograd triaged actionable module: forward ad,open,0,6,https://github.com/pytorch/pytorch/issues/71117,"I added a few operations that will have AD support once some PRs are merged, and I also removed a few duplicated functions. It's worth noting that this list includes some operations that will be removed in the next version, such as `symeig` and similar linalg functions.","Oh, awesome! Thank you! Now we have list of small PRs inbetween large things :) There are some linear algebra functions on this list that are going to be deprecated, so they could get excluded. Most torch.linalg functions support forward AD.","Updated to exclude the deprecated linear algebra functions that will be deprecated, thanks!","About linalg: `linalg.lstsq` has forward AD implemented. svd/pcalowrank should be able to support forward AD as they are composite operations relying on mm and svd, for which `linalg.svd` now has forward AD support, unless legacy svd is used...","NB: 's old stack at https://github.com/pytorch/pytorch/pull/69211 has many of the OpInfos that we need to test the above (binary_cross_entropy{_with_logits}, l1_loss, etc) but I don't know what the status of it is"," wanted to land the stack at some point, but it seems we both have forgotten about it. Want to give this another shot?"
yi,RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.," ðŸ› Describe the bug When using modules having torch.nn.utils.parametrize like:  Optimizing training performance by using:  Training failed with the following error: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. Anomaly detection points to the following culprit:  It seems the cached weights of a parametrize module do not have gradients available when they are used multiple time in a model.  Versions Collecting environment information... PyTorch version: 1.10.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: Could not collect Libc version: glibc2.10 Python version: 3.7.7 (default, Mar 23 2020, 22:36:06)  [GCC 7.3.0] (64bit runtime) Pytho",2022-01-09T02:18:39Z,module: autograd triaged module: nn.utils.parametrize,closed,0,10,https://github.com/pytorch/pytorch/issues/71062,"This model does not have any `nn.utils.Parametrization`s that I can see. Where did you use parametrizations? Parametrised weights can indeed be used several times in the model, and that's exactly what `cached` is for. As a side note, I reckon you've already tried this, but does the training succeed when you do not register the parametrisations?",Thanks Lezcano  for your reply. I cannot share the model but I can reproduce the issue consistently with different models. Here is how I use the parametrization.  Overall the training works WITHOUT the parametrization (standard Conv2D instead of SWSConv2d) but an ideal scenario because the SWS helps the convergence. I have in my models a siamese network for feature extraction which reuses the same weights 2 times. The training also works WITH the parametrization SWSConv2d and with torch.nn.utils.parametrize.cached(): commented. It only failed with the cache enabled randomly during training. I suspect the backward pass try to recompute gradients on SWSConv2d already computed in the past. I do not know if we can force to keep the gradients in a cached parametrization?,"What seems odd to me from thatis the line `self.weight.to(input.device, copy=True)`. Why do you move it to a different device, rather than creating `nn.Sequential` in the correct device or moving the whole model to the correct device via `to(device)`?","That's a good point. I had to include this line because when I use the cache it appears the weights are save from one GPU but used to another GPU. Overall, I move the model to the correct devices but during training when I use cached parametrization the weights can be in another GPU.",I am using DDP with mixed precision training on 8 GPU. Not sure how the weights should be cached?, + caches + multi GPU setups. I'm not sure of what's the behaviour of `parametrization`s together with DDP.,The issue is easier to reproduce with DP and autograddetectanomaly. I am wondering if the problem is simply the cache is global and not thread safe?,"Can you share what your training loop looks like as well please? Or even better a full example we can run to reproduce this? Note that the cached result is computed in a differentiable way. So if you do multiple backwards within the cached context, you are trying to backward multiple times indeed so I would expect this error.","Unfortunately this is a lot of code I cannot share but I will try to create a minimal example to reproduce the issue. I am using Pytorch Lightning for the training loop. The network is a simple feed forward with a siamnese branch, and to be exact, the issue only occurs when the cache is enabled.",I found the issue by working on a minimal example. The issue was simply caching the forward pass not including the computation of the loss:  I think we can close this ticket. ^^
rag,[Model Averaging] Update the documentation of PeriodicModelAverager,"Here 20 is a bad example, since the warmup step is set as 100. 200 iterations will make much more sense. ",2022-01-07T06:00:21Z,oncall: distributed open source cla signed,closed,1,5,https://github.com/pytorch/pytorch/issues/70974," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/wayi1/pytorch/blob/48f296813d3e241f402a14b4bf62bdf4a70332fe/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ","Hi !  Thank you for your pull request.  We **require** contributors to sign our **Contributor License Agreement**, and yours needs attention. You currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70974**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 48f296813d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
agent,DISABLED test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA),Platforms: rocm  ,2022-01-06T05:03:40Z,module: rocm skipped,closed,0,2,https://github.com/pytorch/pytorch/issues/70890,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.   We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows.","Closing, duplicate of  CC(DISABLED test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA))"
agent,DISABLED test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA),Platforms: rocm https://ci.pytorch.org/jenkins/job/pytorchbuilds/job/pytorchlinuxbionicrocm4.5py3.7test1/45//console  ,2022-01-06T00:49:47Z,module: rocm skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/70883,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.   We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows."
agent,DISABLED test_median_nan_values_cuda_float32 (__main__.TestReductionsCUDA),Platforms: rocm  ,2022-01-05T23:13:24Z,module: rocm skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/70878,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float32 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float32 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.   We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows."
agent,DISABLED test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA),Platforms: rocm  ,2022-01-05T23:04:26Z,module: rocm module: tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/70876,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.   We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows."
chat,[pytorch][aten][cuda] move CUDAGeneratorImpl.h to ATen/cuda,"Summary: This patch moves a CUDAspecific file, `CUDAGeneratorImpl.h` to `ATen/cuda` as the following TODO comment in  `CUDAGeneratorImpl.h` suggests:  Differential Revision: D33414890",2022-01-04T23:23:24Z,fb-exported cla signed,closed,0,7,https://github.com/pytorch/pytorch/issues/70650," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/shintaroiwasaki/pytorch/blob/79dbfd6cddab5f24b358769f7036fa0bf7540823/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70650**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 79dbfd6cdd (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33414890,This pull request was **exported** from Phabricator. Differential Revision: D33414890,This pull request was **exported** from Phabricator. Differential Revision: D33414890,This pull request was **exported** from Phabricator. Differential Revision: D33414890,"iwasaki has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
chat,[pytorch][aten][cuda] fix LpNormFunctor,"Summary: `&` has lower precedence than `==`, so `==` will be evaluated first. This behavior should not be intended. This patch fixes it. Test Plan: ðŸ§  Carefully check the change. Differential Revision: D33397964",2022-01-04T15:42:08Z,fb-exported cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/70601," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/shintaroiwasaki/pytorch/blob/0a3484acab514fddea5bb05f0fbecbb9b4258567/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70601**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 0a3484acab (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base d35fc409ad from Jan 04 until Jan 05   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * linuxxenialpy3.7clang7asan / test (default, 1, 3, linux.2xlarge) from Jan 04 until Jan 05 (d35fc409ad  1681323ddc)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D33397964
yi,torch.cat without copying memory," ðŸš€ The feature, motivation and pitch  Principle Today, the concatenation implemented on pytorch consists in the allocation of a new tensor. I would like to know if it is possible to realize a concatenation of contiguous and/or noncontiguous tensors without memory duplication.  Example 1 : contiguous concatenation The following code is executed with allocation of a new tensor `concatenated_tensor`:   I'd like to enable the same scenario, but have `concatenated_tensor` as a view of `tensor1` and `tensor2`. In terms of UX, I don't know what to propose.  Note: since I'm a new pytorch user, maybe the word ""view"" is not appropriate. The lowlevel idea is to consider `concatenated_tensor` as a list of pointers to tensors.  Example 2 : noncontiguous concatenation  Next, I would like to enable the following scenario, if possible:    Alternatives _No response_  Additional context Discussed in CC(torch.utils.data.DataLoader  returned views support) with . See discussion/34609. ",2022-01-04T15:22:32Z,triaged module: numpy module: viewing and reshaping,closed,0,3,https://github.com/pytorch/pytorch/issues/70600,"> I would like to know if it is possible to realize a concatenation of contiguous and/or noncontiguous tensors without memory duplication. The answers of  on https://discuss.pytorch.org/t/concatenatetensorswithoutmemorycopying/34609/13 explain how difficult this is. It does not fit the strided model of a tensor at all. It'd be a ton of work  I think it's safe to say this won't happen. I'll let someone else decide, but I propose to close this issue.","No problem, we'll close this issue.","nestedtensor https://github.com/pytorch/nestedtensor might be doing what you want. Also, if you don't need autograd, `_foreach_*` ops support operations on the lists of tensors, so you don't need to concatenate them in advance."
transformer,[docs] Transformer: no batch dim support doc update,,2022-01-04T10:42:07Z,open source cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/70597," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/kshitij12345/pytorch/blob/03d52811f38fa54def33114ab6eefed281e6a1d9/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:    For more information, please take a look at the CI Flow Wiki. ","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70597**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 03d52811f3 (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base d35fc409ad on Jan 04 from  1:31am to  6:07pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands:   * linuxxenialpy3.7clang7asan / test (default, 1, 3, linux.2xlarge) on Jan 04 from  1:31am to  6:07pm (d35fc409ad  4d08db0cb2)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
rag,[feature request] Exponential moving average (EMA) of a tensor across a dimension," ðŸš€ The feature, motivation and pitch If the dimension size is high, Python loop may be inefficient and maybe inplace computation graph (for efficiency) could confuse TorchScript. In `pandas` this exists as `ewm()` function: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html In NumPy various reimpl methods exist (recurrent; truncated based on convolution): https://stackoverflow.com/questions/42869495/numpyversionofexponentialweightedmovingaverageequivalenttopandasewm  Alternatives Special APIs for returning convolution weights, e.g. https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter1d.html, and the same for exponential weights. Then have a recipe/example in docs of using these weights to create an approximate exponential moving average via convolution. Although maybe given that gaussian / ewm filtering is very popular, both APIs would be useful.  Additional context _No response_ ",2022-01-02T09:06:31Z,feature triaged module: numpy,open,0,1,https://github.com/pytorch/pytorch/issues/70555,probably some API designs for such function already exist in PyTorch time series packages...
agent,DISABLED test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest),Platforms: mac This test was disabled because it is failing on master (recent examples). ,2022-01-01T03:26:48Z,high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,open,0,11,https://github.com/pytorch/pytorch/issues/70546,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest)` * Platforms for which to skip the test: mac Within ~15 minutes, `test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest)` will be disabled in PyTorch CI for these platforms: mac. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.   We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows.","Forgot to triage this one during my oncall  , let's discuss it in next week's oncall meeting? ", Are there any examples I can look at? The recent examples link doesn't have anything since it is probably outdated now: https://www.torchci.com/failure/test_tensorpipe_set_default_timeout%2C%20TensorPipeTensorPipeAgentRpcTest. I tried the test history tool as well and it looks like there are no reports: https://gist.github.com/pritamdamania87/c2b518e7e37b85ab133b6a8e7bf6e328  ,"Went to our data backend and there were two examples:        https://github.com/pytorch/pytorch/runs/4666724994?check_suite_focus=true        https://github.com/pytorch/pytorch/runs/4564324852?check_suite_focus=true raw logs: https://osscirawjobstatus.s3.amazonaws.com/log/4666724994 https://osscirawjobstatus.s3.amazonaws.com/log/4564324852 On Thu, Mar 3, 2022 at 4:58 PM Pritam Damania ***@***.***> wrote: >   Are there any examples I can look at? The > recent examples link doesn't have anything since it is probably outdated > now: > https://www.torchci.com/failure/test_tensorpipe_set_default_timeout%2C%20TensorPipeTensorPipeAgentRpcTest > . > > I tried the test history tool as well and it looks like there are no > reports: > https://gist.github.com/pritamdamania87/35790a03dafa59cf6fa063335836a101 > >   > > â€” > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","> Went to our data backend and there were two examples: Thanks a lot, I was wondering if you could share how to get these examples from the data backend? I usually try to use the `test_history` tool, but in this case it looks like for some reason it couldn't pick up these examples.","Looking at https://osscirawjobstatus.s3.amazonaws.com/log/4564324852, it seems like an ""Address already in use error"":  ?","Ah, I think they are missing reports because the upload test stats step doesn't get to do the uploading part. The step fails when trying to access RDS to upload failures, but this should be fixed now after permissions were set.  CC(MacOS tests sometimes failing with ""botocore.exceptions.ClientError: An error occurred (AccessDeniedException)"")  And for the data, you can query the scuba table like https://fburl.com/scuba/opensource_ci_jobs/k53dbso4","> ? I had a dejavu feeling when seeing this and indeed I think this was hit before:  CC(test_backward_accumulate_grads (__main__.TensorPipeDistAutogradTest) is flaky)  CC(DISABLED test_future_wait_twice (__main__.TensorPipeRpcTest)). I gave my thoughts on this in those two issues, do those explanations make sense here?","> I had a dejavu feeling when seeing this and indeed I think this was hit before:  CC(test_backward_accumulate_grads (__main__.TensorPipeDistAutogradTest) is flaky)  CC(DISABLED test_future_wait_twice (__main__.TensorPipeRpcTest)). I gave my thoughts on this in those two issues, do those explanations make sense here?  Thanks for the pointers, looking through those two issues it seems like there are two potential reasons for this: 1. This should be avoidable if PyTorch passes a separate port for each multiplexing lane.  2. This might be what's going on here: we're turning up and tearing down connections at such a rate that all ports are unavailable. Regarding 1. can you point to where exactly we pass in this port? Regarding 2., I do feel this does seem to be unlikely since there are 65k ports usually available. "," Although looking at the code here: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/tensorpipe_agent.cppL456, it seems like we only pick a default address for UV and not port. If so, I'm assuming the port will always be 0 and the OS should pick one for us. In that case, it is indeed weird that we have an ""Address already in use"" error.","Regarding 1, there's no ""explicit"" ports ever being used by TensorPipe. What happens during these tests is that we might use an explicit port to initialize the TCPStore (or maybe not even that, if we use a FileStore) but, once the store is established and passed to the TensorPipe agent, we only ever create sockets bound on ""automatic"" ports (i.e., a port of 0, which tells the kernel to find a free port, allocate it to us and give us its number) and then the number of this port is inserted into the store so that the clients can read it and use it when connecting. Note that there are two cases in which the listen call can return EADDRINUSE (see https://man7.org/linux/manpages/man2/listen.2.html): either the explicit port number that was requested is occupied, or there are no more automatic port numbers to assign. I suspect we're thus hitting this second case. As for how many ports are actually being used by each test, we can try to estimate it but it's a bit fuzzy. Most tests use 4 ranks (though some use more?) and if they all connect to each other that makes 16 connections (because for each pair the A>B and B>A directions need separate connections). Some (or maybe all?) of these connections might eventually be replaced with a faster backend by TensorPipe (e.g., SHM) but they all ""start"" as TCP during the handshake phase. In fact, depending on which backends are being used, a single TensorPipe pipe might end up using multiple connections (in some cases up to 20, though I don't think this is happening in these tests?). So in the worst case we do indeed get a few hundred connections per test (which might be enough to exhaust the ports) but most likely much fewer. I think there's also the issue that closed connections don't immediately make their port available again. The TCP protocol needs to keep each port in a ""limbo"" for some time to ensure that stray packets that might come in late for the previous connection might not accidentally be delivered to the new owner of a port. I think some of these timeouts are tunable (and I think TensorPipe tries to minimize them?) but not all of them. This means that even if a single test doesn't exhaust the ports, running too many quick tests in a sequence might still do that. Finally there's the issue that we might not really have 65k ports to start with. That's the upper theoretical limit, but it can be lowered in the kernel. You can read the `/proc/sys/net/ipv4/ip_local_port_range` file to know the lower and upper bound of the range of ports that the kernel will use in this automatic assignment. On most baremetal machines I'd say this goes from 4k to 65k (i.e., all of them except some ""reserved"" ports). But I think we're using Docker for our tests and perhaps this limits the number of ports that are made available? Or something along those lines?"
